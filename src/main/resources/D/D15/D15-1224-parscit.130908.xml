<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003874">
<title confidence="0.793784">
From the Virtual to the Real World: Referring to Objects in Real-World
Spatial Scenes
</title>
<author confidence="0.53603">
Phil Bartie
</author>
<affiliation confidence="0.9822545">
School of Natural Sciences
University of Stirling
</affiliation>
<address confidence="0.980086">
Stirling FK9 4LA, UK
</address>
<email confidence="0.996679">
phil.bartie@stir.ac.uk
</email>
<author confidence="0.992774">
Dimitra Gkatzia, Verena Rieser
</author>
<affiliation confidence="0.9942715">
Department of Computer Science
Heriot-Watt University
</affiliation>
<address confidence="0.883707">
Edinburgh EH14 4AS , UK
</address>
<email confidence="0.994347">
{d.gkatzia,v.t.rieser}@hw.ac.uk
</email>
<sectionHeader confidence="0.997305" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868130434783">
Predicting the success of referring expres-
sions (RE) is vital for real-world applica-
tions such as navigation systems. Tradi-
tionally, research has focused on studying
Referring Expression Generation (REG)
in virtual, controlled environments. In this
paper, we describe a novel study of spa-
tial references from real scenes rather than
virtual. First, we investigate how humans
describe objects in open, uncontrolled sce-
narios and compare our findings to those
reported in virtual environments. We show
that REs in real-world scenarios differ sig-
nificantly to those in virtual worlds. Sec-
ond, we propose a novel approach to quan-
tifying image complexity when complete
annotations are not present (e.g. due to
poor object recognition capabitlities), and
third, we present a model for success pre-
diction of REs for objects in real scenes.
Finally, we discuss implications for Nat-
ural Language Generation (NLG) systems
and future directions.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999724285714286">
REG has attracted considerable interest in the
NLG community over the past 20 years (Krahmer
and van Deemter, 2011; Gatt et al., 2014). While
initially, the standard evaluation metric for REG
was human-likeness, as compared to human cor-
pora similarity as in TUNA (Gatt et al., 2009),
the field has moved on to evaluating REG effec-
tiveness by measuring task success in virtual in-
teractive environments (Byron et al., 2009; Gar-
gett et al., 2010; Janarthanam et al., 2012). Vir-
tual environments however eliminate real-world
uncertainty, such object recognition errors or clut-
tered scenes. In this paper, we investigate whether
the lessons learnt in virtual environments can be
</bodyText>
<author confidence="0.642665">
William Mackaness
</author>
<affiliation confidence="0.986818">
School of GeoSciences
University of Edinburgh
</affiliation>
<address confidence="0.914972">
Edinburgh EH8 9XP, UK
</address>
<email confidence="0.980605">
william.mackaness@ed.ac.uk
</email>
<bodyText confidence="0.999906027777778">
transferred to real-world scenes. We consider the
case where we are uncertain about the scene itself,
i.e. we assume that the complexity of the scene
is hidden and we are interested in identifying a
specific object, and thus our work differs from
approaches that generate descriptions for images
such as (Mitchell et al., 2012; Feng and Lapata,
2013; Yang et al., 2011; Yatskar et al., 2014).
Related work has focused on computer gener-
ated objects (van Deemter et al., 2006; Viethen
and Dale, 2008), crafts (Mitchell et al., 2010), or
small objects in a simple background (Mitchell et
al., 2013a; FitzGerald et al., 2013). One notable
exception is the recent work by Kazemzadeh et
al. (2014), who investigate referring expressions
of objects in “complex photographs of real-world
cluttered scenes”. They report that REs are heavily
influenced by the object type. Here, we are inter-
ested in studying REs for visual objects in urban
scenes. As the success of a RE is heavily depen-
dent on the complexity of the scene as well as its
linguistic features, we are interested in modelling
and thus predicting the success of a RE.
Initially, this paper presents and analyses a
novel, real-world corpus REAL (to be released) –
“Referring Expression Anchored Language” (Sec-
tion 2), and compares the findings to those re-
ported in virtual worlds (Gargett et al., 2010). We
then provide a detailed analysis of how syntactic
and semantic features contribute to the success of
REs (Sections 4.1, 4.2, 4.3), accounting for unob-
servable latent variables, such as the complexity
of the visual scene (as described in Section 3). Fi-
nally, we summarise our work and discuss the im-
plications of our work for NLG systems (Section
5). The dataset and models will be released.
</bodyText>
<sectionHeader confidence="0.993998" genericHeader="method">
2 The REAL Corpus
</sectionHeader>
<bodyText confidence="0.991468666666667">
The REAL corpus contains a collection of images
of real-world urban scenes (Fig. 1) together with
verbal descriptions of target objects (see Fig. 2)
</bodyText>
<page confidence="0.946361">
1936
</page>
<note confidence="0.933914">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1936–1942,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.999161">
Figure 1: Original picture. Figure 2: Target object in yellow Figure 3: The identified object by
box. the validators.
</figureCaption>
<bodyText confidence="0.999921555555555">
generated by humans, paired with data on how
successful other people were able to identify the
same object based on these descriptions (Fig. 3).
The data was collected through a web-based inter-
face. The images were taken in Edinburgh (Scot-
land, UK), very early one summer morning. This
was necessary to reduce the occlusion of city ob-
jects from buses and crowds, and to minimise
lighting and weather variations between images.
</bodyText>
<subsectionHeader confidence="0.969202">
2.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.998767">
There were 190 participants recruited (age be-
tween 16 to 71). Each participant was presented
with an urban image (Fig. 1), where the target ob-
ject was outlined by a yellow box (Fig. 2), and was
asked to describe the target using free text. After
completing a (self-specified) number of tasks, par-
ticipants were then asked to validate descriptions
provided by other participants by clicking on the
object using previously unseen images (Fig. 3).
</bodyText>
<table confidence="0.998488">
# participants 190
# images/ stimuli 32
# descriptions 868
# verifications 2618
− ambiguous 201
− not found 75
− correct 1994
− incorrect 251
− NA 7
</table>
<tableCaption confidence="0.99981">
Table 1: The REAL corpus
</tableCaption>
<bodyText confidence="0.999933461538461">
Overall, 868 descriptions across 32 images were
collected, averaging around 27 descriptions per
image. The balance of generation and validations
was adjusted to ensure that all descriptions were
identified by at least 3 other participants, generat-
ing 2618 image tag verifications. All cases were
manually checked to determine if the ‘correct’
(green) or ‘incorrect’ (red) target had been identi-
fied Fig. 3. Overall, 76.2% of human descriptions
provided were successfully identified. For the ex-
periments reported in following sections, we sum-
marised answers categorised as ‘incorrect’, ‘ambi-
tious’ and ‘not found’ as unsuccessful.
</bodyText>
<subsectionHeader confidence="0.99737">
2.2 Comparison to GIVE-2 Corpus
</subsectionHeader>
<bodyText confidence="0.999964454545454">
We now compare this data with human data gen-
erated for the GIVE-2 challenge (Gargett et al.,
2010). In GIVE-2, the target objects have dis-
tinct attributes, such as colour and position. For
instance, an effective RE in GIVE-2 could be “the
third button from the second row”. In real-world
situations though, object properties are less well
defined, making a finite set of pre-defined quali-
ties unfeasible. Consider, for instance, the build-
ing highlighted in Figure 2, for which the follow-
ing descriptions were collected:
</bodyText>
<listItem confidence="0.972963818181818">
1. The Austrian looking white house with the dark
wooden beams at the water side.
2. The white building with the x-shape balconies. It
seems it’s new.
3. The white building with the balconies by the river.
4. Apartments with balconies.
5. The nearest house on right side. It’s black and white.
6. The white and black building on the far right, it has
lots of triangles in its design.
7. The rightmost house with white walls and wood fin-
ishings.
</listItem>
<bodyText confidence="0.997740666666667">
It is evident that the REAL users refer to a va-
riety of object qualities. We observe that all par-
ticipants refer to the colour of the building (white,
black and white, greyish-whitish) and some men-
tion location (by the river, at the water side).
Experimental Factors influencing Task Per-
formance: In REAL, task success is defined as
the ability to correctly identify an object, whereas
in GIVE-2, task success refers to the successful
completion of the navigation task. In contrast to
GIVE-2, not all REAL participants were able to
correctly identify the referred objects (76.2% task
</bodyText>
<page confidence="0.977277">
1937
</page>
<table confidence="0.934867">
GIVE-2 REAL
German English
Overall task success 100% 100% 76.2%
Task success (female) 100% 100% 78.8%
Task success (male) 100% 100% 69.6%
Length of descriptions 5.2 4.7 16.01
(no. words)
Length of descriptions NA NA 97.36
(female)
Length of descriptions NA NA 91.38
(male)
</table>
<tableCaption confidence="0.821461">
Table 2: Descriptive statistics for GIVE-2 and
REAL
</tableCaption>
<bodyText confidence="0.994788388888889">
success). We assume that this is because GIVE-
2 was an interactive setup, where the participants
were able to engage in a clarification dialogue.
Gender: In REAL, gender was not a significant
factor with respect to task success (Mann-Whitney
U test, p = 0.2). Length of REs (no. words): In
REAL, females tend to provide lengthier REs than
males, however the difference is not statistically
significant (Mann-Whitney U test, p = 0.58). In
GIVE-2, only German females produced signifi-
cantly longer descriptions than their male counter-
parts. Relation between length (no. words) and
task success: The REAL data shows a positive
relationship between length and success rate, i.e.
for a one word increase in length, the odds of cor-
rect object identification is significantly increased
(p &lt; 0.05, Logit), i.e. longer and more complex
sentences lead to more successful REs.
</bodyText>
<sectionHeader confidence="0.932345" genericHeader="method">
3 Quantifying the Image Complexity
</sectionHeader>
<bodyText confidence="0.999706230769231">
We assume that the complexity of the urban scene
represented in the image is hidden due to the lack
of semantic annotations. Our dataset does not in-
clude any quantifiable image descriptions, such as
computer vision output as in (Mitchell et al., 2012)
or manual annotations as in (Yatskar et al., 2014).
In addition, the same RE might not always re-
sult in successful identification of an object due
to scene complexity. In order to marginalise the
effect of the scene complexity, we exploit the mul-
tiple available data points per image. This allows
us to estimate the average success rate of each re-
ferring expression SRRE (the proportion of suc-
cessful validations) and the average success rate
of each image SRi (the proportion of the correctly
identified objects in the image). We use SRi to
marginalise over the (hidden) image complexity,
where we assume that some pictures are inher-
ently more complex than others and thus achieve
lower success rates. Similar normalisation meth-
ods are used for user ratings to account for the fact
that some users are more “tolerant” and in general
give higher ratings (Jin and Si, 2004). We employ
Gaussian normalisation (Resnick et al., 1994) to
normalise image success rates by considering the
following factors:
</bodyText>
<listItem confidence="0.919586454545455">
1. Shift of average success rate per image: some
images are inherently easier than others and gain
higher success rates, independently of the REs
used. This factor can be accounted by subtract-
ing average success rates of all images from the
average rating of a specific image x.
2. Different ratings: there are 27 REs per image on
average, some of which are harder to understand
than others, thus they gain lower success rates. To
account for this, the success rates of each image
are divided by the overall SR variance.
</listItem>
<bodyText confidence="0.9938235">
The normalised image success rate (NSRi) per
image x is defined by the following equation:
</bodyText>
<equation confidence="0.997737">
SRi(x) − SRi
NSRi(x) = (1)
�En (SRi(x) − SRi)2
</equation>
<bodyText confidence="0.9993835">
Using the (NSRi), we now investigate the
REs in terms of their linguistic properties, includ-
ing automatically annotated syntactic features and
manually annotated semantic features.
</bodyText>
<sectionHeader confidence="0.994953" genericHeader="method">
4 Modelling REG Success
</sectionHeader>
<bodyText confidence="0.999224">
Unlike previous work, we use both successful and
unsuccessful REs in order to build a model that is
able to predict the success or the failure of a RE.
</bodyText>
<subsectionHeader confidence="0.995725">
4.1 Syntactic Analysis of REG Success
</subsectionHeader>
<bodyText confidence="0.999317352941176">
We use the Stanford CoreNLP tool (Manning et
al., 2014) to syntactically annotate the REs and we
investigate which linguistic features contribute to
the RE success in relation to the image complexity.
Note that these analyses are based on normalised
values, as discussed in Section 3).
Predicting RE Success Rate (SRRE): Initially,
we compare successful and unsuccessful REs by
taking the upper and lower quartiles and extract-
ing their syntactic features., i.e. the top and bottom
25% of REs with respect to their average success
rate, and group them into two groups. We then
extract syntactic features of these two groups and
compare their frequencies (occurrence per RE),
means, and standard deviations (Table 3), and
compare them using a t-test (p &lt; 0.05). The dif-
ference between successful and unsuccessful ex-
</bodyText>
<page confidence="0.936553">
1938
</page>
<table confidence="0.999854941176471">
Successful REs Successful REs
Mean SD Freq. Mean SD Freq.
NP* 7.35 3.958 100 6.7 3.8 100
VP 1.45 1.673 41.8 1.46 1.923 58
PRN .02 .181 2.1 .03 .193 2.4
NNP* .57 1.131 27 .38 .918 19.3
NN* 4.2 2.284 98.8 3.79 2.441 98.1
DT 2.59 1.791 86.7 2.63 1.813 85.4
JJ* 1.92 1.61 80.9 1.66 1.288 81.1
CC .4 .645 32.8 .31 .588 25
PP 2.52 1.754 92.3 2.54 1.778 85.8
ADJP .2 .536 16.2 .041 .597 19.3
ADVP .27 .538 22.8 .25 .539 19.8
RB .34 .639 26.6 .34 .859 21.2
VBN* .22 .465 20.3 .31 .445 10.8
NNS .61 .902 40.7 .72 .782 53.3
CD .27 .552 22 .25 .478 23.6
</table>
<tableCaption confidence="0.997109">
Table 3: Statistics regarding the linguistic fea-
</tableCaption>
<bodyText confidence="0.968519875">
tures in successful vs unsuccessful referring ex-
pressions. (* denotes significant difference at p &lt;
0.05).
pressions lies in the use of NP (Noun phrases),
NNP (Proper noun, singular), NN (Noun, singu-
lar or mass), JJ (Adjective) and VBN (Verb, past
participle) (Table 3). Successful REs include more
NPs, including NNPs and NNs, which indicates
that more than one reference is used to describe
and distinguish a target object. This could mean
that distractors are explicitly mentioned and elim-
inated or that the object of interest has a complex
appearance, as opposed to simply structured ob-
jects, such as buttons, in GIVE-2. For example,
the following description refers to a complex ob-
ject:
The large American-style wooden building with bal-
cony painted cream and red/brown. Ground floor is a
cafe with tables and parasols outside.
In addition, successful REs contain significantly
more adjectives and verbs in past participle1,
which indicates that the object was further de-
scribed and distinguished using its attributes, as
for instance the following description:
Large modern glass fronted building, butted up
against traditional Victorian terrace, slightly set back
from road and with facing bowed frontage.
The main difference between successful and un-
successful REs is the amount of detail provided to
describe and distinguish the target object. This is
also in-line with our previous results that success
is positively correlated to the number of words
</bodyText>
<footnote confidence="0.93178">
1A participle is a form of a verb that is used in a sentence
as modifier, and thus plays a role similar to that of an adjec-
tive or adverb, such as built or worn.
</footnote>
<table confidence="0.9839995">
Models R2
Syntactic: NP+PP+ADVP+CD+length .15
Semantic model: taxonomic + absolute .338
Joint model: PP + taxonomic + absolute .407
</table>
<tableCaption confidence="0.999378">
Table 4: Models and their fit.
</tableCaption>
<bodyText confidence="0.992948542857143">
used (Section 2.2) and it might explain why hu-
mans overspecify.
To further verify this hypothesis, we build a pre-
dictive model of average success rate, using multi-
ple step-wise linear regression with syntactic fea-
tures as predictors. We find a significant (p &lt;
0.05) positive relationship between success rate
and NP, PP (Prepositional phrase), ADVP (Adver-
bial phrase), CD (Cardinal number), and length
(Table 4). NPs are used to distinguish and de-
scribe the target object. ADVPs and PPs serve a
similar function to adjectives in this case, i.e. to
describe further attributes, especially spatial ones,
like “the one near the river”, “next to the yellow
building”. Cardinal numbers are used to refer to
complex structured features of the target object,
e.g. two-story building or two large double doors.
Predicting Image Success Rate (NSRimage):
We repeat a similar analysis for estimating how
syntactic features relate to image success rate, i.e.
how the image complexity, as estimated from the
success rate of an image, influences how humans
describe the target object, i.e. how human gener-
ated descriptions change with respect to the im-
age complexity as estimated from the (normalised)
success rate of an image. We find that humans
use significantly more PPs and number of words
(p &lt; 0.05) when describing complex images.
In sum, syntactical features, which further de-
scribe and distinguish the target object (such as
NPs, ADJ, and ADVPs and PPs) indicate success-
ful REs. However, they cannot fully answer the
question of “what makes a RE successful”, there-
fore we enrich our feature set using manually an-
notated semantic features.
</bodyText>
<subsectionHeader confidence="0.994817">
4.2 Semantic Analysis of REG
</subsectionHeader>
<bodyText confidence="0.999906375">
We extract semantic features by annotating spa-
tial frames of reference as described in (Gargett
et al., 2010). We annotate a sample of the corpus
(100 instances), which allows us to perform a di-
rect comparison between the two corpora.
Comparison to GIVE-2 Corpus: We observe
that in the REAL corpus, the taxonomic property,
the relative property and the macro-level landmark
</bodyText>
<page confidence="0.990247">
1939
</page>
<table confidence="0.999948857142857">
Spatial Frame REAL GIVE-2
Ger- En-
man glish
Taxonomic Property 92* 53.66 58.51
Absolute Property 57* 85.37 92.53
Relative Property 15* 6.83 4.56
Viewer-centred 15 15.61 12.45
Micro-level landmark 9* 13.17 17.84
intrinsic
Distractor Intrinsic 5* 10.73 14.11
Macro-level landmark 43* 6.83 4.15
intrinsic
Deduction by elimina- 1 0.98 3.32
tion
</table>
<tableCaption confidence="0.9575655">
Table 5: Frequency of semantic frames in REAL
vs. GIVE-2 (* denotes significant differences at
</tableCaption>
<bodyText confidence="0.988568818181818">
p &lt; 0.05, x2 test).
intrinsic property of the object in question are used
significantly more often than in the GIVE-2 corpus
(Table 5)2.
In contrast, in GIVE-2 the absolute property of
the object, such as the colour, and references to
distractors are used significantly more often than
in REAL. These results reflect the fact that scenes
in REAL were more complex, and as such, rel-
ative properties to other objects and landmarks
were used more often. In GIVE-2, target objects
were mostly buttons, therefore, absolute descrip-
tions (“the blue button”) or referring to an intrin-
sic distractor (“the red button next to the green”)
are more frequent. In addition, real-world en-
vironments are dynamic. Humans choose to re-
fer to immovable objects (macro-level landmarks)
more often than in closed-world environments. In
GIVE-2, immovable objects are limited to walls,
ceilings or floors, whereas in REAL there is a wide
range of immovable objects /landmarks that a user
can refer to, e.g. another building, rivers, parks,
shops, etc. Landmark descriptions will play an im-
portant role in future navigation systems (Kandan-
gath and Tu, 2015).
Predicting RE Success Rate (5RRE): Next,
we analyse which spatial frames significantly con-
tribute to task success, using multiple step-wise
linear regression.We find that taxonomic and ab-
solute properties significantly (p &lt; 0.05) con-
tribute to the success of a referring expression (Ta-
ble 4). Semantic features explain more of the vari-
ance observed in 5RRE, than syntactic features.
</bodyText>
<footnote confidence="0.9001015">
2Note that for GIVE-2 we consider both, the German and
the English data.
</footnote>
<subsectionHeader confidence="0.914573">
4.3 Joint Model of REG Success
</subsectionHeader>
<bodyText confidence="0.999968727272727">
Both syntactic and semantic features contribute to
the success of a RE. Therefore, we construct a
joint model for predicting 5RRE using step-wise
linear regression over the joint feature space. We
find that both syntactic and semantic features sig-
nificantly (p &lt; 0.05) contribute to the success of a
RE, see Table 4. This model explains almost half
of the variation observed in 5RRE (R2 = .407).
Clarke et al. (2013) reports an influence of visual
salience on REG, therefore, in future, we will in-
vestigate the influence of visual features.
</bodyText>
<sectionHeader confidence="0.992909" genericHeader="conclusions">
5 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.99998927027027">
From the results presented, the following conclu-
sions can be drawn for real-world NLG systems.
Firstly, semantic features have a bigger impact on
the success rate of REs than syntactic features,
i.e. content selection is more important than sur-
face realisation for REG. Secondly, semantic fea-
tures such as taxonomic and absolute properties
can significantly contribute to RE success. Tax-
onomic properties refer to the type of target ob-
ject, and in general depend on the local knowledge
of the information giver. Similarly, the success of
the RE will depend on the expertise of the infor-
mation follower. As such, modelling the user’s
level of knowledge (Janarthenam et al., 2011) and
stylistic differences (Di Fabbrizio et al., 2008) is
crucial. Absolute properties refer to object at-
tributes, such as colour. Attribute selection for
REG has attracted a considerable amount of at-
tention, therefore it would be interesting to inves-
tigate how these automatic attribute selection al-
gorithms perform in real-world, interactive envi-
ronments. Finally, the more complex scenes seem
to justify longer and more complex descriptions.
As such, there is an underlying trade-off which
needs to be optimised, e.g. following the gener-
ation framework described in (Rieser et al., 2014).
In future, we will compare existing REG algo-
rithms on our dataset, in a similar experiment to
Mitchell et al. (2013b). Then, we will extend ex-
isting algorithms to take into account other prop-
erties such as material (e.g. “wooden”), compo-
nents of the referred object (e.g. “balconies”) etc.
Finally, we will incorparate such an algorithm in
interactive settings to investigate the influence of
user dialogue behaviour and the influence of visual
features, such as salience (Clarke et al., 2013), in
order to improve the fit of our predictive model.
</bodyText>
<page confidence="0.991694">
1940
</page>
<sectionHeader confidence="0.999201" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999919142857143">
This research received funding from the EPSRC
GUI project Generation for Uncertain Informa-
tion (EP/L026775/1). The data has been col-
lected through the European Community’s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no. 216594 (SPACEBOOK
project).
</bodyText>
<sectionHeader confidence="0.998955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999039608247423">
Donna Byron, Alexander Koller, Kristina Striegnitz,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2009. Report on the First NLG
Challenge on Generating Instructions in Virtual En-
vironment (GIVE). In 12th European Workshop in
Natural Language Generation (ENLG).
Alasdair D.F. Clarke, Micha Elsner, and Hannah Ro-
hde. 2013. Where’s wally: The influence of visual
salience on referring expression generation. Fron-
tiers in Psychology, 4(329).
Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas
Bangalore. 2008. Referring expression generation
using speaker-based attribute selection and trainable
realization. In 5th International Natural Language
Generation Conference (INLG).
Yansong Feng and Mirella Lapata. 2013. Automatic
caption generation for news images. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
35(4):797–812.
Nicholas FitzGerald, Yoan Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The GIVE-
2 Corpus of Giving Instructions in Virtual Environ-
ments. In 7th International Conference on Lan-
guage Resources and Evaluation (LREC).
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA-REG Challenge 2009: Overview and Eval-
uation Results. In 12th European Workshop in Nat-
ural Language Generation (ENLG).
Albert Gatt, Emiel Krahmer, and Kees van Deemter.
2014. Models and empirical data for the production
of referring expressions. Language, Cognition and
Neuroscience, 29(8):899 – 911.
Srinivasan Janarthanam, Xingkun Liu, and Oliver
Lemon. 2012. A web-based evaluation framework
for spatial instruction-giving systems. In Proc. of
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).
Srinivasan Janarthenam, Helen Hastie, Oliver Lemon,
and Xingkun Liu. 2011. ”the day after the day after
tomorrow?” a machine learning approach to adap-
tive temporal expression generation: training and
evaluation with real users. In 12th Annual Meeting
of the Special Interest Group on Discourse and Dia-
logue.
Rong Jin and Luo Si. 2004. A study of methods for
normalizing user ratings in collaborative filtering. In
Proceedings of the 27th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’04, pages 568–569,
New York, NY, USA. ACM.
Anil Kandangath and Xiaoyuan Tu. 2015. Human-
ized navigation instructions for mapping applica-
tions. US Patent, 04.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
and Tamara Berg. 2014. ReferItGame: Referring to
Objects in Photographs of Natural Scenes. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).
Emiel Krahmer and Kees van Deemter. 2011. Com-
putational generation of referring expressions: A
survey. Computational Linguistics, 38(1):173–218,
2015/02/20.
Christopher Manning, Mihai Surdeanu, John Finkel,
Jenny Bethard, and David McClosky. 2014. The
Stanford CoreNLP Natural Language Processing
Toolkit. In 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL): System
Demonstrations.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2010. Natural reference to objects in a visual do-
main. In 6th International Natural Language Gen-
eration Conference (INLG).
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum´e, III.
2012. Midge: Generating image descriptions from
computer vision detections. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, EACL ’12,
pages 747–756.
Margaret Mitchell, Ehud Reiter, and Kees van Deemter.
2013a. Typicality and object reference. In Cognitive
Science (CogSci).
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013b. Generating expressions that refer to visible
objects. In North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Pe-
ter Bergstorm, and John Riedl. 1994. GroupLens:
an open architecture for collaborative filtering of
netnews. In ACM Conference on Computer Sup-
ported Cooperative Work (CSCW).
</reference>
<page confidence="0.868892">
1941
</page>
<reference confidence="0.9990776">
Verena Rieser, Oliver Lemon, and Simon Keizer. 2014.
Natural language generation as incremental plan-
ning under uncertainty: Adaptive information pre-
sentation for statistical dialogue systems. Au-
dio, Speech, and Language Processing, IEEE/ACM
Transactions on, 22(5):979–994, May.
Kees van Deemter, Ielka van der Sluis, and Albert Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In 4th
International Natutral Language Generation Con-
ference.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In 5th
International Natural Language Generation Confer-
ence (INLG).
Yezhou Yang, Ching Lik Teo, Hal Daum´e, III, and
Yannis Aloimonos. 2011. Corpus-guided sentence
generation of natural images. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Mark Yatskar, Michel Galley, Lucy Vanderwende, and
Luke Zettlemoyer. 2014. See no evil, say no evil:
Description generation from densely labeled images.
In Proceedings of the Third Joint Conference on Lex-
ical and Computational Semantics (*SEM 2014).
</reference>
<page confidence="0.995824">
1942
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.822259">
<title confidence="0.9919585">From the Virtual to the Real World: Referring to Objects in Real-World Spatial Scenes</title>
<author confidence="0.994138">Phil</author>
<affiliation confidence="0.9995165">School of Natural University of</affiliation>
<address confidence="0.996027">Stirling FK9 4LA, UK</address>
<email confidence="0.996188">phil.bartie@stir.ac.uk</email>
<author confidence="0.951996">Dimitra Gkatzia</author>
<author confidence="0.951996">Verena</author>
<affiliation confidence="0.9497085">Department of Computer Heriot-Watt</affiliation>
<address confidence="0.989532">Edinburgh EH14 4AS , UK</address>
<abstract confidence="0.999608208333333">Predicting the success of referring expressions (RE) is vital for real-world applications such as navigation systems. Traditionally, research has focused on studying Referring Expression Generation (REG) in virtual, controlled environments. In this paper, we describe a novel study of spatial references from real scenes rather than virtual. First, we investigate how humans objects in uncontrolled scenarios and compare our findings to those reported in virtual environments. We show that REs in real-world scenarios differ significantly to those in virtual worlds. Second, we propose a novel approach to quantifying image complexity when complete annotations are not present (e.g. due to poor object recognition capabitlities), and third, we present a model for success prediction of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Donna Byron</author>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
<author>Justine Cassell</author>
<author>Robert Dale</author>
<author>Johanna Moore</author>
<author>Jon Oberlander</author>
</authors>
<date>2009</date>
<booktitle>Report on the First NLG Challenge on Generating Instructions in Virtual Environment (GIVE). In 12th European Workshop in Natural Language Generation (ENLG).</booktitle>
<contexts>
<context position="1712" citStr="Byron et al., 2009" startWordPosition="257" endWordPosition="260">rd, we present a model for success prediction of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions. 1 Introduction REG has attracted considerable interest in the NLG community over the past 20 years (Krahmer and van Deemter, 2011; Gatt et al., 2014). While initially, the standard evaluation metric for REG was human-likeness, as compared to human corpora similarity as in TUNA (Gatt et al., 2009), the field has moved on to evaluating REG effectiveness by measuring task success in virtual interactive environments (Byron et al., 2009; Gargett et al., 2010; Janarthanam et al., 2012). Virtual environments however eliminate real-world uncertainty, such object recognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be William Mackaness School of GeoSciences University of Edinburgh Edinburgh EH8 9XP, UK william.mackaness@ed.ac.uk transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work</context>
</contexts>
<marker>Byron, Koller, Striegnitz, Cassell, Dale, Moore, Oberlander, 2009</marker>
<rawString>Donna Byron, Alexander Koller, Kristina Striegnitz, Justine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2009. Report on the First NLG Challenge on Generating Instructions in Virtual Environment (GIVE). In 12th European Workshop in Natural Language Generation (ENLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alasdair D F Clarke</author>
<author>Micha Elsner</author>
<author>Hannah Rohde</author>
</authors>
<title>Where’s wally: The influence of visual salience on referring expression generation. Frontiers in</title>
<date>2013</date>
<journal>Psychology,</journal>
<volume>4</volume>
<issue>329</issue>
<contexts>
<context position="18842" citStr="Clarke et al. (2013)" startWordPosition="3077" endWordPosition="3080">Table 4). Semantic features explain more of the variance observed in 5RRE, than syntactic features. 2Note that for GIVE-2 we consider both, the German and the English data. 4.3 Joint Model of REG Success Both syntactic and semantic features contribute to the success of a RE. Therefore, we construct a joint model for predicting 5RRE using step-wise linear regression over the joint feature space. We find that both syntactic and semantic features significantly (p &lt; 0.05) contribute to the success of a RE, see Table 4. This model explains almost half of the variation observed in 5RRE (R2 = .407). Clarke et al. (2013) reports an influence of visual salience on REG, therefore, in future, we will investigate the influence of visual features. 5 Discussion and Conclusions From the results presented, the following conclusions can be drawn for real-world NLG systems. Firstly, semantic features have a bigger impact on the success rate of REs than syntactic features, i.e. content selection is more important than surface realisation for REG. Secondly, semantic features such as taxonomic and absolute properties can significantly contribute to RE success. Taxonomic properties refer to the type of target object, and i</context>
</contexts>
<marker>Clarke, Elsner, Rohde, 2013</marker>
<rawString>Alasdair D.F. Clarke, Micha Elsner, and Hannah Rohde. 2013. Where’s wally: The influence of visual salience on referring expression generation. Frontiers in Psychology, 4(329).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Di Fabbrizio</author>
<author>Amanda Stent</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Referring expression generation using speaker-based attribute selection and trainable realization.</title>
<date>2008</date>
<booktitle>In 5th International Natural Language Generation Conference (INLG).</booktitle>
<marker>Di Fabbrizio, Stent, Bangalore, 2008</marker>
<rawString>Giuseppe Di Fabbrizio, Amanda Stent, and Srinivas Bangalore. 2008. Referring expression generation using speaker-based attribute selection and trainable realization. In 5th International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic caption generation for news images.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="2428" citStr="Feng and Lapata, 2013" startWordPosition="366" endWordPosition="369">world uncertainty, such object recognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be William Mackaness School of GeoSciences University of Edinburgh Edinburgh EH8 9XP, UK william.mackaness@ed.ac.uk transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the suc</context>
</contexts>
<marker>Feng, Lapata, 2013</marker>
<rawString>Yansong Feng and Mirella Lapata. 2013. Automatic caption generation for news images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(4):797–812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas FitzGerald</author>
<author>Yoan Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Learning distributions over logical forms for referring expression generation.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2700" citStr="FitzGerald et al., 2013" startWordPosition="412" endWordPosition="415">uk transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicting the success of a RE. Initially, this paper presents and analyses a novel, real-world corpus REAL (to be released) – “Ref</context>
</contexts>
<marker>FitzGerald, Artzi, Zettlemoyer, 2013</marker>
<rawString>Nicholas FitzGerald, Yoan Artzi, and Luke Zettlemoyer. 2013. Learning distributions over logical forms for referring expression generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gargett</author>
<author>Konstantina Garoufi</author>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<date>2010</date>
<booktitle>The GIVE2 Corpus of Giving Instructions in Virtual Environments. In 7th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="1734" citStr="Gargett et al., 2010" startWordPosition="261" endWordPosition="265">el for success prediction of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions. 1 Introduction REG has attracted considerable interest in the NLG community over the past 20 years (Krahmer and van Deemter, 2011; Gatt et al., 2014). While initially, the standard evaluation metric for REG was human-likeness, as compared to human corpora similarity as in TUNA (Gatt et al., 2009), the field has moved on to evaluating REG effectiveness by measuring task success in virtual interactive environments (Byron et al., 2009; Gargett et al., 2010; Janarthanam et al., 2012). Virtual environments however eliminate real-world uncertainty, such object recognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be William Mackaness School of GeoSciences University of Edinburgh Edinburgh EH8 9XP, UK william.mackaness@ed.ac.uk transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approach</context>
<context position="3434" citStr="Gargett et al., 2010" startWordPosition="534" endWordPosition="537">objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicting the success of a RE. Initially, this paper presents and analyses a novel, real-world corpus REAL (to be released) – “Referring Expression Anchored Language” (Section 2), and compares the findings to those reported in virtual worlds (Gargett et al., 2010). We then provide a detailed analysis of how syntactic and semantic features contribute to the success of REs (Sections 4.1, 4.2, 4.3), accounting for unobservable latent variables, such as the complexity of the visual scene (as described in Section 3). Finally, we summarise our work and discuss the implications of our work for NLG systems (Section 5). The dataset and models will be released. 2 The REAL Corpus The REAL corpus contains a collection of images of real-world urban scenes (Fig. 1) together with verbal descriptions of target objects (see Fig. 2) 1936 Proceedings of the 2015 Conferen</context>
<context position="6144" citStr="Gargett et al., 2010" startWordPosition="971" endWordPosition="974">tions was adjusted to ensure that all descriptions were identified by at least 3 other participants, generating 2618 image tag verifications. All cases were manually checked to determine if the ‘correct’ (green) or ‘incorrect’ (red) target had been identified Fig. 3. Overall, 76.2% of human descriptions provided were successfully identified. For the experiments reported in following sections, we summarised answers categorised as ‘incorrect’, ‘ambitious’ and ‘not found’ as unsuccessful. 2.2 Comparison to GIVE-2 Corpus We now compare this data with human data generated for the GIVE-2 challenge (Gargett et al., 2010). In GIVE-2, the target objects have distinct attributes, such as colour and position. For instance, an effective RE in GIVE-2 could be “the third button from the second row”. In real-world situations though, object properties are less well defined, making a finite set of pre-defined qualities unfeasible. Consider, for instance, the building highlighted in Figure 2, for which the following descriptions were collected: 1. The Austrian looking white house with the dark wooden beams at the water side. 2. The white building with the x-shape balconies. It seems it’s new. 3. The white building with </context>
<context position="16099" citStr="Gargett et al., 2010" startWordPosition="2634" endWordPosition="2637">y as estimated from the (normalised) success rate of an image. We find that humans use significantly more PPs and number of words (p &lt; 0.05) when describing complex images. In sum, syntactical features, which further describe and distinguish the target object (such as NPs, ADJ, and ADVPs and PPs) indicate successful REs. However, they cannot fully answer the question of “what makes a RE successful”, therefore we enrich our feature set using manually annotated semantic features. 4.2 Semantic Analysis of REG We extract semantic features by annotating spatial frames of reference as described in (Gargett et al., 2010). We annotate a sample of the corpus (100 instances), which allows us to perform a direct comparison between the two corpora. Comparison to GIVE-2 Corpus: We observe that in the REAL corpus, the taxonomic property, the relative property and the macro-level landmark 1939 Spatial Frame REAL GIVE-2 Ger- Enman glish Taxonomic Property 92* 53.66 58.51 Absolute Property 57* 85.37 92.53 Relative Property 15* 6.83 4.56 Viewer-centred 15 15.61 12.45 Micro-level landmark 9* 13.17 17.84 intrinsic Distractor Intrinsic 5* 10.73 14.11 Macro-level landmark 43* 6.83 4.15 intrinsic Deduction by elimina- 1 0.98</context>
</contexts>
<marker>Gargett, Garoufi, Koller, Striegnitz, 2010</marker>
<rawString>Andrew Gargett, Konstantina Garoufi, Alexander Koller, and Kristina Striegnitz. 2010. The GIVE2 Corpus of Giving Instructions in Virtual Environments. In 7th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>The TUNA-REG Challenge 2009: Overview and Evaluation Results. In</title>
<date>2009</date>
<booktitle>12th European Workshop in Natural Language Generation (ENLG).</booktitle>
<contexts>
<context position="1574" citStr="Gatt et al., 2009" startWordPosition="234" endWordPosition="237">ach to quantifying image complexity when complete annotations are not present (e.g. due to poor object recognition capabitlities), and third, we present a model for success prediction of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions. 1 Introduction REG has attracted considerable interest in the NLG community over the past 20 years (Krahmer and van Deemter, 2011; Gatt et al., 2014). While initially, the standard evaluation metric for REG was human-likeness, as compared to human corpora similarity as in TUNA (Gatt et al., 2009), the field has moved on to evaluating REG effectiveness by measuring task success in virtual interactive environments (Byron et al., 2009; Gargett et al., 2010; Janarthanam et al., 2012). Virtual environments however eliminate real-world uncertainty, such object recognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be William Mackaness School of GeoSciences University of Edinburgh Edinburgh EH8 9XP, UK william.mackaness@ed.ac.uk transferred to real-world scenes. We consider the case where we are uncertain about the scene i</context>
</contexts>
<marker>Gatt, Belz, Kow, 2009</marker>
<rawString>Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA-REG Challenge 2009: Overview and Evaluation Results. In 12th European Workshop in Natural Language Generation (ENLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Models and empirical data for the production of referring expressions.</title>
<date>2014</date>
<journal>Language, Cognition and Neuroscience,</journal>
<volume>29</volume>
<issue>8</issue>
<pages>911</pages>
<marker>Gatt, Krahmer, van Deemter, 2014</marker>
<rawString>Albert Gatt, Emiel Krahmer, and Kees van Deemter. 2014. Models and empirical data for the production of referring expressions. Language, Cognition and Neuroscience, 29(8):899 – 911.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Xingkun Liu</author>
<author>Oliver Lemon</author>
</authors>
<title>A web-based evaluation framework for spatial instruction-giving systems.</title>
<date>2012</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1761" citStr="Janarthanam et al., 2012" startWordPosition="266" endWordPosition="269">ion of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions. 1 Introduction REG has attracted considerable interest in the NLG community over the past 20 years (Krahmer and van Deemter, 2011; Gatt et al., 2014). While initially, the standard evaluation metric for REG was human-likeness, as compared to human corpora similarity as in TUNA (Gatt et al., 2009), the field has moved on to evaluating REG effectiveness by measuring task success in virtual interactive environments (Byron et al., 2009; Gargett et al., 2010; Janarthanam et al., 2012). Virtual environments however eliminate real-world uncertainty, such object recognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be William Mackaness School of GeoSciences University of Edinburgh Edinburgh EH8 9XP, UK william.mackaness@ed.ac.uk transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptio</context>
</contexts>
<marker>Janarthanam, Liu, Lemon, 2012</marker>
<rawString>Srinivasan Janarthanam, Xingkun Liu, and Oliver Lemon. 2012. A web-based evaluation framework for spatial instruction-giving systems. In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthenam</author>
<author>Helen Hastie</author>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
</authors>
<title>the day after the day after tomorrow?” a machine learning approach to adaptive temporal expression generation: training and evaluation with real users.</title>
<date>2011</date>
<booktitle>In 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</booktitle>
<contexts>
<context position="19674" citStr="Janarthenam et al., 2011" startWordPosition="3211" endWordPosition="3214"> can be drawn for real-world NLG systems. Firstly, semantic features have a bigger impact on the success rate of REs than syntactic features, i.e. content selection is more important than surface realisation for REG. Secondly, semantic features such as taxonomic and absolute properties can significantly contribute to RE success. Taxonomic properties refer to the type of target object, and in general depend on the local knowledge of the information giver. Similarly, the success of the RE will depend on the expertise of the information follower. As such, modelling the user’s level of knowledge (Janarthenam et al., 2011) and stylistic differences (Di Fabbrizio et al., 2008) is crucial. Absolute properties refer to object attributes, such as colour. Attribute selection for REG has attracted a considerable amount of attention, therefore it would be interesting to investigate how these automatic attribute selection algorithms perform in real-world, interactive environments. Finally, the more complex scenes seem to justify longer and more complex descriptions. As such, there is an underlying trade-off which needs to be optimised, e.g. following the generation framework described in (Rieser et al., 2014). In futur</context>
</contexts>
<marker>Janarthenam, Hastie, Lemon, Liu, 2011</marker>
<rawString>Srinivasan Janarthenam, Helen Hastie, Oliver Lemon, and Xingkun Liu. 2011. ”the day after the day after tomorrow?” a machine learning approach to adaptive temporal expression generation: training and evaluation with real users. In 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Jin</author>
<author>Luo Si</author>
</authors>
<title>A study of methods for normalizing user ratings in collaborative filtering.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’04,</booktitle>
<pages>568--569</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9949" citStr="Jin and Si, 2004" startWordPosition="1605" endWordPosition="1608">ilable data points per image. This allows us to estimate the average success rate of each referring expression SRRE (the proportion of successful validations) and the average success rate of each image SRi (the proportion of the correctly identified objects in the image). We use SRi to marginalise over the (hidden) image complexity, where we assume that some pictures are inherently more complex than others and thus achieve lower success rates. Similar normalisation methods are used for user ratings to account for the fact that some users are more “tolerant” and in general give higher ratings (Jin and Si, 2004). We employ Gaussian normalisation (Resnick et al., 1994) to normalise image success rates by considering the following factors: 1. Shift of average success rate per image: some images are inherently easier than others and gain higher success rates, independently of the REs used. This factor can be accounted by subtracting average success rates of all images from the average rating of a specific image x. 2. Different ratings: there are 27 REs per image on average, some of which are harder to understand than others, thus they gain lower success rates. To account for this, the success rates of e</context>
</contexts>
<marker>Jin, Si, 2004</marker>
<rawString>Rong Jin and Luo Si. 2004. A study of methods for normalizing user ratings in collaborative filtering. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’04, pages 568–569, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Kandangath</author>
<author>Xiaoyuan Tu</author>
</authors>
<title>Humanized navigation instructions for mapping applications.</title>
<date>2015</date>
<tech>US Patent, 04.</tech>
<contexts>
<context position="17937" citStr="Kandangath and Tu, 2015" startWordPosition="2927" endWordPosition="2931"> absolute descriptions (“the blue button”) or referring to an intrinsic distractor (“the red button next to the green”) are more frequent. In addition, real-world environments are dynamic. Humans choose to refer to immovable objects (macro-level landmarks) more often than in closed-world environments. In GIVE-2, immovable objects are limited to walls, ceilings or floors, whereas in REAL there is a wide range of immovable objects /landmarks that a user can refer to, e.g. another building, rivers, parks, shops, etc. Landmark descriptions will play an important role in future navigation systems (Kandangath and Tu, 2015). Predicting RE Success Rate (5RRE): Next, we analyse which spatial frames significantly contribute to task success, using multiple step-wise linear regression.We find that taxonomic and absolute properties significantly (p &lt; 0.05) contribute to the success of a referring expression (Table 4). Semantic features explain more of the variance observed in 5RRE, than syntactic features. 2Note that for GIVE-2 we consider both, the German and the English data. 4.3 Joint Model of REG Success Both syntactic and semantic features contribute to the success of a RE. Therefore, we construct a joint model f</context>
</contexts>
<marker>Kandangath, Tu, 2015</marker>
<rawString>Anil Kandangath and Xiaoyuan Tu. 2015. Humanized navigation instructions for mapping applications. US Patent, 04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sahar Kazemzadeh</author>
<author>Vicente Ordonez</author>
<author>Mark Matten</author>
<author>Tamara Berg</author>
</authors>
<title>ReferItGame: Referring to Objects in Photographs of Natural Scenes.</title>
<date>2014</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2770" citStr="Kazemzadeh et al. (2014)" startWordPosition="424" endWordPosition="427"> uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicting the success of a RE. Initially, this paper presents and analyses a novel, real-world corpus REAL (to be released) – “Referring Expression Anchored Language” (Section 2), and compares the fin</context>
</contexts>
<marker>Kazemzadeh, Ordonez, Matten, Berg, 2014</marker>
<rawString>Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. 2014. ReferItGame: Referring to Objects in Photographs of Natural Scenes. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<pages>2015--02</pages>
<marker>Krahmer, van Deemter, 2011</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2011. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1):173–218, 2015/02/20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Mihai Surdeanu</author>
<author>John Finkel</author>
<author>Jenny Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations.</booktitle>
<contexts>
<context position="11187" citStr="Manning et al., 2014" startWordPosition="1813" endWordPosition="1816">ided by the overall SR variance. The normalised image success rate (NSRi) per image x is defined by the following equation: SRi(x) − SRi NSRi(x) = (1) �En (SRi(x) − SRi)2 Using the (NSRi), we now investigate the REs in terms of their linguistic properties, including automatically annotated syntactic features and manually annotated semantic features. 4 Modelling REG Success Unlike previous work, we use both successful and unsuccessful REs in order to build a model that is able to predict the success or the failure of a RE. 4.1 Syntactic Analysis of REG Success We use the Stanford CoreNLP tool (Manning et al., 2014) to syntactically annotate the REs and we investigate which linguistic features contribute to the RE success in relation to the image complexity. Note that these analyses are based on normalised values, as discussed in Section 3). Predicting RE Success Rate (SRRE): Initially, we compare successful and unsuccessful REs by taking the upper and lower quartiles and extracting their syntactic features., i.e. the top and bottom 25% of REs with respect to their average success rate, and group them into two groups. We then extract syntactic features of these two groups and compare their frequencies (o</context>
</contexts>
<marker>Manning, Surdeanu, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher Manning, Mihai Surdeanu, John Finkel, Jenny Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In 52nd Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Natural reference to objects in a visual domain.</title>
<date>2010</date>
<booktitle>In 6th International Natural Language Generation Conference (INLG).</booktitle>
<marker>Mitchell, van Deemter, Reiter, 2010</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2010. Natural reference to objects in a visual domain. In 6th International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Xufeng Han</author>
<author>Jesse Dodge</author>
<author>Alyssa Mensch</author>
<author>Amit Goyal</author>
<author>Alex Berg</author>
<author>Kota Yamaguchi</author>
<author>Tamara Berg</author>
<author>Karl Stratos</author>
<author>Hal Daum´e</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<pages>747--756</pages>
<marker>Mitchell, Han, Dodge, Mensch, Goyal, Berg, Yamaguchi, Berg, Stratos, Daum´e, 2012</marker>
<rawString>Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg, Karl Stratos, and Hal Daum´e, III. 2012. Midge: Generating image descriptions from computer vision detections. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pages 747–756.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Ehud Reiter</author>
<author>Kees van Deemter</author>
</authors>
<title>Typicality and object reference.</title>
<date>2013</date>
<booktitle>In Cognitive Science (CogSci).</booktitle>
<marker>Mitchell, Reiter, van Deemter, 2013</marker>
<rawString>Margaret Mitchell, Ehud Reiter, and Kees van Deemter. 2013a. Typicality and object reference. In Cognitive Science (CogSci).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Generating expressions that refer to visible objects.</title>
<date>2013</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<marker>Mitchell, van Deemter, Reiter, 2013</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2013b. Generating expressions that refer to visible objects. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Resnick</author>
<author>Neophytos Iacovou</author>
<author>Mitesh Suchak</author>
<author>Peter Bergstorm</author>
<author>John Riedl</author>
</authors>
<title>GroupLens: an open architecture for collaborative filtering of netnews.</title>
<date>1994</date>
<booktitle>In ACM Conference on Computer Supported Cooperative Work (CSCW).</booktitle>
<contexts>
<context position="10006" citStr="Resnick et al., 1994" startWordPosition="1613" endWordPosition="1616">ate the average success rate of each referring expression SRRE (the proportion of successful validations) and the average success rate of each image SRi (the proportion of the correctly identified objects in the image). We use SRi to marginalise over the (hidden) image complexity, where we assume that some pictures are inherently more complex than others and thus achieve lower success rates. Similar normalisation methods are used for user ratings to account for the fact that some users are more “tolerant” and in general give higher ratings (Jin and Si, 2004). We employ Gaussian normalisation (Resnick et al., 1994) to normalise image success rates by considering the following factors: 1. Shift of average success rate per image: some images are inherently easier than others and gain higher success rates, independently of the REs used. This factor can be accounted by subtracting average success rates of all images from the average rating of a specific image x. 2. Different ratings: there are 27 REs per image on average, some of which are harder to understand than others, thus they gain lower success rates. To account for this, the success rates of each image are divided by the overall SR variance. The nor</context>
</contexts>
<marker>Resnick, Iacovou, Suchak, Bergstorm, Riedl, 1994</marker>
<rawString>Paul Resnick, Neophytos Iacovou, Mitesh Suchak, Peter Bergstorm, and John Riedl. 1994. GroupLens: an open architecture for collaborative filtering of netnews. In ACM Conference on Computer Supported Cooperative Work (CSCW).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
<author>Simon Keizer</author>
</authors>
<title>Natural language generation as incremental planning under uncertainty: Adaptive information presentation for statistical dialogue systems.</title>
<date>2014</date>
<journal>Audio, Speech, and Language Processing, IEEE/ACM Transactions on,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="20264" citStr="Rieser et al., 2014" startWordPosition="3301" endWordPosition="3304">dge (Janarthenam et al., 2011) and stylistic differences (Di Fabbrizio et al., 2008) is crucial. Absolute properties refer to object attributes, such as colour. Attribute selection for REG has attracted a considerable amount of attention, therefore it would be interesting to investigate how these automatic attribute selection algorithms perform in real-world, interactive environments. Finally, the more complex scenes seem to justify longer and more complex descriptions. As such, there is an underlying trade-off which needs to be optimised, e.g. following the generation framework described in (Rieser et al., 2014). In future, we will compare existing REG algorithms on our dataset, in a similar experiment to Mitchell et al. (2013b). Then, we will extend existing algorithms to take into account other properties such as material (e.g. “wooden”), components of the referred object (e.g. “balconies”) etc. Finally, we will incorparate such an algorithm in interactive settings to investigate the influence of user dialogue behaviour and the influence of visual features, such as salience (Clarke et al., 2013), in order to improve the fit of our predictive model. 1940 Acknowledgments This research received fundin</context>
</contexts>
<marker>Rieser, Lemon, Keizer, 2014</marker>
<rawString>Verena Rieser, Oliver Lemon, and Simon Keizer. 2014. Natural language generation as incremental planning under uncertainty: Adaptive information presentation for statistical dialogue systems. Audio, Speech, and Language Processing, IEEE/ACM Transactions on, 22(5):979–994, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Ielka van der Sluis</author>
<author>Albert Gatt</author>
</authors>
<title>Building a semantically transparent corpus for the generation of referring expressions.</title>
<date>2006</date>
<booktitle>In 4th International Natutral Language Generation Conference.</booktitle>
<marker>van Deemter, van der Sluis, Gatt, 2006</marker>
<rawString>Kees van Deemter, Ielka van der Sluis, and Albert Gatt. 2006. Building a semantically transparent corpus for the generation of referring expressions. In 4th International Natutral Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>The use of spatial relations in referring expression generation.</title>
<date>2008</date>
<booktitle>In 5th International Natural Language Generation Conference (INLG).</booktitle>
<contexts>
<context position="2577" citStr="Viethen and Dale, 2008" startWordPosition="392" endWordPosition="395">ents can be William Mackaness School of GeoSciences University of Edinburgh Edinburgh EH8 9XP, UK william.mackaness@ed.ac.uk transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicti</context>
</contexts>
<marker>Viethen, Dale, 2008</marker>
<rawString>Jette Viethen and Robert Dale. 2008. The use of spatial relations in referring expression generation. In 5th International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e, III, and Yannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e, III, and Yannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Michel Galley</author>
<author>Lucy Vanderwende</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>See no evil, say no evil: Description generation from densely labeled images.</title>
<date>2014</date>
<booktitle>In Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="2470" citStr="Yatskar et al., 2014" startWordPosition="374" endWordPosition="377"> errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be William Mackaness School of GeoSciences University of Edinburgh Edinburgh EH8 9XP, UK william.mackaness@ed.ac.uk transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the c</context>
<context position="9127" citStr="Yatskar et al., 2014" startWordPosition="1466" endWordPosition="1469">k success: The REAL data shows a positive relationship between length and success rate, i.e. for a one word increase in length, the odds of correct object identification is significantly increased (p &lt; 0.05, Logit), i.e. longer and more complex sentences lead to more successful REs. 3 Quantifying the Image Complexity We assume that the complexity of the urban scene represented in the image is hidden due to the lack of semantic annotations. Our dataset does not include any quantifiable image descriptions, such as computer vision output as in (Mitchell et al., 2012) or manual annotations as in (Yatskar et al., 2014). In addition, the same RE might not always result in successful identification of an object due to scene complexity. In order to marginalise the effect of the scene complexity, we exploit the multiple available data points per image. This allows us to estimate the average success rate of each referring expression SRRE (the proportion of successful validations) and the average success rate of each image SRi (the proportion of the correctly identified objects in the image). We use SRi to marginalise over the (hidden) image complexity, where we assume that some pictures are inherently more compl</context>
</contexts>
<marker>Yatskar, Galley, Vanderwende, Zettlemoyer, 2014</marker>
<rawString>Mark Yatskar, Michel Galley, Lucy Vanderwende, and Luke Zettlemoyer. 2014. See no evil, say no evil: Description generation from densely labeled images. In Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
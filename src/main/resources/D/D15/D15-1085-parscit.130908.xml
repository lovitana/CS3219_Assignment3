<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018985">
<title confidence="0.979665">
Open-Domain Name Error Detection using a Multi-Task RNN
</title>
<author confidence="0.998586">
Hao Cheng Hao Fang Mari Ostendorf
</author>
<affiliation confidence="0.998323">
Department of Electrical Engineering
University of Washington
</affiliation>
<email confidence="0.997593">
{chenghao,hfang,ostendor}@uw.edu
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999652733333333">
Out-of-vocabulary name errors in speech
recognition create significant problems for
downstream language processing, but the
fact that they are rare poses challenges
for automatic detection, particularly in an
open-domain scenario. To address this
problem, a multi-task recurrent neural net-
work language model for sentence-level
name detection is proposed for use in com-
bination with out-of-vocabulary word de-
tection. The sentence-level model is also
effective for leveraging external text data.
Experiments show a 26% improvement in
name-error detection F-score over a sys-
tem using n-gram lexical features.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999654903225807">
Most spoken language processing or dialogue sys-
tems are based on a finite vocabulary, so occa-
sionally a word used will be out of the vocabu-
lary (OOV), in which case the automatic speech
recognition (ASR) system chooses the best match-
ing in-vocabulary sequence of words to cover that
region (where acoustic match dominates the deci-
sion). The most difficult OOV words to cover are
names, since they are less likely to be covered by
morpheme-like subword fragments and they often
result in anomalous recognition output, e.g.
REF: what can we get at Litanfeeth
HYP: what can we get it leaks on feet
While these errors are rare, they create major prob-
lems for language processing, since names tend to
be important for many applications. Thus, it is of
interest to automatically detect such error regions
for additional analysis or human correction.
Named entity recognition (NER) systems have
been applied to speech output (Palmer and Osten-
dorf, 2005; Sudoh et al., 2006), taking advantage
of local contextual cues to names (e.g. titles for
person names), but as illustrated above, neighbor-
ing words are often affected, which obscures lexi-
cal cues to name regions. Parada et al. (2011) re-
duce this problem somewhat by applying an NER
tagger to a word confusion network (WCN) based
on a hybrid word/fragment ASR system.
In addition to the problem of noisy context, au-
tomatic name error detection is challenging be-
cause name errors are rare for a good recognizer.
To learn the cues to name errors, it is neces-
sary to train from the output of the target rec-
ognizer, so machine learning is faced with infre-
quent positive examples for which training data is
very sparse. In addition, in an open domain sys-
tem, automatically-learned lexical context features
from one domain may be useless in another.
In this paper, we address these general problems
– detecting rare events in an open-domain task –
specifically for name error detection. Prior work
addressed the problem of skewed priors by artifi-
cially increasing the error rate by holding names
out of the vocabulary (Chen et al., 2013) or by
factoring the problem into sentence-level name de-
tection and OOV word detection (He et al., 2014)
(since OOV errors in general are more frequent
than name errors). Sentence-level features are also
shown to be more robust than local context in di-
rect name error prediction (Marin, 2015). While
these techniques provide some benefit, the use of
discrete lexical context cues is sensitive to the lim-
ited amount of training data available.
Our work leverages the factored approach, but
improve performance by using a continuous-space
sentence representation for predicting presence of
a name. Specifically, we modify a recurrent neu-
ral network (RNN) language model (LM) to pre-
dict both the word sequence and a sentence-level
name indicator. Combining the LM objective with
name prediction provides a regularization effect
in training that leads to improved sentence-level
</bodyText>
<page confidence="0.953811">
737
</page>
<note confidence="0.9850585">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 737–746,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9997215">
name prediction. The continuous-space model is
also effective for leveraging external text resources
to improve generalization in the open-domain sce-
nario.
The overall framework for speech recognition
and baseline name error detection system is out-
lined in Section 2, and the multi-task (MT) RNN
approach for sentence-level name prediction is in-
troduced in Section 3. Experimental results for
both sentence-level name detection and name error
detection are presented in Section 4, demonstrat-
ing the effectiveness of the approach on test data
that varies in its match to the training data. As
discussed in Section 5, the sentence-level model
is motivated by similar models for other applica-
tions. The paper summarizes key findings and dis-
cusses potential areas for further improvement in
Section 6.
</bodyText>
<sectionHeader confidence="0.939339" genericHeader="method">
2 System Overview and Tasks
</sectionHeader>
<bodyText confidence="0.999984458333334">
The name error detection task explored in this
work is a component in a bidirectional speech-
to-speech translation system for English to/from
Iraqi Arabic with human-computer dialogue inter-
action for error resolution (Ayan et al., 2013), de-
veloped during the DARPA BOLT project. The
training data consists of a range of topics associ-
ated with activities of military personnel, includ-
ing traffic control, military training, civil affairs,
medical checkups, and so on. However, the sys-
tem is expected to handle open-domain tasks, and
thus the evaluation data covers a broader range of
topics, including humanitarian aid and disaster re-
lief, as well as more general topics such as sports,
family and weather. The dialogues often contain
mentions of names and places, many of which are
OOV words to the ASR system. As illustrated in
the previous section, ASR hypotheses necessarily
have errors in OOV regions, but because specific
names are infrequent, even in-vocabulary names
can have these types of error patterns. Therefore,
developing a robust name error detector for ASR
hypotheses is an important component of the sys-
tem to resolve errors and ambiguity.
Detecting OOV errors requires combining ev-
idence of recognizer uncertainty and anomalous
word sequences in a local region. For name er-
rors, lexical cues to names are also useful, e.g. a
person’s title, location prepositions, or keywords
such as “name”. The baseline system for this work
uses structural features extracted from a confusion
network of ASR hypotheses plus ASR word con-
fidence to represent recognizer uncertainty, and
word n-gram context to the left and right of the
target confusion network slot. These features are
combined in a maximum entropy (ME) classifier
trained to predict name errors directly. This is
the same as the baseline used in (He et al., 2014;
Marin, 2015; Marin et al., 2015), but with a differ-
ent ASR system.
Training a classifier to predict whether a sen-
tence has a name is easier than direct name error
prediction, because the positive class is less rare,
and it does not require recognizer output so more
data can be used (e.g. speech transcripts with-
out recognizer output, or written text). In addi-
tion, since the words abutting the name are less
reliable in a recognition hypothesis, the informa-
tion lost by working at the sentence level is mini-
mal. The idea of using sentence-level name pre-
diction is proposed in (He et al., 2014), but in
that work the sentence name posterior is a fea-
ture in the ME model (optionally with word cues
learned by the sentence-level predictor). In our
work, the problem is factored to use the acous-
tic confusibility and local word class features for
OOV error prediction, which is combined with the
sentence-level name posterior for name error pre-
diction. In other words, only two features (poste-
riors) are used in training with the sparse name er-
ror prediction data. An ME classifier is then used
to combine the two features to predict the word-
level name error. The word-level OOV detector is
another ME binary classifier; the full set of fea-
tures used for the word-level OOV detector can be
found in (Marin, 2015).
The main innovation in this work is that we
propose to use a multi-task RNN model for the
sentence-level name prediction, where the train-
ing objective takes into account both the language
modeling task and the sentence-level name predic-
tion task, as described in the next section.
</bodyText>
<sectionHeader confidence="0.993573" genericHeader="method">
3 Multi-task Recurrent Neural Network
</sectionHeader>
<bodyText confidence="0.999671375">
The RNN is a powerful sequential model and
has proven to be useful in many natural lan-
guage processing tasks, including language mod-
eling (Mikolov et al., 2010) and word similarity
(Mikolov et al., 2013c). It also achieves good re-
sults for a variety of text classification problems
when combined with a convolutional neural net-
work (CNN) (Lai et al., 2015). In this paper, we
</bodyText>
<page confidence="0.997554">
738
</page>
<figureCaption confidence="0.998479">
Figure 1: The structure of the proposed MT RNN
</figureCaption>
<bodyText confidence="0.9945445">
model, which predicts both the next word o(t) and
whether the sentence contains a name y(t) at each
time step.
propose an MT RNN for the sentence-level name
prediction task, which augments the word predic-
tion in the RNN language model with an addi-
tional output layer for sentence-level name pre-
diction. Formally, the MT RNN is defined over
</bodyText>
<equation confidence="0.9990318">
t = 1, ... , n for a sentence of length n as:
Mz(t),
f (Wx(t) + Rh(t − 1)) ,
s (Uh(t) + b1) ,
s (V h(t) + b2) .
</equation>
<bodyText confidence="0.99997502173913">
where z(t) E RV is the 1-of-V encoding of the
t-th word in the sentence; x(t) E Rd is the d-
dimensional continuous word embedding corre-
sponding to the t-th word; h(t) E Rk is a k-
dimensional embedding that summarizes the word
sequence through time t; and o(t) and y(t) are re-
spectively the output layers for the language mod-
eling task and the sentence-level prediction task.
The parameters of the model (learned in multi-task
training) include: M E Rd×V , which is usually
referred to as the word embedding matrix; projec-
tion matrices U E RV ×d and V E R2×d; and bias
terms b1, b2 E Rd. f and s are respectively the
sigmoid and softmax functions. Note that the word
sequence associated with a sentence includes start
and end symbols.
The structure of the proposed MT RNN is
shown in Fig. 1. At each time step, the hidden
vector is used to predict both the next word and
a sentence-level indicator of the presence of a
name, providing a probability distribution for both
variables. Thus, the hidden vector ht provides
a continuous representation of the word history
that emphasizes words that are important for pre-
dicting the presence of a name. The vector at
time n can be thought of as a sentence embed-
ding. The sentence-level output yt differs from the
word-dependent label predictor typically used in
named entity detection (or part-of-speech tagging)
in that it is providing a sequentially updated pre-
diction of a sentence-level variable, rather than a
word-level indicator that specifies the location of
a named entity in the sentence. The final predic-
tion yn is used as a feature in the name error de-
tection system. The sentence-level output yt does
not always converge to yn gradually nor is it al-
ways monotonic, since the prediction can change
abruptly (either positively or negatively) as new
words are processed. The sentence-level variable
provides a mechanism for capturing long distance
context, which is particularly useful for speech ap-
plications, where both the name of interest and the
words in its immediate context may be in error.
The training objective is the combination of the
log-likelihood of the word sequence and that of the
sentence-level name prediction:
</bodyText>
<equation confidence="0.983730333333333">
n
E [(1 − λ) log P(w(t)|h(t)) + λ log P(y(t)|h(t))] , (1)
t=1
</equation>
<bodyText confidence="0.999987791666667">
where h(t) = [w(1), ... , w(t − 1)] and A is the
weight on the log-likelihood of the sentence-level
name labels.
Another way to train the model is to predict
the sentence-level name label only at the end of
the sentence, rather than at every time step. Pre-
liminary experiment results show that this model
has inferior performance. We argue that train-
ing with only the sentence-final name label output
can result in unbalanced updates, i.e., information
from the language modeling task is used more of-
ten than that from the sentence-level name predic-
tion task, implying that balancing the use of infor-
mation sources is an important design choice for
multi-task models.
The training objective is optimized using
stochastic gradient descent (SGD). We also exper-
iment with AdaGrad (Duchi et al., 2011), which
has shown to be more stable and converge faster
for non-convex SGD optimization. Since language
model training requires a normalization opera-
tion each time over the whole vocabulary (~60K)
which is computationally intensive, we further
speed up training by using noise contrastive esti-
</bodyText>
<equation confidence="0.99968775">
x(t) =
h(t) =
o(t) =
y(t) =
</equation>
<page confidence="0.98314">
739
</page>
<bodyText confidence="0.999803631578948">
mation (NCE) (Mnih and Teh, 2012). For all mod-
els using NCE, we fix the number of negative sam-
ples to 50.
All the weights are randomly initialized in the
range of [−0.1, 0.1]. The hidden layer size k is
selected from {50, 100, 200} and the task mixing
weight A is select in {0.2, 0.4, 0.6, 0.8}, based on
development set performance. We set the initial
learning rate to 1 and 0.1 for SGD with and with-
out Adagrad respectively. In our experiments, we
observe that models trained with AdaGrad achieve
better performance, so we only report the mod-
els with AdaGrad in this paper. At each training
epoch, we validate the objective on the develop-
ment set. The learning rate is reduced after the
first time the development set loglikelihood de-
creases, and the whole training procedure termi-
nates when the development set loglikelihood de-
creases for the second time.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.987223">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999855034482759">
There are two types of datasets used in this pa-
per: the BOLT dataset and a collection of Reddit
discussions. The first dataset was collected dur-
ing the DARPA TRANSTAC and BOLT projects.
The ASR hypotheses of totally 7088 spoken sen-
tences makes up of the training dataset (BOLT-
Train) for both sentence-level name prediction and
word-level name error detection. There are two
development sets: Dev1 and Dev2. Dev1 is used
for parameter tuning for all models, and Dev2 is
used for training the ME-based word-level name
error detector using the word-level OOV posterior
and sentence-level name posterior. For RNN mod-
els, we tune the hidden layer size and the multi-
task weight A; for the ME-based word-level name
error detector, we tune the regularization param-
eter. Two test sets are used to evaluate sentence-
level name prediction (Test1) and word-level name
error detection (Test2), based on the BOLT phase
2 and 3 evaluations, respectively.
As shown in Table 1, the BOLT topics are cate-
gorized into three domains: TRANSTAC, HADR
and General. The BOLT-Train, Dev1 and Dev2
sets contain only speech from the TRANSTAC do-
main, whereas the Test1 and Test2 sets contain
all three domains. Detailed data split statistics
and domain information are summarized in Ta-
ble 2. Note that there are very few positive sam-
ples of name errors (roughly 1%), whereas for the
</bodyText>
<tableCaption confidence="0.999884">
Table 1: Topics in different domains.
</tableCaption>
<table confidence="0.9973028">
Target Class BOLT- Dev1 Dev2 Test1 Test2
Train
name sentences 7.6 7.5 8.2 8.1 14.0
name errors 0.8 1.1 0.9 0.7 0.8
OOVs 1.7 1.8 1.8 1.8 1.7
</table>
<tableCaption confidence="0.98446">
Table 2: Data splits and statistics, including the
</tableCaption>
<bodyText confidence="0.9993925">
percentage of sentences containing names (name
sentences), the percentage of hypothesized words
that are name errors (name errors), and the per-
centage of words that are OOVs (OOVs).
sentence-level name prediction task and the word-
level OOV prediction task, the data skewness is
somewhat less severe (roughly 8% sentences with
names in most of our data sets).
In order to address the issue of domain mis-
match between the training data and the broad-
domain subsets of the test data, we collect text
from Reddit.com which is a very active discus-
sion forum where users can discuss all kinds
of topics. Reddit has thousands of user-created
and user-moderated subreddits, each emphasiz-
ing a different topic. For example, there is a
general ASKREDDIT subreddit for people to ask
any questions, as well as subreddits targeted for
specific interests, like ASKMEN, ASKWOMEN,
ASKSCIENCE, etc. Although the Reddit discus-
sions are different from BOLT data, they have
a conversational nature and names are often ob-
served within the discussions. Therefore, we hy-
pothesize that they can help improve sentence-
level name prediction in general. We collect data
from 14 subreddits that cover different kinds of
topics (such as politics, news, book suggestions)
and vary in community size. The Stanford Name
Entity Recognizer (Finkel et al., 2005) is used to
detect names in each sentence, and a sentence-
level name label is assigned if there are any names
present.1 The data are tokenized using Stanford
</bodyText>
<footnote confidence="0.9903375">
1The Stanford Name Entity Recognizer achieves 82.3%
F-score on the references of Dev1. Thus, it is expected to
</footnote>
<table confidence="0.993190277777777">
Domain
Topics
Traffic Control, Facilities Inspection,
Civil Affairs, Medical, Combined
Training, Combined Operations
TRANSTAC
HADR
Humanitarian Aid, Disaster Relief
Family, Gardening, Sports, Pets,
Books, Weather, Language, Phone
General
740
Model TRANSTAC HADR General All
BOW + ME 52.9 32.5 11.1 40.8
RNN + ME 11.2 21.7 30.5 15.0
SG + ME 14.7 12.6 16.3 14.6
ST RNN 40.3 37.5 37.8 39.3
MT RNN 59.8 52.0 23.8 51.1
</table>
<tableCaption confidence="0.889856428571429">
Table 3: F-scores on Test1 for sentence-level name
prediction for models trained on BOLT data.
CoreNLP tools (Manning et al., 2014) and are low-
ered cased after running the Stanford Name Entity
Recognizer. In total, we obtain 135K sentences
containing at least one name and 360K sentences
without names.
</tableCaption>
<subsectionHeader confidence="0.994673">
4.2 Sentence-level Name Prediction
</subsectionHeader>
<bodyText confidence="0.999950888888889">
To evaluate the effectiveness of the proposed MT
RNN model, we first apply it to the sentence-
level name prediction task on ASR hypotheses.
For this task, each sample corresponds to a hy-
pothesized sentence generated by the ASR system
and a ground-truth label indicating whether there
are names in that sentence. We compare the MT
RNN with four contrasting models for predicing
whether a sentence includes a name.
</bodyText>
<listItem confidence="0.999852333333333">
• BOW + ME. An ME classifier using a bag-of-
words (BOW) sentence representation.
• SG + ME. An ME classifier is used with the sen-
</listItem>
<bodyText confidence="0.970411333333333">
tence embedding as features, where the embed-
ding uses the skip-gram (SG) model (Mikolov
et al., 2013a) to get word-level embeddings
(with window size 10) and sentence embeddings
are composed by averaging embeddings of all
words in the sentence.
</bodyText>
<listItem confidence="0.728813125">
• RNN + ME. A simple RNN LM is trained (i.e.,
λ in (1) is set to 0), and the hidden layer for
the last word in a sentence provides a sentence-
level embedding that is used in an ME classifier
trained to predict the sentence-level name label.
• ST RNN. A single-task (ST) RNN model is
trained to directly predict the sentence-level
name for each word (i.e., λ in (1) is set to 1).
</listItem>
<bodyText confidence="0.965548285714286">
All models are trained on either BOLT-Train or
BOLT-Train + Reddit, and tuned on Dev1 includ-
ing the dimension of embeddings, E2 regulariza-
tion parameters for the ME classifiers, and so on.
The domain-specific F-scores on Test1 are sum-
marized in Table 3 for training only with the BOLT
give useful labels for the Reddit data.
</bodyText>
<figureCaption confidence="0.969327">
Figure 2: F-scores on Test1 for sentence-level
name prediction for models trained with and with-
out Reddit data. The number in the parenthesis
indicates the portion of the domain in the data.
</figureCaption>
<bodyText confidence="0.999821393939394">
data. The proposed MT RNN achieves the best re-
sults on the TRANSTAC and HADR domains, and
it has significant overall performance improve-
ment over all baseline models. Not surprisingly,
the two approaches that used unsupervised learn-
ing to obtain a sentence-level embedding (SG +
ME, RNN + ME) have the worst performance on
the TRANSTAC and HADR domains, with both
having very low precision (6-18%), with best pre-
cision on the general domain. The BOW + ME and
the ST RNN have similar performance in terms of
overall F-score, but the BOW + ME model is much
better on the TRANSTAC domain, whereas the ST
RNN achieves the best results on the General do-
main. The main failing of the BOW + ME model is
in recall on the General domain, though it also has
relatively low recall on the HADR domain. Note
that the General domain only accounts for 10% of
the Test1 set, so the ST RNN gets lower F-score
overall compared with the MT RNN, which per-
forms best on the other two domains (90% of the
Test1 set). On all domains, the MT RNN greatly
improves precision compared to the ST RNN (at
the expense of recall), and it improves recall com-
pared to the BOW + ME approach (at the expense
of precision).
In order to study the effectiveness of using ex-
ternal data, we also train all models on an enlarged
training set including extra name-tagged sentences
from Reddit. The improvement for each domain
due to also using the Reddit data is shown in Fig. 2
for the three best configurations. (There is no ben-
efit in the unsupervised learning cases.) Com-
</bodyText>
<page confidence="0.98932">
741
</page>
<bodyText confidence="0.999772421052632">
pared with training only on the BOLT data, all
three of these models get substantial overall per-
formance improvement by utilizing the external
domain training data. Since the external training
data covers mostly topics in the General domain,
the performance gain in that domain is most sig-
nificant. For the MT RNN and BOW + ME clas-
sifiers, the additional training data primarily ben-
efits recall, particulary for the General domain.
In contrast, the ST RNN sees an improvement in
precision for all domains. One reason that the
added training text also benefits the models on the
TRANSTAC domain is that over 90% of the words
in the speech recognizer vocabulary are not seen
in the small set of name-labeled speech training
data, which means that the embeddings for these
words are random in the RNNs trained only this
data and the BOW + ME classifier will never use
these words.
</bodyText>
<subsectionHeader confidence="0.997328">
4.3 Word-level Name Error Detection
</subsectionHeader>
<bodyText confidence="0.981611666666667">
We next assess the usefulness of the resulting MT
RNN model for word-level name error detection
on ASR hypotheses. Here, several word-level
name error detection approaches are compared, in-
cluding direct name error prediction and factored
name/OOV prediction approaches.
</bodyText>
<listItem confidence="0.989006173913043">
• OOV thresholding. This system simply uses
the OOV prediction, but with the posterior
threshold tuned for word-level name error based
on the Dev1 set.
• Word Context. This is the baseline ME sys-
tem described in Section 2 and also used as a
baseline in (He et al., 2014; Marin et al., 2015),
which directly predicts the word-level name er-
ror using WCN structural features, current word
Brown class, and up-to trigram left and right
word context information.
• Word Class. This system, from (Marin et
al., 2015), also directly predicts the word-level
name error, but replaces the word n-gram fea-
tures with a smaller number of word class fea-
tures to address the sparse training problem. The
word classes are based on seed words learned
from sentence-level name prediction which are
expanded to classes using a nearest neighbor
distance with RNN embeddings trained on the
BOLT data.
• Word Context + OOV. This system uses an E2
regularized ME classifier to predict the word-
</listItem>
<bodyText confidence="0.8086055">
level name error using two posteriors as fea-
tures: a word-dependent posterior from the
Word Context system and one from the OOV de-
tection system.
</bodyText>
<listItem confidence="0.7628885">
• MT RNN + OOV. This system also uses an E2
regularized ME classifier to predict the word-
</listItem>
<bodyText confidence="0.9967325">
level name error using two posteriors as fea-
tures: the same word-dependent OOV posterior
as above, and the posterior from the sentence-
level name prediction using the MT RNN model
described in Section 4.2, which is constant for
all positions in the sentence.
All of the above models are trained on BOLT-Train
and tuned on Dev1. The ME classifier used in the
Word Context + OOV and the MT RNN + OOV
systems are trained on Dev2, with regularization
weights and decision boundaries tuned on Dev1.
Name error detection results (F-scores) are
summarized in Table 4. The three systems that
use discrete, categorical lexical context features
(word or word class context) in direct name er-
ror prediction have worse results than the OOV
thresholding approach overall, as well as on the
TRANSTAC subset. Presumably this is due to
over-fitting associated with the lexical context fea-
tures. The factored MT RNN + OOV system,
which uses a continuous-space representation of
lexical context, achieves a gain in overall perfor-
mance compared to the other systems and a sub-
stantial gain in performance on the TRANSTAC
domain on which it is trained. Using the Red-
dit data further improves the overall F-score, with
a slight loss on the TRANSTAC subset but sub-
stantial gains in the other domains. Although the
performance improvement in the general domain
is relatively small compared with sentence-level
name prediction, utilizing external data makes the
resulting representation more transferable across
different domains and tasks. The best MT RNN
+ OOV system obtains 26% relative improvement
over the baseline Word Context system (or 17%
improvement over the simple OOV thresholding
approach).
Looking at performance trade-offs in precision
and recall, we find that the use of the RNN systems
mainly improves precision, though there is also a
small gain in recall overall. The added training
data benefits recall for all domains, with a small
loss in precision for the TRANSTAC and HADR
sets. The use of the OOV posterior improves pre-
cision but limits recall, particularly for the general
domain where recall of the OOV posterior alone
</bodyText>
<page confidence="0.993434">
742
</page>
<table confidence="0.999361">
Model TRANSTAC HADR General All
OOV thresholding 41.6 31.2 16.5 30.9
Word Context 37.6 29.0 16.3 28.6
Word Class 33.6 35.9 11.9 27.9
Word Context + OOV 40.2 26.0 14.4 28.4
MT RNN + OOV 47.9 32.8 13.5 34.1
MT RNN† + OOV 46.4 37.5 18.0 36.2
</table>
<tableCaption confidence="0.70147025">
Table 4: F-scores on Test2 for word-level name
error prediction. MT RNN is trained on BOLT-
Train, whereas MT RNN† is trained on BOLT-
Train + Reddit.
</tableCaption>
<bodyText confidence="0.991793073170732">
is only 18% vs. 25% for the word context model
with the OOV posterior information.
To better understand some of the challenges
of this task, consider the following examples
(R=reference, H=hypothesis):
R1: i’m doing good my name is captain
rodriguez
H1: i’m doing good my name is captain
road radios
R2: well it’s got flying lizards knights and
zombies and shit
H2: well it’s gotta flying lives there it’s nights and
some bees and shia
R3: i live in a city called omaha
H3: i live in a city called omar
ASR tokens associated with name errors are un-
derlined and italicized; tokens associated with
non-name OOV errors are simply underlined.
Name errors have a similar character to OOV erors
in that they often have anomalous word sequences
in the region of the OOV word (examples 1 and
2), which is why the OOV posterior is so useful.
However, too much reliance on the OOV posterior
leads to wrongly detecting general OOV errors as
name errors (‘lizards’ and ‘zombies’ in example
2) and missed detection of name errors where the
confusion network cues indicate a plausible hy-
pothesis (‘omaha’ in example 3). Examples 1 and
3 illustrate the importance of lexical cues to names
(‘name ... captain’, ‘city called’), but word-based
cues are unreliable for the systems trained only on
the small amount of domain-specific data. Lever-
aging the reddit data allowed the MT RNN system
to detect the error in example 1 (HADR domain)
that was missed by the word context system. Ex-
ample 3 was only detected by the word context
system when no OOV posterior is used. Though
this example was from the General domain, city
names represent an important error class in the
TRANSTAC data, so the term ‘city’ is learned as
a useful cue.
</bodyText>
<subsectionHeader confidence="0.999663">
4.4 Sentence Embedding
</subsectionHeader>
<bodyText confidence="0.999972173913043">
As discussed in Section 3, we postulate that by
modeling words in a sentence in sequential order
and simutaneously predicting sentence categorical
information, the resulting hidden vector of the last
word should be a good representation of the whole
sentence, i.e., a sentence embedding. To provide
support for this hypothesis and show the impact
of external data, we present the sentence embed-
dings learned by the different RNN variants on
Test2 using the t-Distributed Stochastic Neighbor
Embedding (t-SNE) visualization (van der Maaten
and Hinton, 2008) in Fig. 3. Four models are com-
pared, including three RNNs trained on the BOLT
data, and the MT RNN model trained on BOLT +
Reddit data. Note the visualization method t-SNE
does not take the label information into account
during the learning.
As we can see in Fig. 3a, the positive and neg-
ative sentence embeddings learned by RNN LM
are randomly scattered in the space, indicating
that embeddings learned via unsupervised train-
ing (e.g., solely on word context) may fail to cap-
ture the sentence-level indicators associated with
a particular task, in this case presence of a name.
When the results are ploted in terms of domains,
the embeddings are similarly broadly scattered –
there is no obvious topic representation in the em-
beddings.
When comparing Fig. 3b to Fig. 3a, which is
associated with the RNN using a sentence-final
name indicator, we can see that a lot of posi-
tive vectors have moved to the bottom-left of the
space, though there is still a relatively large over-
lap between positive and negative embeddings. In
Fig. 3c, corresponding to the word-level MT RNN,
there forms a separable subgroup of positive em-
beddings and the overlapping seems to be reduced
as well in contrast to Figs. 3a and 3b. Finally, most
of the positive sentence embeddings produced by
the MT RNN model trained with external Red-
dit data gather at the botton-left of Fig. 3d. In
general, the overlap between positive and nega-
tive sentences decreases from single task models
to multi-task model. The external data make the
proposed model produce more well-shaped group-
ings of sentence embeddings.
</bodyText>
<page confidence="0.995604">
743
</page>
<figure confidence="0.999719740740741">
20
15
10
5
0
5
15
10
20
negative
positive
20 15 10 5 0 5 10 15 20
(c) MT RNN.
(d) MT RNN using extra Reddit data.
(a) RNN LM.
20
20
25
15
10
10
15
0
5
5
20 15 10 5 0 5 10 15 20
negative
positive
20
20
15
10
10
15
5
0
5
20 15 10 5 0 5 10 15 20 25
negative
positive
(b) ST RNN.
negative
positive
20 15 10 5 0 5 10 15 20
25
20
15
10
5
0
5
15
20
10
</figure>
<figureCaption confidence="0.999573">
Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models.
</figureCaption>
<sectionHeader confidence="0.999949" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999952428571429">
Recently, due to the success of continuous repre-
sentation methods, much work has been devoted to
studying methods for learning word embeddings
that capture semantic and syntactic meaning. Al-
though these word embeddings are shown to be
successful in some word-level tasks, such as word
analogy (Mikolov et al., 2013b; Jeffery Penning-
ton, 2014) and semantic role labeling (Collobert
and Weston, 2008), it is still an open question how
best to compose the word embeddings effectively
and make use of them for text understanding.
Recent work on learning continuous sentence
representations usually compose the word em-
beddings using either a convolutional neural net-
work (CNN), a tree-structured recursive NN, or a
variant of an RNN. A typical CNN-based struc-
ture composes word embeddings in a hierachical
fashion (alternating between convolutional layers
and pooling layers) to form the continuous sen-
tence representation for sentence-level classifica-
tion tasks (Kim, 2014; Kalchbrenner et al., 2014;
Lai et al., 2015). These models usually build up
the sentence representation directly from the lexi-
cal surface representation and rely on the pooling
layer to capture the dependencies between words.
Another popular method for continuous sentence
representation is based on the recursive neural net-
work (Socher et al., 2012; Socher et al., 2013;
Tai et al., 2015). These models use a tree struc-
ture to compose a continuous sentence represen-
tation and have the advantages of capturing more
fine-grained sentential structure due to the use of
parsing trees. Note that the RNN-based sequen-
tial modeling used in this paper can be viewed as
a linearized tree-structure model.
In this paper, we train the neural network model
with a multi-task objective reflecting both the
probability of the sequence and the probability that
the sequence contains names. The general idea of
multitask learning dates back to (Caruana, 1997),
and is shown to be effective recently for neu-
ral network models in different natural language
</bodyText>
<page confidence="0.99331">
744
</page>
<bodyText confidence="0.999971642857143">
processing tasks. Collobert and Weston (2008)
propose a unified deep convolutional neural net-
work for different tasks by using a set of task-
independent word embeddings together with a set
of task-specific word embeddings. For each task,
it uses a unique neural network with its own lay-
ers and connections. Liu et al. (2015) propose a
different neural network structure for search query
classification and document retrieval where lower-
level layers and connections are all shared but the
high-level layers are task-specific. For tasks con-
sidered in (Collobert and Weston, 2008) and (Liu
et al., 2015), training samples are task-dependent.
Thus, both models are trained following the SGD
manner by alternating tasks for each training sam-
ples with task-dependent training objectives. In
this paper, we combine the language modeling
task with the sentence-level name prediction task,
and each training sample has labels for both tasks.
Therefore, the SGD training can be done with the
weighted sum of the task-specific objectives for
each training sample, and the language model ob-
jective can be thought of as a regularization term.
Similar settings of multitask learning for neural
network models are employed in phoneme recog-
nition for speech (Seltzer and Droppo, 2013) and
speech synthesis (Wu et al., 2015) as well, but both
of them use equal weights for all tasks.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998734883721">
In this paper, we address an open domain rare
event detection problem, specifically, name er-
ror detection on ASR hypotheses. To alleviate
the data skewness and domain mismatch prob-
lems, we adopt a factored approach (sentence-
level name prediction and OOV error prediction)
and propose an MT RNN for sentence-level name
prediction. The factored model is shown to be
more robust to the sparse training problem. For
the problem of sentence-level name prediction,
the proposed method of combining the language
modeling and sentence-level name prediction ob-
jectives in an MT RNN achieves the best results
among studied models for the domain represented
by the training data as well as in the open-domain
scenario. Visualization of sentence-level embed-
dings show how both the multi-task and the word-
level name label update are important for achiev-
ing good results. The use of unrelated external
training text (which can only be used in sentence-
level name prediction) improves all models, par-
ticularly for the highly-mismatched general do-
main data.
The improvement in performance associated
with using the external text is much smaller on the
word-level name error detection task than on the
sentence-level name prediction. This seems to be
due to the high weight learned for the word-level
posterior. For future work, it is worthwhile look-
ing into whether continuous word and sentence
representations can be combined in the name er-
ror detector to achieve further improvement.
In this work, we proposed a model for learn-
ing sentence representations that might be use-
ful for other sentence classification tasks, such
as review and opinion polarity detection, ques-
tion type classification and so on. As discussed
in Section 5, there are other models that have
been found useful for obtaining continuous sen-
tence embeddings. It would be of interest to in-
vestigate whether other structures are more or less
sensitive to data skew and/or useful for incorporat-
ing multi-domain training data.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999983625">
The authors would like to thank Alex Marin, Ji He,
Wen Wang and all reviewers for useful discussion
and comment. This material is based on work sup-
ported by DARPA under Contract No. HR0011-
12-C-0016 (subcontract 27-001389). Any opin-
ions, findings and conclusions or recommenda-
tions expressed herein are those of the authors and
do not necessarily reflect the views of DARPA.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995984333333333">
N.F. Ayan, A Mandal, M. Frandsen, Jing Zheng,
P. Blasco, A Kathol, F. Bechet, B. Favre, A Marin,
T. Kwiatkowski, M. Ostendorf, L. Zettlemoyer,
P. Salletmayr, J. Hirschberg, and S. Stoyanchev.
2013. “Can you give me another word for hyper-
baric?” Improving speech translation using targeted
clarification questions. In Proc. ICASSP, pages
8391–8395.
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28, July.
Wei Chen, Sankaranarayanan Ananthakrishnan, Rohit
Prasad, and Prem Natarajan. 2013. Variable-span
out-of-vocabulary named entity detection. In Proc.
Interspeech, pages 3761–3765.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Proc.
ICML.
</reference>
<page confidence="0.980692">
745
</page>
<reference confidence="0.999700247191012">
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Machine Learning Re-
search, 12.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proc. ACL.
Ji He, Alex Marin, and Mari Ostendorf. 2014. Effec-
tive data-driven feature learning for detecting name
errors in automatic speech recognition. In Proc.
SLT.
Christopher Manning Jeffery Pennington,
Richard Socher. 2014. Glove: Global vectors
for word representations. In Proc. EMNLP.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proc. ACL.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proc. EMNLP.
Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
classification. In Proc. AAAI.
Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng,
Kevin Duh, and Ye-Yi Wang. 2015. Representation
learning using multi-task deep neural networks for
semantic classification and information retrieval. In
Proc. NAACL.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proc. ACL.
Alex Marin, Mari Ostendorf, and Ji He. 2015. Learn-
ing phrase patterns for asr error detection using se-
mantic similarity. In Proc. Interspeech.
Alex Marin. 2015. Effective use of cross-domain pars-
ing in automatic speech recognition and error detec-
tion. Ph.D. thesis, University of Washington.
Tom´a˘s Mikolov, Martin Karafi´at, Luk´a˘s Burget, Jan
˘Cernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Proc.
Interspeech.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. 2013a. Distributed representations of
words and phrases and their compositionality. In
Proc. NIPS.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. NIPS.
Tom´a˘s Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proc. NAACL-HLT.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proc. ICML.
David Palmer and Mari Ostendorf. 2005. Improv-
ing out-of-vocabulary name resolution. Computer
Speech and language, 19(1):107–128.
Carolina Parada, Mark Dredze, and Frederick Jelinek.
2011. OOV sensitive named-entity recognition in
speech. In Proc. Interspeech, pages 2085–2088.
Michael L. Seltzer and Jasha Droppo. 2013. Multi-
task learning in deep neural networks for improved
phoneme recognition. In Proc. ICASSP.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic compo-
sitionality through recursive matrix-vector space. In
Proc. EMNLP-CoNLL.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proc. EMNLP.
Katsuhito Sudoh, Hajime Tsukada, and Hideki Isozaki.
2006. Incorporating speech recognition confi-
dence into discriminative named entity recognition
of speech. In Proc. ACL.
Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. In Proc. ACL.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Machine Learning
Research, 9, November.
Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts,
and Simon King. 2015. Deep neural networks em-
ploying multi-task learning and stacked bottleneck
features for speech synthesis. In Proc. ICASSP.
</reference>
<page confidence="0.998514">
746
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.970749">
<title confidence="0.99922">Open-Domain Name Error Detection using a Multi-Task RNN</title>
<author confidence="0.999974">Hao Cheng Hao Fang Mari Ostendorf</author>
<affiliation confidence="0.9997515">Department of Electrical University of Washington</affiliation>
<abstract confidence="0.9982004375">Out-of-vocabulary name errors in speech recognition create significant problems for downstream language processing, but the fact that they are rare poses challenges for automatic detection, particularly in an open-domain scenario. To address this problem, a multi-task recurrent neural network language model for sentence-level name detection is proposed for use in combination with out-of-vocabulary word detection. The sentence-level model is also effective for leveraging external text data. Experiments show a 26% improvement in name-error detection F-score over a system using n-gram lexical features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N F Ayan</author>
<author>A Mandal</author>
<author>M Frandsen</author>
<author>Jing Zheng</author>
<author>P Blasco</author>
<author>A Kathol</author>
<author>F Bechet</author>
<author>B Favre</author>
<author>A Marin</author>
<author>T Kwiatkowski</author>
<author>M Ostendorf</author>
<author>L Zettlemoyer</author>
<author>P Salletmayr</author>
<author>J Hirschberg</author>
<author>S Stoyanchev</author>
</authors>
<title>Can you give me another word for hyperbaric?” Improving speech translation using targeted clarification questions.</title>
<date>2013</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>8391--8395</pages>
<contexts>
<context position="5026" citStr="Ayan et al., 2013" startWordPosition="783" endWordPosition="786">detection are presented in Section 4, demonstrating the effectiveness of the approach on test data that varies in its match to the training data. As discussed in Section 5, the sentence-level model is motivated by similar models for other applications. The paper summarizes key findings and discusses potential areas for further improvement in Section 6. 2 System Overview and Tasks The name error detection task explored in this work is a component in a bidirectional speechto-speech translation system for English to/from Iraqi Arabic with human-computer dialogue interaction for error resolution (Ayan et al., 2013), developed during the DARPA BOLT project. The training data consists of a range of topics associated with activities of military personnel, including traffic control, military training, civil affairs, medical checkups, and so on. However, the system is expected to handle open-domain tasks, and thus the evaluation data covers a broader range of topics, including humanitarian aid and disaster relief, as well as more general topics such as sports, family and weather. The dialogues often contain mentions of names and places, many of which are OOV words to the ASR system. As illustrated in the pre</context>
</contexts>
<marker>Ayan, Mandal, Frandsen, Zheng, Blasco, Kathol, Bechet, Favre, Marin, Kwiatkowski, Ostendorf, Zettlemoyer, Salletmayr, Hirschberg, Stoyanchev, 2013</marker>
<rawString>N.F. Ayan, A Mandal, M. Frandsen, Jing Zheng, P. Blasco, A Kathol, F. Bechet, B. Favre, A Marin, T. Kwiatkowski, M. Ostendorf, L. Zettlemoyer, P. Salletmayr, J. Hirschberg, and S. Stoyanchev. 2013. “Can you give me another word for hyperbaric?” Improving speech translation using targeted clarification questions. In Proc. ICASSP, pages 8391–8395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
</authors>
<date>1997</date>
<booktitle>Multitask learning. Machine Learning, 28,</booktitle>
<contexts>
<context position="31663" citStr="Caruana, 1997" startWordPosition="5342" endWordPosition="5343">Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling used in this paper can be viewed as a linearized tree-structure model. In this paper, we train the neural network model with a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to (Caruana, 1997), and is shown to be effective recently for neural network models in different natural language 744 processing tasks. Collobert and Weston (2008) propose a unified deep convolutional neural network for different tasks by using a set of taskindependent word embeddings together with a set of task-specific word embeddings. For each task, it uses a unique neural network with its own layers and connections. Liu et al. (2015) propose a different neural network structure for search query classification and document retrieval where lowerlevel layers and connections are all shared but the high-level la</context>
</contexts>
<marker>Caruana, 1997</marker>
<rawString>Rich Caruana. 1997. Multitask learning. Machine Learning, 28, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chen</author>
<author>Sankaranarayanan Ananthakrishnan</author>
<author>Rohit Prasad</author>
<author>Prem Natarajan</author>
</authors>
<title>Variable-span out-of-vocabulary named entity detection.</title>
<date>2013</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>3761--3765</pages>
<contexts>
<context position="2893" citStr="Chen et al., 2013" startWordPosition="458" endWordPosition="461"> learn the cues to name errors, it is necessary to train from the output of the target recognizer, so machine learning is faced with infrequent positive examples for which training data is very sparse. In addition, in an open domain system, automatically-learned lexical context features from one domain may be useless in another. In this paper, we address these general problems – detecting rare events in an open-domain task – specifically for name error detection. Prior work addressed the problem of skewed priors by artificially increasing the error rate by holding names out of the vocabulary (Chen et al., 2013) or by factoring the problem into sentence-level name detection and OOV word detection (He et al., 2014) (since OOV errors in general are more frequent than name errors). Sentence-level features are also shown to be more robust than local context in direct name error prediction (Marin, 2015). While these techniques provide some benefit, the use of discrete lexical context cues is sensitive to the limited amount of training data available. Our work leverages the factored approach, but improve performance by using a continuous-space sentence representation for predicting presence of a name. Spec</context>
</contexts>
<marker>Chen, Ananthakrishnan, Prasad, Natarajan, 2013</marker>
<rawString>Wei Chen, Sankaranarayanan Ananthakrishnan, Rohit Prasad, and Prem Natarajan. 2013. Variable-span out-of-vocabulary named entity detection. In Proc. Interspeech, pages 3761–3765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="30147" citStr="Collobert and Weston, 2008" startWordPosition="5103" endWordPosition="5106">20 15 10 5 0 5 10 15 20 25 negative positive (b) ST RNN. negative positive 20 15 10 5 0 5 10 15 20 25 20 15 10 5 0 5 15 20 10 Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models. 5 Related Work Recently, due to the success of continuous representation methods, much work has been devoted to studying methods for learning word embeddings that capture semantic and syntactic meaning. Although these word embeddings are shown to be successful in some word-level tasks, such as word analogy (Mikolov et al., 2013b; Jeffery Pennington, 2014) and semantic role labeling (Collobert and Weston, 2008), it is still an open question how best to compose the word embeddings effectively and make use of them for text understanding. Recent work on learning continuous sentence representations usually compose the word embeddings using either a convolutional neural network (CNN), a tree-structured recursive NN, or a variant of an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representation for sentence-level classification tasks (Kim, 2014; Kalchbrenner et al., 2014; L</context>
<context position="31808" citStr="Collobert and Weston (2008)" startWordPosition="5363" endWordPosition="5366">esentation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling used in this paper can be viewed as a linearized tree-structure model. In this paper, we train the neural network model with a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to (Caruana, 1997), and is shown to be effective recently for neural network models in different natural language 744 processing tasks. Collobert and Weston (2008) propose a unified deep convolutional neural network for different tasks by using a set of taskindependent word embeddings together with a set of task-specific word embeddings. For each task, it uses a unique neural network with its own layers and connections. Liu et al. (2015) propose a different neural network structure for search query classification and document retrieval where lowerlevel layers and connections are all shared but the high-level layers are task-specific. For tasks considered in (Collobert and Weston, 2008) and (Liu et al., 2015), training samples are task-dependent. Thus, b</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Machine Learning Research,</journal>
<volume>12</volume>
<contexts>
<context position="12173" citStr="Duchi et al., 2011" startWordPosition="2011" endWordPosition="2014">ame label only at the end of the sentence, rather than at every time step. Preliminary experiment results show that this model has inferior performance. We argue that training with only the sentence-final name label output can result in unbalanced updates, i.e., information from the language modeling task is used more often than that from the sentence-level name prediction task, implying that balancing the use of information sources is an important design choice for multi-task models. The training objective is optimized using stochastic gradient descent (SGD). We also experiment with AdaGrad (Duchi et al., 2011), which has shown to be more stable and converge faster for non-convex SGD optimization. Since language model training requires a normalization operation each time over the whole vocabulary (~60K) which is computationally intensive, we further speed up training by using noise contrastive estix(t) = h(t) = o(t) = y(t) = 739 mation (NCE) (Mnih and Teh, 2012). For all models using NCE, we fix the number of negative samples to 50. All the weights are randomly initialized in the range of [−0.1, 0.1]. The hidden layer size k is selected from {50, 100, 200} and the task mixing weight A is select in {</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Machine Learning Research, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="16287" citStr="Finkel et al., 2005" startWordPosition="2702" endWordPosition="2705">example, there is a general ASKREDDIT subreddit for people to ask any questions, as well as subreddits targeted for specific interests, like ASKMEN, ASKWOMEN, ASKSCIENCE, etc. Although the Reddit discussions are different from BOLT data, they have a conversational nature and names are often observed within the discussions. Therefore, we hypothesize that they can help improve sentencelevel name prediction in general. We collect data from 14 subreddits that cover different kinds of topics (such as politics, news, book suggestions) and vary in community size. The Stanford Name Entity Recognizer (Finkel et al., 2005) is used to detect names in each sentence, and a sentencelevel name label is assigned if there are any names present.1 The data are tokenized using Stanford 1The Stanford Name Entity Recognizer achieves 82.3% F-score on the references of Dev1. Thus, it is expected to Domain Topics Traffic Control, Facilities Inspection, Civil Affairs, Medical, Combined Training, Combined Operations TRANSTAC HADR Humanitarian Aid, Disaster Relief Family, Gardening, Sports, Pets, Books, Weather, Language, Phone General 740 Model TRANSTAC HADR General All BOW + ME 52.9 32.5 11.1 40.8 RNN + ME 11.2 21.7 30.5 15.0 </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji He</author>
<author>Alex Marin</author>
<author>Mari Ostendorf</author>
</authors>
<title>Effective data-driven feature learning for detecting name errors in automatic speech recognition.</title>
<date>2014</date>
<booktitle>In Proc. SLT.</booktitle>
<contexts>
<context position="2997" citStr="He et al., 2014" startWordPosition="476" endWordPosition="479">ine learning is faced with infrequent positive examples for which training data is very sparse. In addition, in an open domain system, automatically-learned lexical context features from one domain may be useless in another. In this paper, we address these general problems – detecting rare events in an open-domain task – specifically for name error detection. Prior work addressed the problem of skewed priors by artificially increasing the error rate by holding names out of the vocabulary (Chen et al., 2013) or by factoring the problem into sentence-level name detection and OOV word detection (He et al., 2014) (since OOV errors in general are more frequent than name errors). Sentence-level features are also shown to be more robust than local context in direct name error prediction (Marin, 2015). While these techniques provide some benefit, the use of discrete lexical context cues is sensitive to the limited amount of training data available. Our work leverages the factored approach, but improve performance by using a continuous-space sentence representation for predicting presence of a name. Specifically, we modify a recurrent neural network (RNN) language model (LM) to predict both the word sequen</context>
<context position="6621" citStr="He et al., 2014" startWordPosition="1041" endWordPosition="1044">zer uncertainty and anomalous word sequences in a local region. For name errors, lexical cues to names are also useful, e.g. a person’s title, location prepositions, or keywords such as “name”. The baseline system for this work uses structural features extracted from a confusion network of ASR hypotheses plus ASR word confidence to represent recognizer uncertainty, and word n-gram context to the left and right of the target confusion network slot. These features are combined in a maximum entropy (ME) classifier trained to predict name errors directly. This is the same as the baseline used in (He et al., 2014; Marin, 2015; Marin et al., 2015), but with a different ASR system. Training a classifier to predict whether a sentence has a name is easier than direct name error prediction, because the positive class is less rare, and it does not require recognizer output so more data can be used (e.g. speech transcripts without recognizer output, or written text). In addition, since the words abutting the name are less reliable in a recognition hypothesis, the information lost by working at the sentence level is minimal. The idea of using sentence-level name prediction is proposed in (He et al., 2014), bu</context>
<context position="21976" citStr="He et al., 2014" startWordPosition="3690" endWordPosition="3693"> classifier will never use these words. 4.3 Word-level Name Error Detection We next assess the usefulness of the resulting MT RNN model for word-level name error detection on ASR hypotheses. Here, several word-level name error detection approaches are compared, including direct name error prediction and factored name/OOV prediction approaches. • OOV thresholding. This system simply uses the OOV prediction, but with the posterior threshold tuned for word-level name error based on the Dev1 set. • Word Context. This is the baseline ME system described in Section 2 and also used as a baseline in (He et al., 2014; Marin et al., 2015), which directly predicts the word-level name error using WCN structural features, current word Brown class, and up-to trigram left and right word context information. • Word Class. This system, from (Marin et al., 2015), also directly predicts the word-level name error, but replaces the word n-gram features with a smaller number of word class features to address the sparse training problem. The word classes are based on seed words learned from sentence-level name prediction which are expanded to classes using a nearest neighbor distance with RNN embeddings trained on the </context>
</contexts>
<marker>He, Marin, Ostendorf, 2014</marker>
<rawString>Ji He, Alex Marin, and Mari Ostendorf. 2014. Effective data-driven feature learning for detecting name errors in automatic speech recognition. In Proc. SLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning Jeffery Pennington</author>
<author>Richard Socher</author>
</authors>
<title>Glove: Global vectors for word representations. In</title>
<date>2014</date>
<booktitle>Proc. EMNLP.</booktitle>
<marker>Pennington, Socher, 2014</marker>
<rawString>Christopher Manning Jeffery Pennington, Richard Socher. 2014. Glove: Global vectors for word representations. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences. In</title>
<date>2014</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="30744" citStr="Kalchbrenner et al., 2014" startWordPosition="5193" endWordPosition="5196">(Collobert and Weston, 2008), it is still an open question how best to compose the word embeddings effectively and make use of them for text understanding. Recent work on learning continuous sentence representations usually compose the word embeddings using either a convolutional neural network (CNN), a tree-structured recursive NN, or a variant of an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representation for sentence-level classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="30717" citStr="Kim, 2014" startWordPosition="5191" endWordPosition="5192">e labeling (Collobert and Weston, 2008), it is still an open question how best to compose the word embeddings effectively and make use of them for text understanding. Recent work on learning continuous sentence representations usually compose the word embeddings using either a convolutional neural network (CNN), a tree-structured recursive NN, or a variant of an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representation for sentence-level classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RN</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siwei Lai</author>
<author>Liheng Xu</author>
<author>Kang Liu</author>
<author>Jun Zhao</author>
</authors>
<title>Recurrent convolutional neural networks for text classification.</title>
<date>2015</date>
<booktitle>In Proc. AAAI.</booktitle>
<contexts>
<context position="8609" citStr="Lai et al., 2015" startWordPosition="1385" endWordPosition="1388"> a multi-task RNN model for the sentence-level name prediction, where the training objective takes into account both the language modeling task and the sentence-level name prediction task, as described in the next section. 3 Multi-task Recurrent Neural Network The RNN is a powerful sequential model and has proven to be useful in many natural language processing tasks, including language modeling (Mikolov et al., 2010) and word similarity (Mikolov et al., 2013c). It also achieves good results for a variety of text classification problems when combined with a convolutional neural network (CNN) (Lai et al., 2015). In this paper, we 738 Figure 1: The structure of the proposed MT RNN model, which predicts both the next word o(t) and whether the sentence contains a name y(t) at each time step. propose an MT RNN for the sentence-level name prediction task, which augments the word prediction in the RNN language model with an additional output layer for sentence-level name prediction. Formally, the MT RNN is defined over t = 1, ... , n for a sentence of length n as: Mz(t), f (Wx(t) + Rh(t − 1)) , s (Uh(t) + b1) , s (V h(t) + b2) . where z(t) E RV is the 1-of-V encoding of the t-th word in the sentence; x(t)</context>
<context position="30763" citStr="Lai et al., 2015" startWordPosition="5197" endWordPosition="5200">), it is still an open question how best to compose the word embeddings effectively and make use of them for text understanding. Recent work on learning continuous sentence representations usually compose the word embeddings using either a convolutional neural network (CNN), a tree-structured recursive NN, or a variant of an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representation for sentence-level classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling used in this paper</context>
</contexts>
<marker>Lai, Xu, Liu, Zhao, 2015</marker>
<rawString>Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent convolutional neural networks for text classification. In Proc. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Liu</author>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Kevin Duh</author>
<author>Ye-Yi Wang</author>
</authors>
<title>Representation learning using multi-task deep neural networks for semantic classification and information retrieval.</title>
<date>2015</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="32086" citStr="Liu et al. (2015)" startWordPosition="5411" endWordPosition="5414"> a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to (Caruana, 1997), and is shown to be effective recently for neural network models in different natural language 744 processing tasks. Collobert and Weston (2008) propose a unified deep convolutional neural network for different tasks by using a set of taskindependent word embeddings together with a set of task-specific word embeddings. For each task, it uses a unique neural network with its own layers and connections. Liu et al. (2015) propose a different neural network structure for search query classification and document retrieval where lowerlevel layers and connections are all shared but the high-level layers are task-specific. For tasks considered in (Collobert and Weston, 2008) and (Liu et al., 2015), training samples are task-dependent. Thus, both models are trained following the SGD manner by alternating tasks for each training samples with task-dependent training objectives. In this paper, we combine the language modeling task with the sentence-level name prediction task, and each training sample has labels for bot</context>
</contexts>
<marker>Liu, Gao, He, Deng, Duh, Wang, 2015</marker>
<rawString>Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. 2015. Representation learning using multi-task deep neural networks for semantic classification and information retrieval. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="17100" citStr="Manning et al., 2014" startWordPosition="2836" endWordPosition="2839">r achieves 82.3% F-score on the references of Dev1. Thus, it is expected to Domain Topics Traffic Control, Facilities Inspection, Civil Affairs, Medical, Combined Training, Combined Operations TRANSTAC HADR Humanitarian Aid, Disaster Relief Family, Gardening, Sports, Pets, Books, Weather, Language, Phone General 740 Model TRANSTAC HADR General All BOW + ME 52.9 32.5 11.1 40.8 RNN + ME 11.2 21.7 30.5 15.0 SG + ME 14.7 12.6 16.3 14.6 ST RNN 40.3 37.5 37.8 39.3 MT RNN 59.8 52.0 23.8 51.1 Table 3: F-scores on Test1 for sentence-level name prediction for models trained on BOLT data. CoreNLP tools (Manning et al., 2014) and are lowered cased after running the Stanford Name Entity Recognizer. In total, we obtain 135K sentences containing at least one name and 360K sentences without names. 4.2 Sentence-level Name Prediction To evaluate the effectiveness of the proposed MT RNN model, we first apply it to the sentencelevel name prediction task on ASR hypotheses. For this task, each sample corresponds to a hypothesized sentence generated by the ASR system and a ground-truth label indicating whether there are names in that sentence. We compare the MT RNN with four contrasting models for predicing whether a sentenc</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Marin</author>
<author>Mari Ostendorf</author>
<author>Ji He</author>
</authors>
<title>Learning phrase patterns for asr error detection using semantic similarity.</title>
<date>2015</date>
<booktitle>In Proc. Interspeech.</booktitle>
<contexts>
<context position="6655" citStr="Marin et al., 2015" startWordPosition="1047" endWordPosition="1050">word sequences in a local region. For name errors, lexical cues to names are also useful, e.g. a person’s title, location prepositions, or keywords such as “name”. The baseline system for this work uses structural features extracted from a confusion network of ASR hypotheses plus ASR word confidence to represent recognizer uncertainty, and word n-gram context to the left and right of the target confusion network slot. These features are combined in a maximum entropy (ME) classifier trained to predict name errors directly. This is the same as the baseline used in (He et al., 2014; Marin, 2015; Marin et al., 2015), but with a different ASR system. Training a classifier to predict whether a sentence has a name is easier than direct name error prediction, because the positive class is less rare, and it does not require recognizer output so more data can be used (e.g. speech transcripts without recognizer output, or written text). In addition, since the words abutting the name are less reliable in a recognition hypothesis, the information lost by working at the sentence level is minimal. The idea of using sentence-level name prediction is proposed in (He et al., 2014), but in that work the sentence name p</context>
<context position="21997" citStr="Marin et al., 2015" startWordPosition="3694" endWordPosition="3697">never use these words. 4.3 Word-level Name Error Detection We next assess the usefulness of the resulting MT RNN model for word-level name error detection on ASR hypotheses. Here, several word-level name error detection approaches are compared, including direct name error prediction and factored name/OOV prediction approaches. • OOV thresholding. This system simply uses the OOV prediction, but with the posterior threshold tuned for word-level name error based on the Dev1 set. • Word Context. This is the baseline ME system described in Section 2 and also used as a baseline in (He et al., 2014; Marin et al., 2015), which directly predicts the word-level name error using WCN structural features, current word Brown class, and up-to trigram left and right word context information. • Word Class. This system, from (Marin et al., 2015), also directly predicts the word-level name error, but replaces the word n-gram features with a smaller number of word class features to address the sparse training problem. The word classes are based on seed words learned from sentence-level name prediction which are expanded to classes using a nearest neighbor distance with RNN embeddings trained on the BOLT data. • Word Con</context>
</contexts>
<marker>Marin, Ostendorf, He, 2015</marker>
<rawString>Alex Marin, Mari Ostendorf, and Ji He. 2015. Learning phrase patterns for asr error detection using semantic similarity. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Marin</author>
</authors>
<title>Effective use of cross-domain parsing in automatic speech recognition and error detection.</title>
<date>2015</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Washington.</institution>
<contexts>
<context position="3185" citStr="Marin, 2015" startWordPosition="509" endWordPosition="510">omain may be useless in another. In this paper, we address these general problems – detecting rare events in an open-domain task – specifically for name error detection. Prior work addressed the problem of skewed priors by artificially increasing the error rate by holding names out of the vocabulary (Chen et al., 2013) or by factoring the problem into sentence-level name detection and OOV word detection (He et al., 2014) (since OOV errors in general are more frequent than name errors). Sentence-level features are also shown to be more robust than local context in direct name error prediction (Marin, 2015). While these techniques provide some benefit, the use of discrete lexical context cues is sensitive to the limited amount of training data available. Our work leverages the factored approach, but improve performance by using a continuous-space sentence representation for predicting presence of a name. Specifically, we modify a recurrent neural network (RNN) language model (LM) to predict both the word sequence and a sentence-level name indicator. Combining the LM objective with name prediction provides a regularization effect in training that leads to improved sentence-level 737 Proceedings o</context>
<context position="6634" citStr="Marin, 2015" startWordPosition="1045" endWordPosition="1046">nd anomalous word sequences in a local region. For name errors, lexical cues to names are also useful, e.g. a person’s title, location prepositions, or keywords such as “name”. The baseline system for this work uses structural features extracted from a confusion network of ASR hypotheses plus ASR word confidence to represent recognizer uncertainty, and word n-gram context to the left and right of the target confusion network slot. These features are combined in a maximum entropy (ME) classifier trained to predict name errors directly. This is the same as the baseline used in (He et al., 2014; Marin, 2015; Marin et al., 2015), but with a different ASR system. Training a classifier to predict whether a sentence has a name is easier than direct name error prediction, because the positive class is less rare, and it does not require recognizer output so more data can be used (e.g. speech transcripts without recognizer output, or written text). In addition, since the words abutting the name are less reliable in a recognition hypothesis, the information lost by working at the sentence level is minimal. The idea of using sentence-level name prediction is proposed in (He et al., 2014), but in that wor</context>
<context position="7932" citStr="Marin, 2015" startWordPosition="1274" endWordPosition="1275">learned by the sentence-level predictor). In our work, the problem is factored to use the acoustic confusibility and local word class features for OOV error prediction, which is combined with the sentence-level name posterior for name error prediction. In other words, only two features (posteriors) are used in training with the sparse name error prediction data. An ME classifier is then used to combine the two features to predict the wordlevel name error. The word-level OOV detector is another ME binary classifier; the full set of features used for the word-level OOV detector can be found in (Marin, 2015). The main innovation in this work is that we propose to use a multi-task RNN model for the sentence-level name prediction, where the training objective takes into account both the language modeling task and the sentence-level name prediction task, as described in the next section. 3 Multi-task Recurrent Neural Network The RNN is a powerful sequential model and has proven to be useful in many natural language processing tasks, including language modeling (Mikolov et al., 2010) and word similarity (Mikolov et al., 2013c). It also achieves good results for a variety of text classification proble</context>
</contexts>
<marker>Marin, 2015</marker>
<rawString>Alex Marin. 2015. Effective use of cross-domain parsing in automatic speech recognition and error detection. Ph.D. thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´a˘s Mikolov</author>
<author>Martin Karafi´at</author>
<author>Luk´a˘s Burget</author>
<author>Jan ˘Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proc. Interspeech.</booktitle>
<marker>Mikolov, Karafi´at, Burget, ˘Cernock´y, Khudanpur, 2010</marker>
<rawString>Tom´a˘s Mikolov, Martin Karafi´at, Luk´a˘s Burget, Jan ˘Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proc. Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>I Sutskever</author>
<author>K Chen</author>
<author>G S Corrado</author>
<author>J Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="8455" citStr="Mikolov et al., 2013" startWordPosition="1359" endWordPosition="1362">fier; the full set of features used for the word-level OOV detector can be found in (Marin, 2015). The main innovation in this work is that we propose to use a multi-task RNN model for the sentence-level name prediction, where the training objective takes into account both the language modeling task and the sentence-level name prediction task, as described in the next section. 3 Multi-task Recurrent Neural Network The RNN is a powerful sequential model and has proven to be useful in many natural language processing tasks, including language modeling (Mikolov et al., 2010) and word similarity (Mikolov et al., 2013c). It also achieves good results for a variety of text classification problems when combined with a convolutional neural network (CNN) (Lai et al., 2015). In this paper, we 738 Figure 1: The structure of the proposed MT RNN model, which predicts both the next word o(t) and whether the sentence contains a name y(t) at each time step. propose an MT RNN for the sentence-level name prediction task, which augments the word prediction in the RNN language model with an additional output layer for sentence-level name prediction. Formally, the MT RNN is defined over t = 1, ... , n for a sentence of le</context>
<context position="17947" citStr="Mikolov et al., 2013" startWordPosition="2980" endWordPosition="2983">ctiveness of the proposed MT RNN model, we first apply it to the sentencelevel name prediction task on ASR hypotheses. For this task, each sample corresponds to a hypothesized sentence generated by the ASR system and a ground-truth label indicating whether there are names in that sentence. We compare the MT RNN with four contrasting models for predicing whether a sentence includes a name. • BOW + ME. An ME classifier using a bag-ofwords (BOW) sentence representation. • SG + ME. An ME classifier is used with the sentence embedding as features, where the embedding uses the skip-gram (SG) model (Mikolov et al., 2013a) to get word-level embeddings (with window size 10) and sentence embeddings are composed by averaging embeddings of all words in the sentence. • RNN + ME. A simple RNN LM is trained (i.e., λ in (1) is set to 0), and the hidden layer for the last word in a sentence provides a sentencelevel embedding that is used in an ME classifier trained to predict the sentence-level name label. • ST RNN. A single-task (ST) RNN model is trained to directly predict the sentence-level name for each word (i.e., λ in (1) is set to 1). All models are trained on either BOLT-Train or BOLT-Train + Reddit, and tuned</context>
<context position="30063" citStr="Mikolov et al., 2013" startWordPosition="5091" endWordPosition="5094">0 15 0 5 5 20 15 10 5 0 5 10 15 20 negative positive 20 20 15 10 10 15 5 0 5 20 15 10 5 0 5 10 15 20 25 negative positive (b) ST RNN. negative positive 20 15 10 5 0 5 10 15 20 25 20 15 10 5 0 5 15 20 10 Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models. 5 Related Work Recently, due to the success of continuous representation methods, much work has been devoted to studying methods for learning word embeddings that capture semantic and syntactic meaning. Although these word embeddings are shown to be successful in some word-level tasks, such as word analogy (Mikolov et al., 2013b; Jeffery Pennington, 2014) and semantic role labeling (Collobert and Weston, 2008), it is still an open question how best to compose the word embeddings effectively and make use of them for text understanding. Recent work on learning continuous sentence representations usually compose the word embeddings using either a convolutional neural network (CNN), a tree-structured recursive NN, or a variant of an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representat</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013a. Distributed representations of words and phrases and their compositionality. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="8455" citStr="Mikolov et al., 2013" startWordPosition="1359" endWordPosition="1362">fier; the full set of features used for the word-level OOV detector can be found in (Marin, 2015). The main innovation in this work is that we propose to use a multi-task RNN model for the sentence-level name prediction, where the training objective takes into account both the language modeling task and the sentence-level name prediction task, as described in the next section. 3 Multi-task Recurrent Neural Network The RNN is a powerful sequential model and has proven to be useful in many natural language processing tasks, including language modeling (Mikolov et al., 2010) and word similarity (Mikolov et al., 2013c). It also achieves good results for a variety of text classification problems when combined with a convolutional neural network (CNN) (Lai et al., 2015). In this paper, we 738 Figure 1: The structure of the proposed MT RNN model, which predicts both the next word o(t) and whether the sentence contains a name y(t) at each time step. propose an MT RNN for the sentence-level name prediction task, which augments the word prediction in the RNN language model with an additional output layer for sentence-level name prediction. Formally, the MT RNN is defined over t = 1, ... , n for a sentence of le</context>
<context position="17947" citStr="Mikolov et al., 2013" startWordPosition="2980" endWordPosition="2983">ctiveness of the proposed MT RNN model, we first apply it to the sentencelevel name prediction task on ASR hypotheses. For this task, each sample corresponds to a hypothesized sentence generated by the ASR system and a ground-truth label indicating whether there are names in that sentence. We compare the MT RNN with four contrasting models for predicing whether a sentence includes a name. • BOW + ME. An ME classifier using a bag-ofwords (BOW) sentence representation. • SG + ME. An ME classifier is used with the sentence embedding as features, where the embedding uses the skip-gram (SG) model (Mikolov et al., 2013a) to get word-level embeddings (with window size 10) and sentence embeddings are composed by averaging embeddings of all words in the sentence. • RNN + ME. A simple RNN LM is trained (i.e., λ in (1) is set to 0), and the hidden layer for the last word in a sentence provides a sentencelevel embedding that is used in an ME classifier trained to predict the sentence-level name label. • ST RNN. A single-task (ST) RNN model is trained to directly predict the sentence-level name for each word (i.e., λ in (1) is set to 1). All models are trained on either BOLT-Train or BOLT-Train + Reddit, and tuned</context>
<context position="30063" citStr="Mikolov et al., 2013" startWordPosition="5091" endWordPosition="5094">0 15 0 5 5 20 15 10 5 0 5 10 15 20 negative positive 20 20 15 10 10 15 5 0 5 20 15 10 5 0 5 10 15 20 25 negative positive (b) ST RNN. negative positive 20 15 10 5 0 5 10 15 20 25 20 15 10 5 0 5 15 20 10 Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models. 5 Related Work Recently, due to the success of continuous representation methods, much work has been devoted to studying methods for learning word embeddings that capture semantic and syntactic meaning. Although these word embeddings are shown to be successful in some word-level tasks, such as word analogy (Mikolov et al., 2013b; Jeffery Pennington, 2014) and semantic role labeling (Collobert and Weston, 2008), it is still an open question how best to compose the word embeddings effectively and make use of them for text understanding. Recent work on learning continuous sentence representations usually compose the word embeddings using either a convolutional neural network (CNN), a tree-structured recursive NN, or a variant of an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representat</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´a˘s Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proc. NAACL-HLT.</booktitle>
<contexts>
<context position="8455" citStr="Mikolov et al., 2013" startWordPosition="1359" endWordPosition="1362">fier; the full set of features used for the word-level OOV detector can be found in (Marin, 2015). The main innovation in this work is that we propose to use a multi-task RNN model for the sentence-level name prediction, where the training objective takes into account both the language modeling task and the sentence-level name prediction task, as described in the next section. 3 Multi-task Recurrent Neural Network The RNN is a powerful sequential model and has proven to be useful in many natural language processing tasks, including language modeling (Mikolov et al., 2010) and word similarity (Mikolov et al., 2013c). It also achieves good results for a variety of text classification problems when combined with a convolutional neural network (CNN) (Lai et al., 2015). In this paper, we 738 Figure 1: The structure of the proposed MT RNN model, which predicts both the next word o(t) and whether the sentence contains a name y(t) at each time step. propose an MT RNN for the sentence-level name prediction task, which augments the word prediction in the RNN language model with an additional output layer for sentence-level name prediction. Formally, the MT RNN is defined over t = 1, ... , n for a sentence of le</context>
<context position="17947" citStr="Mikolov et al., 2013" startWordPosition="2980" endWordPosition="2983">ctiveness of the proposed MT RNN model, we first apply it to the sentencelevel name prediction task on ASR hypotheses. For this task, each sample corresponds to a hypothesized sentence generated by the ASR system and a ground-truth label indicating whether there are names in that sentence. We compare the MT RNN with four contrasting models for predicing whether a sentence includes a name. • BOW + ME. An ME classifier using a bag-ofwords (BOW) sentence representation. • SG + ME. An ME classifier is used with the sentence embedding as features, where the embedding uses the skip-gram (SG) model (Mikolov et al., 2013a) to get word-level embeddings (with window size 10) and sentence embeddings are composed by averaging embeddings of all words in the sentence. • RNN + ME. A simple RNN LM is trained (i.e., λ in (1) is set to 0), and the hidden layer for the last word in a sentence provides a sentencelevel embedding that is used in an ME classifier trained to predict the sentence-level name label. • ST RNN. A single-task (ST) RNN model is trained to directly predict the sentence-level name for each word (i.e., λ in (1) is set to 1). All models are trained on either BOLT-Train or BOLT-Train + Reddit, and tuned</context>
<context position="30063" citStr="Mikolov et al., 2013" startWordPosition="5091" endWordPosition="5094">0 15 0 5 5 20 15 10 5 0 5 10 15 20 negative positive 20 20 15 10 10 15 5 0 5 20 15 10 5 0 5 10 15 20 25 negative positive (b) ST RNN. negative positive 20 15 10 5 0 5 10 15 20 25 20 15 10 5 0 5 15 20 10 Figure 3: t-SNE visualization of sentence embeddings learned from different RNN models. 5 Related Work Recently, due to the success of continuous representation methods, much work has been devoted to studying methods for learning word embeddings that capture semantic and syntactic meaning. Although these word embeddings are shown to be successful in some word-level tasks, such as word analogy (Mikolov et al., 2013b; Jeffery Pennington, 2014) and semantic role labeling (Collobert and Weston, 2008), it is still an open question how best to compose the word embeddings effectively and make use of them for text understanding. Recent work on learning continuous sentence representations usually compose the word embeddings using either a convolutional neural network (CNN), a tree-structured recursive NN, or a variant of an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representat</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tom´a˘s Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proc. NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="12531" citStr="Mnih and Teh, 2012" startWordPosition="2070" endWordPosition="2073">l name prediction task, implying that balancing the use of information sources is an important design choice for multi-task models. The training objective is optimized using stochastic gradient descent (SGD). We also experiment with AdaGrad (Duchi et al., 2011), which has shown to be more stable and converge faster for non-convex SGD optimization. Since language model training requires a normalization operation each time over the whole vocabulary (~60K) which is computationally intensive, we further speed up training by using noise contrastive estix(t) = h(t) = o(t) = y(t) = 739 mation (NCE) (Mnih and Teh, 2012). For all models using NCE, we fix the number of negative samples to 50. All the weights are randomly initialized in the range of [−0.1, 0.1]. The hidden layer size k is selected from {50, 100, 200} and the task mixing weight A is select in {0.2, 0.4, 0.6, 0.8}, based on development set performance. We set the initial learning rate to 1 and 0.1 for SGD with and without Adagrad respectively. In our experiments, we observe that models trained with AdaGrad achieve better performance, so we only report the models with AdaGrad in this paper. At each training epoch, we validate the objective on the </context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Palmer</author>
<author>Mari Ostendorf</author>
</authors>
<title>Improving out-of-vocabulary name resolution. Computer Speech and language,</title>
<date>2005</date>
<contexts>
<context position="1761" citStr="Palmer and Ostendorf, 2005" startWordPosition="264" endWordPosition="268">es the decision). The most difficult OOV words to cover are names, since they are less likely to be covered by morpheme-like subword fragments and they often result in anomalous recognition output, e.g. REF: what can we get at Litanfeeth HYP: what can we get it leaks on feet While these errors are rare, they create major problems for language processing, since names tend to be important for many applications. Thus, it is of interest to automatically detect such error regions for additional analysis or human correction. Named entity recognition (NER) systems have been applied to speech output (Palmer and Ostendorf, 2005; Sudoh et al., 2006), taking advantage of local contextual cues to names (e.g. titles for person names), but as illustrated above, neighboring words are often affected, which obscures lexical cues to name regions. Parada et al. (2011) reduce this problem somewhat by applying an NER tagger to a word confusion network (WCN) based on a hybrid word/fragment ASR system. In addition to the problem of noisy context, automatic name error detection is challenging because name errors are rare for a good recognizer. To learn the cues to name errors, it is necessary to train from the output of the target</context>
</contexts>
<marker>Palmer, Ostendorf, 2005</marker>
<rawString>David Palmer and Mari Ostendorf. 2005. Improving out-of-vocabulary name resolution. Computer Speech and language, 19(1):107–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolina Parada</author>
<author>Mark Dredze</author>
<author>Frederick Jelinek</author>
</authors>
<title>OOV sensitive named-entity recognition in speech.</title>
<date>2011</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>2085--2088</pages>
<contexts>
<context position="1996" citStr="Parada et al. (2011)" startWordPosition="304" endWordPosition="307">: what can we get it leaks on feet While these errors are rare, they create major problems for language processing, since names tend to be important for many applications. Thus, it is of interest to automatically detect such error regions for additional analysis or human correction. Named entity recognition (NER) systems have been applied to speech output (Palmer and Ostendorf, 2005; Sudoh et al., 2006), taking advantage of local contextual cues to names (e.g. titles for person names), but as illustrated above, neighboring words are often affected, which obscures lexical cues to name regions. Parada et al. (2011) reduce this problem somewhat by applying an NER tagger to a word confusion network (WCN) based on a hybrid word/fragment ASR system. In addition to the problem of noisy context, automatic name error detection is challenging because name errors are rare for a good recognizer. To learn the cues to name errors, it is necessary to train from the output of the target recognizer, so machine learning is faced with infrequent positive examples for which training data is very sparse. In addition, in an open domain system, automatically-learned lexical context features from one domain may be useless in</context>
</contexts>
<marker>Parada, Dredze, Jelinek, 2011</marker>
<rawString>Carolina Parada, Mark Dredze, and Frederick Jelinek. 2011. OOV sensitive named-entity recognition in speech. In Proc. Interspeech, pages 2085–2088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Seltzer</author>
<author>Jasha Droppo</author>
</authors>
<title>Multitask learning in deep neural networks for improved phoneme recognition.</title>
<date>2013</date>
<booktitle>In Proc. ICASSP.</booktitle>
<contexts>
<context position="33030" citStr="Seltzer and Droppo, 2013" startWordPosition="5556" endWordPosition="5559">, both models are trained following the SGD manner by alternating tasks for each training samples with task-dependent training objectives. In this paper, we combine the language modeling task with the sentence-level name prediction task, and each training sample has labels for both tasks. Therefore, the SGD training can be done with the weighted sum of the task-specific objectives for each training sample, and the language model objective can be thought of as a regularization term. Similar settings of multitask learning for neural network models are employed in phoneme recognition for speech (Seltzer and Droppo, 2013) and speech synthesis (Wu et al., 2015) as well, but both of them use equal weights for all tasks. 6 Conclusion In this paper, we address an open domain rare event detection problem, specifically, name error detection on ASR hypotheses. To alleviate the data skewness and domain mismatch problems, we adopt a factored approach (sentencelevel name prediction and OOV error prediction) and propose an MT RNN for sentence-level name prediction. The factored model is shown to be more robust to the sparse training problem. For the problem of sentence-level name prediction, the proposed method of combin</context>
</contexts>
<marker>Seltzer, Droppo, 2013</marker>
<rawString>Michael L. Seltzer and Jasha Droppo. 2013. Multitask learning in deep neural networks for improved phoneme recognition. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector space.</title>
<date>2012</date>
<booktitle>In Proc. EMNLP-CoNLL.</booktitle>
<contexts>
<context position="31068" citStr="Socher et al., 2012" startWordPosition="5243" endWordPosition="5246"> NN, or a variant of an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representation for sentence-level classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling used in this paper can be viewed as a linearized tree-structure model. In this paper, we train the neural network model with a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to (Caruana, 1997), and</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector space. In Proc. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank. In</title>
<date>2013</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="31089" citStr="Socher et al., 2013" startWordPosition="5247" endWordPosition="5250">an RNN. A typical CNN-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representation for sentence-level classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling used in this paper can be viewed as a linearized tree-structure model. In this paper, we train the neural network model with a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to (Caruana, 1997), and is shown to be effec</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Incorporating speech recognition confidence into discriminative named entity recognition of speech.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1782" citStr="Sudoh et al., 2006" startWordPosition="269" endWordPosition="272">ifficult OOV words to cover are names, since they are less likely to be covered by morpheme-like subword fragments and they often result in anomalous recognition output, e.g. REF: what can we get at Litanfeeth HYP: what can we get it leaks on feet While these errors are rare, they create major problems for language processing, since names tend to be important for many applications. Thus, it is of interest to automatically detect such error regions for additional analysis or human correction. Named entity recognition (NER) systems have been applied to speech output (Palmer and Ostendorf, 2005; Sudoh et al., 2006), taking advantage of local contextual cues to names (e.g. titles for person names), but as illustrated above, neighboring words are often affected, which obscures lexical cues to name regions. Parada et al. (2011) reduce this problem somewhat by applying an NER tagger to a word confusion network (WCN) based on a hybrid word/fragment ASR system. In addition to the problem of noisy context, automatic name error detection is challenging because name errors are rare for a good recognizer. To learn the cues to name errors, it is necessary to train from the output of the target recognizer, so machi</context>
</contexts>
<marker>Sudoh, Tsukada, Isozaki, 2006</marker>
<rawString>Katsuhito Sudoh, Hajime Tsukada, and Hideki Isozaki. 2006. Incorporating speech recognition confidence into discriminative named entity recognition of speech. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="31108" citStr="Tai et al., 2015" startWordPosition="5251" endWordPosition="5254">-based structure composes word embeddings in a hierachical fashion (alternating between convolutional layers and pooling layers) to form the continuous sentence representation for sentence-level classification tasks (Kim, 2014; Kalchbrenner et al., 2014; Lai et al., 2015). These models usually build up the sentence representation directly from the lexical surface representation and rely on the pooling layer to capture the dependencies between words. Another popular method for continuous sentence representation is based on the recursive neural network (Socher et al., 2012; Socher et al., 2013; Tai et al., 2015). These models use a tree structure to compose a continuous sentence representation and have the advantages of capturing more fine-grained sentential structure due to the use of parsing trees. Note that the RNN-based sequential modeling used in this paper can be viewed as a linearized tree-structure model. In this paper, we train the neural network model with a multi-task objective reflecting both the probability of the sequence and the probability that the sequence contains names. The general idea of multitask learning dates back to (Caruana, 1997), and is shown to be effective recently for n</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing data using t-SNE.</title>
<date>2008</date>
<journal>Machine Learning Research,</journal>
<volume>9</volume>
<marker>van der Maaten, Hinton, 2008</marker>
<rawString>Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Machine Learning Research, 9, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhizheng Wu</author>
<author>Cassia Valentini-Botinhao</author>
<author>Oliver Watts</author>
<author>Simon King</author>
</authors>
<title>Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis. In</title>
<date>2015</date>
<booktitle>Proc. ICASSP.</booktitle>
<contexts>
<context position="33069" citStr="Wu et al., 2015" startWordPosition="5563" endWordPosition="5566">er by alternating tasks for each training samples with task-dependent training objectives. In this paper, we combine the language modeling task with the sentence-level name prediction task, and each training sample has labels for both tasks. Therefore, the SGD training can be done with the weighted sum of the task-specific objectives for each training sample, and the language model objective can be thought of as a regularization term. Similar settings of multitask learning for neural network models are employed in phoneme recognition for speech (Seltzer and Droppo, 2013) and speech synthesis (Wu et al., 2015) as well, but both of them use equal weights for all tasks. 6 Conclusion In this paper, we address an open domain rare event detection problem, specifically, name error detection on ASR hypotheses. To alleviate the data skewness and domain mismatch problems, we adopt a factored approach (sentencelevel name prediction and OOV error prediction) and propose an MT RNN for sentence-level name prediction. The factored model is shown to be more robust to the sparse training problem. For the problem of sentence-level name prediction, the proposed method of combining the language modeling and sentence-</context>
</contexts>
<marker>Wu, Valentini-Botinhao, Watts, King, 2015</marker>
<rawString>Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, and Simon King. 2015. Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis. In Proc. ICASSP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
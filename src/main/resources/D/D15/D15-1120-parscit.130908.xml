<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000028">
<title confidence="0.981619">
Touch-Based Pre-Post-Editing of Machine Translation Output
</title>
<author confidence="0.89266">
Benjamin Marie Aur´elien Max
</author>
<note confidence="0.4669795">
LIMSI-CNRS, Orsay, France LIMSI-CNRS, Orsay, France
Lingua et Machina, Le Chesnay, France Univ. Paris Sud, Orsay, France
</note>
<email confidence="0.969306">
benjamin.marie@limsi.fr aurelien.max@limsi.fr
</email>
<sectionHeader confidence="0.993134" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977384615385">
We introduce pre-post-editing, possibly
the most basic form of interactive trans-
lation, as a touch-based interaction with
iteratively improved translation hypothe-
ses prior to classical post-editing. We re-
port simulated experiments that yield very
large improvements on classical evalua-
tion metrics (up to 21 BLEU) as well as
on a parameterized variant of the TER
metric that takes into account the cost of
matching/touching tokens, confirming the
promising prospects of the novel transla-
tion scenarios offered by our approach.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972716981132">
As shown by oracle studies (Wisniewski et al.,
2010; Turchi et al., 2012; Marie and Max, 2013),
Statistical Machine Translation (SMT) systems
produce results that are of significantly lower qual-
ity than what could be produced from their avail-
able resources. As a pragmatic solution, human
intervention is commonly used for improving au-
tomatic draft translations, in so-called post-editing
(PE), but is also studied earlier in the translation
process in a variety of interactive strategies, in-
cluding e.g. completion assistance and local trans-
lation choices (e.g. (Foster et al., 2002; Koehn and
Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Al-
though interactive machine translation does facil-
itate the work of the SMT system in certain situa-
tions by allowing it to make efficient use of knowl-
edge contributed by the human translator, post-
editing has been shown to remain a faster alter-
native (Green et al., 2014). Nevertheless, this ac-
tivity usually requires complex intervention from
an expert translator (Carl et al., 2011).
In this work we reduce interaction with an SMT
system to its most basic form: similarly to what a
human translator is likely to do when first reading
a draft translation to post-edit, we require a user
to simply spot those segments of a draft transla-
tion that can participate in an acceptable transla-
tion. The corresponding information is then used
by a SMT system in a soft way to improve the
draft translation. This process may be iteratively
repeated as long as enough improvements are ob-
tained, and terminates with classical post-editing
on the obtained translation, hence we dub it pre-
post-editing (PPE). We resort to simulated pre-
post-editing and post-editing, as in other works
(Carl et al., 2011; Denkowski et al., 2014), to
measure translation performance on some avail-
able reference translation using both classical met-
rics and a variant of the TER metric (Snover et
al., 2006), where, essentially, the cost of a token
matching operation is a parameterized fraction of
the cost of the other token edit operations. With
the implementation of appropriate strategies in the
SMT system, we show under reasonable assump-
tions that this approach has the potential to signifi-
cantly reduce the amount of human effort required
to obtain a final translation.
In the remainder of this article, we describe the
technical details of pre-post-editing (Section 2),
report experiments conducted on two translation
directions and two domains (Section 3), and fi-
nally discuss our proposal and introduce our future
work (Section 4).
</bodyText>
<sectionHeader confidence="0.555823" genericHeader="method">
2 Touch-based pre-post-editing
</sectionHeader>
<bodyText confidence="0.999968166666667">
In our PPE framework, the human pre-post-editor
has to mark n-grams from a translation hypoth-
esis that can take part in a correct translation.1
The annotated n-grams are counted, as an n-gram
can appear more than once in the same sentence,
and a “positive” 6-gram language model (LM)
</bodyText>
<footnote confidence="0.999066">
1A touch-based interface when a keyboard is not available
or typing is inconvenient lends itself particularly well to PPE.
</footnote>
<page confidence="0.811024">
1040
</page>
<note confidence="0.6913695">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1040–1045,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999729978723405">
(positive-lm) is trained on these counts2. A
“negative” LM (negative-lm) is also trained
on the counted n-grams left unannotated. Then,
all bi-phrases from the SMT system’s phrase ta-
ble that match an annotated n-gram, according to
the source token alignments provided by the de-
coder, are removed from the main phrase table
and stored in a separate “positive” phrase table
(positive-pt). Conversely, n-grams contain-
ing at least one token left unannotated are consid-
ered as incorrect, and the set of bi-phrases match-
ing these n-grams are removed and stored in a
“negative” phrase table (negative-pt).
As source tokens can appear more than once in a
source text, they are located: an identifier is con-
catenated to each token to make it unique in the
source text. Tokens of the source phrases in the
phrase table are accordingly also located, so each
bi-phrase is duplicated as needed to cover all lo-
cated tokens. Using located tokens allows our PPE
framework to treat differently source tokens that
are correctly translated from incorrectly translated
ones in the same sentence or text. Figure 1 shows
an example of phrase table extraction, using lo-
cated source tokens3, for one iteration of PPE.
If an n-gram is annotated as correct, all its in-
ner n-grams of lower order are also deemed cor-
rect. Although annotating translations of high
quality may be less expensive by explicitely anno-
tating incorrect n-grams instead of correct ones,
such annotations would not permit to identify cor-
rect n-grams inside incorrect ones, as illustrated
in Figure 2. PPE can thus be worded as a simple
problem for the pre-post-editor: which sequences
of tokens should appear in the final translation?
The newly extracted phrase tables and LMs4,
along with the remainder of the original phrase
table and the original LM, are used to re-decode
the source text in a first iteration of PPE. A new
PPE annotation can then be performed on the new
translations. The newly extracted “positive” and
“negative” phrase tables are merged with the cor-
responding phrase table of the previous iteration.
The extracted n-gram counts from the current iter-
ation and the counts of the previous iterations are
summed, and the LMs are re-trained with the up-
dated counts. A new iteration of PPE is then per-
</bodyText>
<footnote confidence="0.9972916">
2We used SRILM (Stolcke, 2002) to train the LMs with
Witten-Bell smoothing.
3Subsequent examples do not use located tokens.
4The extracted LMs are sentence-level, and are only used
on their specific sentence during PPE.
</footnote>
<table confidence="0.5780312">
source un@0 retour@1 au@2 calme@3 pr´ecaire@4 .@5
hypothesis a return to calm is precarious .
target ref. return to precarious calm .
positive-pt negative-pt
source target source target
retour@1 au@2 return to pr´ecaire@4 is precarious
pr´ecaire@4 precarious calme@3 pr´ecaire@4 calm is precarious
.@5 . pr´ecaire@4 .@5 is precarious.
positive-lm negative-lm
n-gram count n-gram count
</table>
<figureCaption confidence="0.8081614">
return 1 a 1
return to 1 a return to 1
to 1 to precarious 1
calm 1 to calm is precarious . 1
Figure 1: Examples of some of the bi-phrases and
</figureCaption>
<bodyText confidence="0.64399">
n-grams extracted for phrase tables and language
models according to a reference translation.
</bodyText>
<figure confidence="0.919344">
source son impopularit´e semble ˆetre en grande partie due au chˆomage
PPE#0 his unpopularity seems to be owing
PPE#1 his unpopularity seems to be largely owing to unemployment
target ref his unpopularity seems to be largely owing to unemployment
</figure>
<figureCaption confidence="0.987929">
Figure 2: Annotation example for two correct to-
</figureCaption>
<bodyText confidence="0.930909777777778">
kens forming an incorrect n-gram. At the first PPE
iteration a reordering is performed and the new hy-
pothesis now matches the reference translation.
formed with the updated models. The weights for
all, old or new, models in the log-linear combina-
tion are found by tuning on a development set for
each PPE iteration.5 Figure 3 illustrates 4 itera-
tions of PPE from an initial translation hypothesis
assuming a given target reference translation.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999892">
3.1 Data and systems
</subsectionHeader>
<bodyText confidence="0.999982727272727">
We ran experiments on two translation tasks of
different domains: the WMT’14 Medical trans-
lation task (medical) and the WMT’11 news
translation task (news) for the language pair en-fr
on both translation directions. For both tasks we
trained two competitive phrase-based SMT sys-
tems using Moses (Koehn et al., 2007) and WMT
data6 (see Table 1). The tuning for all systems,
including our iteration-specific PPE systems, was
performed with kb-mira (Cherry and Foster,
2012).
</bodyText>
<subsectionHeader confidence="0.999937">
3.2 An adapted evaluation metric: TERPPE
</subsectionHeader>
<bodyText confidence="0.919213">
Classical MT evaluation metrics cannot take into
account the interactive cost of PPE, and thus do
</bodyText>
<footnote confidence="0.815443">
5In this work, we did not exceed 5 iterations.
6http://www.statmt.org/wmt14
largely to unemployment
</footnote>
<page confidence="0.890371">
1041
</page>
<bodyText confidence="0.506880333333333">
source c’ est la r´eponse a` une nouvelle prise de conscience selon laquelle les entreprises chinoises sont indispensables
a` la survie ´economique de Taiwan
target ref it is responding to anew awareness that Chinese business is essential to Taiwan’s economic survival.
</bodyText>
<figureCaption confidence="0.98984375">
Figure 3: Example of a pre-post-edition trace for French to English translation (using the news task,
cf. Section 3) using a given implicit target reference translation for simulating pre-post-editing and post-
editing. Each newly touched phrase is indicated with a green background. Phrases with a gray back-
ground indicate previously touched phrases but their tokens remain individually touchable by the user.
</figureCaption>
<figure confidence="0.982244523809524">
PPE#1
it is
the response to a new awareness that Chinese
companies are essential to
the economic
survival of Taiwan
Taiwan’s economic survival.
firms are essential to
to a new awareness that Chinese enterprises
is essential to Taiwan’s economic survival.
PPE#2
it is the reply
it is responding
to a new awareness that Chinese businesses
PPE#3
it is responding to a new awareness that Chinese business
PPE#4
is essential to Taiwan’s economic survival.
PPE#0 this is
the answer to a new awareness that Chinese
is essential to Taiwan’s economic survival.
</figure>
<table confidence="0.972787222222222">
Tasks Corpus Sentences Tokens (fr-en)
train 12M 383M - 318M
news dev 2,525 73k - 65k
test 3,003 85k - 74k
train 4.9M 91M - 78M
medical dev 500 12k - 10k
test 1,000 26k - 21k
specialized LM 146M - 78M
for both tasks LM 2.5B - 6B
</table>
<tableCaption confidence="0.999883">
Table 1: Data used in this work.
</tableCaption>
<bodyText confidence="0.9999768125">
not allow us to make direct comparisons with PE.
We thus adapt the TER (Snover et al., 2006)
metric, which typically uses 4 types of token
edits: substitution (s), insertion (i),
deletion (d) and shift (f). While these edit
types all have a (debatable) uniform cost of 1, the
operation of matching (m) a correct token is ig-
nored. We posit that this operation is in fact per-
formed by a human translator during PE (at the
minimum, by recognizing and skipping tokens),
and that it can be directly compared to our touch-
based selection of tokens for PPE. However, we
cannot at this stage of our work provide a realistic
cost value for this operation, and so we introduce
a match cost parameter α, and use the following
as our PPE-aware metric:
</bodyText>
<equation confidence="0.997866666666667">
#s + #d + #i + #f + α#m
TERPPE = (1)
r + αr
</equation>
<bodyText confidence="0.9999555">
where r is the number of tokens in the reference
translation. Note that a null value for α makes
TERPPE correspond to TER, while a value of 1
would indicate that a token matching/touch (m)
is e.g. as costly as a token rewriting (s). We antic-
ipate that a realistic value for α given a reasonably
skilled user should be rather small, but we will
provide TERPPE results for the full range [0, 1].
</bodyText>
<subsectionHeader confidence="0.995562">
3.3 Experimental results
</subsectionHeader>
<bodyText confidence="0.97990227027027">
To validate our approach, we initially used a sim-
ulated post-editing paradigm (Carl et al., 2011;
Denkowski et al., 2014) in which non-post-edited
reference translations are used in lieu of human
post-editions. Results on TER (Snover et al.,
2006) and BLEU (Papineni et al., 2002), tuning
on both metrics, are provided in Tables 2 (news)
and 3 (medical).
First, we observe that whatever the metric and
the task, the first iteration of PPE always yields
a significant improvement over the Moses initial
system (e.g. up to +9.8 BLEU and -8.2 TER for
news fr→en). Unsurprisingly, tuning on a met-
ric yields better results for the same metric for
the first iteration; however, we note that this is
not always true for the TER metric at later itera-
tions (cf. news en→fr). More generally, tuning
on the TER metric results in lower improvements
for news, which are mostly concentrated on the
first iterations; as systems tuned on BLEU have
been found to produce better translations than sys-
tems tuned on TER (Cer et al., 2010), only BLEU
tuning was used for medical.7
Improvements follow an interesting pattern
over PPE iterations: for instance, on news
fr→en, BLEU scores steadily increase after each
new touch-based iteration and reach a gain
of +21.1 BLEU and -12.3 TER over the initial
Moses translation after 5 PPE iterations. Re-
sults are very comparable on both language pairs
and both domains, e.g. gains of +12.1 BLEU
and -9.7 TER are obtained on fr→en medical.
The lesser amplitude of the gains obtained after
5 iterations may be attributed to the higher ini-
7We have observed a tendency of the TER tuning to shrink
the size of hypotheses, resulting in higher brevity penalty val-
ues for BLEU and a higher number of insertions for TER.
</bodyText>
<page confidence="0.964014">
1042
</page>
<table confidence="0.999788444444444">
Iteration fr→en en→fr
tuned with TER tuned with BLEU tuned with TER tuned with BLEU
TER BLEU TER BLEU TER BLEU TER BLEU
Moses 51.1 28.2 52.7 28.6 52.3 29.7 51.8 31.1
PPE iteration 1 42.9 35.4 46.7 38.4 44.4 35.0 47.3 39.6
PPE iteration 2 40.8 37.3 43.7 43.4 43.0 36.3 44.6 43.9
PPE iteration 3 40.8 37.8 42.2 46.2 42.5 36.4 43.5 46.6
PPE iteration 4 39.9 37.9 40.9 48.3 42.3 36.5 42.3 48.2
PPE iteration 5 39.9 37.9 40.4 49.7 42.2 36.6 41.0 49.5
</table>
<tableCaption confidence="0.930424">
Table 2: PPE results on the news task.
</tableCaption>
<table confidence="0.997982111111111">
Iteration fr→en en→fr
tuned with BLEU tuned with BLEU
TER BLEU TER BLEU
Moses 42.2 37.1 44.0 38.8
PPE iteration 1 36.9 44.9 37.2 48.3
PPE iteration 2 34.8 47.5 35.3 51.1
PPE iteration 3 34.1 48.5 33.5 52.9
PPE iteration 4 32.9 49.2 32.4 54.0
PPE iteration 5 32.5 49.2 32.1 54.8
</table>
<figureCaption confidence="0.927008333333333">
tial quality of the translations in the medical
task (e.g. 37.1 BLEU vs 28.6 BLEU in fr→en for
Moses with BLEU tuning).
</figureCaption>
<figure confidence="0.997110066666667">
45
60
58
56
54
52
50
Moses 1-best
PPE iteration 1
PPE iteration 2
PPE iteration 3
PPE iteration 4
PPE iteration 5
42
0.0 0.2 0.4 0.6 0.8 1.0
α
(a) Tuned with TER
60
55
50
Moses 1-best
PPE iteration 1
PPE iteration 2
PPE iteration 3
PPE iteration 4
PPE iteration 5
40
0.0 0.2 0.4 0.6 0.8 1.0
α
(b) Tuned with BLEU
</figure>
<figureCaption confidence="0.908924">
Figure 4: PPE results on the en→fr news task.
</figureCaption>
<figure confidence="0.9215895">
4 Discussion and future work
TERppe
TERppe
48
</figure>
<page confidence="0.7254375">
46
44
</page>
<tableCaption confidence="0.997278">
Table 3: PPE results on the medical task.
</tableCaption>
<bodyText confidence="0.99990669047619">
Figures 4 and 5 show how our TERPPE metric
varies for different values of our α parameter (re-
call that α = 0 corresponds to TER). Essentially,
whatever the value of α, we observe that any it-
eration of PPE dominates PE (Moses 1-best),
but with a tendency to become as costly as PE for
high, but probably unrealistic values of α. Tuning
with BLEU allows us to bring regular improve-
ments as the number of iteration increases, while
tuning with TER makes the amplitude of the gains
decrease faster.
Furthermore, results shown in Table 4 point
out the complementarity between negative
models (negative-lm and negative-pt)
and positive models (positive-lm and
positive-pt), with a drop of almost 10 BLEU
points compared to the corresponding config-
uration using all models when removing one
type of models on both translation directions.
The language models (negative-lm and
positive-lm) seem to play a more impor-
tant role during PPE than the phrase tables
(negative-pt and positive-pt), with
a drop of 9.6 BLEU points on news fr→en
when removing the language models against a
significantly lower drop of 4.4 BLEU points when
removing the phrase tables.
We have introduced pre-post-editing, a minimalist
interactive machine translation paradigm where a
user is only asked to spot text fragments that may
be used in the final translation. Our approach is
quite comparable to the two-pass procedure de-
scribed by Luong et al. (2014) using word-level
confidence estimation (e.g. (Bach et al., 2011)) to
update the cost of the search graph hypotheses.
However, contrarily to Luong et al.’s work, our
PPE framework is efficiently multi-pass, updates
the models over iterations and relies on more in-
formative annotations made at n-gram-level. Our
evaluation based on simulated post-editing has re-
vealed a large potential for translation improve-
ment. Interestingly, the type of interaction defined
</bodyText>
<page confidence="0.884096">
1043
</page>
<table confidence="0.999758333333333">
Configuration fr→en en→fr
tuned with BLEU tuned with BLEU
TER BLEU TER BLEU
Moses 52.7 28.6 51.8 31.1
PPE w/ all models 40.4 49.7 41.0 49.5
PPE w/o negative-pt and negative-lm 45.2 39.4 47.1 39.0
PPE w/o positive-pt and positive-lm 46.7 39.8 48.3 39.8
PPE w/o negative-pt and positive-pt 45.0 45.3 46.5 44.9
PPE w/o negative-lm and positive-lm 42.7 40.1 43.2 42.0
</table>
<tableCaption confidence="0.999362">
Table 4: PPE results for the news task after 5 iterations using various configurations.
</tableCaption>
<figure confidence="0.906805818181818">
60
55
50
45
40
35
0.0 0.2 0.4 0.6 0.8 1.0
α
(a) Tuned with TER
α
(b) Tuned with BLEU
</figure>
<figureCaption confidence="0.999887">
Figure 5: PPE results on the fr→en news task.
</figureCaption>
<bodyText confidence="0.999982259259259">
is very different from that expected of a post-editor
or in existing interactive translation modes, and
lends itself nicely to touch-based interaction. Fur-
thermore, our proposal may in fact define a new
role in Computer-Assisted Translation, with PPE
being performed on-the-go on mobile devices by
more people than available human translators, and
even possibly by monolinguals of the target lan-
guage whose contribution may be more efficiently
exploited than that of monolinguals of the source
language (e.g. (Resnik et al., 2010)).
In terms of usability, our future work will fo-
cus on two important questions: (a) study the
actual use of PPE in an interactive setting and
tune the α parameter for our TERPPE metric on
HTER (Snover et al., 2006) traces, and (b) study
whether PPE alters in any positive way the work
of the human translator performing the resid-
ual post-editing, hoping that PE could become
a less tedious task by nature. We further an-
ticipate that some additions would improve our
approach, including dealing early with out-of-
vocabulary phrases, proposing local drop-down
options (e.g. (Koehn and Haddow, 2009)), possi-
bly clustered by senses, allowing the user to eas-
ily fix reordering issues, and adapting PPE to be
discourse-aware (e.g. (Ture et al., 2012)).
</bodyText>
<sectionHeader confidence="0.998888" genericHeader="method">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99935925">
The authors would like to thank the anonymous
reviewers for their helpful comments and sugges-
tions. The work of the first author is supported by
a CIFRE grant from French ANRT.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999113909090909">
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A Method for Measuring Ma-
chine Translation Confidence. In Proceedings of
ACL, Portland, USA.
Michael Carl, Dragsted Barbara, Jakob Elming, Hardt
Daniel, and Jakobsen Arnt Lykke. 2011. The Pro-
cess of Post-Editing: A pilot study. In Copenhagen
Studies in Language.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Proceedings
of NAACL, Los Angeles, USA.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of NAACL, Montr´eal, Canada.
Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from Post-Editing : Online Model
Adaptation for Statistical Machine Translation. In
Proceedings of EACL, Gothenburg, Sweden.
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-Friendly Text Prediction for Translators.
In Proceedings of EMNLP, Philadelphia, USA.
</reference>
<figure confidence="0.990152">
40
0.0 0.2 0.4 0.6 0.8 1.0
45
65
60
55
50
Moses 1-best
PPE iteration 1
PPE iteration 2
PPE iteration 3
PPE iteration 4
PPE iteration 5
TERppe
TERppe
Moses 1-best
PPE iteration 1
PPE iteration 2
PPE iteration 3
PPE iteration 4
</figure>
<page confidence="0.969303">
1044
</page>
<reference confidence="0.999781339285714">
Jes´us Gonz´alez-Rubio, Daniel Ortiz-Martinez, Jos´e-
Miguel Benedi, and Francisco Casacuberta. 2013.
Interactive Machine Translation using Hierarchical
Translation Models. In Proceedings of EMNLP,
Seattle, USA.
Spence Green, Sida Wang, Jason Chuang, Jeffrey Heer,
Sebastian Schuster, and Christopher D. Manning.
2014. Human Effort and Machine Learnability in
Computer Aided Translation. In Proceedings of
EMNLP, Doha, Qatar.
Philipp Koehn and Barry Haddow. 2009. Interactive
assistance to human translators using statistical ma-
chine translation methods. In Proceedings of MT
Summit, Ottawa, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings ofACL, demo session, Prague, Czech
Republic.
Benjamin Marie and Aur´elien Max. 2013. A Study
in Greedy Oracle Improvement of Translation Hy-
potheses. In Proceedings of IWSLT, Heidelberg,
Germany.
Luong Ngoc Quang, Laurent Besacier, and Lecouteux
Benjamin. 2014. An Efficient Two-Pass Decoder
for SMT Using Word Confidence Estimation. In
Proceedings of EAMT, Dubrovnik, Croatia.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, Philadelphia, USA.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kron-
rod, Alex Quinn, and Benjamin B. Bederson. 2010.
Improving Translation via Targeted Paraphrasing.
In Proceedings of EMNLP, Cambridge, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of AMTA, Cambridge, USA.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
Denver, USA.
Marco Turchi, Tijl De Bie, Cyril Goutte, and Nello
Cristianini. 2012. Learning to Translate: A Sta-
tistical and Computational Analysis. Advances in
Artificial Intelligence.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging Consistent Translation Choices.
In Proceedings of NAACL, Montr´eal, Canada.
Guillaume Wisniewski, Alexandre Allauzen, and
Franc¸ois Yvon. 2010. Assessing Phrase-Based
Translation Models with Oracle Decoding. In Pro-
ceedings of EMNLP, Cambridge, USA.
</reference>
<page confidence="0.990604">
1045
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.505441">
<title confidence="0.996907">Touch-Based Pre-Post-Editing of Machine Translation Output</title>
<author confidence="0.994415">Benjamin Marie Aur´elien Max</author>
<affiliation confidence="0.740031">LIMSI-CNRS, Orsay, France LIMSI-CNRS, Orsay, France</affiliation>
<address confidence="0.584094">Lingua et Machina, Le Chesnay, France Univ. Paris Sud, Orsay, France</address>
<email confidence="0.862237">benjamin.marie@limsi.fraurelien.max@limsi.fr</email>
<abstract confidence="0.998294071428571">introduce possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of tokens, confirming the promising prospects of the novel translation scenarios offered by our approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Fei Huang</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Goodness: A Method for Measuring Machine Translation Confidence.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Portland, USA.</location>
<contexts>
<context position="15846" citStr="Bach et al., 2011" startWordPosition="2640" endWordPosition="2643">ositive-lm) seem to play a more important role during PPE than the phrase tables (negative-pt and positive-pt), with a drop of 9.6 BLEU points on news fr→en when removing the language models against a significantly lower drop of 4.4 BLEU points when removing the phrase tables. We have introduced pre-post-editing, a minimalist interactive machine translation paradigm where a user is only asked to spot text fragments that may be used in the final translation. Our approach is quite comparable to the two-pass procedure described by Luong et al. (2014) using word-level confidence estimation (e.g. (Bach et al., 2011)) to update the cost of the search graph hypotheses. However, contrarily to Luong et al.’s work, our PPE framework is efficiently multi-pass, updates the models over iterations and relies on more informative annotations made at n-gram-level. Our evaluation based on simulated post-editing has revealed a large potential for translation improvement. Interestingly, the type of interaction defined 1043 Configuration fr→en en→fr tuned with BLEU tuned with BLEU TER BLEU TER BLEU Moses 52.7 28.6 51.8 31.1 PPE w/ all models 40.4 49.7 41.0 49.5 PPE w/o negative-pt and negative-lm 45.2 39.4 47.1 39.0 PPE</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011. Goodness: A Method for Measuring Machine Translation Confidence. In Proceedings of ACL, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Carl</author>
<author>Dragsted Barbara</author>
<author>Jakob Elming</author>
<author>Hardt Daniel</author>
<author>Jakobsen Arnt Lykke</author>
</authors>
<title>The Process of Post-Editing: A pilot study.</title>
<date>2011</date>
<booktitle>In Copenhagen Studies in Language.</booktitle>
<contexts>
<context position="1831" citStr="Carl et al., 2011" startWordPosition="269" endWordPosition="272">studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilitate the work of the SMT system in certain situations by allowing it to make efficient use of knowledge contributed by the human translator, postediting has been shown to remain a faster alternative (Green et al., 2014). Nevertheless, this activity usually requires complex intervention from an expert translator (Carl et al., 2011). In this work we reduce interaction with an SMT system to its most basic form: similarly to what a human translator is likely to do when first reading a draft translation to post-edit, we require a user to simply spot those segments of a draft translation that can participate in an acceptable translation. The corresponding information is then used by a SMT system in a soft way to improve the draft translation. This process may be iteratively repeated as long as enough improvements are obtained, and terminates with classical post-editing on the obtained translation, hence we dub it prepost-edi</context>
<context position="11379" citStr="Carl et al., 2011" startWordPosition="1850" endWordPosition="1853">rameter α, and use the following as our PPE-aware metric: #s + #d + #i + #f + α#m TERPPE = (1) r + αr where r is the number of tokens in the reference translation. Note that a null value for α makes TERPPE correspond to TER, while a value of 1 would indicate that a token matching/touch (m) is e.g. as costly as a token rewriting (s). We anticipate that a realistic value for α given a reasonably skilled user should be rather small, but we will provide TERPPE results for the full range [0, 1]. 3.3 Experimental results To validate our approach, we initially used a simulated post-editing paradigm (Carl et al., 2011; Denkowski et al., 2014) in which non-post-edited reference translations are used in lieu of human post-editions. Results on TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), tuning on both metrics, are provided in Tables 2 (news) and 3 (medical). First, we observe that whatever the metric and the task, the first iteration of PPE always yields a significant improvement over the Moses initial system (e.g. up to +9.8 BLEU and -8.2 TER for news fr→en). Unsurprisingly, tuning on a metric yields better results for the same metric for the first iteration; however, we note that this is not</context>
</contexts>
<marker>Carl, Barbara, Elming, Daniel, Lykke, 2011</marker>
<rawString>Michael Carl, Dragsted Barbara, Jakob Elming, Hardt Daniel, and Jakobsen Arnt Lykke. 2011. The Process of Post-Editing: A pilot study. In Copenhagen Studies in Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
<author>Daniel Jurafsky</author>
</authors>
<title>The best lexical metric for phrase-based statistical mt system optimization.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Los Angeles, USA.</location>
<contexts>
<context position="12301" citStr="Cer et al., 2010" startWordPosition="2008" endWordPosition="2011"> the task, the first iteration of PPE always yields a significant improvement over the Moses initial system (e.g. up to +9.8 BLEU and -8.2 TER for news fr→en). Unsurprisingly, tuning on a metric yields better results for the same metric for the first iteration; however, we note that this is not always true for the TER metric at later iterations (cf. news en→fr). More generally, tuning on the TER metric results in lower improvements for news, which are mostly concentrated on the first iterations; as systems tuned on BLEU have been found to produce better translations than systems tuned on TER (Cer et al., 2010), only BLEU tuning was used for medical.7 Improvements follow an interesting pattern over PPE iterations: for instance, on news fr→en, BLEU scores steadily increase after each new touch-based iteration and reach a gain of +21.1 BLEU and -12.3 TER over the initial Moses translation after 5 PPE iterations. Results are very comparable on both language pairs and both domains, e.g. gains of +12.1 BLEU and -9.7 TER are obtained on fr→en medical. The lesser amplitude of the gains obtained after 5 iterations may be attributed to the higher ini7We have observed a tendency of the TER tuning to shrink th</context>
</contexts>
<marker>Cer, Manning, Jurafsky, 2010</marker>
<rawString>Daniel Cer, Christopher D. Manning, and Daniel Jurafsky. 2010. The best lexical metric for phrase-based statistical mt system optimization. In Proceedings of NAACL, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="8343" citStr="Cherry and Foster, 2012" startWordPosition="1325" endWordPosition="1328">illustrates 4 iterations of PPE from an initial translation hypothesis assuming a given target reference translation. 3 Experiments 3.1 Data and systems We ran experiments on two translation tasks of different domains: the WMT’14 Medical translation task (medical) and the WMT’11 news translation task (news) for the language pair en-fr on both translation directions. For both tasks we trained two competitive phrase-based SMT systems using Moses (Koehn et al., 2007) and WMT data6 (see Table 1). The tuning for all systems, including our iteration-specific PPE systems, was performed with kb-mira (Cherry and Foster, 2012). 3.2 An adapted evaluation metric: TERPPE Classical MT evaluation metrics cannot take into account the interactive cost of PPE, and thus do 5In this work, we did not exceed 5 iterations. 6http://www.statmt.org/wmt14 largely to unemployment 1041 source c’ est la r´eponse a` une nouvelle prise de conscience selon laquelle les entreprises chinoises sont indispensables a` la survie ´economique de Taiwan target ref it is responding to anew awareness that Chinese business is essential to Taiwan’s economic survival. Figure 3: Example of a pre-post-edition trace for French to English translation (usi</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of NAACL, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
</authors>
<title>Learning from Post-Editing : Online Model Adaptation for Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL, Gothenburg,</booktitle>
<contexts>
<context position="2561" citStr="Denkowski et al., 2014" startWordPosition="393" endWordPosition="396">ranslator is likely to do when first reading a draft translation to post-edit, we require a user to simply spot those segments of a draft translation that can participate in an acceptable translation. The corresponding information is then used by a SMT system in a soft way to improve the draft translation. This process may be iteratively repeated as long as enough improvements are obtained, and terminates with classical post-editing on the obtained translation, hence we dub it prepost-editing (PPE). We resort to simulated prepost-editing and post-editing, as in other works (Carl et al., 2011; Denkowski et al., 2014), to measure translation performance on some available reference translation using both classical metrics and a variant of the TER metric (Snover et al., 2006), where, essentially, the cost of a token matching operation is a parameterized fraction of the cost of the other token edit operations. With the implementation of appropriate strategies in the SMT system, we show under reasonable assumptions that this approach has the potential to significantly reduce the amount of human effort required to obtain a final translation. In the remainder of this article, we describe the technical details of</context>
<context position="11404" citStr="Denkowski et al., 2014" startWordPosition="1854" endWordPosition="1857">the following as our PPE-aware metric: #s + #d + #i + #f + α#m TERPPE = (1) r + αr where r is the number of tokens in the reference translation. Note that a null value for α makes TERPPE correspond to TER, while a value of 1 would indicate that a token matching/touch (m) is e.g. as costly as a token rewriting (s). We anticipate that a realistic value for α given a reasonably skilled user should be rather small, but we will provide TERPPE results for the full range [0, 1]. 3.3 Experimental results To validate our approach, we initially used a simulated post-editing paradigm (Carl et al., 2011; Denkowski et al., 2014) in which non-post-edited reference translations are used in lieu of human post-editions. Results on TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), tuning on both metrics, are provided in Tables 2 (news) and 3 (medical). First, we observe that whatever the metric and the task, the first iteration of PPE always yields a significant improvement over the Moses initial system (e.g. up to +9.8 BLEU and -8.2 TER for news fr→en). Unsurprisingly, tuning on a metric yields better results for the same metric for the first iteration; however, we note that this is not always true for the TER </context>
</contexts>
<marker>Denkowski, Dyer, Lavie, 2014</marker>
<rawString>Michael Denkowski, Chris Dyer, and Alon Lavie. 2014. Learning from Post-Editing : Online Model Adaptation for Statistical Machine Translation. In Proceedings of EACL, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Philippe Langlais</author>
<author>Guy Lapalme</author>
</authors>
<title>User-Friendly Text Prediction for Translators.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1389" citStr="Foster et al., 2002" startWordPosition="197" endWordPosition="200">red by our approach. 1 Introduction As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower quality than what could be produced from their available resources. As a pragmatic solution, human intervention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilitate the work of the SMT system in certain situations by allowing it to make efficient use of knowledge contributed by the human translator, postediting has been shown to remain a faster alternative (Green et al., 2014). Nevertheless, this activity usually requires complex intervention from an expert translator (Carl et al., 2011). In this work we reduce interaction with an SMT system to its most basic form: similarly to what a human translator is likely to do when first reading a draf</context>
</contexts>
<marker>Foster, Langlais, Lapalme, 2002</marker>
<rawString>George Foster, Philippe Langlais, and Guy Lapalme. 2002. User-Friendly Text Prediction for Translators. In Proceedings of EMNLP, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Daniel Ortiz-Martinez</author>
<author>Jos´eMiguel Benedi</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Interactive Machine Translation using Hierarchical Translation Models.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Seattle, USA.</location>
<marker>Gonz´alez-Rubio, Ortiz-Martinez, Benedi, Casacuberta, 2013</marker>
<rawString>Jes´us Gonz´alez-Rubio, Daniel Ortiz-Martinez, Jos´eMiguel Benedi, and Francisco Casacuberta. 2013. Interactive Machine Translation using Hierarchical Translation Models. In Proceedings of EMNLP, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Sida Wang</author>
<author>Jason Chuang</author>
<author>Jeffrey Heer</author>
<author>Sebastian Schuster</author>
<author>Christopher D Manning</author>
</authors>
<title>Human Effort and Machine Learnability in Computer Aided Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Doha, Qatar.</location>
<contexts>
<context position="1718" citStr="Green et al., 2014" startWordPosition="253" endWordPosition="256">rvention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilitate the work of the SMT system in certain situations by allowing it to make efficient use of knowledge contributed by the human translator, postediting has been shown to remain a faster alternative (Green et al., 2014). Nevertheless, this activity usually requires complex intervention from an expert translator (Carl et al., 2011). In this work we reduce interaction with an SMT system to its most basic form: similarly to what a human translator is likely to do when first reading a draft translation to post-edit, we require a user to simply spot those segments of a draft translation that can participate in an acceptable translation. The corresponding information is then used by a SMT system in a soft way to improve the draft translation. This process may be iteratively repeated as long as enough improvements </context>
</contexts>
<marker>Green, Wang, Chuang, Heer, Schuster, Manning, 2014</marker>
<rawString>Spence Green, Sida Wang, Jason Chuang, Jeffrey Heer, Sebastian Schuster, and Christopher D. Manning. 2014. Human Effort and Machine Learnability in Computer Aided Translation. In Proceedings of EMNLP, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Interactive assistance to human translators using statistical machine translation methods.</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit,</booktitle>
<location>Ottawa, Canada.</location>
<contexts>
<context position="1413" citStr="Koehn and Haddow, 2009" startWordPosition="201" endWordPosition="204">1 Introduction As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower quality than what could be produced from their available resources. As a pragmatic solution, human intervention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilitate the work of the SMT system in certain situations by allowing it to make efficient use of knowledge contributed by the human translator, postediting has been shown to remain a faster alternative (Green et al., 2014). Nevertheless, this activity usually requires complex intervention from an expert translator (Carl et al., 2011). In this work we reduce interaction with an SMT system to its most basic form: similarly to what a human translator is likely to do when first reading a draft translation to post-ed</context>
</contexts>
<marker>Koehn, Haddow, 2009</marker>
<rawString>Philipp Koehn and Barry Haddow. 2009. Interactive assistance to human translators using statistical machine translation methods. In Proceedings of MT Summit, Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL, demo session,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="8187" citStr="Koehn et al., 2007" startWordPosition="1301" endWordPosition="1304">dels. The weights for all, old or new, models in the log-linear combination are found by tuning on a development set for each PPE iteration.5 Figure 3 illustrates 4 iterations of PPE from an initial translation hypothesis assuming a given target reference translation. 3 Experiments 3.1 Data and systems We ran experiments on two translation tasks of different domains: the WMT’14 Medical translation task (medical) and the WMT’11 news translation task (news) for the language pair en-fr on both translation directions. For both tasks we trained two competitive phrase-based SMT systems using Moses (Koehn et al., 2007) and WMT data6 (see Table 1). The tuning for all systems, including our iteration-specific PPE systems, was performed with kb-mira (Cherry and Foster, 2012). 3.2 An adapted evaluation metric: TERPPE Classical MT evaluation metrics cannot take into account the interactive cost of PPE, and thus do 5In this work, we did not exceed 5 iterations. 6http://www.statmt.org/wmt14 largely to unemployment 1041 source c’ est la r´eponse a` une nouvelle prise de conscience selon laquelle les entreprises chinoises sont indispensables a` la survie ´economique de Taiwan target ref it is responding to anew awar</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings ofACL, demo session, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Marie</author>
<author>Aur´elien Max</author>
</authors>
<title>A Study in Greedy Oracle Improvement of Translation Hypotheses.</title>
<date>2013</date>
<booktitle>In Proceedings of IWSLT,</booktitle>
<location>Heidelberg, Germany.</location>
<contexts>
<context position="900" citStr="Marie and Max, 2013" startWordPosition="124" endWordPosition="127">-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, confirming the promising prospects of the novel translation scenarios offered by our approach. 1 Introduction As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower quality than what could be produced from their available resources. As a pragmatic solution, human intervention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although interactive machine translation does facilit</context>
</contexts>
<marker>Marie, Max, 2013</marker>
<rawString>Benjamin Marie and Aur´elien Max. 2013. A Study in Greedy Oracle Improvement of Translation Hypotheses. In Proceedings of IWSLT, Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luong Ngoc Quang</author>
<author>Laurent Besacier</author>
<author>Lecouteux Benjamin</author>
</authors>
<title>An Efficient Two-Pass Decoder for SMT Using Word Confidence Estimation.</title>
<date>2014</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<location>Dubrovnik, Croatia.</location>
<marker>Quang, Besacier, Benjamin, 2014</marker>
<rawString>Luong Ngoc Quang, Laurent Besacier, and Lecouteux Benjamin. 2014. An Efficient Two-Pass Decoder for SMT Using Word Confidence Estimation. In Proceedings of EAMT, Dubrovnik, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="11563" citStr="Papineni et al., 2002" startWordPosition="1879" endWordPosition="1882">l value for α makes TERPPE correspond to TER, while a value of 1 would indicate that a token matching/touch (m) is e.g. as costly as a token rewriting (s). We anticipate that a realistic value for α given a reasonably skilled user should be rather small, but we will provide TERPPE results for the full range [0, 1]. 3.3 Experimental results To validate our approach, we initially used a simulated post-editing paradigm (Carl et al., 2011; Denkowski et al., 2014) in which non-post-edited reference translations are used in lieu of human post-editions. Results on TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), tuning on both metrics, are provided in Tables 2 (news) and 3 (medical). First, we observe that whatever the metric and the task, the first iteration of PPE always yields a significant improvement over the Moses initial system (e.g. up to +9.8 BLEU and -8.2 TER for news fr→en). Unsurprisingly, tuning on a metric yields better results for the same metric for the first iteration; however, we note that this is not always true for the TER metric at later iterations (cf. news en→fr). More generally, tuning on the TER metric results in lower improvements for news, which are mostly concentrated on </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Olivia Buzek</author>
<author>Chang Hu</author>
<author>Yakov Kronrod</author>
<author>Alex Quinn</author>
<author>Benjamin B Bederson</author>
</authors>
<title>Improving Translation via Targeted Paraphrasing.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Cambridge, USA.</location>
<contexts>
<context position="17360" citStr="Resnik et al., 2010" startWordPosition="2888" endWordPosition="2891"> α (a) Tuned with TER α (b) Tuned with BLEU Figure 5: PPE results on the fr→en news task. is very different from that expected of a post-editor or in existing interactive translation modes, and lends itself nicely to touch-based interaction. Furthermore, our proposal may in fact define a new role in Computer-Assisted Translation, with PPE being performed on-the-go on mobile devices by more people than available human translators, and even possibly by monolinguals of the target language whose contribution may be more efficiently exploited than that of monolinguals of the source language (e.g. (Resnik et al., 2010)). In terms of usability, our future work will focus on two important questions: (a) study the actual use of PPE in an interactive setting and tune the α parameter for our TERPPE metric on HTER (Snover et al., 2006) traces, and (b) study whether PPE alters in any positive way the work of the human translator performing the residual post-editing, hoping that PE could become a less tedious task by nature. We further anticipate that some additions would improve our approach, including dealing early with out-ofvocabulary phrases, proposing local drop-down options (e.g. (Koehn and Haddow, 2009)), p</context>
</contexts>
<marker>Resnik, Buzek, Hu, Kronrod, Quinn, Bederson, 2010</marker>
<rawString>Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod, Alex Quinn, and Benjamin B. Bederson. 2010. Improving Translation via Targeted Paraphrasing. In Proceedings of EMNLP, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<location>Cambridge, USA.</location>
<contexts>
<context position="2720" citStr="Snover et al., 2006" startWordPosition="419" endWordPosition="422">ticipate in an acceptable translation. The corresponding information is then used by a SMT system in a soft way to improve the draft translation. This process may be iteratively repeated as long as enough improvements are obtained, and terminates with classical post-editing on the obtained translation, hence we dub it prepost-editing (PPE). We resort to simulated prepost-editing and post-editing, as in other works (Carl et al., 2011; Denkowski et al., 2014), to measure translation performance on some available reference translation using both classical metrics and a variant of the TER metric (Snover et al., 2006), where, essentially, the cost of a token matching operation is a parameterized fraction of the cost of the other token edit operations. With the implementation of appropriate strategies in the SMT system, we show under reasonable assumptions that this approach has the potential to significantly reduce the amount of human effort required to obtain a final translation. In the remainder of this article, we describe the technical details of pre-post-editing (Section 2), report experiments conducted on two translation directions and two domains (Section 3), and finally discuss our proposal and int</context>
<context position="10173" citStr="Snover et al., 2006" startWordPosition="1627" endWordPosition="1630">sponding to a new awareness that Chinese businesses PPE#3 it is responding to a new awareness that Chinese business PPE#4 is essential to Taiwan’s economic survival. PPE#0 this is the answer to a new awareness that Chinese is essential to Taiwan’s economic survival. Tasks Corpus Sentences Tokens (fr-en) train 12M 383M - 318M news dev 2,525 73k - 65k test 3,003 85k - 74k train 4.9M 91M - 78M medical dev 500 12k - 10k test 1,000 26k - 21k specialized LM 146M - 78M for both tasks LM 2.5B - 6B Table 1: Data used in this work. not allow us to make direct comparisons with PE. We thus adapt the TER (Snover et al., 2006) metric, which typically uses 4 types of token edits: substitution (s), insertion (i), deletion (d) and shift (f). While these edit types all have a (debatable) uniform cost of 1, the operation of matching (m) a correct token is ignored. We posit that this operation is in fact performed by a human translator during PE (at the minimum, by recognizing and skipping tokens), and that it can be directly compared to our touchbased selection of tokens for PPE. However, we cannot at this stage of our work provide a realistic cost value for this operation, and so we introduce a match cost parameter α, </context>
<context position="11530" citStr="Snover et al., 2006" startWordPosition="1873" endWordPosition="1876">ce translation. Note that a null value for α makes TERPPE correspond to TER, while a value of 1 would indicate that a token matching/touch (m) is e.g. as costly as a token rewriting (s). We anticipate that a realistic value for α given a reasonably skilled user should be rather small, but we will provide TERPPE results for the full range [0, 1]. 3.3 Experimental results To validate our approach, we initially used a simulated post-editing paradigm (Carl et al., 2011; Denkowski et al., 2014) in which non-post-edited reference translations are used in lieu of human post-editions. Results on TER (Snover et al., 2006) and BLEU (Papineni et al., 2002), tuning on both metrics, are provided in Tables 2 (news) and 3 (medical). First, we observe that whatever the metric and the task, the first iteration of PPE always yields a significant improvement over the Moses initial system (e.g. up to +9.8 BLEU and -8.2 TER for news fr→en). Unsurprisingly, tuning on a metric yields better results for the same metric for the first iteration; however, we note that this is not always true for the TER metric at later iterations (cf. news en→fr). More generally, tuning on the TER metric results in lower improvements for news, </context>
<context position="17575" citStr="Snover et al., 2006" startWordPosition="2928" endWordPosition="2931">o touch-based interaction. Furthermore, our proposal may in fact define a new role in Computer-Assisted Translation, with PPE being performed on-the-go on mobile devices by more people than available human translators, and even possibly by monolinguals of the target language whose contribution may be more efficiently exploited than that of monolinguals of the source language (e.g. (Resnik et al., 2010)). In terms of usability, our future work will focus on two important questions: (a) study the actual use of PPE in an interactive setting and tune the α parameter for our TERPPE metric on HTER (Snover et al., 2006) traces, and (b) study whether PPE alters in any positive way the work of the human translator performing the residual post-editing, hoping that PE could become a less tedious task by nature. We further anticipate that some additions would improve our approach, including dealing early with out-ofvocabulary phrases, proposing local drop-down options (e.g. (Koehn and Haddow, 2009)), possibly clustered by senses, allowing the user to easily fix reordering issues, and adapting PPE to be discourse-aware (e.g. (Ture et al., 2012)). 5 Acknowledgements The authors would like to thank the anonymous rev</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of AMTA, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<location>Denver, USA.</location>
<contexts>
<context position="6288" citStr="Stolcke, 2002" startWordPosition="998" endWordPosition="999"> The newly extracted phrase tables and LMs4, along with the remainder of the original phrase table and the original LM, are used to re-decode the source text in a first iteration of PPE. A new PPE annotation can then be performed on the new translations. The newly extracted “positive” and “negative” phrase tables are merged with the corresponding phrase table of the previous iteration. The extracted n-gram counts from the current iteration and the counts of the previous iterations are summed, and the LMs are re-trained with the updated counts. A new iteration of PPE is then per2We used SRILM (Stolcke, 2002) to train the LMs with Witten-Bell smoothing. 3Subsequent examples do not use located tokens. 4The extracted LMs are sentence-level, and are only used on their specific sentence during PPE. source un@0 retour@1 au@2 calme@3 pr´ecaire@4 .@5 hypothesis a return to calm is precarious . target ref. return to precarious calm . positive-pt negative-pt source target source target retour@1 au@2 return to pr´ecaire@4 is precarious pr´ecaire@4 precarious calme@3 pr´ecaire@4 calm is precarious .@5 . pr´ecaire@4 .@5 is precarious. positive-lm negative-lm n-gram count n-gram count return 1 a 1 return to 1 </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of ICSLP, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
</authors>
<title>Tijl De Bie, Cyril Goutte, and Nello Cristianini.</title>
<date>2012</date>
<booktitle>Learning to Translate: A Statistical and Computational Analysis. Advances in Artificial Intelligence.</booktitle>
<marker>Turchi, 2012</marker>
<rawString>Marco Turchi, Tijl De Bie, Cyril Goutte, and Nello Cristianini. 2012. Learning to Translate: A Statistical and Computational Analysis. Advances in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Douglas W Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Encouraging Consistent Translation Choices.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Montr´eal, Canada.</location>
<marker>Ture, Oard, Resnik, 2012</marker>
<rawString>Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012. Encouraging Consistent Translation Choices. In Proceedings of NAACL, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Wisniewski</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Assessing Phrase-Based Translation Models with Oracle Decoding.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Cambridge, USA.</location>
<contexts>
<context position="857" citStr="Wisniewski et al., 2010" startWordPosition="116" endWordPosition="119">urelien.max@limsi.fr Abstract We introduce pre-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, confirming the promising prospects of the novel translation scenarios offered by our approach. 1 Introduction As shown by oracle studies (Wisniewski et al., 2010; Turchi et al., 2012; Marie and Max, 2013), Statistical Machine Translation (SMT) systems produce results that are of significantly lower quality than what could be produced from their available resources. As a pragmatic solution, human intervention is commonly used for improving automatic draft translations, in so-called post-editing (PE), but is also studied earlier in the translation process in a variety of interactive strategies, including e.g. completion assistance and local translation choices (e.g. (Foster et al., 2002; Koehn and Haddow, 2009; Gonz´alez-Rubio et al., 2013)). Although i</context>
</contexts>
<marker>Wisniewski, Allauzen, Yvon, 2010</marker>
<rawString>Guillaume Wisniewski, Alexandre Allauzen, and Franc¸ois Yvon. 2010. Assessing Phrase-Based Translation Models with Oracle Decoding. In Proceedings of EMNLP, Cambridge, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008667">
<title confidence="0.989394">
Evaluation of Word Vector Representations by Subspace Alignment
</title>
<author confidence="0.982284">
Yulia Tsvetkov Manaal Faruqui Wang Ling Guillaume Lample Chris Dyer
</author>
<affiliation confidence="0.870665333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.991372">
{ytsvetko, mfaruqui, lingwang, glample, cdyer}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993714" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969875">
Unsupervisedly learned word vectors have
proven to provide exceptionally effective
features in many NLP tasks. Most common
intrinsic evaluations of vector quality mea-
sure correlation with similarity judgments.
However, these often correlate poorly with
how well the learned representations per-
form as features in downstream evaluation
tasks. We present QVEC—a computation-
ally inexpensive intrinsic evaluation mea-
sure of the quality of word embeddings
based on alignment to a matrix of features
extracted from manually crafted lexical
resources—that obtains strong correlation
with performance of the vectors in a battery
of downstream semantic evaluation tasks.1
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896777777778">
A major attraction of vector space word represen-
tations is that they can be derived from large unan-
notated corpora, and they are useful as a source of
features for downstream NLP tasks that are learned
from small amounts of supervision. Unsupervised
word vectors have been shown to benefit parsing
(Lazaridou et al., 2013; Bansal et al., 2014), chunk-
ing (Turian et al., 2010), named entity recognition
(Guo et al., 2014) and sentiment analysis (Socher
et al., 2013), among others.
Despite their ubiquity, there is no standard
scheme for intrinsically evaluating the quality of
word vectors: a vector quality is traditionally
judged by its utility in downstream NLP tasks. This
lack of standardized evaluation is due, in part, to
word vectors’ major criticism: word vectors are
linguistically opaque in a sense that it is still not
clear how to interpret individual vector dimensions,
</bodyText>
<footnote confidence="0.619025333333333">
1The evaluation script and linguistic vectors described in
this paper are available at
https://github.com/ytsvetko/qvec
</footnote>
<bodyText confidence="0.999923138888889">
and, consequently, it is not clear how to score a
non-interpretable representation. Nevertheless, to
facilitate development of better word vector models
and for better error analysis of word vectors, it is
desirable (1) to compare word vector models easily,
without recourse to multiple extrinsic applications
whose implementation and runtime can be costly;
and (2) to understand how features in word vectors
contribute to downstream tasks.
We propose a simple intrinsic evaluation mea-
sure for word vectors. Our measure is based on
component-wise correlations with manually con-
structed “linguistic” word vectors whose compo-
nents have well-defined linguistic properties (§2).
Since vectors are typically used to provide features
to downstream learning problems, our measure
favors recall (rather than precision), which cap-
tures our intuition that meaningless dimensions
in induced vector representations are less harmful
than important dimensions that are missing. We
thus align dimensions in a distributional word vec-
tor model with the linguistic dimension vectors to
maximize the cumulative correlation of the aligned
dimensions (§3). The resulting sum of correla-
tions of the aligned dimensions is our evaluation
score. Since the dimensions in the linguistic vectors
are linguistically-informed, the alignment provides
an “annotation” of components of the word vector
space being evaluated.
To show that our proposed score is meaning-
ful, we compare our intrinsic evaluation model to
the standard (semantic) extrinsic evaluation bench-
marks (§4). For nine off-the-shelf word vector
representation models, our model obtains high cor-
relation (0.34 &lt; r &lt; 0.89) with the extrinsic
tasks (§5).
</bodyText>
<sectionHeader confidence="0.988908" genericHeader="method">
2 Linguistic Dimension Word Vectors
</sectionHeader>
<bodyText confidence="0.999538666666667">
The crux of our evaluation method lies in quanti-
fying the similarity between a distributional word
vector model and a (gold-standard) linguistic re-
</bodyText>
<page confidence="0.959971">
2049
</page>
<note confidence="0.653358">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2049–2054,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.976767911764706">
source capturing human knowledge. To evaluate
the semantic content of word vectors, we exploit an
existing semantic resource—SemCor (Miller et al.,
1993). From the SemCor annotations we construct
a set of linguistic word vectors, details are given in
the rest of this section; table 1 shows an example
of the vectors.
WordNet (Fellbaum, 1998, WN) partitions
nouns and verbs into coarse semantic cate-
gories known as supersenses (Ciaramita and Al-
tun, 2006; Nastase, 2008).2 There are 41
supersense types: 26 for nouns and 15 for
verbs, for example, NOUN.BODY, NOUN.ANIMAL,
VERB.CONSUMPTION, or VERB.MOTION. Sem-
Cor is a WordNet-annotated corpus that captures,
among others, supersense annotations of Word-
Net’s 13,174 noun lemmas and 5,686 verb lemmas
at least once. We construct term frequency vec-
tors normalized to probabilities for all nouns and
verbs that occur in SemCor at least 5 times. The
resulting set of 4,199 linguistic word vectors has
41 interpretable columns.
WORD NN.ANIMAL NN.FOOD · · · VB.MOTION
fish 0.68 0.16 · · · 0.00
duck 0.31 0.00 · · · 0.69
chicken 0.33 0.67 · · · 0.00
resource. We obtain an alignment between the
word vector dimensions and the linguistic dimen-
sions which maximizes the correlation between the
aligned dimensions of the two matrices. This is 1:n
alignment: one distributional dimension is aligned
to at most one linguistic property, whereas one lin-
guistic property can be aligned to n distributional
dimensions; see figure 1.
</bodyText>
<figureCaption confidence="0.8932465">
Figure 1: The filled vertical vectors represent the word vector
in the word vector matrix X and the linguistic property matrix
S. The horizontal hollow vectors represent the “distributional
dimension vector” in X and “linguistic dimension vector” in
S. The arrows show mapping between distributional and
linguistic vector dimensions.
</figureCaption>
<bodyText confidence="0.995502">
Let A E 10, 11DxP be a matrix of alignments
such that aij = 1 iff xi is aligned to sj, otherwise
aij = 0. If r(xi,sj) is the Pearson’s correlation
between vectors xi and sj, then our objective is
defined as:
</bodyText>
<equation confidence="0.891258833333333">
N
N
S
X
D
P
</equation>
<tableCaption confidence="0.970245">
Table 1: Oracle linguistic word vectors, constructed from a QVEC = max L P r(xi,sj) x aij (1)
linguistic resource containing semantic annotations. A|� j aij&lt;1 i=1 j=1
</tableCaption>
<sectionHeader confidence="0.957762" genericHeader="method">
3 Word Vector Evaluation Model
</sectionHeader>
<bodyText confidence="0.997264222222222">
We align dimensions of distributional word vectors
to dimensions (linguistic properties) in the linguis-
tic vectors described in §2 to maximize the cu-
mulative correlation of the aligned dimensions. By
projecting linguistic annotations via the alignments,
we also obtain plausible annotations of dimensions
in the distributional word vectors. In this section,
we formally describe the model, which we call the
QVEC.
Let the number of common words in the vocabu-
lary of the distributional and linguistic word vectors
be N. We define, the distributional vector matrix
X E RDxN with every row as a dimension vector
x E R1xN. D denotes word vector dimensional-
ity. Similarly, S E RPxN is the linguistic prop-
erty matrix with every row as a linguistic property
vector s E R1xN. P denotes linguistic proper-
ties obtained from a manually-annotated linguistic
</bodyText>
<footnote confidence="0.830145333333333">
2Supersenses are known as “lexicographer classes”
in WordNet documentation, http://wordnet.
princeton.edu/man/lexnames.5WN.html
</footnote>
<bodyText confidence="0.999972727272727">
The constraint Ej aij &lt; 1, warrants that one dis-
tributional dimension is aligned to at most one lin-
guistic dimension. The total correlation between
two matrices QVEC is our intrinsic evaluation mea-
sure of a set of word vectors relative to a set of
linguistic properties.
The QVEC’s underlying hypothesis is that dimen-
sions in distributional vectors correspond to linguis-
tic properties of words. It is motivated, among oth-
ers, by the effectiveness of word vectors in linear
models implying that linear combinations of fea-
tures (vector dimensions) produce relevant, salient
content. Via the alignments aij we obtain labels on
dimensions in the distributional word vectors. The
magnitude of the correlation r(xi, sj) corresponds
to the annotation confidence: the higher the corre-
lation, the more salient the linguistic content of the
dimension. Clearly, dimensions in the linguistic
matrix S do not capture every possible linguistic
property, and low correlations often correspond to
the missing information in the linguistic matrix.
Thus, QVEC is a recall-oriented measure: highly-
</bodyText>
<page confidence="0.946111">
2050
</page>
<bodyText confidence="0.99994575">
correlated alignments provide evaluation and anno-
tation of vector dimensions, and missing informa-
tion or noisy dimensions do not significantly affect
the score since the correlations are low.
</bodyText>
<sectionHeader confidence="0.994478" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.997031">
4.1 Word Vector Models
</subsectionHeader>
<bodyText confidence="0.997341157894737">
To test the QVEC, we select a diverse suite of
popular/state-of-the-art word vector models. All
vectors are trained on 1 billion tokens (213,093
types) of English Wikipedia corpus with vector
dimensionality 50, 100, 200, 300, 500, 1000.
CBOW and Skip-Gram (SG). The WORD2VEC
tool (Mikolov et al., 2013) is fast and widely-used.
In the SG model, each word’s Huffman code is
used as an input to a log-linear classifier with a
continuous projection layer and words within a
given context window are predicted. In the CBOW
model a word is predicted given the context words.3
CWindow and Structured Skip-Gram (SSG).
Ling et al. (2015b) propose a syntactic modifica-
tion to the WORD2VEC models that accounts for
word order information, obtaining state-of-the-art
performance in syntactic downstream tasks.4
CBOW with Attention (Attention). Ling et al.
(2015a) further improve the WORD2VEC CBOW
model by employing an attention model which
finds, within the contextual words, the words that
are relevant for each prediction. These vectors have
been shown to benefit both semantically and syn-
tactically oriented tasks.
GloVe. Global vectors for word representations
(Pennington et al., 2014) are trained on aggregated
global word-word co-occurrence statistics from a
corpus, and the resulting representations show in-
teresting linear substructures of the vector space.5
Latent Semantic Analysis (LSA). We construct
word-word co-occurrence matrix X; every element
in the matrix is the pointwise mutual information
between the two words (Church and Hanks, 1990).
Then, truncated singular value decomposition is
applied to factorize X, where we keep the k largest
singular values. Low dimensional word vectors of
dimension k are obtained from Uk where X ≈
UkEVkT (Landauer and Dumais, 1997).
</bodyText>
<footnote confidence="0.995924">
3https://code.google.com/p/word2vec
4https://github.com/wlin12/wang2vec
5http://www-nlp.stanford.edu/projects/
glove/
</footnote>
<bodyText confidence="0.974246">
GloVe+WN, GloVe+PPDB, LSA+WN,
LSA+PPDB. We use retrofitting (Faruqui
et al., 2015) as a post-processing step to enrich
GloVe and LSA vectors with semantic information
from WordNet and Paraphrase database (PPDB)
(Ganitkevitch et al., 2013).6
</bodyText>
<subsectionHeader confidence="0.992262">
4.2 Semantic Evaluation Benchmarks
</subsectionHeader>
<bodyText confidence="0.999951921052632">
We compare the QVEC to six standard extrinsic
semantic tasks for evaluating word vectors; we now
briefly describe the tasks.
Word Similarity. We use three different bench-
marks to measure word similarity. The first one
is the WS-353 dataset (Finkelstein et al., 2001),
which contains 353 pairs of English words that have
been assigned similarity ratings by humans. The
second is the MEN dataset (Bruni et al., 2012) of
3,000 words pairs sampled from words that occur
at least 700 times in a large web corpus. The third
dataset is SimLex-999 (Hill et al., 2014) which has
been constructed to overcome the shortcomings of
WS-353 and contains 999 pairs of adjectives, nouns
and verbs. Word similarity is computed using co-
sine similarity between two words and the perfor-
mance of word vectors is computed by Spearman’s
rank correlation between the rankings produced by
vector model against the human rankings.7
Text Classification. We consider four binary cat-
egorization tasks from the 20 Newsgroups (20NG)
dataset.8 Each task involves categorizing a docu-
ment according to two related categories with train-
ing/dev/test split in accordance with Yogatama and
Smith (2014). For example, a classification task
is between two categories of Sports: baseball vs
hockey. We report the average classification accu-
racy across the four tasks. Our next downstream
semantic task is the sentiment analysis task (Senti)
(Socher et al., 2013) which is a binary classification
task between positive and negative movie reviews
using the standard training/dev/test split and re-
port accuracy on the test set. In both cases, we
use the average of the word vectors of words in
a document (and sentence, respectively) and use
them as features in an `2-regularized logistic regres-
sion classifier. Finally, we evaluate vectors on the
metaphor detection (Metaphor) (Tsvetkov et al.,
</bodyText>
<footnote confidence="0.9999018">
6https://github.com/mfaruqui/
retrofitting
7We employ an implementation of a suite of word similar-
ity tasks at wordvectors.org (Faruqui and Dyer, 2014).
8http://qwone.com/~jason/20Newsgroups
</footnote>
<page confidence="0.996506">
2051
</page>
<bodyText confidence="0.99976025">
2014a).9 The system uses word vectors as features
in a random forest classifier to label adjective-noun
pairs as literal/metaphoric. We report the system
accuracy in 5-fold cross validation.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999706666666667">
To test the efficiency of QVEC in capturing the se-
mantic content of word vectors, we evaluate how
well QVEC’s scores correspond to the scores of
word vector models on semantic benchmarks. We
compute the Pearson’s correlation coefficient r to
quantify the linear relationship between the scor-
ings. We begin with comparison of QVEC with one
extrinsic task—Senti—evaluating 300-dimensional
vectors.
</bodyText>
<table confidence="0.998957230769231">
Model QVEC Senti
CBOW 40.3 90.0
SG 35.9 80.5
CWindow 28.1 76.2
SSG 40.5 81.2
Attention 40.8 80.1
GloVe 34.4 79.4
GloVe+WN 42.1 79.6
GloVe+PPDB 39.2 79.7
LSA 19.7 76.9
LSA+WN 29.4 77.5
LSA+PPDB 28.4 77.3
Correlation (r) 0.87
</table>
<tableCaption confidence="0.9424945">
Table 2: Intrinsic (QVEC) and extrinsic scores of the 300-
dimensional vectors trained using different word vector mod-
els and evaluated on the Senti task. Pearson’s correlation
between the intrinsic and extrinsic scores is r = 0.87.
</tableCaption>
<bodyText confidence="0.999860705882353">
As we show in table 2, the Pearson’s correla-
tion between the intrinsic and extrinsic scores is
r = 0.87. To account for variance in WORD2VEC
representations (due to their random initialization
and negative sampling strategies, the representa-
tions are different for each run of the model), and
to compare QVEC to a larger set of vectors, we now
train three versions of vector sets per model. This
results in 21 word vector sets: three vector sets
per five WORD2VEC models plus GloVe, LSA, and
retrofitting vectors shown in table 2. The Pearson’s
correlation computed on the extended set of com-
parison points (in the same experimental setup as
in table 2) is r = 0.88. In the rest of this section we
report results on the extended suite of word vectors.
We now extend the table 2 results, and show
correlations between the QVEC and extrinsic scores
</bodyText>
<footnote confidence="0.852259">
9https://github.com/ytsvetko/metaphor
</footnote>
<bodyText confidence="0.999023454545454">
across all benchmarks for 300-dimensional vectors.
Table 3 summarizes the results. The QVEC obtains
high positive correlation with all the semantic tasks.
Table 4 shows, for the same 300-dimensional
vectors, that QVEC’s correlation with the down-
stream text classification tasks is on par with or
higher than the correlation between the word sim-
ilarity and text classification tasks. Higher corre-
lating methods—in our experiments, QVEC and
MEN—are better predictors of quality in down-
stream tasks.
</bodyText>
<table confidence="0.9981012">
20NG Metaphor Senti
WS-353 0.55 0.25 0.46
MEN 0.76 0.49 0.55
SimLex 0.56 0.44 0.51
QVEC 0.74 0.75 0.88
</table>
<tableCaption confidence="0.9732905">
Table 4: Pearson’s correlations between word similar-
ity/QVEC scores and the downstream text classification tasks.
</tableCaption>
<figureCaption confidence="0.952856111111111">
Next, we measure correlations of QVEC with
the extrinsic tasks across word vector models with
different dimensionality. The results are shown in
figure 2.
Figure 2: Pearson’s correlation between QVEC scores and the
semantic benchmarks across word vector models on vectors
of different dimensionality. The scores at dimension 300
correspond to the results shown in table 3. The scores in the
legend show average correlation across dimensions.
</figureCaption>
<bodyText confidence="0.999926461538461">
To summarize, we observe high positive correla-
tion between QVEC and the downstream tasks, con-
sistent across the tasks and across different models
with vectors of different dimensionalities.
Since QVEC favors recall over precision, larger
numbers of dimensions will ceteris paribus result
in higher scores—but not necessarily higher corre-
lations with downstream tasks. We therefore im-
pose the restriction that QVEC only be used to com-
pare vectors of the same size, but we now show
that its correlation with downstream tasks is sta-
ble, conditional on the size of the vectors being
compared. We aggregate rankings by individual
</bodyText>
<page confidence="0.991254">
2052
</page>
<table confidence="0.8890535">
WS-353 MEN SimLex
r 0.34 0.63 0.68
20NG Metaphor Senti
0.74 0.75 0.88
</table>
<tableCaption confidence="0.982985">
Table 3: Pearson’s correlations between QVEC scores of the 300-dimensional vectors trained using different word vector models
and the scores of the downstream tasks on the same vectors.
</tableCaption>
<table confidence="0.999962333333333">
50 100 200 300 500 1000
p(QVEC, Senti) 0.32 0.57 0.73 0.78 0.72 0.60
p(QVEC, All) 0.66 0.59 0.63 0.65 0.62 0.59
</table>
<tableCaption confidence="0.998604666666667">
Table 5: Spearman’s rank-order correlation between the QVEC ranking of the word vector models and the ranking produced by
(1) the Senti task, or (2) the aggregated ranking of all tasks (All). We rank separately models of vectors of different dimensionality
(table columns).
</tableCaption>
<bodyText confidence="0.999195666666667">
downstream tasks into a global ranking using the
Kemeny–Young rank aggregation algorithm, for
each dimension separately (Kemeny, 1959). The al-
gorithm finds a ranking which minimizes pairwise
disagreement of individual rankers. Table 5 shows
Spearman’s rank correlation between the rankings
produced by the QVEC and the Senti task/the ag-
gregated ranking. For example, ranking of 300-
dimensional models produced by Senti is {SSG,
CBOW, SG, Attention, GloVe+PPDB, GloVe+WN,
GloVe, LSA+WN, LSA+PPDB, LSA, CWindow},
and the QVEC’s ranking is {GloVe+WN, Attention,
SSG, CBOW, GloVe+PPDB, SG, GloVe, LSA+WN,
LSA+PPDB, CWindow, LSA}. The Spearman’s
p between the two rankings is 0.78. We note,
however, that there is a considerable variation be-
tween rankings across all models and across all
dimensions, for example the SimLex ranking pro-
duced for the same 300-dimensional vectors is
{GloVe+PPDB, GloVe+WN, SG, LSA+PPDB, SSG,
CBOW, Attention, CWindow, LSA+WN, GloVe,
LSA}, and p(Senti, SimLex) = 0.46. In a recent
related study, Schnabel et al. (2015) also observe
that existing word similarity and text categoriza-
tion evaluations yield different orderings of word
vector models. This task-specifity of rankings em-
phasizes the deficiency of evaluating word vector
models solely on downstream tasks, and the need
of a standardized intrinsic evaluation approach that
quantifies linguistic content of word vectors.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999871928571429">
Aligning dimensions of linguistic and distributional
vectors enables projection of linguistic annotations
via the alignments, and thereby facilitates quali-
tative analysis of individual dimensions in distri-
butional vectors. Albeit noisy, we find correspon-
dence between the projected labels of distributional
columns and the column content. For example, in
the 50-dimensional SG model top-10 ranked words
in a dimension aligned to NOUN.BODY with r=0.26
are amputated, sprained, palsy, semenya, lacera-
tions, genital, cervical, concussion, congenital, ab-
dominal. This interesting by-product of our method
will be addressed in future work.
While we experiment with linguistic vectors
capturing semantic concepts, our methodology is
generally applicable to other linguistic resources
(Faruqui and Dyer, 2015). For example, part-
of-speech annotations extracted from a treebank
would yield linguistic vectors capturing syntactic
content of vectors. Thus, QVEC can be used as a
task-specific evaluator; we will investigate this in
future work.
A useful property of supersenses (features in
our linguistic vectors) is that they are stable across
languages (Schneider et al., 2013; Tsvetkov et al.,
2014b). Cross-lingual vector evaluation and eval-
uation of multilingual word vectors with QVEC is
thus an additional promising research avenue.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999965">
We propose a method for intrinsic evalua-
tion of word vectors which shows strong
relationship—both linear and monotonic—with the
scores/rankings produced by the downstream tasks.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998706">
We are grateful to the anonymous reviewers for
constructive feedback. This work was supported by
the U.S. Army Research Laboratory and the U.S.
Army Research Office under contract/grant number
W911NF-10-1-0533.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989499333333333">
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proc. of ACL.
</reference>
<page confidence="0.978919">
2053
</page>
<note confidence="0.920534142857143">
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proc. of ACL.
Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015b. Two/too simple adaptations of
word2vec for syntax problems. In Proc. of
NAACL.
</note>
<reference confidence="0.999023418604651">
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22–29.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594–602.
Manaal Faruqui and Chris Dyer. 2014. Community
evaluation and exchange of word vectors at wordvec-
tors.org. In Proc. of ACL (Demonstrations).
Manaal Faruqui and Chris Dyer. 2015. Non-
distributional word vector representations. In Proc.
ACL.
Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Noah A. Smith, and Eduard Hovy. 2015.
Retrofitting word vectors to semantic lexicons. In
Proc. of NAACL.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In Proc. of WWW.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proc. of NAACL.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proc. of EMNLP.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
SimLex-999: Evaluating semantic models with (gen-
uine) similarity estimation. CoRR, abs/1408.3456.
John G. Kemeny. 1959. Mathematics without numbers.
88(4):577–591.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to Plato’s problem: The latent semantic an-
alysis theory of acquisition, induction, and represen-
tation of knowledge. Psychological review.
Angeliki Lazaridou, Eva Maria Vecchi, and Marco
Baroni. 2013. Fish transporters and miracle
homes: How compositional distributional semantics
can help NP parsing. In Proc. of EMNLP.
Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, Silvio
Amir, Ramon Fermandez, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015a. Not all contexts
are created equal: Better word representations with
variable attention. In Proc. of EMNLP.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of HLT, pages 303–308.
Vivi Nastase. 2008. Unsupervised all-words word
sense disambiguation with grammatical dependen-
cies. In Proc. of IJCNLP, pages 7–12.
Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. GloVe: Global vectors for word
representation. In Proc. of EMNLP.
Tobias Schnabel, Igor Labutov, David Mimno, and
Thorsten Joachims. 2015. Evaluation methods
for unsupervised word embeddings. In Proc. of
EMNLP.
Nathan Schneider, Behrang Mohit, Chris Dyer, Kemal
Oflazer, and Noah A. Smith. 2013. Supersense tag-
ging for Arabic: the MT-in-the-middle attack. In
Proc. NAACL-HLT, pages 661–667.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. of EMNLP.
Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman,
Eric Nyberg, and Chris Dyer. 2014a. Metaphor de-
tection with cross-lingual model transfer. In Proc.
ACL, pages 248–258.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014b.
Augmenting English adjective senses with super-
senses. In Proc. LREC.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL.
Dani Yogatama and Noah A Smith. 2014. Linguistic
structured sparsity in text categorization. In Proc. of
ACL.
</reference>
<page confidence="0.99551">
2054
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.531909">
<title confidence="0.843001333333333">Evaluation of Word Vector Representations by Subspace Alignment Yulia Tsvetkov Manaal Faruqui Wang Ling Guillaume Lample Chris Language Technologies</title>
<affiliation confidence="0.971199">Carnegie Mellon</affiliation>
<address confidence="0.999308">Pittsburgh, PA, 15213,</address>
<email confidence="0.998399">ytsvetko@cs.cmu.edu</email>
<email confidence="0.998399">mfaruqui@cs.cmu.edu</email>
<email confidence="0.998399">lingwang@cs.cmu.edu</email>
<email confidence="0.998399">glample@cs.cmu.edu</email>
<email confidence="0.998399">cdyer@cs.cmu.edu</email>
<abstract confidence="0.999761">Unsupervisedly learned word vectors have proven to provide exceptionally effective features in many NLP tasks. Most common intrinsic evaluations of vector quality measure correlation with similarity judgments. However, these often correlate poorly with how well the learned representations perform as features in downstream evaluation We present computationally inexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery</abstract>
<intro confidence="0.866718">downstream semantic evaluation</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1306" citStr="Bansal et al., 2014" startWordPosition="183" endWordPosition="186">luation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1The evaluation script and linguistic vectors described in </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="10096" citStr="Church and Hanks, 1990" startWordPosition="1535" endWordPosition="1538">which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the vector space.5 Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X, where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from Uk where X ≈ UkEVkT (Landauer and Dumais, 1997). 3https://code.google.com/p/word2vec 4https://github.com/wlin12/wang2vec 5http://www-nlp.stanford.edu/projects/ glove/ GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 4.2 Semanti</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>594--602</pages>
<contexts>
<context position="4491" citStr="Ciaramita and Altun, 2006" startWordPosition="654" endWordPosition="658"> Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN.BODY, NOUN.ANIMAL, VERB.CONSUMPTION, or VERB.MOTION. SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD NN.ANIMAL NN.FOOD · · · VB.MOTION fish 0.68 0.16 · · · 0.00 duck 0.31 0.00 · </context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proc. of EMNLP, pages 594–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Community evaluation and exchange of word vectors at wordvectors.org.</title>
<date>2014</date>
<booktitle>In Proc. of ACL (Demonstrations).</booktitle>
<contexts>
<context position="12722" citStr="Faruqui and Dyer, 2014" startWordPosition="1929" endWordPosition="1932">alysis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2-regularized logistic regression classifier. Finally, we evaluate vectors on the metaphor detection (Metaphor) (Tsvetkov et al., 6https://github.com/mfaruqui/ retrofitting 7We employ an implementation of a suite of word similarity tasks at wordvectors.org (Faruqui and Dyer, 2014). 8http://qwone.com/~jason/20Newsgroups 2051 2014a).9 The system uses word vectors as features in a random forest classifier to label adjective-noun pairs as literal/metaphoric. We report the system accuracy in 5-fold cross validation. 5 Results To test the efficiency of QVEC in capturing the semantic content of word vectors, we evaluate how well QVEC’s scores correspond to the scores of word vector models on semantic benchmarks. We compute the Pearson’s correlation coefficient r to quantify the linear relationship between the scorings. We begin with comparison of QVEC with one extrinsic task—</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Community evaluation and exchange of word vectors at wordvectors.org. In Proc. of ACL (Demonstrations).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Nondistributional word vector representations.</title>
<date>2015</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="19355" citStr="Faruqui and Dyer, 2015" startWordPosition="2947" endWordPosition="2950">idual dimensions in distributional vectors. Albeit noisy, we find correspondence between the projected labels of distributional columns and the column content. For example, in the 50-dimensional SG model top-10 ranked words in a dimension aligned to NOUN.BODY with r=0.26 are amputated, sprained, palsy, semenya, lacerations, genital, cervical, concussion, congenital, abdominal. This interesting by-product of our method will be addressed in future work. While we experiment with linguistic vectors capturing semantic concepts, our methodology is generally applicable to other linguistic resources (Faruqui and Dyer, 2015). For example, partof-speech annotations extracted from a treebank would yield linguistic vectors capturing syntactic content of vectors. Thus, QVEC can be used as a task-specific evaluator; we will investigate this in future work. A useful property of supersenses (features in our linguistic vectors) is that they are stable across languages (Schneider et al., 2013; Tsvetkov et al., 2014b). Cross-lingual vector evaluation and evaluation of multilingual word vectors with QVEC is thus an additional promising research avenue. 7 Conclusion We propose a method for intrinsic evaluation of word vector</context>
</contexts>
<marker>Faruqui, Dyer, 2015</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2015. Nondistributional word vector representations. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Jesse Dodge</author>
<author>Sujay Kumar Jauhar</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
<author>Eduard Hovy</author>
</authors>
<title>Retrofitting word vectors to semantic lexicons.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="10526" citStr="Faruqui et al., 2015" startWordPosition="1587" endWordPosition="1590">5 Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X, where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from Uk where X ≈ UkEVkT (Landauer and Dumais, 1997). 3https://code.google.com/p/word2vec 4https://github.com/wlin12/wang2vec 5http://www-nlp.stanford.edu/projects/ glove/ GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 4.2 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks. Word Similarity. We use three different benchmarks to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the MEN dataset (Bruni et al</context>
</contexts>
<marker>Faruqui, Dodge, Jauhar, Dyer, Smith, Hovy, 2015</marker>
<rawString>Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Noah A. Smith, and Eduard Hovy. 2015. Retrofitting word vectors to semantic lexicons. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="10986" citStr="Finkelstein et al., 2001" startWordPosition="1657" endWordPosition="1660"> 4https://github.com/wlin12/wang2vec 5http://www-nlp.stanford.edu/projects/ glove/ GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 4.2 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks. Word Similarity. We use three different benchmarks to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the MEN dataset (Bruni et al., 2012) of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus. The third dataset is SimLex-999 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vec</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: the concept revisited. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL.</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting embedding features for simple semi-supervised learning.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1383" citStr="Guo et al., 2014" startWordPosition="196" endWordPosition="199"> of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequentl</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi-supervised learning. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>SimLex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="11279" citStr="Hill et al., 2014" startWordPosition="1709" endWordPosition="1712">kevitch et al., 2013).6 4.2 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks. Word Similarity. We use three different benchmarks to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001), which contains 353 pairs of English words that have been assigned similarity ratings by humans. The second is the MEN dataset (Bruni et al., 2012) of 3,000 words pairs sampled from words that occur at least 700 times in a large web corpus. The third dataset is SimLex-999 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Each task involves categorizing a document according to two related categories with training/dev/test split in accordance with Yogatama and Smith (2</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. SimLex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John G Kemeny</author>
</authors>
<date>1959</date>
<note>Mathematics without numbers. 88(4):577–591.</note>
<contexts>
<context position="17268" citStr="Kemeny, 1959" startWordPosition="2648" endWordPosition="2649">ing different word vector models and the scores of the downstream tasks on the same vectors. 50 100 200 300 500 1000 p(QVEC, Senti) 0.32 0.57 0.73 0.78 0.72 0.60 p(QVEC, All) 0.66 0.59 0.63 0.65 0.62 0.59 Table 5: Spearman’s rank-order correlation between the QVEC ranking of the word vector models and the ranking produced by (1) the Senti task, or (2) the aggregated ranking of all tasks (All). We rank separately models of vectors of different dimensionality (table columns). downstream tasks into a global ranking using the Kemeny–Young rank aggregation algorithm, for each dimension separately (Kemeny, 1959). The algorithm finds a ranking which minimizes pairwise disagreement of individual rankers. Table 5 shows Spearman’s rank correlation between the rankings produced by the QVEC and the Senti task/the aggregated ranking. For example, ranking of 300- dimensional models produced by Senti is {SSG, CBOW, SG, Attention, GloVe+PPDB, GloVe+WN, GloVe, LSA+WN, LSA+PPDB, LSA, CWindow}, and the QVEC’s ranking is {GloVe+WN, Attention, SSG, CBOW, GloVe+PPDB, SG, GloVe, LSA+WN, LSA+PPDB, CWindow, LSA}. The Spearman’s p between the two rankings is 0.78. We note, however, that there is a considerable variation</context>
</contexts>
<marker>Kemeny, 1959</marker>
<rawString>John G. Kemeny. 1959. Mathematics without numbers. 88(4):577–591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<note>Psychological review.</note>
<contexts>
<context position="10324" citStr="Landauer and Dumais, 1997" startWordPosition="1572" endWordPosition="1575">tions (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the vector space.5 Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X, where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from Uk where X ≈ UkEVkT (Landauer and Dumais, 1997). 3https://code.google.com/p/word2vec 4https://github.com/wlin12/wang2vec 5http://www-nlp.stanford.edu/projects/ glove/ GloVe+WN, GloVe+PPDB, LSA+WN, LSA+PPDB. We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6 4.2 Semantic Evaluation Benchmarks We compare the QVEC to six standard extrinsic semantic tasks for evaluating word vectors; we now briefly describe the tasks. Word Similarity. We use three different benchmarks to measure word similarity. </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Eva Maria Vecchi</author>
<author>Marco Baroni</author>
</authors>
<title>Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1284" citStr="Lazaridou et al., 2013" startWordPosition="179" endWordPosition="182">nexpensive intrinsic evaluation measure of the quality of word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1The evaluation script and linguistic</context>
</contexts>
<marker>Lazaridou, Vecchi, Baroni, 2013</marker>
<rawString>Angeliki Lazaridou, Eva Maria Vecchi, and Marco Baroni. 2013. Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Lin Chu-Cheng</author>
<author>Yulia Tsvetkov</author>
<author>Silvio Amir</author>
<author>Ramon Fermandez</author>
<author>Chris Dyer</author>
<author>Alan W Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Not all contexts are created equal: Better word representations with variable attention.</title>
<date>2015</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9176" citStr="Ling et al. (2015" startWordPosition="1404" endWordPosition="1407"> QVEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD2VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD2VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD2VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-</context>
</contexts>
<marker>Ling, Chu-Cheng, Tsvetkov, Amir, Fermandez, Dyer, Black, Trancoso, 2015</marker>
<rawString>Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, Silvio Amir, Ramon Fermandez, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015a. Not all contexts are created equal: Better word representations with variable attention. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proc. of ICLR.</booktitle>
<contexts>
<context position="8850" citStr="Mikolov et al., 2013" startWordPosition="1348" endWordPosition="1351">tion in the linguistic matrix. Thus, QVEC is a recall-oriented measure: highly2050 correlated alignments provide evaluation and annotation of vector dimensions, and missing information or noisy dimensions do not significantly affect the score since the correlations are low. 4 Experimental Setup 4.1 Word Vector Models To test the QVEC, we select a diverse suite of popular/state-of-the-art word vector models. All vectors are trained on 1 billion tokens (213,093 types) of English Wikipedia corpus with vector dimensionality 50, 100, 200, 300, 500, 1000. CBOW and Skip-Gram (SG). The WORD2VEC tool (Mikolov et al., 2013) is fast and widely-used. In the SG model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. In the CBOW model a word is predicted given the context words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD2VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD2VEC CBOW model by employ</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proc. of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proc. of HLT,</booktitle>
<pages>303--308</pages>
<contexts>
<context position="4191" citStr="Miller et al., 1993" startWordPosition="605" endWordPosition="608">our model obtains high correlation (0.34 &lt; r &lt; 0.89) with the extrinsic tasks (§5). 2 Linguistic Dimension Word Vectors The crux of our evaluation method lies in quantifying the similarity between a distributional word vector model and a (gold-standard) linguistic re2049 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2049–2054, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN.BODY, NOUN.ANIMAL, VERB.CONSUMPTION, or VERB.MOTION. SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at le</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proc. of HLT, pages 303–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
</authors>
<title>Unsupervised all-words word sense disambiguation with grammatical dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of IJCNLP,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="4507" citStr="Nastase, 2008" startWordPosition="659" endWordPosition="660">thods in Natural Language Processing, pages 2049–2054, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. source capturing human knowledge. To evaluate the semantic content of word vectors, we exploit an existing semantic resource—SemCor (Miller et al., 1993). From the SemCor annotations we construct a set of linguistic word vectors, details are given in the rest of this section; table 1 shows an example of the vectors. WordNet (Fellbaum, 1998, WN) partitions nouns and verbs into coarse semantic categories known as supersenses (Ciaramita and Altun, 2006; Nastase, 2008).2 There are 41 supersense types: 26 for nouns and 15 for verbs, for example, NOUN.BODY, NOUN.ANIMAL, VERB.CONSUMPTION, or VERB.MOTION. SemCor is a WordNet-annotated corpus that captures, among others, supersense annotations of WordNet’s 13,174 noun lemmas and 5,686 verb lemmas at least once. We construct term frequency vectors normalized to probabilities for all nouns and verbs that occur in SemCor at least 5 times. The resulting set of 4,199 linguistic word vectors has 41 interpretable columns. WORD NN.ANIMAL NN.FOOD · · · VB.MOTION fish 0.68 0.16 · · · 0.00 duck 0.31 0.00 · · · 0.69 chicken</context>
</contexts>
<marker>Nastase, 2008</marker>
<rawString>Vivi Nastase. 2008. Unsupervised all-words word sense disambiguation with grammatical dependencies. In Proc. of IJCNLP, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9729" citStr="Pennington et al., 2014" startWordPosition="1483" endWordPosition="1486">ntext words.3 CWindow and Structured Skip-Gram (SSG). Ling et al. (2015b) propose a syntactic modification to the WORD2VEC models that accounts for word order information, obtaining state-of-the-art performance in syntactic downstream tasks.4 CBOW with Attention (Attention). Ling et al. (2015a) further improve the WORD2VEC CBOW model by employing an attention model which finds, within the contextual words, the words that are relevant for each prediction. These vectors have been shown to benefit both semantically and syntactically oriented tasks. GloVe. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the vector space.5 Latent Semantic Analysis (LSA). We construct word-word co-occurrence matrix X; every element in the matrix is the pointwise mutual information between the two words (Church and Hanks, 1990). Then, truncated singular value decomposition is applied to factorize X, where we keep the k largest singular values. Low dimensional word vectors of dimension k are obtained from Uk where X ≈ UkEVkT (Landauer and Dumais, 1997). 3ht</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Schnabel</author>
<author>Igor Labutov</author>
<author>David Mimno</author>
<author>Thorsten Joachims</author>
</authors>
<title>Evaluation methods for unsupervised word embeddings.</title>
<date>2015</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="18178" citStr="Schnabel et al. (2015)" startWordPosition="2783" endWordPosition="2786"> {SSG, CBOW, SG, Attention, GloVe+PPDB, GloVe+WN, GloVe, LSA+WN, LSA+PPDB, LSA, CWindow}, and the QVEC’s ranking is {GloVe+WN, Attention, SSG, CBOW, GloVe+PPDB, SG, GloVe, LSA+WN, LSA+PPDB, CWindow, LSA}. The Spearman’s p between the two rankings is 0.78. We note, however, that there is a considerable variation between rankings across all models and across all dimensions, for example the SimLex ranking produced for the same 300-dimensional vectors is {GloVe+PPDB, GloVe+WN, SG, LSA+PPDB, SSG, CBOW, Attention, CWindow, LSA+WN, GloVe, LSA}, and p(Senti, SimLex) = 0.46. In a recent related study, Schnabel et al. (2015) also observe that existing word similarity and text categorization evaluations yield different orderings of word vector models. This task-specifity of rankings emphasizes the deficiency of evaluating word vector models solely on downstream tasks, and the need of a standardized intrinsic evaluation approach that quantifies linguistic content of word vectors. 6 Future Work Aligning dimensions of linguistic and distributional vectors enables projection of linguistic annotations via the alignments, and thereby facilitates qualitative analysis of individual dimensions in distributional vectors. Al</context>
</contexts>
<marker>Schnabel, Labutov, Mimno, Joachims, 2015</marker>
<rawString>Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. 2015. Evaluation methods for unsupervised word embeddings. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Behrang Mohit</author>
<author>Chris Dyer</author>
<author>Kemal Oflazer</author>
<author>Noah A Smith</author>
</authors>
<title>Supersense tagging for Arabic: the MT-in-the-middle attack.</title>
<date>2013</date>
<booktitle>In Proc. NAACL-HLT,</booktitle>
<pages>661--667</pages>
<marker>Schneider, Mohit, Dyer, Oflazer, Smith, 2013</marker>
<rawString>Nathan Schneider, Behrang Mohit, Chris Dyer, Kemal Oflazer, and Noah A. Smith. 2013. Supersense tagging for Arabic: the MT-in-the-middle attack. In Proc. NAACL-HLT, pages 661–667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1428" citStr="Socher et al., 2013" startWordPosition="203" endWordPosition="206">ed lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1The evaluation script and linguistic vectors described in this paper are available at https://github.com/ytsvetko/qvec and, consequently, it is not clear how to score a non-interpr</context>
<context position="12140" citStr="Socher et al., 2013" startWordPosition="1841" endWordPosition="1844">y Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Each task involves categorizing a document according to two related categories with training/dev/test split in accordance with Yogatama and Smith (2014). For example, a classification task is between two categories of Sports: baseball vs hockey. We report the average classification accuracy across the four tasks. Our next downstream semantic task is the sentiment analysis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2-regularized logistic regression classifier. Finally, we evaluate vectors on the metaphor detection (Metaphor) (Tsvetkov et al., 6https://github.com/mfaruqui/ retrofitting 7We employ an implementation of a suite of word similarity tasks at wordvectors.org (Faruqui and Dyer, 2014). 8http://qwone.co</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Leonid Boytsov</author>
<author>Anatole Gershman</author>
<author>Eric Nyberg</author>
<author>Chris Dyer</author>
</authors>
<title>Metaphor detection with cross-lingual model transfer. In</title>
<date>2014</date>
<booktitle>Proc. ACL,</booktitle>
<pages>248--258</pages>
<marker>Tsvetkov, Boytsov, Gershman, Nyberg, Dyer, 2014</marker>
<rawString>Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg, and Chris Dyer. 2014a. Metaphor detection with cross-lingual model transfer. In Proc. ACL, pages 248–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Nathan Schneider</author>
<author>Dirk Hovy</author>
<author>Archna Bhatia</author>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Augmenting English adjective senses with supersenses. In</title>
<date>2014</date>
<booktitle>Proc. LREC.</booktitle>
<marker>Tsvetkov, Schneider, Hovy, Bhatia, Faruqui, Dyer, 2014</marker>
<rawString>Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna Bhatia, Manaal Faruqui, and Chris Dyer. 2014b. Augmenting English adjective senses with supersenses. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1338" citStr="Turian et al., 2010" startWordPosition="189" endWordPosition="192">f word embeddings based on alignment to a matrix of features extracted from manually crafted lexical resources—that obtains strong correlation with performance of the vectors in a battery of downstream semantic evaluation tasks.1 1 Introduction A major attraction of vector space word representations is that they can be derived from large unannotated corpora, and they are useful as a source of features for downstream NLP tasks that are learned from small amounts of supervision. Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others. Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks. This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, 1The evaluation script and linguistic vectors described in this paper are available at http</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Noah A Smith</author>
</authors>
<title>Linguistic structured sparsity in text categorization.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="11883" citStr="Yogatama and Smith (2014)" startWordPosition="1801" endWordPosition="1804">9 (Hill et al., 2014) which has been constructed to overcome the shortcomings of WS-353 and contains 999 pairs of adjectives, nouns and verbs. Word similarity is computed using cosine similarity between two words and the performance of word vectors is computed by Spearman’s rank correlation between the rankings produced by vector model against the human rankings.7 Text Classification. We consider four binary categorization tasks from the 20 Newsgroups (20NG) dataset.8 Each task involves categorizing a document according to two related categories with training/dev/test split in accordance with Yogatama and Smith (2014). For example, a classification task is between two categories of Sports: baseball vs hockey. We report the average classification accuracy across the four tasks. Our next downstream semantic task is the sentiment analysis task (Senti) (Socher et al., 2013) which is a binary classification task between positive and negative movie reviews using the standard training/dev/test split and report accuracy on the test set. In both cases, we use the average of the word vectors of words in a document (and sentence, respectively) and use them as features in an `2-regularized logistic regression classifi</context>
</contexts>
<marker>Yogatama, Smith, 2014</marker>
<rawString>Dani Yogatama and Noah A Smith. 2014. Linguistic structured sparsity in text categorization. In Proc. of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
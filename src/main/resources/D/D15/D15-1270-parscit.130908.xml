<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015500">
<title confidence="0.997516">
Discourse Element Identification in Student Essays
based on Global and Local Cohesion
</title>
<author confidence="0.997609">
Wei Songt, Ruiji Fut, Lizhen Liut, Ting Liu§
</author>
<affiliation confidence="0.995553">
tInformation Engineering, Capital Normal University, Bejing 100048, China
tIflytek Research Beijing, Beijing 100083, China
§Harbin Institute of Technology, Harbin 150001, China
</affiliation>
<email confidence="0.993081">
{wsong, lzliu}@cnu.edu.cn, rjfu@iflytek.com, tliu@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.993785" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980230769231">
We present a method of using cohesion to
improve discourse element identification
for sentences in student essays. New fea-
tures for each sentence are derived by con-
sidering its relations to global and local
cohesion, which are created by means of
cohesive resources and subtopic coverage.
In our experiments, we obtain significant
improvements on identifying all discourse
elements, especially of +5% F1 score on
thesis and main idea. The analysis shows
that global cohesion can better capture the-
sis statements.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999596083333333">
Automatic discourse analysis of student essays
can benefit many downstream applications such
as essay rating, text organization assessment and
writing instruction. In this paper we focus on iden-
tifying discourse elements for sentences in persua-
sive essays written by Chinese high school stu-
dents. Discourse elements represent the contribu-
tions that sentences can make to text organization.
Typical discourse elements and their functions in
persuasive writing are summarized in Table 1.
Previous work mainly exploits the properties of
a sentence itself or adjacent sentences for this task.
In this work, we explore cohesion to express re-
lations among sentences through the whole text.
Cohesion can be defined as a set of resources link-
ing within a text that organize the text together
(Halliday and Hasan, 1976). It can be achieved
through the use of reference, ellipsis, substitution,
conjunction and lexical cohesion. Among them,
lexical cohesion has been widely used for mod-
eling local coherence and applied to related ap-
plications (Barzilay and Elhadad, 1999; Barzilay
and Lapata, 2008; Galley et al., 2003; Hsueh et al.,
2006; Filippova and Strube, 2006). Since cohesion
</bodyText>
<subsectionHeader confidence="0.818946">
Element Definition
</subsectionHeader>
<table confidence="0.900953066666667">
Introduction (I) introduces the background and/or
grabs readers’ attention
Prompt (P) restates or summarize the prompt
states the author’s main claim on the
Thesis (T)
issue for which he/she is arguing
Main idea (M) asserts foundational ideas or aspects
that are related to the thesis
Supporting provides evidence to explain or
idea (S) support the thesis and main ideas
Conclusion (C) concludes the whole essay or one of
the main ideas
doesn’t fit into the above elements or
Other (O)
makes no meaningful contribution
</table>
<tableCaption confidence="0.996075">
Table 1: Definitions of discourse elements.
</tableCaption>
<bodyText confidence="0.999964730769231">
is closely related to the structure of text (Morris
and Hirst, 1991), it motivates us to explore similar
techniques for discourse element identification. In
addition, its ease of implementation is also attrac-
tive. Other options for representing text structure
such as full-text discourse parsers (Marcu, 2000)
may be not available or don’t have satisfied per-
formance, especially for non-English languages.
However, modeling local coherence alone is not
adequate to distinguish discourse elements in per-
suasive essays. For example, a main idea may be
followed by a supporting idea sentence. The two
sentences can be coherent but their discourse ele-
ments are different. To deal with this, global cohe-
sion should be exploited. Considering that in per-
suasive writing, thesis, main ideas and conclusion,
which are termed thesis statements by Burstein
et al. (2001), are expected to relate to each other
(Higgins et al., 2004). It is likely that cohesive re-
lations exist among them through the whole text.
We make a focused contribution by investigat-
ing global and local cohesive relations. We create
sentence chains based on cohesive resources and
examine whether the chains represent global co-
hesion or local cohesion. Our hypothesis is that
global cohesion can better capture thesis state-
</bodyText>
<page confidence="0.884384">
2255
</page>
<note confidence="0.8668275">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2255–2261,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<table confidence="0.901441333333333">
Corpus #Essays Avg.#paras Avg.#sents Element distributions Kappa
I P T M S C O
C1 367 7.4 22.7 0.077 0.080 0.087 0.135 0.514 0.095 0.008 0.94
C2 346 7.8 22.5 0.070 0.027 0.069 0.181 0.530 0.114 0.006 0.93
C3 197 9.1 27.7 0.082 − 0.045 0.187 0.571 0.106 0.007 0.91
Avg. 303 7.9 23.7 0.077 0.053 0.067 0.169 0.538 0.105 0.007 0.93
</table>
<tableCaption confidence="0.999478">
Table 2: Basic statistics of the annotated corpora. Para and sent are short for paragraph and sentence.
</tableCaption>
<bodyText confidence="0.999798555555556">
ments and help distinguishing them from over-
whelming supporting idea sentences. Experi-
ments were conducted on essays written by Chi-
nese high school students in the mother tongue.
The results confirm our hypothesis. Our method
achieves significant improvements of +%5 F1
score on thesis and main idea sentences by adding
cohesion features. The features related to global
cohesion are most discriminative.
</bodyText>
<sectionHeader confidence="0.978774" genericHeader="method">
2 Data Annotation
</sectionHeader>
<bodyText confidence="0.999936914893617">
We mainly use the discourse elements defined by
Burstein et al. (2003b) except for adding a prompt
element. The discourse element definitions are
listed in Table 1. We asked two labelers from the
college of liberal art of a university to conduct data
annotation. Provided a detail manual with element
definitions, explanations and examples, the label-
ers assigned a discourse element to each sentence.
We collected three corpora, two of which (C1
and C2) are prompt-directed and one (C3) is
prompt-free. All essays were written by Chinese
high school students in Chinese. The prompt-
directed essays are samples of essays written by
senior high school students when they were tak-
ing a mock examination. The students were re-
quired to write a pervasive essay related to a given
prompt. The prompts of corpora C1 and C2 are
different. The prompt-free essays in C3 were
crawled from an online writing assistance website,
where the essays were used as writing examples
of persuasive essays written by high school stu-
dents. The average essay lengths on three corpora
are 795, 772 and 864 Chinese characters respec-
tively. The other basic statistics of the annotated
corpora are listed in Table 2.
During annotation, the labelers found cases of
difficulties about ambiguous elements. For exam-
ple, content about the prompt and the main thesis
can be mentioned in the same sentence. In such
cases, thesis statements have priority over other el-
ements to be labeled, since identifying thesis state-
ments is more important for some potential appli-
cations (Burstein et al., 2001).
From each corpus, 100 essays were labeled by
both annotators for computing agreements, and
the others were labeled independently. The label
agreements measured with Kappa (Cohen et al.,
1960) are high as shown in Table 2. The disagree-
ments were resolved by discussion. The distri-
butions of discourse elements are also shown in
Table 2. We can see that they are imbalanced.
The supporting idea sentences account for more
than 53%, while the thesis statements account
for only 34% in total. As a result, the distinc-
tion between minority thesis statements from over-
whelming supporting idea sentences is a major
challenge.
</bodyText>
<sectionHeader confidence="0.992941" genericHeader="method">
3 Discourse Element Identification
</sectionHeader>
<bodyText confidence="0.999983">
Identifying discourse elements in student essays
can be seen as a functional segmentation of dis-
course (Webber et al., 2011). In this work, we fo-
cus on utilizing supervised feature-based machine
learning models for this task.
</bodyText>
<subsectionHeader confidence="0.999479">
3.1 Learning Models
</subsectionHeader>
<bodyText confidence="0.999977733333333">
Discourse element identification can be casted
as a classification problem that sentences are
classified independently using a classifier, e.g.
naive Bayes (Burstein et al., 2001), decision tree
(Burstein et al., 2003b) and Support Vector Ma-
chines (SVMs) (Stab and Gurevych, 2014). It can
also be solved in a sequence labeling framework,
which models the whole sentence sequence and
captures the correlations among predictions. For
example, Conditional Random Fields (CRFs) have
be studied for similar task on argumentative zon-
ing of scientific documents (Guo et al., 2011).
We will evaluate different types of features us-
ing two representative models respectively: the
SVM model and the linear-chain CRF model.
</bodyText>
<page confidence="0.925484">
2256
</page>
<subsectionHeader confidence="0.99932">
3.2 Basic Features
</subsectionHeader>
<bodyText confidence="0.999987428571429">
Before feature extraction, sentence splitting, word
segmentation,POS and NE tagging are done us-
ing a Chinese language processing toolkit (Che
et al., 2010). Most basic features are adapted from
previous work (Burstein et al., 2003a; Stab and
Gurevych, 2014; Persing et al., 2010). For each
sentence, the following feature sets are extracted.
Position features The relative position of its para-
graph (first, last or body) in the essay and its rel-
ative position (first, last or body) in the paragraph
are modeled as a set of binary features. The index
of the sentence is also used as a feature.
Indicator features Cue words/phrases like “ ik
)j(in my opinion)” and “,0&apos; Z.(in conclusion)” are
used as indicators. Partial indicators are adapted
from the ones used by Persing et al. (2010). More
Chinese specific indicators are then augmented
manually. We use a binary feature denoting a ref-
erence to the first person (“ (I)”,“ (f 7(We)”) in
the sentence. We also use a binary feature to in-
dicate whether the sentence contains a modal verb
like “AiA(should)” and “tf(hope)”.
Lexical features Binary features are modeled for
all connectives and adverbs, which are identified
based on POS tags.
Structural features The number of words, the
number of clauses in the sentence and the num-
ber of sentences in the same paragraph are used as
features. We also define binary features based on
punctuation which indicate whether the sentence
ends with a full stop, question mark, exclamation
mark or no sentence-final punctuation.
Topic and prompt features For each sentence,
the cosine similarities to the essay title and to the
prompt are used as features.
</bodyText>
<sectionHeader confidence="0.954467" genericHeader="method">
4 Identification based on Cohesion
</sectionHeader>
<subsectionHeader confidence="0.992262">
4.1 Cohesive Chains
</subsectionHeader>
<bodyText confidence="0.9999687">
We mainly exploit reference and lexical cohesion.
Creating identity chains Reference refers to re-
sources for referring to a participant whose iden-
tity is recoverable (Schiffrin et al., 2008). We fo-
cus on person identities, because person names
might be mentioned when describing facts. Firstly,
we extract all nouns/entities with a POS/NE tag
person as identities. Secondly, we conduct a sim-
ple third-person pronoun resolution by selecting
the nearest proper antecedent identity within the
same paragraph. Finally, an identity and all its
anaphora together form an identity chain.
Creating lexical chains Lexical cohesion is re-
ferred to relations between text using lexical
repetition, synonymy or near synonymy. We
don’t distinguish between systematic semantic
relations and non-systematic semantic relations
(Berzl´anovich et al., 2008) nor use a thesaurus
(Hirst and St-Onge, 1998). Instead, we compute
the relatedness of two words based on their dis-
tributed representations, which are learned using
the Word2Vec toolkit (Mikolov et al., 2013). The
data for learning word representations consists of
student essays and textbooks crawled from the
Web. The vocabulary size is about 490k.
We extract nouns, adjectives and verbs (exclud-
ing auxiliary verbs) instead of using nouns only
(Hirst and St-Onge, 1998) for constructing lexical
chains . We firstly cluster words into clusters in a
graph based manner. Each word corresponds to a
node in a graph. If the relatedness of two words
is larger than a threshold T, they are considered as
related and linked by an edge. After constructing
all edges in this way, every connected subgraph
forms a word cluster. Through the essay, all oc-
currences of the words from the same cluster form
a lexical chain.
We discard identity and lexical chains that exist
within single sentences, since they can’t capture
cohesive relations among sentences.
</bodyText>
<subsectionHeader confidence="0.978616">
4.2 Global and Local Sentence Chains
</subsectionHeader>
<bodyText confidence="0.999983761904762">
We organize sentences based on cohesive chains.
Sentences that contain members from the same
identity chain or lexical chain form a sentence
chain. The sentence chains represent cohesive re-
lations among sentences.
In persuasive writing, discourse elements are
commonly linked globally. For example, main
ideas are usually related to each other because
they are about different aspects of the main the-
sis, and thesis and conclusion should echo each
other as well. Therefore, we attempt to explic-
itly categorize sentence chains into local chains
and global chains based on subtopic coverage.
A local chain represents sentences that share co-
hesive relations and gather locally within single
subtopics. In contrast, a global chain represents
sentences with cohesive relations distribute across
multiple subtopics. Heuristically, we expect that
thesis statements can be better captured by global
chains, while sentences that state facts or provide
evidences are associated to local chains.
</bodyText>
<page confidence="0.972418">
2257
</page>
<table confidence="0.906065875">
Global sentence chains
Paragraph 1 2 3 4 5
Sentence 1 2 3 4 5 6 7 8 9 10 11
Cohesive 1
chains
2
3
4
</table>
<figureCaption confidence="0.947758">
Figure 1: An illustration of global/local sentence
chains. Each solid node in the grid indicates that a
sentence contains a word from a cohesive chain.
</figureCaption>
<bodyText confidence="0.999916210526316">
Although subtopics can be identified by existing
text segmentation algorithms (Hearst, 1997; Filip-
pova and Strube, 2006), we observe that in stu-
dent essays a subtopic boundary usually coincides
with a paragraph boundary and almost all subtopic
segments are within one paragraph and only a few
of subtopics are within two or more paragraphs.
Therefore, we simply assume that each paragraph
corresponds to a subtopic. Based on the assump-
tion, chain classification is approximated based on
chain span over paragraphs. A sentence chain is
classified as a global chain, if its members appear
in at least N paragraphs, otherwise it is classified
as a local chain. We set N = 3, which means
a global chain would cover at least two subtopics
considering most subtopics finish within two para-
graphs. One sentence can be involved in multiple
sentence chains. An illustration of global and local
sentence chains is shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.998273">
4.3 Cohesion Features from Sentence Chains
</subsectionHeader>
<bodyText confidence="0.999819219512195">
We develop cohesion features for a sentence from
the sentence chains that involve it. Such features
are beyond the intrinsic properties of the sentence
itself but describe relations to other sentences.
Chain-type features We consider four combined
types of sentence chains: global-identity, local-
identity, global-lexical and local-lexical chains.
The number of each type of chains that involve the
sentence is used as a feature.
Global-title feature If the sentence is in a global
sentence chain and the corresponding cohesive
chain contains a word in the title, a binary fea-
ture global-title is set as true, otherwise set as
false. Containing globally distributed title words
is thought of as an indicator of thesis statements.
Interaction features Hasan (1984, 1985) defined
that an interaction between two chains takes place
when multiple members of a chain relate in the
same way to more than one members of another
chain, which can be used to distinguish central
tokens from peripheral tokens. Hoey (1991) ex-
plored similar interactions to assess the centrality
of sentences. This indicates that chain interactions
might be signals of important content.
Similarly, we say two sentence chains interact
with each other, if they have more than one sen-
tence in common. An example is shown in Figure
1. Moreover, if two chains are both global chain,
we term it a global interaction, otherwise a local
interaction. The shared sentences by two chains
are named as global or local interaction sentences
accordingly. Two binary features are derived: the
sentence is or not a global-interaction sentence
and it is or not a local-interaction sentence.
Strength features We attempt to measure the
overall strength of the sentence chains that involve
the sentence. The features include the number of
chains, the maximum and average number of cov-
ered sentences and paragraphs over chains, among
which the ones related to paragraphs can be seen
as measuring the global cohesion strength.
</bodyText>
<sectionHeader confidence="0.997916" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.947297">
5.1 Settings
</subsectionHeader>
<bodyText confidence="0.999933347826087">
We evaluated the effectiveness of Cohesion fea-
tures by comparing with the baseline that uses the
Basic features introduced in Section 3.2.
We adopted precision(P), recall(R) and F1-
measure(F1) as evaluation metrics. The thresh-
old T used to determine whether two words are
related was set to 0.8 empirically. Because sen-
tences with the discourse element Other are few,
we didn’t evaluate the performance on it.
We conducted experiments on three corpora re-
spectively using 5-fold cross-validation. We com-
pared various SVM classifiers with different ker-
nels implemented in the LibSVM toolkit (Chang
and Lin, 2011) and the linear-chain CRF model
(Lafferty et al., 2001). When using CRF, the pre-
diction of previous sentence is considered for cur-
rent sentence. In our experiments, CRF achieves
significant superior performance than SVM both
when using basic features alone and after adding
cohesion features. This indicates that incorporat-
ing correlations among sequential predictions are
important for this task. Next, we only report the
experimental results of using the CRF Model.
</bodyText>
<table confidence="0.98171525">
Local sentence chains
A chain interaction
2258
Element Features C1 C2 C3
P R F1 P R F1 P R F1 avg. △ (F1)
Basic 84.5 89.6 Introduction 82.2 80.7 81.5 80.6 90.1 85.0 +3.7
+ Cohesion 87.2 90.8 86.8 85.6 84.8 85.2 87.3 94.4 90.6
88.8
Prompt Basic 89.7 86.9 88.2 77.2 69.0 72.5 − − − +1.9
+ Cohesion 91.1 89.2 90.1 82.0 69.1 74.4 − − −
Thesis Basic 76.5 69.0 72.4 69.9 61.1 64.9 73.3 57.5 64.0 +5.1
+ Cohesion 78.3 73.1 75.5 75.4 63.8 68.6 77.3 68.9 72.7
Basic 71.4 59.1 Main idea 69.0 60.9 64.6 69.4 54.0 60.7 +5.4
+ Cohesion 75.7 65.3 64.5 73.6 61.3 66.8 75.7 64.3 69.4
70.0
Supporting Basic 86.1 91.4 88.6 83.8 89.6 86.6 83.8 90.5 87.0 +1.8
idea + Cohesion 88.0 92.3 90.1 84.2 91.6 87.7 87.7 92.2 89.9
Basic 87.2 89.9 Conclusion 85.6 88.5 87.0 88.1 91.0 89.5 +2.2
+ Cohesion 89.1 91.9 88.4 86.0 90.7 88.2 92.1 94.0 93.1
90.4
</table>
<tableCaption confidence="0.999731">
Table 3: Experimental results on six discourse elements over three corpora using the CRF model.
</tableCaption>
<subsectionHeader confidence="0.997801">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999992314285714">
The experimental results on three corpora are
shown in Table 3. We tested statistical signifi-
cance for F1 scores and found that all improve-
ments were significant with p &lt; 0.01 based on
the pairwise t-test. We can see that adding cohe-
sion features obtain improvements on all discourse
elements over three corpora. Especially, the cohe-
sion features contribute to large improvements of
+5.1% and +5.4% average F1 score on identifying
thesis and main idea sentences. By analyzing the
confusion matrix, we found that the improvements
mainly come from more accurately distinguishing
thesis and main idea sentences from introduction
and supporting idea sentences.
We are interested in that which between the lo-
cal and global cohesion contributes more to dis-
tinguish thesis statements. To this end, we used
Area under the ROC Curve (AUC)(Swets, 1988)
to measure the discriminative power of individual
features. Larger AUC of a feature indicates bet-
ter discriminative performance. The experiment
was done on the dataset mixing all sentences from
three corpora. We divided sentences into thesis
statements and non-thesis statements according to
their true element labels. As the results in Ta-
ble 4 show, global cohesion related features are
of higher rank with regard to the discriminative
power. Local cohesion relates more to non-thesis
statements, though it is not so discriminative as
global cohesion. The results indicate that sepa-
rating global cohesion from local cohesion help
the distinction between thesis statements and oth-
ers. Features related to identity chains alone don’t
show much discriminative ability. But they in-
crease the macro F1 score by 0.9% in combina-
</bodyText>
<table confidence="0.999429357142857">
Cohesion Feature AUC
Global-lexical 0.712
Avg.#paras 0.670
Global-title 0.664
Max.#para 0.659
Global-interaction 0.654
Max.#sents 0.636
Avg.#sents 0.613
#Chains 0.601
Local-title 0.522
Global-identity 0.510
Local-identity 0.481
Local-interaction 0.476
Local-lexical 0.431
</table>
<tableCaption confidence="0.9906335">
Table 4: Discriminative powers of individual fea-
tures by Areas under ROC curve (AUC).
</tableCaption>
<bodyText confidence="0.865853">
tion with other features.
</bodyText>
<sectionHeader confidence="0.998388" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999874">
We have investigated the impact of cohesion for
identifying discourse elements in student essays.
Our method creates sentence chains by means
of cohesive resources and separates global chains
from local ones based on the subtopic coverage.
New features for each sentence are derived from
the properties of the sentence chains involving it.
Experimental results show the effectiveness of co-
hesion features and the discriminative ability of
global cohesion for identifying thesis statements.
</bodyText>
<sectionHeader confidence="0.998301" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9430475">
This research is supported by the National Natu-
ral Science Foundation of China (No.61402304),
the Beijing Municipal Natural Science Founda-
tion (No.4154065) and the Humanity &amp; Social
Science General Project of Ministry of Education
(No.14YJAZH046).
</bodyText>
<page confidence="0.992346">
2259
</page>
<sectionHeader confidence="0.985988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.96174252688172">
Regina Barzilay and Michael Elhadad. Using lex-
ical chains for text summarization. Advances in
automatic text summarization, pages 111–121,
1999.
Regina Barzilay and Mirella Lapata. Modeling lo-
cal coherence: An entity-based approach. Com-
putational Linguistics, 34(1):1–34, 2008.
Ildik´o Berzl´anovich, Markus Egg, and Gisela Re-
deker. Coherence structure and lexical cohesion
in expository and persuasive texts. In Proceed-
ings of the Workshop Constraints in Discourse
III, pages 19–26, 2008.
Jill Burstein, Daniel Marcu, Slava Andreyev, and
Martin Chodorow. Towards automatic classifi-
cation of discourse elements in essays. In Pro-
ceedings of the 39th annual Meeting on Associ-
ation for Computational Linguistics, pages 98–
105. Association for Computational Linguis-
tics, 2001.
Jill Burstein, Martin Chodorow, and Claudia Lea-
cock. Criterionsm online essay evaluation: An
application for automated evaluation of student
essays. In IAAI, pages 3–10, 2003a.
Jill Burstein, Daniel Marcu, and Kevin Knight.
Finding the write stuff: Automatic identifica-
tion of discourse structure in student essays. In-
telligent Systems, IEEE, 18(1):32–39, 2003b.
Chih-Chung Chang and Chih-Jen Lin. Libsvm:
a library for support vector machines. ACM
Transactions on Intelligent Systems and Tech-
nology (TIST), 2(3):27, 2011.
Wanxiang Che, Zhenghua Li, and Ting Liu. Ltp:
A chinese language technology platform. In
Proceedings of the 23rd International Confer-
ence on Computational Linguistics: Demon-
strations, pages 13–16. Association for Compu-
tational Linguistics, 2010.
Jacob Cohen et al. A coefficient of agreement for
nominal scales. Educational and psychological
measurement, 20(1):37–46, 1960.
Katja Filippova and Michael Strube. Using
linguistically motivated features for paragraph
boundary identification. In Proceedings of the
2006 Conference on Empirical Methods in Nat-
ural Language Processing, pages 267–274. As-
sociation for Computational Linguistics, 2006.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. Discourse segmen-
tation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting on Association
for Computational Linguistics-Volume 1, pages
562–569. Association for Computational Lin-
guistics, 2003.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
A weakly-supervised approach to argumenta-
tive zoning of scientific documents. In Proceed-
ings of the Conference on Empirical Methods
in Natural Language Processing, pages 273–
283. Association for Computational Linguis-
tics, 2011.
MAK Halliday and Ruqaiya Hasan. Cohesion in
english (english language). London, 1976; Mar-
tin JR, 1976.
Ruqaiya Hasan. Coherence and cohesive harmony.
Understanding Reading Comprehesion: Cog-
nition, Lanugage and The Structure of Prose,
1984.
Ruqaiya Hasan. The texture of a text. Language,
Context and Text, 1985.
Marti A. Hearst. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computa-
tional Linguistics, pages 33–64, 1997.
Derrick Higgins, Jill Burstein, Daniel Marcu, and
Claudia Gentile. Evaluating multiple aspects of
coherence in student essays. In HLT-NAACL,
pages 185–192, 2004.
Graeme Hirst and David St-Onge. Lexical chains
as representations of context for the detection
and correction of malapropisms. WordNet:
An electronic lexical database, 305:305–332,
1998.
Michael Hoey. Patterns of lexis in text. 1991.
Pei-Yun Hsueh, Johanna D Moore, and Steve Re-
nals. Automatic segmentation of multiparty di-
alogue. In EACL, 2006.
John Lafferty, Andrew McCallum, and Fer-
nando CN Pereira. Conditional random fields:
Probabilistic models for segmenting and label-
ing sequence data. In ICML, pages 282–289.
Morgan Kaufmann, 2001.
Daniel Marcu. The theory and practice of dis-
course parsing and summarization. MIT press,
2000.
</reference>
<page confidence="0.687332">
2260
</page>
<reference confidence="0.989868392857143">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compo-
sitionality. In Advances in neural information
processing systems, pages 3111–3119, 2013.
Jane Morris and Graeme Hirst. Lexical cohesion
computed by thesaural relations as an indicator
of the structure of text. Computational linguis-
tics, 17(1):21–48, 1991.
Isaac Persing, Alan Davis, and Vincent Ng. Mod-
eling organization in student essays. In Pro-
ceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing,
pages 229–239. Association for Computational
Linguistics, 2010.
Deborah Schiffrin, Deborah Tannen, and Heidi E
Hamilton. The handbook of discourse analysis.
John Wiley &amp; Sons, 2008.
Christian Stab and Iryna Gurevych. Identifying ar-
gumentative discourse structures in persuasive
essays. In EMNLP 2014, pages 46–56, 2014.
John A Swets. Measuring the accuracy of diag-
nostic systems. Science, 240(4857):1285–1293,
1988.
Bonnie Webber, Markus Egg, and Valia Kor-
doni. Discourse structure and language tech-
nology. Natural Language Engineering, 18(4):
437–490, 2011.
</reference>
<page confidence="0.991503">
2261
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.377839">
<title confidence="0.9995565">Discourse Element Identification in Student based on Global and Local Cohesion</title>
<author confidence="0.948253">Ruiji Lizhen Ting</author>
<affiliation confidence="0.714124">Engineering, Capital Normal University, Bejing 100048, Research Beijing, Beijing 100083, Institute of Technology, Harbin 150001,</affiliation>
<email confidence="0.839709">rjfu@iflytek.com,tliu@ir.hit.edu.cn</email>
<abstract confidence="0.992841846153846">We present a method of using cohesion to improve discourse element identification for sentences in student essays. New features for each sentence are derived by considering its relations to global and local cohesion, which are created by means of cohesive resources and subtopic coverage. In our experiments, we obtain significant improvements on identifying all discourse especially of +5% on The analysis shows global cohesion can better capture the-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization. Advances in automatic text summarization,</title>
<date>1999</date>
<pages>111--121</pages>
<contexts>
<context position="1970" citStr="Barzilay and Elhadad, 1999" startWordPosition="288" endWordPosition="291">s in persuasive writing are summarized in Table 1. Previous work mainly exploits the properties of a sentence itself or adjacent sentences for this task. In this work, we explore cohesion to express relations among sentences through the whole text. Cohesion can be defined as a set of resources linking within a text that organize the text together (Halliday and Hasan, 1976). It can be achieved through the use of reference, ellipsis, substitution, conjunction and lexical cohesion. Among them, lexical cohesion has been widely used for modeling local coherence and applied to related applications (Barzilay and Elhadad, 1999; Barzilay and Lapata, 2008; Galley et al., 2003; Hsueh et al., 2006; Filippova and Strube, 2006). Since cohesion Element Definition Introduction (I) introduces the background and/or grabs readers’ attention Prompt (P) restates or summarize the prompt states the author’s main claim on the Thesis (T) issue for which he/she is arguing Main idea (M) asserts foundational ideas or aspects that are related to the thesis Supporting provides evidence to explain or idea (S) support the thesis and main ideas Conclusion (C) concludes the whole essay or one of the main ideas doesn’t fit into the above ele</context>
</contexts>
<marker>Barzilay, Elhadad, 1999</marker>
<rawString>Regina Barzilay and Michael Elhadad. Using lexical chains for text summarization. Advances in automatic text summarization, pages 111–121, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1997" citStr="Barzilay and Lapata, 2008" startWordPosition="292" endWordPosition="295">summarized in Table 1. Previous work mainly exploits the properties of a sentence itself or adjacent sentences for this task. In this work, we explore cohesion to express relations among sentences through the whole text. Cohesion can be defined as a set of resources linking within a text that organize the text together (Halliday and Hasan, 1976). It can be achieved through the use of reference, ellipsis, substitution, conjunction and lexical cohesion. Among them, lexical cohesion has been widely used for modeling local coherence and applied to related applications (Barzilay and Elhadad, 1999; Barzilay and Lapata, 2008; Galley et al., 2003; Hsueh et al., 2006; Filippova and Strube, 2006). Since cohesion Element Definition Introduction (I) introduces the background and/or grabs readers’ attention Prompt (P) restates or summarize the prompt states the author’s main claim on the Thesis (T) issue for which he/she is arguing Main idea (M) asserts foundational ideas or aspects that are related to the thesis Supporting provides evidence to explain or idea (S) support the thesis and main ideas Conclusion (C) concludes the whole essay or one of the main ideas doesn’t fit into the above elements or Other (O) makes no</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ildik´o Berzl´anovich</author>
<author>Markus Egg</author>
<author>Gisela Redeker</author>
</authors>
<title>Coherence structure and lexical cohesion in expository and persuasive texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop Constraints in Discourse III,</booktitle>
<pages>pages</pages>
<marker>Berzl´anovich, Egg, Redeker, 2008</marker>
<rawString>Ildik´o Berzl´anovich, Markus Egg, and Gisela Redeker. Coherence structure and lexical cohesion in expository and persuasive texts. In Proceedings of the Workshop Constraints in Discourse III, pages 19–26, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Slava Andreyev</author>
<author>Martin Chodorow</author>
</authors>
<title>Towards automatic classification of discourse elements in essays.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="3525" citStr="Burstein et al. (2001)" startWordPosition="531" endWordPosition="534">senting text structure such as full-text discourse parsers (Marcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive essays. For example, a main idea may be followed by a supporting idea sentence. The two sentences can be coherent but their discourse elements are different. To deal with this, global cohesion should be exploited. Considering that in persuasive writing, thesis, main ideas and conclusion, which are termed thesis statements by Burstein et al. (2001), are expected to relate to each other (Higgins et al., 2004). It is likely that cohesive relations exist among them through the whole text. We make a focused contribution by investigating global and local cohesive relations. We create sentence chains based on cohesive resources and examine whether the chains represent global cohesion or local cohesion. Our hypothesis is that global cohesion can better capture thesis state2255 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2255–2261, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for C</context>
<context position="6567" citStr="Burstein et al., 2001" startWordPosition="1025" endWordPosition="1028">d as writing examples of persuasive essays written by high school students. The average essay lengths on three corpora are 795, 772 and 864 Chinese characters respectively. The other basic statistics of the annotated corpora are listed in Table 2. During annotation, the labelers found cases of difficulties about ambiguous elements. For example, content about the prompt and the main thesis can be mentioned in the same sentence. In such cases, thesis statements have priority over other elements to be labeled, since identifying thesis statements is more important for some potential applications (Burstein et al., 2001). From each corpus, 100 essays were labeled by both annotators for computing agreements, and the others were labeled independently. The label agreements measured with Kappa (Cohen et al., 1960) are high as shown in Table 2. The disagreements were resolved by discussion. The distributions of discourse elements are also shown in Table 2. We can see that they are imbalanced. The supporting idea sentences account for more than 53%, while the thesis statements account for only 34% in total. As a result, the distinction between minority thesis statements from overwhelming supporting idea sentences i</context>
</contexts>
<marker>Burstein, Marcu, Andreyev, Chodorow, 2001</marker>
<rawString>Jill Burstein, Daniel Marcu, Slava Andreyev, and Martin Chodorow. Towards automatic classification of discourse elements in essays. In Proceedings of the 39th annual Meeting on Association for Computational Linguistics, pages 98– 105. Association for Computational Linguistics, 2001.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jill Burstein</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Criterionsm online essay evaluation: An application for automated evaluation of student essays.</title>
<booktitle>In IAAI,</booktitle>
<pages>3--10</pages>
<marker>Burstein, Chodorow, Leacock, </marker>
<rawString>Jill Burstein, Martin Chodorow, and Claudia Leacock. Criterionsm online essay evaluation: An application for automated evaluation of student essays. In IAAI, pages 3–10, 2003a.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<title>Finding the write stuff: Automatic identification of discourse structure in student essays.</title>
<journal>Intelligent Systems, IEEE,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>2003</pages>
<marker>Burstein, Marcu, Knight, </marker>
<rawString>Jill Burstein, Daniel Marcu, and Kevin Knight. Finding the write stuff: Automatic identification of discourse structure in student essays. Intelligent Systems, IEEE, 18(1):32–39, 2003b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology (TIST),</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="16649" citStr="Chang and Lin, 2011" startWordPosition="2630" endWordPosition="2633">Settings We evaluated the effectiveness of Cohesion features by comparing with the baseline that uses the Basic features introduced in Section 3.2. We adopted precision(P), recall(R) and F1- measure(F1) as evaluation metrics. The threshold T used to determine whether two words are related was set to 0.8 empirically. Because sentences with the discourse element Other are few, we didn’t evaluate the performance on it. We conducted experiments on three corpora respectively using 5-fold cross-validation. We compared various SVM classifiers with different kernels implemented in the LibSVM toolkit (Chang and Lin, 2011) and the linear-chain CRF model (Lafferty et al., 2001). When using CRF, the prediction of previous sentence is considered for current sentence. In our experiments, CRF achieves significant superior performance than SVM both when using basic features alone and after adding cohesion features. This indicates that incorporating correlations among sequential predictions are important for this task. Next, we only report the experimental results of using the CRF Model. Local sentence chains A chain interaction 2258 Element Features C1 C2 C3 P R F1 P R F1 P R F1 avg. △ (F1) Basic 84.5 89.6 Introducti</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Ltp: A chinese language technology platform.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations,</booktitle>
<pages>13--16</pages>
<contexts>
<context position="8360" citStr="Che et al., 2010" startWordPosition="1303" endWordPosition="1306"> 2014). It can also be solved in a sequence labeling framework, which models the whole sentence sequence and captures the correlations among predictions. For example, Conditional Random Fields (CRFs) have be studied for similar task on argumentative zoning of scientific documents (Guo et al., 2011). We will evaluate different types of features using two representative models respectively: the SVM model and the linear-chain CRF model. 2256 3.2 Basic Features Before feature extraction, sentence splitting, word segmentation,POS and NE tagging are done using a Chinese language processing toolkit (Che et al., 2010). Most basic features are adapted from previous work (Burstein et al., 2003a; Stab and Gurevych, 2014; Persing et al., 2010). For each sentence, the following feature sets are extracted. Position features The relative position of its paragraph (first, last or body) in the essay and its relative position (first, last or body) in the paragraph are modeled as a set of binary features. The index of the sentence is also used as a feature. Indicator features Cue words/phrases like “ ik )j(in my opinion)” and “,0&apos; Z.(in conclusion)” are used as indicators. Partial indicators are adapted from the ones</context>
</contexts>
<marker>Che, Li, Liu, 2010</marker>
<rawString>Wanxiang Che, Zhenghua Li, and Ting Liu. Ltp: A chinese language technology platform. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, pages 13–16. Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and psychological measurement,</booktitle>
<volume>20</volume>
<issue>1</issue>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen et al. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37–46, 1960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Using linguistically motivated features for paragraph boundary identification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>267--274</pages>
<contexts>
<context position="2067" citStr="Filippova and Strube, 2006" startWordPosition="304" endWordPosition="307">of a sentence itself or adjacent sentences for this task. In this work, we explore cohesion to express relations among sentences through the whole text. Cohesion can be defined as a set of resources linking within a text that organize the text together (Halliday and Hasan, 1976). It can be achieved through the use of reference, ellipsis, substitution, conjunction and lexical cohesion. Among them, lexical cohesion has been widely used for modeling local coherence and applied to related applications (Barzilay and Elhadad, 1999; Barzilay and Lapata, 2008; Galley et al., 2003; Hsueh et al., 2006; Filippova and Strube, 2006). Since cohesion Element Definition Introduction (I) introduces the background and/or grabs readers’ attention Prompt (P) restates or summarize the prompt states the author’s main claim on the Thesis (T) issue for which he/she is arguing Main idea (M) asserts foundational ideas or aspects that are related to the thesis Supporting provides evidence to explain or idea (S) support the thesis and main ideas Conclusion (C) concludes the whole essay or one of the main ideas doesn’t fit into the above elements or Other (O) makes no meaningful contribution Table 1: Definitions of discourse elements. i</context>
<context position="13189" citStr="Filippova and Strube, 2006" startWordPosition="2074" endWordPosition="2078">ts sentences with cohesive relations distribute across multiple subtopics. Heuristically, we expect that thesis statements can be better captured by global chains, while sentences that state facts or provide evidences are associated to local chains. 2257 Global sentence chains Paragraph 1 2 3 4 5 Sentence 1 2 3 4 5 6 7 8 9 10 11 Cohesive 1 chains 2 3 4 Figure 1: An illustration of global/local sentence chains. Each solid node in the grid indicates that a sentence contains a word from a cohesive chain. Although subtopics can be identified by existing text segmentation algorithms (Hearst, 1997; Filippova and Strube, 2006), we observe that in student essays a subtopic boundary usually coincides with a paragraph boundary and almost all subtopic segments are within one paragraph and only a few of subtopics are within two or more paragraphs. Therefore, we simply assume that each paragraph corresponds to a subtopic. Based on the assumption, chain classification is approximated based on chain span over paragraphs. A sentence chain is classified as a global chain, if its members appear in at least N paragraphs, otherwise it is classified as a local chain. We set N = 3, which means a global chain would cover at least </context>
</contexts>
<marker>Filippova, Strube, 2006</marker>
<rawString>Katja Filippova and Michael Strube. Using linguistically motivated features for paragraph boundary identification. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 267–274. Association for Computational Linguistics, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Eric FoslerLussier, and Hongyan Jing. Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>562--569</pages>
<marker>Galley, McKeown, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric FoslerLussier, and Hongyan Jing. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 562–569. Association for Computational Linguistics, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufan Guo</author>
<author>Anna Korhonen</author>
<author>Thierry Poibeau</author>
</authors>
<title>A weakly-supervised approach to argumentative zoning of scientific documents.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>273--283</pages>
<contexts>
<context position="8042" citStr="Guo et al., 2011" startWordPosition="1255" endWordPosition="1258">g models for this task. 3.1 Learning Models Discourse element identification can be casted as a classification problem that sentences are classified independently using a classifier, e.g. naive Bayes (Burstein et al., 2001), decision tree (Burstein et al., 2003b) and Support Vector Machines (SVMs) (Stab and Gurevych, 2014). It can also be solved in a sequence labeling framework, which models the whole sentence sequence and captures the correlations among predictions. For example, Conditional Random Fields (CRFs) have be studied for similar task on argumentative zoning of scientific documents (Guo et al., 2011). We will evaluate different types of features using two representative models respectively: the SVM model and the linear-chain CRF model. 2256 3.2 Basic Features Before feature extraction, sentence splitting, word segmentation,POS and NE tagging are done using a Chinese language processing toolkit (Che et al., 2010). Most basic features are adapted from previous work (Burstein et al., 2003a; Stab and Gurevych, 2014; Persing et al., 2010). For each sentence, the following feature sets are extracted. Position features The relative position of its paragraph (first, last or body) in the essay and</context>
</contexts>
<marker>Guo, Korhonen, Poibeau, 2011</marker>
<rawString>Yufan Guo, Anna Korhonen, and Thierry Poibeau. A weakly-supervised approach to argumentative zoning of scientific documents. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 273– 283. Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MAK Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<title>Cohesion in english (english language).</title>
<date>1976</date>
<location>London,</location>
<contexts>
<context position="1719" citStr="Halliday and Hasan, 1976" startWordPosition="250" endWordPosition="253">s on identifying discourse elements for sentences in persuasive essays written by Chinese high school students. Discourse elements represent the contributions that sentences can make to text organization. Typical discourse elements and their functions in persuasive writing are summarized in Table 1. Previous work mainly exploits the properties of a sentence itself or adjacent sentences for this task. In this work, we explore cohesion to express relations among sentences through the whole text. Cohesion can be defined as a set of resources linking within a text that organize the text together (Halliday and Hasan, 1976). It can be achieved through the use of reference, ellipsis, substitution, conjunction and lexical cohesion. Among them, lexical cohesion has been widely used for modeling local coherence and applied to related applications (Barzilay and Elhadad, 1999; Barzilay and Lapata, 2008; Galley et al., 2003; Hsueh et al., 2006; Filippova and Strube, 2006). Since cohesion Element Definition Introduction (I) introduces the background and/or grabs readers’ attention Prompt (P) restates or summarize the prompt states the author’s main claim on the Thesis (T) issue for which he/she is arguing Main idea (M) </context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>MAK Halliday and Ruqaiya Hasan. Cohesion in english (english language). London, 1976; Martin JR, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruqaiya Hasan</author>
</authors>
<title>Coherence and cohesive harmony. Understanding Reading Comprehesion: Cognition, Lanugage and The Structure of Prose,</title>
<date>1984</date>
<contexts>
<context position="14796" citStr="Hasan (1984" startWordPosition="2335" endWordPosition="2336">e itself but describe relations to other sentences. Chain-type features We consider four combined types of sentence chains: global-identity, localidentity, global-lexical and local-lexical chains. The number of each type of chains that involve the sentence is used as a feature. Global-title feature If the sentence is in a global sentence chain and the corresponding cohesive chain contains a word in the title, a binary feature global-title is set as true, otherwise set as false. Containing globally distributed title words is thought of as an indicator of thesis statements. Interaction features Hasan (1984, 1985) defined that an interaction between two chains takes place when multiple members of a chain relate in the same way to more than one members of another chain, which can be used to distinguish central tokens from peripheral tokens. Hoey (1991) explored similar interactions to assess the centrality of sentences. This indicates that chain interactions might be signals of important content. Similarly, we say two sentence chains interact with each other, if they have more than one sentence in common. An example is shown in Figure 1. Moreover, if two chains are both global chain, we term it a</context>
</contexts>
<marker>Hasan, 1984</marker>
<rawString>Ruqaiya Hasan. Coherence and cohesive harmony. Understanding Reading Comprehesion: Cognition, Lanugage and The Structure of Prose, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruqaiya Hasan</author>
</authors>
<title>The texture of a text. Language, Context and Text,</title>
<date>1985</date>
<marker>Hasan, 1985</marker>
<rawString>Ruqaiya Hasan. The texture of a text. Language, Context and Text, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics,</title>
<date>1997</date>
<pages>33--64</pages>
<contexts>
<context position="13160" citStr="Hearst, 1997" startWordPosition="2072" endWordPosition="2073">chain represents sentences with cohesive relations distribute across multiple subtopics. Heuristically, we expect that thesis statements can be better captured by global chains, while sentences that state facts or provide evidences are associated to local chains. 2257 Global sentence chains Paragraph 1 2 3 4 5 Sentence 1 2 3 4 5 6 7 8 9 10 11 Cohesive 1 chains 2 3 4 Figure 1: An illustration of global/local sentence chains. Each solid node in the grid indicates that a sentence contains a word from a cohesive chain. Although subtopics can be identified by existing text segmentation algorithms (Hearst, 1997; Filippova and Strube, 2006), we observe that in student essays a subtopic boundary usually coincides with a paragraph boundary and almost all subtopic segments are within one paragraph and only a few of subtopics are within two or more paragraphs. Therefore, we simply assume that each paragraph corresponds to a subtopic. Based on the assumption, chain classification is approximated based on chain span over paragraphs. A sentence chain is classified as a global chain, if its members appear in at least N paragraphs, otherwise it is classified as a local chain. We set N = 3, which means a globa</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, pages 33–64, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Claudia Gentile</author>
</authors>
<title>Evaluating multiple aspects of coherence in student essays.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>185--192</pages>
<contexts>
<context position="3586" citStr="Higgins et al., 2004" startWordPosition="542" endWordPosition="545">rcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive essays. For example, a main idea may be followed by a supporting idea sentence. The two sentences can be coherent but their discourse elements are different. To deal with this, global cohesion should be exploited. Considering that in persuasive writing, thesis, main ideas and conclusion, which are termed thesis statements by Burstein et al. (2001), are expected to relate to each other (Higgins et al., 2004). It is likely that cohesive relations exist among them through the whole text. We make a focused contribution by investigating global and local cohesive relations. We create sentence chains based on cohesive resources and examine whether the chains represent global cohesion or local cohesion. Our hypothesis is that global cohesion can better capture thesis state2255 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2255–2261, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Corpus #Essays Avg.#paras Avg.#sent</context>
</contexts>
<marker>Higgins, Burstein, Marcu, Gentile, 2004</marker>
<rawString>Derrick Higgins, Jill Burstein, Daniel Marcu, and Claudia Gentile. Evaluating multiple aspects of coherence in student essays. In HLT-NAACL, pages 185–192, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>305--305</pages>
<contexts>
<context position="10789" citStr="Hirst and St-Onge, 1998" startWordPosition="1689" endWordPosition="1692">ed when describing facts. Firstly, we extract all nouns/entities with a POS/NE tag person as identities. Secondly, we conduct a simple third-person pronoun resolution by selecting the nearest proper antecedent identity within the same paragraph. Finally, an identity and all its anaphora together form an identity chain. Creating lexical chains Lexical cohesion is referred to relations between text using lexical repetition, synonymy or near synonymy. We don’t distinguish between systematic semantic relations and non-systematic semantic relations (Berzl´anovich et al., 2008) nor use a thesaurus (Hirst and St-Onge, 1998). Instead, we compute the relatedness of two words based on their distributed representations, which are learned using the Word2Vec toolkit (Mikolov et al., 2013). The data for learning word representations consists of student essays and textbooks crawled from the Web. The vocabulary size is about 490k. We extract nouns, adjectives and verbs (excluding auxiliary verbs) instead of using nouns only (Hirst and St-Onge, 1998) for constructing lexical chains . We firstly cluster words into clusters in a graph based manner. Each word corresponds to a node in a graph. If the relatedness of two words </context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. Lexical chains as representations of context for the detection and correction of malapropisms. WordNet: An electronic lexical database, 305:305–332, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hoey</author>
</authors>
<title>Patterns of lexis in text.</title>
<date>1991</date>
<contexts>
<context position="15045" citStr="Hoey (1991)" startWordPosition="2377" endWordPosition="2378">entence is used as a feature. Global-title feature If the sentence is in a global sentence chain and the corresponding cohesive chain contains a word in the title, a binary feature global-title is set as true, otherwise set as false. Containing globally distributed title words is thought of as an indicator of thesis statements. Interaction features Hasan (1984, 1985) defined that an interaction between two chains takes place when multiple members of a chain relate in the same way to more than one members of another chain, which can be used to distinguish central tokens from peripheral tokens. Hoey (1991) explored similar interactions to assess the centrality of sentences. This indicates that chain interactions might be signals of important content. Similarly, we say two sentence chains interact with each other, if they have more than one sentence in common. An example is shown in Figure 1. Moreover, if two chains are both global chain, we term it a global interaction, otherwise a local interaction. The shared sentences by two chains are named as global or local interaction sentences accordingly. Two binary features are derived: the sentence is or not a global-interaction sentence and it is or</context>
</contexts>
<marker>Hoey, 1991</marker>
<rawString>Michael Hoey. Patterns of lexis in text. 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei-Yun Hsueh</author>
<author>Johanna D Moore</author>
<author>Steve Renals</author>
</authors>
<title>Automatic segmentation of multiparty dialogue.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<contexts>
<context position="2038" citStr="Hsueh et al., 2006" startWordPosition="300" endWordPosition="303">oits the properties of a sentence itself or adjacent sentences for this task. In this work, we explore cohesion to express relations among sentences through the whole text. Cohesion can be defined as a set of resources linking within a text that organize the text together (Halliday and Hasan, 1976). It can be achieved through the use of reference, ellipsis, substitution, conjunction and lexical cohesion. Among them, lexical cohesion has been widely used for modeling local coherence and applied to related applications (Barzilay and Elhadad, 1999; Barzilay and Lapata, 2008; Galley et al., 2003; Hsueh et al., 2006; Filippova and Strube, 2006). Since cohesion Element Definition Introduction (I) introduces the background and/or grabs readers’ attention Prompt (P) restates or summarize the prompt states the author’s main claim on the Thesis (T) issue for which he/she is arguing Main idea (M) asserts foundational ideas or aspects that are related to the thesis Supporting provides evidence to explain or idea (S) support the thesis and main ideas Conclusion (C) concludes the whole essay or one of the main ideas doesn’t fit into the above elements or Other (O) makes no meaningful contribution Table 1: Definit</context>
</contexts>
<marker>Hsueh, Moore, Renals, 2006</marker>
<rawString>Pei-Yun Hsueh, Johanna D Moore, and Steve Renals. Automatic segmentation of multiparty dialogue. In EACL, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="16704" citStr="Lafferty et al., 2001" startWordPosition="2639" endWordPosition="2642">eatures by comparing with the baseline that uses the Basic features introduced in Section 3.2. We adopted precision(P), recall(R) and F1- measure(F1) as evaluation metrics. The threshold T used to determine whether two words are related was set to 0.8 empirically. Because sentences with the discourse element Other are few, we didn’t evaluate the performance on it. We conducted experiments on three corpora respectively using 5-fold cross-validation. We compared various SVM classifiers with different kernels implemented in the LibSVM toolkit (Chang and Lin, 2011) and the linear-chain CRF model (Lafferty et al., 2001). When using CRF, the prediction of previous sentence is considered for current sentence. In our experiments, CRF achieves significant superior performance than SVM both when using basic features alone and after adding cohesion features. This indicates that incorporating correlations among sequential predictions are important for this task. Next, we only report the experimental results of using the CRF Model. Local sentence chains A chain interaction 2258 Element Features C1 C2 C3 P R F1 P R F1 P R F1 avg. △ (F1) Basic 84.5 89.6 Introduction 82.2 80.7 81.5 80.6 90.1 85.0 +3.7 + Cohesion 87.2 9</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289. Morgan Kaufmann, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The theory and practice of discourse parsing and summarization.</title>
<date>2000</date>
<publisher>MIT press,</publisher>
<contexts>
<context position="2975" citStr="Marcu, 2000" startWordPosition="446" endWordPosition="447">re related to the thesis Supporting provides evidence to explain or idea (S) support the thesis and main ideas Conclusion (C) concludes the whole essay or one of the main ideas doesn’t fit into the above elements or Other (O) makes no meaningful contribution Table 1: Definitions of discourse elements. is closely related to the structure of text (Morris and Hirst, 1991), it motivates us to explore similar techniques for discourse element identification. In addition, its ease of implementation is also attractive. Other options for representing text structure such as full-text discourse parsers (Marcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive essays. For example, a main idea may be followed by a supporting idea sentence. The two sentences can be coherent but their discourse elements are different. To deal with this, global cohesion should be exploited. Considering that in persuasive writing, thesis, main ideas and conclusion, which are termed thesis statements by Burstein et al. (2001), are expected to relate to each other (Higgins et</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. The theory and practice of discourse parsing and summarization. MIT press, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="10951" citStr="Mikolov et al., 2013" startWordPosition="1714" endWordPosition="1717">n by selecting the nearest proper antecedent identity within the same paragraph. Finally, an identity and all its anaphora together form an identity chain. Creating lexical chains Lexical cohesion is referred to relations between text using lexical repetition, synonymy or near synonymy. We don’t distinguish between systematic semantic relations and non-systematic semantic relations (Berzl´anovich et al., 2008) nor use a thesaurus (Hirst and St-Onge, 1998). Instead, we compute the relatedness of two words based on their distributed representations, which are learned using the Word2Vec toolkit (Mikolov et al., 2013). The data for learning word representations consists of student essays and textbooks crawled from the Web. The vocabulary size is about 490k. We extract nouns, adjectives and verbs (excluding auxiliary verbs) instead of using nouns only (Hirst and St-Onge, 1998) for constructing lexical chains . We firstly cluster words into clusters in a graph based manner. Each word corresponds to a node in a graph. If the relatedness of two words is larger than a threshold T, they are considered as related and linked by an edge. After constructing all edges in this way, every connected subgraph forms a wor</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="2734" citStr="Morris and Hirst, 1991" startWordPosition="410" endWordPosition="413">tion (I) introduces the background and/or grabs readers’ attention Prompt (P) restates or summarize the prompt states the author’s main claim on the Thesis (T) issue for which he/she is arguing Main idea (M) asserts foundational ideas or aspects that are related to the thesis Supporting provides evidence to explain or idea (S) support the thesis and main ideas Conclusion (C) concludes the whole essay or one of the main ideas doesn’t fit into the above elements or Other (O) makes no meaningful contribution Table 1: Definitions of discourse elements. is closely related to the structure of text (Morris and Hirst, 1991), it motivates us to explore similar techniques for discourse element identification. In addition, its ease of implementation is also attractive. Other options for representing text structure such as full-text discourse parsers (Marcu, 2000) may be not available or don’t have satisfied performance, especially for non-English languages. However, modeling local coherence alone is not adequate to distinguish discourse elements in persuasive essays. For example, a main idea may be followed by a supporting idea sentence. The two sentences can be coherent but their discourse elements are different. </context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational linguistics, 17(1):21–48, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac Persing</author>
<author>Alan Davis</author>
<author>Vincent Ng</author>
</authors>
<title>Modeling organization in student essays.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>229--239</pages>
<contexts>
<context position="8484" citStr="Persing et al., 2010" startWordPosition="1323" endWordPosition="1326">e correlations among predictions. For example, Conditional Random Fields (CRFs) have be studied for similar task on argumentative zoning of scientific documents (Guo et al., 2011). We will evaluate different types of features using two representative models respectively: the SVM model and the linear-chain CRF model. 2256 3.2 Basic Features Before feature extraction, sentence splitting, word segmentation,POS and NE tagging are done using a Chinese language processing toolkit (Che et al., 2010). Most basic features are adapted from previous work (Burstein et al., 2003a; Stab and Gurevych, 2014; Persing et al., 2010). For each sentence, the following feature sets are extracted. Position features The relative position of its paragraph (first, last or body) in the essay and its relative position (first, last or body) in the paragraph are modeled as a set of binary features. The index of the sentence is also used as a feature. Indicator features Cue words/phrases like “ ik )j(in my opinion)” and “,0&apos; Z.(in conclusion)” are used as indicators. Partial indicators are adapted from the ones used by Persing et al. (2010). More Chinese specific indicators are then augmented manually. We use a binary feature denoti</context>
</contexts>
<marker>Persing, Davis, Ng, 2010</marker>
<rawString>Isaac Persing, Alan Davis, and Vincent Ng. Modeling organization in student essays. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 229–239. Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Schiffrin</author>
<author>Deborah Tannen</author>
<author>Heidi E Hamilton</author>
</authors>
<title>The handbook of discourse analysis.</title>
<date>2008</date>
<publisher>John Wiley &amp; Sons,</publisher>
<contexts>
<context position="10095" citStr="Schiffrin et al., 2008" startWordPosition="1588" endWordPosition="1591"> and the number of sentences in the same paragraph are used as features. We also define binary features based on punctuation which indicate whether the sentence ends with a full stop, question mark, exclamation mark or no sentence-final punctuation. Topic and prompt features For each sentence, the cosine similarities to the essay title and to the prompt are used as features. 4 Identification based on Cohesion 4.1 Cohesive Chains We mainly exploit reference and lexical cohesion. Creating identity chains Reference refers to resources for referring to a participant whose identity is recoverable (Schiffrin et al., 2008). We focus on person identities, because person names might be mentioned when describing facts. Firstly, we extract all nouns/entities with a POS/NE tag person as identities. Secondly, we conduct a simple third-person pronoun resolution by selecting the nearest proper antecedent identity within the same paragraph. Finally, an identity and all its anaphora together form an identity chain. Creating lexical chains Lexical cohesion is referred to relations between text using lexical repetition, synonymy or near synonymy. We don’t distinguish between systematic semantic relations and non-systematic</context>
</contexts>
<marker>Schiffrin, Tannen, Hamilton, 2008</marker>
<rawString>Deborah Schiffrin, Deborah Tannen, and Heidi E Hamilton. The handbook of discourse analysis. John Wiley &amp; Sons, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Stab</author>
<author>Iryna Gurevych</author>
</authors>
<title>Identifying argumentative discourse structures in persuasive essays.</title>
<date>2014</date>
<booktitle>In EMNLP 2014,</booktitle>
<pages>46--56</pages>
<contexts>
<context position="7749" citStr="Stab and Gurevych, 2014" startWordPosition="1210" endWordPosition="1213">m overwhelming supporting idea sentences is a major challenge. 3 Discourse Element Identification Identifying discourse elements in student essays can be seen as a functional segmentation of discourse (Webber et al., 2011). In this work, we focus on utilizing supervised feature-based machine learning models for this task. 3.1 Learning Models Discourse element identification can be casted as a classification problem that sentences are classified independently using a classifier, e.g. naive Bayes (Burstein et al., 2001), decision tree (Burstein et al., 2003b) and Support Vector Machines (SVMs) (Stab and Gurevych, 2014). It can also be solved in a sequence labeling framework, which models the whole sentence sequence and captures the correlations among predictions. For example, Conditional Random Fields (CRFs) have be studied for similar task on argumentative zoning of scientific documents (Guo et al., 2011). We will evaluate different types of features using two representative models respectively: the SVM model and the linear-chain CRF model. 2256 3.2 Basic Features Before feature extraction, sentence splitting, word segmentation,POS and NE tagging are done using a Chinese language processing toolkit (Che et</context>
</contexts>
<marker>Stab, Gurevych, 2014</marker>
<rawString>Christian Stab and Iryna Gurevych. Identifying argumentative discourse structures in persuasive essays. In EMNLP 2014, pages 46–56, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Swets</author>
</authors>
<title>Measuring the accuracy of diagnostic systems.</title>
<date>1988</date>
<journal>Science,</journal>
<volume>240</volume>
<issue>4857</issue>
<contexts>
<context position="18904" citStr="Swets, 1988" startWordPosition="3020" endWordPosition="3021">ohesion features obtain improvements on all discourse elements over three corpora. Especially, the cohesion features contribute to large improvements of +5.1% and +5.4% average F1 score on identifying thesis and main idea sentences. By analyzing the confusion matrix, we found that the improvements mainly come from more accurately distinguishing thesis and main idea sentences from introduction and supporting idea sentences. We are interested in that which between the local and global cohesion contributes more to distinguish thesis statements. To this end, we used Area under the ROC Curve (AUC)(Swets, 1988) to measure the discriminative power of individual features. Larger AUC of a feature indicates better discriminative performance. The experiment was done on the dataset mixing all sentences from three corpora. We divided sentences into thesis statements and non-thesis statements according to their true element labels. As the results in Table 4 show, global cohesion related features are of higher rank with regard to the discriminative power. Local cohesion relates more to non-thesis statements, though it is not so discriminative as global cohesion. The results indicate that separating global co</context>
</contexts>
<marker>Swets, 1988</marker>
<rawString>John A Swets. Measuring the accuracy of diagnostic systems. Science, 240(4857):1285–1293, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Markus Egg</author>
<author>Valia Kordoni</author>
</authors>
<title>Discourse structure and language technology.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>437--490</pages>
<contexts>
<context position="7347" citStr="Webber et al., 2011" startWordPosition="1150" endWordPosition="1153">d with Kappa (Cohen et al., 1960) are high as shown in Table 2. The disagreements were resolved by discussion. The distributions of discourse elements are also shown in Table 2. We can see that they are imbalanced. The supporting idea sentences account for more than 53%, while the thesis statements account for only 34% in total. As a result, the distinction between minority thesis statements from overwhelming supporting idea sentences is a major challenge. 3 Discourse Element Identification Identifying discourse elements in student essays can be seen as a functional segmentation of discourse (Webber et al., 2011). In this work, we focus on utilizing supervised feature-based machine learning models for this task. 3.1 Learning Models Discourse element identification can be casted as a classification problem that sentences are classified independently using a classifier, e.g. naive Bayes (Burstein et al., 2001), decision tree (Burstein et al., 2003b) and Support Vector Machines (SVMs) (Stab and Gurevych, 2014). It can also be solved in a sequence labeling framework, which models the whole sentence sequence and captures the correlations among predictions. For example, Conditional Random Fields (CRFs) have</context>
</contexts>
<marker>Webber, Egg, Kordoni, 2011</marker>
<rawString>Bonnie Webber, Markus Egg, and Valia Kordoni. Discourse structure and language technology. Natural Language Engineering, 18(4): 437–490, 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
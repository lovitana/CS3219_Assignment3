<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.981371">
Do Multi-Sense Embeddings Improve Natural Language Understanding?
</title>
<author confidence="0.99864">
Jiwei Li
</author>
<affiliation confidence="0.825115333333333">
Computer Science Department
Stanford University
Stanford, CA 94305, USA
</affiliation>
<email confidence="0.997933">
jiweil@stanford.edu
</email>
<sectionHeader confidence="0.997387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99985071875">
Learning a distinct representation for each
sense of an ambiguous word could lead
to more powerful and fine-grained mod-
els of vector-space representations. Yet
while ‘multi-sense’ methods have been
proposed and tested on artificial word-
similarity tasks, we don’t know if they im-
prove real natural language understanding
tasks. In this paper we introduce a multi-
sense embedding model based on Chinese
Restaurant Processes that achieves state of
the art performance on matching human
word similarity judgments, and propose
a pipelined architecture for incorporating
multi-sense embeddings into language un-
derstanding.
We then test the performance of our model
on part-of-speech tagging, named entity
recognition, sentiment analysis, semantic
relation identification and semantic relat-
edness, controlling for embedding dimen-
sionality. We find that multi-sense embed-
dings do improve performance on some
tasks (part-of-speech tagging, semantic re-
lation identification, semantic relatedness)
but not on others (named entity recogni-
tion, various forms of sentiment analysis).
We discuss how these differences may be
caused by the different role of word sense
information in each of the tasks. The re-
sults highlight the importance of testing
embedding models in real applications.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999293">
Enriching vector models of word meaning so
they can represent multiple word senses per word
type seems to offer the potential to improve
many language understanding tasks. Most tra-
ditional embedding models associate each word
</bodyText>
<author confidence="0.707532">
Dan Jurafsky
</author>
<affiliation confidence="0.842914">
Computer Science Department
Stanford University
Stanford, CA 94305, USA
</affiliation>
<email confidence="0.984902">
jurafsky@stanford.edu
</email>
<bodyText confidence="0.99992764">
type with a single embedding (e.g., Bengio et al.
(2006)). Thus the embedding for homonymous
words like bank (with senses including ‘sloping
land’ and ‘financial institution’) is forced to rep-
resent some uneasy central tendency between the
various meanings. More fine-grained embeddings
that represent more natural regions in semantic
space could thus improve language understanding.
Early research pointed out that embeddings
could model aspects of word sense (Kintsch,
2001) and recent research has proposed a number
of models that represent each word type by dif-
ferent senses, each sense associated with a sense-
specific embedding (Kintsch, 2001; Reisinger and
Mooney, 2010; Neelakantan et al., 2014; Huang et
al., 2012; Chen et al., 2014; Pina and Johansson,
2014; Wu and Giles, 2015; Liu et al., 2015). Such
sense-specific embeddings have shown improved
performance on simple artificial tasks like match-
ing human word similarity judgments— WS353
(Rubenstein and Goodenough, 1965) or MC30
(Huang et al., 2012).
Incorporating multisense word embeddings into
general NLP tasks requires a pipelined architec-
ture that addresses three major steps:
</bodyText>
<listItem confidence="0.925485166666667">
1. Sense-specific representation learning:
learn word sense specific embeddings from a
large corpus, either unsupervised or aided by
external resources like WordNet.
2. Sense induction: given a text unit (a phrase,
sentence, document, etc.), infer word senses
for its tokens and associate them with corre-
sponding sense-specific embeddings.
3. Representation acquisition for phrases or
sentences: learn representations for text
units given sense-specific embeddings and
pass them to machine learning classifiers.
</listItem>
<bodyText confidence="0.838572">
Most existing work on multi-sense embeddings
emphasizes the first step by learning sense spe-
</bodyText>
<page confidence="0.928046">
1722
</page>
<note confidence="0.985124">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1722–1732,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9977552">
cific embeddings, but does not explore the next
two steps. These are important steps, however,
since it isn’t clear how existing multi-sense em-
beddings can be incorporated into and benefit real-
world NLU tasks.
We propose a pipelined architecture to address
all three steps and apply it to a variety of NLP
tasks: part-of-speech tagging, named entity recog-
nition, sentiment analysis, semantic relation iden-
tification and semantic relatedness. We find:
</bodyText>
<listItem confidence="0.904893157894737">
• Multi-sense embeddings give improved per-
formance in some tasks (e.g., semantic sim-
ilarity for words and sentences, seman-
tic relation identification part-of-speech tag-
ging), but not others (e.g., sentiment analysis,
named entity extraction). In our analysis we
offer some suggested explanations for these
differences.
• Some of the improvements for multi-sense
embeddings are no longer visible when us-
ing more sophisticated neural models like
LSTMs which have more flexibility in fil-
tering away the informational chaff from the
wheat.
• It is important to carefully compare against
embeddings of the same dimensionality.
• When doing so, the most straightforward way
to yield better performance on these tasks is
just to increase embedding dimensionality.
</listItem>
<bodyText confidence="0.999678166666667">
After describing related work, we introduce the
new unsupervised sense-learning model in section
3, give our sense-induction algorithm in section 4,
and then in following sections evaluate its perfor-
mance for word similarity, and then various NLP
tasks.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999838777777778">
Neural embedding learning frameworks represent
each token with a dense vector representation, op-
timized through predicting neighboring words or
decomposing co-occurrence matrices (Bengio et
al., 2006; Collobert and Weston, 2008; Mnih and
Hinton, 2007; Mikolov et al., 2013; Mikolov et al.,
2010; Pennington et al., 2014). Standard neural
models represent each word with a single unique
vector representation.
Recent work has begun to augment the neu-
ral paradigm to address the multi-sense problem
by associating each word with a series of sense
specific embeddings. The central idea is to aug-
ment standard embedding learning models like
skip-grams by disambiguating word senses based
on local co-occurrence— e.g., the fruit “apple”
tends to co-occur with the words “cider, tree, pear”
while the homophonous IT company co-occurs
with words like “iphone”, “Google” or “ipod”.
For example Reisinger and Mooney (2010) and
Huang et al. (2012) propose ways to develop mul-
tiple embeddings per word type by pre-clustering
the contexts of each token to create a fixed num-
ber of senses for each word, and then relabel-
ing each word token with the clustered sense be-
fore learning embeddings. Neelakantan et al.
(2014) extend these models by relaxing the as-
sumption that each word must have a fixed num-
ber of senses and using a non-parametric model
setting a threshold to decide when a new sense
cluster should be split off; Liu et al. (2015)
learns sense/topic specific embeddings by com-
bining neural frameworks with LDA topic mod-
els. Wu and Giles (2015) disambiguate sense em-
beddings from Wikipedia by first clustering wiki
documents. Chen et al. (2014) turn to external re-
sources and used a predefined inventory of senses,
building a distinct representation for every sense
defined by the Wordnet dictionary. Other rele-
vant work includes Qiu et al. (2014) who main-
tains separate representations for different part-of-
speech tags of the same word.
Recent work is mostly evaluated on the rela-
tively artificial task of matching human word sim-
ilarity judgments.
</bodyText>
<sectionHeader confidence="0.839332" genericHeader="method">
3 Learning Sense-Specific Embeddings
</sectionHeader>
<bodyText confidence="0.999981266666667">
We propose to build on this previous literature,
most specifically Huang et al. (2012) and Nee-
lakantan et al. (2014), to develop an algorithm
for learning multiple embeddings for each word
type, each embedding corresponding to a distinct
induced word sense. Such an algorithm should
have the property that a word should be associated
with a new sense vector just when evidence in the
context (e.g., neighboring words, document-level
co-occurrence statistics) suggests that it is suffi-
ciently different from its early senses. Such a line
of thinking naturally points to Chinese Restau-
rant Processes (CRP) (Blei et al., 2004; Teh et
al., 2006) which have been applied in the related
field of word sense induction. In the analogy of
</bodyText>
<page confidence="0.959327">
1723
</page>
<bodyText confidence="0.9999486">
CRP, the current word could either sit at one of
the existing tables (belonging to one of the exist-
ing senses) or choose a new table (a new sense).
The decision is made by measuring semantic re-
latedness (based on local context information and
global document information) and the number of
customers already sitting at that table (the popu-
larity of word senses). We propose such a model
and show that it improves over the state of the art
on a standard word similarity task.
</bodyText>
<subsectionHeader confidence="0.999812">
3.1 Chinese Restaurant Processes
</subsectionHeader>
<bodyText confidence="0.994147074074074">
We offer a brief overview of Chinese Restaurant
Processes in this section; readers interested in
more details can consult the original papers (Blei
et al., 2004; Teh et al., 2006; Pitman, 1995).
CRP can be viewed as a practical interpretation
of Dirichlet Processes (Ferguson, 1973) for non-
parametric clustering. In the analogy, each data
point is compared to a customer in a restaurant.
The restaurant has a series of tables t, each of
which serves a dish dt. This dish can be viewed as
the index of a cluster or a topic. The next customer
w to enter would either choose an existing table,
sharing the dish (cluster) already served or choos-
ing a new cluster based on the following probabil-
ity distribution:
As in the standard vector-space model, each to-
ken w is associated with a K dimensional global
embedding ew. Additionally, it is associated with
a set of senses Zw = {z1w, z2w, ..., z|Zw|
w } where
|Zw |denotes the number of senses discovered for
word w. Each sense z is associated with a distinct
sense-specific embedding ezw. When we encounter
a new token w in the text, at the first stage, we
maximize the probability of seeing the current to-
ken given its context as in standard language mod-
els using the global vector ew:
</bodyText>
<equation confidence="0.999815">
p(ew|eneigh) = F(ew, eneigh) (2)
</equation>
<bodyText confidence="0.956174230769231">
F() can take different forms in different learn-
ing paradigms, e.g., F = Hw�∈neigh p(ew, ewe)
for skip-gram or F = p(ew, g(ew)) for SENNA
(Collobert and Weston, 2008) and CBOW, where
g(eneigh) denotes a function that projects the con-
catenation of neighboring vectors to a vector with
the same dimension as ew for SENNA and the
bag-or-word averaging for CBOW (Mikolov et al.,
2013).
Unlike traditional one-word-one-vector frame-
works, eneigh includes sense information in addi-
tion to the global vectors for neighbors. eneigh can
therefore be written as2.
</bodyText>
<equation confidence="0.9794605">
γP(w|dnew) if t is new
(1)
</equation>
<bodyText confidence="0.998644833333333">
where Nt denotes the number of customers al-
ready sitting at table t and P(w|dt) denotes the
probability of assigning the current data point to
cluster dt. γ is the hyper parameter controlling the
preference for sitting at a new table.
CRPs exhibit a useful “rich get richer” prop-
erty because they take into account the popular-
ity of different word senses. They are also more
flexible than a simple threshold strategy for set-
ting up new clusters, due to the robustness intro-
duced by adopting the relative ratio of P(w|dt)
and P(w|dnew).
</bodyText>
<subsectionHeader confidence="0.9991865">
3.2 Incorporating CRP into Distributed
Language Models
</subsectionHeader>
<bodyText confidence="0.7463988">
We describe how we incorporate CRP into a stan-
dard distributed language model1.
1We omit details about training standard distributed mod-
els; see Collobert and Weston (2008) and Mikolov et al.
(2013).
</bodyText>
<equation confidence="0.866061">
eneigh = {en−k,,..., en−1, en+1, ..., en−k}
</equation>
<bodyText confidence="0.999976166666667">
Next we would use CRP to decide which sense
the current occurrence corresponds to, or construct
a new sense if it is a new meaning that we have not
encountered before. Based on CRP, the probabil-
ity that assigns the current occurrence to each of
the discovered senses or a new sense is given by:
</bodyText>
<equation confidence="0.939028333333333">
Pr(zw = z) ∝ { Nwz P(ezw|context)
if z already exists
γP(w|znew) if z is new
</equation>
<bodyText confidence="0.909742416666667">
where Nwz denotes the number of times already
assigned to sense z for token w. P(ezw|context)
denotes the probability that current occurrence be-
longing to (or generated by) sense z.
The algorithm for parameter update for the one
token predicting procedure is illustrated in Figure
2For models that predict succeeding words, sense labels
for preceding words have already been decided. For models
that predict words using both left and right contexts, the la-
bels for right-context words have not been decided yet. In
such cases we just use its global word vector to fill up the
position.
</bodyText>
<equation confidence="0.922605">
Pr( tw = ) t ∝ I NtP(w |dt) if t already exists
</equation>
<page confidence="0.926609">
1724
</page>
<listItem confidence="0.988376888888889">
1: Input: Token sequence {wn, wneigh}.
2: Update parameters involved in Equ (3)(4)
based on current word prediction.
3: Sample sense label z from CRP.
4: If a new sense label z is sampled:
5: - add z to Zwn
6: - ezwn = argmaxp(wn|zm)
7: else: update parameters involved based on
sampled sense label z.
</listItem>
<figureCaption confidence="0.975864">
Figure 1: Incorporating CRP into Neural Lan-
guage Models.
</figureCaption>
<bodyText confidence="0.99866064">
1: Line 2 shows parameter updating through pre-
dicting the occurrence of current token. Lines 4-6
illustrate the situation when a new word sense is
detected, in which case we would add the newly
detected sense z into Zwn. The vector representa-
tion ezw for the newly detected sense would be ob-
tained by maximizing the function p(ezw|context).
As we can see, the model performs word-sense
clustering and embedding learning jointly, each
one affecting the other. The prediction of the
global vector of the current token (line2) is based
on both the global and sense-specific embeddings
of its neighbors, as will be updated through pre-
dicting the current token. Similarly, once the sense
label is decided (line7), the model will adjust the
embeddings for neighboring words, both global
word vectors and sense-specific vectors.
Training We train embeddings using Giga-
word5 + Wikipedia2014. The training approach
is implemented using skip-grams (SG) (Mikolov
et al., 2013). We induced senses for the top
200,000 most frequent words (and used a unified
“unknown” token for other less-frequent tokens).
The window size is set to 11. We iterate three
times over the corpus.
</bodyText>
<sectionHeader confidence="0.952583" genericHeader="method">
4 Obtaining Word Representations for
NLU tasks
</sectionHeader>
<bodyText confidence="0.999978307692308">
Next we describe how we decide sense labels for
tokens in context. The scenario is treated as a in-
ference procedure for sense labels where all global
word embeddings and sense-specific embeddings
are kept fixed.
Given a document or a sentence, we have an
objective function with respect to sense labels
by multiplying Eq.2 over each containing token.
Computing the global optimum sense labeling—
in which every word gets an optimal sense label—
requires searching over the space of all senses for
all words, which can be expensive. We therefore
chose two simplified heuristic approaches:
</bodyText>
<listItem confidence="0.983183125">
• Greedy Search: Assign each token the lo-
cally optimum sense label and represent the
current token with the embedding associated
with that sense.
• Expectation: Compute the probability of
each possible sense for the current word, and
represent the word with the expectation vec-
tor:
</listItem>
<equation confidence="0.887916">
�ew = p(w|z, context) · ezw
z∈Zw
</equation>
<sectionHeader confidence="0.980372" genericHeader="method">
5 Word Similarity Evaluation
</sectionHeader>
<bodyText confidence="0.99997684375">
We evaluate our embeddings by comparing with
other multi-sense embeddings on the standard ar-
tificial task for matching human word similarity
judgments.
Early work used similarity datasets like WS353
(Finkelstein et al., 2001) or RG (Rubenstein and
Goodenough, 1965), whose context-free nature
makes them a poor evaluation. We therefore adopt
Stanford’s Contextual Word Similarities (SCWS)
(Huang et al., 2012), in which human judgments
are associated with pairs of words in context. Thus
for example “bank” in the context of “river bank”
would have low relatedness with “deficit” in the
context “financial deficit”.
We first use the Greedy or Expectation strate-
gies to obtain word vectors for tokens given their
context. These vectors are then used as input to get
the value of cosine similarity between two words.
Performances are reported in Table 1. Con-
sistent with earlier work (e.g.., Neelakantan
et al. (2014)), we find that multi-sense em-
beddings result in better performance in the
context-dependent SCWS task (SG+Greedy and
SG+Expect are better than SG). As expected, per-
formance is not as high when global level in-
formation is ignored when choosing word senses
(SG+Greedy) as when it is included (SG+Expect),
as neighboring words don’t provide sufficient in-
formation for word sense disambiguation.
To note, the proposed CRF models work a little
better than earlier baselines, which gives some ev-
idence that it is sufficiently strong to stand in for
</bodyText>
<page confidence="0.958662">
1725
</page>
<table confidence="0.9843695">
Model SCWS Correlation
SkipGram 66.4
SG+Greedy 69.1
SG+Expect 69.7
Chen 68.4
Neelakantan 69.3
</table>
<tableCaption confidence="0.982175">
Table 1: Performances for different set of multi-
</tableCaption>
<bodyText confidence="0.984240714285714">
sense embeddings (300d) evaluated on SCWS
by measuring the Spearman correlation between
each model’s similarity and the human judgments.
Baselines performances are reprinted from Nee-
lakantan et al. (2014) and Chen et al. (2014);
we report the best performance across all settings
mentioned in their paper.
this class of multi-sense models and serves as a
promise for being extended to NLU tasks.
Visualization Table 2 shows examples of se-
mantically related words given the local context.
Word embeddings for tokens are obtained by using
the inferred sense labels from the Greedy model
and are then used to search for nearest neighbors in
the vector space based on cosine similarity. Like
earlier models (e.g., Neelakantan et al. (2014)).,
the model can disambiguate different word senses
(in examples like bank, rock and apple) based on
their local context; although of course the model
is also capable of dealing with polysemy—senses
that are less distinct.
</bodyText>
<sectionHeader confidence="0.996639" genericHeader="method">
6 Experiments on NLP Tasks
</sectionHeader>
<bodyText confidence="0.99998">
Having shown that multi-sense embeddings im-
prove word similarity tasks, we turn to ask
whether they improve real-world NLU tasks: POS
tagging, NER tagging, sentiment analysis at the
phrase and sentence level, semantic relationship
identification and sentence-level semantic related-
ness. For each task, we experimented on the fol-
lowing sets of embeddings, which are trained us-
ing the word2vec package on the same corpus:
</bodyText>
<listItem confidence="0.9979708">
• Standard one-word-one-vector embeddings
from skip-gram (50d).
• Sense disambiguated embeddings from Sec-
tion 3 and 4 using Greedy Search and Expec-
tation (50d)
• The concatenation of global word embed-
dings and sense-specific embeddings (100d).
• Standard one-word-one-vector skip-gram
embeddings with dimensionality doubled
(100d) (100d is the correct corresponding
</listItem>
<bodyText confidence="0.950443322580646">
baseline since the concatenation above
doubles the dimensionality of word vectors)
• Embeddings with very high dimensionality
(300d).
As far as possible we try to perform an apple-
to-apple comparison on these tasks, and our goal
is an analytic one—to investigate how well se-
mantic information can be encoded in multi-sense
embeddings and how they can improve NLU
performances—rather than an attempt to create
state-of-the-art results. Thus for example, in tag-
ging tasks (e.g., NER, POS), we follow the proto-
cols in (Collobert et al., 2011) using the concate-
nation of neighboring embeddings as input fea-
tures rather than treating embeddings as auxiliary
features which are fed into a CRF model along
with other manually developed features as in Pen-
nington et al. (2014). Or for experiments on senti-
ment and other tasks where sentence level embed-
dings are required we only employ standard recur-
rent or recursive models for sentence embedding
rather than models with sophisticated state-of-the-
art methods (e.g., Tai et al. (2015; Irsoy and Cardie
(2014)).
Significance testing for comparing models is
done via the bootstrap test (Efron and Tibshirani,
1994). Unless otherwise noted, significant testing
is performed on one-word-one-vector embedding
(50d) versus multi-sense embedding using Expec-
tation inference (50d) and one-vector embedding
(100d) versus Expectation (100d).
</bodyText>
<subsectionHeader confidence="0.994426">
6.1 The Tasks
</subsectionHeader>
<bodyText confidence="0.998699588235294">
Named Entity Recognition We use the
CoNLL-2003 English benchmark for training,
and test on the CoNLL-2003 test data. We follow
the protocols in Collobert et al. (2011), using
the concatenation of neighboring embeddings as
input to a multi-layer neural model. We employ
a five-layer neural architecture, comprised of
an input layer, three convolutional layers with
rectifier linear activation function and a softmax
output layer. Training is done by gradient descent
with minibatches where each sentence is treated
as one batch. Learning rate, window size, number
of hidden units of hidden layers, L2 regulariza-
tions and number of iterations are tuned on the
development set.
Part-of-Speech Tagging We use Sections 0–18
of the Wall Street Journal (WSJ) data for train-
</bodyText>
<page confidence="0.739377">
1726
</page>
<bodyText confidence="0.9647445">
Context Nearest Neighbors
Apple is a kind of fruit. pear, cherry, mango, juice, peach, plum, fruit, cider, apples, tomato, orange, bean, pie
Apple releases its new ipads. microsoft, intel, dell, ipad, macintosh, ipod, iphone, google, computer, imac, hardware
He borrowed the money from banks. banking, credit, investment, finance, citibank, currency, assets, loads, imf, hsbc
along the shores of lakes, land, coast, river, waters, stream, inland, area, coasts, shoreline, shores, peninsula
banks of rivers
Basalt is the commonest volcanic rock. boulder, stone, rocks, sand, mud, limestone, volcanic, sedimentary, pelt, lava, basalt
Rock is the music of teenage rebellion. band, pop, bands, song, rap, album, jazz. blues, singer, hip-pop, songs, guitar, musician
</bodyText>
<tableCaption confidence="0.991502666666667">
Table 2: Nearest neighbors of words given context. The embeddings from context words are first in-
ferred with the Greedy strategy; nearest neighbors are computed by cosine similarity between word
embeddings. Similar phenomena have been observed in earlier work (Neelakantan et al., 2014)
</tableCaption>
<table confidence="0.998824">
Standard (50) Greedy (50) Expectation( 50)
0.852 0.852 (+0) 0.854 (+0.02)
Standard (100) Global+G (100) Global+E (100)
0.867 0.866 (-0.01) 0.871 (+0.04)
Standard (300)
0.882
</table>
<tableCaption confidence="0.847139">
Table 3: Accuracy for Different Models on
</tableCaption>
<bodyText confidence="0.864199">
Name Entity Recognition. Global+E stands
for Global+Expectation inference and Global+G
stands for Global+Greedy inference. p-value
0.223 for Standard(50) verse Expectation (50) and
0.310 for Standard(100) verse Expectation (100).
ing, sections 19–21 for validation and sections
22–24 for testing. Similar to NER, we trained 5-
layer neural models which take the concatenation
of neighboring embeddings as inputs. We adopt a
similar training and parameter tuning strategy as
for POS tagging.
</bodyText>
<table confidence="0.67754">
Standard (50) Greedy (50) Expectation (50)
0.925 0.934 (+0.09) 0.938 (+0.13)
Standard (100) Global+G (100) Global+E (100)
0.940 0.946 (+0.06) 0.952 (+0.12)
Standard (300)
0.954
</table>
<tableCaption confidence="0.774697333333333">
Table 4: Accuracy for Different Models on Part of
Speech Tagging. P-value 0.033 for 50d and 0.031
for 100d.
</tableCaption>
<bodyText confidence="0.990803105263158">
Sentence-level Sentiment Classification (Pang)
The sentiment dataset of Pang et al. (2002) con-
sists of movie reviews with a sentiment label for
each sentence. We divide the original dataset
into training(8101)/dev(500)/testing(2000). Word
embeddings are initialized using the aforemen-
tioned types of embeddings and kept fixed in the
learning procedure. Sentence level embeddings
are achieved by using standard sequence recur-
rent neural models (Pearlmutter, 1989) (for de-
tails, please refer to Appendix section). The ob-
tained embedding is then fed into a sigmoid clas-
sifier. Convolutional matrices at the word level are
randomized from [-0.1, 0.1] and learned from se-
quence models. For training, we adopt AdaGrad
with mini-batch. Parameters (i.e., L2 penalty,
learning rate and mini batch size) are tuned on
the development set. Due to space limitations, we
omit details of recurrent models and training.
</bodyText>
<equation confidence="0.980461333333333">
Standard (50) Greedy (50) Expectation (50)
0.750 0.752(+0.02) 0.750(+0.00)
Standard (100) Global+G (100) Global+E (100)
0.768 0.765(-0.03) 0.763(-0.05)
Standard (300)
0.774
</equation>
<bodyText confidence="0.940257375">
Table 5: Accuracy for Different Models on Sen-
timent Analysis (Pang et al.’s dataset). P-value
0.442 for 50d and 0.375 for 100d.
Sentiment Analysis–Stanford Treebank The
Stanford Sentiment Treebank (Socher et al., 2013)
contains gold-standard labels for each constituent
in the parse tree (phrase level), thus allowing us to
investigate a sentiment task at a finer granularity
than the dataset in Pang et al. (2002) where
labels are only found at the top of each sentence,
The sentences in the treebank were split into a
training(8544)/development(1101)/testing(2210)
dataset.
Following Socher et al. (2013) we obtained em-
beddings for tree nodes by using a recursive neu-
ral network model, where the embedding for par-
ent node is obtained in a bottom-up fashion based
on its children. The embeddings for each parse
tree constituent are output to a softmax layer; see
Socher et al. (2013).
We focus on the standard version of recursive
neural models. Again we fixed word embeddings
to each of the different embedding settings de-
scribed above3. Similarly, we adopted AdaGrad
</bodyText>
<footnote confidence="0.778143">
3Note that this is different from the settings used in
</footnote>
<page confidence="0.994224">
1727
</page>
<bodyText confidence="0.999357333333333">
with mini-batch. Parameters (i.e., L2 penalty,
learning rate and mini batch size) are tuned on
the development set. The number of iterations is
treated as a variable to tune and parameters are
harvested based on the best performance on the
development set.
</bodyText>
<table confidence="0.973478666666667">
Standard (50) Greedy (50) Expectation (50)
0.818 0.815 (-0.03) 0.820 (+0.02)
Standard (100) Global+G (100) Global+E (100)
0.838 0.840 (+0.02) 0.838 (+0.00)
Standard (300)
0.854
</table>
<tableCaption confidence="0.92139075">
Table 6: Accuracy for Different Models on Sen-
timent Analysis (binary classification on Stanford
Sentiment Treebank.). P-value 0.250 for 50d and
0.401 for 100d.
</tableCaption>
<sectionHeader confidence="0.462064" genericHeader="method">
Semantic Relationship Classification
</sectionHeader>
<bodyText confidence="0.999501">
SemEval-2010 Task 8 (Hendrickx et al., 2009)
is to find semantic relationships between pairs of
nominals, e.g., in “My [apartment]e1 has a pretty
large [kitchen]e2” classifying the relation between
[apartment] and [kitchen] as component-whole.
The dataset contains 9 ordered relationships, so
the task is formalized as a 19-class classifica-
tion problem, with directed relations treated as
separate labels; see Hendrickx et al. (2009) for
details.
We follow the recursive implementations de-
fined in Socher et al. (2012). The path in the parse
tree between the two nominals is retrieved, and the
embedding is calculated based on recursive mod-
els and fed to a softmax classifier. For pure com-
parison purpose, we only use embeddings as fea-
tures and do not explore other combination of ar-
tificial features. We adopt the same training strat-
egy as for the sentiment task (e.g., Adagrad, mini-
batches, etc).
</bodyText>
<equation confidence="0.900457666666667">
Standard (50) Greedy (50) Expectation (50)
0.748 0.760 (+0.12) 0.762 (+0.14)
Standard(100) Global+G (100) Global+E (100)
0.770 0.782 (+0.12) 0.778 (+0.18)
Standard(300)
0.798
</equation>
<figureCaption confidence="0.2830264">
Table 7: Accuracy for Different Models on Se-
mantic Relationship Identification. P-value 0.017
for 50d and 0.020 for 100d.
(Socher et al., 2013) where word vectors were treated as pa-
rameters to optimize.
</figureCaption>
<bodyText confidence="0.990888">
Sentence Semantic Relatedness We use the
Sentences Involving Compositional Knowledge
(SICK) dataset (Marelli et al., 2014) consist-
ing of 9927 sentence pairs, split into train-
ing(4500)/development(500)/Testing(4927). Each
sentence pair is associated with a gold-standard la-
bel ranging from 1 to 5, indicating how semanti-
cally related are the two sentences, from 1 (the two
sentences are unrelated) to 5 (the two are very re-
lated).
In our setting, the similarity between two sen-
tences is measured based on sentence-level em-
beddings. Let s1 and s2 denote two sentences
and esi and est denote corresponding embeddings.
esi and est are achieved through recurrent or re-
cursive models (as illustrated in Appendix sec-
tion). Again, word embeddings are obtained by
simple table look up in one-word-one-vector set-
tings and inferred using the Greedy or Expecta-
tion strategy in multi-sense settings. We adopt two
different recurrent models for acquiring sentence-
level embeddings, a standard recurrent model and
an LSTM model (Hochreiter and Schmidhuber,
1997).
The similarity score is predicted using a regres-
sion model built on the structure of a three layer
convolutional model, with concatenation of es1
and es2 as input, and a regression score from 1-
5 as output. We adopted the same training strat-
egy as described earlier. The trained model is then
used to predict the relatedness score between two
new sentences. Performance is measured using
Pearson’s r between the predicted score and gold-
standard labels.
</bodyText>
<equation confidence="0.919594833333333">
Standard( 50) Greedy (50) Expectation (50)
0.824 0.838(+0.14) 0.836(+0.12)
Standard (100) Global+G (100) Global+E (100)
0.835 0.840 (+0.05) 0.845 (+0.10)
Standard(300)
0.850
</equation>
<tableCaption confidence="0.913758666666667">
Table 8: Pearson’s r for Different Models on Se-
mantic Relatedness for Standard Models. P-value
0.028 for 50d and 0.042 for 100d.
</tableCaption>
<subsectionHeader confidence="0.99774">
6.2 Discussions
</subsectionHeader>
<bodyText confidence="0.999479666666667">
Results for different tasks are represented in Ta-
bles 3-9.
At first glance it seems that multi-sense em-
beddings do indeed offer superior performance,
since combining global vectors with sense-specific
vectors introduces a consistent performance boost
</bodyText>
<page confidence="0.845258">
1728
</page>
<equation confidence="0.740674">
Standard(50) Greedy(50) Expectation(50)
0.843 0.848 (+0.05) 0.846 (+0.03)
Standard(100) Global+G (100) Global+E (100)
0.850 0.853 (+0.03) 0.854 (+0.04)
Standard(300)
0.850
</equation>
<tableCaption confidence="0.647666">
Table 9: Pearson’s r for Different Models on Se-
mantic Relatedness for LSTM Models. P-value
0.145 for 50d and 0.170 for 100d.
</tableCaption>
<bodyText confidence="0.991984307692308">
for every task, when compared with the standard
(50d) setting. But of course this is an unfair
comparison; combining global vector with sense-
specific vector doubles the dimensionality of vec-
tor to 100, making comparison with standard di-
mensionality (50d) unfair. When comparing with
standard (100), the conclusions become more nu-
anced.
For every task, the +Expectation method has
performances that often seem to be higher than the
simple baseline (both for the 50d case or the 100d
case). However, only some of these differences are
significant.
</bodyText>
<listItem confidence="0.966568765957447">
(1) Using multi-sense embeddings is signifi-
cantly helpful for tasks like semantic relatedness
(Tables 7-8). This is sensible since sentence mean-
ing here is sensitive to the semantics of one partic-
ular word, which could vary with word sense and
which would directly be reflected on the related-
ness score.
(2) By contrast, for sentiment analysis (Tables
5-6), much of the task depends on correctly identi-
fying a few sentiment words like “good” or “bad”,
whose senses tend to have similar sentiment val-
ues, and hence for which multi-sense embeddings
offer little help. Multi-sense embeddings might
promise to help sentiment analysis for some cases,
like disambiguating the word “sound” in “safe and
sound” versus “movie sound”. But we suspect that
such cases are not common, explaining the non-
significance of the improvement. Furthermore, the
advantages of neural models in sentiment analysis
tasks presumably lie in their capability to capture
local composition like negation, and it’s not clear
how helpful multi-sense embeddings are for that
aspect.
(3) Similarly, multi-sense embeddings help for
POS tagging, but not for NER tagging (Table 3-4).
Word senses have long been known to be related
to POS tags. But the largest proportion of NER
tags consists of the negative not-a-NER (“O”) tag,
each of which is likely correctly labelable regard-
less of whether senses are disambiguated or not
(since presumably if a word is not a named entity,
most of its senses are not named entities either).
(4) As we apply more sophisticated models like
LSTM to semantic relatedness tasks (in Table 9),
the advantages caused by multi-sense embeddings
disappears.
(5) Doubling the number of dimensions is suf-
ficient to increase performance as much as using
the complex multi-sense algorithm. (Of course in-
creasing vector dimensionality (to 300) boosts per-
formance even more, although at the significant
cost of exponentially increasing time complexity.)
We do larger one-word-one-vector embeddings do
so well? We suggest some hypotheses:
• though information about distinct senses is
encoded in one-word-one-vector embeddings
in a mixed and less structured way, we sus-
</listItem>
<bodyText confidence="0.901167826086957">
pect that the compositional nature of neural
models is able to separate the informational
chaff from the wheat and choose what infor-
mation to take up, bridging the gap between
single vector and multi-sense paradigms. For
models like LSTMs which are better at do-
ing such a job by using gates to control in-
formation flow, the difference between two
paradigms should thus be further narrowed,
as indeed we found.
• The pipeline model proposed in the work re-
quires sense-label inference (i.e., step 2). We
proposed two strategies: GREEDY and EX-
PECTATION, and found that GREEDY mod-
els perform worse than EXPECTATION, as
we might expect4. But even EXPECTATION
can be viewed as another form of one-word-
one-vector models, just one where different
senses are entangled but weighted to empha-
size the important ones. Again, this suggests
another cause for the strong relative perfor-
mance of larger-dimensioned one-word-one-
vector models.
</bodyText>
<sectionHeader confidence="0.999274" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99981425">
In this paper, we expand ongoing research into
multi-sense embeddings by first proposing a new
version based on Chinese restaurant processes that
achieves state of the art performance on simple
</bodyText>
<footnote confidence="0.998883">
4GREEDY models work in a more aggressive way and
likely make mistakes due to the non-global-optimum nature
and limited context information
</footnote>
<page confidence="0.995825">
1729
</page>
<bodyText confidence="0.999953666666667">
word similarity matching tasks. We then intro-
duce a pipeline system for incorporating multi-
sense embeddings into NLP applications, and ex-
amine multiple NLP tasks to see whether and
when multi-sense embeddings can introduce per-
formance boosts. Our results suggest that sim-
ply increasing the dimensionality of baseline
skip-gram embeddings is sometimes sufficient to
achieve the same performance wins that come
from using multi-sense embeddings. That is, the
most straightforward way to yield better perfor-
mance on these tasks is just to increase embedding
dimensionality.
Our results come with some caveats. In partic-
ular, our conclusions are based on the pipelined
system that we introduce, and other multi-sense
embedding systems (e.g., a more advanced sense
learning model or a better sense label model or
a completely different pipeline system) may find
stronger effects of multi-sense models. Nonethe-
less we do consistently find improvements for
multi-sense embeddings in some tasks (part-of-
speech tagging and semantic relation identifica-
tion), suggesting the benefits of our multi-sense
models and those of others. Perhaps the most im-
portant implication of our results may be the ev-
idence they provide for the importance of going
beyond simple human-matching tasks, and testing
embedding models by using them as components
in real NLP applications.
</bodyText>
<sectionHeader confidence="0.998624" genericHeader="conclusions">
8 Appendix
</sectionHeader>
<bodyText confidence="0.999103214285714">
In sentiment classification and sentence seman-
tic relatedness tasks, classification models require
embeddings that represent the input at a sentence
or phrase level. We adopt recurrent networks
(standard ones or LSTMs) and recursive networks
in order to map a sequence of tokens with various
length to a vector representation.
Recurrent Networks A recurrent network suc-
cessively takes word wt at step t, combines its vec-
tor representation et with the previously built hid-
den vector ht−1 from time t − 1, calculates the re-
sulting current embedding ht, and passes it to the
next step. The embedding ht for the current time t
is thus:
</bodyText>
<equation confidence="0.941862">
ht = tanh(W · ht−1 + V · et) (5)
</equation>
<bodyText confidence="0.999951615384615">
where W and V denote compositional matrices. If
Ns denote the length of the sequence, hNs repre-
sents the whole sequence S.
Recursive Networks Standard recursive models
work in a similar way by working on neighbor-
ing words by parse tree order rather than sequence
order. They compute the representation for each
parent node based on its immediate children re-
cursively in a bottom-up fashion until reaching the
root of the tree. For a given node η in the tree
and its left child ηleft (with representation eleft) and
right child ηright (with representation eright), the
standard recursive network calculates eη:
</bodyText>
<equation confidence="0.987005">
eη = tanh(W · eηleft + V · eηright) (6)
</equation>
<bodyText confidence="0.995129153846154">
Long Short Term Memory (LSTM) LSTM
models (Hochreiter and Schmidhuber, 1997) are
defined as follows: given a sequence of inputs
X = {x1, x2,..., xnX }, an LSTM associates each
timestep with an input, memory and output gate,
respectively denoted as it, ft and ot. We nota-
tionally disambiguate e and h, where et denote the
vector for an individual text unit (e.g., word or sen-
tence) at time step t while ht denotes the vector
computed by the LSTM model at time t by com-
bining et and ht−1. σ denotes the sigmoid func-
tion. W E R4Kx2K. The vector representation ht
for each time-step t is given by:
</bodyText>
<equation confidence="0.604279285714286">
&amp;quot; it # &amp;quot; σ # &amp;quot; #
ft = σ ht−1
ot W · (7)
lt σ et
tanh
ct = ft · ct−1 + it · lt (8)
hst = ot · ct (9)
</equation>
<sectionHeader confidence="0.996748" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999949066666667">
We would like to thank Sam Bowman, Ignacio
Cases, Kevin Gu, Gabor Angeli, Sida Wang, Percy
Liang and other members of the Stanford NLP
group, as well as anonymous reviewers for their
helpful advice on various aspects of this work. We
gratefully acknowledge the support of the NSF via
award IIS-1514268, the Defense Advanced Re-
search Projects Agency (DARPA) Deep Explo-
ration and Filtering of Text (DEFT) Program un-
der Air Force Research Laboratory (AFRL) con-
tract no. FA8750-13-2-0040. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the views of NSF,
DARPA, AFRL, or the US government.
</bodyText>
<page confidence="0.986261">
1730
</page>
<sectionHeader confidence="0.99611" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998917235849056">
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.
David M. Blei, Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested chinese restaurant process.
Advances in neural information processing systems,
16.
Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1025–1035.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Bradley Efron and Robert J Tibshirani. 1994. An in-
troduction to the bootstrap. CRC press.
Thomas S Ferguson. 1973. A bayesian analysis of
some nonparametric problems. The annals of statis-
tics, pages 209–230.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In Proceedings of the 10th inter-
national conference on World Wide Web, pages 406–
414. ACM.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task
8: Multi-way classification of semantic relations
between pairs of nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 94–99.
Association for Computational Linguistics.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173–202.
Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong
Sun. 2015. Topical word embeddings. In Twenty-
Ninth AAAI Conference on Artificial Intelligence.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. SemEval-2014.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641–648. ACM.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
Barak A Pearlmutter. 1989. Learning state space tra-
jectories in recurrent neural networks. Neural Com-
putation, 1(2):263–269.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543.
Luis Nieto Pina and Richard Johansson. 2014. A sim-
ple and efficient method to generate word sense rep-
resentations. arXiv preprint arXiv:1412.6045.
Jim Pitman. 1995. Exchangeable and partially ex-
changeable random partitions. Probability theory
and related fields, 102(2):145–158.
</reference>
<page confidence="0.816227">
1731
</page>
<reference confidence="0.999706342857143">
Lin Qiu, Yong Cao, Zaiqing Nie, and Yong Rui. 2014.
Learning word representation considering proximity
and ambiguity. In Twenty-Eighth AAAI Conference
on Artificial Intelligence.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In NAACL.
Herbert Rubenstein and John B Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627–633.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP.
Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical dirichlet
processes. Journal of the american statistical as-
sociation, 101(476).
Zhaohui Wu and C. Lee Giles. 2015. Sense-aware se-
mantic analysis: A multi-prototype word representa-
tion model using wikipedia. In Twenty-Ninth AAAI
Conference on Artificial Intelligence.
</reference>
<page confidence="0.994229">
1732
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.618078">
<title confidence="0.99488">Do Multi-Sense Embeddings Improve Natural Language Understanding?</title>
<author confidence="0.934963">Jiwei</author>
<affiliation confidence="0.8361935">Computer Science Stanford</affiliation>
<address confidence="0.995692">Stanford, CA 94305,</address>
<email confidence="0.999799">jiweil@stanford.edu</email>
<abstract confidence="0.999671393939394">Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while ‘multi-sense’ methods have been proposed and tested on artificial wordsimilarity tasks, we don’t know if they improve real natural language understanding tasks. In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language understanding. We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness, controlling for embedding dimensionality. We find that multi-sense embeddings do improve performance on some tasks (part-of-speech tagging, semantic relation identification, semantic relatedness) but not on others (named entity recognition, various forms of sentiment analysis). We discuss how these differences may be caused by the different role of word sense information in each of the tasks. The results highlight the importance of testing embedding models in real applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer.</publisher>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process. Advances in neural information processing systems,</title>
<date>2004</date>
<pages>16</pages>
<contexts>
<context position="7930" citStr="Blei et al., 2004" startWordPosition="1188" endWordPosition="1191">opose to build on this previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense. Such an algorithm should have the property that a word should be associated with a new sense vector just when evidence in the context (e.g., neighboring words, document-level co-occurrence statistics) suggests that it is sufficiently different from its early senses. Such a line of thinking naturally points to Chinese Restaurant Processes (CRP) (Blei et al., 2004; Teh et al., 2006) which have been applied in the related field of word sense induction. In the analogy of 1723 CRP, the current word could either sit at one of the existing tables (belonging to one of the existing senses) or choose a new table (a new sense). The decision is made by measuring semantic relatedness (based on local context information and global document information) and the number of customers already sitting at that table (the popularity of word senses). We propose such a model and show that it improves over the state of the art on a standard word similarity task. 3.1 Chinese </context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>David M. Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. 2004. Hierarchical topic models and the nested chinese restaurant process. Advances in neural information processing systems, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxiong Chen</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
</authors>
<title>A unified model for word sense representation and disambiguation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1025--1035</pages>
<contexts>
<context position="2537" citStr="Chen et al., 2014" startWordPosition="363" endWordPosition="366">ncluding ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1. Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet. 2. Sense induction: given a </context>
<context position="6866" citStr="Chen et al. (2014)" startWordPosition="1020" endWordPosition="1023">ts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (2015) disambiguate sense embeddings from Wikipedia by first clustering wiki documents. Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary. Other relevant work includes Qiu et al. (2014) who maintains separate representations for different part-ofspeech tags of the same word. Recent work is mostly evaluated on the relatively artificial task of matching human word similarity judgments. 3 Learning Sense-Specific Embeddings We propose to build on this previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning mul</context>
<context position="16577" citStr="Chen et al. (2014)" startWordPosition="2636" endWordPosition="2639">t), as neighboring words don’t provide sufficient information for word sense disambiguation. To note, the proposed CRF models work a little better than earlier baselines, which gives some evidence that it is sufficiently strong to stand in for 1725 Model SCWS Correlation SkipGram 66.4 SG+Greedy 69.1 SG+Expect 69.7 Chen 68.4 Neelakantan 69.3 Table 1: Performances for different set of multisense embeddings (300d) evaluated on SCWS by measuring the Spearman correlation between each model’s similarity and the human judgments. Baselines performances are reprinted from Neelakantan et al. (2014) and Chen et al. (2014); we report the best performance across all settings mentioned in their paper. this class of multi-sense models and serves as a promise for being extended to NLU tasks. Visualization Table 2 shows examples of semantically related words given the local context. Word embeddings for tokens are obtained by using the inferred sense labels from the Greedy model and are then used to search for nearest neighbors in the vector space based on cosine similarity. Like earlier models (e.g., Neelakantan et al. (2014))., the model can disambiguate different word senses (in examples like bank, rock and apple)</context>
</contexts>
<marker>Chen, Liu, Sun, 2014</marker>
<rawString>Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A unified model for word sense representation and disambiguation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025–1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5451" citStr="Collobert and Weston, 2008" startWordPosition="786" endWordPosition="789">n doing so, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality. After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks. 2 Related Work Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation. Recent work has begun to augment the neural paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs</context>
<context position="9982" citStr="Collobert and Weston, 2008" startWordPosition="1551" endWordPosition="1554">bedding ew. Additionally, it is associated with a set of senses Zw = {z1w, z2w, ..., z|Zw| w } where |Zw |denotes the number of senses discovered for word w. Each sense z is associated with a distinct sense-specific embedding ezw. When we encounter a new token w in the text, at the first stage, we maximize the probability of seeing the current token given its context as in standard language models using the global vector ew: p(ew|eneigh) = F(ew, eneigh) (2) F() can take different forms in different learning paradigms, e.g., F = Hw�∈neigh p(ew, ewe) for skip-gram or F = p(ew, g(ew)) for SENNA (Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013). Unlike traditional one-word-one-vector frameworks, eneigh includes sense information in addition to the global vectors for neighbors. eneigh can therefore be written as2. γP(w|dnew) if t is new (1) where Nt denotes the number of customers already sitting at table t and P(w|dt) denotes the probability of assigning the current data point to cluster dt. γ is the hyper parameter contro</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="18660" citStr="Collobert et al., 2011" startWordPosition="2959" endWordPosition="2962">ram embeddings with dimensionality doubled (100d) (100d is the correct corresponding baseline since the concatenation above doubles the dimensionality of word vectors) • Embeddings with very high dimensionality (300d). As far as possible we try to perform an appleto-apple comparison on these tasks, and our goal is an analytic one—to investigate how well semantic information can be encoded in multi-sense embeddings and how they can improve NLU performances—rather than an attempt to create state-of-the-art results. Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014). Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)). Significance testing for comparing models is done via the bootstrap test (Efron and Ti</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An introduction to the bootstrap.</title>
<date>1994</date>
<publisher>CRC press.</publisher>
<contexts>
<context position="19275" citStr="Efron and Tibshirani, 1994" startWordPosition="3057" endWordPosition="3060">t al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014). Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)). Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994). Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d). 6.1 The Tasks Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data. We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model. We employ a five-layer neural architecture, comprised of an input layer, three convolutional layers with re</context>
</contexts>
<marker>Efron, Tibshirani, 1994</marker>
<rawString>Bradley Efron and Robert J Tibshirani. 1994. An introduction to the bootstrap. CRC press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A bayesian analysis of some nonparametric problems. The annals of statistics,</title>
<date>1973</date>
<pages>209--230</pages>
<contexts>
<context position="8833" citStr="Ferguson, 1973" startWordPosition="1345" endWordPosition="1346">suring semantic relatedness (based on local context information and global document information) and the number of customers already sitting at that table (the popularity of word senses). We propose such a model and show that it improves over the state of the art on a standard word similarity task. 3.1 Chinese Restaurant Processes We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Blei et al., 2004; Teh et al., 2006; Pitman, 1995). CRP can be viewed as a practical interpretation of Dirichlet Processes (Ferguson, 1973) for nonparametric clustering. In the analogy, each data point is compared to a customer in a restaurant. The restaurant has a series of tables t, each of which serves a dish dt. This dish can be viewed as the index of a cluster or a topic. The next customer w to enter would either choose an existing table, sharing the dish (cluster) already served or choosing a new cluster based on the following probability distribution: As in the standard vector-space model, each token w is associated with a K dimensional global embedding ew. Additionally, it is associated with a set of senses Zw = {z1w, z2w</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S Ferguson. 1973. A bayesian analysis of some nonparametric problems. The annals of statistics, pages 209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="14969" citStr="Finkelstein et al., 2001" startWordPosition="2383" endWordPosition="2386">We therefore chose two simplified heuristic approaches: • Greedy Search: Assign each token the locally optimum sense label and represent the current token with the embedding associated with that sense. • Expectation: Compute the probability of each possible sense for the current word, and represent the word with the expectation vector: �ew = p(w|z, context) · ezw z∈Zw 5 Word Similarity Evaluation We evaluate our embeddings by comparing with other multi-sense embeddings on the standard artificial task for matching human word similarity judgments. Early work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and Goodenough, 1965), whose context-free nature makes them a poor evaluation. We therefore adopt Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context. Thus for example “bank” in the context of “river bank” would have low relatedness with “deficit” in the context “financial deficit”. We first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context. These vectors are then used as input to get the value of cosine similarity between two words. Performanc</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, pages 406– 414. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>94--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2009</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="27482" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="4298" endWordPosition="4301">e very related). In our setting, the similarity between two sentences is measured based on sentence-level embeddings. Let s1 and s2 denote two sentences and esi and est denote corresponding embeddings. esi and est are achieved through recurrent or recursive models (as illustrated in Appendix section). Again, word embeddings are obtained by simple table look up in one-word-one-vector settings and inferred using the Greedy or Expectation strategy in multi-sense settings. We adopt two different recurrent models for acquiring sentencelevel embeddings, a standard recurrent model and an LSTM model (Hochreiter and Schmidhuber, 1997). The similarity score is predicted using a regression model built on the structure of a three layer convolutional model, with concatenation of es1 and es2 as input, and a regression score from 1- 5 as output. We adopted the same training strategy as described earlier. The trained model is then used to predict the relatedness score between two new sentences. Performance is measured using Pearson’s r between the predicted score and goldstandard labels. Standard( 50) Greedy (50) Expectation (50) 0.824 0.838(+0.14) 0.836(+0.12) Standard (100) Global+G (100) Global+E (100) 0.835 0.840 (+0.05) 0.84</context>
<context position="35541" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="5575" endWordPosition="5578">resents the whole sequence S. Recursive Networks Standard recursive models work in a similar way by working on neighboring words by parse tree order rather than sequence order. They compute the representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree. For a given node η in the tree and its left child ηleft (with representation eleft) and right child ηright (with representation eright), the standard recursive network calculates eη: eη = tanh(W · eηleft + V · eηright) (6) Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2,..., xnX }, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot. We notationally disambiguate e and h, where et denote the vector for an individual text unit (e.g., word or sentence) at time step t while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1. σ denotes the sigmoid function. W E R4Kx2K. The vector representation ht for each time-step t is given by: &amp;quot; it # &amp;quot; σ # &amp;quot; # ft = σ ht−1 ot W · (7) lt σ et tanh ct = ft · ct−1 + it · lt (8) </context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2518" citStr="Huang et al., 2012" startWordPosition="359" endWordPosition="362"> bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1. Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet. 2. Sense </context>
<context position="6161" citStr="Huang et al. (2012)" startWordPosition="898" endWordPosition="901">4). Standard neural models represent each word with a single unique vector representation. Recent work has begun to augment the neural paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”. For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (</context>
<context position="7394" citStr="Huang et al. (2012)" startWordPosition="1103" endWordPosition="1106">uate sense embeddings from Wikipedia by first clustering wiki documents. Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary. Other relevant work includes Qiu et al. (2014) who maintains separate representations for different part-ofspeech tags of the same word. Recent work is mostly evaluated on the relatively artificial task of matching human word similarity judgments. 3 Learning Sense-Specific Embeddings We propose to build on this previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense. Such an algorithm should have the property that a word should be associated with a new sense vector just when evidence in the context (e.g., neighboring words, document-level co-occurrence statistics) suggests that it is sufficiently different from its early senses. Such a line of thinking naturally points to Chinese Restaurant Processes (CRP) (Blei et al., 2004; Teh et al., 2006) which have been applied in the related field</context>
<context position="15153" citStr="Huang et al., 2012" startWordPosition="2409" endWordPosition="2412">that sense. • Expectation: Compute the probability of each possible sense for the current word, and represent the word with the expectation vector: �ew = p(w|z, context) · ezw z∈Zw 5 Word Similarity Evaluation We evaluate our embeddings by comparing with other multi-sense embeddings on the standard artificial task for matching human word similarity judgments. Early work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and Goodenough, 1965), whose context-free nature makes them a poor evaluation. We therefore adopt Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context. Thus for example “bank” in the context of “river bank” would have low relatedness with “deficit” in the context “financial deficit”. We first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context. These vectors are then used as input to get the value of cosine similarity between two words. Performances are reported in Table 1. Consistent with earlier work (e.g.., Neelakantan et al. (2014)), we find that multi-sense embeddings result in better performance in the context-dependent S</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="19171" citStr="Irsoy and Cardie (2014)" startWordPosition="3042" endWordPosition="3045">esults. Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014). Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)). Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994). Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d). 6.1 The Tasks Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data. We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model. We </context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Kintsch</author>
</authors>
<date>2001</date>
<journal>Predication. Cognitive Science,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2273" citStr="Kintsch, 2001" startWordPosition="321" endWordPosition="322">ing models associate each word Dan Jurafsky Computer Science Department Stanford University Stanford, CA 94305, USA jurafsky@stanford.edu type with a single embedding (e.g., Bengio et al. (2006)). Thus the embedding for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP task</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Walter Kintsch. 2001. Predication. Cognitive Science, 25(2):173–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Zhiyuan Liu</author>
<author>Tat-Seng Chua</author>
<author>Maosong Sun</author>
</authors>
<title>Topical word embeddings.</title>
<date>2015</date>
<booktitle>In TwentyNinth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2602" citStr="Liu et al., 2015" startWordPosition="375" endWordPosition="378"> represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1. Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet. 2. Sense induction: given a text unit (a phrase, sentence, document, etc.), infer word senses</context>
<context position="6653" citStr="Liu et al. (2015)" startWordPosition="987" endWordPosition="990">mpany co-occurs with words like “iphone”, “Google” or “ipod”. For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (2015) disambiguate sense embeddings from Wikipedia by first clustering wiki documents. Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary. Other relevant work includes Qiu et al. (2014) who maintains separate representations for different part-ofspeech tags of the same word. Recent work is mostly evaluated on the relatively artificial task of matching human word simi</context>
</contexts>
<marker>Liu, Liu, Chua, Sun, 2015</marker>
<rawString>Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2015. Topical word embeddings. In TwentyNinth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<tech>SemEval-2014.</tech>
<contexts>
<context position="26558" citStr="Marelli et al., 2014" startWordPosition="4153" endWordPosition="4156">of artificial features. We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc). Standard (50) Greedy (50) Expectation (50) 0.748 0.760 (+0.12) 0.762 (+0.14) Standard(100) Global+G (100) Global+E (100) 0.770 0.782 (+0.12) 0.778 (+0.18) Standard(300) 0.798 Table 7: Accuracy for Different Models on Semantic Relationship Identification. P-value 0.017 for 50d and 0.020 for 100d. (Socher et al., 2013) where word vectors were treated as parameters to optimize. Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927). Each sentence pair is associated with a gold-standard label ranging from 1 to 5, indicating how semantically related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very related). In our setting, the similarity between two sentences is measured based on sentence-level embeddings. Let s1 and s2 denote two sentences and esi and est denote corresponding embeddings. esi and est are achieved through recurrent or recursive models (as illustrated in Appendix section). Again,</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="5496" citStr="Mikolov et al., 2013" startWordPosition="794" endWordPosition="797">etter performance on these tasks is just to increase embedding dimensionality. After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks. 2 Related Work Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation. Recent work has begun to augment the neural paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”</context>
<context position="10196" citStr="Mikolov et al., 2013" startWordPosition="1587" endWordPosition="1590">dding ezw. When we encounter a new token w in the text, at the first stage, we maximize the probability of seeing the current token given its context as in standard language models using the global vector ew: p(ew|eneigh) = F(ew, eneigh) (2) F() can take different forms in different learning paradigms, e.g., F = Hw�∈neigh p(ew, ewe) for skip-gram or F = p(ew, g(ew)) for SENNA (Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013). Unlike traditional one-word-one-vector frameworks, eneigh includes sense information in addition to the global vectors for neighbors. eneigh can therefore be written as2. γP(w|dnew) if t is new (1) where Nt denotes the number of customers already sitting at table t and P(w|dt) denotes the probability of assigning the current data point to cluster dt. γ is the hyper parameter controlling the preference for sitting at a new table. CRPs exhibit a useful “rich get richer” property because they take into account the popularity of different word senses. They are also more flexible than a simple th</context>
<context position="13566" citStr="Mikolov et al., 2013" startWordPosition="2157" endWordPosition="2160">we can see, the model performs word-sense clustering and embedding learning jointly, each one affecting the other. The prediction of the global vector of the current token (line2) is based on both the global and sense-specific embeddings of its neighbors, as will be updated through predicting the current token. Similarly, once the sense label is decided (line7), the model will adjust the embeddings for neighboring words, both global word vectors and sense-specific vectors. Training We train embeddings using Gigaword5 + Wikipedia2014. The training approach is implemented using skip-grams (SG) (Mikolov et al., 2013). We induced senses for the top 200,000 most frequent words (and used a unified “unknown” token for other less-frequent tokens). The window size is set to 11. We iterate three times over the corpus. 4 Obtaining Word Representations for NLU tasks Next we describe how we decide sense labels for tokens in context. The scenario is treated as a inference procedure for sense labels where all global word embeddings and sense-specific embeddings are kept fixed. Given a document or a sentence, we have an objective function with respect to sense labels by multiplying Eq.2 over each containing token. Com</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5474" citStr="Mnih and Hinton, 2007" startWordPosition="790" endWordPosition="793">tforward way to yield better performance on these tasks is just to increase embedding dimensionality. After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks. 2 Related Work Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation. Recent work has begun to augment the neural paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphon</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient nonparametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2498" citStr="Neelakantan et al., 2014" startWordPosition="355" endWordPosition="358"> for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1. Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources lik</context>
<context position="6429" citStr="Neelakantan et al. (2014)" startWordPosition="945" endWordPosition="948">ea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”. For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (2015) disambiguate sense embeddings from Wikipedia by first clustering wiki documents. Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary. Other </context>
<context position="15659" citStr="Neelakantan et al. (2014)" startWordPosition="2493" endWordPosition="2496">re makes them a poor evaluation. We therefore adopt Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context. Thus for example “bank” in the context of “river bank” would have low relatedness with “deficit” in the context “financial deficit”. We first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context. These vectors are then used as input to get the value of cosine similarity between two words. Performances are reported in Table 1. Consistent with earlier work (e.g.., Neelakantan et al. (2014)), we find that multi-sense embeddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG). As expected, performance is not as high when global level information is ignored when choosing word senses (SG+Greedy) as when it is included (SG+Expect), as neighboring words don’t provide sufficient information for word sense disambiguation. To note, the proposed CRF models work a little better than earlier baselines, which gives some evidence that it is sufficiently strong to stand in for 1725 Model SCWS Correlation SkipGram 66.4 SG+Greedy 69.1</context>
<context position="17085" citStr="Neelakantan et al. (2014)" startWordPosition="2719" endWordPosition="2722">y and the human judgments. Baselines performances are reprinted from Neelakantan et al. (2014) and Chen et al. (2014); we report the best performance across all settings mentioned in their paper. this class of multi-sense models and serves as a promise for being extended to NLU tasks. Visualization Table 2 shows examples of semantically related words given the local context. Word embeddings for tokens are obtained by using the inferred sense labels from the Greedy model and are then used to search for nearest neighbors in the vector space based on cosine similarity. Like earlier models (e.g., Neelakantan et al. (2014))., the model can disambiguate different word senses (in examples like bank, rock and apple) based on their local context; although of course the model is also capable of dealing with polysemy—senses that are less distinct. 6 Experiments on NLP Tasks Having shown that multi-sense embeddings improve word similarity tasks, we turn to ask whether they improve real-world NLU tasks: POS tagging, NER tagging, sentiment analysis at the phrase and sentence level, semantic relationship identification and sentence-level semantic relatedness. For each task, we experimented on the following sets of embedd</context>
<context position="21328" citStr="Neelakantan et al., 2014" startWordPosition="3359" endWordPosition="3362">waters, stream, inland, area, coasts, shoreline, shores, peninsula banks of rivers Basalt is the commonest volcanic rock. boulder, stone, rocks, sand, mud, limestone, volcanic, sedimentary, pelt, lava, basalt Rock is the music of teenage rebellion. band, pop, bands, song, rap, album, jazz. blues, singer, hip-pop, songs, guitar, musician Table 2: Nearest neighbors of words given context. The embeddings from context words are first inferred with the Greedy strategy; nearest neighbors are computed by cosine similarity between word embeddings. Similar phenomena have been observed in earlier work (Neelakantan et al., 2014) Standard (50) Greedy (50) Expectation( 50) 0.852 0.852 (+0) 0.854 (+0.02) Standard (100) Global+G (100) Global+E (100) 0.867 0.866 (-0.01) 0.871 (+0.04) Standard (300) 0.882 Table 3: Accuracy for Different Models on Name Entity Recognition. Global+E stands for Global+Expectation inference and Global+G stands for Global+Greedy inference. p-value 0.223 for Standard(50) verse Expectation (50) and 0.310 for Standard(100) verse Expectation (100). ing, sections 19–21 for validation and sections 22–24 for testing. Similar to NER, we trained 5- layer neural models which take the concatenation of neig</context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="22411" citStr="Pang et al. (2002)" startWordPosition="3516" endWordPosition="3519">1 for validation and sections 22–24 for testing. Similar to NER, we trained 5- layer neural models which take the concatenation of neighboring embeddings as inputs. We adopt a similar training and parameter tuning strategy as for POS tagging. Standard (50) Greedy (50) Expectation (50) 0.925 0.934 (+0.09) 0.938 (+0.13) Standard (100) Global+G (100) Global+E (100) 0.940 0.946 (+0.06) 0.952 (+0.12) Standard (300) 0.954 Table 4: Accuracy for Different Models on Part of Speech Tagging. P-value 0.033 for 50d and 0.031 for 100d. Sentence-level Sentiment Classification (Pang) The sentiment dataset of Pang et al. (2002) consists of movie reviews with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). Word embeddings are initialized using the aforementioned types of embeddings and kept fixed in the learning procedure. Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) (for details, please refer to Appendix section). The obtained embedding is then fed into a sigmoid classifier. Convolutional matrices at the word level are randomized from [-0.1, 0.1] and learned from sequence models. For train</context>
<context position="23812" citStr="Pang et al. (2002)" startWordPosition="3726" endWordPosition="3729">recurrent models and training. Standard (50) Greedy (50) Expectation (50) 0.750 0.752(+0.02) 0.750(+0.00) Standard (100) Global+G (100) Global+E (100) 0.768 0.765(-0.03) 0.763(-0.05) Standard (300) 0.774 Table 5: Accuracy for Different Models on Sentiment Analysis (Pang et al.’s dataset). P-value 0.442 for 50d and 0.375 for 100d. Sentiment Analysis–Stanford Treebank The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granularity than the dataset in Pang et al. (2002) where labels are only found at the top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset. Following Socher et al. (2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children. The embeddings for each parse tree constituent are output to a softmax layer; see Socher et al. (2013). We focus on the standard version of recursive neural models. Again we fixed word embeddings to each of the different embedding s</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barak A Pearlmutter</author>
</authors>
<title>Learning state space trajectories in recurrent neural networks.</title>
<date>1989</date>
<journal>Neural Computation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="22783" citStr="Pearlmutter, 1989" startWordPosition="3570" endWordPosition="3571">.946 (+0.06) 0.952 (+0.12) Standard (300) 0.954 Table 4: Accuracy for Different Models on Part of Speech Tagging. P-value 0.033 for 50d and 0.031 for 100d. Sentence-level Sentiment Classification (Pang) The sentiment dataset of Pang et al. (2002) consists of movie reviews with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). Word embeddings are initialized using the aforementioned types of embeddings and kept fixed in the learning procedure. Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) (for details, please refer to Appendix section). The obtained embedding is then fed into a sigmoid classifier. Convolutional matrices at the word level are randomized from [-0.1, 0.1] and learned from sequence models. For training, we adopt AdaGrad with mini-batch. Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set. Due to space limitations, we omit details of recurrent models and training. Standard (50) Greedy (50) Expectation (50) 0.750 0.752(+0.02) 0.750(+0.00) Standard (100) Global+G (100) Global+E (100) 0.768 0.765(-0.03) 0.763(-0.05) Standa</context>
</contexts>
<marker>Pearlmutter, 1989</marker>
<rawString>Barak A Pearlmutter. 1989. Learning state space trajectories in recurrent neural networks. Neural Computation, 1(2):263–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1532--1543</pages>
<contexts>
<context position="5544" citStr="Pennington et al., 2014" startWordPosition="802" endWordPosition="805">increase embedding dimensionality. After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks. 2 Related Work Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation. Recent work has begun to augment the neural paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”. For example Reisinger and Mooney (2010) and Hu</context>
<context position="18889" citStr="Pennington et al. (2014)" startWordPosition="2996" endWordPosition="3000"> possible we try to perform an appleto-apple comparison on these tasks, and our goal is an analytic one—to investigate how well semantic information can be encoded in multi-sense embeddings and how they can improve NLU performances—rather than an attempt to create state-of-the-art results. Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014). Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)). Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994). Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (1</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Nieto Pina</author>
<author>Richard Johansson</author>
</authors>
<title>A simple and efficient method to generate word sense representations. arXiv preprint arXiv:1412.6045.</title>
<date>2014</date>
<contexts>
<context position="2563" citStr="Pina and Johansson, 2014" startWordPosition="367" endWordPosition="370">and’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1. Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet. 2. Sense induction: given a text unit (a phrase, sente</context>
</contexts>
<marker>Pina, Johansson, 2014</marker>
<rawString>Luis Nieto Pina and Richard Johansson. 2014. A simple and efficient method to generate word sense representations. arXiv preprint arXiv:1412.6045.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
</authors>
<title>Exchangeable and partially exchangeable random partitions. Probability theory and related fields,</title>
<date>1995</date>
<pages>102--2</pages>
<contexts>
<context position="8744" citStr="Pitman, 1995" startWordPosition="1332" endWordPosition="1333">f the existing senses) or choose a new table (a new sense). The decision is made by measuring semantic relatedness (based on local context information and global document information) and the number of customers already sitting at that table (the popularity of word senses). We propose such a model and show that it improves over the state of the art on a standard word similarity task. 3.1 Chinese Restaurant Processes We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Blei et al., 2004; Teh et al., 2006; Pitman, 1995). CRP can be viewed as a practical interpretation of Dirichlet Processes (Ferguson, 1973) for nonparametric clustering. In the analogy, each data point is compared to a customer in a restaurant. The restaurant has a series of tables t, each of which serves a dish dt. This dish can be viewed as the index of a cluster or a topic. The next customer w to enter would either choose an existing table, sharing the dish (cluster) already served or choosing a new cluster based on the following probability distribution: As in the standard vector-space model, each token w is associated with a K dimensiona</context>
</contexts>
<marker>Pitman, 1995</marker>
<rawString>Jim Pitman. 1995. Exchangeable and partially exchangeable random partitions. Probability theory and related fields, 102(2):145–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Qiu</author>
<author>Yong Cao</author>
<author>Zaiqing Nie</author>
<author>Yong Rui</author>
</authors>
<title>Learning word representation considering proximity and ambiguity.</title>
<date>2014</date>
<booktitle>In Twenty-Eighth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7069" citStr="Qiu et al. (2014)" startWordPosition="1053" endWordPosition="1056"> relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (2015) disambiguate sense embeddings from Wikipedia by first clustering wiki documents. Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary. Other relevant work includes Qiu et al. (2014) who maintains separate representations for different part-ofspeech tags of the same word. Recent work is mostly evaluated on the relatively artificial task of matching human word similarity judgments. 3 Learning Sense-Specific Embeddings We propose to build on this previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense. Such an algorithm should have the property that a word should be associated with a new sense vector jus</context>
</contexts>
<marker>Qiu, Cao, Nie, Rui, 2014</marker>
<rawString>Lin Qiu, Yong Cao, Zaiqing Nie, and Yong Rui. 2014. Learning word representation considering proximity and ambiguity. In Twenty-Eighth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2472" citStr="Reisinger and Mooney, 2010" startWordPosition="351" endWordPosition="354"> (2006)). Thus the embedding for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1. Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided</context>
<context position="6137" citStr="Reisinger and Mooney (2010)" startWordPosition="893" endWordPosition="896">l., 2010; Pennington et al., 2014). Standard neural models represent each word with a single unique vector representation. Recent work has begun to augment the neural paradigm to address the multi-sense problem by associating each word with a series of sense specific embeddings. The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”. For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topi</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="2780" citStr="Rubenstein and Goodenough, 1965" startWordPosition="398" endWordPosition="401"> improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1. Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet. 2. Sense induction: given a text unit (a phrase, sentence, document, etc.), infer word senses for its tokens and associate them with corresponding sense-specific embeddings. 3. Representation acquisition for phrases or sentences: learn representations for text units give</context>
<context position="15009" citStr="Rubenstein and Goodenough, 1965" startWordPosition="2389" endWordPosition="2392"> heuristic approaches: • Greedy Search: Assign each token the locally optimum sense label and represent the current token with the embedding associated with that sense. • Expectation: Compute the probability of each possible sense for the current word, and represent the word with the expectation vector: �ew = p(w|z, context) · ezw z∈Zw 5 Word Similarity Evaluation We evaluate our embeddings by comparing with other multi-sense embeddings on the standard artificial task for matching human word similarity judgments. Early work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and Goodenough, 1965), whose context-free nature makes them a poor evaluation. We therefore adopt Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context. Thus for example “bank” in the context of “river bank” would have low relatedness with “deficit” in the context “financial deficit”. We first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context. These vectors are then used as input to get the value of cosine similarity between two words. Performances are reported in Table 1. Consistent w</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25677" citStr="Socher et al. (2012)" startWordPosition="4014" endWordPosition="4017"> Sentiment Treebank.). P-value 0.250 for 50d and 0.401 for 100d. Semantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2” classifying the relation between [apartment] and [kitchen] as component-whole. The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details. We follow the recursive implementations defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier. For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features. We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc). Standard (50) Greedy (50) Expectation (50) 0.748 0.760 (+0.12) 0.762 (+0.14) Standard(100) Global+G (100) Global+E (100) 0.770 0.782 (+0.12) 0.778 (+0.18) Standard(300) 0.798 Table 7: Accuracy for Different Models on Se</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="23616" citStr="Socher et al., 2013" startWordPosition="3694" endWordPosition="3697">dels. For training, we adopt AdaGrad with mini-batch. Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set. Due to space limitations, we omit details of recurrent models and training. Standard (50) Greedy (50) Expectation (50) 0.750 0.752(+0.02) 0.750(+0.00) Standard (100) Global+G (100) Global+E (100) 0.768 0.765(-0.03) 0.763(-0.05) Standard (300) 0.774 Table 5: Accuracy for Different Models on Sentiment Analysis (Pang et al.’s dataset). P-value 0.442 for 50d and 0.375 for 100d. Sentiment Analysis–Stanford Treebank The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granularity than the dataset in Pang et al. (2002) where labels are only found at the top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset. Following Socher et al. (2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children. The embeddings for each parse tree c</context>
<context position="26376" citStr="Socher et al., 2013" startWordPosition="4127" endWordPosition="4130">edding is calculated based on recursive models and fed to a softmax classifier. For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features. We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc). Standard (50) Greedy (50) Expectation (50) 0.748 0.760 (+0.12) 0.762 (+0.14) Standard(100) Global+G (100) Global+E (100) 0.770 0.782 (+0.12) 0.778 (+0.18) Standard(300) 0.798 Table 7: Accuracy for Different Models on Semantic Relationship Identification. P-value 0.017 for 50d and 0.020 for 100d. (Socher et al., 2013) where word vectors were treated as parameters to optimize. Sentence Semantic Relatedness We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927). Each sentence pair is associated with a gold-standard label ranging from 1 to 5, indicating how semantically related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very related). In our setting, the similarity between two sentences is measured based on sentence-level embeddings. Let s1 an</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</title>
<date>2015</date>
<contexts>
<context position="19146" citStr="Tai et al. (2015" startWordPosition="3038" endWordPosition="3041">state-of-the-art results. Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014). Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)). Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994). Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d). 6.1 The Tasks Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data. We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a mult</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the american statistical association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="7949" citStr="Teh et al., 2006" startWordPosition="1192" endWordPosition="1195">his previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense. Such an algorithm should have the property that a word should be associated with a new sense vector just when evidence in the context (e.g., neighboring words, document-level co-occurrence statistics) suggests that it is sufficiently different from its early senses. Such a line of thinking naturally points to Chinese Restaurant Processes (CRP) (Blei et al., 2004; Teh et al., 2006) which have been applied in the related field of word sense induction. In the analogy of 1723 CRP, the current word could either sit at one of the existing tables (belonging to one of the existing senses) or choose a new table (a new sense). The decision is made by measuring semantic relatedness (based on local context information and global document information) and the number of customers already sitting at that table (the popularity of word senses). We propose such a model and show that it improves over the state of the art on a standard word similarity task. 3.1 Chinese Restaurant Processe</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical dirichlet processes. Journal of the american statistical association, 101(476).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaohui Wu</author>
<author>C Lee Giles</author>
</authors>
<title>Sense-aware semantic analysis: A multi-prototype word representation model using wikipedia.</title>
<date>2015</date>
<booktitle>In Twenty-Ninth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2583" citStr="Wu and Giles, 2015" startWordPosition="371" endWordPosition="374">ution’) is forced to represent some uneasy central tendency between the various meanings. More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding. Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015). Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012). Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps: 1. Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet. 2. Sense induction: given a text unit (a phrase, sentence, document, etc.)</context>
<context position="6766" citStr="Wu and Giles (2015)" startWordPosition="1005" endWordPosition="1008"> et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings. Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models. Wu and Giles (2015) disambiguate sense embeddings from Wikipedia by first clustering wiki documents. Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary. Other relevant work includes Qiu et al. (2014) who maintains separate representations for different part-ofspeech tags of the same word. Recent work is mostly evaluated on the relatively artificial task of matching human word similarity judgments. 3 Learning Sense-Specific Embeddings We propose to build on this previous literature, most spec</context>
</contexts>
<marker>Wu, Giles, 2015</marker>
<rawString>Zhaohui Wu and C. Lee Giles. 2015. Sense-aware semantic analysis: A multi-prototype word representation model using wikipedia. In Twenty-Ninth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
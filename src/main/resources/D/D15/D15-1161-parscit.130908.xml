<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017913">
<title confidence="0.9988345">
Not All Contexts Are Created Equal:
Better Word Representations with Variable Attention
</title>
<author confidence="0.9945565">
Wang Ling Lin Chu-Cheng Yulia Tsvetkov Silvio Amir
Ram´on Fernandez Astudillo Chris Dyer Alan W Black Isabel Trancoso
</author>
<affiliation confidence="0.97042">
L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
Instituto Superior T´ecnico, Lisbon, Portugal
</affiliation>
<email confidence="0.7954215">
{lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu
{ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt
</email>
<sectionHeader confidence="0.979669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999966">
We introduce an extension to the bag-of-
words model for learning words represen-
tations that take into account both syn-
tactic and semantic properties within lan-
guage. This is done by employing an at-
tention model that finds within the con-
textual words, the words that are relevant
for each prediction. The general intuition
of our model is that some words are only
relevant for predicting local context (e.g.
function words), while other words are
more suited for determining global con-
text, such as the topic of the document.
Experiments performed on both semanti-
cally and syntactically oriented tasks show
gains using our model over the existing
bag of words model. Furthermore, com-
pared to other more sophisticated models,
our model scales better as we increase the
size of the context of the model.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908961538462">
Learning word representations using raw text data
have been shown to improve many NLP tasks,
such as part-of-speech tagging (Collobert et al.,
2011), dependency parsing (Chen and Manning,
2014; Kong et al., 2014) and machine transla-
tion (Liu et al., 2014; Kalchbrenner and Blunsom,
2013; Devlin et al., 2014; Sutskever et al., 2014).
These embeddings are generally learnt by defining
an objective function, which predicts words con-
ditioned on the context surrounding those words.
Once trained, these can be used as features (Turian
et al., 2010), as initializations of other neural net-
works (Hinton and Salakhutdinov, 2012; Erhan et
al., 2010; Guo et al., 2014).
The continuous bag-of-words (Mikolov et al.,
2013) is one of the many models that learns word
representations from raw textual data. While these
models are adequate for learning semantic fea-
tures, one of the problems of this model is the
lack of sensitivity for word order, which limits
their ability of learn syntactically motivated em-
beddings (Ling et al., 2015a; Bansal et al., 2014).
While models have been proposed to address this
problem, the complexity of these models (“Struc-
tured skip-n-gram” and “CWindow”) grows lin-
early as size of the window of words considered
increases, as a new set of parameters is created
for each relative position. On the other hand, the
continuous bag-of-words model requires no addi-
tional parameters as it builds the context repre-
sentation by summing over the embeddings in the
window and its performance is an order of magni-
tude higher than of other models.
In this work, we propose an extension to the
continuous bag-of-words model, which adds an at-
tention model that considers contextual words dif-
ferently depending on the word type and its rela-
tive position to the predicted word (distance to the
left/right). The main intuition behind our model
is that the prediction of a word is only dependent
on certain words within the context. For instance,
in the sentence We won the game! Nicely played!,
the prediction of the word played, depends on both
the syntactic relation from nicely, which narrows
down the list of candidates to verbs, and on the
semantic relation from game, which further nar-
rows down the list of candidates to verbs related
to games. On the other hand, the words we and
the add very little to this particular prediction. On
the other hand, the word the is important for pre-
dicting the word game, since it is generally fol-
lowed by nouns. Thus, we observe that the same
</bodyText>
<page confidence="0.938852">
1367
</page>
<note confidence="0.6531495">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1367–1372,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9993955">
word can be informative in some contexts and not
in others. In this case, distance is a key factor, as
the word the is informative to predict the immedi-
ate neighboring words, but not distance ones.
</bodyText>
<sectionHeader confidence="0.974615" genericHeader="method">
2 Attention-Based Continuous
Bag-of-words
</sectionHeader>
<subsectionHeader confidence="0.99908">
2.1 Continuous Bag-Of-Words (CBOW)
</subsectionHeader>
<bodyText confidence="0.999970266666667">
The work in (Mikolov et al., 2013) is frequently
used to learn word embeddings. It defines pro-
jection matrix W E Rd×|V  |where d is the em-
bedding dimension with the vocabulary V . These
parameters are optimized by by maximizing the
likelihood that words are predicted from their con-
text. Two models were defined, the skip-gram
model and the continuous bag-of-words model.
In this work, we focus on the continuous bag-
of-words model. The CBOW model predicts the
center word w0 given a representation of the sur-
rounding words w−b, ... , w−1, w1, wb, where b
is a hyperparameter defining the window of con-
text words. The context vector is obtained by
averaging the embeddings of each word c =
</bodyText>
<equation confidence="0.5256175">
1- wi and the prediction of the cen-
2b iE[—b,b]—{0}
</equation>
<bodyText confidence="0.999604">
ter word w0 is obtained by performing a softmax
over all the vocabulary V . More formally, define
the output matrix O E R|V |×dw, which maps the
context vector c into a |V |-dimensional vector rep-
resenting the predicted word, and maximizes the
following probability:
</bodyText>
<equation confidence="0.9596865">
exp v&gt;0 Oc
p(v0  |w[−b,b]−{0}) = Ev∈V exp v&gt;Oc (1)
</equation>
<bodyText confidence="0.999986025">
where Oc corresponds to the projection of the
context vector c onto the vocabulary V and v
is a one-hot representation. For larger vocabu-
laries it is inefficient to compute the normalizer
Ev∈V exp v&gt;Oc. Solutions for problem are us-
ing the hierarchical softmax objective function
(Mikolov et al., 2013) or resorting to negative
sampling to approximate the normalizer (Gold-
berg and Levy, 2014).
The continuous bag-of-words model differs
from other proposed models in the sense that its
complexity does not rise substantially as we in-
crease the window b, since it only requires two
extra additions to compute c, which correspond to
dw operations each. On the other hand, the skip-
n-gram model requires two extra predictions cor-
responding to dw x V operations each, which is
an order of magnitude more expensive even when
subsampling V . However, the drawback the bag-
of-words model is that it does not learn embed-
dings that are prone for learning syntactically ori-
ented tasks, mainly due to lack of sensitivity to
word order, since the context is defined by a sum
of surrounding words. Extensions are proposed
in (Ling et al., 2015a), where the sum if replaced
by the concatenation of the word embeddings in
the order these occur. However, this model does
not scale well as b increases as it requires V x dw
more parameters for each new word in the win-
dow.
Finally, setting a good value for b is difficult as
larger values may introduce a degenerative behav-
ior in the model, as more effort is spent predict-
ing words that are conditioned on unrelated words,
while smaller values of b may lead to cases where
the window size is not large enough include words
that are semantically related. For syntactic tasks,
it has been shown that increasing the window size
can adversely impact in the quality of the embed-
dings (Bansal et al., 2014; Lin et al., 2015).
</bodyText>
<subsectionHeader confidence="0.999458">
2.2 CBOW with Attention
</subsectionHeader>
<bodyText confidence="0.999984">
We present a solution to these problems while
maintaining the efficiency underlying the bag-of-
words model, and allowing it to consider contex-
tual words within the window in a non-uniform
way. We first rewrite the context window c as:
</bodyText>
<equation confidence="0.9700005">
�c = ai(wi)wi (2)
i∈[−b,b]−{0}
</equation>
<bodyText confidence="0.999496111111111">
where we replace the average of the word embed-
dings with a weighted sum of the individual word
embeddings within the context. That is, each word
is wi at relative position i is attributed an attention
level representing how much the attention model
believes this it is important to look at in order to
predict the center word. The attention ai(w) given
to word w E V at the relative position i is com-
puted as:
</bodyText>
<equation confidence="0.950205666666667">
exp kw,i + si
ai(w) = (3)
Ej∈[−b,b]−{0} exp kw,j + sj
</equation>
<bodyText confidence="0.999268571428572">
where K E R|V |×2b (with elements ki,j) is a set
of parameters that which determines the impor-
tance of each word type in each (relative) position,
s E R2b is a bias, which is conditioned only on
the relative position. As this is essentially a soft-
max over context words, the default bag-of-words
model can be seen as a special case of this model
</bodyText>
<page confidence="0.960723">
1368
</page>
<bodyText confidence="0.999979909090909">
where all parameters K and s are fixed at zero.
Computing the attention of all words in the input
requires 2b operations, as it simply requires re-
trieving one value from the lookup matrix K for
each word and one value from the bias s for each
word in the window. Considering that these mod-
els must be trainable on billions of tokens, effi-
ciency is paramount. Although more sophisticated
attentional models are certainly imaginable (Bah-
danau et al., 2014), ours is a good balance of com-
putational efficiency and modeling expressivity.
</bodyText>
<subsectionHeader confidence="0.998124">
2.3 Parameter Learning
</subsectionHeader>
<bodyText confidence="0.999885">
Gradients of the loss function with respect to
the parameters (W, O, K, s) are computed with
backpropagation, and parameters are updated after
each training instance using a fixed learning rate.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995018">
3.1 Word Vectors
</subsectionHeader>
<bodyText confidence="0.999912655172414">
We used a subsample from an English Wikipedia
dump1 containing 10 million documents, contain-
ing a total of 530 million tokens. We built word
embeddings using the original CBOW and our
proposed attentional model on this dataset.
In both cases, word vectors were constructed us-
ing window size b = 20, which enables us to cap-
ture longer-range dependencies between words.
We set the embedding size d,,, = 50 and used a
negative sampling rate of 10. Finally, the vocabu-
lary was reduced to words with more than 40 oc-
currences. In terms of computational speed, the
original bag-of-words implementation was able to
compute approximately 220k words per second,
while our model computes approximately 100k
words per second. The slowdown is tied to the
fact that we are computing the gradients, the atten-
tion model parameters, as well as the word embed-
dings. On the other hand, the skip-n-gram model
process words at only 10k words per second, as it
must predict every word in the window b.
Figure 1 illustrates the attention model for the
prediction of the word south in the sentence an-
tartica has little rainfall with the south pole mak-
ing it a continental desert. Darker cell indicate
higher attention values from a(i, w). We can ob-
serve that function words (has, the and a) tend to
be attributed very low attentions, as these are gen-
erally less predictive power. On the other hand,
</bodyText>
<footnote confidence="0.741317">
1Collected in September of 2014
</footnote>
<figureCaption confidence="0.99886">
Figure 1: Illustration of the inferred attention pa-
</figureCaption>
<bodyText confidence="0.9081132">
rameters for a sentence from our training data
when predicting the word south; darker cells in-
dicate higher weights.
content words, such as antartica, rainfall, conti-
nental and desert are attributed higher weights as
these words provide hints that the predicted word
is likely to be related to these words. Finally, the
word pole is assigned the highest attention as it
close to the predicted word, and there is a very
likely chance that south will precede pole.
</bodyText>
<subsectionHeader confidence="0.999207">
3.2 Syntax Evaluation
</subsectionHeader>
<bodyText confidence="0.999702461538462">
For syntax, we evaluate our embeddings in the
domain of part-of-speech tagging in both su-
pervised (Ling et al., 2015b) and unsupervised
tasks (Lin et al., 2015). This later task is newly
proposed, but we argue that success in it is a com-
pelling demonstration of separation of words into
syntactically coherent clusters.
Part-of-speech induction. The work in (Lin et
al., 2015) attempts to infer POS tags with a
standard bigram hmm, which uses word embed-
dings to infer POS tags without supervision. We
use the same dataset, obtained from the ConLL
2007 shared task (Nivre et al., 2007) Scoring is
performed using the V-measure (Rosenberg and
Hirschberg, 2007), which is used to predict syn-
tactic classes at the word level. It has been shown
in (Lin et al., 2015) that word embeddings learnt
from structured skip-ngrams tend to work better
at this task, mainly because it is less sensitive to
larger window sizes. These results are consistent
with our observations found in Table 1, in rows
“Skip-ngram” and “SSkip-ngram”. We can ob-
serve that our attention based CBOW model (row
“CBOW Attention”) improves over these results
for both tasks and also the original CBOW model
(row “CBOW”).
</bodyText>
<figure confidence="0.99067925">
antartica has little rainfall with the
pole
making
it
south
a
continental
desert
</figure>
<page confidence="0.982855">
1369
</page>
<table confidence="0.9994464">
POS Induction POS Tagging Sentiment Analysis
CBOW 50.40 97.03 71.99
Skip-ngram 33.86 97.19 72.10
SSkip-ngram 47.64 97.40 69.96
CBOW Attention 54.00 97.39 71.39
</table>
<tableCaption confidence="0.870381">
Table 1: Results for unsupervised POS induction, supervised POS tagging and Sentiment Analysis (one
per column) using different types of embeddings (one per row).
</tableCaption>
<bodyText confidence="0.997940066666667">
Part-of-speech tagging. The evaluation is per-
formed on the English PTB, with the standard
train (Sections 0-18), dev (Sections 19-21) and test
(Sections 22-24) splits. The model is trained with
the Bidirectional LSTM model presented in (Ling
et al., 2015b) using the same hyper-parameters.
Results on the POS accuracy on the test set are
reported on Table 1. We can observe our model
can obtain similar results compared to the struc-
tured skip-ngram model on this task, while train-
ing the model is significantly faster. The gap be-
tween the usage of different embeddings is not as
large as in POS induction, as this is a supervised
task, where pre-training generally leads to smaller
improvements.
</bodyText>
<subsectionHeader confidence="0.999093">
3.3 Semantic Evaluation
</subsectionHeader>
<bodyText confidence="0.999947733333333">
To evaluate the quality of our vectors in terms
of semantics, we use the sentiment analysis task
(Senti) (Socher et al., 2013), which is a binary
classification task for movie reviews. We sim-
ply use the mean of the word vectors of words
in a sentence, and use them as features in an E2-
regularized logistic regression classifier. We use
the standard training/dev/test split and report ac-
curacy on the test set in table 1.
We can see that in this task, our models do
not perform as well as the CBOW and Skipngram
model, which hints that our model is learning em-
beddings that learn more towards syntax. This is
expected as it is generally uncommon for embed-
dings to outperform existing models on both syn-
tactic and semantic tasks simultaneously, as em-
beddings tend to be either more semantically or
syntactically oriented. It is clear that the skipn-
gram model learns embeddings that are more se-
mantically oriented as it performs badly on all
syntactic tasks. The structured skip-ngram model
on the other hand performs badly on the syntactic
tasks, but we observe a large drop on this semanti-
cally oriented task. Our attention-based model, on
the other hand, out performs all other models on
syntax-based tasks, while maintaining a compet-
itive score on semantic tasks. This is an encour-
aging result that shows that it is possible to learn
representations that can perform well on both se-
mantic and syntactic tasks.
</bodyText>
<sectionHeader confidence="0.999915" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999877222222223">
Many methods have been proposed for learning
word representations. Earlier work learns em-
beddings using a recurrent language model (Col-
lobert et al., 2011), while several simpler and
more lightweight adaptations have been pro-
posed (Huang et al., 2012; Mikolov et al., 2013).
While most of the learnt vectors are semantically
oriented, work has been done in order to ex-
tend the model to learn syntactically oriented em-
beddings (Ling et al., 2015a). Attention models
are common in vision related tasks (Tang et al.,
2014), where models learn to pay attention to cer-
tain parts of a image in order to make accurate
predictions. This idea has been recently intro-
duced in many NLP tasks, such as machine trans-
lation (Bahdanau et al., 2014). In the area of word
representation learning, no prior work that uses at-
tention models exists to our knowledge.
</bodyText>
<sectionHeader confidence="0.996115" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999921785714286">
In this work, we presented an extension to the
CBOW model by introducing an attention model
to select relevant words within the context to make
more accurate predictions. As consequence, the
model learns representations that are both syntac-
tic and semantically motivated that do not degrade
with large window sizes, compared to the orig-
inal CBOW and skip-ngram models. Efficiency
is maintained by learning a position-based atten-
tion model, which can compute the attention of
surrounding words with a relatively small number
of operations. Finally, we show improvements on
syntactically oriented tasks, without degrading re-
sults significantly on semantically oriented tasks.
</bodyText>
<page confidence="0.992025">
1370
</page>
<sectionHeader confidence="0.98414" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999245">
The PhD thesis of Wang Ling is supported by
FCT grant SFRH/BD/51157/2010. This research
was supported in part by the U.S. Army Re-
search Laboratory, the U.S. Army Research Office
under contract/grant number W911NF-10-1-0533
and NSF IIS-1054319 and FCT through the pluri-
anual contract UID/CEC/50021/2013 and grant
number SFRH/BPD/68428/2010.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998154618556701">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, Baltimore, MD, USA, June.
Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learn-
ing Research, 11:625–660.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proceedings of
EMNLP.
Geoffrey E Hinton and Ruslan Salakhutdinov. 2012.
A better way to pretrain deep boltzmann machines.
In Advances in Neural Information Processing Sys-
tems, pages 2447–2455.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In EMNLP, pages
1700–1709.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proc. of EMNLP, pages 1001–1012,
Doha, Qatar, October.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2015. Unsupervised POS induction with
word embeddings. In Proceedings of NAACL.
Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015a. Two/too simple adaptations of
word2vec for syntax problems. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies. Association
for Computational Linguistics.
Wang Ling, Tiago Lu´ıis, Lu´ıis Marujo, R´amon Fer-
nandez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015b. Finding func-
tion in form: Compositional character models for
open vocabulary word representation. EMNLP.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of ACL, pages
1491–1500.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
915–932, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning(EMNLP-CoNLL), pages 410–
420.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
</reference>
<page confidence="0.814085">
1371
</page>
<reference confidence="0.999707444444444">
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. of EMNLP.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.
Yichuan Tang, Nitish Srivastava, and Ruslan R
Salakhutdinov. 2014. Learning generative models
with visual attention. In Advances in Neural Infor-
mation Processing Systems, pages 1808–1816.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 384–394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.994051">
1372
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.549395">
<title confidence="0.9978915">Not All Contexts Are Created Equal: Better Word Representations with Variable Attention</title>
<author confidence="0.9585345">Wang Ling Lin Chu-Cheng Yulia Tsvetkov Silvio Amir Ram´on Fernandez Astudillo Chris Dyer Alan W Black Isabel Trancoso</author>
<affiliation confidence="0.824051">Spoken Systems Lab, INESC-ID, Lisbon, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, Instituto Superior T´ecnico, Lisbon,</affiliation>
<abstract confidence="0.999671666666667">We introduce an extension to the bag-ofwords model for learning words representations that take into account both syntactic and semantic properties within language. This is done by employing an attention model that finds within the contextual words, the words that are relevant for each prediction. The general intuition of our model is that some words are only relevant for predicting local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2014</date>
<location>CoRR, abs/1409.0473.</location>
<contexts>
<context position="8740" citStr="Bahdanau et al., 2014" startWordPosition="1464" endWordPosition="1468">ned only on the relative position. As this is essentially a softmax over context words, the default bag-of-words model can be seen as a special case of this model 1368 where all parameters K and s are fixed at zero. Computing the attention of all words in the input requires 2b operations, as it simply requires retrieving one value from the lookup matrix K for each word and one value from the bias s for each word in the window. Considering that these models must be trainable on billions of tokens, efficiency is paramount. Although more sophisticated attentional models are certainly imaginable (Bahdanau et al., 2014), ours is a good balance of computational efficiency and modeling expressivity. 2.3 Parameter Learning Gradients of the loss function with respect to the parameters (W, O, K, s) are computed with backpropagation, and parameters are updated after each training instance using a fixed learning rate. 3 Experiments 3.1 Word Vectors We used a subsample from an English Wikipedia dump1 containing 10 million documents, containing a total of 530 million tokens. We built word embeddings using the original CBOW and our proposed attentional model on this dataset. In both cases, word vectors were constructe</context>
<context position="15487" citStr="Bahdanau et al., 2014" startWordPosition="2596" endWordPosition="2599">rent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Conclusions In this work, we presented an extension to the CBOW model by introducing an attention model to select relevant words within the context to make more accurate predictions. As consequence, the model learns representations that are both syntactic and semantically motivated that do not degrade with large window sizes, compared to the original CBOW and skip-ngram models. Efficiency is maintained by learning a position-based attention model, which can compute the attention o</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2359" citStr="Bansal et al., 2014" startWordPosition="355" endWordPosition="358">onditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. In this work, we propose an extension to the continuous bag-of-words model, which adds an</context>
<context position="7161" citStr="Bansal et al., 2014" startWordPosition="1179" endWordPosition="1182">der these occur. However, this model does not scale well as b increases as it requires V x dw more parameters for each new word in the window. Finally, setting a good value for b is difficult as larger values may introduce a degenerative behavior in the model, as more effort is spent predicting words that are conditioned on unrelated words, while smaller values of b may lead to cases where the window size is not large enough include words that are semantically related. For syntactic tasks, it has been shown that increasing the window size can adversely impact in the quality of the embeddings (Bansal et al., 2014; Lin et al., 2015). 2.2 CBOW with Attention We present a solution to these problems while maintaining the efficiency underlying the bag-ofwords model, and allowing it to consider contextual words within the window in a non-uniform way. We first rewrite the context window c as: �c = ai(wi)wi (2) i∈[−b,b]−{0} where we replace the average of the word embeddings with a weighted sum of the individual word embeddings within the context. That is, each word is wi at relative position i is attributed an attention level representing how much the attention model believes this it is important to look at </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<contexts>
<context position="1502" citStr="Chen and Manning, 2014" startWordPosition="215" endWordPosition="218"> local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual da</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1458" citStr="Collobert et al., 2011" startWordPosition="209" endWordPosition="212">t some words are only relevant for predicting local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that lea</context>
<context position="14909" citStr="Collobert et al., 2011" startWordPosition="2495" endWordPosition="2499">e structured skip-ngram model on the other hand performs badly on the syntactic tasks, but we observe a large drop on this semantically oriented task. Our attention-based model, on the other hand, out performs all other models on syntax-based tasks, while maintaining a competitive score on semantic tasks. This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="1617" citStr="Devlin et al., 2014" startWordPosition="235" endWordPosition="238">pic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack o</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dumitru Erhan</author>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pierre-Antoine Manzagol</author>
<author>Pascal Vincent</author>
<author>Samy Bengio</author>
</authors>
<title>Why does unsupervised pre-training help deep learning?</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--625</pages>
<contexts>
<context position="1952" citStr="Erhan et al., 2010" startWordPosition="287" endWordPosition="290">tions using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered incre</context>
</contexts>
<marker>Erhan, Bengio, Courville, Manzagol, Vincent, Bengio, 2010</marker>
<rawString>Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. 2010. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</title>
<date>2014</date>
<contexts>
<context position="5724" citStr="Goldberg and Levy, 2014" startWordPosition="924" endWordPosition="928">mally, define the output matrix O E R|V |×dw, which maps the context vector c into a |V |-dimensional vector representing the predicted word, and maximizes the following probability: exp v&gt;0 Oc p(v0 |w[−b,b]−{0}) = Ev∈V exp v&gt;Oc (1) where Oc corresponds to the projection of the context vector c onto the vocabulary V and v is a one-hot representation. For larger vocabularies it is inefficient to compute the normalizer Ev∈V exp v&gt;Oc. Solutions for problem are using the hierarchical softmax objective function (Mikolov et al., 2013) or resorting to negative sampling to approximate the normalizer (Goldberg and Levy, 2014). The continuous bag-of-words model differs from other proposed models in the sense that its complexity does not rise substantially as we increase the window b, since it only requires two extra additions to compute c, which correspond to dw operations each. On the other hand, the skipn-gram model requires two extra predictions corresponding to dw x V operations each, which is an order of magnitude more expensive even when subsampling V . However, the drawback the bagof-words model is that it does not learn embeddings that are prone for learning syntactically oriented tasks, mainly due to lack </context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting embedding features for simple semi-supervised learning.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1971" citStr="Guo et al., 2014" startWordPosition="291" endWordPosition="294"> data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set </context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi-supervised learning. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>A better way to pretrain deep boltzmann machines.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2447--2455</pages>
<contexts>
<context position="1932" citStr="Hinton and Salakhutdinov, 2012" startWordPosition="283" endWordPosition="286">duction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of wo</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E Hinton and Ruslan Salakhutdinov. 2012. A better way to pretrain deep boltzmann machines. In Advances in Neural Information Processing Systems, pages 2447–2455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15004" citStr="Huang et al., 2012" startWordPosition="2511" endWordPosition="2514">e a large drop on this semantically oriented task. Our attention-based model, on the other hand, out performs all other models on syntax-based tasks, while maintaining a competitive score on semantic tasks. This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Co</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="1596" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="231" endWordPosition="234">g global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of thi</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In EMNLP, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
<author>Archna Bhatia</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1001--1012</pages>
<location>Doha, Qatar,</location>
<contexts>
<context position="1522" citStr="Kong et al., 2014" startWordPosition="219" endWordPosition="222">ction words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these mode</context>
</contexts>
<marker>Kong, Schneider, Swayamdipta, Bhatia, Dyer, Smith, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A. Smith. 2014. A dependency parser for tweets. In Proc. of EMNLP, pages 1001–1012, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Cheng Lin</author>
<author>Waleed Ammar</author>
<author>Chris Dyer</author>
<author>Lori Levin</author>
</authors>
<title>Unsupervised POS induction with word embeddings.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="7180" citStr="Lin et al., 2015" startWordPosition="1183" endWordPosition="1186">ver, this model does not scale well as b increases as it requires V x dw more parameters for each new word in the window. Finally, setting a good value for b is difficult as larger values may introduce a degenerative behavior in the model, as more effort is spent predicting words that are conditioned on unrelated words, while smaller values of b may lead to cases where the window size is not large enough include words that are semantically related. For syntactic tasks, it has been shown that increasing the window size can adversely impact in the quality of the embeddings (Bansal et al., 2014; Lin et al., 2015). 2.2 CBOW with Attention We present a solution to these problems while maintaining the efficiency underlying the bag-ofwords model, and allowing it to consider contextual words within the window in a non-uniform way. We first rewrite the context window c as: �c = ai(wi)wi (2) i∈[−b,b]−{0} where we replace the average of the word embeddings with a weighted sum of the individual word embeddings within the context. That is, each word is wi at relative position i is attributed an attention level representing how much the attention model believes this it is important to look at in order to predict</context>
<context position="11172" citStr="Lin et al., 2015" startWordPosition="1875" endWordPosition="1878"> our training data when predicting the word south; darker cells indicate higher weights. content words, such as antartica, rainfall, continental and desert are attributed higher weights as these words provide hints that the predicted word is likely to be related to these words. Finally, the word pole is assigned the highest attention as it close to the predicted word, and there is a very likely chance that south will precede pole. 3.2 Syntax Evaluation For syntax, we evaluate our embeddings in the domain of part-of-speech tagging in both supervised (Ling et al., 2015b) and unsupervised tasks (Lin et al., 2015). This later task is newly proposed, but we argue that success in it is a compelling demonstration of separation of words into syntactically coherent clusters. Part-of-speech induction. The work in (Lin et al., 2015) attempts to infer POS tags with a standard bigram hmm, which uses word embeddings to infer POS tags without supervision. We use the same dataset, obtained from the ConLL 2007 shared task (Nivre et al., 2007) Scoring is performed using the V-measure (Rosenberg and Hirschberg, 2007), which is used to predict syntactic classes at the word level. It has been shown in (Lin et al., 2015</context>
</contexts>
<marker>Lin, Ammar, Dyer, Levin, 2015</marker>
<rawString>Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori Levin. 2015. Unsupervised POS induction with word embeddings. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Two/too simple adaptations of word2vec for syntax problems.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association</booktitle>
<contexts>
<context position="2336" citStr="Ling et al., 2015" startWordPosition="351" endWordPosition="354">ich predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. In this work, we propose an extension to the continuous bag-of-wor</context>
<context position="6459" citStr="Ling et al., 2015" startWordPosition="1052" endWordPosition="1055">substantially as we increase the window b, since it only requires two extra additions to compute c, which correspond to dw operations each. On the other hand, the skipn-gram model requires two extra predictions corresponding to dw x V operations each, which is an order of magnitude more expensive even when subsampling V . However, the drawback the bagof-words model is that it does not learn embeddings that are prone for learning syntactically oriented tasks, mainly due to lack of sensitivity to word order, since the context is defined by a sum of surrounding words. Extensions are proposed in (Ling et al., 2015a), where the sum if replaced by the concatenation of the word embeddings in the order these occur. However, this model does not scale well as b increases as it requires V x dw more parameters for each new word in the window. Finally, setting a good value for b is difficult as larger values may introduce a degenerative behavior in the model, as more effort is spent predicting words that are conditioned on unrelated words, while smaller values of b may lead to cases where the window size is not large enough include words that are semantically related. For syntactic tasks, it has been shown that</context>
<context position="11128" citStr="Ling et al., 2015" startWordPosition="1868" endWordPosition="1871">red attention parameters for a sentence from our training data when predicting the word south; darker cells indicate higher weights. content words, such as antartica, rainfall, continental and desert are attributed higher weights as these words provide hints that the predicted word is likely to be related to these words. Finally, the word pole is assigned the highest attention as it close to the predicted word, and there is a very likely chance that south will precede pole. 3.2 Syntax Evaluation For syntax, we evaluate our embeddings in the domain of part-of-speech tagging in both supervised (Ling et al., 2015b) and unsupervised tasks (Lin et al., 2015). This later task is newly proposed, but we argue that success in it is a compelling demonstration of separation of words into syntactically coherent clusters. Part-of-speech induction. The work in (Lin et al., 2015) attempts to infer POS tags with a standard bigram hmm, which uses word embeddings to infer POS tags without supervision. We use the same dataset, obtained from the ConLL 2007 shared task (Nivre et al., 2007) Scoring is performed using the V-measure (Rosenberg and Hirschberg, 2007), which is used to predict syntactic classes at the word l</context>
<context position="12860" citStr="Ling et al., 2015" startWordPosition="2147" endWordPosition="2150">king it south a continental desert 1369 POS Induction POS Tagging Sentiment Analysis CBOW 50.40 97.03 71.99 Skip-ngram 33.86 97.19 72.10 SSkip-ngram 47.64 97.40 69.96 CBOW Attention 54.00 97.39 71.39 Table 1: Results for unsupervised POS induction, supervised POS tagging and Sentiment Analysis (one per column) using different types of embeddings (one per row). Part-of-speech tagging. The evaluation is performed on the English PTB, with the standard train (Sections 0-18), dev (Sections 19-21) and test (Sections 22-24) splits. The model is trained with the Bidirectional LSTM model presented in (Ling et al., 2015b) using the same hyper-parameters. Results on the POS accuracy on the test set are reported on Table 1. We can observe our model can obtain similar results compared to the structured skip-ngram model on this task, while training the model is significantly faster. The gap between the usage of different embeddings is not as large as in POS induction, as this is a supervised task, where pre-training generally leads to smaller improvements. 3.3 Semantic Evaluation To evaluate the quality of our vectors in terms of semantics, we use the sentiment analysis task (Senti) (Socher et al., 2013), which </context>
<context position="15198" citStr="Ling et al., 2015" startWordPosition="2545" endWordPosition="2548">ntic tasks. This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Conclusions In this work, we presented an extension to the CBOW model by introducing an attention model to select relevant words within the context to make more accurate predictions. As consequenc</context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2015</marker>
<rawString>Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015a. Two/too simple adaptations of word2vec for syntax problems. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
</authors>
<title>Tiago Lu´ıis, Lu´ıis Marujo,</title>
<date>2015</date>
<publisher>EMNLP.</publisher>
<location>R´amon</location>
<marker>Ling, 2015</marker>
<rawString>Wang Ling, Tiago Lu´ıis, Lu´ıis Marujo, R´amon Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015b. Finding function in form: Compositional character models for open vocabulary word representation. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1491--1500</pages>
<contexts>
<context position="1564" citStr="Liu et al., 2014" startWordPosition="227" endWordPosition="230">ted for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic feat</context>
</contexts>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proceedings of ACL, pages 1491–1500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2023" citStr="Mikolov et al., 2013" startWordPosition="298" endWordPosition="301"> such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position.</context>
<context position="4311" citStr="Mikolov et al., 2013" startWordPosition="680" endWordPosition="683">important for predicting the word game, since it is generally followed by nouns. Thus, we observe that the same 1367 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1367–1372, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. word can be informative in some contexts and not in others. In this case, distance is a key factor, as the word the is informative to predict the immediate neighboring words, but not distance ones. 2 Attention-Based Continuous Bag-of-words 2.1 Continuous Bag-Of-Words (CBOW) The work in (Mikolov et al., 2013) is frequently used to learn word embeddings. It defines projection matrix W E Rd×|V |where d is the embedding dimension with the vocabulary V . These parameters are optimized by by maximizing the likelihood that words are predicted from their context. Two models were defined, the skip-gram model and the continuous bag-of-words model. In this work, we focus on the continuous bagof-words model. The CBOW model predicts the center word w0 given a representation of the surrounding words w−b, ... , w−1, w1, wb, where b is a hyperparameter defining the window of context words. The context vector is </context>
<context position="5634" citStr="Mikolov et al., 2013" startWordPosition="911" endWordPosition="914">0} ter word w0 is obtained by performing a softmax over all the vocabulary V . More formally, define the output matrix O E R|V |×dw, which maps the context vector c into a |V |-dimensional vector representing the predicted word, and maximizes the following probability: exp v&gt;0 Oc p(v0 |w[−b,b]−{0}) = Ev∈V exp v&gt;Oc (1) where Oc corresponds to the projection of the context vector c onto the vocabulary V and v is a one-hot representation. For larger vocabularies it is inefficient to compute the normalizer Ev∈V exp v&gt;Oc. Solutions for problem are using the hierarchical softmax objective function (Mikolov et al., 2013) or resorting to negative sampling to approximate the normalizer (Goldberg and Levy, 2014). The continuous bag-of-words model differs from other proposed models in the sense that its complexity does not rise substantially as we increase the window b, since it only requires two extra additions to compute c, which correspond to dw operations each. On the other hand, the skipn-gram model requires two extra predictions corresponding to dw x V operations each, which is an order of magnitude more expensive even when subsampling V . However, the drawback the bagof-words model is that it does not lear</context>
<context position="15027" citStr="Mikolov et al., 2013" startWordPosition="2515" endWordPosition="2518">is semantically oriented task. Our attention-based model, on the other hand, out performs all other models on syntax-based tasks, while maintaining a competitive score on semantic tasks. This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Conclusions In this work,</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>915--932</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP-CoNLL),</booktitle>
<pages>410--420</pages>
<contexts>
<context position="11670" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="1957" endWordPosition="1960"> embeddings in the domain of part-of-speech tagging in both supervised (Ling et al., 2015b) and unsupervised tasks (Lin et al., 2015). This later task is newly proposed, but we argue that success in it is a compelling demonstration of separation of words into syntactically coherent clusters. Part-of-speech induction. The work in (Lin et al., 2015) attempts to infer POS tags with a standard bigram hmm, which uses word embeddings to infer POS tags without supervision. We use the same dataset, obtained from the ConLL 2007 shared task (Nivre et al., 2007) Scoring is performed using the V-measure (Rosenberg and Hirschberg, 2007), which is used to predict syntactic classes at the word level. It has been shown in (Lin et al., 2015) that word embeddings learnt from structured skip-ngrams tend to work better at this task, mainly because it is less sensitive to larger window sizes. These results are consistent with our observations found in Table 1, in rows “Skip-ngram” and “SSkip-ngram”. We can observe that our attention based CBOW model (row “CBOW Attention”) improves over these results for both tasks and also the original CBOW model (row “CBOW”). antartica has little rainfall with the pole making it south a continental</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. 2007. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP-CoNLL), pages 410– 420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="13452" citStr="Socher et al., 2013" startWordPosition="2247" endWordPosition="2250">sented in (Ling et al., 2015b) using the same hyper-parameters. Results on the POS accuracy on the test set are reported on Table 1. We can observe our model can obtain similar results compared to the structured skip-ngram model on this task, while training the model is significantly faster. The gap between the usage of different embeddings is not as large as in POS induction, as this is a supervised task, where pre-training generally leads to smaller improvements. 3.3 Semantic Evaluation To evaluate the quality of our vectors in terms of semantics, we use the sentiment analysis task (Senti) (Socher et al., 2013), which is a binary classification task for movie reviews. We simply use the mean of the word vectors of words in a sentence, and use them as features in an E2- regularized logistic regression classifier. We use the standard training/dev/test split and report accuracy on the test set in table 1. We can see that in this task, our models do not perform as well as the CBOW and Skipngram model, which hints that our model is learning embeddings that learn more towards syntax. This is expected as it is generally uncommon for embeddings to outperform existing models on both syntactic and semantic tas</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="1642" citStr="Sutskever et al., 2014" startWordPosition="239" endWordPosition="242">Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word or</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yichuan Tang</author>
<author>Nitish Srivastava</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Learning generative models with visual attention.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1808--1816</pages>
<contexts>
<context position="15273" citStr="Tang et al., 2014" startWordPosition="2557" endWordPosition="2560">o learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Conclusions In this work, we presented an extension to the CBOW model by introducing an attention model to select relevant words within the context to make more accurate predictions. As consequence, the model learns representations that are both syntactic and semanticall</context>
</contexts>
<marker>Tang, Srivastava, Salakhutdinov, 2014</marker>
<rawString>Yichuan Tang, Nitish Srivastava, and Ruslan R Salakhutdinov. 2014. Learning generative models with visual attention. In Advances in Neural Information Processing Systems, pages 1808–1816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1855" citStr="Turian et al., 2010" startWordPosition="272" endWordPosition="275">better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Stru</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
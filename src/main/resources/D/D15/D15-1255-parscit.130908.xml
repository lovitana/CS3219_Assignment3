<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.995463">
Exploiting Debate Portals for Semi-Supervised Argumentation Mining
in User-Generated Web Discourse
</title>
<author confidence="0.953011">
Ivan Habernal† and Iryna Gurevych†‡
</author>
<affiliation confidence="0.919711">
†Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
‡Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research
</affiliation>
<email confidence="0.818133">
www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.995553" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999680227272727">
Analyzing arguments in user-generated
Web discourse has recently gained atten-
tion in argumentation mining, an evolving
field of NLP. Current approaches, which
employ fully-supervised machine learn-
ing, are usually domain dependent and
suffer from the lack of large and diverse
annotated corpora. However, annotating
arguments in discourse is costly, error-
prone, and highly context-dependent. We
asked whether leveraging unlabeled data
in a semi-supervised manner can boost
the performance of argument component
identification and to which extent is the
approach independent of domain and reg-
ister. We propose novel features that ex-
ploit clustering of unlabeled data from de-
bate portals based on a word embeddings
representation. Using these features, we
significantly outperform several baselines
in the cross-validation, cross-domain, and
cross-register evaluation scenarios.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999864111111111">
Argumentation mining, an evolving sub-field of
NLP, deals with analyzing argumentation1 in var-
ious genres, such as legal cases (Mochales and
Moens, 2011), student essays (Stab and Gurevych,
2014a), and medical and scientific articles (Green,
2014; Teufel and Moens, 2002). Recently, the fo-
cus of argumentation mining has also shifted to the
Web registers (such as comments to articles, forum
posts, or blogs) which is motivated by the need of
</bodyText>
<footnote confidence="0.667473571428571">
1Argumentation is a verbal activity for which the goal
consists of convincing the listener or reader of the acceptabil-
ity of a standpoint by means of a constellation ofpropositions
justifying or refuting the proposition expressed in the stand-
point (van Eemeren et al., 2002) or the art of persuading
others to think or act in a definite way, including all writing
and speaking which is persuasive inform (Ketcham, 1917).
</footnote>
<bodyText confidence="0.999567146341463">
retrieving and understanding ordinary people’s ar-
guments to various contentious topics on the large
scale. Applications include passenger rights and
protection (Park and Cardie, 2014), hotel reviews
(Wachsmuth et al., 2014), and controversies in ed-
ucation (Habernal et al., 2014).
Despite the plethora of existing argumentation
theories (van Eemeren et al., 2014), the preva-
lent view in argumentation mining treats argu-
ments as discourse structures consisting of sev-
eral argument components, such as claims and
premises (Peldszus and Stede, 2013). Current
approaches to automatic analysis of argumenta-
tion usually follow the fully supervised machine-
learning paradigm (Biran and Rambow, 2011;
Stab and Gurevych, 2014b; Park and Cardie,
2014) and rely on manually annotated datasets.
Only few publicly available argumentation cor-
pora exist, as annotations are costly, error-prone,
and require skilled human annotators (Stab and
Gurevych, 2014a; Habernal et al., 2014).
To overcome the limited scope and size of the
existing annotated corpora, semi-supervised meth-
ods can be adopted, as they gain performance by
exploiting large unlabeled datasets (Settles, 2012).
However, unlike in other NLP tasks where data
can be cheaply labeled using for example distant
supervision, employing such methods in argumen-
tation mining is questionable. First, argumenta-
tion is an act of persuasion (Nettel and Roque,
2011; Mercier and Sperber, 2011) but not all user-
generated texts can be treated as persuasive (Park
and Cardie, 2014; Habernal et al., 2014), thus the
selection of an appropriate unlabeled dataset rep-
resents a problem on its own. Second, argument
components (e.g., claims or premises) are highly
context-dependent and cannot be easily labeled
in distant data using predefined patterns. So far,
semi-supervised methods for argumentation min-
ing remain unexplored.
In this article, we tackle argumentation min-
</bodyText>
<page confidence="0.954256">
2127
</page>
<note confidence="0.984899">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2127–2137,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999452892857143">
ing of user-generated Web data by exploiting de-
bate portals—semi-structured discussion websites
where members pose contentious questions to the
community and allow others to pick a side and
provide their opinions and arguments in order to
‘win’ the debate.2 Our first research question
is whether debate portals (which contain noisy
user-generated data) can be utilized in a semi-
supervised manner for fine-grained identification
of argument components. As a second research
question, we investigate to what extent our meth-
ods are domain independent and evaluate their
adaptation across several domains and registers.
Our contribution is three-fold. First, to the best
of our knowledge, we present the first successful
attempt to semi-supervised argumentation mining
in Web data based on exploiting unlabeled exter-
nal resources. We leverage these resources and
derive features in an unsupervised manner by pro-
jecting data from debate portals into a latent argu-
ment space using unsupervised word embeddings
and clustering. Second, our novel features sig-
nificantly outperform state-of-the-art features in
all scenarios, namely in cross-validation, cross-
domain evaluation, and cross-register evaluation.
Third, to ensure full reproducibility of our experi-
ments, we provide all data and source codes under
free licenses.3
</bodyText>
<sectionHeader confidence="0.999896" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999822421052632">
Analysis of argumentation has been an active topic
in numerous research areas, such as philosophy
(van Eemeren et al., 2014), communication studies
(Mercier and Sperber, 2011), and informal logic
(Blair, 2004), among others. In this section, we
will focus on the most related works on argumen-
tation mining techniques in NLP in the first part,
with an emphasis on Web data in the second part.
Mochales and Moens (2011) based their work
on argumentation schemes (Walton et al., 2008)
and experimented with Araucaria and ECHR
datasets using supervised models to classify ar-
gumentative and non-argumentative sentences (≈
0.7F1) and their structure. Feng and Hirst (2011)
classified argument schemes on the Araucaria
dataset, reaching 0.6-0.9 accuracy. Experiments
on this dataset were also conducted by Rooney et
al. (2012), who classified sentences to four cate-
gories (conclusion, premise, conclusion-premise,
</bodyText>
<footnote confidence="0.9988175">
2For instance createdebate.com or debate.org
3https://github.com/habernal/emnlp2015
</footnote>
<bodyText confidence="0.99970246">
and none) and achieved 0.65 accuracy. These
approaches assume the text is already segmented
into argument components. Stab and Gurevych
(2014b) examined argumentation in persuasive
essays and classified argument components into
four categories (premise, claim, major claim, non-
argumentative) using SVM and achieved 0.73
macro F1 score. They further classified argu-
ment relations (support and attack) and reached
0.72 macro F1 score. The best-performing fea-
tures were structural features (such as the location
or length ratios), as persuasive essays usually com-
ply with a certain structure which can be seen as a
potential drawback of this approach.
Regarding user-generated Web data, Biran and
Rambow (2011) used naive Bayes for classifying
justification of subjective claims from blogs and
Wikipedia talk pages, relying on features from
RST Treebank and manually-processed n-grams.
In similar Web registers, Rosenthal and McK-
eown (2012) automatically determined whether
a sentence is a claim using logistic regression
and various lexical and sentiment-related features
and achieved accuracy about 0.66-0.71. Park
and Cardie (2014) classified propositions in user
comments into three classes (verifiable experi-
ential, verifiable non-experiential, and unverifi-
able) using SVM and reached 0.69 macro F1
score. Goudas et al. (2014) identified premises in
Greek social media texts using BIO encoding and
achieved 0.42 F1 score with Conditional Random
Fields. The research gaps in the above-mentioned
approaches are the following. First, the argumen-
tation models are simplified to either claims or a
few types of premises/propositions. Second, the
segmentation of discourse into argument compo-
nents is ignored (except the work of Goudas et al.
(2014)). Recently, Boltuˇzi´c and ˇSnajder (2015)
employed hierarchical clustering to cluster argu-
ments in online debates using embeddings projec-
tion, but in contrast to our work they performed
only intrinsic evaluation of the clusters.
Debate portals have been used in a related body
of research, such as classifying support and attack
between posts by Cabrio and Villata (2012), or
stance detection by Hasan and Ng (2013) or Got-
tipati et al. (2013). These approaches consider
the complete documents (posts) but do not ana-
lyze the micro-level argumentation (e.g., claims or
premises).
</bodyText>
<page confidence="0.987894">
2128
</page>
<bodyText confidence="0.897226636363636">
Doc #2823 (article comment, public-private-schools): [claim: I agree - Kids can do great in the public school system and
parents DO need to be involved.] The more people leave, the worse its going to become. [premise: The public school system
lets them deal with real life too, unfortunate that it may be but that is what’s out there in college and the work force too.]
[premise: There are still great teachers in the public schools - lets stand behind them.]
Doc #2224 (forumpost, single-sex-education): [backing: . . I went to an all boys school –]. [claim: Can’t say I particularly liked
it, I would of much preferred gone to a co-ed.] [premise: It is closer to the ’real world’ that way. Kids should grow up in the
company of both sexes... They will be more at ease around the opposite sex when they are older and it just makes sense.] If it
is purely education you are concerned about (and not so much behaviour), our year (at a private school) went shockingly bad
in OP scores. We were the worst in 12 years and were beaten by LOTS of co-ed and public schools... So you can never tell. In
saying that my sister really enjoyed going to an all girls school. Her year went really well too. Ask your daughters what they
would prefer... [backing: Btw,. I work at.a co-ed school at the moment and the kids there get on just fine.]
</bodyText>
<figureCaption confidence="0.994845">
Figure 1: Two examples of argument annotation of an article comment and a forum post.
</figureCaption>
<sectionHeader confidence="0.994914" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999834171428571">
As data for training and evaluation of our methods,
we use a corpus consisting of 340 English docu-
ments (approx. 90k tokens) annotated4 with argu-
mentation by Habernal et al. (2014). Compared to
other corpora mentioned in the related work, this
corpus is the largest one to date that covers dif-
ferent domains and spans several registers of user-
generated Web content. In particular, the corpus
comprises four registers (comments to articles, fo-
rum posts, blogs, and argumentative newswire ar-
ticles) and covers six domains related to educatio-
nal controversies (homeschooling, private vs. pub-
lic schools, mainstreaming, single-sex education,
prayer in schools, and redshirting).
The argumentation model used in this corpus
is based on extended Toulmin’s model (Toulmin,
1958). Each document contains usually one ar-
gument, where each argument consists of several
argument components. There are five different
components in this model, namely, the claim (the
statement about to be established in the argument
which conveys author’s stance towards the topic),
the premise(s) (propositions that are intended to
give reasons of some kind for the claim), the back-
ing (additional information used to back-up the
argument), the rebuttal (attacks the claim), and
the refutation (which attacks the rebuttal). Rela-
tions between the argument components are en-
coded implicitly in the function of the particular
component type, for instance, premises are always
attached to the claim. We made two observations
in the data: the claim is often implicit (must be
inferred by the reader), and some sentences have
no argumentative function (thus are not labeled by
any argument component).5
</bodyText>
<footnote confidence="0.9969215">
4Available at www.ukp.tu-darmstadt.de/data/
argumentation-mining/
5A publication containing a thorough analysis of the
dataset is pending.
</footnote>
<bodyText confidence="0.93690675">
Figure 1 depicts two example annotations from
the corpus. Argument components were annotated
on the token level as non-overlapping annotation
spans. We therefore represent the argument anno-
tations using BIO encoding. Each token is labeled
with one of the 11 categories (5 argument com-
ponent types × B or I tag + one O category for
non-argumentative text).
</bodyText>
<sectionHeader confidence="0.99304" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.937637533333334">
We cast the task of identifying argument compo-
nents as a sequence tagging problem and employ
SVMhmm (Joachims et al., 2009).6 For linguis-
tic annotations and feature engineering, we rely
on two UIMA-based frameworks – DKProCore
(Eckart de Castilho and Gurevych, 2014) and
DKProTC (Daxenberger et al., 2014).
Although the argument component annotations
in the corpus are aligned to the token boundaries
(token-level annotations), the minimal classifica-
tion unit in our sequence tagging approach is set
to the sentence level. First, this allows us to cap-
ture rich features that are available for entire sen-
tences as opposed to the token level. Second, by
modeling sequences on the token level we would
lose the advantage of SVMhmm to estimate depen-
dencies between labels, as the label context is lim-
ited due to computational feasibility. On the token
level, the label sequences are rather static (long se-
quences with the same label), as opposed to the
sentence level. Before the classification step, we
adjust all annotation boundaries (note that we use
11 BIO labels) so that they are aligned to the sen-
tence boundaries and each sentence is then treated
as a single classification unit with one label (for
example, the first sentence from Figure 1 with
token labels Claim-B, Claim-I, Claim-I, ... be-
6Keerthi and Sundararajan (2007) conclude that perfor-
mance of SVMhmm is comparable to another widely used
method, Conditional Random Fields (Lafferty et al., 2001)
</bodyText>
<page confidence="0.984939">
2129
</page>
<bodyText confidence="0.9996465">
comes Claim-B). After classification, the labels
are mapped back to tokens (so that, for example,
Claim-B sentence label is transformed to Claim-
B, Claim-I, ... token labels). However, all eval-
uations are performed on the token level and the
performance is always measured against the orig-
inal token labels. Using this approximation, we
lose only about 10% of F1 performance.7
</bodyText>
<subsectionHeader confidence="0.994482">
4.1 Baseline features
</subsectionHeader>
<bodyText confidence="0.9998242">
Lexical baseline (FS0) We encode the presence
of unigrams, bigrams, and trigrams in the sentence
as ‘one-hot’ (binary) features.
Structural and syntactic features (FS1) Since
the presence of discourse markers has been shown
to be helpful in argument component analysis (e.g,
“therefore” and “since” for premises or “think”
and “believe” for claims), we encode the first
and last three words as binary features. Further-
more, we capture the relative position of the sen-
tence in the paragraph and the document, the num-
ber of part of speech 1-3 grams, maximum de-
pendency tree depth, constituency tree produc-
tion rules, and number of sub-clauses (Stab and
Gurevych, 2014b). We used Stanford POS Tagger
(Toutanova et al., 2003), Berkeley parser (Petrov
et al., 2006), and Malt parser (Nivre, 2009).
Sentiment and topic features (FS2) We as-
sume that claims express sentiment, thus we com-
pute five sentiment categories (from very nega-
tive to very positive) using Stanford sentiment an-
alyzer (Socher et al., 2013) and use these values
directly as features. Furthermore, in order to help
detecting off-topic and non-argument sentences,
we employ topic model features. In particular, we
use features taken from a vector representation of
the sentence obtained by using Gibbs sampling on
LDA model (Blei et al., 2003; McCallum, 2002)
with topics trained on unlabeled data provided as
a part of the corpus.8
Semantic and discourse features (FS3) Fea-
tures based on semantic frames has been intro-
duced in relevant works on stance recognition
(Hasan and Ng, 2013). Our features, based on
PropBank semantic role labels and obtained from
</bodyText>
<footnote confidence="0.9159906">
7In only 1% of the sentences there are two or more argu-
ment components in it; we arbitrarily choose the largest one.
8The number of topics was empirically set to 30, therefore
for each sentence the topic distribution results into 30 real-
valued features.
</footnote>
<bodyText confidence="0.998594411764706">
NLP Semantic Role Labeler (Choi, 2012), ex-
tract various semantic information (agent, predi-
cate + agent, predicate + agent + patient + (op-
tional) negation, argument type + argument value)
and discourse markers. Discourse relations also
play an important role in argumentation analysis
(Cabrio et al., 2013). We thus employ binary fea-
tures (such as the presence of the sentence in a
chain, the transition type, the distance to previ-
ous/next sentences in the chain, or the number
of inter-sentence coreference links) obtained from
Stanford Coreference Chain Resolver (Lee et al.,
2013). Furthermore, we include features result-
ing from a PTDB-style discourse parser (Li et al.,
2012), such as the type of discourse relation (ex-
plicit, implicit), the presence of discourse connec-
tives, and attributions.
</bodyText>
<subsectionHeader confidence="0.98503">
4.2 Unsupervised features
</subsectionHeader>
<bodyText confidence="0.997696375">
We enrich the above-mentioned features by uti-
lizing external large unlabeled resources – debate
portals. They fulfill several criteria, namely (a)
they are ‘argumentative’ (meant as opposed to,
for example, prose or encyclopedic genres), (b)
they are comprised of user-generated content and
(c) and there is at least some overlap with topics
from our experimental corpus. On the other hand,
they contain noisy texts of questionable quality
and they do not provide any specific argumentative
structure (in fact, these debates are simple discus-
sions to a topic, where each post is only labeled
with a pro or contra stance). Nevertheless, we as-
sume that the posts from (unlabeled) debate por-
tals contain valuable information that will help us
with classifying argument components in labeled
data. In order to do so, we employ clustering based
on latent semantics, which we now formalize as
argument space features.
We assume that phrases (sentences or docu-
ments) can be projected into a latent vector space,
using, typically, a sum or a weighted average of
all the word embeddings vectors in the phrase; see
for example (Le and Mikolov, 2014). Neighbor-
ing vectors in the latent vector space exhibit some
interesting properties, such as semantic similarity
(thoroughly studied within the distributional se-
mantics area). If the latent vector space is clus-
tered, each n-dimensional vector gets reduced to
a single cluster number; such clusters have been
used directly as features in many tasks, such as
NER (Turian et al., 2010), POS tagging (Owoputi
</bodyText>
<page confidence="0.93629">
2130
</page>
<bodyText confidence="0.999958319148936">
et al., 2013), or sentiment analysis (Habernal and
Brychc´ın, 2013).
We build upon the above-mentioned approach
(described by Søgaard (2013) as ‘clusters-as-
features’ semi-supervised paradigm) and extend it
further. We take both sentences and posts from
the unlabeled debate portals, project them into a
latent space using word embeddings and cluster
them. The motivation is that these clusters will
contain similar phrases or (similar ‘arguments’).
Centroids of these clusters would then represent
a ‘prototypical argument’ (note that the centroids
exist only in the latent vector space and thus do
not correspond to any existing sentence or post).
Then we project each sentence (classification unit)
in the labeled data to the latent vector space, com-
pute its distance vector to all the cluster cen-
troids, and encode this distance vector directly as
real-valued features. By contrast to the above-
mentioned works using a single cluster label as
a feature, the distance vector to cluster centroids
resembles a soft labeling where each sentence be-
longs to several clusters with a certain ‘weight’.
We also use the latent vector space representation
of the sentence directly as a feature vector.
As unlabeled data, we use data from two largest
debate portals.9 As a pre-processing step we
removed all posts with less than one ‘point’
earned.10 The data were then indexed using the
Lucene framework and the top 100 debates for
each of the 6 domains were retrieved which re-
sulted into 5,759 posts (≈ 35k sentences) in the
unlabeled data in total. Our approach is formal-
ized in the following paragraph.
Argument space features (FS4) Let e(w) be
the embedding vector of word w and tfidf(w)
be the TD-IDF value of w. Sentence s� =
(w1, ... , wn) is then projected into the embed-
ding space E as se = Eni=1 tfidf(wi)e(wi)n−1
so dim(se) = dim(E). Analogically to s, we
project the entire post a� = (w1, ... , wm) to
the same embedding space E such that de =
Emi=1 tfidf(wi)—e(wi)m−1.
Let K be the number of sentence clusters in
E and ck a centroid vector of cluster k ∈ K.
Then sc denotes the distance of sentence se to
the sentence cluster centroids such that sc =
</bodyText>
<footnote confidence="0.950097">
9createdebate.com and convinceme.net, li-
censed under Creative Commons (CC-BY and CC0, resp.)
10‘Points’ is the sum of up-votes/down-votes by other users
to the particular post. Zero-point posts were usually noisy and
spam-like.
</footnote>
<equation confidence="0.9089885">
(cos(4,c1), ... , cos(se,ck)) where dim(Q =
K and cos(•, •) denotes cosine similarity. Ana-
</equation>
<bodyText confidence="0.9984906875">
logically, let L be the number of post clusters
in E and dl a centroid vector of cluster l ∈
L. Then sa denotes the distance of sentence
se to the post cluster centroids such that sa =
(cos(se,d1), ... , cos(se,al)). We construct the
feature vector by concatenating se, sc and sa.
For word embeddings, we use pre-trained skip-
gram word vectors11 produced by Mikolov et
al. (2013) (dim(E) = 300). To create clus-
ters for the argument space features, we used
CLUTO software package12 with Repeated Bi-
section clustering method (Zhao and Karypis,
2002). We clustered the data using different
hyper-parameters K and L (we experimented
with K = {50,100,500, 1000} and L =
{50,100, 500,1000}).
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999948137931034">
We investigate three evaluation scenarios. First,
we report 10-fold cross validation over all 340
documents, where the data are randomly dis-
tributed across the folds regardless of the domain
or register. In this scenario, the model can bene-
fit from domain-dependent features for the testing
data, such as lexical knowledge (FS0) or domain-
relevant argument space features (FS4). Second,
we evaluate the cross-domain performance; the
model is always trained on five domains and tested
on the sixth one. In this settings, we also re-
move all features that exploit distant data relevant
to the test set. For instance, if the test domain
is mainstreaming, we exclude all debates relevant
to this domain before constructing the argument
space features (FS4). This evaluates the model’s
cross-domain performance without any target do-
main data available. Finally, we test cross-register
performance in two set-ups: we train the models
using comments and forum posts and test on blogs
and newswire articles, and then the other way
round. We divided the data into these two parts
based on similar properties of blogs/articles and
comments/forums, such as the length, or the dis-
tribution of argumentative and non-argumentative
text.
In the evaluation, we focus on F1 scores
achieved on claims, premises, backing, and non-
argumentative text (the ‘O’ class). Although the
</bodyText>
<footnote confidence="0.9990435">
11https://code.google.com/p/word2vec/
12http://www.cs.umn.edu/˜karypis/cluto
</footnote>
<page confidence="0.851243">
2131
</page>
<table confidence="0.999926545454546">
FS B-B B-I C-B C-I O P-B P-I Avg
Human .664 .579 .739 .728 .833 .673 .736 .707
0 .154 .211 .118 .159 .718 .202 .272 .262
01* .237 .254 .167 .129 .671 .280 .356 .299
4* .194 .283 .225 .197 .715 .230 .292 .305
012* .258 .282 .189 .172 .685 .276 .359 .317
1234† .235 .315 .181 .145 .690 .290 .394 .321
0123* .313 .333 .152 .140 .691 .287 .372 .327
01234† .265 .332 .183 .167 .690 .314 .405 .337
34* .232 .344 .256 .235 .704 .269 .372 .345
234† .238 .339 .253 .227 .703 .291 .388 .348
</table>
<tableCaption confidence="0.845070666666667">
Table 1: Fl results for the 10-fold cross-validation scenario.
Feature set combination (the FS column) naming is explained
in Section 4.1. Class labels: B-B/I = Backing-B/I, C-B/I =
Claim-B/I, O = non-argumentative, P-B/I = Premise-B/I. Star
(*) denotes that the row is significantly better than the previ-
ous row; dagger (†) means the row is not significantly better
than the previous row, but is significantly better than the pre-
vious row minus one; p &lt; 0.001 using exact Liddell’s test
(Liddell, 1983).
</tableCaption>
<bodyText confidence="0.999458229508197">
classifier is trained and tested on all 11 classes in-
cluding rebuttal and refutation, we do not report
performance of these two argument components—
the results are very poor regardless of the param-
eters for two reasons. First, these classes are un-
derrepresented in the data (Rebuttal-B, Rebuttal-
I, Refutation-B and Refutation-I are present in
only about 4% of sentences). Second, the inter-
annotator agreement reached on these classes were
reported to be very low (Habernal et al., 2014).
Cross validation results Table 1 shows results
for the cross-validation scenario. The human base-
line in the first row is an average score between
three original annotators of the dataset. The base-
line features (FS0) perform poorly, yet they beat
the random assignment and majority vote (&lt; 0.12
F1). The argument space features (FS4) increase
the performance in every combination. The best
results for claims are achieved when only dis-
course, sentiment, and argument space features
are involved (FS3 and FS4), whereas premises and
backing benefit from the presence of lexical, syn-
tactic, and semantic features (the richest feature
set). The overall average best results are obtained
from a feature combination with higher level of
abstraction, in particular without low-level lexical
features from FS0.
After the cross validation experiments, we also
fixed the hyperparameters (using grid search) to
K = 1000, L = 100 for the cluster sizes and t = 1
and e = 0 for the hyperparameters of SVMhmm .
Cross-domain results For each domain, the
cross-domain results are shown in Table 2. On
average, the best results are about 0.10 F1 points
worse than in the cross-validation settings (Table
1). In all domains, the best average performance
was achieved using only the argument space fea-
tures (FS4); in four cases this system significantly
outperforms all other systems (p &lt; 0.001). More-
over, more high-level feature set combinations
that also contain argument space features (such
as FS2+FS3+F4 or FS3+FS4) yield usually bet-
ter results for particular argument components in
contrast to features based on lexical or syntactic
information (FS0 and FS1). For identifying non-
argumentative texts, there is no clear winner with
respect to feature set abstraction (in three domains
the best results are achieved using FS4 but in other
three domains the baseline FS0 performs best).
Cross-register results The argument space fea-
tures (FS4) performs best in average also in the
cross-register evaluation (see Table 3). In recog-
nizing premises, better results were achieved by a
system trained on blogs and articles and tested on
comments and forum posts. Recognizing claims
exhibits similar behavior. On the other hand,
recognizing non-argumentative text performs bet-
ter in the opposite direction. On average, the
cross-register results are much worse than cross-
validation and slightly worse than cross-domain
results.
</bodyText>
<subsectionHeader confidence="0.984481">
5.1 Error analysis
</subsectionHeader>
<bodyText confidence="0.997372052631579">
First, we quantitatively investigate errors in the
cross-validation scenario. The confusion matrix in
Table 4 shows that about 50-60% of errors for each
argument component were caused by misclassify-
ing it as non-argumentative (the ‘O’ class). The
system tends to prefer the ‘O’ predictions because
of the high presence of non-argumentative sen-
tences in the corpus (about 57%). Backing is often
confused with premises; in particular, Backing-B
with Premise-B in 14%, Backing-I with Premise-I
in 17%. These two argument components have a
similar function–to support the claim–so the dif-
ferences in the discourse (which are sometimes
very subtle) confuse the system. Note that despite
the confusion between these classes, the -I and -B
tags mostly remain the same (the system correctly
predicts whether the argument component begins
or not).13
We also analyzed the errors of the best-
</bodyText>
<footnote confidence="0.64342325">
13To provide the complete picture, we also show the previ-
ously unreported classes (rebuttal and refutation). Rebuttal is
usually misclassified as non-argumentative or premise, refu-
tation as either non-argumentative, backing, or premise.
</footnote>
<page confidence="0.975837">
2132
</page>
<table confidence="0.999944516129032">
FS B-B B-I C-B C-I O P-B P-I Avg FS B-B B-I C-B C-I O P-B P-I Avg
Target domain: Homeschooling Target domain: Public vs. private schools
01234 .039 .249 .000 .000 .145 .000 .000 .062 01 .000 .000 .026 .004 .645 .000 .000 .096
34 .000 .000 .000 .027 .005 .184 .386 .086 012 .000 .000 .026 .005 .647 .000 .000 .097
1234 .063 .263 .000 .000 .289 .000 .000 .088 01234 .000 .000 .000 .000 .591 .069 .093 .108
234 .026 .030 .000 .000 .000 .197 .387 .091 0 .026 .038 .000 .000 .638 .019 .054 .111
01 .000 .000 .000 .000 .689 .000 .000 .098 0123 .058 .064 .019 .023 .622 .019 .025 .119
012 .000 .000 .000 .000 .690 .000 .017 .101 234 .000 .089 .026 .042 .013 .240 .424 .119
0123 .000 .020 .000 .000 .689 .000 .018 .104 1234 .000 .022 .000 .000 .496 .133 .239 .127
0 .000 .000 .063 .032 .683 .079 .098 .136 34 .051 .166 .037 .045 .011 .203 .386 .128
4o .182 .258 .069 .069 .700 .143 .224 .235 4o .228 .251 .275 .270 .653 .232 .220 .304
Target domain: Mainstreaming Target domain: Redshirting
01234 .065 .262 .000 .000 .086 .000 .054 .067 34 .073 .047 .000 .000 .144 .132 .265 .094
34 .000 .000 .000 .000 .000 .184 .352 .077 01234 .000 .000 .076 .070 .251 .024 .264 .098
234 .000 .287 .000 .000 .000 .222 .241 .107 234 .079 .162 .000 .000 .195 .101 .179 .102
0 .000 .000 .000 .000 .689 .054 .046 .113 0 .000 .000 .000 .000 .740 .000 .000 .106
1234 .126 .279 .000 .000 .060 .158 .221 .121 01 .000 .000 .000 .000 .733 .000 .029 .109
01 .000 .000 .000 .000 .666 .103 .079 .121 012 .000 .000 .000 .000 .738 .049 .045 .119
012 .000 .000 .000 .000 .663 .054 .141 .123 1234 .102 .321 .000 .000 .118 .022 .277 .120
0123 .000 .000 .000 .000 .630 .261 .307 .171 0123 .304 .356 .000 .000 .603 .082 .108 .208
4* .222 .448 .000 .000 .674 .145 .247 .248 4o .226 .390 .000 .000 .736 .161 .227 .249
Target domain: Prayer in schools Target domain: Single-sex education
1234 .040 .150 .000 .000 .163 .000 .014 .052 0123 .137 .178 .000 .000 .107 .000 .000 .060
0123 .000 .000 .000 .000 .080 .061 .292 .062 012 .000 .033 .000 .000 .712 .000 .000 .106
01234 .000 .115 .000 .000 .080 .149 .175 .074 01234 .138 .194 .024 .036 .247 .056 .148 .120
234 .058 .042 .000 .000 .012 .215 .303 .090 34 .065 .124 .000 .000 .073 .208 .379 .121
34 .000 .000 .098 .105 .034 .203 .297 .105 01 .000 .000 .000 .000 .708 .092 .209 .144
0 .000 .111 .000 .000 .745 .000 .000 .122 0 .000 .000 .000 .000 .728 .154 .130 .145
01 .000 .115 .000 .000 .810 .000 .000 .132 234 .061 .125 .000 .000 .395 .180 .269 .147
012 .000 .000 .027 .045 .689 .120 .187 .153 1234 .067 .187 .078 .073 .522 .067 .117 .159
4 .000 .146 .083 .048 .695 .168 .156 .185 4o .104 .185 .000 .000 .689 .204 .397 .226
</table>
<tableCaption confidence="0.90849625">
Table 2: Fl results for the cross-domain evaluation scenario ranked by performance. Feature set combination naming (the FS
column) is explained in Section 4.1. Class labels: B-B/I = Backing-B/I, C-B/I = Claim-B/I, O = non-argumentative, P-B/I =
Premise-B/I. Diamond (o) in the last (winning) row signals a significant difference between this row and all other rows while
star (*) denotes that the row is significantly better than the previous row; P &lt; 0.001 using exact Liddell’s test (Liddell, 1983).
</tableCaption>
<table confidence="0.999869909090909">
FS B-B B-I C-B C-I O P-B P-I Avg FS B-B B-I C-B C-I O P-B P-I Avg
Train: blogs, articles; Test: comments, forums Train: comments, forums; Test: blogs, articles
01234 .063 .259 .027 .051 .147 .000 .064 .087 34 .052 .130 .036 .037 .057 .000 .000 .045
012 .000 .000 .000 .000 .643 .000 .000 .092 01234 .000 .008 .000 .000 .003 .080 .301 .056
0 .010 .237 .000 .000 .352 .014 .036 .093 234 .055 .182 .033 .036 .121 .025 .015 .067
01 .000 .000 .000 .000 .643 .010 .013 .095 1234 .071 .176 .014 .021 .050 .061 .290 .098
0123 .021 .032 .000 .000 .645 .005 .002 .101 0 .000 .000 .000 .000 .773 .012 .019 .115
1234 .097 .215 .052 .068 .369 .000 .013 .116 01 .000 .000 .051 .058 .720 .025 .043 .128
234 .042 .068 .065 .068 .534 .093 .168 .148 012 .000 .000 .039 .037 .746 .063 .046 .133
34 .030 .061 .098 .099 .221 .211 .385 .158 0123 .000 .000 .000 .000 .679 .099 .227 .144
4o .076 .206 .167 .158 .611 .151 .209 .225 4o .142 .162 .061 .032 .693 .161 .353 .229
</table>
<tableCaption confidence="0.80641225">
Table 3: Fl results for the cross-register evaluation scenario ranked by performance. Feature set combination naming (the
FS column) is explained in Section 4.1. Class labels: B-B/I = Backing-B/I, C-B/I = Claim-B/I, O = non-argumentative, P-B/I
= Premise-B/I. Diamond (o) in the last (winning) row signals a significant difference between this row and all other rows;
P &lt; 0.001 using exact Liddell’s test (Liddell, 1983).
</tableCaption>
<figureCaption confidence="0.680604692307692">
performing cross-domain system in detail.14 We
randomly sampled 40 documents and manually
compared the predicted arguments with the gold
data. We found that 11 predicted documents were
simply wrong or no argument components were
predicted at all (e.g., document #1640, #1658,
#1021, #5258). Most of these errors occur in
blogs, which seem to convey rather complex
argumentation structure (#1666, #1197, #4586,
#5258). In 8 documents, we identified that only
some premises were (correctly) spotted by the sys-
tem. This happened mostly in long comments
(#452) and blogs (#400, #697, #4583). In 7 inves-
</figureCaption>
<footnote confidence="0.998661">
14Available also as PDF at https://github.com/
habernal/emnlp2015; we use #ID to point to the par-
ticular documents.
</footnote>
<bodyText confidence="0.998949533333333">
tigated documents, we identified errors caused by
slightly different boundaries of recognized argu-
ment components (#4517, #2447, #2252, #4840)
or when multiple segments were merged/split
(#1604, #2180, #2310).
By analyzing the predicted output, we also
found that in 12 documents the recognized argu-
ment components seemed to be valid to some ex-
tent, although this was our subjective judge. For
instance, in #4285 (see Figure 2), the first premise
was misclassified as a claim. The gold-data ar-
gument was annotated as an enthymeme (with im-
plicit claim that advocates private schools), while
in the prediction, the same proposition was iden-
tified as the an explicit claim supporting private
</bodyText>
<page confidence="0.952701">
2133
</page>
<table confidence="0.9999215">
↓ gold \ pred. → Bac-B Bac-I Cla-B Cla-I O Pre-B Pre-I Reb-B Reb-I Ref-B Ref-I
Backing-B 54 12 12 1 106 31 8 0 0 0 0
Backing-I 12 3,238 1 353 5,089 17 1,777 1 18 0 45
Claim-B 7 3 41 5 107 19 9 1 1 0 2
Claim-I 0 160 0 713 2,095 1 456 0 25 0 25
O 97 3,170 53 1,135 36,061 156 5,459 4 178 1 38
Premise-B 35 17 17 2 290 142 28 6 0 0 1
Premise-I 18 1,680 2 544 10,779 51 7,015 2 234 2 41
Rebuttal-B 3 4 3 2 40 9 7 0 0 0 0
Rebuttal-I 1 199 0 47 1,063 10 859 0 0 0 0
Refutation-B 2 2 0 1 16 1 3 0 1 0 0
Refutation-I 0 86 0 7 592 2 148 0 6 0 0
</table>
<tableCaption confidence="0.99995">
Table 4: Confusion matrix for the best performing configuration in the cross-validation scenario.
</tableCaption>
<bodyText confidence="0.999225777777778">
schools with one premise why the education was
not satisfying, which might be also another valid
interpretation. The second example #2180 in Fig-
ure 2 shows that the boundaries of the predicted
premises are mixed up (two recognized instead of
three), but the longer backing is also meaningful.
These examples demonstrate that argument analy-
sis is in some cases ambiguous and allows for dif-
ferent valid interpretations.
</bodyText>
<sectionHeader confidence="0.996155" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999960692307692">
In this article, we proposed a semi-supervised
model for argumentation mining of user-generated
Web content. We developed new unsupervised
features for argument component identification
that exploit clustering of unlabeled argumenta-
tive data from debate portals based on word em-
beddings representation. With the help of these
features we significantly improved performance
of the argumentation mining system and outper-
formed several baselines. While the improvement
was decent in cross-validation scenario, we gained
almost 100% improvement in cross-domain and
cross-register settings.
We evaluated the methods on a publicly avail-
able corpus annotated with argumentation that ori-
gins from user-generated Web data. By a de-
tailed analysis of the errors, we pointed out the
strengths (such as domain adaptability) and weak-
nesses (such as unsatisfying results for rebuttal
and refutation components), as well as the chal-
lenges for the argumentation mining task (such as
boundary identification issues or ambiguous argu-
ments). If we put our results into the context of
existing works, the most relevant one by (Goudas
et al., 2014) achieved 0.42 F1 score on identifying
only premises. We get comparable results in the
cross-validation settings (F1 0.31-0.40) yet with
more complex argumentation model (five different
components).
Although argumentation mining in user-
generated Web discourse has a long way to go
(our methods currently achieve only about 50% of
human performance), we see a huge potential for
various future tasks, such as information seeking
for better-informed personal decision making
or support for argument quality assessment. To
foster the research within the community, we
provide all source codes and data required for the
experiments under free licenses.
</bodyText>
<sectionHeader confidence="0.979346" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999960583333333">
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No I/82806
and by the German Institute for Educatio-
nal Research (DIPF). Access to the CERIT-
SC computing and storage facilities provided
under the programme Center CERIT Scientific
Cloud, part of the Operational Program Re-
search and Development for Innovations, reg. no.
CZ. 1.05/3.2.00/08.0144, is greatly appreciated.
Lastly, we would like to thank the anonymous re-
viewers for their valuable feedback.
</bodyText>
<sectionHeader confidence="0.965295" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.953639416666667">
Or Biran and Owen Rambow. 2011. Identifying jus-
tifications in written dialogs by classifying text as
argumentative. International Journal of Semantic
Computing, 5(4):363–381.
J. Anthony Blair. 2004. Argument and its uses. Infor-
mal Logic, 24:137151.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.
Filip Boltuˇzi´c and Jan ˇSnajder. 2015. Identifying
Prominent Arguments in Online Debates Using Se-
mantic Textual Similarity. In Proceedings of the 2nd
</reference>
<page confidence="0.997696">
2134
</page>
<subsectionHeader confidence="0.677097">
Gold
</subsectionHeader>
<bodyText confidence="0.999508428571429">
[premise: I sent my kid to private school so that she could
get a better education.] [backing: She was at a public school
that was 90% hispanic.] [premise: The problem was not
their race but the fact that they were way behind in reading
langauge and math. This situation was holding my kid and
preventing her from excelling in her studies.] Do you think I
should of just left my kid in this class? Give me a break!
</bodyText>
<subsectionHeader confidence="0.777785">
Predicted
</subsectionHeader>
<bodyText confidence="0.99597">
[claim: I sent my kid to private school so that she could
get a better education.] She was at a public school that was
90% hispanic. [premise: The problem was not their race but
the fact that they were way behind in reading langauge and
math.] This situation was holding my kid and preventing her
from excelling in her studies. Do you think I should of just
left my kid in this class? Give me a break!
</bodyText>
<figure confidence="0.830807">
(a) Doc #4285 (article comme nt, public vs. private schools)
</figure>
<subsectionHeader confidence="0.510996">
Gold
</subsectionHeader>
<bodyText confidence="0.865220823529412">
[claim: Personally i’d go co-ed.]
[backing: . . As someone who went to a same sex school
for 8 years, I found it lacked the diversity you. ..get in. a.
co-ed environment.] [premise: I found the attitude and
behaviour of students in the co ed school to be better, and
i attribute that to the influence of the opposite sex.] [premise:
There’s no doubt boys behave a little different when girls are
watching, and i also found boys were quite good at limiting
the bitchyness girls are renowned for. So both kept one
another in line, and made for a more positive and dynamic
environment.]
[premise: I also think there’s a few extra life lessons
and skills children can learn at co ed schools. Dating,
relationships, interacting with the opposite sex, i think
children at co ed schools tend to have a far better grasp of
these skills then students who’ve only attended same sex
schools.]
</bodyText>
<figure confidence="0.989381">
(b) Doc #2180 (forum post, single-sex education)
</figure>
<figureCaption confidence="0.996586">
Figure 2: Examples of gold data annotations (on the left-hand side) and system predictions in the best-performing cross-domain
evaluation scenario (on the right-hand side).
</figureCaption>
<bodyText confidence="0.923893555555556">
Predicted
[backing: . . Personally i’d go co-ed.
As someone who went to a same sex school for 8 years, I
.
found it lacked the diversity you get in. a co-ed environment.
I found the attitude and behaviour of students in the co ed.
school to be better, and.i attribute that to the influence of the
opposite sex.] [premise: There’s no doubt boys behave a
. . . .
little different when girls are watching, and i also found boys
were quite good at limiting the bitchyness girls are renowned
for.] [premise: So both kept one another in line, and made
for a more positive and dynamic environment.
I also think there’s a few extra life lessons and skills children
can learn at co ed schools. Dating, relationships, interacting
with the opposite sex, i think children at co ed schools tend
to have a far better grasp of these skills then students who’ve
only attended same sex schools.]
</bodyText>
<reference confidence="0.994169745762712">
Workshop on Argumentation Mining, pages 110–
115, Denver, Colorado. Association for Computa-
tional Linguistics.
Elena Cabrio and Serena Villata. 2012. Combining
textual entailment and argumentation theory for sup-
porting online debates interactions. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume
2, ACL ’12, pages 208–212, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Elena Cabrio, Sara Tonelli, and Serena Villata. 2013.
From discourse analysis to argumentation schemes
and back: Relations and differences. In Jo˜ao Leite,
Tran Cao Son, Paolo Torroni, Leon Torre, and Stefan
Woltran, editors, Proceedings of 14th International
Workshop on Computational Logic in Multi-Agent
Systems, volume 8143 of Lecture Notes in Computer
Science, pages 1–17. Springer Berlin Heidelberg.
Jinho D. Choi. 2012. Optimization of Natural Lan-
guage Processing Components for Robustness and
Scalability. Ph.D. Thesis, University of Colorado
Boulder, Computer Science and Cognitive Science.
Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. DKPro TC:
a Java-based framework for supervised learning
experiments on textual data. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
61–66, Baltimore, Maryland, June. Association for
Computational Linguistics.
Richard Eckart de Castilho and Iryna Gurevych. 2014.
A broad-coverage collection of portable NLP com-
ponents for building shareable analysis pipelines. In
Nancy Ide and Jens Grivolla, editors, Proceedings
of the Workshop on Open Infrastructures and Anal-
ysis Frameworks for HLT (OIAF4HLT) at COLING
2014, pages 1–11, Dublin, Ireland. Association for
Computational Linguistics and Dublin City Univer-
sity.
Vanessa Wei Feng and Graeme Hirst. 2011. Classi-
fying arguments by scheme. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ’11, pages 987–996, Portland,
Oregon. Association for Computational Linguistics.
Swapna Gottipati, Minghui Qiu, Yanchuan Sim, Jing
Jiang, and Noah A. Smith. 2013. Learning topics
and positions from Debatepedia. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1858–1868, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Theodosis Goudas, Christos Louizos, Georgios Petasis,
and Vangelis Karkaletsis. 2014. Argument extrac-
tion from news, blogs, and social media. In Aristidis
Likas, Konstantinos Blekas, and Dimitris Kalles, ed-
itors, Artificial Intelligence: Methods and Applica-
tions, pages 287–299. Springer International Pub-
lishing.
</reference>
<page confidence="0.91816">
2135
</page>
<reference confidence="0.999191345454546">
Nancy L Green. 2014. Argumentation for scientific
claims in a biomedical research article. In Elena
Cabrio, Serena Villata, and Adam Wyner, editors,
Proceedings of the Workshop on Frontiers and Con-
nections between Argumentation Theory and Nat-
ural Language Processing, pages 5–10, Bertinoro,
Italy, July. CEUR-WS.
Ivan Habernal and Tom´aˇs Brychcin. 2013. Semantic
spaces for sentiment analysis. In Text, Speech and
Dialogue, volume 8082 of Lecture Notes in Com-
puter Science, pages 482–489, Berlin Heidelberg.
Springer.
Ivan Habernal, Judith Eckle-Kohler, and Iryna
Gurevych. 2014. Argumentation mining on the
web from information seeking perspective. In Elena
Cabrio, Serena Villata, and Adam Wyner, editors,
Proceedings of the Workshop on Frontiers and Con-
nections between Argumentation Theory and Natu-
ral Language Processing, pages 26–39, Bertinoro,
Italy, July. CEUR-WS.
Kazi Saidul Hasan and Vincent Ng. 2013. Stance
classification of ideological debates: Data, mod-
els, features, and constraints. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1348–1356, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Thorsten Joachims, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural SVMs. Machine Learning, 77(1):27–59.
Sathiya Keerthi and Sellamanickam Sundararajan.
2007. CRF versus SVM-struct for sequence label-
ing. Technical report, Yahoo! Research.
Victor Alvin Ketcham. 1917. The theory and prac-
tice of argumentation and debate. Macmillan, New
York.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Label-
ing Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, ICML ’01, pages 282–289, San Francisco, CA.
Morgan Kaufmann Publishers Inc.
Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Tony
Jebara and Eric P. Xing, editors, Proceedings of the
31st International Conference on Machine Learning
(ICML-14), volume 32, pages 1188–1196, Beijing,
China. JMLR Workshop and Conference Proceed-
ings.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic Coreference Resolution
Based on Entity-centric, Precision-ranked Rules.
Computational Linguistics, 39(4):885–916.
Lianghao Li, Xiaoming Jin, Sinno Jialin Pan, and Jian-
Tao Sun. 2012. Multi-domain active learning for
text classification. In Proceedings of the 18th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’12, pages 1086–
1094, Beijing, China. ACM.
Douglas Liddell. 1983. Simplified exact analysis of
case-referent studies: matched pairs; dichotomous
exposure. Journal of Epidemiology &amp; Community
Health, 37(1):82–84.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Hugo Mercier and Dan Sperber. 2011. Why do hu-
mans reason? Arguments for an argumentative the-
ory. The Behavioral and Brain Sciences, 34(2):57–
74; discussion 74–111.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.
Raquel Mochales and Marie-Francine Moens. 2011.
Argumentation mining. Artificial Intelligence and
Law, 19(1):1–22, April.
Ana Laura Nettel and Georges Roque. 2011. Persua-
sive argumentation versus manipulation. Argumen-
tation, 26(1):55–69.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1, ACL ’09, pages 351–359, Suntec, Singapore.
Association for Computational Linguistics.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved Part-of-Speech Tagging for
Online Conversational Text with Word Clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380–390, Atlanta, Georgia. Association for
Computational Linguistics.
Joonsuk Park and Claire Cardie. 2014. Identifying
appropriate support for propositions in online user
comments. In Proceedings of the First Workshop
on Argumentation Mining, pages 29–38, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.
Andreas Peldszus and Manfred Stede. 2013. Ranking
the annotators: An agreement study on argumenta-
tion structure. In Proceedings of the 7th Linguistic
</reference>
<page confidence="0.821458">
2136
</page>
<reference confidence="0.999218684782609">
Annotation Workshop and Interoperability with Dis-
course, pages 196–204, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433–440, Sydney, Australia. Association for
Computational Linguistics.
Niall Rooney, Hui Wang, and Fiona Browne. 2012.
Applying kernel methods to argumentation min-
ing. In Proceedings of the Twenty-Fifth Interna-
tional Florida Artificial Intelligence Research Soci-
ety Conference, pages 272–275. Association for the
Advancement of Artificial Intelligence.
Sara Rosenthal and Kathleen McKeown. 2012. De-
tecting opinionated claims in online discussions. In
2012 IEEE Sixth International Conference on Se-
mantic Computing, pages 30–37, Palermo, Italy.
IEEE.
Burr Settles. 2012. Active Learning. Morgan &amp; Clay-
pool Publishers.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Anders Søgaard. 2013. Semi-Supervised Learning and
Domain Adaptation in Natural Language Process-
ing. Morgan &amp; Claypool Publishers.
Christian Stab and Iryna Gurevych. 2014a. Annotating
argument components and relations in persuasive es-
says. In Proceedings of COLING 2014, the 25th In-
ternational Conference on Computational Linguis-
tics: Technical Papers, pages 1501–1510, Dublin,
Ireland, August. Dublin City University and Associ-
ation for Computational Linguistics.
Christian Stab and Iryna Gurevych. 2014b. Identify-
ing argumentative discourse structures in persuasive
essays. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 46–56, Doha, Qatar, October. As-
sociation for Computational Linguistics.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28:409–445.
Stephen E. Toulmin. 1958. The Uses of Argument.
Cambridge University Press.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1, NAACL ’03, pages 173–180, Edmonton,
Canada. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), number July, pages 384–
394, Uppsala, Sweden. Association for Computa-
tional Linguistics.
Frans H. van Eemeren, R. Grootendorst, and A. F.
Snoeck Henkemans. 2002. Argumentation: Anal-
ysis, evaluation, presentation. Lawrence Erlbaum,
Mahwah, NJ, USA.
Frans H. van Eemeren, Bart Garssen, Erik C. W.
Krabbe, A. Francisca Snoeck Henkemans, Bart
Verheij, and Jean H. M. Wagemans. 2014.
Handbook of Argumentation Theory. Springer,
Berlin/Heidelberg.
Henning Wachsmuth, Martin Trenkmann, Benno Stein,
Gregor Engels, and Tsvetomira Palakarska. 2014.
A review corpus for argumentation analysis. In
Alexander Gelbukh, editor, 15th International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics (CICLing 14), pages 115–127,
Kathmandu, Nepal. Springer.
Douglas Walton, Christopher Reed, and Fabrizio
Macagno. 2008. Argumentation Schemes. Cam-
bridge University Press.
Y. Zhao and G. Karypis. 2002. Criterion functions
for document clustering: Experiments and analysis.
Technical report, Department of Computer Science,
University of Minnesota, Minneapolis.
</reference>
<page confidence="0.994812">
2137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671721">
<title confidence="0.945743">Exploiting Debate Portals for Semi-Supervised Argumentation in User-Generated Web Discourse</title>
<author confidence="0.704012">Knowledge Processing Lab</author>
<affiliation confidence="0.998053666666667">Department of Computer Science, Technische Universit¨at Knowledge Processing Lab German Institute for Educational</affiliation>
<email confidence="0.982839">www.ukp.tu-darmstadt.de</email>
<abstract confidence="0.999684652173913">Analyzing arguments in user-generated Web discourse has recently gained attention in argumentation mining, an evolving field of NLP. Current approaches, which employ fully-supervised machine learning, are usually domain dependent and suffer from the lack of large and diverse annotated corpora. However, annotating arguments in discourse is costly, errorprone, and highly context-dependent. We asked whether leveraging unlabeled data in a semi-supervised manner can boost the performance of argument component identification and to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Owen Rambow</author>
</authors>
<title>Identifying justifications in written dialogs by classifying text as argumentative.</title>
<date>2011</date>
<journal>International Journal of Semantic Computing,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="2817" citStr="Biran and Rambow, 2011" startWordPosition="399" endWordPosition="402">contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such metho</context>
<context position="7261" citStr="Biran and Rambow (2011)" startWordPosition="1050" endWordPosition="1053">gmented into argument components. Stab and Gurevych (2014b) examined argumentation in persuasive essays and classified argument components into four categories (premise, claim, major claim, nonargumentative) using SVM and achieved 0.73 macro F1 score. They further classified argument relations (support and attack) and reached 0.72 macro F1 score. The best-performing features were structural features (such as the location or length ratios), as persuasive essays usually comply with a certain structure which can be seen as a potential drawback of this approach. Regarding user-generated Web data, Biran and Rambow (2011) used naive Bayes for classifying justification of subjective claims from blogs and Wikipedia talk pages, relying on features from RST Treebank and manually-processed n-grams. In similar Web registers, Rosenthal and McKeown (2012) automatically determined whether a sentence is a claim using logistic regression and various lexical and sentiment-related features and achieved accuracy about 0.66-0.71. Park and Cardie (2014) classified propositions in user comments into three classes (verifiable experiential, verifiable non-experiential, and unverifiable) using SVM and reached 0.69 macro F1 score.</context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>Or Biran and Owen Rambow. 2011. Identifying justifications in written dialogs by classifying text as argumentative. International Journal of Semantic Computing, 5(4):363–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Anthony Blair</author>
</authors>
<title>Argument and its uses. Informal Logic,</title>
<date>2004</date>
<pages>24--137151</pages>
<contexts>
<context position="5772" citStr="Blair, 2004" startWordPosition="835" endWordPosition="836">ate portals into a latent argument space using unsupervised word embeddings and clustering. Second, our novel features significantly outperform state-of-the-art features in all scenarios, namely in cross-validation, crossdomain evaluation, and cross-register evaluation. Third, to ensure full reproducibility of our experiments, we provide all data and source codes under free licenses.3 2 Related work Analysis of argumentation has been an active topic in numerous research areas, such as philosophy (van Eemeren et al., 2014), communication studies (Mercier and Sperber, 2011), and informal logic (Blair, 2004), among others. In this section, we will focus on the most related works on argumentation mining techniques in NLP in the first part, with an emphasis on Web data in the second part. Mochales and Moens (2011) based their work on argumentation schemes (Walton et al., 2008) and experimented with Araucaria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et </context>
</contexts>
<marker>Blair, 2004</marker>
<rawString>J. Anthony Blair. 2004. Argument and its uses. Informal Logic, 24:137151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="15648" citStr="Blei et al., 2003" startWordPosition="2394" endWordPosition="2397">OS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data provided as a part of the corpus.8 Semantic and discourse features (FS3) Features based on semantic frames has been introduced in relevant works on stance recognition (Hasan and Ng, 2013). Our features, based on PropBank semantic role labels and obtained from 7In only 1% of the sentences there are two or more argument components in it; we arbitrarily choose the largest one. 8The number of topics was empirically set to 30, therefore for each sentence the topic distribution results into 30 realvalued features. NLP Semantic Role Labeler (Cho</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Boltuˇzi´c</author>
<author>Jan ˇSnajder</author>
</authors>
<title>Identifying Prominent Arguments in Online Debates Using Semantic Textual Similarity.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2nd</booktitle>
<marker>Boltuˇzi´c, ˇSnajder, 2015</marker>
<rawString>Filip Boltuˇzi´c and Jan ˇSnajder. 2015. Identifying Prominent Arguments in Online Debates Using Semantic Textual Similarity. In Proceedings of the 2nd</rawString>
</citation>
<citation valid="false">
<booktitle>Workshop on Argumentation Mining,</booktitle>
<pages>110--115</pages>
<institution>Denver, Colorado. Association for Computational Linguistics.</institution>
<marker></marker>
<rawString>Workshop on Argumentation Mining, pages 110– 115, Denver, Colorado. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Cabrio</author>
<author>Serena Villata</author>
</authors>
<title>Combining textual entailment and argumentation theory for supporting online debates interactions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12,</booktitle>
<pages>208--212</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="8672" citStr="Cabrio and Villata (2012)" startWordPosition="1258" endWordPosition="1261">ed approaches are the following. First, the argumentation models are simplified to either claims or a few types of premises/propositions. Second, the segmentation of discourse into argument components is ignored (except the work of Goudas et al. (2014)). Recently, Boltuˇzi´c and ˇSnajder (2015) employed hierarchical clustering to cluster arguments in online debates using embeddings projection, but in contrast to our work they performed only intrinsic evaluation of the clusters. Debate portals have been used in a related body of research, such as classifying support and attack between posts by Cabrio and Villata (2012), or stance detection by Hasan and Ng (2013) or Gottipati et al. (2013). These approaches consider the complete documents (posts) but do not analyze the micro-level argumentation (e.g., claims or premises). 2128 Doc #2823 (article comment, public-private-schools): [claim: I agree - Kids can do great in the public school system and parents DO need to be involved.] The more people leave, the worse its going to become. [premise: The public school system lets them deal with real life too, unfortunate that it may be but that is what’s out there in college and the work force too.] [premise: There ar</context>
</contexts>
<marker>Cabrio, Villata, 2012</marker>
<rawString>Elena Cabrio and Serena Villata. 2012. Combining textual entailment and argumentation theory for supporting online debates interactions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers - Volume 2, ACL ’12, pages 208–212, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Elena Cabrio</author>
<author>Sara Tonelli</author>
<author>Serena Villata</author>
</authors>
<title>From discourse analysis to argumentation schemes and back: Relations and differences.</title>
<date>2013</date>
<booktitle>Proceedings of 14th International Workshop on Computational Logic in Multi-Agent Systems,</booktitle>
<volume>8143</volume>
<pages>1--17</pages>
<editor>In Jo˜ao Leite, Tran Cao Son, Paolo Torroni, Leon Torre, and Stefan Woltran, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="16523" citStr="Cabrio et al., 2013" startWordPosition="2536" endWordPosition="2539">es, based on PropBank semantic role labels and obtained from 7In only 1% of the sentences there are two or more argument components in it; we arbitrarily choose the largest one. 8The number of topics was empirically set to 30, therefore for each sentence the topic distribution results into 30 realvalued features. NLP Semantic Role Labeler (Choi, 2012), extract various semantic information (agent, predicate + agent, predicate + agent + patient + (optional) negation, argument type + argument value) and discourse markers. Discourse relations also play an important role in argumentation analysis (Cabrio et al., 2013). We thus employ binary features (such as the presence of the sentence in a chain, the transition type, the distance to previous/next sentences in the chain, or the number of inter-sentence coreference links) obtained from Stanford Coreference Chain Resolver (Lee et al., 2013). Furthermore, we include features resulting from a PTDB-style discourse parser (Li et al., 2012), such as the type of discourse relation (explicit, implicit), the presence of discourse connectives, and attributions. 4.2 Unsupervised features We enrich the above-mentioned features by utilizing external large unlabeled res</context>
</contexts>
<marker>Cabrio, Tonelli, Villata, 2013</marker>
<rawString>Elena Cabrio, Sara Tonelli, and Serena Villata. 2013. From discourse analysis to argumentation schemes and back: Relations and differences. In Jo˜ao Leite, Tran Cao Son, Paolo Torroni, Leon Torre, and Stefan Woltran, editors, Proceedings of 14th International Workshop on Computational Logic in Multi-Agent Systems, volume 8143 of Lecture Notes in Computer Science, pages 1–17. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
</authors>
<title>Optimization of Natural Language Processing Components for Robustness and Scalability.</title>
<date>2012</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Colorado Boulder, Computer Science and Cognitive Science.</institution>
<contexts>
<context position="16256" citStr="Choi, 2012" startWordPosition="2497" endWordPosition="2498">003; McCallum, 2002) with topics trained on unlabeled data provided as a part of the corpus.8 Semantic and discourse features (FS3) Features based on semantic frames has been introduced in relevant works on stance recognition (Hasan and Ng, 2013). Our features, based on PropBank semantic role labels and obtained from 7In only 1% of the sentences there are two or more argument components in it; we arbitrarily choose the largest one. 8The number of topics was empirically set to 30, therefore for each sentence the topic distribution results into 30 realvalued features. NLP Semantic Role Labeler (Choi, 2012), extract various semantic information (agent, predicate + agent, predicate + agent + patient + (optional) negation, argument type + argument value) and discourse markers. Discourse relations also play an important role in argumentation analysis (Cabrio et al., 2013). We thus employ binary features (such as the presence of the sentence in a chain, the transition type, the distance to previous/next sentences in the chain, or the number of inter-sentence coreference links) obtained from Stanford Coreference Chain Resolver (Lee et al., 2013). Furthermore, we include features resulting from a PTDB</context>
</contexts>
<marker>Choi, 2012</marker>
<rawString>Jinho D. Choi. 2012. Optimization of Natural Language Processing Components for Robustness and Scalability. Ph.D. Thesis, University of Colorado Boulder, Computer Science and Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Oliver Ferschke</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>DKPro TC: a Java-based framework for supervised learning experiments on textual data.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>61--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="12785" citStr="Daxenberger et al., 2014" startWordPosition="1932" endWordPosition="1935"> the corpus. Argument components were annotated on the token level as non-overlapping annotation spans. We therefore represent the argument annotations using BIO encoding. Each token is labeled with one of the 11 categories (5 argument component types × B or I tag + one O category for non-argumentative text). 4 Method We cast the task of identifying argument components as a sequence tagging problem and employ SVMhmm (Joachims et al., 2009).6 For linguistic annotations and feature engineering, we rely on two UIMA-based frameworks – DKProCore (Eckart de Castilho and Gurevych, 2014) and DKProTC (Daxenberger et al., 2014). Although the argument component annotations in the corpus are aligned to the token boundaries (token-level annotations), the minimal classification unit in our sequence tagging approach is set to the sentence level. First, this allows us to capture rich features that are available for entire sentences as opposed to the token level. Second, by modeling sequences on the token level we would lose the advantage of SVMhmm to estimate dependencies between labels, as the label context is limited due to computational feasibility. On the token level, the label sequences are rather static (long sequen</context>
</contexts>
<marker>Daxenberger, Ferschke, Gurevych, Zesch, 2014</marker>
<rawString>Johannes Daxenberger, Oliver Ferschke, Iryna Gurevych, and Torsten Zesch. 2014. DKPro TC: a Java-based framework for supervised learning experiments on textual data. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 61–66, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
</authors>
<title>A broad-coverage collection of portable NLP components for building shareable analysis pipelines.</title>
<date>2014</date>
<booktitle>In Nancy Ide and Jens Grivolla, editors, Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland.</location>
<marker>de Castilho, Gurevych, 2014</marker>
<rawString>Richard Eckart de Castilho and Iryna Gurevych. 2014. A broad-coverage collection of portable NLP components for building shareable analysis pipelines. In Nancy Ide and Jens Grivolla, editors, Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014, pages 1–11, Dublin, Ireland. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Classifying arguments by scheme.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>987--996</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon.</location>
<contexts>
<context position="6229" citStr="Feng and Hirst (2011)" startWordPosition="907" endWordPosition="910">topic in numerous research areas, such as philosophy (van Eemeren et al., 2014), communication studies (Mercier and Sperber, 2011), and informal logic (Blair, 2004), among others. In this section, we will focus on the most related works on argumentation mining techniques in NLP in the first part, with an emphasis on Web data in the second part. Mochales and Moens (2011) based their work on argumentation schemes (Walton et al., 2008) and experimented with Araucaria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et al. (2012), who classified sentences to four categories (conclusion, premise, conclusion-premise, 2For instance createdebate.com or debate.org 3https://github.com/habernal/emnlp2015 and none) and achieved 0.65 accuracy. These approaches assume the text is already segmented into argument components. Stab and Gurevych (2014b) examined argumentation in persuasive essays and classified argument components into four categories (premise, claim, major claim, n</context>
</contexts>
<marker>Feng, Hirst, 2011</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2011. Classifying arguments by scheme. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 987–996, Portland, Oregon. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Gottipati</author>
<author>Minghui Qiu</author>
<author>Yanchuan Sim</author>
<author>Jing Jiang</author>
<author>Noah A Smith</author>
</authors>
<title>Learning topics and positions from Debatepedia.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1858--1868</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="8743" citStr="Gottipati et al. (2013)" startWordPosition="1271" endWordPosition="1275">lified to either claims or a few types of premises/propositions. Second, the segmentation of discourse into argument components is ignored (except the work of Goudas et al. (2014)). Recently, Boltuˇzi´c and ˇSnajder (2015) employed hierarchical clustering to cluster arguments in online debates using embeddings projection, but in contrast to our work they performed only intrinsic evaluation of the clusters. Debate portals have been used in a related body of research, such as classifying support and attack between posts by Cabrio and Villata (2012), or stance detection by Hasan and Ng (2013) or Gottipati et al. (2013). These approaches consider the complete documents (posts) but do not analyze the micro-level argumentation (e.g., claims or premises). 2128 Doc #2823 (article comment, public-private-schools): [claim: I agree - Kids can do great in the public school system and parents DO need to be involved.] The more people leave, the worse its going to become. [premise: The public school system lets them deal with real life too, unfortunate that it may be but that is what’s out there in college and the work force too.] [premise: There are still great teachers in the public schools - lets stand behind them.]</context>
</contexts>
<marker>Gottipati, Qiu, Sim, Jiang, Smith, 2013</marker>
<rawString>Swapna Gottipati, Minghui Qiu, Yanchuan Sim, Jing Jiang, and Noah A. Smith. 2013. Learning topics and positions from Debatepedia. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1858–1868, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodosis Goudas</author>
</authors>
<title>Christos Louizos, Georgios Petasis, and Vangelis Karkaletsis.</title>
<date>2014</date>
<booktitle>In Aristidis Likas, Konstantinos Blekas, and Dimitris Kalles, editors, Artificial Intelligence: Methods and Applications,</booktitle>
<pages>287--299</pages>
<publisher>Springer International Publishing.</publisher>
<marker>Goudas, 2014</marker>
<rawString>Theodosis Goudas, Christos Louizos, Georgios Petasis, and Vangelis Karkaletsis. 2014. Argument extraction from news, blogs, and social media. In Aristidis Likas, Konstantinos Blekas, and Dimitris Kalles, editors, Artificial Intelligence: Methods and Applications, pages 287–299. Springer International Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy L Green</author>
</authors>
<title>Argumentation for scientific claims in a biomedical research article.</title>
<date>2014</date>
<booktitle>Proceedings of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing,</booktitle>
<pages>5--10</pages>
<editor>In Elena Cabrio, Serena Villata, and Adam Wyner, editors,</editor>
<publisher>CEUR-WS.</publisher>
<location>Bertinoro, Italy,</location>
<contexts>
<context position="1509" citStr="Green, 2014" startWordPosition="198" endWordPosition="199">nd to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 1 Introduction Argumentation mining, an evolving sub-field of NLP, deals with analyzing argumentation1 in various genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including all writing and speaking which is persuasive inform</context>
</contexts>
<marker>Green, 2014</marker>
<rawString>Nancy L Green. 2014. Argumentation for scientific claims in a biomedical research article. In Elena Cabrio, Serena Villata, and Adam Wyner, editors, Proceedings of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing, pages 5–10, Bertinoro, Italy, July. CEUR-WS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Habernal</author>
<author>Tom´aˇs Brychcin</author>
</authors>
<title>Semantic spaces for sentiment analysis.</title>
<date>2013</date>
<booktitle>In Text, Speech and Dialogue,</booktitle>
<volume>8082</volume>
<pages>482--489</pages>
<publisher>Springer.</publisher>
<location>Berlin Heidelberg.</location>
<marker>Habernal, Brychcin, 2013</marker>
<rawString>Ivan Habernal and Tom´aˇs Brychcin. 2013. Semantic spaces for sentiment analysis. In Text, Speech and Dialogue, volume 8082 of Lecture Notes in Computer Science, pages 482–489, Berlin Heidelberg. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Habernal</author>
<author>Judith Eckle-Kohler</author>
<author>Iryna Gurevych</author>
</authors>
<title>Argumentation mining on the web from information seeking perspective.</title>
<date>2014</date>
<booktitle>Proceedings of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing,</booktitle>
<pages>26--39</pages>
<editor>In Elena Cabrio, Serena Villata, and Adam Wyner, editors,</editor>
<publisher>CEUR-WS.</publisher>
<location>Bertinoro, Italy,</location>
<contexts>
<context position="2406" citStr="Habernal et al., 2014" startWordPosition="339" endWordPosition="342">the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including all writing and speaking which is persuasive inform (Ketcham, 1917). retrieving and understanding ordinary people’s arguments to various contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, </context>
<context position="3665" citStr="Habernal et al., 2014" startWordPosition="529" endWordPosition="532">(Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such methods in argumentation mining is questionable. First, argumentation is an act of persuasion (Nettel and Roque, 2011; Mercier and Sperber, 2011) but not all usergenerated texts can be treated as persuasive (Park and Cardie, 2014; Habernal et al., 2014), thus the selection of an appropriate unlabeled dataset represents a problem on its own. Second, argument components (e.g., claims or premises) are highly context-dependent and cannot be easily labeled in distant data using predefined patterns. So far, semi-supervised methods for argumentation mining remain unexplored. In this article, we tackle argumentation min2127 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2127–2137, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ing of user-generated Web data by </context>
<context position="10488" citStr="Habernal et al. (2014)" startWordPosition="1582" endWordPosition="1585">. We were the worst in 12 years and were beaten by LOTS of co-ed and public schools... So you can never tell. In saying that my sister really enjoyed going to an all girls school. Her year went really well too. Ask your daughters what they would prefer... [backing: Btw,. I work at.a co-ed school at the moment and the kids there get on just fine.] Figure 1: Two examples of argument annotation of an article comment and a forum post. 3 Data As data for training and evaluation of our methods, we use a corpus consisting of 340 English documents (approx. 90k tokens) annotated4 with argumentation by Habernal et al. (2014). Compared to other corpora mentioned in the related work, this corpus is the largest one to date that covers different domains and spans several registers of usergenerated Web content. In particular, the corpus comprises four registers (comments to articles, forum posts, blogs, and argumentative newswire articles) and covers six domains related to educational controversies (homeschooling, private vs. public schools, mainstreaming, single-sex education, prayer in schools, and redshirting). The argumentation model used in this corpus is based on extended Toulmin’s model (Toulmin, 1958). Each do</context>
<context position="24673" citStr="Habernal et al., 2014" startWordPosition="3875" endWordPosition="3878">n the previous row, but is significantly better than the previous row minus one; p &lt; 0.001 using exact Liddell’s test (Liddell, 1983). classifier is trained and tested on all 11 classes including rebuttal and refutation, we do not report performance of these two argument components— the results are very poor regardless of the parameters for two reasons. First, these classes are underrepresented in the data (Rebuttal-B, RebuttalI, Refutation-B and Refutation-I are present in only about 4% of sentences). Second, the interannotator agreement reached on these classes were reported to be very low (Habernal et al., 2014). Cross validation results Table 1 shows results for the cross-validation scenario. The human baseline in the first row is an average score between three original annotators of the dataset. The baseline features (FS0) perform poorly, yet they beat the random assignment and majority vote (&lt; 0.12 F1). The argument space features (FS4) increase the performance in every combination. The best results for claims are achieved when only discourse, sentiment, and argument space features are involved (FS3 and FS4), whereas premises and backing benefit from the presence of lexical, syntactic, and semanti</context>
</contexts>
<marker>Habernal, Eckle-Kohler, Gurevych, 2014</marker>
<rawString>Ivan Habernal, Judith Eckle-Kohler, and Iryna Gurevych. 2014. Argumentation mining on the web from information seeking perspective. In Elena Cabrio, Serena Villata, and Adam Wyner, editors, Proceedings of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing, pages 26–39, Bertinoro, Italy, July. CEUR-WS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazi Saidul Hasan</author>
<author>Vincent Ng</author>
</authors>
<title>Stance classification of ideological debates: Data, models, features, and constraints.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>1348--1356</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="8716" citStr="Hasan and Ng (2013)" startWordPosition="1266" endWordPosition="1269">ntation models are simplified to either claims or a few types of premises/propositions. Second, the segmentation of discourse into argument components is ignored (except the work of Goudas et al. (2014)). Recently, Boltuˇzi´c and ˇSnajder (2015) employed hierarchical clustering to cluster arguments in online debates using embeddings projection, but in contrast to our work they performed only intrinsic evaluation of the clusters. Debate portals have been used in a related body of research, such as classifying support and attack between posts by Cabrio and Villata (2012), or stance detection by Hasan and Ng (2013) or Gottipati et al. (2013). These approaches consider the complete documents (posts) but do not analyze the micro-level argumentation (e.g., claims or premises). 2128 Doc #2823 (article comment, public-private-schools): [claim: I agree - Kids can do great in the public school system and parents DO need to be involved.] The more people leave, the worse its going to become. [premise: The public school system lets them deal with real life too, unfortunate that it may be but that is what’s out there in college and the work force too.] [premise: There are still great teachers in the public schools</context>
<context position="15891" citStr="Hasan and Ng, 2013" startWordPosition="2434" endWordPosition="2437">ve to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data provided as a part of the corpus.8 Semantic and discourse features (FS3) Features based on semantic frames has been introduced in relevant works on stance recognition (Hasan and Ng, 2013). Our features, based on PropBank semantic role labels and obtained from 7In only 1% of the sentences there are two or more argument components in it; we arbitrarily choose the largest one. 8The number of topics was empirically set to 30, therefore for each sentence the topic distribution results into 30 realvalued features. NLP Semantic Role Labeler (Choi, 2012), extract various semantic information (agent, predicate + agent, predicate + agent + patient + (optional) negation, argument type + argument value) and discourse markers. Discourse relations also play an important role in argumentatio</context>
</contexts>
<marker>Hasan, Ng, 2013</marker>
<rawString>Kazi Saidul Hasan and Vincent Ng. 2013. Stance classification of ideological debates: Data, models, features, and constraints. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1348–1356, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
<author>Thomas Finley</author>
<author>ChunNam John Yu</author>
</authors>
<title>Cutting-plane training of structural SVMs.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>77</volume>
<issue>1</issue>
<contexts>
<context position="12603" citStr="Joachims et al., 2009" startWordPosition="1905" endWordPosition="1908">lable at www.ukp.tu-darmstadt.de/data/ argumentation-mining/ 5A publication containing a thorough analysis of the dataset is pending. Figure 1 depicts two example annotations from the corpus. Argument components were annotated on the token level as non-overlapping annotation spans. We therefore represent the argument annotations using BIO encoding. Each token is labeled with one of the 11 categories (5 argument component types × B or I tag + one O category for non-argumentative text). 4 Method We cast the task of identifying argument components as a sequence tagging problem and employ SVMhmm (Joachims et al., 2009).6 For linguistic annotations and feature engineering, we rely on two UIMA-based frameworks – DKProCore (Eckart de Castilho and Gurevych, 2014) and DKProTC (Daxenberger et al., 2014). Although the argument component annotations in the corpus are aligned to the token boundaries (token-level annotations), the minimal classification unit in our sequence tagging approach is set to the sentence level. First, this allows us to capture rich features that are available for entire sentences as opposed to the token level. Second, by modeling sequences on the token level we would lose the advantage of SV</context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>Thorsten Joachims, Thomas Finley, and ChunNam John Yu. 2009. Cutting-plane training of structural SVMs. Machine Learning, 77(1):27–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sathiya Keerthi</author>
<author>Sellamanickam Sundararajan</author>
</authors>
<title>CRF versus SVM-struct for sequence labeling.</title>
<date>2007</date>
<tech>Technical report, Yahoo! Research.</tech>
<contexts>
<context position="13809" citStr="Keerthi and Sundararajan (2007)" startWordPosition="2100" endWordPosition="2104"> lose the advantage of SVMhmm to estimate dependencies between labels, as the label context is limited due to computational feasibility. On the token level, the label sequences are rather static (long sequences with the same label), as opposed to the sentence level. Before the classification step, we adjust all annotation boundaries (note that we use 11 BIO labels) so that they are aligned to the sentence boundaries and each sentence is then treated as a single classification unit with one label (for example, the first sentence from Figure 1 with token labels Claim-B, Claim-I, Claim-I, ... be6Keerthi and Sundararajan (2007) conclude that performance of SVMhmm is comparable to another widely used method, Conditional Random Fields (Lafferty et al., 2001) 2129 comes Claim-B). After classification, the labels are mapped back to tokens (so that, for example, Claim-B sentence label is transformed to ClaimB, Claim-I, ... token labels). However, all evaluations are performed on the token level and the performance is always measured against the original token labels. Using this approximation, we lose only about 10% of F1 performance.7 4.1 Baseline features Lexical baseline (FS0) We encode the presence of unigrams, bigram</context>
</contexts>
<marker>Keerthi, Sundararajan, 2007</marker>
<rawString>Sathiya Keerthi and Sellamanickam Sundararajan. 2007. CRF versus SVM-struct for sequence labeling. Technical report, Yahoo! Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Alvin Ketcham</author>
</authors>
<title>The theory and practice of argumentation and debate.</title>
<date>1917</date>
<publisher>Macmillan,</publisher>
<location>New York.</location>
<contexts>
<context position="2125" citStr="Ketcham, 1917" startWordPosition="301" endWordPosition="302">Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including all writing and speaking which is persuasive inform (Ketcham, 1917). retrieving and understanding ordinary people’s arguments to various contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argume</context>
</contexts>
<marker>Ketcham, 1917</marker>
<rawString>Victor Alvin Ketcham. 1917. The theory and practice of argumentation and debate. Macmillan, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="13940" citStr="Lafferty et al., 2001" startWordPosition="2121" endWordPosition="2124">the token level, the label sequences are rather static (long sequences with the same label), as opposed to the sentence level. Before the classification step, we adjust all annotation boundaries (note that we use 11 BIO labels) so that they are aligned to the sentence boundaries and each sentence is then treated as a single classification unit with one label (for example, the first sentence from Figure 1 with token labels Claim-B, Claim-I, Claim-I, ... be6Keerthi and Sundararajan (2007) conclude that performance of SVMhmm is comparable to another widely used method, Conditional Random Fields (Lafferty et al., 2001) 2129 comes Claim-B). After classification, the labels are mapped back to tokens (so that, for example, Claim-B sentence label is transformed to ClaimB, Claim-I, ... token labels). However, all evaluations are performed on the token level and the performance is always measured against the original token labels. Using this approximation, we lose only about 10% of F1 performance.7 4.1 Baseline features Lexical baseline (FS0) We encode the presence of unigrams, bigrams, and trigrams in the sentence as ‘one-hot’ (binary) features. Structural and syntactic features (FS1) Since the presence of disco</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>Proceedings of the 31st International Conference on Machine Learning (ICML-14),</booktitle>
<volume>32</volume>
<pages>1188--1196</pages>
<editor>In Tony Jebara and Eric P. Xing, editors,</editor>
<location>Beijing,</location>
<contexts>
<context position="18182" citStr="Mikolov, 2014" startWordPosition="2804" endWordPosition="2805">are simple discussions to a topic, where each post is only labeled with a pro or contra stance). Nevertheless, we assume that the posts from (unlabeled) debate portals contain valuable information that will help us with classifying argument components in labeled data. In order to do so, we employ clustering based on latent semantics, which we now formalize as argument space features. We assume that phrases (sentences or documents) can be projected into a latent vector space, using, typically, a sum or a weighted average of all the word embeddings vectors in the phrase; see for example (Le and Mikolov, 2014). Neighboring vectors in the latent vector space exhibit some interesting properties, such as semantic similarity (thoroughly studied within the distributional semantics area). If the latent vector space is clustered, each n-dimensional vector gets reduced to a single cluster number; such clusters have been used directly as features in many tasks, such as NER (Turian et al., 2010), POS tagging (Owoputi 2130 et al., 2013), or sentiment analysis (Habernal and Brychc´ın, 2013). We build upon the above-mentioned approach (described by Søgaard (2013) as ‘clusters-asfeatures’ semi-supervised paradig</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), volume 32, pages 1188–1196, Beijing, China. JMLR Workshop and Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic Coreference Resolution Based on Entity-centric, Precision-ranked Rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="16800" citStr="Lee et al., 2013" startWordPosition="2581" endWordPosition="2584">sults into 30 realvalued features. NLP Semantic Role Labeler (Choi, 2012), extract various semantic information (agent, predicate + agent, predicate + agent + patient + (optional) negation, argument type + argument value) and discourse markers. Discourse relations also play an important role in argumentation analysis (Cabrio et al., 2013). We thus employ binary features (such as the presence of the sentence in a chain, the transition type, the distance to previous/next sentences in the chain, or the number of inter-sentence coreference links) obtained from Stanford Coreference Chain Resolver (Lee et al., 2013). Furthermore, we include features resulting from a PTDB-style discourse parser (Li et al., 2012), such as the type of discourse relation (explicit, implicit), the presence of discourse connectives, and attributions. 4.2 Unsupervised features We enrich the above-mentioned features by utilizing external large unlabeled resources – debate portals. They fulfill several criteria, namely (a) they are ‘argumentative’ (meant as opposed to, for example, prose or encyclopedic genres), (b) they are comprised of user-generated content and (c) and there is at least some overlap with topics from our experi</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic Coreference Resolution Based on Entity-centric, Precision-ranked Rules. Computational Linguistics, 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lianghao Li</author>
<author>Xiaoming Jin</author>
<author>Sinno Jialin Pan</author>
<author>JianTao Sun</author>
</authors>
<title>Multi-domain active learning for text classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’12,</booktitle>
<pages>1086--1094</pages>
<publisher>ACM.</publisher>
<location>Beijing, China.</location>
<contexts>
<context position="16897" citStr="Li et al., 2012" startWordPosition="2596" endWordPosition="2599">c information (agent, predicate + agent, predicate + agent + patient + (optional) negation, argument type + argument value) and discourse markers. Discourse relations also play an important role in argumentation analysis (Cabrio et al., 2013). We thus employ binary features (such as the presence of the sentence in a chain, the transition type, the distance to previous/next sentences in the chain, or the number of inter-sentence coreference links) obtained from Stanford Coreference Chain Resolver (Lee et al., 2013). Furthermore, we include features resulting from a PTDB-style discourse parser (Li et al., 2012), such as the type of discourse relation (explicit, implicit), the presence of discourse connectives, and attributions. 4.2 Unsupervised features We enrich the above-mentioned features by utilizing external large unlabeled resources – debate portals. They fulfill several criteria, namely (a) they are ‘argumentative’ (meant as opposed to, for example, prose or encyclopedic genres), (b) they are comprised of user-generated content and (c) and there is at least some overlap with topics from our experimental corpus. On the other hand, they contain noisy texts of questionable quality and they do no</context>
</contexts>
<marker>Li, Jin, Pan, Sun, 2012</marker>
<rawString>Lianghao Li, Xiaoming Jin, Sinno Jialin Pan, and JianTao Sun. 2012. Multi-domain active learning for text classification. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’12, pages 1086– 1094, Beijing, China. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Liddell</author>
</authors>
<title>Simplified exact analysis of case-referent studies: matched pairs; dichotomous exposure.</title>
<date>1983</date>
<journal>Journal of Epidemiology &amp; Community Health,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="24184" citStr="Liddell, 1983" startWordPosition="3798" endWordPosition="3799">7 .690 .314 .405 .337 34* .232 .344 .256 .235 .704 .269 .372 .345 234† .238 .339 .253 .227 .703 .291 .388 .348 Table 1: Fl results for the 10-fold cross-validation scenario. Feature set combination (the FS column) naming is explained in Section 4.1. Class labels: B-B/I = Backing-B/I, C-B/I = Claim-B/I, O = non-argumentative, P-B/I = Premise-B/I. Star (*) denotes that the row is significantly better than the previous row; dagger (†) means the row is not significantly better than the previous row, but is significantly better than the previous row minus one; p &lt; 0.001 using exact Liddell’s test (Liddell, 1983). classifier is trained and tested on all 11 classes including rebuttal and refutation, we do not report performance of these two argument components— the results are very poor regardless of the parameters for two reasons. First, these classes are underrepresented in the data (Rebuttal-B, RebuttalI, Refutation-B and Refutation-I are present in only about 4% of sentences). Second, the interannotator agreement reached on these classes were reported to be very low (Habernal et al., 2014). Cross validation results Table 1 shows results for the cross-validation scenario. The human baseline in the f</context>
<context position="31351" citStr="Liddell, 1983" startWordPosition="5035" endWordPosition="5036">078 .073 .522 .067 .117 .159 4 .000 .146 .083 .048 .695 .168 .156 .185 4o .104 .185 .000 .000 .689 .204 .397 .226 Table 2: Fl results for the cross-domain evaluation scenario ranked by performance. Feature set combination naming (the FS column) is explained in Section 4.1. Class labels: B-B/I = Backing-B/I, C-B/I = Claim-B/I, O = non-argumentative, P-B/I = Premise-B/I. Diamond (o) in the last (winning) row signals a significant difference between this row and all other rows while star (*) denotes that the row is significantly better than the previous row; P &lt; 0.001 using exact Liddell’s test (Liddell, 1983). FS B-B B-I C-B C-I O P-B P-I Avg FS B-B B-I C-B C-I O P-B P-I Avg Train: blogs, articles; Test: comments, forums Train: comments, forums; Test: blogs, articles 01234 .063 .259 .027 .051 .147 .000 .064 .087 34 .052 .130 .036 .037 .057 .000 .000 .045 012 .000 .000 .000 .000 .643 .000 .000 .092 01234 .000 .008 .000 .000 .003 .080 .301 .056 0 .010 .237 .000 .000 .352 .014 .036 .093 234 .055 .182 .033 .036 .121 .025 .015 .067 01 .000 .000 .000 .000 .643 .010 .013 .095 1234 .071 .176 .014 .021 .050 .061 .290 .098 0123 .021 .032 .000 .000 .645 .005 .002 .101 0 .000 .000 .000 .000 .773 .012 .019 .11</context>
<context position="32723" citStr="Liddell, 1983" startWordPosition="5292" endWordPosition="5293">.746 .063 .046 .133 34 .030 .061 .098 .099 .221 .211 .385 .158 0123 .000 .000 .000 .000 .679 .099 .227 .144 4o .076 .206 .167 .158 .611 .151 .209 .225 4o .142 .162 .061 .032 .693 .161 .353 .229 Table 3: Fl results for the cross-register evaluation scenario ranked by performance. Feature set combination naming (the FS column) is explained in Section 4.1. Class labels: B-B/I = Backing-B/I, C-B/I = Claim-B/I, O = non-argumentative, P-B/I = Premise-B/I. Diamond (o) in the last (winning) row signals a significant difference between this row and all other rows; P &lt; 0.001 using exact Liddell’s test (Liddell, 1983). performing cross-domain system in detail.14 We randomly sampled 40 documents and manually compared the predicted arguments with the gold data. We found that 11 predicted documents were simply wrong or no argument components were predicted at all (e.g., document #1640, #1658, #1021, #5258). Most of these errors occur in blogs, which seem to convey rather complex argumentation structure (#1666, #1197, #4586, #5258). In 8 documents, we identified that only some premises were (correctly) spotted by the system. This happened mostly in long comments (#452) and blogs (#400, #697, #4583). In 7 inves</context>
</contexts>
<marker>Liddell, 1983</marker>
<rawString>Douglas Liddell. 1983. Simplified exact analysis of case-referent studies: matched pairs; dichotomous exposure. Journal of Epidemiology &amp; Community Health, 37(1):82–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="15665" citStr="McCallum, 2002" startWordPosition="2398" endWordPosition="2399">a et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data provided as a part of the corpus.8 Semantic and discourse features (FS3) Features based on semantic frames has been introduced in relevant works on stance recognition (Hasan and Ng, 2013). Our features, based on PropBank semantic role labels and obtained from 7In only 1% of the sentences there are two or more argument components in it; we arbitrarily choose the largest one. 8The number of topics was empirically set to 30, therefore for each sentence the topic distribution results into 30 realvalued features. NLP Semantic Role Labeler (Choi, 2012), extract</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Mercier</author>
<author>Dan Sperber</author>
</authors>
<title>Why do humans reason? Arguments for an argumentative theory. The Behavioral and Brain Sciences, 34(2):57– 74; discussion 74–111.</title>
<date>2011</date>
<contexts>
<context position="3557" citStr="Mercier and Sperber, 2011" startWordPosition="510" endWordPosition="513">lable argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such methods in argumentation mining is questionable. First, argumentation is an act of persuasion (Nettel and Roque, 2011; Mercier and Sperber, 2011) but not all usergenerated texts can be treated as persuasive (Park and Cardie, 2014; Habernal et al., 2014), thus the selection of an appropriate unlabeled dataset represents a problem on its own. Second, argument components (e.g., claims or premises) are highly context-dependent and cannot be easily labeled in distant data using predefined patterns. So far, semi-supervised methods for argumentation mining remain unexplored. In this article, we tackle argumentation min2127 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2127–2137, Lisbon, Portugal</context>
<context position="5738" citStr="Mercier and Sperber, 2011" startWordPosition="828" endWordPosition="831"> unsupervised manner by projecting data from debate portals into a latent argument space using unsupervised word embeddings and clustering. Second, our novel features significantly outperform state-of-the-art features in all scenarios, namely in cross-validation, crossdomain evaluation, and cross-register evaluation. Third, to ensure full reproducibility of our experiments, we provide all data and source codes under free licenses.3 2 Related work Analysis of argumentation has been an active topic in numerous research areas, such as philosophy (van Eemeren et al., 2014), communication studies (Mercier and Sperber, 2011), and informal logic (Blair, 2004), among others. In this section, we will focus on the most related works on argumentation mining techniques in NLP in the first part, with an emphasis on Web data in the second part. Mochales and Moens (2011) based their work on argumentation schemes (Walton et al., 2008) and experimented with Araucaria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset</context>
</contexts>
<marker>Mercier, Sperber, 2011</marker>
<rawString>Hugo Mercier and Dan Sperber. 2011. Why do humans reason? Arguments for an argumentative theory. The Behavioral and Brain Sciences, 34(2):57– 74; discussion 74–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<editor>In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="21444" citStr="Mikolov et al. (2013)" startWordPosition="3348" endWordPosition="3351">Y and CC0, resp.) 10‘Points’ is the sum of up-votes/down-votes by other users to the particular post. Zero-point posts were usually noisy and spam-like. (cos(4,c1), ... , cos(se,ck)) where dim(Q = K and cos(•, •) denotes cosine similarity. Analogically, let L be the number of post clusters in E and dl a centroid vector of cluster l ∈ L. Then sa denotes the distance of sentence se to the post cluster centroids such that sa = (cos(se,d1), ... , cos(se,al)). We construct the feature vector by concatenating se, sc and sa. For word embeddings, we use pre-trained skipgram word vectors11 produced by Mikolov et al. (2013) (dim(E) = 300). To create clusters for the argument space features, we used CLUTO software package12 with Repeated Bisection clustering method (Zhao and Karypis, 2002). We clustered the data using different hyper-parameters K and L (we experimented with K = {50,100,500, 1000} and L = {50,100, 500,1000}). 5 Results We investigate three evaluation scenarios. First, we report 10-fold cross validation over all 340 documents, where the data are randomly distributed across the folds regardless of the domain or register. In this scenario, the model can benefit from domain-dependent features for the </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Argumentation mining.</title>
<date>2011</date>
<journal>Artificial Intelligence and Law,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1416" citStr="Mochales and Moens, 2011" startWordPosition="183" endWordPosition="186">unlabeled data in a semi-supervised manner can boost the performance of argument component identification and to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 1 Introduction Argumentation mining, an evolving sub-field of NLP, deals with analyzing argumentation1 in various genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to </context>
<context position="5980" citStr="Mochales and Moens (2011)" startWordPosition="871" endWordPosition="874">in cross-validation, crossdomain evaluation, and cross-register evaluation. Third, to ensure full reproducibility of our experiments, we provide all data and source codes under free licenses.3 2 Related work Analysis of argumentation has been an active topic in numerous research areas, such as philosophy (van Eemeren et al., 2014), communication studies (Mercier and Sperber, 2011), and informal logic (Blair, 2004), among others. In this section, we will focus on the most related works on argumentation mining techniques in NLP in the first part, with an emphasis on Web data in the second part. Mochales and Moens (2011) based their work on argumentation schemes (Walton et al., 2008) and experimented with Araucaria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et al. (2012), who classified sentences to four categories (conclusion, premise, conclusion-premise, 2For instance createdebate.com or debate.org 3https://github.com/habernal/emnlp2015 and none) and achieved 0.6</context>
</contexts>
<marker>Mochales, Moens, 2011</marker>
<rawString>Raquel Mochales and Marie-Francine Moens. 2011. Argumentation mining. Artificial Intelligence and Law, 19(1):1–22, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Laura Nettel</author>
<author>Georges Roque</author>
</authors>
<title>Persuasive argumentation versus manipulation.</title>
<date>2011</date>
<journal>Argumentation,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="3529" citStr="Nettel and Roque, 2011" startWordPosition="506" endWordPosition="509">. Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such methods in argumentation mining is questionable. First, argumentation is an act of persuasion (Nettel and Roque, 2011; Mercier and Sperber, 2011) but not all usergenerated texts can be treated as persuasive (Park and Cardie, 2014; Habernal et al., 2014), thus the selection of an appropriate unlabeled dataset represents a problem on its own. Second, argument components (e.g., claims or premises) are highly context-dependent and cannot be easily labeled in distant data using predefined patterns. So far, semi-supervised methods for argumentation mining remain unexplored. In this article, we tackle argumentation min2127 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages</context>
</contexts>
<marker>Nettel, Roque, 2011</marker>
<rawString>Ana Laura Nettel and Georges Roque. 2011. Persuasive argumentation versus manipulation. Argumentation, 26(1):55–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-projective dependency parsing in expected linear time.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, ACL ’09,</booktitle>
<pages>351--359</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="15135" citStr="Nivre, 2009" startWordPosition="2314" endWordPosition="2315">presence of discourse markers has been shown to be helpful in argument component analysis (e.g, “therefore” and “since” for premises or “think” and “believe” for claims), we encode the first and last three words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data provided as a part of the corpu</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, ACL ’09, pages 351–359, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>380--390</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta,</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 380–390, Atlanta, Georgia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying appropriate support for propositions in online user comments.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Argumentation Mining,</booktitle>
<pages>29--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="2310" citStr="Park and Cardie, 2014" startWordPosition="324" endWordPosition="327">ed by the need of 1Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including all writing and speaking which is persuasive inform (Ketcham, 1917). retrieving and understanding ordinary people’s arguments to various contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. O</context>
<context position="3641" citStr="Park and Cardie, 2014" startWordPosition="525" endWordPosition="528">illed human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such methods in argumentation mining is questionable. First, argumentation is an act of persuasion (Nettel and Roque, 2011; Mercier and Sperber, 2011) but not all usergenerated texts can be treated as persuasive (Park and Cardie, 2014; Habernal et al., 2014), thus the selection of an appropriate unlabeled dataset represents a problem on its own. Second, argument components (e.g., claims or premises) are highly context-dependent and cannot be easily labeled in distant data using predefined patterns. So far, semi-supervised methods for argumentation mining remain unexplored. In this article, we tackle argumentation min2127 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2127–2137, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ing of use</context>
<context position="7685" citStr="Park and Cardie (2014)" startWordPosition="1109" endWordPosition="1112">or length ratios), as persuasive essays usually comply with a certain structure which can be seen as a potential drawback of this approach. Regarding user-generated Web data, Biran and Rambow (2011) used naive Bayes for classifying justification of subjective claims from blogs and Wikipedia talk pages, relying on features from RST Treebank and manually-processed n-grams. In similar Web registers, Rosenthal and McKeown (2012) automatically determined whether a sentence is a claim using logistic regression and various lexical and sentiment-related features and achieved accuracy about 0.66-0.71. Park and Cardie (2014) classified propositions in user comments into three classes (verifiable experiential, verifiable non-experiential, and unverifiable) using SVM and reached 0.69 macro F1 score. Goudas et al. (2014) identified premises in Greek social media texts using BIO encoding and achieved 0.42 F1 score with Conditional Random Fields. The research gaps in the above-mentioned approaches are the following. First, the argumentation models are simplified to either claims or a few types of premises/propositions. Second, the segmentation of discourse into argument components is ignored (except the work of Goudas</context>
</contexts>
<marker>Park, Cardie, 2014</marker>
<rawString>Joonsuk Park and Claire Cardie. 2014. Identifying appropriate support for propositions in online user comments. In Proceedings of the First Workshop on Argumentation Mining, pages 29–38, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Peldszus</author>
<author>Manfred Stede</author>
</authors>
<title>Ranking the annotators: An agreement study on argumentation structure.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>196--204</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2673" citStr="Peldszus and Stede, 2013" startWordPosition="379" endWordPosition="382">including all writing and speaking which is persuasive inform (Ketcham, 1917). retrieving and understanding ordinary people’s arguments to various contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (</context>
</contexts>
<marker>Peldszus, Stede, 2013</marker>
<rawString>Andreas Peldszus and Manfred Stede. 2013. Ranking the annotators: An agreement study on argumentation structure. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 196–204, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>433--440</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="15104" citStr="Petrov et al., 2006" startWordPosition="2307" endWordPosition="2310">and syntactic features (FS1) Since the presence of discourse markers has been shown to be helpful in argument component analysis (e.g, “therefore” and “since” for premises or “think” and “believe” for claims), we encode the first and last three words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 433–440, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niall Rooney</author>
<author>Hui Wang</author>
<author>Fiona Browne</author>
</authors>
<title>Applying kernel methods to argumentation mining.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference,</booktitle>
<pages>272--275</pages>
<contexts>
<context position="6382" citStr="Rooney et al. (2012)" startWordPosition="929" endWordPosition="932">air, 2004), among others. In this section, we will focus on the most related works on argumentation mining techniques in NLP in the first part, with an emphasis on Web data in the second part. Mochales and Moens (2011) based their work on argumentation schemes (Walton et al., 2008) and experimented with Araucaria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et al. (2012), who classified sentences to four categories (conclusion, premise, conclusion-premise, 2For instance createdebate.com or debate.org 3https://github.com/habernal/emnlp2015 and none) and achieved 0.65 accuracy. These approaches assume the text is already segmented into argument components. Stab and Gurevych (2014b) examined argumentation in persuasive essays and classified argument components into four categories (premise, claim, major claim, nonargumentative) using SVM and achieved 0.73 macro F1 score. They further classified argument relations (support and attack) and reached 0.72 macro F1 sc</context>
</contexts>
<marker>Rooney, Wang, Browne, 2012</marker>
<rawString>Niall Rooney, Hui Wang, and Fiona Browne. 2012. Applying kernel methods to argumentation mining. In Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference, pages 272–275. Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Kathleen McKeown</author>
</authors>
<title>Detecting opinionated claims in online discussions.</title>
<date>2012</date>
<booktitle>In 2012 IEEE Sixth International Conference on Semantic Computing,</booktitle>
<pages>30--37</pages>
<publisher>IEEE.</publisher>
<location>Palermo, Italy.</location>
<contexts>
<context position="7491" citStr="Rosenthal and McKeown (2012)" startWordPosition="1082" endWordPosition="1086">ieved 0.73 macro F1 score. They further classified argument relations (support and attack) and reached 0.72 macro F1 score. The best-performing features were structural features (such as the location or length ratios), as persuasive essays usually comply with a certain structure which can be seen as a potential drawback of this approach. Regarding user-generated Web data, Biran and Rambow (2011) used naive Bayes for classifying justification of subjective claims from blogs and Wikipedia talk pages, relying on features from RST Treebank and manually-processed n-grams. In similar Web registers, Rosenthal and McKeown (2012) automatically determined whether a sentence is a claim using logistic regression and various lexical and sentiment-related features and achieved accuracy about 0.66-0.71. Park and Cardie (2014) classified propositions in user comments into three classes (verifiable experiential, verifiable non-experiential, and unverifiable) using SVM and reached 0.69 macro F1 score. Goudas et al. (2014) identified premises in Greek social media texts using BIO encoding and achieved 0.42 F1 score with Conditional Random Fields. The research gaps in the above-mentioned approaches are the following. First, the </context>
</contexts>
<marker>Rosenthal, McKeown, 2012</marker>
<rawString>Sara Rosenthal and Kathleen McKeown. 2012. Detecting opinionated claims in online discussions. In 2012 IEEE Sixth International Conference on Semantic Computing, pages 30–37, Palermo, Italy. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Active Learning.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="3287" citStr="Settles, 2012" startWordPosition="470" endWordPosition="471">. Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such methods in argumentation mining is questionable. First, argumentation is an act of persuasion (Nettel and Roque, 2011; Mercier and Sperber, 2011) but not all usergenerated texts can be treated as persuasive (Park and Cardie, 2014; Habernal et al., 2014), thus the selection of an appropriate unlabeled dataset represents a problem on its own. Second, argument components (e.g., claims or premises) are highly context-dependent and cannot be easily labeled in distant data usi</context>
</contexts>
<marker>Settles, 2012</marker>
<rawString>Burr Settles. 2012. Active Learning. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="15348" citStr="Socher et al., 2013" startWordPosition="2347" endWordPosition="2350"> words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002) with topics trained on unlabeled data provided as a part of the corpus.8 Semantic and discourse features (FS3) Features based on semantic frames has been introduced in relevant works on stance recognition (Hasan and Ng, 2013). Our features, based on PropBank semantic role labels an</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<date>2013</date>
<booktitle>Semi-Supervised Learning and Domain Adaptation in Natural Language Processing.</booktitle>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="18733" citStr="Søgaard (2013)" startWordPosition="2888" endWordPosition="2889">s vectors in the phrase; see for example (Le and Mikolov, 2014). Neighboring vectors in the latent vector space exhibit some interesting properties, such as semantic similarity (thoroughly studied within the distributional semantics area). If the latent vector space is clustered, each n-dimensional vector gets reduced to a single cluster number; such clusters have been used directly as features in many tasks, such as NER (Turian et al., 2010), POS tagging (Owoputi 2130 et al., 2013), or sentiment analysis (Habernal and Brychc´ın, 2013). We build upon the above-mentioned approach (described by Søgaard (2013) as ‘clusters-asfeatures’ semi-supervised paradigm) and extend it further. We take both sentences and posts from the unlabeled debate portals, project them into a latent space using word embeddings and cluster them. The motivation is that these clusters will contain similar phrases or (similar ‘arguments’). Centroids of these clusters would then represent a ‘prototypical argument’ (note that the centroids exist only in the latent vector space and thus do not correspond to any existing sentence or post). Then we project each sentence (classification unit) in the labeled data to the latent vecto</context>
</contexts>
<marker>Søgaard, 2013</marker>
<rawString>Anders Søgaard. 2013. Semi-Supervised Learning and Domain Adaptation in Natural Language Processing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Stab</author>
<author>Iryna Gurevych</author>
</authors>
<title>Annotating argument components and relations in persuasive essays.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1501--1510</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1457" citStr="Stab and Gurevych, 2014" startWordPosition="189" endWordPosition="192">can boost the performance of argument component identification and to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 1 Introduction Argumentation mining, an evolving sub-field of NLP, deals with analyzing argumentation1 in various genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including</context>
<context position="2842" citStr="Stab and Gurevych, 2014" startWordPosition="403" endWordPosition="406">e large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such methods in argumentation minin</context>
<context position="6695" citStr="Stab and Gurevych (2014" startWordPosition="966" endWordPosition="969">aria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et al. (2012), who classified sentences to four categories (conclusion, premise, conclusion-premise, 2For instance createdebate.com or debate.org 3https://github.com/habernal/emnlp2015 and none) and achieved 0.65 accuracy. These approaches assume the text is already segmented into argument components. Stab and Gurevych (2014b) examined argumentation in persuasive essays and classified argument components into four categories (premise, claim, major claim, nonargumentative) using SVM and achieved 0.73 macro F1 score. They further classified argument relations (support and attack) and reached 0.72 macro F1 score. The best-performing features were structural features (such as the location or length ratios), as persuasive essays usually comply with a certain structure which can be seen as a potential drawback of this approach. Regarding user-generated Web data, Biran and Rambow (2011) used naive Bayes for classifying </context>
<context position="15009" citStr="Stab and Gurevych, 2014" startWordPosition="2292" endWordPosition="2295">nce of unigrams, bigrams, and trigrams in the sentence as ‘one-hot’ (binary) features. Structural and syntactic features (FS1) Since the presence of discourse markers has been shown to be helpful in argument component analysis (e.g, “therefore” and “since” for premises or “think” and “believe” for claims), we encode the first and last three words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs s</context>
</contexts>
<marker>Stab, Gurevych, 2014</marker>
<rawString>Christian Stab and Iryna Gurevych. 2014a. Annotating argument components and relations in persuasive essays. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1501–1510, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Stab</author>
<author>Iryna Gurevych</author>
</authors>
<title>Identifying argumentative discourse structures in persuasive essays.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>46--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="1457" citStr="Stab and Gurevych, 2014" startWordPosition="189" endWordPosition="192">can boost the performance of argument component identification and to which extent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 1 Introduction Argumentation mining, an evolving sub-field of NLP, deals with analyzing argumentation1 in various genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including</context>
<context position="2842" citStr="Stab and Gurevych, 2014" startWordPosition="403" endWordPosition="406">e large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation corpora exist, as annotations are costly, error-prone, and require skilled human annotators (Stab and Gurevych, 2014a; Habernal et al., 2014). To overcome the limited scope and size of the existing annotated corpora, semi-supervised methods can be adopted, as they gain performance by exploiting large unlabeled datasets (Settles, 2012). However, unlike in other NLP tasks where data can be cheaply labeled using for example distant supervision, employing such methods in argumentation minin</context>
<context position="6695" citStr="Stab and Gurevych (2014" startWordPosition="966" endWordPosition="969">aria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et al. (2012), who classified sentences to four categories (conclusion, premise, conclusion-premise, 2For instance createdebate.com or debate.org 3https://github.com/habernal/emnlp2015 and none) and achieved 0.65 accuracy. These approaches assume the text is already segmented into argument components. Stab and Gurevych (2014b) examined argumentation in persuasive essays and classified argument components into four categories (premise, claim, major claim, nonargumentative) using SVM and achieved 0.73 macro F1 score. They further classified argument relations (support and attack) and reached 0.72 macro F1 score. The best-performing features were structural features (such as the location or length ratios), as persuasive essays usually comply with a certain structure which can be seen as a potential drawback of this approach. Regarding user-generated Web data, Biran and Rambow (2011) used naive Bayes for classifying </context>
<context position="15009" citStr="Stab and Gurevych, 2014" startWordPosition="2292" endWordPosition="2295">nce of unigrams, bigrams, and trigrams in the sentence as ‘one-hot’ (binary) features. Structural and syntactic features (FS1) Since the presence of discourse markers has been shown to be helpful in argument component analysis (e.g, “therefore” and “since” for premises or “think” and “believe” for claims), we encode the first and last three words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs s</context>
</contexts>
<marker>Stab, Gurevych, 2014</marker>
<rawString>Christian Stab and Iryna Gurevych. 2014b. Identifying argumentative discourse structures in persuasive essays. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 46–56, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--409</pages>
<contexts>
<context position="1534" citStr="Teufel and Moens, 2002" startWordPosition="200" endWordPosition="203">xtent is the approach independent of domain and register. We propose novel features that exploit clustering of unlabeled data from debate portals based on a word embeddings representation. Using these features, we significantly outperform several baselines in the cross-validation, cross-domain, and cross-register evaluation scenarios. 1 Introduction Argumentation mining, an evolving sub-field of NLP, deals with analyzing argumentation1 in various genres, such as legal cases (Mochales and Moens, 2011), student essays (Stab and Gurevych, 2014a), and medical and scientific articles (Green, 2014; Teufel and Moens, 2002). Recently, the focus of argumentation mining has also shifted to the Web registers (such as comments to articles, forum posts, or blogs) which is motivated by the need of 1Argumentation is a verbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including all writing and speaking which is persuasive inform (Ketcham, 1917). retriev</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28:409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Toulmin</author>
</authors>
<title>The Uses of Argument.</title>
<date>1958</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11079" citStr="Toulmin, 1958" startWordPosition="1672" endWordPosition="1673"> Habernal et al. (2014). Compared to other corpora mentioned in the related work, this corpus is the largest one to date that covers different domains and spans several registers of usergenerated Web content. In particular, the corpus comprises four registers (comments to articles, forum posts, blogs, and argumentative newswire articles) and covers six domains related to educational controversies (homeschooling, private vs. public schools, mainstreaming, single-sex education, prayer in schools, and redshirting). The argumentation model used in this corpus is based on extended Toulmin’s model (Toulmin, 1958). Each document contains usually one argument, where each argument consists of several argument components. There are five different components in this model, namely, the claim (the statement about to be established in the argument which conveys author’s stance towards the topic), the premise(s) (propositions that are intended to give reasons of some kind for the claim), the backing (additional information used to back-up the argument), the rebuttal (attacks the claim), and the refutation (which attacks the rebuttal). Relations between the argument components are encoded implicitly in the func</context>
</contexts>
<marker>Toulmin, 1958</marker>
<rawString>Stephen E. Toulmin. 1958. The Uses of Argument. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Edmonton, Canada.</location>
<contexts>
<context position="15065" citStr="Toutanova et al., 2003" startWordPosition="2301" endWordPosition="2304">s ‘one-hot’ (binary) features. Structural and syntactic features (FS1) Since the presence of discourse markers has been shown to be helpful in argument component analysis (e.g, “therefore” and “since” for premises or “think” and “believe” for claims), we encode the first and last three words as binary features. Furthermore, we capture the relative position of the sentence in the paragraph and the document, the number of part of speech 1-3 grams, maximum dependency tree depth, constituency tree production rules, and number of sub-clauses (Stab and Gurevych, 2014b). We used Stanford POS Tagger (Toutanova et al., 2003), Berkeley parser (Petrov et al., 2006), and Malt parser (Nivre, 2009). Sentiment and topic features (FS2) We assume that claims express sentiment, thus we compute five sentiment categories (from very negative to very positive) using Stanford sentiment analyzer (Socher et al., 2013) and use these values directly as features. Furthermore, in order to help detecting off-topic and non-argument sentences, we employ topic model features. In particular, we use features taken from a vector representation of the sentence obtained by using Gibbs sampling on LDA model (Blei et al., 2003; McCallum, 2002)</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -Volume 1, NAACL ’03, pages 173–180, Edmonton, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), number July,</booktitle>
<pages>384--394</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="18565" citStr="Turian et al., 2010" startWordPosition="2862" endWordPosition="2865">features. We assume that phrases (sentences or documents) can be projected into a latent vector space, using, typically, a sum or a weighted average of all the word embeddings vectors in the phrase; see for example (Le and Mikolov, 2014). Neighboring vectors in the latent vector space exhibit some interesting properties, such as semantic similarity (thoroughly studied within the distributional semantics area). If the latent vector space is clustered, each n-dimensional vector gets reduced to a single cluster number; such clusters have been used directly as features in many tasks, such as NER (Turian et al., 2010), POS tagging (Owoputi 2130 et al., 2013), or sentiment analysis (Habernal and Brychc´ın, 2013). We build upon the above-mentioned approach (described by Søgaard (2013) as ‘clusters-asfeatures’ semi-supervised paradigm) and extend it further. We take both sentences and posts from the unlabeled debate portals, project them into a latent space using word embeddings and cluster them. The motivation is that these clusters will contain similar phrases or (similar ‘arguments’). Centroids of these clusters would then represent a ‘prototypical argument’ (note that the centroids exist only in the laten</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), number July, pages 384– 394, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans H van Eemeren</author>
<author>R Grootendorst</author>
<author>A F</author>
</authors>
<title>Snoeck Henkemans.</title>
<date>2002</date>
<location>Mahwah, NJ, USA.</location>
<marker>van Eemeren, Grootendorst, F, 2002</marker>
<rawString>Frans H. van Eemeren, R. Grootendorst, and A. F. Snoeck Henkemans. 2002. Argumentation: Analysis, evaluation, presentation. Lawrence Erlbaum, Mahwah, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans H van Eemeren</author>
<author>Bart Garssen</author>
<author>Erik C W Krabbe</author>
<author>A Francisca Snoeck Henkemans</author>
<author>Bart Verheij</author>
<author>Jean H M Wagemans</author>
</authors>
<title>Handbook of Argumentation Theory.</title>
<date>2014</date>
<publisher>Springer, Berlin/Heidelberg.</publisher>
<marker>van Eemeren, Garssen, Krabbe, Henkemans, Verheij, Wagemans, 2014</marker>
<rawString>Frans H. van Eemeren, Bart Garssen, Erik C. W. Krabbe, A. Francisca Snoeck Henkemans, Bart Verheij, and Jean H. M. Wagemans. 2014. Handbook of Argumentation Theory. Springer, Berlin/Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henning Wachsmuth</author>
<author>Martin Trenkmann</author>
<author>Benno Stein</author>
<author>Gregor Engels</author>
<author>Tsvetomira Palakarska</author>
</authors>
<title>A review corpus for argumentation analysis.</title>
<date>2014</date>
<booktitle>In Alexander Gelbukh, editor, 15th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 14),</booktitle>
<pages>115--127</pages>
<publisher>Springer.</publisher>
<location>Kathmandu, Nepal.</location>
<contexts>
<context position="2350" citStr="Wachsmuth et al., 2014" startWordPosition="330" endWordPosition="333">erbal activity for which the goal consists of convincing the listener or reader of the acceptability of a standpoint by means of a constellation ofpropositions justifying or refuting the proposition expressed in the standpoint (van Eemeren et al., 2002) or the art of persuading others to think or act in a definite way, including all writing and speaking which is persuasive inform (Ketcham, 1917). retrieving and understanding ordinary people’s arguments to various contentious topics on the large scale. Applications include passenger rights and protection (Park and Cardie, 2014), hotel reviews (Wachsmuth et al., 2014), and controversies in education (Habernal et al., 2014). Despite the plethora of existing argumentation theories (van Eemeren et al., 2014), the prevalent view in argumentation mining treats arguments as discourse structures consisting of several argument components, such as claims and premises (Peldszus and Stede, 2013). Current approaches to automatic analysis of argumentation usually follow the fully supervised machinelearning paradigm (Biran and Rambow, 2011; Stab and Gurevych, 2014b; Park and Cardie, 2014) and rely on manually annotated datasets. Only few publicly available argumentation</context>
</contexts>
<marker>Wachsmuth, Trenkmann, Stein, Engels, Palakarska, 2014</marker>
<rawString>Henning Wachsmuth, Martin Trenkmann, Benno Stein, Gregor Engels, and Tsvetomira Palakarska. 2014. A review corpus for argumentation analysis. In Alexander Gelbukh, editor, 15th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 14), pages 115–127, Kathmandu, Nepal. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Walton</author>
<author>Christopher Reed</author>
<author>Fabrizio Macagno</author>
</authors>
<title>Argumentation Schemes.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6044" citStr="Walton et al., 2008" startWordPosition="881" endWordPosition="884">ation. Third, to ensure full reproducibility of our experiments, we provide all data and source codes under free licenses.3 2 Related work Analysis of argumentation has been an active topic in numerous research areas, such as philosophy (van Eemeren et al., 2014), communication studies (Mercier and Sperber, 2011), and informal logic (Blair, 2004), among others. In this section, we will focus on the most related works on argumentation mining techniques in NLP in the first part, with an emphasis on Web data in the second part. Mochales and Moens (2011) based their work on argumentation schemes (Walton et al., 2008) and experimented with Araucaria and ECHR datasets using supervised models to classify argumentative and non-argumentative sentences (≈ 0.7F1) and their structure. Feng and Hirst (2011) classified argument schemes on the Araucaria dataset, reaching 0.6-0.9 accuracy. Experiments on this dataset were also conducted by Rooney et al. (2012), who classified sentences to four categories (conclusion, premise, conclusion-premise, 2For instance createdebate.com or debate.org 3https://github.com/habernal/emnlp2015 and none) and achieved 0.65 accuracy. These approaches assume the text is already segmente</context>
</contexts>
<marker>Walton, Reed, Macagno, 2008</marker>
<rawString>Douglas Walton, Christopher Reed, and Fabrizio Macagno. 2008. Argumentation Schemes. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhao</author>
<author>G Karypis</author>
</authors>
<title>Criterion functions for document clustering: Experiments and analysis.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, University of Minnesota,</institution>
<location>Minneapolis.</location>
<contexts>
<context position="21612" citStr="Zhao and Karypis, 2002" startWordPosition="3375" endWordPosition="3378">.. , cos(se,ck)) where dim(Q = K and cos(•, •) denotes cosine similarity. Analogically, let L be the number of post clusters in E and dl a centroid vector of cluster l ∈ L. Then sa denotes the distance of sentence se to the post cluster centroids such that sa = (cos(se,d1), ... , cos(se,al)). We construct the feature vector by concatenating se, sc and sa. For word embeddings, we use pre-trained skipgram word vectors11 produced by Mikolov et al. (2013) (dim(E) = 300). To create clusters for the argument space features, we used CLUTO software package12 with Repeated Bisection clustering method (Zhao and Karypis, 2002). We clustered the data using different hyper-parameters K and L (we experimented with K = {50,100,500, 1000} and L = {50,100, 500,1000}). 5 Results We investigate three evaluation scenarios. First, we report 10-fold cross validation over all 340 documents, where the data are randomly distributed across the folds regardless of the domain or register. In this scenario, the model can benefit from domain-dependent features for the testing data, such as lexical knowledge (FS0) or domainrelevant argument space features (FS4). Second, we evaluate the cross-domain performance; the model is always tra</context>
</contexts>
<marker>Zhao, Karypis, 2002</marker>
<rawString>Y. Zhao and G. Karypis. 2002. Criterion functions for document clustering: Experiments and analysis. Technical report, Department of Computer Science, University of Minnesota, Minneapolis.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
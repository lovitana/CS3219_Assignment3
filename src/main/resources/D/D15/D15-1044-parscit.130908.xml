<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.994298">
A Neural Attention Model for Sentence Summarization
</title>
<author confidence="0.838297">
Alexander M. Rush
</author>
<affiliation confidence="0.652884">
Facebook AI Research /
</affiliation>
<address confidence="0.593031">
Harvard SEAS
</address>
<email confidence="0.973916">
srush@seas.harvard.edu
</email>
<author confidence="0.787255">
Sumit Chopra
</author>
<affiliation confidence="0.742374">
Facebook AI Research
</affiliation>
<email confidence="0.928975">
spchopra@fb.com
</email>
<author confidence="0.91471">
Jason Weston
</author>
<affiliation confidence="0.860326">
Facebook AI Research
</affiliation>
<email confidence="0.975991">
jase@fb.com
</email>
<sectionHeader confidence="0.997161" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999559333333333">
Summarization based on text extraction is
inherently limited, but generation-style ab-
stractive methods have proven challeng-
ing to build. In this work, we propose
a fully data-driven approach to abstrac-
tive sentence summarization. Our method
utilizes a local attention-based model that
generates each word of the summary con-
ditioned on the input sentence. While the
model is structurally simple, it can eas-
ily be trained end-to-end and scales to a
large amount of training data. The model
shows significant performance gains on
the DUC-2004 shared task compared with
several strong baselines.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997207136363636">
Summarization is an important challenge of natu-
ral language understanding. The aim is to produce
a condensed representation of an input text that
captures the core meaning of the original. Most
successful summarization systems utilize extrac-
tive approaches that crop out and stitch together
portions of the text to produce a condensed ver-
sion. In contrast, abstractive summarization at-
tempts to produce a bottom-up summary, aspects
of which may not appear as part of the original.
We focus on the task of sentence-level sum-
marization. While much work on this task has
looked at deletion-based sentence compression
techniques (Knight and Marcu (2002), among
many others), studies of human summarizers show
that it is common to apply various other operations
while condensing, such as paraphrasing, general-
ization, and reordering (Jing, 2002). Past work
has modeled this abstractive summarization prob-
lem either using linguistically-inspired constraints
(Dorr et al., 2003; Zajic et al., 2004) or with syn-
tactic transformations of the input text (Cohn and
</bodyText>
<figureCaption confidence="0.9873424">
Figure 1: Example output of the attention-based summa-
rization (ABS) system. The heatmap represents a soft align-
ment between the input (right) and the generated summary
(top). The columns represent the distribution over the input
after generating each word.
</figureCaption>
<bodyText confidence="0.99151280952381">
Lapata, 2008; Woodsend et al., 2010). These ap-
proaches are described in more detail in Section 6.
We instead explore a fully data-driven approach
for generating abstractive summaries. Inspired by
the recent success of neural machine translation,
we combine a neural language model with a con-
textual input encoder. Our encoder is modeled
off of the attention-based encoder of Bahdanau et
al. (2014) in that it learns a latent soft alignment
over the input text to help inform the summary (as
shown in Figure 1). Crucially both the encoder
and the generation model are trained jointly on the
sentence summarization task. The model is de-
scribed in detail in Section 3. Our model also in-
corporates a beam-search decoder as well as addi-
tional features to model extractive elements; these
aspects are discussed in Sections 4 and 5.
This approach to summarization, which we call
Attention-Based Summarization (ABS), incorpo-
rates less linguistic structure than comparable ab-
stractive summarization approaches, but can easily
</bodyText>
<page confidence="0.986052">
379
</page>
<note confidence="0.9857775">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379–389,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.928775">
Input (x1, ... , x18). First sentence of article:
russian defense minister ivanov called sunday for the creation of a joint front for combating global terrorism
Output (y1, ... , y8). Generated headline:
</bodyText>
<note confidence="0.261808">
russia calls for joint front against terrorism ⇐ g(terrorism, x, for, joint, front, against)
</note>
<figureCaption confidence="0.95706725">
Figure 2: Example input sentence and the generated summary. The score of generating yi+1 (terrorism) is based on the
context yr (for ... against) as well as the input x1 ... x18. Note that the summary generated is abstractive which makes
it possible to generalize (russian defense minister to russia) and paraphrase (for combating to against),
in addition to compressing (dropping the creation of), see Jing (2002) for a survey of these editing operations.
</figureCaption>
<bodyText confidence="0.998744515151515">
scale to train on a large amount of data. Since our
system makes no assumptions about the vocabu-
lary of the generated summary it can be trained
directly on any document-summary pair.1 This
allows us to train a summarization model for
headline-generation on a corpus of article pairs
from Gigaword (Graff et al., 2003) consisting of
around 4 million articles. An example of genera-
tion is given in Figure 2, and we discuss the details
of this task in Section 7.
To test the effectiveness of this approach we
run extensive comparisons with multiple abstrac-
tive and extractive baselines, including traditional
syntax-based systems, integer linear program-
constrained systems, information-retrieval style
approaches, as well as statistical phrase-based ma-
chine translation. Section 8 describes the results
of these experiments. Our approach outperforms
a machine translation system trained on the same
large-scale dataset and yields a large improvement
over the highest scoring system in the DUC-2004
competition.
a sequence y1, ... , yN. Note that in contrast to
related tasks, like machine translation, we will as-
sume that the output length N is fixed, and that
the system knows the length of the summary be-
fore generation.2
Next consider the problem of gen-
erating summaries. Define the set
Y C ({0,1}V , ... , {0,1}V ) as all possible
sentences of length N, i.e. for all i and y E Y, yi
is an indicator. We say a system is abstractive if it
tries to find the optimal sequence from this set Y,
</bodyText>
<equation confidence="0.617725">
arg max s(x, y), (1)
yEY
</equation>
<bodyText confidence="0.998407333333333">
under a scoring function s : X xY H R. Contrast
this to a fully extractive sentence summary3 which
transfers words from the input:
</bodyText>
<equation confidence="0.455874">
mE11,...MJN
</equation>
<bodyText confidence="0.965633">
arg max s(x, x[m1,...,mN]), (2)
or to the related problem of sentence compression
that concentrates on deleting words from the input:
2 Background arg max s(x, x[m1,...,mN]). (3)
mE11,...MJN,mi−1&lt;mi
We begin by defining the sentence summarization
task. Given an input sentence, the goal is to pro-
duce a condensed summary. Let the input con-
sist of a sequence of M words x1, ... , xM com-
ing from a fixed vocabulary V of size |V |= V .
We will represent each word as an indicator vector
xi E {0,1}V for i E {1, ... , M}, sentences as a
sequence of indicators, and X as the set of possi-
ble inputs. Furthermore define the notation x[i,j,k]
to indicate the sub-sequence of elements i, j, k.
A summarizer takes x as input and outputs a
shortened sentence y of length N &lt; M. We will
assume that the words in the summary also come
from the same vocabulary V and that the output is
</bodyText>
<footnote confidence="0.854271333333333">
1In contrast to a large-scale sentence compression sys-
tems like Filippova and Altun (2013) which require mono-
tonic aligned compressions.
</footnote>
<bodyText confidence="0.999485375">
While abstractive summarization poses a more dif-
ficult generation challenge, the lack of hard con-
straints gives the system more freedom in genera-
tion and allows it to fit with a wider range of train-
ing data.
In this work we focus on factored scoring func-
tions, s, that take into account a fixed window of
previous words:
</bodyText>
<equation confidence="0.983717">
s(x, y) ≈ N−1� g(yi+1, x, yc), (4)
i=0
</equation>
<footnote confidence="0.9845565">
2For the DUC-2004 evaluation, it is actually the number
of bytes of the output that is capped. More detail is given in
Section 7.
3Unfortunately the literature is inconsistent on the formal
definition of this distinction. Some systems self-described as
abstractive would be extractive under our definition.
</footnote>
<page confidence="0.998037">
380
</page>
<bodyText confidence="0.936612">
where we define yc °= y[i−C+1,...,i] for a window
of size C.
In particular consider the conditional log-
probability of a summary given the input,
s(x, y) = log p(y|x; 0). We can write this as:
</bodyText>
<equation confidence="0.9835505">
log p(y|x; 0) � N−1� log p(yi+1|x, yc; 0),
i=0
</equation>
<bodyText confidence="0.999869">
where we make a Markov assumption on the
length of the context as size C and assume for
i &lt; 1, yi is a special start symbol (S).
With this scoring function in mind, our main
focus will be on modelling the local conditional
distribution: p(yi+1|x, yc; 0). The next section
defines a parameterization for this distribution, in
Section 4, we return to the question of generation
for factored models, and in Section 5 we introduce
a modified factored scoring function.
</bodyText>
<sectionHeader confidence="0.99688" genericHeader="introduction">
3 Model
</sectionHeader>
<bodyText confidence="0.965792666666667">
The distribution of interest, p(yi+1|x, yc; 0), is
a conditional language model based on the in-
put sentence x. Past work on summarization and
compression has used a noisy-channel approach to
split and independently estimate a language model
and a conditional summarization model (Banko et
al., 2000; Knight and Marcu, 2002; Daum´e III and
Marcu, 2002), i.e.,
arg max log p(y|x) = argmax log p(y)p(x|y)
</bodyText>
<equation confidence="0.760152">
Y Y
</equation>
<bodyText confidence="0.999921571428571">
where p(y) and p(x|y) are estimated separately.
Here we instead follow work in neural machine
translation and directly parameterize the original
distribution as a neural network. The network con-
tains both a neural probabilistic language model
and an encoder which acts as a conditional sum-
marization model.
</bodyText>
<subsectionHeader confidence="0.998064">
3.1 Neural Language Model
</subsectionHeader>
<bodyText confidence="0.99956">
The core of our parameterization is a language
model for estimating the contextual probability of
the next word. The language model is adapted
from a standard feed-forward neural network lan-
guage model (NNLM), particularly the class of
NNLMs described by Bengio et al. (2003). The
full model is:
</bodyText>
<equation confidence="0.998486666666667">
p(yi+1|yc, x; 0) a exp(Vh + Wenc(x, yc)),
˜yc = [Eyi−C+1, ... , Eyi],
h = tanh(U˜yc).
</equation>
<figureCaption confidence="0.985534333333333">
Figure 3: (a) A network diagram for the NNLM decoder
with additional encoder element. (b) A network diagram for
the attention-based encoder enc3.
</figureCaption>
<equation confidence="0.77871175">
The parameters are 0 = (E, U, V, W) where
E E RDxV is a word embedding matrix, U E
R(CD)xH, V E RV xH, W E RV xH are weight
matrices,4 D is the size of the word embeddings,
</equation>
<bodyText confidence="0.997929714285714">
and h is a hidden layer of size H. The black-box
function enc is a contextual encoder term that re-
turns a vector of size H representing the input and
current context; we consider several possible vari-
ants, described subsequently. Figure 3a gives a
schematic representation of the decoder architec-
ture.
</bodyText>
<subsectionHeader confidence="0.998536">
3.2 Encoders
</subsectionHeader>
<bodyText confidence="0.999770454545455">
Note that without the encoder term this represents
a standard language model. By incorporating in
enc and training the two elements jointly we cru-
cially can incorporate the input text into genera-
tion. We discuss next several possible instantia-
tions of the encoder.
Bag-of-Words Encoder Our most basic model
simply uses the bag-of-words of the input sentence
embedded down to size H, while ignoring proper-
ties of the original order or relationships between
neighboring words. We write this model as:
</bodyText>
<equation confidence="0.999983666666667">
enc1(x,yc) = pT˜x,
p = [1/M, ... , 1/M],
x˜ = [Fx1, ... , FxM].
</equation>
<bodyText confidence="0.91248475">
Where the input-side embedding matrix F E
RHxV is the only new parameter of the encoder
and p E [0, 1]M is a uniform distribution over the
input words.
</bodyText>
<footnote confidence="0.990637666666667">
4Each of the weight matrices U, V, W also has a cor-
responding bias term. For readability, we omit these terms
throughout the paper.
</footnote>
<figure confidence="0.8087631">
p(yi+1|x, yc; 0)
enc
W
x
V
˜yc
yc
h
U
E
(a) (b)
enc3
F G
x¯
x˜
x
P
˜yC
yc
p
</figure>
<page confidence="0.990881">
381
</page>
<bodyText confidence="0.999960125">
For summarization this model can capture the
relative importance of words to distinguish con-
tent words from stop words or embellishments.
Potentially the model can also learn to combine
words; although it is inherently limited in repre-
senting contiguous phrases.
Convolutional Encoder To address some of the
modelling issues with bag-of-words we also con-
sider using a deep convolutional encoder for the
input sentence. This architecture improves on the
bag-of-words model by allowing local interactions
between words while also not requiring the con-
text yc while encoding the input.
We utilize a standard time-delay neural network
(TDNN) architecture, alternating between tempo-
ral convolution layers and max pooling layers.
</bodyText>
<equation confidence="0.999578857142857">
dj, enc2(x, yc)j = max˜xL i,j, (5)
i
di, l E {1,... L}, ˜xlj = tanh(max{¯xl2i−1, ¯xl2i}),
(6)
di, l E {1, ... L}, ¯xli = Ql˜xl−1
[i−Q,...,i+Q], (7)
˜x0 = [Fx1, ... , FxM]. (8)
</equation>
<bodyText confidence="0.9987237">
Where F is a word embedding matrix and
QLxHx2Q+1 consists of a set of filters for each
layer 11,... L}. Eq. 7 is a temporal (1D) convolu-
tion layer, Eq. 6 consists of a 2-element temporal
max pooling layer and a pointwise non-linearity,
and final output Eq. 5 is a max over time. At each
layer x˜ is one half the size of ¯x. For simplicity
we assume that the convolution is padded at the
boundaries, and that M is greater than 2L so that
the dimensions are well-defined.
Attention-Based Encoder While the convolu-
tional encoder has richer capacity than bag-of-
words, it still is required to produce a single rep-
resentation for the entire input sentence. A simi-
lar issue in machine translation inspired Bahdanau
et al. (2014) to instead utilize an attention-based
contextual encoder that constructs a representation
based on the generation context. Here we note that
if we exploit this context, we can actually use a
rather simple model similar to bag-of-words:
</bodyText>
<equation confidence="0.988341571428571">
enc3(x, yc) = pT¯x,
p a exp(˜xP˜y&apos;),
x˜ = [Fx1, ... , Fxm],
˜yc = [Gyi−C+1, ... , Gyi],
i+Q
Vi ¯xi = ˜xi/Q.
q=i−Q
</equation>
<bodyText confidence="0.999581894736842">
Where G E RDxV is an embedding of the con-
text, P E 1fL Hx(CD) is a new weight matrix pa-
rameter malLp�ping between the context embedding
and input embedding, and Q is a smoothing win-
dow. The full model is shown in Figure 3b.
Informally we can think of this model as simply
replacing the uniform distribution in bag-of-words
with a learned soft alignment, P, between the in-
put and the summary. Figure 1 shows an exam-
ple of this distribution p as a summary is gener-
ated. The soft alignment is then used to weight
the smoothed version of the input x¯ when con-
structing the representation. For instance if the
current context aligns well with position i then
the words xi−Q, ... , xi+Q are highly weighted
by the encoder. Together with the NNLM, this
model can be seen as a stripped-down version
of the attention-based neural machine translation
model.5
</bodyText>
<subsectionHeader confidence="0.997848">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.9997552">
The lack of generation constraints makes it pos-
sible to train the model on arbitrary input-output
pairs. Once we have defined the local condi-
tional model, p(yi+1Jx, yc; B), we can estimate
the parameters to minimize the negative log-
likelihood of a set of summaries. Define this train-
ing set as consisting of J input-summary pairs
(x(1), y(1)), ... , (x(J), y(J)). The negative log-
likelihood conveniently factors6 into a term for
each token in the summary:
</bodyText>
<equation confidence="0.968354">
log p(y(j)|x(j); B),
log p(y(j)
i+1|x(j), yc; B).
</equation>
<bodyText confidence="0.999783666666667">
We minimize NLL by using mini-batch stochastic
gradient descent. The details are described further
in Section 7.
</bodyText>
<footnote confidence="0.65804325">
5To be explicit, compared to Bahdanau et al. (2014)
our model uses an NNLM instead of a target-side LSTM,
source-side windowed averaging instead of a source-side bi-
directional RNN, and a weighted dot-product for alignment
instead of an alignment MLP.
6This is dependent on using the gold standard contexts
yc. An alternative is to use the predicted context within a
structured or reenforcement-learning style objective.
</footnote>
<equation confidence="0.995749125">
J
NLL(B) = − E
j=1
J
E
j=1
N−1�
i=1
</equation>
<page confidence="0.991434">
382
</page>
<sectionHeader confidence="0.842556" genericHeader="method">
4 Generating Summaries
</sectionHeader>
<bodyText confidence="0.9984395">
We now return to the problem of generating sum-
maries. Recall from Eq. 4 that our goal is to find,
can be computed as a mini-batch, which in prac-
tice greatly reduces the factor of K.
</bodyText>
<listItem confidence="0.696042">
5 Extension: Extractive Tuning
</listItem>
<equation confidence="0.961375">
y* = arg max
YEY
</equation>
<bodyText confidence="0.999581157894737">
Unlike phrase-based machine translation where
inference is NP-hard, it actually is tractable in the-
ory to compute y*. Since there is no explicit hard
alignment constraint, Viterbi decoding can be ap-
plied and requires O(NV C) time to find an exact
solution. In practice though V is large enough to
make this difficult.
An alternative approach is to approximate the
arg max with a strictly greedy or deterministic de-
coder. While decoders of this form can produce
very bad approximations, they have shown to be
relatively effective and fast for neural MT models
(Sutskever et al., 2014).
A compromise between exact and greedy de-
coding is to use a beam-search decoder (Algo-
rithm 1) which maintains the full vocabulary V
while limiting itself to K potential hypotheses at
each position of the summary. The beam-search
algorithm is shown here:
</bodyText>
<table confidence="0.550978666666667">
Algorithm 1 Beam Search
Input: Parameters θ, beam size K, input x
Output: Approx. K-best summaries
</table>
<equation confidence="0.951079769230769">
π[0] ← {e}
S = V if abstractive else {xi  |∀i}
for i = 0 to N − 1 do
. Generate Hypotheses
N ← {[y, yi+1]  |y ∈ π[i], yi+1 ∈ S}
. Hypothesis Recombination
H ← Jy ∈ N  |s(y, x) &gt; s(Y , x) &apos; �
l ∀y ∈ N s.t. yc = yc
. Filter K-Max
π[i + 1] ← K-arg max g(yi+1, yc, x) + s(y, x)
YEN
end for
return π[N]
</equation>
<bodyText confidence="0.999557629629629">
As with Viterbi this beam search algorithm is
much simpler than beam search for phrase-based
MT. Because there is no explicit constraint that
each source word be used exactly once there is
no need to maintain a bit set and we can sim-
ply move from left-to-right generating words. The
beam search algorithm requires O(KNV) time.
From a computational perspective though, each
round of beam search is dominated by computing
p(yi|x, yc) for each of the K hypotheses. These
While we will see that the attention-based model
is effective at generating summaries, it does miss
an important aspect seen in the human-generated
references. In particular the abstractive model
does not have the capacity to find extractive word
matches when necessary, for example transferring
unseen proper noun phrases from the input. Simi-
lar issues have also been observed in neural trans-
lation models particularly in terms of translating
rare words (Luong et al., 2014).
To address this issue we experiment with tuning
a very small set of additional features that trade-
off the abstractive/extractive tendency of the sys-
tem. We do this by modifying our scoring function
to directly estimate the probability of a summary
using a log-linear model, as is standard in machine
translation:
</bodyText>
<equation confidence="0.9936075">
p(y|x; θ, α) a exp(αT N−1� f(yi+1, x, yc)).
i=0
</equation>
<bodyText confidence="0.9992205">
Where α E R5 is a weight vector and f is a fea-
ture function. Finding the best summary under this
distribution corresponds to maximizing a factored
scoring function s,
</bodyText>
<equation confidence="0.7111295">
s(y, x) = N−1� αTf(yi+1, x, yc).
i=0
</equation>
<bodyText confidence="0.99877">
where g(yi+1, x, yc) g αTf(yi+1, x, yc) to sat-
isfy Eq. 4. The function f is defined to combine
the local conditional probability with some addi-
tional indicator featrues:
</bodyText>
<equation confidence="0.977789">
f(yi+1, x, yc) = [ log p(yi+1|x, yc; θ),
1{]j. yi+1 = xj },
1{]j. yi+1−k = xj−k Vk E {0,1}},
1{]j. yi+1−k = xj−k Vk E {0, 1, 2}},
1{]k &gt; j. yi = xk, yi+1 = xj} ].
</equation>
<bodyText confidence="0.9993426">
These features correspond to indicators of uni-
gram, bigram, and trigram match with the input as
well as reordering of input words. Note that set-
ting α = (1, 0, ... , 0) gives a model identical to
standard ABS.
</bodyText>
<equation confidence="0.8564985">
N−1� g(yi+1, x, yc).
i=0
</equation>
<page confidence="0.983389">
383
</page>
<bodyText confidence="0.999911428571429">
After training the main neural model, we fix θ
and tune the α parameters. We follow the statis-
tical machine translation setup and use minimum-
error rate training (MERT) to tune for the summa-
rization metric on tuning data (Och, 2003). This
tuning step is also identical to the one used for the
phrase-based machine translation baseline.
</bodyText>
<sectionHeader confidence="0.999975" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999913241935484">
Abstractive sentence summarization has been tra-
ditionally connected to the task of headline gener-
ation. Our work is similar to early work of Banko
et al. (2000) who developed a statistical machine
translation-inspired approach for this task using a
corpus of headline-article pairs. We extend this
approach by: (1) using a neural summarization
model as opposed to a count-based noisy-channel
model, (2) training the model on much larger scale
(25K compared to 4 million articles), (3) and al-
lowing fully abstractive decoding.
This task was standardized around the DUC-
2003 and DUC-2004 competitions (Over et al.,
2007). The TOPIARY system (Zajic et al., 2004)
performed the best in this task, and is described in
detail in the next section. We point interested read-
ers to the DUC web page (http://duc.nist.
gov/) for the full list of systems entered in this
shared task.
More recently, Cohn and Lapata (2008) give a
compression method which allows for more ar-
bitrary transformations. They extract tree trans-
duction rules from aligned, parsed texts and learn
weights on transfomations using a max-margin
learning algorithm. Woodsend et al. (2010) pro-
pose a quasi-synchronous grammar approach uti-
lizing both context-free parses and dependency
parses to produce legible summaries. Both of
these approaches differ from ours in that they di-
rectly use the syntax of the input/output sentences.
The latter system is W&amp;L in our results; we at-
tempted to train the former system T3 on this
dataset but could not train it at scale.
In addition to Banko et al. (2000) there has been
some work using statistical machine translation
directly for abstractive summary. Wubben et al.
(2012) utilize MOSES directly as a method for text
simplification.
Recently Filippova and Altun (2013) developed
a strictly extractive system that is trained on a rel-
atively large corpora (250K sentences) of article-
title pairs. Because their focus is extractive com-
pression, the sentences are transformed by a series
of heuristics such that the words are in monotonic
alignment. Our system does not require this align-
ment step but instead uses the text directly.
Neural MT This work is closely related to re-
cent work on neural network language models
(NNLM) and to work on neural machine transla-
tion. The core of our model is a NNLM based on
that of Bengio et al. (2003).
Recently, there have been several papers about
models for machine translation (Kalchbrenner and
Blunsom, 2013; Cho et al., 2014; Sutskever et al.,
2014). Of these our model is most closely related
to the attention-based model of Bahdanau et al.
(2014), which explicitly finds a soft alignment be-
tween the current position and the input source.
Most of these models utilize recurrent neural net-
works (RNNs) for generation as opposed to feed-
forward models. We hope to incorporate an RNN-
LM in future work.
</bodyText>
<sectionHeader confidence="0.997825" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999315">
We experiment with our attention-based sentence
summarization model on the task of headline gen-
eration. In this section we describe the corpora
used for this task, the baseline methods we com-
pare with, and implementation details of our ap-
proach.
</bodyText>
<subsectionHeader confidence="0.99764">
7.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999939857142857">
The standard sentence summarization evaluation
set is associated with the DUC-2003 and DUC-
2004 shared tasks (Over et al., 2007). The
data for this task consists of 500 news arti-
cles from the New York Times and Associated
Press Wire services each paired with 4 different
human-generated reference summaries (not actu-
ally headlines), capped at 75 bytes. This data
set is evaluation-only, although the similarly sized
DUC-2003 data set was made available for the
task. The expectation is for a summary of roughly
14 words, based on the text of a complete arti-
cle (although we only make use of the first sen-
tence). The full data set is available by request at
http://duc.nist.gov/data.html.
For this shared task, systems were entered and
evaluated using several variants of the recall-
oriented ROUGE metric (Lin, 2004). To make
recall-only evaluation unbiased to length, out-
put of all systems is cut-off after 75-characters
and no bonus is given for shorter summaries.
</bodyText>
<page confidence="0.997711">
384
</page>
<bodyText confidence="0.999339660377359">
Unlike BLEU which interpolates various n-gram
matches, there are several versions of ROUGE
for different match lengths. The DUC evaluation
uses ROUGE-1 (unigrams), ROUGE-2 (bigrams),
and ROUGE-L (longest-common substring), all of
which we report.
In addition to the standard DUC-2014 evalu-
ation, we also report evaluation on single refer-
ence headline-generation using a randomly held-
out subset of Gigaword. This evaluation is closer
to the task the model is trained for, and it allows
us to use a bigger evaluation set, which we will in-
clude in our code release. For this evaluation, we
tune systems to generate output of the average title
length.
For training data for both tasks, we utilize the
annotated Gigaword data set (Graff et al., 2003;
Napoles et al., 2012), which consists of standard
Gigaword, preprocessed with Stanford CoreNLP
tools (Manning et al., 2014). Our model only uses
annotations for tokenization and sentence separa-
tion, although several of the baselines use parsing
and tagging as well. Gigaword contains around 9.5
million news articles sourced from various domes-
tic and international news services over the last
two decades.
For our training set, we pair the headline of each
article with its first sentence to create an input-
summary pair. While the model could in theory be
trained on any pair, Gigaword contains many spu-
rious headline-article pairs. We therefore prune
training based on the following heuristic filters:
(1) Are there no non-stop-words in common? (2)
Does the title contain a byline or other extrane-
ous editing marks? (3) Does the title have a ques-
tion mark or colon? After applying these filters,
the training set consists of roughly J = 4 million
title-article pairs. We apply a minimal preprocess-
ing step using PTB tokenization, lower-casing, re-
placing all digit characters with #, and replacing
of word types seen less than 5 times with UNK.
We also remove all articles from the time-period
of the DUC evaluation. release.
The complete input training vocabulary consists
of 119 million word tokens and 110K unique word
types with an average sentence size of 31.3 words.
The headline vocabulary consists of 31 million to-
kens and 69K word types with the average title
of length 8.3 words (note that this is significantly
shorter than the DUC summaries). On average
there are 4.6 overlapping word types between the
headline and the input; although only 2.6 in the
first 75-characters of the input.
</bodyText>
<subsectionHeader confidence="0.99735">
7.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999922936170213">
Due to the variety of approaches to the sentence
summarization problem, we report a broad set of
headline-generation baselines.
From the DUC-2004 task we include the PRE-
FIX baseline that simply returns the first 75-
characters of the input as the headline. We
also report the winning system on this shared
task, TOPIARY (Zajic et al., 2004). TOPIARY
merges a compression system using linguistically-
motivated transformations of the input (Dorr et al.,
2003) with an unsupervised topic detection (UTD)
algorithm that appends key phrases from the full
article onto the compressed output. Woodsend et
al. (2010) (described above) also report results on
the DUC dataset.
The DUC task also includes a set of manual
summaries performed by 8 human summarizers
each summarizing half of the test data sentences
(yielding 4 references per sentence). We report the
average inter-annotater agreement score as REF-
ERENCE. For reference, the best human evaluator
scores 31.7 ROUGE-1.
We also include several baselines that have ac-
cess to the same training data as our system. The
first is a sentence compression baseline COM-
PRESS (Clarke and Lapata, 2008). This model
uses the syntactic structure of the original sentence
along with a language model trained on the head-
line data to produce a compressed output. The
syntax and language model are combined with a
set of linguistic constraints and decoding is per-
formed with an ILP solver.
To control for memorizing titles from training,
we implement an information retrieval baseline,
IR. This baseline indexes the training set, and
gives the title for the article with highest BM-25
match to the input (see Manning et al. (2008)).
Finally, we use a phrase-based statistical ma-
chine translation system trained on Gigaword
to produce summaries, MOSES+ (Koehn et al.,
2007). To improve the baseline for this task, we
augment the phrase table with “deletion” rules
mapping each article word to E, include an addi-
tional deletion feature for these rules, and allow
for an infinite distortion limit. We also explic-
itly tune the model using MERT to target the 75-
byte capped ROUGE score as opposed to standard
</bodyText>
<page confidence="0.996361">
385
</page>
<table confidence="0.999869090909091">
Model ROUGE-1 DUC-2004 ROUGE-L ROUGE-1 Gigaword Ext. %
ROUGE-2 ROUGE-2 ROUGE-L
IR 11.06 1.67 9.67 16.91 5.55 15.58 29.2
PREFIX 22.43 6.49 19.65 23.14 8.25 21.73 100
COMPRESS 19.77 4.02 17.30 19.63 5.13 18.28 100
W&amp;L 22 6 17 - - - -
TOPIARY 25.12 6.46 20.12 - - - -
MOSES+ 26.50 8.13 22.85 28.77 12.10 26.44 70.5
ABS 26.55 7.06 22.05 30.88 12.22 27.77 85.4
ABS+ 28.18 8.49 23.81 31.00 12.65 28.34 91.5
REFERENCE 29.21 8.38 24.46 - - - 45.6
</table>
<tableCaption confidence="0.9981695">
Table 1: Experimental results on the main summary tasks on various ROUGE metrics . Baseline models are described in
detail in Section 7.2. We report the percentage of tokens in the summary that also appear in the input for Gigaword as Ext %.
</tableCaption>
<bodyText confidence="0.9569566">
BLEU-based tuning. Unfortunately, one remain-
ing issue is that it is non-trivial to modify the trans-
lation decoder to produce fixed-length outputs, so
we tune the system to produce roughly the ex-
pected length.
</bodyText>
<subsectionHeader confidence="0.994948">
7.3 Implementation
</subsectionHeader>
<bodyText confidence="0.99991624">
For training, we use mini-batch stochastic gradient
descent to minimize negative log-likelihood. We
use a learning rate of 0.05, and split the learning
rate by half if validation log-likelihood does not
improve for an epoch. Training is performed with
shuffled mini-batches of size 64. The minibatches
are grouped by input length. After each epoch, we
renormalize the embedding tables (Hinton et al.,
2012). Based on the validation set, we set hyper-
parameters as D = 200, H = 400, C = 5, L = 3,
and Q = 2.
Our implementation uses the Torch numerical
framework (http://torch.ch/) and will be
openly available along with the data pipeline. Cru-
cially, training is performed on GPUs and would
be intractable or require approximations other-
wise. Processing 1000 mini-batches with D =
200, H = 400 requires 160 seconds. Best valida-
tion accuracy is reached after 15 epochs through
the data, which requires around 4 days of training.
Additionally, as described in Section 5 we apply
a MERT tuning step after training using the DUC-
2003 data. For this step we use Z-MERT (Zaidan,
2009). We refer to the main model as ABS and the
tuned model as ABS+.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.999569023809524">
Our main results are presented in Table 1. We
run experiments both using the DUC-2004 eval-
uation data set (500 sentences, 4 references, 75
bytes) with all systems and a randomly held-out
Gigaword test set (2000 sentences, 1 reference).
We first note that the baselines COMPRESS and IR
do relatively poorly on both datasets, indicating
that neither just having article information or lan-
guage model information alone is sufficient for the
task. The PREFIX baseline actually performs sur-
prisingly well on ROUGE-1 which makes sense
given the earlier observed overlap between article
and summary.
Both ABS and MOSES+ perform better
than TOPIARY, particularly on ROUGE-2 and
ROUGE-L in DUC. The full model ABS+ scores
the best on these tasks, and is significantly better
based on the default ROUGE confidence level
than TOPIARY on all metrics, and MOSES+ on
ROUGE-1 for DUC as well as ROUGE-1 and
ROUGE-L for Gigaword. Note that the additional
extractive features bias the system towards re-
taining more input words, which is useful for the
underlying metric.
Next we consider ablations to the model and al-
gorithm structure. Table 2 shows experiments for
the model with various encoders. For these exper-
iments we look at the perplexity of the system as
a language model on validation data, which con-
trols for the variable of inference and tuning. The
NNLM language model with no encoder gives a
gain over the standard n-gram language model.
Including even the bag-of-words encoder reduces
perplexity number to below 50. Both the convo-
lutional encoder and the attention-based encoder
further reduce the perplexity, with attention giving
a value below 30.
We also consider model and decoding ablations
on the main summary model, shown in Table 3.
These experiments compare to the BoW encoding
models, compare beam search and greedy decod-
ing, as well as restricting the system to be com-
</bodyText>
<page confidence="0.996917">
386
</page>
<table confidence="0.979340833333333">
Model Encoder Perplexity
KN-Smoothed 5-Gram none 183.2
Feed-Forward NNLM none 145.9
Bag-of-Word enc1 43.6
Convolutional (TDNN) enc2 35.9
Attention-Based (ABS) enc3 27.1
</table>
<tableCaption confidence="0.7231715">
Table 2: Perplexity results on the Gigaword validation
set comparing various language models with C=5 and end-
to-end summarization models. The encoders are defined in
Section 3.
</tableCaption>
<table confidence="0.9999508">
Decoder Model Cons. R-1 R-2 R-L
Greedy ABS+ Abs 26.67 6.72 21.70
Beam BOW Abs 22.15 4.60 18.23
Beam ABS+ Ext 27.89 7.56 22.84
Beam ABS+ Abs 28.48 8.91 23.97
</table>
<tableCaption confidence="0.97172725">
Table 3: ROUGE scores on DUC-2003 development data
for various versions of inference. Greedy and Beam are de-
scribed in Section 4. Ext. is a purely extractive version of the
system (Eq. 2)
</tableCaption>
<bodyText confidence="0.999793473684211">
plete extractive. Of these features, the biggest im-
pact is from using a more powerful encoder (atten-
tion versus BoW), as well as using beam search to
generate summaries. The abstractive nature of the
system helps, but for ROUGE even using pure ex-
tractive generation is effective.
Finally we consider example summaries shown
in Figure 4. Despite improving on the base-
line scores, this model is far from human per-
formance on this task. Generally the models are
good at picking out key words from the input,
such as names and places. However, both models
will reorder words in syntactically incorrect ways,
for instance in Sentence 7 both models have the
wrong subject. ABS often uses more interesting
re-wording, for instance new nz pm after election
in Sentence 4, but this can also lead to attachment
mistakes such a russian oil giant chevron in Sen-
tence 11.
</bodyText>
<sectionHeader confidence="0.996808" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.937231734939759">
We have presented a neural attention-based model
for abstractive summarization, based on recent de-
velopments in neural machine translation. We
combine this probabilistic model with a genera-
tion algorithm which produces accurate abstrac-
tive summaries. As a next step we would like
to further improve the grammaticality of the sum-
maries in a data-driven way, as well as scale this
system to generate paragraph-level summaries.
Both pose additional challenges in terms of effi-
cient alignment and consistency in generation.
I(1): a detained iranian-american academic accused of acting against
national security has been released from a tehran prison after a hefty
bail was posted, a to p judiciary official said tuesday .
G: iranian-american academic held in tehran released on bail
A: detained iranian-american academic released from jail after posting
bail
A+: detained iranian-american academic released from prison after
hefty bail
I(2): ministers from the european union and its mediterranean neighbors
gathered here under heavy security on monday for an unprecedented
conference on economic and political cooperation.
G: european mediterranean ministers gather for landmark conference
by julie bradford
A: mediterranean neighbors gather for unprecedented conference on
heavy security
A+: mediterranean neighbors gather under heavy security for unprece-
dented conference
I(3): the death toll from a school collapse in a haitian shanty-town rose
to ## after rescue workers uncovered a classroom with ## dead students
and their teacher , officials said saturday .
G: toll rises to ## in haiti school unk : official
A: death toll in haiti school accident rises to ##
A+: death toll in haiti school to ## dead students
I(4): australian foreign minister stephen smith sunday congratulated
new zealand ’s new prime minister-elect john key as he praised ousted
leader helen clark as a “ gutsy ” and respected politician .
G: time caught up with nz ’s gutsy clark says australian fm
A: australian foreign minister congratulates new nz pm after election
A+: australian foreign minister congratulates smith new zealand as
leader
I(5): two drunken south african fans hurled racist abuse at the country
’s rugby sevens coach after the team were eliminated from the weekend
’s hong kong tournament, reports said tuesday .
G: rugby union: racist taunts mar hong kong sevens: report
A: south african fans hurl racist taunts at rugby sevens
A+: south african fans racist abuse at rugby sevens tournament
I(6): christian conservatives – kingmakers in the last two us presidential
elections – may have less success in getting their pick elected in #### ,
political observers say .
G: christian conservatives power diminished ahead of #### vote
A: christian conservatives may have less success in #### election
A+: christian conservatives in the last two us presidential elections
I(7): the white house on thursday warned iran of possible new sanctions
after the un nuclear watchdog reported that tehran had begun sensitive
nuclear work at a key site in defiance of un resolutions .
G: us warns iran of step backward on nuclear issue
A: iran warns of possible new sanctions on nuclear work
A+: un nuclear watchdog warns iran of possible new sanctions
I(8): thousands of kashmiris chanting pro-pakistan slogans on sunday
attended a rally to welcome back a hardline separatist leader who
underwent cancer treatment in mumbai .
G: thousands attend rally for kashmir hardliner
A: thousands rally in support of hardline kashmiri separatist leader
A+: thousands of kashmiris rally to welcome back cancer treatment
I(9): an explosion in iraq ’s restive northeastern province of diyala
killed two us soldiers and wounded two more , the military reported
monday .
G: two us soldiers killed in iraq blast december toll ###
A: # us two soldiers killed in restive northeast province
A+: explosion in restive northeastern province kills two us soldiers
I(10): russian world no. # nikolay davydenko became the fifth with-
drawal through injury or illness at the sydney international wednesday ,
retiring from his second round match with a foot injury.
G: tennis : davydenko pulls out of sydney with injury
A: davydenko pulls out of sydney international with foot injury
A+: russian world no. # davydenko retires at sydney international
I(11): russia ’s gas and oil giant gazprom and us oil major chevron have
set up a joint venture based in resource-rich northwestern siberia , the
interfax news agency reported thursday quoting gazprom officials.
G: gazprom chevron set up joint venture
A: russian oil giant chevron set up siberia joint venture
A+: russia ’s gazprom set up joint venture in siberia
</bodyText>
<figureCaption confidence="0.9920445">
Figure 4: Example sentence summaries produced on Gi-
gaword. I is the input, A is ABS, and G is the true headline.
</figureCaption>
<page confidence="0.995178">
387
</page>
<sectionHeader confidence="0.996244" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999780952830189">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.
Michele Banko, Vibhu O Mittal, and Michael J Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 318–325. Association for Computational
Linguistics.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. arXiv preprint arXiv:1406.1078.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal ofArtificialIntelli-
gence Research, pages 399–429.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 137–144. Asso-
ciation for Computational Linguistics.
Hal Daum´e III and Daniel Marcu. 2002. A noisy-
channel model for document compression. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 449–456. As-
sociation for Computational Linguistics.
Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 03 on Text summarization workshop-Volume
5, pages 1–8. Association for Computational Lin-
guistics.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In EMNLP, pages 1481–1491.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2003. English gigaword. Linguistic Data
Consortium, Philadelphia.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Hongyan Jing. 2002. Using hidden markov modeling
to decompose human-written summaries. Computa-
tional linguistics, 28(4):527–543.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In EMNLP, pages
1700–1709.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91–107.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.
Thang Luong, Ilya Sutskever, Quoc V Le, Oriol
Vinyals, and Wojciech Zaremba. 2014. Addressing
the rare word problem in neural machine translation.
arXiv preprint arXiv:1410.8206.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to informa-
tion retrieval, volume 1. Cambridge university press
Cambridge.
Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, pages 95–100. Association for Compu-
tational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.
Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing &amp; Management,
43(6):1506–1520.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.
</reference>
<page confidence="0.983132">
388
</page>
<reference confidence="0.99943545">
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of the 2010 conference on empirical
methods in natural language processing, pages 513–
523. Association for Computational Linguistics.
Sander Wubben, Antal Van Den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
1015–1024. Association for Computational Linguis-
tics.
Omar Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
David Zajic, Bonnie Dorr, and Richard Schwartz.
2004. Bbn/umd at duc-2004: Topiary. In Pro-
ceedings of the HLT-NAACL 2004 Document Under-
standing Workshop, Boston, pages 112–119.
</reference>
<page confidence="0.999146">
389
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.107355">
<title confidence="0.999865">A Neural Attention Model for Sentence Summarization</title>
<author confidence="0.996961">M Alexander</author>
<affiliation confidence="0.66548">Facebook AI Research Harvard</affiliation>
<email confidence="0.998885">srush@seas.harvard.edu</email>
<title confidence="0.282666">Sumit</title>
<author confidence="0.693705">Facebook AI</author>
<email confidence="0.99252">spchopra@fb.com</email>
<author confidence="0.921532">Jason Facebook AI</author>
<email confidence="0.99959">jase@fb.com</email>
<abstract confidence="0.9997450625">Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</title>
<date>2014</date>
<contexts>
<context position="2549" citStr="Bahdanau et al. (2014)" startWordPosition="383" endWordPosition="386">attention-based summarization (ABS) system. The heatmap represents a soft alignment between the input (right) and the generated summary (top). The columns represent the distribution over the input after generating each word. Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1). Crucially both the encoder and the generation model are trained jointly on the sentence summarization task. The model is described in detail in Section 3. Our model also incorporates a beam-search decoder as well as additional features to model extractive elements; these aspects are discussed in Sections 4 and 5. This approach to summarization, which we call Attention-Based Summarization (ABS), incorporates less linguistic structure than comparable abstractive summarization approac</context>
<context position="12473" citStr="Bahdanau et al. (2014)" startWordPosition="2082" endWordPosition="2085"> L}. Eq. 7 is a temporal (1D) convolution layer, Eq. 6 consists of a 2-element temporal max pooling layer and a pointwise non-linearity, and final output Eq. 5 is a max over time. At each layer x˜ is one half the size of ¯x. For simplicity we assume that the convolution is padded at the boundaries, and that M is greater than 2L so that the dimensions are well-defined. Attention-Based Encoder While the convolutional encoder has richer capacity than bag-ofwords, it still is required to produce a single representation for the entire input sentence. A similar issue in machine translation inspired Bahdanau et al. (2014) to instead utilize an attention-based contextual encoder that constructs a representation based on the generation context. Here we note that if we exploit this context, we can actually use a rather simple model similar to bag-of-words: enc3(x, yc) = pT¯x, p a exp(˜xP˜y&apos;), x˜ = [Fx1, ... , Fxm], ˜yc = [Gyi−C+1, ... , Gyi], i+Q Vi ¯xi = ˜xi/Q. q=i−Q Where G E RDxV is an embedding of the context, P E 1fL Hx(CD) is a new weight matrix parameter malLp�ping between the context embedding and input embedding, and Q is a smoothing window. The full model is shown in Figure 3b. Informally we can think o</context>
<context position="14356" citStr="Bahdanau et al. (2014)" startWordPosition="2409" endWordPosition="2412">ible to train the model on arbitrary input-output pairs. Once we have defined the local conditional model, p(yi+1Jx, yc; B), we can estimate the parameters to minimize the negative loglikelihood of a set of summaries. Define this training set as consisting of J input-summary pairs (x(1), y(1)), ... , (x(J), y(J)). The negative loglikelihood conveniently factors6 into a term for each token in the summary: log p(y(j)|x(j); B), log p(y(j) i+1|x(j), yc; B). We minimize NLL by using mini-batch stochastic gradient descent. The details are described further in Section 7. 5To be explicit, compared to Bahdanau et al. (2014) our model uses an NNLM instead of a target-side LSTM, source-side windowed averaging instead of a source-side bidirectional RNN, and a weighted dot-product for alignment instead of an alignment MLP. 6This is dependent on using the gold standard contexts yc. An alternative is to use the predicted context within a structured or reenforcement-learning style objective. J NLL(B) = − E j=1 J E j=1 N−1� i=1 382 4 Generating Summaries We now return to the problem of generating summaries. Recall from Eq. 4 that our goal is to find, can be computed as a mini-batch, which in practice greatly reduces the</context>
<context position="21271" citStr="Bahdanau et al. (2014)" startWordPosition="3610" endWordPosition="3613">med by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly. Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source. Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models. We hope to incorporate an RNNLM in future work. 7 Experimental Setup We experiment with our attention-based sentence summarization model on the task of headline generation. In this section we describe the corpora used for this task, the baseline methods we compare with, and implementation details of our approach. 7.1 Data Set The standard sentence summarization evaluation set is associated wit</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu O Mittal</author>
<author>Michael J Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>318--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8468" citStr="Banko et al., 2000" startWordPosition="1384" endWordPosition="1387">on in mind, our main focus will be on modelling the local conditional distribution: p(yi+1|x, yc; 0). The next section defines a parameterization for this distribution, in Section 4, we return to the question of generation for factored models, and in Section 5 we introduce a modified factored scoring function. 3 Model The distribution of interest, p(yi+1|x, yc; 0), is a conditional language model based on the input sentence x. Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model (Banko et al., 2000; Knight and Marcu, 2002; Daum´e III and Marcu, 2002), i.e., arg max log p(y|x) = argmax log p(y)p(x|y) Y Y where p(y) and p(x|y) are estimated separately. Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network. The network contains both a neural probabilistic language model and an encoder which acts as a conditional summarization model. 3.1 Neural Language Model The core of our parameterization is a language model for estimating the contextual probability of the next word. The language model is adapted from a standard </context>
<context position="18842" citStr="Banko et al. (2000)" startWordPosition="3212" endWordPosition="3215"> Note that setting α = (1, 0, ... , 0) gives a model identical to standard ABS. N−1� g(yi+1, x, yc). i=0 383 After training the main neural model, we fix θ and tune the α parameters. We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003). This tuning step is also identical to the one used for the phrase-based machine translation baseline. 6 Related Work Abstractive sentence summarization has been traditionally connected to the task of headline generation. Our work is similar to early work of Banko et al. (2000) who developed a statistical machine translation-inspired approach for this task using a corpus of headline-article pairs. We extend this approach by: (1) using a neural summarization model as opposed to a count-based noisy-channel model, (2) training the model on much larger scale (25K compared to 4 million articles), (3) and allowing fully abstractive decoding. This task was standardized around the DUC2003 and DUC-2004 competitions (Over et al., 2007). The TOPIARY system (Zajic et al., 2004) performed the best in this task, and is described in detail in the next section. We point interested </context>
<context position="20236" citStr="Banko et al. (2000)" startWordPosition="3439" endWordPosition="3442">hich allows for more arbitrary transformations. They extract tree transduction rules from aligned, parsed texts and learn weights on transfomations using a max-margin learning algorithm. Woodsend et al. (2010) propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce legible summaries. Both of these approaches differ from ours in that they directly use the syntax of the input/output sentences. The latter system is W&amp;L in our results; we attempted to train the former system T3 on this dataset but could not train it at scale. In addition to Banko et al. (2000) there has been some work using statistical machine translation directly for abstractive summary. Wubben et al. (2012) utilize MOSES directly as a method for text simplification. Recently Filippova and Altun (2013) developed a strictly extractive system that is trained on a relatively large corpora (250K sentences) of articletitle pairs. Because their focus is extractive compression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly. Neural MT This work is cl</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Michele Banko, Vibhu O Mittal, and Michael J Witbrock. 2000. Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 318–325. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="9184" citStr="Bengio et al. (2003)" startWordPosition="1499" endWordPosition="1502">)p(x|y) Y Y where p(y) and p(x|y) are estimated separately. Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network. The network contains both a neural probabilistic language model and an encoder which acts as a conditional summarization model. 3.1 Neural Language Model The core of our parameterization is a language model for estimating the contextual probability of the next word. The language model is adapted from a standard feed-forward neural network language model (NNLM), particularly the class of NNLMs described by Bengio et al. (2003). The full model is: p(yi+1|yc, x; 0) a exp(Vh + Wenc(x, yc)), ˜yc = [Eyi−C+1, ... , Eyi], h = tanh(U˜yc). Figure 3: (a) A network diagram for the NNLM decoder with additional encoder element. (b) A network diagram for the attention-based encoder enc3. The parameters are 0 = (E, U, V, W) where E E RDxV is a word embedding matrix, U E R(CD)xH, V E RV xH, W E RV xH are weight matrices,4 D is the size of the word embeddings, and h is a hidden layer of size H. The black-box function enc is a contextual encoder term that returns a vector of size H representing the input and current context; we cons</context>
<context position="21018" citStr="Bengio et al. (2003)" startWordPosition="3570" endWordPosition="3573"> simplification. Recently Filippova and Altun (2013) developed a strictly extractive system that is trained on a relatively large corpora (250K sentences) of articletitle pairs. Because their focus is extractive compression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly. Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source. Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models. We hope to incorporate an RNNLM in future work. 7 Experimental Setup We experiment with our attention-based sentence summarization model on the</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal ofArtificialIntelligence Research,</journal>
<pages>399--429</pages>
<contexts>
<context position="26368" citStr="Clarke and Lapata, 2008" startWordPosition="4442" endWordPosition="4445">appends key phrases from the full article onto the compressed output. Woodsend et al. (2010) (described above) also report results on the DUC dataset. The DUC task also includes a set of manual summaries performed by 8 human summarizers each summarizing half of the test data sentences (yielding 4 references per sentence). We report the average inter-annotater agreement score as REFERENCE. For reference, the best human evaluator scores 31.7 ROUGE-1. We also include several baselines that have access to the same training data as our system. The first is a sentence compression baseline COMPRESS (Clarke and Lapata, 2008). This model uses the syntactic structure of the original sentence along with a language model trained on the headline data to produce a compressed output. The syntax and language model are combined with a set of linguistic constraints and decoding is performed with an ILP solver. To control for memorizing titles from training, we implement an information retrieval baseline, IR. This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see Manning et al. (2008)). Finally, we use a phrase-based statistical machine translation system train</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal ofArtificialIntelligence Research, pages 399–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>137--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19589" citStr="Cohn and Lapata (2008)" startWordPosition="3333" endWordPosition="3336"> extend this approach by: (1) using a neural summarization model as opposed to a count-based noisy-channel model, (2) training the model on much larger scale (25K compared to 4 million articles), (3) and allowing fully abstractive decoding. This task was standardized around the DUC2003 and DUC-2004 competitions (Over et al., 2007). The TOPIARY system (Zajic et al., 2004) performed the best in this task, and is described in detail in the next section. We point interested readers to the DUC web page (http://duc.nist. gov/) for the full list of systems entered in this shared task. More recently, Cohn and Lapata (2008) give a compression method which allows for more arbitrary transformations. They extract tree transduction rules from aligned, parsed texts and learn weights on transfomations using a max-margin learning algorithm. Woodsend et al. (2010) propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce legible summaries. Both of these approaches differ from ours in that they directly use the syntax of the input/output sentences. The latter system is W&amp;L in our results; we attempted to train the former system T3 on this dataset but could not train </context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 137–144. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisychannel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>449--456</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Daum´e, Marcu, 2002</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2002. A noisychannel model for document compression. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 449–456. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>David Zajic</author>
<author>Richard Schwartz</author>
</authors>
<title>Hedge trimmer: A parse-and-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLTNAACL 03 on Text summarization workshop-Volume 5,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1811" citStr="Dorr et al., 2003" startWordPosition="266" endWordPosition="269">st, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Figure 1: Example output of the attention-based summarization (ABS) system. The heatmap represents a soft alignment between the input (right) and the generated summary (top). The columns represent the distribution over the input after generating each word. Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a </context>
<context position="25685" citStr="Dorr et al., 2003" startWordPosition="4333" endWordPosition="4336">es). On average there are 4.6 overlapping word types between the headline and the input; although only 2.6 in the first 75-characters of the input. 7.2 Baselines Due to the variety of approaches to the sentence summarization problem, we report a broad set of headline-generation baselines. From the DUC-2004 task we include the PREFIX baseline that simply returns the first 75- characters of the input as the headline. We also report the winning system on this shared task, TOPIARY (Zajic et al., 2004). TOPIARY merges a compression system using linguisticallymotivated transformations of the input (Dorr et al., 2003) with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output. Woodsend et al. (2010) (described above) also report results on the DUC dataset. The DUC task also includes a set of manual summaries performed by 8 human summarizers each summarizing half of the test data sentences (yielding 4 references per sentence). We report the average inter-annotater agreement score as REFERENCE. For reference, the best human evaluator scores 31.7 ROUGE-1. We also include several baselines that have access to the same training data as our syst</context>
</contexts>
<marker>Dorr, Zajic, Schwartz, 2003</marker>
<rawString>Bonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLTNAACL 03 on Text summarization workshop-Volume 5, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Yasemin Altun</author>
</authors>
<title>Overcoming the lack of parallel data in sentence compression.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1481--1491</pages>
<contexts>
<context position="6740" citStr="Filippova and Altun (2013)" startWordPosition="1087" endWordPosition="1090">consist of a sequence of M words x1, ... , xM coming from a fixed vocabulary V of size |V |= V . We will represent each word as an indicator vector xi E {0,1}V for i E {1, ... , M}, sentences as a sequence of indicators, and X as the set of possible inputs. Furthermore define the notation x[i,j,k] to indicate the sub-sequence of elements i, j, k. A summarizer takes x as input and outputs a shortened sentence y of length N &lt; M. We will assume that the words in the summary also come from the same vocabulary V and that the output is 1In contrast to a large-scale sentence compression systems like Filippova and Altun (2013) which require monotonic aligned compressions. While abstractive summarization poses a more difficult generation challenge, the lack of hard constraints gives the system more freedom in generation and allows it to fit with a wider range of training data. In this work we focus on factored scoring functions, s, that take into account a fixed window of previous words: s(x, y) ≈ N−1� g(yi+1, x, yc), (4) i=0 2For the DUC-2004 evaluation, it is actually the number of bytes of the output that is capped. More detail is given in Section 7. 3Unfortunately the literature is inconsistent on the formal def</context>
<context position="20450" citStr="Filippova and Altun (2013)" startWordPosition="3470" endWordPosition="3473">0) propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce legible summaries. Both of these approaches differ from ours in that they directly use the syntax of the input/output sentences. The latter system is W&amp;L in our results; we attempted to train the former system T3 on this dataset but could not train it at scale. In addition to Banko et al. (2000) there has been some work using statistical machine translation directly for abstractive summary. Wubben et al. (2012) utilize MOSES directly as a method for text simplification. Recently Filippova and Altun (2013) developed a strictly extractive system that is trained on a relatively large corpora (250K sentences) of articletitle pairs. Because their focus is extractive compression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly. Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been seve</context>
</contexts>
<marker>Filippova, Altun, 2013</marker>
<rawString>Katja Filippova and Yasemin Altun. 2013. Overcoming the lack of parallel data in sentence compression. In EMNLP, pages 1481–1491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2003</date>
<booktitle>English gigaword. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="4437" citStr="Graff et al., 2003" startWordPosition="682" endWordPosition="685">s the input x1 ... x18. Note that the summary generated is abstractive which makes it possible to generalize (russian defense minister to russia) and paraphrase (for combating to against), in addition to compressing (dropping the creation of), see Jing (2002) for a survey of these editing operations. scale to train on a large amount of data. Since our system makes no assumptions about the vocabulary of the generated summary it can be trained directly on any document-summary pair.1 This allows us to train a summarization model for headline-generation on a corpus of article pairs from Gigaword (Graff et al., 2003) consisting of around 4 million articles. An example of generation is given in Figure 2, and we discuss the details of this task in Section 7. To test the effectiveness of this approach we run extensive comparisons with multiple abstractive and extractive baselines, including traditional syntax-based systems, integer linear programconstrained systems, information-retrieval style approaches, as well as statistical phrase-based machine translation. Section 8 describes the results of these experiments. Our approach outperforms a machine translation system trained on the same large-scale dataset a</context>
<context position="23515" citStr="Graff et al., 2003" startWordPosition="3979" endWordPosition="3982">C evaluation uses ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring), all of which we report. In addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation set, which we will include in our code release. For this evaluation, we tune systems to generate output of the average title length. For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al., 2014). Our model only uses annotations for tokenization and sentence separation, although several of the baselines use parsing and tagging as well. Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades. For our training set, we pair the headline of each article with its first sentence to create an inputsummary pair. While the model could in theory be trained on any pair, Gigaword contains man</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2003</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="28684" citStr="Hinton et al., 2012" startWordPosition="4834" endWordPosition="4837">ing. Unfortunately, one remaining issue is that it is non-trivial to modify the translation decoder to produce fixed-length outputs, so we tune the system to produce roughly the expected length. 7.3 Implementation For training, we use mini-batch stochastic gradient descent to minimize negative log-likelihood. We use a learning rate of 0.05, and split the learning rate by half if validation log-likelihood does not improve for an epoch. Training is performed with shuffled mini-batches of size 64. The minibatches are grouped by input length. After each epoch, we renormalize the embedding tables (Hinton et al., 2012). Based on the validation set, we set hyperparameters as D = 200, H = 400, C = 5, L = 3, and Q = 2. Our implementation uses the Torch numerical framework (http://torch.ch/) and will be openly available along with the data pipeline. Crucially, training is performed on GPUs and would be intractable or require approximations otherwise. Processing 1000 mini-batches with D = 200, H = 400 requires 160 seconds. Best validation accuracy is reached after 15 epochs through the data, which requires around 4 days of training. Additionally, as described in Section 5 we apply a MERT tuning step after traini</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<booktitle>Computational linguistics,</booktitle>
<pages>28--4</pages>
<contexts>
<context position="1681" citStr="Jing, 2002" startWordPosition="251" endWordPosition="252">ilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. In contrast, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Figure 1: Example output of the attention-based summarization (ABS) system. The heatmap represents a soft alignment between the input (right) and the generated summary (top). The columns represent the distribution over the input after generating each word. Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data</context>
<context position="4077" citStr="Jing (2002)" startWordPosition="623" endWordPosition="624">the creation of a joint front for combating global terrorism Output (y1, ... , y8). Generated headline: russia calls for joint front against terrorism ⇐ g(terrorism, x, for, joint, front, against) Figure 2: Example input sentence and the generated summary. The score of generating yi+1 (terrorism) is based on the context yr (for ... against) as well as the input x1 ... x18. Note that the summary generated is abstractive which makes it possible to generalize (russian defense minister to russia) and paraphrase (for combating to against), in addition to compressing (dropping the creation of), see Jing (2002) for a survey of these editing operations. scale to train on a large amount of data. Since our system makes no assumptions about the vocabulary of the generated summary it can be trained directly on any document-summary pair.1 This allows us to train a summarization model for headline-generation on a corpus of article pairs from Gigaword (Graff et al., 2003) consisting of around 4 million articles. An example of generation is given in Figure 2, and we discuss the details of this task in Section 7. To test the effectiveness of this approach we run extensive comparisons with multiple abstractive</context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Hongyan Jing. 2002. Using hidden markov modeling to decompose human-written summaries. Computational linguistics, 28(4):527–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="21129" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="3585" endWordPosition="3588">rained on a relatively large corpora (250K sentences) of articletitle pairs. Because their focus is extractive compression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly. Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source. Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models. We hope to incorporate an RNNLM in future work. 7 Experimental Setup We experiment with our attention-based sentence summarization model on the task of headline generation. In this section we describe the corpora used for this task, the baseline methods </context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In EMNLP, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="1490" citStr="Knight and Marcu (2002)" startWordPosition="221" endWordPosition="224">ant challenge of natural language understanding. The aim is to produce a condensed representation of an input text that captures the core meaning of the original. Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. In contrast, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Figure 1: Example output of the attention-based summarization (ABS) system. The heatmap represents a soft alignment between the input (right) and the generated summary (top). The columns represen</context>
<context position="8492" citStr="Knight and Marcu, 2002" startWordPosition="1388" endWordPosition="1391"> focus will be on modelling the local conditional distribution: p(yi+1|x, yc; 0). The next section defines a parameterization for this distribution, in Section 4, we return to the question of generation for factored models, and in Section 5 we introduce a modified factored scoring function. 3 Model The distribution of interest, p(yi+1|x, yc; 0), is a conditional language model based on the input sentence x. Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model (Banko et al., 2000; Knight and Marcu, 2002; Daum´e III and Marcu, 2002), i.e., arg max log p(y|x) = argmax log p(y)p(x|y) Y Y where p(y) and p(x|y) are estimated separately. Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network. The network contains both a neural probabilistic language model and an encoder which acts as a conditional summarization model. 3.1 Neural Language Model The core of our parameterization is a language model for estimating the contextual probability of the next word. The language model is adapted from a standard feed-forward neural netw</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Kevin Knight and Daniel Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artificial Intelligence, 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27032" citStr="Koehn et al., 2007" startWordPosition="4550" endWordPosition="4553">the original sentence along with a language model trained on the headline data to produce a compressed output. The syntax and language model are combined with a set of linguistic constraints and decoding is performed with an ILP solver. To control for memorizing titles from training, we implement an information retrieval baseline, IR. This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see Manning et al. (2008)). Finally, we use a phrase-based statistical machine translation system trained on Gigaword to produce summaries, MOSES+ (Koehn et al., 2007). To improve the baseline for this task, we augment the phrase table with “deletion” rules mapping each article word to E, include an additional deletion feature for these rules, and allow for an infinite distortion limit. We also explicitly tune the model using MERT to target the 75- byte capped ROUGE score as opposed to standard 385 Model ROUGE-1 DUC-2004 ROUGE-L ROUGE-1 Gigaword Ext. % ROUGE-2 ROUGE-2 ROUGE-L IR 11.06 1.67 9.67 16.91 5.55 15.58 29.2 PREFIX 22.43 6.49 19.65 23.14 8.25 21.73 100 COMPRESS 19.77 4.02 17.30 19.63 5.13 18.28 100 W&amp;L 22 6 17 - - - - TOPIARY 25.12 6.46 20.12 - - - </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="22616" citStr="Lin, 2004" startWordPosition="3835" endWordPosition="3836">es and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes. This data set is evaluation-only, although the similarly sized DUC-2003 data set was made available for the task. The expectation is for a summary of roughly 14 words, based on the text of a complete article (although we only make use of the first sentence). The full data set is available by request at http://duc.nist.gov/data.html. For this shared task, systems were entered and evaluated using several variants of the recalloriented ROUGE metric (Lin, 2004). To make recall-only evaluation unbiased to length, output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries. 384 Unlike BLEU which interpolates various n-gram matches, there are several versions of ROUGE for different match lengths. The DUC evaluation uses ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring), all of which we report. In addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task </context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Ilya Sutskever</author>
<author>Quoc V Le</author>
<author>Oriol Vinyals</author>
<author>Wojciech Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206.</title>
<date>2014</date>
<contexts>
<context position="17193" citStr="Luong et al., 2014" startWordPosition="2910" endWordPosition="2913">e. From a computational perspective though, each round of beam search is dominated by computing p(yi|x, yc) for each of the K hypotheses. These While we will see that the attention-based model is effective at generating summaries, it does miss an important aspect seen in the human-generated references. In particular the abstractive model does not have the capacity to find extractive word matches when necessary, for example transferring unseen proper noun phrases from the input. Similar issues have also been observed in neural translation models particularly in terms of translating rare words (Luong et al., 2014). To address this issue we experiment with tuning a very small set of additional features that tradeoff the abstractive/extractive tendency of the system. We do this by modifying our scoring function to directly estimate the probability of a summary using a log-linear model, as is standard in machine translation: p(y|x; θ, α) a exp(αT N−1� f(yi+1, x, yc)). i=0 Where α E R5 is a weight vector and f is a feature function. Finding the best summary under this distribution corresponds to maximizing a factored scoring function s, s(y, x) = N−1� αTf(yi+1, x, yc). i=0 where g(yi+1, x, yc) g αTf(yi+1, </context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2014</marker>
<rawString>Thang Luong, Ilya Sutskever, Quoc V Le, Oriol Vinyals, and Wojciech Zaremba. 2014. Addressing the rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to information retrieval, volume 1. Cambridge university press Cambridge.</title>
<date>2008</date>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to information retrieval, volume 1. Cambridge university press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The stanford corenlp natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="23640" citStr="Manning et al., 2014" startWordPosition="3997" endWordPosition="4000">n addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation set, which we will include in our code release. For this evaluation, we tune systems to generate output of the average title length. For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al., 2014). Our model only uses annotations for tokenization and sentence separation, although several of the baselines use parsing and tagging as well. Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades. For our training set, we pair the headline of each article with its first sentence to create an inputsummary pair. While the model could in theory be trained on any pair, Gigaword contains many spurious headline-article pairs. We therefore prune training based on the following heuristic filters: (1) Are there no non</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated gigaword.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,</booktitle>
<pages>95--100</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 95–100. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18563" citStr="Och, 2003" startWordPosition="3168" endWordPosition="3169">p(yi+1|x, yc; θ), 1{]j. yi+1 = xj }, 1{]j. yi+1−k = xj−k Vk E {0,1}}, 1{]j. yi+1−k = xj−k Vk E {0, 1, 2}}, 1{]k &gt; j. yi = xk, yi+1 = xj} ]. These features correspond to indicators of unigram, bigram, and trigram match with the input as well as reordering of input words. Note that setting α = (1, 0, ... , 0) gives a model identical to standard ABS. N−1� g(yi+1, x, yc). i=0 383 After training the main neural model, we fix θ and tune the α parameters. We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003). This tuning step is also identical to the one used for the phrase-based machine translation baseline. 6 Related Work Abstractive sentence summarization has been traditionally connected to the task of headline generation. Our work is similar to early work of Banko et al. (2000) who developed a statistical machine translation-inspired approach for this task using a corpus of headline-article pairs. We extend this approach by: (1) using a neural summarization model as opposed to a count-based noisy-channel model, (2) training the model on much larger scale (25K compared to 4 million articles), </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
<author>Hoa Dang</author>
<author>Donna Harman</author>
</authors>
<date>2007</date>
<booktitle>Duc in context. Information Processing &amp; Management,</booktitle>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="19299" citStr="Over et al., 2007" startWordPosition="3282" endWordPosition="3285"> Abstractive sentence summarization has been traditionally connected to the task of headline generation. Our work is similar to early work of Banko et al. (2000) who developed a statistical machine translation-inspired approach for this task using a corpus of headline-article pairs. We extend this approach by: (1) using a neural summarization model as opposed to a count-based noisy-channel model, (2) training the model on much larger scale (25K compared to 4 million articles), (3) and allowing fully abstractive decoding. This task was standardized around the DUC2003 and DUC-2004 competitions (Over et al., 2007). The TOPIARY system (Zajic et al., 2004) performed the best in this task, and is described in detail in the next section. We point interested readers to the DUC web page (http://duc.nist. gov/) for the full list of systems entered in this shared task. More recently, Cohn and Lapata (2008) give a compression method which allows for more arbitrary transformations. They extract tree transduction rules from aligned, parsed texts and learn weights on transfomations using a max-margin learning algorithm. Woodsend et al. (2010) propose a quasi-synchronous grammar approach utilizing both context-free</context>
<context position="21930" citStr="Over et al., 2007" startWordPosition="3719" endWordPosition="3722"> between the current position and the input source. Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models. We hope to incorporate an RNNLM in future work. 7 Experimental Setup We experiment with our attention-based sentence summarization model on the task of headline generation. In this section we describe the corpora used for this task, the baseline methods we compare with, and implementation details of our approach. 7.1 Data Set The standard sentence summarization evaluation set is associated with the DUC-2003 and DUC2004 shared tasks (Over et al., 2007). The data for this task consists of 500 news articles from the New York Times and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes. This data set is evaluation-only, although the similarly sized DUC-2003 data set was made available for the task. The expectation is for a summary of roughly 14 words, based on the text of a complete article (although we only make use of the first sentence). The full data set is available by request at http://duc.nist.gov/data.html. For this shared task, systems were enter</context>
</contexts>
<marker>Over, Dang, Harman, 2007</marker>
<rawString>Paul Over, Hoa Dang, and Donna Harman. 2007. Duc in context. Information Processing &amp; Management, 43(6):1506–1520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="15601" citStr="Sutskever et al., 2014" startWordPosition="2622" endWordPosition="2625">ion: Extractive Tuning y* = arg max YEY Unlike phrase-based machine translation where inference is NP-hard, it actually is tractable in theory to compute y*. Since there is no explicit hard alignment constraint, Viterbi decoding can be applied and requires O(NV C) time to find an exact solution. In practice though V is large enough to make this difficult. An alternative approach is to approximate the arg max with a strictly greedy or deterministic decoder. While decoders of this form can produce very bad approximations, they have shown to be relatively effective and fast for neural MT models (Sutskever et al., 2014). A compromise between exact and greedy decoding is to use a beam-search decoder (Algorithm 1) which maintains the full vocabulary V while limiting itself to K potential hypotheses at each position of the summary. The beam-search algorithm is shown here: Algorithm 1 Beam Search Input: Parameters θ, beam size K, input x Output: Approx. K-best summaries π[0] ← {e} S = V if abstractive else {xi |∀i} for i = 0 to N − 1 do . Generate Hypotheses N ← {[y, yi+1] |y ∈ π[i], yi+1 ∈ S} . Hypothesis Recombination H ← Jy ∈ N |s(y, x) &gt; s(Y , x) &apos; � l ∀y ∈ N s.t. yc = yc . Filter K-Max π[i + 1] ← K-arg max </context>
<context position="21172" citStr="Sutskever et al., 2014" startWordPosition="3593" endWordPosition="3596">es) of articletitle pairs. Because their focus is extractive compression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly. Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source. Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models. We hope to incorporate an RNNLM in future work. 7 Experimental Setup We experiment with our attention-based sentence summarization model on the task of headline generation. In this section we describe the corpora used for this task, the baseline methods we compare with, and implementation details</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Generation with quasi-synchronous grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 conference on empirical methods in natural language processing,</booktitle>
<pages>513--523</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2188" citStr="Woodsend et al., 2010" startWordPosition="326" endWordPosition="329"> apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Figure 1: Example output of the attention-based summarization (ABS) system. The heatmap represents a soft alignment between the input (right) and the generated summary (top). The columns represent the distribution over the input after generating each word. Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1). Crucially both the encoder and the generation model are trained jointly on the sentence summarization task. The model is desc</context>
<context position="19826" citStr="Woodsend et al. (2010)" startWordPosition="3368" endWordPosition="3371">ng. This task was standardized around the DUC2003 and DUC-2004 competitions (Over et al., 2007). The TOPIARY system (Zajic et al., 2004) performed the best in this task, and is described in detail in the next section. We point interested readers to the DUC web page (http://duc.nist. gov/) for the full list of systems entered in this shared task. More recently, Cohn and Lapata (2008) give a compression method which allows for more arbitrary transformations. They extract tree transduction rules from aligned, parsed texts and learn weights on transfomations using a max-margin learning algorithm. Woodsend et al. (2010) propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce legible summaries. Both of these approaches differ from ours in that they directly use the syntax of the input/output sentences. The latter system is W&amp;L in our results; we attempted to train the former system T3 on this dataset but could not train it at scale. In addition to Banko et al. (2000) there has been some work using statistical machine translation directly for abstractive summary. Wubben et al. (2012) utilize MOSES directly as a method for text simplification. Recently Fi</context>
<context position="25836" citStr="Woodsend et al. (2010)" startWordPosition="4356" endWordPosition="4359"> 7.2 Baselines Due to the variety of approaches to the sentence summarization problem, we report a broad set of headline-generation baselines. From the DUC-2004 task we include the PREFIX baseline that simply returns the first 75- characters of the input as the headline. We also report the winning system on this shared task, TOPIARY (Zajic et al., 2004). TOPIARY merges a compression system using linguisticallymotivated transformations of the input (Dorr et al., 2003) with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output. Woodsend et al. (2010) (described above) also report results on the DUC dataset. The DUC task also includes a set of manual summaries performed by 8 human summarizers each summarizing half of the test data sentences (yielding 4 references per sentence). We report the average inter-annotater agreement score as REFERENCE. For reference, the best human evaluator scores 31.7 ROUGE-1. We also include several baselines that have access to the same training data as our system. The first is a sentence compression baseline COMPRESS (Clarke and Lapata, 2008). This model uses the syntactic structure of the original sentence a</context>
</contexts>
<marker>Woodsend, Feng, Lapata, 2010</marker>
<rawString>Kristian Woodsend, Yansong Feng, and Mirella Lapata. 2010. Generation with quasi-synchronous grammar. In Proceedings of the 2010 conference on empirical methods in natural language processing, pages 513– 523. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal Van Den Bosch</author>
<author>Emiel Krahmer</author>
</authors>
<title>Sentence simplification by monolingual machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>1015--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Wubben, Van Den Bosch, Krahmer, 2012</marker>
<rawString>Sander Wubben, Antal Van Den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 1015–1024. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
</authors>
<title>Z-mert: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="29353" citStr="Zaidan, 2009" startWordPosition="4955" endWordPosition="4956"> D = 200, H = 400, C = 5, L = 3, and Q = 2. Our implementation uses the Torch numerical framework (http://torch.ch/) and will be openly available along with the data pipeline. Crucially, training is performed on GPUs and would be intractable or require approximations otherwise. Processing 1000 mini-batches with D = 200, H = 400 requires 160 seconds. Best validation accuracy is reached after 15 epochs through the data, which requires around 4 days of training. Additionally, as described in Section 5 we apply a MERT tuning step after training using the DUC2003 data. For this step we use Z-MERT (Zaidan, 2009). We refer to the main model as ABS and the tuned model as ABS+. 8 Results Our main results are presented in Table 1. We run experiments both using the DUC-2004 evaluation data set (500 sentences, 4 references, 75 bytes) with all systems and a randomly held-out Gigaword test set (2000 sentences, 1 reference). We first note that the baselines COMPRESS and IR do relatively poorly on both datasets, indicating that neither just having article information or language model information alone is sufficient for the task. The PREFIX baseline actually performs surprisingly well on ROUGE-1 which makes se</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar Zaidan. 2009. Z-mert: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Zajic</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Bbn/umd at duc-2004: Topiary.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>112--119</pages>
<location>Boston,</location>
<contexts>
<context position="1832" citStr="Zajic et al., 2004" startWordPosition="270" endWordPosition="273">marization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Figure 1: Example output of the attention-based summarization (ABS) system. The heatmap represents a soft alignment between the input (right) and the generated summary (top). The columns represent the distribution over the input after generating each word. Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model</context>
<context position="19340" citStr="Zajic et al., 2004" startWordPosition="3289" endWordPosition="3292">been traditionally connected to the task of headline generation. Our work is similar to early work of Banko et al. (2000) who developed a statistical machine translation-inspired approach for this task using a corpus of headline-article pairs. We extend this approach by: (1) using a neural summarization model as opposed to a count-based noisy-channel model, (2) training the model on much larger scale (25K compared to 4 million articles), (3) and allowing fully abstractive decoding. This task was standardized around the DUC2003 and DUC-2004 competitions (Over et al., 2007). The TOPIARY system (Zajic et al., 2004) performed the best in this task, and is described in detail in the next section. We point interested readers to the DUC web page (http://duc.nist. gov/) for the full list of systems entered in this shared task. More recently, Cohn and Lapata (2008) give a compression method which allows for more arbitrary transformations. They extract tree transduction rules from aligned, parsed texts and learn weights on transfomations using a max-margin learning algorithm. Woodsend et al. (2010) propose a quasi-synchronous grammar approach utilizing both context-free parses and dependency parses to produce </context>
<context position="25569" citStr="Zajic et al., 2004" startWordPosition="4317" endWordPosition="4320">K word types with the average title of length 8.3 words (note that this is significantly shorter than the DUC summaries). On average there are 4.6 overlapping word types between the headline and the input; although only 2.6 in the first 75-characters of the input. 7.2 Baselines Due to the variety of approaches to the sentence summarization problem, we report a broad set of headline-generation baselines. From the DUC-2004 task we include the PREFIX baseline that simply returns the first 75- characters of the input as the headline. We also report the winning system on this shared task, TOPIARY (Zajic et al., 2004). TOPIARY merges a compression system using linguisticallymotivated transformations of the input (Dorr et al., 2003) with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output. Woodsend et al. (2010) (described above) also report results on the DUC dataset. The DUC task also includes a set of manual summaries performed by 8 human summarizers each summarizing half of the test data sentences (yielding 4 references per sentence). We report the average inter-annotater agreement score as REFERENCE. For reference, the best human eva</context>
</contexts>
<marker>Zajic, Dorr, Schwartz, 2004</marker>
<rawString>David Zajic, Bonnie Dorr, and Richard Schwartz. 2004. Bbn/umd at duc-2004: Topiary. In Proceedings of the HLT-NAACL 2004 Document Understanding Workshop, Boston, pages 112–119.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
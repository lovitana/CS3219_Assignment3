<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990987">
Bilingual Structured Language Models for Statistical Machine Translation
</title>
<author confidence="0.98312">
Ekaterina Garmash and Christof Monz
</author>
<affiliation confidence="0.993225">
Informatics Institute, University of Amsterdam
</affiliation>
<address confidence="0.930447">
Science Park 904, 1098 XH Amsterdam, The Netherlands
</address>
<email confidence="0.998254">
{e.garmash,c.monz}@uva.nl
</email>
<sectionHeader confidence="0.993871" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999832888888889">
This paper describes a novel target-side
syntactic language model for phrase-based
statistical machine translation, bilingual
structured language model. Our approach
represents a new way to adapt structured
language models (Chelba and Jelinek,
2000) to statistical machine translation,
and a first attempt to adapt them to phrase-
based statistical machine translation. We
propose a number of variations of the
bilingual structured language model and
evaluate them in a series of rescoring ex-
periments. Rescoring of 1000-best transla-
tion lists produces statistically significant
improvements of up to 0.7 BLEU over a
strong baseline for Chinese-English, but
does not yield improvements for Arabic-
English.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961111111111">
Many model components of competitive statisti-
cal machine translation (SMT) systems are based
on rather simplistic definitions with little linguis-
tic grounding, which includes the definitions of
phrase pairs, lexicalized reordering, and n-gram
language models. However, earlier work has also
shown that statistical MT can benefit from ad-
ditional linguistically motivated models. Most
prominent among the linguistically motivated ap-
proaches are syntax-based MT systems which
take into account the syntactic structure of sen-
tences through CKY decoding and categorial la-
bels (Zollmann and Venugopal, 2006; Shen et al.,
2008). On the other hand, the commonly used
phrase-based SMT approaches can also reap some
of the benefits of using syntactic information by
integrating linguistic components addressing spe-
cific phenomena, such as Cherry (2008), Carpuat
et al. (2010), Crego and Yvon (2010), Ge (2010),
Xiang et al. (2011), Lerner and Petrov (2013),
Garmash and Monz (2014).
This paper is a contribution to the existing body
of work on how syntactically motivated models
help translation performance. We work with the
phrase-based SMT (PBSMT) (Koehn et al., 2003)
framework as the baseline system. Our choice is
motivated by the fact that PBSMT is a conceptu-
ally simple and therefore flexible framework. It is
typically quite straightforward to integrate an ad-
ditional model into the system. Also, PBSMT is
the most widely used framework in the SMT re-
search community, which ensures comparability
of our results to other people’s work on the topic.
There is a variety of ways syntax can be used in
a PBSMT model. Typically a syntactic represen-
tation of a source sentence is used to define con-
straints on the order in which the decoder trans-
lates it. For example, Cherry (2008) defines soft
constraints based on the notion of syntactic cohe-
sion (Section 2). Ge (2010) captures reordering
patterns by defining soft constraints based on the
currently translated word’s POS tag and the words
structurally related to it. On the other hand, tar-
get syntax is more challenging to use in PBSMT,
since a target-side syntactic model does not have
access to the whole target sentence at decoding.
Post and Gildea (2008) is one of the few target-
side syntactic approaches applicable to PBSMT,
but it has been shown not to improve translation.
Their approach uses a target side parser as a lan-
guage model: one of the reasons why it fails is
that a parser assumes its input to be grammatical
and chooses the most likely parse for it. What we
are interested in during translation is how gram-
</bodyText>
<page confidence="0.911906">
2398
</page>
<note confidence="0.9900605">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.8118625">
0 1 2 0 1 2 0 1 2 3 0 1 2 3
(a) (b) (c) (d)
</figure>
<bodyText confidence="0.987721166666667">
matical the target sentence actually is.
In addition to reordering constraints, source
syntax can be used for target-side language mod-
eling. A target side string can be encoded with
source-syntactic building blocks and then scored
as to how well-formed it is. Crego and Yvon
(2010), Niehues et al. (2011), Garmash and Monz
(2014) model target sequences as strings of tokens
built from the target POS tag and the POS tags of
the source words related to it through alignment
and the source parse. In this paper, we define
a target-side syntactic language model that takes
structural constraints from the source sentence, but
uses the words from the target side (as ‘building
blocks’). We do it by adapting an existing mono-
lingual model of Chelba and Jelinek (2000), struc-
tured language models, to the bilingual setting.
Our contributions can be summarized as follows:
</bodyText>
<listItem confidence="0.9169921875">
• we propose a novel method to adapt monolin-
gual structured language models (Chelba and
Jelinek, 2000) (Section 3) to a PBSMT sys-
tem (Section 4), which does not require an
external on-the-fly parser, but only uses the
given source-side syntactic analysis to infer
structural relations between target words;
• building on the existing literature, we pro-
pose a set of deterministic rules that incre-
mentally build up a parse of a target trans-
lation hypothesis based on the source parse
(Section 4);
• we evaluate our models in a series of rescor-
ing experiments and achieve statistically sig-
nificant improvements of up to 0.7 BLEU for
Chinese-English (Section 5).
</listItem>
<bodyText confidence="0.998624666666667">
Before describing the models, we motivate our
method with a common assumption about cross-
lingual correspondence (Section 2).
</bodyText>
<sectionHeader confidence="0.8085465" genericHeader="method">
2 Direct correspondence assumption and
syntactic cohesion in SMT
</sectionHeader>
<bodyText confidence="0.820625326923077">
Before we apply the syntactic model introduced in
Section 3 to the bilingual setting (Section 4), we
first explain two widely used assumptions about
syntactic correspondence across languages.
We take a dependency tree to be a syntactic rep-
resentation of a sentence and reason about other
syntactic assumptions and models in its terms.
In this work, we choose a dependency structure
over a constituency structure because the former
Figure 1: Examples of projective and non-
projective parses. (a-b): projective (a) and non-
projective (b) parses of the same dependency tree.
(b) is non-projective because node 1 is not a de-
scendant of either 0 or 2 (it is the parent of 2). (c-
d): projective (c) and non-projective (d) parses of
the same dependency tree. Node 2 in (d) is placed
between its sibling (node 1) and the child of its
sibling (node 3), neither of which is its ancestor.
is more primitive.1 A dependency parse D is a
dependency tree analysis of a sentence W, and
we will think of it as a relation between words
of W, such that D(w, v) if w is a parent (head)
of v (v being a child/modifier). D can be gener-
alized to D* which is an relation between words
that are connected by a continuous path in a de-
pendency tree (i.e. D*(w, v) if D(w, v) or if lu
s.t. D(w, u) ∧ D*(u, v)). We assume unlabeled
dependency trees. Finally, we make a projectivity
assumption, which is supported by empirical data
in many languages (Kuhlmann and Nivre, 2006;
Havelka, 2007), and makes a model computation-
ally less expensive. A dependency parse D of a
sentence W = wi, ... , wn is projective, if for ev-
ery word pair wi, wj E W s.t. D(wi, wj) it holds
that every wk E W s.t. i &lt; k &lt; j or j &lt; k &lt; i is
a descendant of wi, i.e., D*(wi, wk); see Figure 1.
Most NLP models that address the interaction
of two or more languages are based (explicitly or
implicitly) on the direct correspondence assump-
tion (DCA) (Hwa et al., 2002). It states that close
translation equivalents in different languages have
the same dependency structure. This is grounded
linguistically, as translation equivalence implies
semantic equivalence and therefore thematic rela-
tions are preserved (Hwa et al., 2002). Thus de-
pendency relations are preserved, as they are de-
fined based on thematic relations between words.
On the other hand, there is plenty empirical evi-
dence supporting the violation of DCA under cer-
tain conditions (Hwa et al., 2002). For instance,
even semantically very close sentences in differ-
ent languages may have a different number of
</bodyText>
<footnote confidence="0.99528525">
1A dependency parse (a dependency tree analysis of a sen-
tence) is more primitive because every constituency parse can
be formalized as a projective dependency parse with labeled
relations, but not vice versa (Osborne, 2008).
</footnote>
<page confidence="0.99711">
2399
</page>
<figureCaption confidence="0.552862222222222">
Figure 2: Examples of cohesive and uncohesive
translations. (a-b): cohesive (a) and uncohesive
(b) translations of the same dependency parse. (b)
is uncohesive because words a and c translate the
source subtree {(1, 2)}, but the target word b does
not translate this subtree. (c-d): cohesive (c) and
uncohesive (d) translations. (d) is uncohesive be-
cause a and c translate the source subtree {(0,1)},
but b does not translate it.
</figureCaption>
<bodyText confidence="0.996585177777778">
words. Syntactic divergence increases if the two
languages are typologically different.
Even though DCA only holds up to a certain
level of precision, it is widely used in NLP. There
are models of cross-lingual transfer that define
syntactic structure of one language by condition-
ing it on the structure of semantically equiva-
lent sentences in another language (Naseem et al.,
2012). DCA has also been used in SMT. In partic-
ular, syntax-based SMT is built implicitly around
this assumption (Wu, 1997; Yamada and Knight,
2001). In Quirk and Menezes (2006) DCA is
explicitly implemented by defining a translation
model in terms of treelet pairs where target-side
treelets are produced by projecting source depen-
dencies via word alignments.
Closely related to DCA is the notion of syn-
tactic cohesion of translation (Fox, 2002; Cherry,
2008). This is a constraint that does not allow for
non-projective reordering: Given a source parse
DS, a translation W is cohesive if all translated
target words wZ, wj do not have any word wk be-
tween them such that there is a source subtree sub
in DS such that some parts of it are translated by
wZ and wj but not by wk (Figure 2). Cherry (2008)
and Bach et al. (2009) define a set of soft con-
straints based on the syntactic cohesion assump-
tion which are applicable to PBSMT decoding.
They only require phrase applications, and not
necessarily individual target words, to conform to
the cohesion principle. For example, if we imag-
ine a situation where a subtree as in Figure 2(b)
is translated as a whole with one phrase applica-
tion (and not word by word), then it does not vio-
late the cohesion principle, although it is internally
uncohesive. Both our approach and Cherry (2008)
implement the idea of conforming the target trans-
lation to the source syntactic structure, but in dif-
ferent ways. Approaches like Cherry (2008) de-
fine principles that constrain the decoder in order
to produce better translations. Our goal is to have
a model that allows for a more direct way of evalu-
ation of how well-formed the target translation is.
In Section 5 we compare translation performance
of the two approaches.
</bodyText>
<sectionHeader confidence="0.977433" genericHeader="method">
3 Structured language models
</sectionHeader>
<bodyText confidence="0.999836717948718">
As discussed in Sections 1 and 2, we would like to
test how much a PBSMT can benefit from an ad-
ditional syntax-based LM. In this section, we de-
scribe a syntactic language model, structured LM
(SLM) (Chelba and Jelinek, 2000), that we extend
to a bilingual setting and apply to SMT in Sec-
tion 4. SLMs have been applied in SMT before
(Yamada and Knight, 2001; Yu et al., 2014), but
as we show in Section 4, we provide a much sim-
pler method to integrate it into the system. While
a SLM is not the only syntactically defined LM,
it is one of the few that models sentence genera-
tion sequentially. And due to the way the decoding
procedure of PBSMT is defined, it is natural and
straightforward to use models whose score can be
computed sequentially. Other syntactic language
models define sentence generation hierarchically
(Shen et al., 2008; Sennrich, 2015), which com-
plicates their integration into a PBSMT system.
The linguistic intuition behind SLMs is that the
structural children of a word do not essentially
change its distributional properties but just provide
additional specification. In Figure 3(a) the word
president has two modifiers: the and former and
it follows yesterday (an adjunct) and precedes met
(a predicate). This ordering is correct in English.
If instead its modifier was a or an entire relative
clause, it would not make it incorrect.
To capture this observation, (Chelba and Je-
linek, 2000) propose a language model where each
word wZ of a sentence W is predicted by an or-
dered subset of the words preceding wZ. This con-
ditioning subset is selected based on the syntactic
properties of the preceding sequence WZ−1: the
strong predictors are kept and the weak ones are
left out. The strong predictors are the set of ex-
posed heads. Given a subsequence WZ−1 and its
associated parse DZ−1, exposed heads are the roots
of all the disconnected subtrees in DZ−1. Note that
</bodyText>
<figure confidence="0.973761833333333">
a b c a b c a b c d a b c d
(a) (b) (c) (d)
0 1 2
0 1 2
0 1 2 3
0 1 2 3
2400
yesterday
met
president
press
the former the
</figure>
<bodyText confidence="0.99744025">
sequence of exposed heads Expos(W, D). Then a
tag ti is predicted, and the parse Di−i of Wi−1
is extended to Di incorporating wi and ti (where
Wi−1 is the prefix of W preceding wi):
</bodyText>
<figure confidence="0.980897857142857">
(a) p(W, D) = |W|
1 jp(wi  |Expos(Wi−1 , Di−1))
i=1
yesterday president met
the former
yesterday president met
who É in London two days before
</figure>
<figureCaption confidence="0.998902">
Figure 3: A fully parsed sentence (a) and its partial
</figureCaption>
<bodyText confidence="0.969615551724138">
parse (b) during sequential generation. The par-
tial parse in (b) has two disconnected subtrees with
roots yesterday and president. These roots are the
exposed heads for met. (c) is an alternative sen-
tence with a similar structure: president is still a
root of a subtree, and thus and an exposed head.
a parse Di−1 is not necessarily fully connected and
thus a word can have multiple conditioning words.
For an example, consider again Figure 3(a). In
a left-to-right scenario, when met is generated, a
regular n-gram LM conditions it on yesterday the
former president, while a SLM conditions it on
yesterday president, since these two words are the
exposed heads with respect to met (Figure 3(b)).
The words the and former are modifiers of pres-
ident and they get filtered out. Thus we obtain a
less specific conditioning history, which may lead
to the resulting model being less sparse. Another
potential benefit is that SLMs can capture long-
distance reordering: If president had as its mod-
ifier a relative clause (Figure 3(c)) then a simple
n-gram LM would be conditioned on days before
(assuming n = 3), while an SLM would condition
met on yesterday president.
Summarizing the ideas of words being con-
ditioned on a structurally defined subset of the
preceding sentence, Chelba and Jelinek (2000)
formalize the generation process of W as fol-
lows:2 Each new word wi is conditioned on a
</bodyText>
<footnote confidence="0.5506475">
2The original model by (Chelba and Jelinek, 2000) is de-
fined in terms of a lexicalized constituency grammar, but as
</footnote>
<equation confidence="0.937810333333333">
· p(ti|wi, Expos(Wi−1, Di−1))
· p(Di|wi, ti, Expos(Wi−1, Di−1)).
(1)
</equation>
<bodyText confidence="0.9961225">
They use a shift-reduce parser with reduce-left,
reduce-right, and shift operations.
</bodyText>
<sectionHeader confidence="0.93781" genericHeader="method">
4 Bilingual structured language models
</sectionHeader>
<bodyText confidence="0.99998544">
In this section, we combine the direct correspon-
dence assumption (Section 2) and SLMs (Sec-
tion 3), and define bilingual structured language
models (BiSLMs) for PBSMT. Structured LMs
have been successfully applied in SMT before.
Yamada and Knight (2001) use SLMs in a string-
to-tree SMT system where a derivation of a target-
side parse tree is part of the decoding algorithm,
and target syntactic representations are obtained
‘for free’. Yu et al. (2014) use an on-the-fly shift-
reduce parser to build an incremental target parse.
The approaches sketched above rely on re-
sources that a standard PBSMT system does not
have access to by default. Phrase-based decoders
do not provide us with a parse of the target
sentence, and inferring the parse of a target string
with an external parser is computationally expen-
sive and potentially unreliable (see Section 1).
Our main insight is that in a bilingual setting one
does not need an additional probabilistic target
parsing model. We assume that the source parse is
given (precomputed) and that the DCA (Section 2)
holds, and project the parse deterministically onto
the target side via word alignments3. We obtain
the following equation:
</bodyText>
<equation confidence="0.9706815">
p(ti |Expos(Ti−1, (2)
ProjP(DS, S, Ti−1))),
</equation>
<bodyText confidence="0.8943735">
where T is a target sentence, Ti−1 is the sequence
in T preceding the i-th target word ti, S is a
we discussed in Section 2, constituency parses can be trans-
formed into dependency parses.
3Phrase-internal word alignments are stored in the phrase
table and are available at decoding time, see Section 4.4.
</bodyText>
<equation confidence="0.954764">
arrived
the
p(T |S, DS) = 1 j |T|
i=1
</equation>
<page confidence="0.841175">
2401
</page>
<bodyText confidence="0.883102166666667">
pujing shuo ta xihuan suoyou de eluosi funv
putin said he likes all russian women
pujing shuo ta xihuan
putin said he likes
pujing shuo ta xihuan suoyou de eluosi funv
putin said he likes all russian women
</bodyText>
<figureCaption confidence="0.856809666666667">
Figure 4: Chinese-English sentence pair (a) and
sets of exposed heads (underlined) at different
generation (b and c) steps of a bilingual SLM.
</figureCaption>
<bodyText confidence="0.999819">
source sentence, DS is a source dependency parse,
and ProjP is a function that returns a partial tar-
get parse DT i−1 by projecting DS onto Ti−1. In
words, at each time step i we predict the next word
ti conditioned on the exposed heads of the partial
parse of Ti−1 projected from the source side. We
limit Expos to returning the four preceding exposed
heads.4 Because the function ProjP is determinis-
tic and because we do not have to predict tags for
words, Equation 2 is simpler than Equation 1.
We first illustrate Equation 2 with an example
in Figure 4. Since word alignment is monotonic
in Figure 4(a), it is straightforward to project the
source dependencies onto the target side. We aim
to imitate a monolingual parser in the way we
build up our projected parse: Reduce operations
should be invoked whenever both of the subtrees
involved in the operation are complete, i.e., are
not expected to have any more modifiers (Sec-
tion 4.2). For example, when the target word likes
is produced its exposed heads are said and he (Fig-
ure 4(b)), since Putin is a modifier of said. Like-
wise, the exposed heads for women are said likes
all Russian (Figure 4(c)).
In what follows we discuss how to define ProjP.
Compared to projection approaches like (Quirk
</bodyText>
<footnote confidence="0.983650625">
4As written above, we choose the dependency structures
over the lexicalized constituency ones because the latter can
be mapped to the former. It is thus more likely that a pro-
jected dependency tree is still be a well-formed parse, than a
projected constituency tree. We decided to work with struc-
tural models that are more flexible, but one may also define
BiSLM in terms of the more constraining constituency trees
and see if the such model has better generalization power.
</footnote>
<bodyText confidence="0.998904666666667">
and Menezes, 2006), we would like our model
to project a source parse incrementally, allowing
it to be used in a PBSMT decoder. We think of
ProjP as a function that computes the output in
two stages: first, it infers from the source parse the
dependency relations between target words (Sec-
tion 4.1), second, it decides how to parse the tar-
get sequence, i.e. in which order to assign these
dependencies (Section 4.2). Additionally, in Sec-
tion 4.3 we propose to use additional labelings of
target words, and in Section 4.4 we describe some
important implementation details.
</bodyText>
<subsectionHeader confidence="0.992411">
4.1 Dependency graph projection
</subsectionHeader>
<bodyText confidence="0.999963277777778">
Adoption of DCA (Section 2) allows to build up a
target dependency tree from a source tree by pro-
jecting the latter through word alignments. The
definition of DCA can be rephrased as requiring
a one-to-one correspondence map between words
of a sentence pair, allowing one to unambigu-
ously map dependencies: Given a source parse,
if t1 is the head of t2, then map(t1) is the head
of map(t2). The correspondence relation that
we have in PBSMT is the word alignment align:
in the most general case, it is a many-to-many
correspondence, and the straightforward projec-
tion described above can lead to incorrect depen-
dency structures. To overcome these problems,
we describe a simple ordered set of projection
rules, based on the ones specified by (Quirk and
Menezes, 2006) (and we point out if otherwise).
The general idea behind this set of rules is to ex-
tract a one-to-one function align1−1 from source
words to target words from align and use it to
project source dependencies as described in the
paragraph above (R1 below). We then use addi-
tional rules (R2-R4 below) for the target words
that are not in align1−1. Given a source sen-
tence 5 with a parse DS, a target sentence T and
word alignment align, align1−1 is extracted as fol-
lows: For all ti E T with multiple aligned source
words {si1, si2, ...} only align1−1(si1) = ti (only
leftmost source word is kept, the links from the
rest of the source words are removed5). For all
si E 5 with aligned target words {ti1, ti2, ...} keep
the link only for the leftmost aligned target word:
align1−1(si) = ti1. For example, in Figure 5(b)
the link between f0 and e1 is not in align1−1, and
in Figure 5(c) the link between f1 and e0 is re-
moved (and the arc from f2 to f1 is not projected).
</bodyText>
<footnote confidence="0.983591">
5This is an ad-hoc solution, other heuristics could be used.
</footnote>
<page confidence="0.959476">
2402
</page>
<equation confidence="0.635528">
f0 f1 f2
e0 e1 e2
f0 f1
e0 e1 e2 e3
</equation>
<figure confidence="0.86453875">
(c)
(b)
(a)
(d)
f0 f1
e0 e1 e3
f0 f1 f2
e0 e1
(a)
(b)
e0 e1 e2
f0 f1 f2
f0 f1 f2
e0 e1 e2
f2
e3
</figure>
<figureCaption confidence="0.981132">
Figure 5: Examples for dependency projection
rules. (a): no alignment links get removed (R1).
</figureCaption>
<bodyText confidence="0.824001545454545">
(b): f0 − e1 link is removed from align1−1 (R1).
(c): f1 − e0 link gets removed (R1). (d): e1 and
e2 get adjoined to e0 (R2). (e): R3a. (f): R3b.
(g) demonstrates two versions of R4: the dashed
arrow gets ‘realized’ only if we adjoin unaligned
words to the preceding head.
The following rules should be applied in order
(as else-if conditions). Given a source sentence
5 with a parse DS, a target sentence T and word
alignment align between them, ti E T is a head of
tj E T (i.e. DT (ti, tj)):
</bodyText>
<listItem confidence="0.946349333333333">
(R1) if there are sk, sl E 5 s.t. DS(sk, sl) and
align1−1(sk) = ti and align1−1(sl) = tj; see Fig-
ures 5(a)-5(c);
(R2) if Is E 5 s.t. align1−1(s) = ti and (s, tj) E
align. This rule deals with one-to-many align-
ments; see Figure 5(d);
</listItem>
<bodyText confidence="0.826873625">
(R3a) if Isk s.t. align1−1(sk) = ti and Isl s.t.
(sl, tj) E align and and DS(sl, sk), and ti linearly
precedes tj. In words: if two target words are in
align1−1 but do not get connected via R1, find a
source word aligned to the second target word that
may get them connected; see Figure 5(e);
(R3b) same as R3a, but in case tj precedes ti (i.e.,
find an additional source word aligned to the first
target word; see Figure 5(f)).6
(R4) In case FIs (s, tj) E align (tj is unaligned),
we consider two strategies: We simplify the rule of
Quirk and Menezes (2006) (dealing with the same
situation) by adjoining it to the immediately pre-
ceding head. We also consider a strategy whereby
the word remains unconnected to any word in the
sentence; see Figure 5(g).
</bodyText>
<footnote confidence="0.725696">
6R3a and R3b differ from the rules proposed in Quirk and
Menezes (2006) dealing with the same situation, since we had
to adapt it to the left-to-right parsing scenario.
</footnote>
<figureCaption confidence="0.884027">
Figure 6: (a): The dashed lines are the dependency
</figureCaption>
<bodyText confidence="0.9104675">
arcs that would project through word alignment,
resulting in a non-projective projective (impossi-
ble under strong source-completeness). (b): The
dashed lines are the parse produced under weak
source-completeness. Under strong completeness
none of the words will get connected.
</bodyText>
<subsectionHeader confidence="0.982438">
4.2 BiSLM parsing procedure
</subsectionHeader>
<bodyText confidence="0.999832081081081">
Given an inference procedure for dependency re-
lations between target words (Section 4.1), one
can specify in which order the corresponding de-
pendency arcs are assigned to the target sentence.
We define an incremental parsing procedure in
terms of three operations: shift, left-reduce, and
right-reduce. The operations are applied as soon
as the sufficient conditions hold: We specify the
conditions using the following structural proper-
ties. A target subtree is source-complete if all the
descendants of align−1
1−1(root(sub)) (source corre-
spondent of the root of the current subtree) (Sec-
tion 4.1) have been translated and reduced. A tar-
get subtree is complete if it is source-complete and
all the target words that are its children through
non-projected arcs (through R2 or R4 in Sec-
tion 4.1) have been translated and reduced. The
bilingual parsing operations and the sufficient con-
ditions for them are defined as follows:
Shift: after the word is produced it is shifted onto
the stack as an elementary subtree.
Left-reduce: if a disconnected subtree subi
and a disconnected subtree subi−1 imme-
diately preceding it are both complete and
DT(root(subi), root(subi−1)), adjoin subi−1
to subi so that root(subi−1) is a modifier of
root(subi).
Right-reduce: analogous to left-reduce, but
DT(root(subi−1), root(subi)).
In the case of non-cohesive translation the re-
sulting target dependencies are non-projective.
Our definition of left- and right-reduce only pro-
duces projective parses. For a non-cohesive
translation, certain subtrees will never be source-
complete and will never be reduced; see Fig-
ure 6(a). Note that this is not a disadvantage
</bodyText>
<equation confidence="0.574987727272727">
(e)
(f)
(g)
f0 f1 f2
e0 e1
f0 f1
e0 e1 e1
f0
e0
f1 f2
e1
</equation>
<page confidence="0.94768">
2403
</page>
<bodyText confidence="0.9999799">
of our model. Cherry (2008) simply assumes
that non-cohesive reordering should be penalized,
and our model is able to learn this pattern. We
also consider an alternative to incorporating non-
cohesive alignments by relaxing the definition of
completeness for subtrees: A projected subtree
sub is weakly source-complete if all descendants
of all source word(s) which are aligned to the root
of sub have been translated and, only if the defini-
tion of reduce applies, reduced; see Figure 6(b).
</bodyText>
<subsectionHeader confidence="0.999141">
4.3 Syntactic labeling of tokens
</subsectionHeader>
<bodyText confidence="0.999945733333333">
One of the problems with SLMs in general is that
at time steps i and j the sets of exposed heads for
ti and tj can differ in size, which may imply dif-
ferent predictive power. To this end, we add an ad-
ditional detail to our model: Each time a reduction
occurs, we label the root of the subtree to which
another subtree has been adjoined, thus making
the conditioning history more specific. We use the
following labelings:
Reduction labeling: if a subtree is adjoint to sub
from the left, then label root(sub) with LR. If it is
adjoint from the right, then label it with RR.
Reduction POS-labeling: same as in simple re-
duction labeling, but add the POS tag of the root
of the reduced subtree to the label.
</bodyText>
<subsectionHeader confidence="0.989741">
4.4 Implementation and training
</subsectionHeader>
<bodyText confidence="0.999743">
To use BiSLM during decoding, one needs access
to phrase-internal alignments and target POS tags.
We store phrase-internal alignments and target-
side POS annotations of each phrase in the phrase
table, based on the most frequent internal align-
ment during training and the most likely target-
side POS labeling tˆ given the phrase pair: tˆ =
arg max¯t p(¯t|¯e, ¯f). We train BiSLMs on the par-
allel training data (Section 5.1) and use the Stan-
ford dependency parser (Chang et al., 2009) for
Chinese and and the Stanford constituency parser
(Green and Manning, 2010) for Arabic7. POS-
tagging of the training data is produced with the
Stanford POS-tagger (Toutanova et al., 2003). We
learn a 5-gram model using SRILM (Stolcke et al.,
2011) with modified Kneser-Ney smoothing.
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999648">
To evaluate the effectiveness of BiSLMs for PB-
SMT, we performed rescoring experiments for
</bodyText>
<footnote confidence="0.950648">
7We extract dependency parses from its output based on
Collins (1999)
</footnote>
<bodyText confidence="0.9944976">
Arabic-English and Chinese-English. We com-
pare the resulting 1-best translation lists with an
output of the baseline system and the baseline aug-
mented with soft cohesion constraints from Bach
et al. (2009).
</bodyText>
<table confidence="0.990274333333333">
System MT06 MT08 MT06+MT08
baseline 32.60 25.94 29.56
cohesion 32.52 25.98 29.54
</table>
<tableCaption confidence="0.981593666666667">
Table 1: Chinese-English baseline and compari-
son model (Cherry, 2008; Bach et al., 2009) re-
sults.
</tableCaption>
<table confidence="0.999332666666667">
System MT08 MT09 MT08+MT09
baseline 45.84 48.61 47.18
cohesion constr. 45.61 48.49 47.02
</table>
<tableCaption confidence="0.9853155">
Table 2: Arabic-English baseline and comparison
model (Cherry, 2008; Bach et al., 2009) results.
</tableCaption>
<subsectionHeader confidence="0.939563">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999868272727273">
This section provides information about our base-
line system. Word-alignment is produced with
GIZA++ (Och and Ney, 2003). We use an in-
house implementation of a PBSMT system similar
to Moses (Koehn et al., 2007). Our baseline has
all standard PBSMT features including language
model, lexical weighting, and lexicalized reorder-
ing. The distortion limit is set to 5. A 5-gram LM
is trained on the English Gigaword corpus (1.6B
tokens) using SRILM with modified Kneser-Ney
smoothing and linear interpolation. Information
about the training data for the Arabic-English and
Chinese-English systems is in Table 3.8 Feature
weights are tuned using pairwise ranking opti-
mization (Hopkins and May, 2011) on the MT04
benchmark (for both language pairs). For testing,
we use MT08 and MT09 for Arabic, and MT06
and MT08 for Chinese. We use case-insensitive
BLEU (Papineni et al., 2002) as evaluation met-
ric. Approximate randomization (Noreen, 1989;
Riezler and Maxwell, 2005) is used to detect sta-
tistically significant differences.
</bodyText>
<subsectionHeader confidence="0.996442">
5.2 Baseline and comparison systems
</subsectionHeader>
<bodyText confidence="0.999888">
As a comparison model, we implemented six fea-
tures from Cherry (2008) and Bach et al. (2009)9
and added them to the log-linear interpolation used
</bodyText>
<footnote confidence="0.99512175">
8The standard LDC corpora were used for training.
9Exhaustive and non-exhaustive interruption check, ex-
haustive and non-exhaustive interruption count, verb- and
noun-dominated subtree interruption count.
</footnote>
<page confidence="0.988335">
2404
</page>
<note confidence="0.5940566">
Training set N. of lines N. of tokens
Source side of Ar-En set 4,376,320 148M
Target side of Ar-En set 4,376,320 146M
Source side of Ch-En set 2,104,652 20M
Target side of Ch-En set 2,104,652 28M
</note>
<tableCaption confidence="0.985913">
Table 3: Training data for Arabic-English and
Chinese-English experiments.
</tableCaption>
<bodyText confidence="0.993802714285714">
by the baseline system. Since these features are bi-
nary or count-based, we cannot use them directly
in rescoring. For that reason we integrated the fea-
tures into the decoder and tuned the correspond-
ing weights. The results for Chinese-English and
Arabic-English translation experiments are pre-
sented in Table 1 and 2, respectively. We see that
adding the cohesion constraints does not improve
performance. This finding is different from, for
example, Feng et al. (2010), where they get im-
provement for Chinese-English: however, we note
that their training set is smaller than ours, and their
baseline is weaker as it does not contain lexical-
ized distortion models.
</bodyText>
<subsectionHeader confidence="0.999547">
5.3 Rescoring experiments
</subsectionHeader>
<bodyText confidence="0.995886166666667">
Rescoring with BiSLMs is performed as follows:
For the test runs of the baseline system we com-
pute the n = 1000 best translation hypotheses
for each source sentence and extract their deriva-
tions (sequence of phrase pair applications). Each
phrase pair in our implementation is associated
with a unique phrase-internal alignment and tar-
get POS-sequence. We fully reconstruct word-
alignment for each pair of a source sentence and
its translation hypothesis. We project a precom-
puted source parse onto the target side and com-
pute representations of the target sentence to be
computed by a BiSLM. For each hypothesis, we
take its BiSLM score and its score assigned by
the baseline system and compute the final score
as a weighted sum of the original baseline score
and a length-normalized BiSLM score10, where
the weight A is empirically set to 0.3:
</bodyText>
<figure confidence="0.459084666666667">
scoreBiSLM
A ·len th + (1 − A) · scoreBaseline (3)
8 Hypothesis
</figure>
<subsectionHeader confidence="0.936597">
5.3.1 Chinese-English
</subsectionHeader>
<bodyText confidence="0.999377666666667">
Our main focus here is Chinese-English, since it
has more instances of longer-distance reordering,
at which syntax-based models are typically good.
</bodyText>
<footnote confidence="0.923648333333333">
10Normalization is needed to ensure comparability of
scores for translation hypotheses of different lengths, since
longer translation hypotheses will have lower scores.
</footnote>
<table confidence="0.999679357142857">
labeling complete unalign BLEU diff.
-adjoin
plain strong + 30.09N +0.53
- 30.20N +0.64
weak + 30.11N +0.55
- 30.22N +0.66
reduce strong + 29.94M +0.40
- 30.19N +0.63
weak + 30.09N +0.53
- 30.24N +0.68
reduce-POS strong + 30.09N +0.53
- 30.25N +0.69
weak + 30.05N +0.49
- 30.25N +0.69
</table>
<tableCaption confidence="0.989904">
Table 4: Rescoring experiments for Chinese
MT06+08 1000-best translation sets. Unrescored
</tableCaption>
<bodyText confidence="0.985223324324324">
BLEU is 29.56. The column labeling contains in-
formation about the kind of labeling used on the
target side of a BiSLM: just target words, target
words with a reduction label, or target words with
a reduction label and a POS of the root of the re-
duced subtree (Section 4.3). The column com-
plete indicates whether we use a strong or weak
definition of a complete subtree (Section 4.2). The
column unalign-adjoin indicates whether we ad-
join an unaligned target word to the preceding
subtree (Section 4.1). Statistically significant im-
provements over the baseline are marked • at the
p &lt; .01 level and ° at the p &lt; .05 level. • marks
significant decrease at the p &lt; .01 level.
SLMs by design are good at capturing longer-
distance dependencies. We try out several varia-
tions of BiSLM. First, we test whether to use a
strong or weak definition of a complete subtree
(Section 4.2). Second, we investigate whether to
adjoin unaligned target words to a preceding head
(Section 4.1; unalign-adjoin+/-). Third, we com-
pare several target-side labeling methods (Sec-
tion 4.3): plain (just target words), reduce (LR or
RR) or reduce-POS (LR POS or RR POS, where
POS is the tag of the root of the reduced subtree).
The rescoring results are presented in Table 4.
The results show statistically significant im-
provement over the baseline of up to 0.7 BLEU
(for all of the employed BiSLM variants except
one). The rescoring experiments also demonstrate
the tendency of the unalign-adjoin- feature value
to produce higher scores than unalign-adjoin+.
But the other two distinguishing features do not
have an effect on BLEU scores. As future work,
we are interested in examining if these features
produce the same distribution of scores when a
BiSLM is fully integrated into the decoder.
</bodyText>
<page confidence="0.964745">
2405
</page>
<table confidence="0.999785428571429">
labeling complete unalign BLEU diff.
-adjoin
plain strong + 47.20 -0.02
- 47.00H -0.18
weak + 47.22 +0.04
- 46.98H -0.20
reduce strong + 47.15 -0.03
- 46.99H -0.19
weak + 47.09 -0.09
- 46.98H -0.20
reduce-POS strong + 47.15 -0.03
- 46.98H -0.20
weak + 47.17 +0.01
- 47.00H -0.18
</table>
<tableCaption confidence="0.997405666666667">
Table 5: Rescoring experiments for Arabic
MT08+09 n-best translation sets. Unrescored
BLEU for is 47.18. For notation see Table 4.
</tableCaption>
<subsectionHeader confidence="0.965864">
5.3.2 Arabic-English
</subsectionHeader>
<bodyText confidence="0.999977114285714">
We also rescore the n-best lists for the output of
the Arabic-English baseline system and results are
shown in Table 5. Arabic and English are typolog-
ically very different, but the range of reordering is
much smaller than for Chinese-English. We ex-
pect reordering-related models to have lesser ef-
fect on Arabic as compared to Chinese (Carpuat
et al., 2010). Experimental results on Arabic-
English could indicate what kind of translation
aspect benefits from BiSLMs. We see that for
Arabic-English, just as for the cohesion constraint,
BiSLM have little effect on BLEU scores, or
even decrease them. This is a weak indication
that BiSLMs are better at capturing reordering as-
pects. As for the varying features defining dif-
ferent BiSLM versions, we again see little effect
of the labeling type or subtree completeness def-
inition. On the other hand, we see the oppo-
site pattern for the unalign-adjoin feature, where
unalign-adjoin+ is preferred.
To gain further insight into the different effect
of BiSLM on the two language pairs, we evalu-
ated our experimental output against a reordering-
sensitive metric LRscore (Birch et al., 2010). We
use the version of LRscore which is an average of
the inverse Kendall’s Tau distance and the Ham-
ming distance. In order to compute alignments for
test sets which are needed to compute the score we
concatenated the parallel text with an additional
250K lines of parallel text from the training data to
ensure better generalization of the alignment algo-
rithm (GIZA++). The LRscores of the baseline are
compared to the best performing BiSLM system
with respect to BLEU, for each of the language
pair. The results are provided in Tables 6 and 7.
</bodyText>
<table confidence="0.411648666666667">
system LRscore MT06+08
baseline
BiSLM
</table>
<tableCaption confidence="0.403144">
Table 6: LRscores (average inverse Kendall’s
Tau distance and Hamming distance) for Chinese-
English baseline and BiSLM with reduce-labeling,
weak completeness, unalign-adjoin-.
</tableCaption>
<table confidence="0.527089666666667">
system LRscore MT08+09
baseline
BiSLM
</table>
<tableCaption confidence="0.686809666666667">
Table 7: LRscores for Arabic-English baseline and
BiSLM with plain-labeling, weak completeness,
unalign-adjoin+.
</tableCaption>
<bodyText confidence="0.999969">
As expected, the scores for Chinese-English are
much lower than for Arabic-English, which is con-
sistent with the observation reordering is more dif-
ficult for Chinese-English. BiSLM yields larger
improvements for Chinese-English suggesting that
the proposed model helps addressing difficult re-
ordering problems. While there are also small im-
provements for Arabic-English the they may be
too small to be detectable by BLEU.
</bodyText>
<sectionHeader confidence="0.999538" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999582666666667">
In this paper we proposed a novel way to adapt
structured language models to phrase-based SMT.
Our method requires minimal changes to the PB-
SMT pipeline. We tried a number of variations
of our model and evaluated them in rescoring ex-
periments, resulting in statistically significant im-
provement for Chinese-English. The model is
based on the idea of syntactic transfer (DCA; Sec-
tion 2) and the positive result indicates its ability
to capture syntactic patterns across languages. For
Arabic-English, we did not observe any improve-
ments, suggesting that our models indeed mainly
improve reordering aspects. Improvements in
rescoring are a positive indication that our model
may be a strong feature during decoding. As fu-
ture work, we will fully integrate our model into a
PBSMT decoder and evaluate it on other language
pairs with different reordering distributions.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9960474">
We thank the reviewers for their useful com-
ments. This research was funded in part by the
Netherlands Organization for Scientific Research
(NWO) under project numbers 639.022.213 and
612.001.218.
</bodyText>
<figure confidence="0.9519005">
0.4736
0.4907
0.6671
0.6719
</figure>
<page confidence="0.985991">
2406
</page>
<sectionHeader confidence="0.988657" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999549290909091">
Nguyen Bach, Stephan Vogel, and Colin Cherry. 2009.
Cohesive constraints in a beam search phrase-based
decoder. In Proceedings of the 2009 Annual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 1–4.
Association for Computational Linguistics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for mt evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15–26.
Marine Carpuat, Yuval Marton, and Nizar Habash.
2010. Improving Arabic-to-English statistical ma-
chine translation by reordering post-verbal subjects
for alignment. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 178–183. Association
for Computational Linguistics.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51–59. Association for Computational Linguistics.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14(4):283–332.
Colin Cherry. 2008. Cohesive phrase-based decoding
for statistical machine translation. In Proceedings
of Association for Computational Linguistics, pages
72–80.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Josep M. Crego and Franc¸ois Yvon. 2010. Improv-
ing reordering with linguistically informed bilin-
gual n-grams. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 197–205. Association for Computational Lin-
guistics.
Minwei Feng, Arne Mauser, and Hermann Ney. 2010.
A source-side decoding sequence model for statis-
tical machine translation. In Conference of the As-
sociation for Machine Translation in the Americas,
Denver, Colorado, USA.
Heidi J. Fox. 2002. Phrasal cohesion and statisti-
cal machine translation. In Proceedings the Con-
ference on Empirical Methods in Natural Language
Processing, pages 304–311. Association for Com-
putational Linguistics.
Ekaterina Garmash and Christof Monz. 2014.
Dependency-based bilingual language models for
reordering in statistical machine translation. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 1689–1700, Doha, Qatar, October. Associa-
tion for Computational Linguistics.
Niyu Ge. 2010. A direct syntax-driven reordering
model for phrase-based machine translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
849–857. Association for Computational Linguis-
tics.
Spence Green and Christopher D. Manning. 2010.
Better arabic parsing: Baselines, evaluations, and
analysis. In Proceedings of the 23rd International
Conference on Computational Linguistics, pages
394–402. Association for Computational Linguis-
tics.
Jiri Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 608–615. Association for Com-
putational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1352–1362. Association for Computational
Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 392–399. Associ-
ation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48–54. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics on Interactive
Poster and Demonstration Sessions, pages 177–180.
Association for Computational Linguistics.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 507–514, Sydney, Australia,
July. Association for Computational Linguistics.
Uri Lerner and Slav Petrov. 2013. Source-side classi-
fier preordering for machine translation. In Proceed-
ings of the Empirical Methods in Natural Language
Processing.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
</reference>
<page confidence="0.757269">
2407
</page>
<reference confidence="0.999714411764706">
parsing. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 629–637. Asso-
ciation for Computational Linguistics.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider context by using bilin-
gual language models in machine translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 198–206. Association
for Computational Linguistics.
Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Testing Hypotheses. An Introduction. Wiley-
Interscience.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Timothy Osborne. 2008. Major constituents and two
dependency grammar constraints on sharing in coor-
dination. Linguistics, 46(6):1109–1165.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual meeting of the Association for Compu-
tational Linguistics, pages 311–318. Association for
Computational Linguistics.
Matt Post and Daniel Gildea. 2008. Parsers as lan-
guage models for statistical machine translation. In
Proceedings of the Eighth Conference of the Asso-
ciation for Machine Translation in the Americas,
pages 172–181. Citeseer.
Chris Quirk and Arul Menezes. 2006. Dependency
treelet translation: The convergence of statistical
and example-based machine translation? Machine
Translation, 20:43–65, March.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization.
Rico Sennrich. 2015. Modelling and optimizing on
syntactic n-grams for statistical machine translation.
Transactions of the Association for Computational
Linguistics, 3:169–182.
Libin Shen, Jinxi Xu, and Ralph M. Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of the Association for Computational
Linguistics, pages 577–585.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and out-
look. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop, page 5.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 173–180. Association for Computational Lin-
guistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Bing Xiang, Niyu Ge, and Abraham Ittycheriah. 2011.
Improving reordering for statistical machine trans-
lation with smoothed priors and syntactic features.
In Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
pages 61–69. Association for Computational Lin-
guistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics, pages 523–530. Association
for Computational Linguistics.
Heng Yu, Haitao Mi, Liang Huang, and Qun Liu. 2014.
A structured language model for incremental tree-
to-string translation. Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics, pages 1133–1143.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138–141. Association for
Computational Linguistics.
</reference>
<page confidence="0.991335">
2408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.117040">
<title confidence="0.990979">Bilingual Structured Language Models for Statistical Machine Translation</title>
<author confidence="0.501239">Garmash</author>
<affiliation confidence="0.577081">Informatics Institute, University of</affiliation>
<address confidence="0.353408">Science Park 904, 1098 XH Amsterdam, The</address>
<abstract confidence="0.998195555555556">This paper describes a novel target-side syntactic language model for phrase-based statistical machine translation, bilingual structured language model. Our approach represents a new way to adapt structured language models (Chelba and Jelinek, 2000) to statistical machine translation, and a first attempt to adapt them to phrasebased statistical machine translation. We propose a number of variations of the bilingual structured language model and evaluate them in a series of rescoring experiments. Rescoring of 1000-best translation lists produces statistically significant improvements of up to 0.7 BLEU over a strong baseline for Chinese-English, but does not yield improvements for Arabic-</abstract>
<intro confidence="0.437237">English.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Stephan Vogel</author>
<author>Colin Cherry</author>
</authors>
<title>Cohesive constraints in a beam search phrase-based decoder.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1--4</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9867" citStr="Bach et al. (2009)" startWordPosition="1628" endWordPosition="1631"> by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS, a translation W is cohesive if all translated target words wZ, wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wZ and wj but not by wk (Figure 2). Cherry (2008) and Bach et al. (2009) define a set of soft constraints based on the syntactic cohesion assumption which are applicable to PBSMT decoding. They only require phrase applications, and not necessarily individual target words, to conform to the cohesion principle. For example, if we imagine a situation where a subtree as in Figure 2(b) is translated as a whole with one phrase application (and not word by word), then it does not violate the cohesion principle, although it is internally uncohesive. Both our approach and Cherry (2008) implement the idea of conforming the target translation to the source syntactic structur</context>
<context position="27306" citStr="Bach et al. (2009)" startWordPosition="4653" endWordPosition="4656">arser (Green and Manning, 2010) for Arabic7. POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System MT06 MT08 MT06+MT08 baseline 32.60 25.94 29.56 cohesion 32.52 25.98 29.54 Table 1: Chinese-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. System MT08 MT09 MT08+MT09 baseline 45.84 48.61 47.18 cohesion constr. 45.61 48.49 47.02 Table 2: Arabic-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. 5.1 Experimental setup This section provides information about our baseline system. Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007).</context>
<context position="28843" citStr="Bach et al. (2009)" startWordPosition="4891" endWordPosition="4894">ining data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8The standard LDC corpora were used for training. 9Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count. 2404 Training set N. of lines N. of tokens Source side of Ar-En set 4,376,320 148M Target side of Ar-En set 4,376,320 146M Source side of Ch-En set 2,104,652 20M Target side of Ch-En set 2,104,652 28M Table 3: Training data for Arabic-English and Chinese-English experiments. by the baseline system. Since these features are binary or count-b</context>
</contexts>
<marker>Bach, Vogel, Cherry, 2009</marker>
<rawString>Nguyen Bach, Stephan Vogel, and Colin Cherry. 2009. Cohesive constraints in a beam search phrase-based decoder. In Proceedings of the 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1–4. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Phil Blunsom</author>
</authors>
<title>Metrics for mt evaluation: evaluating reordering.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="35023" citStr="Birch et al., 2010" startWordPosition="5900" endWordPosition="5903">e cohesion constraint, BiSLM have little effect on BLEU scores, or even decrease them. This is a weak indication that BiSLMs are better at capturing reordering aspects. As for the varying features defining different BiSLM versions, we again see little effect of the labeling type or subtree completeness definition. On the other hand, we see the opposite pattern for the unalign-adjoin feature, where unalign-adjoin+ is preferred. To gain further insight into the different effect of BiSLM on the two language pairs, we evaluated our experimental output against a reorderingsensitive metric LRscore (Birch et al., 2010). We use the version of LRscore which is an average of the inverse Kendall’s Tau distance and the Hamming distance. In order to compute alignments for test sets which are needed to compute the score we concatenated the parallel text with an additional 250K lines of parallel text from the training data to ensure better generalization of the alignment algorithm (GIZA++). The LRscores of the baseline are compared to the best performing BiSLM system with respect to BLEU, for each of the language pair. The results are provided in Tables 6 and 7. system LRscore MT06+08 baseline BiSLM Table 6: LRscor</context>
</contexts>
<marker>Birch, Osborne, Blunsom, 2010</marker>
<rawString>Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for mt evaluation: evaluating reordering. Machine Translation, 24(1):15–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
</authors>
<title>Improving Arabic-to-English statistical machine translation by reordering post-verbal subjects for alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>178--183</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1827" citStr="Carpuat et al. (2010)" startWordPosition="253" endWordPosition="256">anguage models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community</context>
<context position="34249" citStr="Carpuat et al., 2010" startWordPosition="5776" endWordPosition="5779">ak + 47.09 -0.09 - 46.98H -0.20 reduce-POS strong + 47.15 -0.03 - 46.98H -0.20 weak + 47.17 +0.01 - 47.00H -0.18 Table 5: Rescoring experiments for Arabic MT08+09 n-best translation sets. Unrescored BLEU for is 47.18. For notation see Table 4. 5.3.2 Arabic-English We also rescore the n-best lists for the output of the Arabic-English baseline system and results are shown in Table 5. Arabic and English are typologically very different, but the range of reordering is much smaller than for Chinese-English. We expect reordering-related models to have lesser effect on Arabic as compared to Chinese (Carpuat et al., 2010). Experimental results on ArabicEnglish could indicate what kind of translation aspect benefits from BiSLMs. We see that for Arabic-English, just as for the cohesion constraint, BiSLM have little effect on BLEU scores, or even decrease them. This is a weak indication that BiSLMs are better at capturing reordering aspects. As for the varying features defining different BiSLM versions, we again see little effect of the labeling type or subtree completeness definition. On the other hand, we see the opposite pattern for the unalign-adjoin feature, where unalign-adjoin+ is preferred. To gain furthe</context>
</contexts>
<marker>Carpuat, Marton, Habash, 2010</marker>
<rawString>Marine Carpuat, Yuval Marton, and Nizar Habash. 2010. Improving Arabic-to-English statistical machine translation by reordering post-verbal subjects for alignment. In Proceedings of the ACL 2010 Conference Short Papers, pages 178–183. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Huihsin Tseng</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Discriminative reordering with chinese grammatical relations features.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>51--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26640" citStr="Chang et al., 2009" startWordPosition="4549" endWordPosition="4552">ame as in simple reduction labeling, but add the POS tag of the root of the reduced subtree to the label. 4.4 Implementation and training To use BiSLM during decoding, one needs access to phrase-internal alignments and target POS tags. We store phrase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling tˆ given the phrase pair: tˆ = arg max¯t p(¯t|¯e, ¯f). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7. POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baselin</context>
</contexts>
<marker>Chang, Tseng, Jurafsky, Manning, 2009</marker>
<rawString>Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning. 2009. Discriminative reordering with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation, pages 51–59. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="4540" citStr="Chelba and Jelinek (2000)" startWordPosition="717" endWordPosition="720">ng. A target side string can be encoded with source-syntactic building blocks and then scored as to how well-formed it is. Crego and Yvon (2010), Niehues et al. (2011), Garmash and Monz (2014) model target sequences as strings of tokens built from the target POS tag and the POS tags of the source words related to it through alignment and the source parse. In this paper, we define a target-side syntactic language model that takes structural constraints from the source sentence, but uses the words from the target side (as ‘building blocks’). We do it by adapting an existing monolingual model of Chelba and Jelinek (2000), structured language models, to the bilingual setting. Our contributions can be summarized as follows: • we propose a novel method to adapt monolingual structured language models (Chelba and Jelinek, 2000) (Section 3) to a PBSMT system (Section 4), which does not require an external on-the-fly parser, but only uses the given source-side syntactic analysis to infer structural relations between target words; • building on the existing literature, we propose a set of deterministic rules that incrementally build up a parse of a target translation hypothesis based on the source parse (Section 4); </context>
<context position="11055" citStr="Chelba and Jelinek, 2000" startWordPosition="1831" endWordPosition="1834">tion to the source syntactic structure, but in different ways. Approaches like Cherry (2008) define principles that constrain the decoder in order to produce better translations. Our goal is to have a model that allows for a more direct way of evaluation of how well-formed the target translation is. In Section 5 we compare translation performance of the two approaches. 3 Structured language models As discussed in Sections 1 and 2, we would like to test how much a PBSMT can benefit from an additional syntax-based LM. In this section, we describe a syntactic language model, structured LM (SLM) (Chelba and Jelinek, 2000), that we extend to a bilingual setting and apply to SMT in Section 4. SLMs have been applied in SMT before (Yamada and Knight, 2001; Yu et al., 2014), but as we show in Section 4, we provide a much simpler method to integrate it into the system. While a SLM is not the only syntactically defined LM, it is one of the few that models sentence generation sequentially. And due to the way the decoding procedure of PBSMT is defined, it is natural and straightforward to use models whose score can be computed sequentially. Other syntactic language models define sentence generation hierarchically (Shen</context>
<context position="14516" citStr="Chelba and Jelinek (2000)" startWordPosition="2446" endWordPosition="2449">ect to met (Figure 3(b)). The words the and former are modifiers of president and they get filtered out. Thus we obtain a less specific conditioning history, which may lead to the resulting model being less sparse. Another potential benefit is that SLMs can capture longdistance reordering: If president had as its modifier a relative clause (Figure 3(c)) then a simple n-gram LM would be conditioned on days before (assuming n = 3), while an SLM would condition met on yesterday president. Summarizing the ideas of words being conditioned on a structurally defined subset of the preceding sentence, Chelba and Jelinek (2000) formalize the generation process of W as follows:2 Each new word wi is conditioned on a 2The original model by (Chelba and Jelinek, 2000) is defined in terms of a lexicalized constituency grammar, but as · p(ti|wi, Expos(Wi−1, Di−1)) · p(Di|wi, ti, Expos(Wi−1, Di−1)). (1) They use a shift-reduce parser with reduce-left, reduce-right, and shift operations. 4 Bilingual structured language models In this section, we combine the direct correspondence assumption (Section 2) and SLMs (Section 3), and define bilingual structured language models (BiSLMs) for PBSMT. Structured LMs have been successful</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
</authors>
<title>Cohesive phrase-based decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of Association for Computational Linguistics,</booktitle>
<pages>72--80</pages>
<contexts>
<context position="1804" citStr="Cherry (2008)" startWordPosition="251" endWordPosition="252">g, and n-gram language models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the</context>
<context position="9502" citStr="Cherry, 2008" startWordPosition="1558" endWordPosition="1559"> define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS, a translation W is cohesive if all translated target words wZ, wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wZ and wj but not by wk (Figure 2). Cherry (2008) and Bach et al. (2009) define a set of soft constraints based on the syntactic cohesion assumption which are applicable to PBSMT decoding. They only require phrase applications, and not necessarily individual target words, to conform to the cohesion princip</context>
<context position="24928" citStr="Cherry (2008)" startWordPosition="4255" endWordPosition="4256">re both complete and DT(root(subi), root(subi−1)), adjoin subi−1 to subi so that root(subi−1) is a modifier of root(subi). Right-reduce: analogous to left-reduce, but DT(root(subi−1), root(subi)). In the case of non-cohesive translation the resulting target dependencies are non-projective. Our definition of left- and right-reduce only produces projective parses. For a non-cohesive translation, certain subtrees will never be sourcecomplete and will never be reduced; see Figure 6(a). Note that this is not a disadvantage (e) (f) (g) f0 f1 f2 e0 e1 f0 f1 e0 e1 e1 f0 e0 f1 f2 e1 2403 of our model. Cherry (2008) simply assumes that non-cohesive reordering should be penalized, and our model is able to learn this pattern. We also consider an alternative to incorporating noncohesive alignments by relaxing the definition of completeness for subtrees: A projected subtree sub is weakly source-complete if all descendants of all source word(s) which are aligned to the root of sub have been translated and, only if the definition of reduce applies, reduced; see Figure 6(b). 4.3 Syntactic labeling of tokens One of the problems with SLMs in general is that at time steps i and j the sets of exposed heads for ti a</context>
<context position="27457" citStr="Cherry, 2008" startWordPosition="4677" endWordPosition="4678">gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System MT06 MT08 MT06+MT08 baseline 32.60 25.94 29.56 cohesion 32.52 25.98 29.54 Table 1: Chinese-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. System MT08 MT09 MT08+MT09 baseline 45.84 48.61 47.18 cohesion constr. 45.61 48.49 47.02 Table 2: Arabic-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. 5.1 Experimental setup This section provides information about our baseline system. Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5</context>
<context position="28820" citStr="Cherry (2008)" startWordPosition="4888" endWordPosition="4889">tion about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8The standard LDC corpora were used for training. 9Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count. 2404 Training set N. of lines N. of tokens Source side of Ar-En set 4,376,320 148M Target side of Ar-En set 4,376,320 146M Source side of Ch-En set 2,104,652 20M Target side of Ch-En set 2,104,652 28M Table 3: Training data for Arabic-English and Chinese-English experiments. by the baseline system. Since these feature</context>
</contexts>
<marker>Cherry, 2008</marker>
<rawString>Colin Cherry. 2008. Cohesive phrase-based decoding for statistical machine translation. In Proceedings of Association for Computational Linguistics, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="27100" citStr="Collins (1999)" startWordPosition="4623" endWordPosition="4624">pair: tˆ = arg max¯t p(¯t|¯e, ¯f). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7. POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System MT06 MT08 MT06+MT08 baseline 32.60 25.94 29.56 cohesion 32.52 25.98 29.54 Table 1: Chinese-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. System MT08 MT09 MT08+MT09 baseline 45.84 48.61 47.18 cohesion constr. 45.61 48.49 47.02 Table 2: Arabic-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. 5.1 Experimental setup This</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Improving reordering with linguistically informed bilingual n-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>197--205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1850" citStr="Crego and Yvon (2010)" startWordPosition="257" endWordPosition="260">, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures compara</context>
<context position="4059" citStr="Crego and Yvon (2010)" startWordPosition="635" endWordPosition="638">d chooses the most likely parse for it. What we are interested in during translation is how gram2398 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 0 1 2 0 1 2 0 1 2 3 0 1 2 3 (a) (b) (c) (d) matical the target sentence actually is. In addition to reordering constraints, source syntax can be used for target-side language modeling. A target side string can be encoded with source-syntactic building blocks and then scored as to how well-formed it is. Crego and Yvon (2010), Niehues et al. (2011), Garmash and Monz (2014) model target sequences as strings of tokens built from the target POS tag and the POS tags of the source words related to it through alignment and the source parse. In this paper, we define a target-side syntactic language model that takes structural constraints from the source sentence, but uses the words from the target side (as ‘building blocks’). We do it by adapting an existing monolingual model of Chelba and Jelinek (2000), structured language models, to the bilingual setting. Our contributions can be summarized as follows: • we propose a </context>
</contexts>
<marker>Crego, Yvon, 2010</marker>
<rawString>Josep M. Crego and Franc¸ois Yvon. 2010. Improving reordering with linguistically informed bilingual n-grams. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 197–205. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>A source-side decoding sequence model for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="29846" citStr="Feng et al. (2010)" startWordPosition="5045" endWordPosition="5048">ce side of Ch-En set 2,104,652 20M Target side of Ch-En set 2,104,652 28M Table 3: Training data for Arabic-English and Chinese-English experiments. by the baseline system. Since these features are binary or count-based, we cannot use them directly in rescoring. For that reason we integrated the features into the decoder and tuned the corresponding weights. The results for Chinese-English and Arabic-English translation experiments are presented in Table 1 and 2, respectively. We see that adding the cohesion constraints does not improve performance. This finding is different from, for example, Feng et al. (2010), where they get improvement for Chinese-English: however, we note that their training set is smaller than ours, and their baseline is weaker as it does not contain lexicalized distortion models. 5.3 Rescoring experiments Rescoring with BiSLMs is performed as follows: For the test runs of the baseline system we compute the n = 1000 best translation hypotheses for each source sentence and extract their derivations (sequence of phrase pair applications). Each phrase pair in our implementation is associated with a unique phrase-internal alignment and target POS-sequence. We fully reconstruct word</context>
</contexts>
<marker>Feng, Mauser, Ney, 2010</marker>
<rawString>Minwei Feng, Arne Mauser, and Hermann Ney. 2010. A source-side decoding sequence model for statistical machine translation. In Conference of the Association for Machine Translation in the Americas, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>304--311</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9487" citStr="Fox, 2002" startWordPosition="1556" endWordPosition="1557">ansfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS, a translation W is cohesive if all translated target words wZ, wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wZ and wj but not by wk (Figure 2). Cherry (2008) and Bach et al. (2009) define a set of soft constraints based on the syntactic cohesion assumption which are applicable to PBSMT decoding. They only require phrase applications, and not necessarily individual target words, to conform to the c</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proceedings the Conference on Empirical Methods in Natural Language Processing, pages 304–311. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Garmash</author>
<author>Christof Monz</author>
</authors>
<title>Dependency-based bilingual language models for reordering in statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1689--1700</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="1933" citStr="Garmash and Monz (2014)" startWordPosition="271" endWordPosition="274">guistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other people’s work on the topic. There is a variety of wa</context>
<context position="4107" citStr="Garmash and Monz (2014)" startWordPosition="643" endWordPosition="646">e are interested in during translation is how gram2398 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 0 1 2 0 1 2 0 1 2 3 0 1 2 3 (a) (b) (c) (d) matical the target sentence actually is. In addition to reordering constraints, source syntax can be used for target-side language modeling. A target side string can be encoded with source-syntactic building blocks and then scored as to how well-formed it is. Crego and Yvon (2010), Niehues et al. (2011), Garmash and Monz (2014) model target sequences as strings of tokens built from the target POS tag and the POS tags of the source words related to it through alignment and the source parse. In this paper, we define a target-side syntactic language model that takes structural constraints from the source sentence, but uses the words from the target side (as ‘building blocks’). We do it by adapting an existing monolingual model of Chelba and Jelinek (2000), structured language models, to the bilingual setting. Our contributions can be summarized as follows: • we propose a novel method to adapt monolingual structured lan</context>
</contexts>
<marker>Garmash, Monz, 2014</marker>
<rawString>Ekaterina Garmash and Christof Monz. 2014. Dependency-based bilingual language models for reordering in statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1689–1700, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
</authors>
<title>A direct syntax-driven reordering model for phrase-based machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>849--857</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1861" citStr="Ge (2010)" startWordPosition="261" endWordPosition="262"> shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of o</context>
</contexts>
<marker>Ge, 2010</marker>
<rawString>Niyu Ge. 2010. A direct syntax-driven reordering model for phrase-based machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 849–857. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Christopher D Manning</author>
</authors>
<title>Better arabic parsing: Baselines, evaluations, and analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>394--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26719" citStr="Green and Manning, 2010" startWordPosition="4561" endWordPosition="4564">e reduced subtree to the label. 4.4 Implementation and training To use BiSLM during decoding, one needs access to phrase-internal alignments and target POS tags. We store phrase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling tˆ given the phrase pair: tˆ = arg max¯t p(¯t|¯e, ¯f). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7. POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System MT06</context>
</contexts>
<marker>Green, Manning, 2010</marker>
<rawString>Spence Green and Christopher D. Manning. 2010. Better arabic parsing: Baselines, evaluations, and analysis. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 394–402. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Havelka</author>
</authors>
<title>Beyond projectivity: Multilingual evaluation of constraints and measures on nonprojective structures.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>608--615</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6952" citStr="Havelka, 2007" startWordPosition="1132" endWordPosition="1133">ther of which is its ancestor. is more primitive.1 A dependency parse D is a dependency tree analysis of a sentence W, and we will think of it as a relation between words of W, such that D(w, v) if w is a parent (head) of v (v being a child/modifier). D can be generalized to D* which is an relation between words that are connected by a continuous path in a dependency tree (i.e. D*(w, v) if D(w, v) or if lu s.t. D(w, u) ∧ D*(u, v)). We assume unlabeled dependency trees. Finally, we make a projectivity assumption, which is supported by empirical data in many languages (Kuhlmann and Nivre, 2006; Havelka, 2007), and makes a model computationally less expensive. A dependency parse D of a sentence W = wi, ... , wn is projective, if for every word pair wi, wj E W s.t. D(wi, wj) it holds that every wk E W s.t. i &lt; k &lt; j or j &lt; k &lt; i is a descendant of wi, i.e., D*(wi, wk); see Figure 1. Most NLP models that address the interaction of two or more languages are based (explicitly or implicitly) on the direct correspondence assumption (DCA) (Hwa et al., 2002). It states that close translation equivalents in different languages have the same dependency structure. This is grounded linguistically, as translati</context>
</contexts>
<marker>Havelka, 2007</marker>
<rawString>Jiri Havelka. 2007. Beyond projectivity: Multilingual evaluation of constraints and measures on nonprojective structures. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 608–615. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1352--1362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28388" citStr="Hopkins and May, 2011" startWordPosition="4818" endWordPosition="4821">nment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8The standard LDC corpora were used for training. 9Exhaustive and non-exhaustive interrupti</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1352–1362. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Okan Kolak</author>
</authors>
<title>Evaluating translational correspondence using annotation projection.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>392--399</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7401" citStr="Hwa et al., 2002" startWordPosition="1224" endWordPosition="1227">unlabeled dependency trees. Finally, we make a projectivity assumption, which is supported by empirical data in many languages (Kuhlmann and Nivre, 2006; Havelka, 2007), and makes a model computationally less expensive. A dependency parse D of a sentence W = wi, ... , wn is projective, if for every word pair wi, wj E W s.t. D(wi, wj) it holds that every wk E W s.t. i &lt; k &lt; j or j &lt; k &lt; i is a descendant of wi, i.e., D*(wi, wk); see Figure 1. Most NLP models that address the interaction of two or more languages are based (explicitly or implicitly) on the direct correspondence assumption (DCA) (Hwa et al., 2002). It states that close translation equivalents in different languages have the same dependency structure. This is grounded linguistically, as translation equivalence implies semantic equivalence and therefore thematic relations are preserved (Hwa et al., 2002). Thus dependency relations are preserved, as they are defined based on thematic relations between words. On the other hand, there is plenty empirical evidence supporting the violation of DCA under certain conditions (Hwa et al., 2002). For instance, even semantically very close sentences in different languages may have a different number</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Kolak, 2002</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2002. Evaluating translational correspondence using annotation projection. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 392–399. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2123" citStr="Koehn et al., 2003" startWordPosition="301" endWordPosition="304">coding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other people’s work on the topic. There is a variety of ways syntax can be used in a PBSMT model. Typically a syntactic representation of a source sentence is used to define constraints on the order in which the decoder translates it. For example, </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27905" citStr="Koehn et al., 2007" startWordPosition="4746" endWordPosition="4749">m Bach et al. (2009). System MT06 MT08 MT06+MT08 baseline 32.60 25.94 29.56 cohesion 32.52 25.98 29.54 Table 1: Chinese-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. System MT08 MT09 MT08+MT09 baseline 45.84 48.61 47.18 cohesion constr. 45.61 48.49 47.02 Table 2: Arabic-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. 5.1 Experimental setup This section provides information about our baseline system. Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly non-projective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>507--514</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="6936" citStr="Kuhlmann and Nivre, 2006" startWordPosition="1128" endWordPosition="1131"> its sibling (node 3), neither of which is its ancestor. is more primitive.1 A dependency parse D is a dependency tree analysis of a sentence W, and we will think of it as a relation between words of W, such that D(w, v) if w is a parent (head) of v (v being a child/modifier). D can be generalized to D* which is an relation between words that are connected by a continuous path in a dependency tree (i.e. D*(w, v) if D(w, v) or if lu s.t. D(w, u) ∧ D*(u, v)). We assume unlabeled dependency trees. Finally, we make a projectivity assumption, which is supported by empirical data in many languages (Kuhlmann and Nivre, 2006; Havelka, 2007), and makes a model computationally less expensive. A dependency parse D of a sentence W = wi, ... , wn is projective, if for every word pair wi, wj E W s.t. D(wi, wj) it holds that every wk E W s.t. i &lt; k &lt; j or j &lt; k &lt; i is a descendant of wi, i.e., D*(wi, wk); see Figure 1. Most NLP models that address the interaction of two or more languages are based (explicitly or implicitly) on the direct correspondence assumption (DCA) (Hwa et al., 2002). It states that close translation equivalents in different languages have the same dependency structure. This is grounded linguistical</context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-projective dependency structures. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 507–514, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Lerner</author>
<author>Slav Petrov</author>
</authors>
<title>Source-side classifier preordering for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1908" citStr="Lerner and Petrov (2013)" startWordPosition="267" endWordPosition="270">enefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other people’s work on the topic.</context>
</contexts>
<marker>Lerner, Petrov, 2013</marker>
<rawString>Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In Proceedings of the Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<title>Selective sharing for multilingual dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>629--637</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9047" citStr="Naseem et al., 2012" startWordPosition="1483" endWordPosition="1486">the source subtree {(1, 2)}, but the target word b does not translate this subtree. (c-d): cohesive (c) and uncohesive (d) translations. (d) is uncohesive because a and c translate the source subtree {(0,1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS, a translation W is cohesive if all translated </context>
</contexts>
<marker>Naseem, Barzilay, Globerson, 2012</marker>
<rawString>Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 629–637. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Wider context by using bilingual language models in machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>198--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4082" citStr="Niehues et al. (2011)" startWordPosition="639" endWordPosition="642">ly parse for it. What we are interested in during translation is how gram2398 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 0 1 2 0 1 2 0 1 2 3 0 1 2 3 (a) (b) (c) (d) matical the target sentence actually is. In addition to reordering constraints, source syntax can be used for target-side language modeling. A target side string can be encoded with source-syntactic building blocks and then scored as to how well-formed it is. Crego and Yvon (2010), Niehues et al. (2011), Garmash and Monz (2014) model target sequences as strings of tokens built from the target POS tag and the POS tags of the source words related to it through alignment and the source parse. In this paper, we define a target-side syntactic language model that takes structural constraints from the source sentence, but uses the words from the target side (as ‘building blocks’). We do it by adapting an existing monolingual model of Chelba and Jelinek (2000), structured language models, to the bilingual setting. Our contributions can be summarized as follows: • we propose a novel method to adapt m</context>
</contexts>
<marker>Niehues, Herrmann, Vogel, Waibel, 2011</marker>
<rawString>Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider context by using bilingual language models in machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 198–206. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses. An Introduction.</title>
<date>1989</date>
<publisher>WileyInterscience.</publisher>
<contexts>
<context position="28629" citStr="Noreen, 1989" startWordPosition="4859" endWordPosition="4860">dering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8The standard LDC corpora were used for training. 9Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count. 2404 Training set N. of lines N. of tokens Source side of Ar-En set 4,376,320 148M Target side of Ar-En set 4,376,320 146M Sourc</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses. An Introduction. WileyInterscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="27815" citStr="Och and Ney, 2003" startWordPosition="4729" endWordPosition="4732">tput of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System MT06 MT08 MT06+MT08 baseline 32.60 25.94 29.56 cohesion 32.52 25.98 29.54 Table 1: Chinese-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. System MT08 MT09 MT08+MT09 baseline 45.84 48.61 47.18 cohesion constr. 45.61 48.49 47.02 Table 2: Arabic-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. 5.1 Experimental setup This section provides information about our baseline system. Word-alignment is produced with GIZA++ (Och and Ney, 2003). We use an inhouse implementation of a PBSMT system similar to Moses (Koehn et al., 2007). Our baseline has all standard PBSMT features including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Osborne</author>
</authors>
<title>Major constituents and two dependency grammar constraints on sharing in coordination.</title>
<date>2008</date>
<journal>Linguistics,</journal>
<volume>46</volume>
<issue>6</issue>
<contexts>
<context position="8228" citStr="Osborne, 2008" startWordPosition="1353" endWordPosition="1354">atic relations are preserved (Hwa et al., 2002). Thus dependency relations are preserved, as they are defined based on thematic relations between words. On the other hand, there is plenty empirical evidence supporting the violation of DCA under certain conditions (Hwa et al., 2002). For instance, even semantically very close sentences in different languages may have a different number of 1A dependency parse (a dependency tree analysis of a sentence) is more primitive because every constituency parse can be formalized as a projective dependency parse with labeled relations, but not vice versa (Osborne, 2008). 2399 Figure 2: Examples of cohesive and uncohesive translations. (a-b): cohesive (a) and uncohesive (b) translations of the same dependency parse. (b) is uncohesive because words a and c translate the source subtree {(1, 2)}, but the target word b does not translate this subtree. (c-d): cohesive (c) and uncohesive (d) translations. (d) is uncohesive because a and c translate the source subtree {(0,1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely</context>
</contexts>
<marker>Osborne, 2008</marker>
<rawString>Timothy Osborne. 2008. Major constituents and two dependency grammar constraints on sharing in coordination. Linguistics, 46(6):1109–1165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="28567" citStr="Papineni et al., 2002" startWordPosition="4849" endWordPosition="4852">atures including language model, lexical weighting, and lexicalized reordering. The distortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8The standard LDC corpora were used for training. 9Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count. 2404 Training set N. of lines N. of tokens Source side of Ar-En se</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Parsers as language models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>172--181</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="3167" citStr="Post and Gildea (2008)" startWordPosition="479" endWordPosition="482">n be used in a PBSMT model. Typically a syntactic representation of a source sentence is used to define constraints on the order in which the decoder translates it. For example, Cherry (2008) defines soft constraints based on the notion of syntactic cohesion (Section 2). Ge (2010) captures reordering patterns by defining soft constraints based on the currently translated word’s POS tag and the words structurally related to it. On the other hand, target syntax is more challenging to use in PBSMT, since a target-side syntactic model does not have access to the whole target sentence at decoding. Post and Gildea (2008) is one of the few targetside syntactic approaches applicable to PBSMT, but it has been shown not to improve translation. Their approach uses a target side parser as a language model: one of the reasons why it fails is that a parser assumes its input to be grammatical and chooses the most likely parse for it. What we are interested in during translation is how gram2398 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2398–2408, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 0 1 2 0 1 2 0 1 2 3 0 1 2 3 (a) (</context>
</contexts>
<marker>Post, Gildea, 2008</marker>
<rawString>Matt Post and Daniel Gildea. 2008. Parsers as language models for statistical machine translation. In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas, pages 172–181. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Dependency treelet translation: The convergence of statistical and example-based machine translation? Machine Translation,</title>
<date>2006</date>
<pages>20--43</pages>
<contexts>
<context position="9219" citStr="Quirk and Menezes (2006)" startWordPosition="1512" endWordPosition="1515"> c translate the source subtree {(0,1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS, a translation W is cohesive if all translated target words wZ, wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translated by wZ and wj but not by wk </context>
<context position="19932" citStr="Quirk and Menezes, 2006" startWordPosition="3364" endWordPosition="3367">e definition of DCA can be rephrased as requiring a one-to-one correspondence map between words of a sentence pair, allowing one to unambiguously map dependencies: Given a source parse, if t1 is the head of t2, then map(t1) is the head of map(t2). The correspondence relation that we have in PBSMT is the word alignment align: in the most general case, it is a many-to-many correspondence, and the straightforward projection described above can lead to incorrect dependency structures. To overcome these problems, we describe a simple ordered set of projection rules, based on the ones specified by (Quirk and Menezes, 2006) (and we point out if otherwise). The general idea behind this set of rules is to extract a one-to-one function align1−1 from source words to target words from align and use it to project source dependencies as described in the paragraph above (R1 below). We then use additional rules (R2-R4 below) for the target words that are not in align1−1. Given a source sentence 5 with a parse DS, a target sentence T and word alignment align, align1−1 is extracted as follows: For all ti E T with multiple aligned source words {si1, si2, ...} only align1−1(si1) = ti (only leftmost source word is kept, the l</context>
<context position="22475" citStr="Quirk and Menezes (2006)" startWordPosition="3863" endWordPosition="3866">gn. This rule deals with one-to-many alignments; see Figure 5(d); (R3a) if Isk s.t. align1−1(sk) = ti and Isl s.t. (sl, tj) E align and and DS(sl, sk), and ti linearly precedes tj. In words: if two target words are in align1−1 but do not get connected via R1, find a source word aligned to the second target word that may get them connected; see Figure 5(e); (R3b) same as R3a, but in case tj precedes ti (i.e., find an additional source word aligned to the first target word; see Figure 5(f)).6 (R4) In case FIs (s, tj) E align (tj is unaligned), we consider two strategies: We simplify the rule of Quirk and Menezes (2006) (dealing with the same situation) by adjoining it to the immediately preceding head. We also consider a strategy whereby the word remains unconnected to any word in the sentence; see Figure 5(g). 6R3a and R3b differ from the rules proposed in Quirk and Menezes (2006) dealing with the same situation, since we had to adapt it to the left-to-right parsing scenario. Figure 6: (a): The dashed lines are the dependency arcs that would project through word alignment, resulting in a non-projective projective (impossible under strong source-completeness). (b): The dashed lines are the parse produced un</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Dependency treelet translation: The convergence of statistical and example-based machine translation? Machine Translation, 20:43–65, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="28657" citStr="Riezler and Maxwell, 2005" startWordPosition="4861" endWordPosition="4864">stortion limit is set to 5. A 5-gram LM is trained on the English Gigaword corpus (1.6B tokens) using SRILM with modified Kneser-Ney smoothing and linear interpolation. Information about the training data for the Arabic-English and Chinese-English systems is in Table 3.8 Feature weights are tuned using pairwise ranking optimization (Hopkins and May, 2011) on the MT04 benchmark (for both language pairs). For testing, we use MT08 and MT09 for Arabic, and MT06 and MT08 for Chinese. We use case-insensitive BLEU (Papineni et al., 2002) as evaluation metric. Approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005) is used to detect statistically significant differences. 5.2 Baseline and comparison systems As a comparison model, we implemented six features from Cherry (2008) and Bach et al. (2009)9 and added them to the log-linear interpolation used 8The standard LDC corpora were used for training. 9Exhaustive and non-exhaustive interruption check, exhaustive and non-exhaustive interruption count, verb- and noun-dominated subtree interruption count. 2404 Training set N. of lines N. of tokens Source side of Ar-En set 4,376,320 148M Target side of Ar-En set 4,376,320 146M Source side of Ch-En set 2,104,65</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Modelling and optimizing on syntactic n-grams for statistical machine translation.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--169</pages>
<contexts>
<context position="11685" citStr="Sennrich, 2015" startWordPosition="1945" endWordPosition="1946">d to a bilingual setting and apply to SMT in Section 4. SLMs have been applied in SMT before (Yamada and Knight, 2001; Yu et al., 2014), but as we show in Section 4, we provide a much simpler method to integrate it into the system. While a SLM is not the only syntactically defined LM, it is one of the few that models sentence generation sequentially. And due to the way the decoding procedure of PBSMT is defined, it is natural and straightforward to use models whose score can be computed sequentially. Other syntactic language models define sentence generation hierarchically (Shen et al., 2008; Sennrich, 2015), which complicates their integration into a PBSMT system. The linguistic intuition behind SLMs is that the structural children of a word do not essentially change its distributional properties but just provide additional specification. In Figure 3(a) the word president has two modifiers: the and former and it follows yesterday (an adjunct) and precedes met (a predicate). This ordering is correct in English. If instead its modifier was a or an entire relative clause, it would not make it incorrect. To capture this observation, (Chelba and Jelinek, 2000) propose a language model where each word</context>
</contexts>
<marker>Sennrich, 2015</marker>
<rawString>Rico Sennrich. 2015. Modelling and optimizing on syntactic n-grams for statistical machine translation. Transactions of the Association for Computational Linguistics, 3:169–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph M Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1582" citStr="Shen et al., 2008" startWordPosition="216" endWordPosition="219">ny model components of competitive statistical machine translation (SMT) systems are based on rather simplistic definitions with little linguistic grounding, which includes the definitions of phrase pairs, lexicalized reordering, and n-gram language models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated </context>
<context position="11668" citStr="Shen et al., 2008" startWordPosition="1941" endWordPosition="1944">000), that we extend to a bilingual setting and apply to SMT in Section 4. SLMs have been applied in SMT before (Yamada and Knight, 2001; Yu et al., 2014), but as we show in Section 4, we provide a much simpler method to integrate it into the system. While a SLM is not the only syntactically defined LM, it is one of the few that models sentence generation sequentially. And due to the way the decoding procedure of PBSMT is defined, it is natural and straightforward to use models whose score can be computed sequentially. Other syntactic language models define sentence generation hierarchically (Shen et al., 2008; Sennrich, 2015), which complicates their integration into a PBSMT system. The linguistic intuition behind SLMs is that the structural children of a word do not essentially change its distributional properties but just provide additional specification. In Figure 3(a) the word president has two modifiers: the and former and it follows yesterday (an adjunct) and precedes met (a predicate). This ordering is correct in English. If instead its modifier was a or an entire relative clause, it would not make it incorrect. To capture this observation, (Chelba and Jelinek, 2000) propose a language mode</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph M. Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the Association for Computational Linguistics, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>Srilm at sixteen: Update and outlook.</title>
<date>2011</date>
<booktitle>In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop,</booktitle>
<pages>5</pages>
<contexts>
<context position="26890" citStr="Stolcke et al., 2011" startWordPosition="4590" endWordPosition="4593">ase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling tˆ given the phrase pair: tˆ = arg max¯t p(¯t|¯e, ¯f). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7. POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System MT06 MT08 MT06+MT08 baseline 32.60 25.94 29.56 cohesion 32.52 25.98 29.54 Table 1: Chinese-English baseline and comparison model (Cherry, 2008; Bach et al., 2009) results. Sys</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. Srilm at sixteen: Update and outlook. In Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop, page 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26830" citStr="Toutanova et al., 2003" startWordPosition="4579" endWordPosition="4582">o phrase-internal alignments and target POS tags. We store phrase-internal alignments and targetside POS annotations of each phrase in the phrase table, based on the most frequent internal alignment during training and the most likely targetside POS labeling tˆ given the phrase pair: tˆ = arg max¯t p(¯t|¯e, ¯f). We train BiSLMs on the parallel training data (Section 5.1) and use the Stanford dependency parser (Chang et al., 2009) for Chinese and and the Stanford constituency parser (Green and Manning, 2010) for Arabic7. POStagging of the training data is produced with the Stanford POS-tagger (Toutanova et al., 2003). We learn a 5-gram model using SRILM (Stolcke et al., 2011) with modified Kneser-Ney smoothing. 5 Experiments To evaluate the effectiveness of BiSLMs for PBSMT, we performed rescoring experiments for 7We extract dependency parses from its output based on Collins (1999) Arabic-English and Chinese-English. We compare the resulting 1-best translation lists with an output of the baseline system and the baseline augmented with soft cohesion constraints from Bach et al. (2009). System MT06 MT08 MT06+MT08 baseline 32.60 25.94 29.56 cohesion 32.52 25.98 29.54 Table 1: Chinese-English baseline and com</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="9164" citStr="Wu, 1997" startWordPosition="1505" endWordPosition="1506">ations. (d) is uncohesive because a and c translate the source subtree {(0,1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS, a translation W is cohesive if all translated target words wZ, wj do not have any word wk between them such that there is a source subtree sub in DS such that some</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Xiang</author>
<author>Niyu Ge</author>
<author>Abraham Ittycheriah</author>
</authors>
<title>Improving reordering for statistical machine translation with smoothed priors and syntactic features.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>61--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1882" citStr="Xiang et al. (2011)" startWordPosition="263" endWordPosition="266"> statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our choice is motivated by the fact that PBSMT is a conceptually simple and therefore flexible framework. It is typically quite straightforward to integrate an additional model into the system. Also, PBSMT is the most widely used framework in the SMT research community, which ensures comparability of our results to other p</context>
</contexts>
<marker>Xiang, Ge, Ittycheriah, 2011</marker>
<rawString>Bing Xiang, Niyu Ge, and Abraham Ittycheriah. 2011. Improving reordering for statistical machine translation with smoothed priors and syntactic features. In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 61–69. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9190" citStr="Yamada and Knight, 2001" startWordPosition="1507" endWordPosition="1510">) is uncohesive because a and c translate the source subtree {(0,1)}, but b does not translate it. words. Syntactic divergence increases if the two languages are typologically different. Even though DCA only holds up to a certain level of precision, it is widely used in NLP. There are models of cross-lingual transfer that define syntactic structure of one language by conditioning it on the structure of semantically equivalent sentences in another language (Naseem et al., 2012). DCA has also been used in SMT. In particular, syntax-based SMT is built implicitly around this assumption (Wu, 1997; Yamada and Knight, 2001). In Quirk and Menezes (2006) DCA is explicitly implemented by defining a translation model in terms of treelet pairs where target-side treelets are produced by projecting source dependencies via word alignments. Closely related to DCA is the notion of syntactic cohesion of translation (Fox, 2002; Cherry, 2008). This is a constraint that does not allow for non-projective reordering: Given a source parse DS, a translation W is cohesive if all translated target words wZ, wj do not have any word wk between them such that there is a source subtree sub in DS such that some parts of it are translate</context>
<context position="11187" citStr="Yamada and Knight, 2001" startWordPosition="1857" endWordPosition="1860">der in order to produce better translations. Our goal is to have a model that allows for a more direct way of evaluation of how well-formed the target translation is. In Section 5 we compare translation performance of the two approaches. 3 Structured language models As discussed in Sections 1 and 2, we would like to test how much a PBSMT can benefit from an additional syntax-based LM. In this section, we describe a syntactic language model, structured LM (SLM) (Chelba and Jelinek, 2000), that we extend to a bilingual setting and apply to SMT in Section 4. SLMs have been applied in SMT before (Yamada and Knight, 2001; Yu et al., 2014), but as we show in Section 4, we provide a much simpler method to integrate it into the system. While a SLM is not the only syntactically defined LM, it is one of the few that models sentence generation sequentially. And due to the way the decoding procedure of PBSMT is defined, it is natural and straightforward to use models whose score can be computed sequentially. Other syntactic language models define sentence generation hierarchically (Shen et al., 2008; Sennrich, 2015), which complicates their integration into a PBSMT system. The linguistic intuition behind SLMs is tha</context>
<context position="15166" citStr="Yamada and Knight (2001)" startWordPosition="2548" endWordPosition="2551">process of W as follows:2 Each new word wi is conditioned on a 2The original model by (Chelba and Jelinek, 2000) is defined in terms of a lexicalized constituency grammar, but as · p(ti|wi, Expos(Wi−1, Di−1)) · p(Di|wi, ti, Expos(Wi−1, Di−1)). (1) They use a shift-reduce parser with reduce-left, reduce-right, and shift operations. 4 Bilingual structured language models In this section, we combine the direct correspondence assumption (Section 2) and SLMs (Section 3), and define bilingual structured language models (BiSLMs) for PBSMT. Structured LMs have been successfully applied in SMT before. Yamada and Knight (2001) use SLMs in a stringto-tree SMT system where a derivation of a targetside parse tree is part of the decoding algorithm, and target syntactic representations are obtained ‘for free’. Yu et al. (2014) use an on-the-fly shiftreduce parser to build an incremental target parse. The approaches sketched above rely on resources that a standard PBSMT system does not have access to by default. Phrase-based decoders do not provide us with a parse of the target sentence, and inferring the parse of a target string with an external parser is computationally expensive and potentially unreliable (see Section</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>A structured language model for incremental treeto-string translation.</title>
<date>2014</date>
<booktitle>Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics,</booktitle>
<pages>1133--1143</pages>
<contexts>
<context position="11205" citStr="Yu et al., 2014" startWordPosition="1861" endWordPosition="1864">etter translations. Our goal is to have a model that allows for a more direct way of evaluation of how well-formed the target translation is. In Section 5 we compare translation performance of the two approaches. 3 Structured language models As discussed in Sections 1 and 2, we would like to test how much a PBSMT can benefit from an additional syntax-based LM. In this section, we describe a syntactic language model, structured LM (SLM) (Chelba and Jelinek, 2000), that we extend to a bilingual setting and apply to SMT in Section 4. SLMs have been applied in SMT before (Yamada and Knight, 2001; Yu et al., 2014), but as we show in Section 4, we provide a much simpler method to integrate it into the system. While a SLM is not the only syntactically defined LM, it is one of the few that models sentence generation sequentially. And due to the way the decoding procedure of PBSMT is defined, it is natural and straightforward to use models whose score can be computed sequentially. Other syntactic language models define sentence generation hierarchically (Shen et al., 2008; Sennrich, 2015), which complicates their integration into a PBSMT system. The linguistic intuition behind SLMs is that the structural c</context>
<context position="15365" citStr="Yu et al. (2014)" startWordPosition="2583" endWordPosition="2586">) · p(Di|wi, ti, Expos(Wi−1, Di−1)). (1) They use a shift-reduce parser with reduce-left, reduce-right, and shift operations. 4 Bilingual structured language models In this section, we combine the direct correspondence assumption (Section 2) and SLMs (Section 3), and define bilingual structured language models (BiSLMs) for PBSMT. Structured LMs have been successfully applied in SMT before. Yamada and Knight (2001) use SLMs in a stringto-tree SMT system where a derivation of a targetside parse tree is part of the decoding algorithm, and target syntactic representations are obtained ‘for free’. Yu et al. (2014) use an on-the-fly shiftreduce parser to build an incremental target parse. The approaches sketched above rely on resources that a standard PBSMT system does not have access to by default. Phrase-based decoders do not provide us with a parse of the target sentence, and inferring the parse of a target string with an external parser is computationally expensive and potentially unreliable (see Section 1). Our main insight is that in a bilingual setting one does not need an additional probabilistic target parsing model. We assume that the source parse is given (precomputed) and that the DCA (Secti</context>
</contexts>
<marker>Yu, Mi, Huang, Liu, 2014</marker>
<rawString>Heng Yu, Haitao Mi, Liang Huang, and Qun Liu. 2014. A structured language model for incremental treeto-string translation. Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics, pages 1133–1143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1562" citStr="Zollmann and Venugopal, 2006" startWordPosition="212" endWordPosition="215">abicEnglish. 1 Introduction Many model components of competitive statistical machine translation (SMT) systems are based on rather simplistic definitions with little linguistic grounding, which includes the definitions of phrase pairs, lexicalized reordering, and n-gram language models. However, earlier work has also shown that statistical MT can benefit from additional linguistically motivated models. Most prominent among the linguistically motivated approaches are syntax-based MT systems which take into account the syntactic structure of sentences through CKY decoding and categorial labels (Zollmann and Venugopal, 2006; Shen et al., 2008). On the other hand, the commonly used phrase-based SMT approaches can also reap some of the benefits of using syntactic information by integrating linguistic components addressing specific phenomena, such as Cherry (2008), Carpuat et al. (2010), Crego and Yvon (2010), Ge (2010), Xiang et al. (2011), Lerner and Petrov (2013), Garmash and Monz (2014). This paper is a contribution to the existing body of work on how syntactically motivated models help translation performance. We work with the phrase-based SMT (PBSMT) (Koehn et al., 2003) framework as the baseline system. Our </context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings of the Workshop on Statistical Machine Translation, pages 138–141. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
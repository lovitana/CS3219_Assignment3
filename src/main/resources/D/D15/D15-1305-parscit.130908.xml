<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000127">
<title confidence="0.858248">
Reinforcing the Topic of Embeddings with Theta Pure Dependence for
</title>
<author confidence="0.849756">
Text Classification*
Ning Xing1, Yuexian Hou1, Peng Zhang1, Wenjie Li2, Dawei Song1
</author>
<affiliation confidence="0.9987925">
1School of Computer Science and Technology, Tianjin University, China
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong
</affiliation>
<email confidence="0.994145">
{xingning,yxhou,pzhang,dwsong}@tju.edu.cn,cswjli@comp.polyu.edu.hk
</email>
<sectionHeader confidence="0.997338" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998935625">
For sentiment classification, it is often
recognized that embedding based on dis-
tributional hypothesis is weak in captur-
ing sentiment contrast–contrasting words
may have similar local context. Based on
broader context, we propose to incorporate
Theta Pure Dependence (TPD) into the
Paragraph Vector method to reinforce top-
ical and sentimental information. TPD has
a theoretical guarantee that the word de-
pendency is pure, i.e., the dependence pat-
tern has the integral meaning whose under-
lying distribution can not be conditionally
factorized. Our method outperforms the
state-of-the-art performance on text clas-
sification tasks.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999343">
Word embeddings can be learned by training a
neural probabilistic language model or a uni-
fied neural network architecture for various NLP
tasks (Bengio et al., 2003; Collobert and We-
ston, 2008; Collobert et al., 2011). In global
context-aware neural language model (Huang et
al., 2012), the global context vector is a weighted
average of all word embeddings of a single docu-
ment/paragraph. After trained with all word em-
beddings belonging to the current paragraph, a re-
sulting Paragraph Vector can be obtained. Actu-
ally, Le and Mikolov’s Paragraph Vector (Le and
Mikolov, 2014) is trained based on the log-linear
neural language model (Mikolov et al., 2013a).
For text classification, using a straightforward
extension of language model (e.g. Le and
Mikolov’s Paragraph Vector) is considered not to
be sensible. Embeddings learned for text classifi-
cation should be very different from that learned
for language modeling. For example, language
</bodyText>
<note confidence="0.831577">
Corresponding authors: Yuexian Hou and Peng Zhang.
</note>
<bodyText confidence="0.999812024390244">
models often calculate the probability of a sen-
tence, therefore this is a good movie and this is
a bad movie may not be discriminated from each
other. In sentiment analysis task, the semantic
representation of words needs to tell word good
from bad, even if the two words have the same
local context. For this reason, the local depen-
dency is insufficient to model topical or sentiment
information. Fortunately, if we have the global
context of good like interesting or amazing, the
sentiment meaning of the embedding will be ex-
plicit. However, the training of log-linear neural
language model is based on local word dependen-
cies (e.g., the co-occurrence of the words in a local
window). Thus, Paragraph Vector can not explic-
itly model the word dependencies for those words
that do not frequently appear in a local window but
are actually closely dependent on each other.
In this paper, our aim is to extend the Paragraph
Vector with global context which can capture topi-
cal or sentiment information effectively. However,
if one explicitly considers the dependency patterns
that are beyond the local window level, there is a
possibility that the noisy dependency patterns can
be involved and modeled in the distributed repre-
sentation methods. Moreover, there should be an
unique and explicit topical meaning in the patterns
to guarantee no ambiguity in the global context.
Therefore, we need a dependency mining method
that not only models the long range dependency
patterns, but also provides a theoretical guarantee
that the dependency patterns are pure. Here, the
“pure” dependency pattern is an integral seman-
tic meaning/concept that cannot be factorized into
sub dependency patterns.
In the language of statistics, Conditional Pure
Dependence (CPD) means that the underlying dis-
tribution of the dependency patterns cannot be fac-
torized under certain conditions (e.g., priors, ob-
served words, etc.). It has been proved that CPD
is the high-level pure dependence in (Hou et al.,
</bodyText>
<page confidence="0.855351">
2551
</page>
<note confidence="0.6347305">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2551–2556,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99835875">
2013). However, judging CPD is NP-hard (Chick-
ering et al., 2004). Fortunately, Theta Pure De-
pendence (TPD) is the sufficient criteria of CPD
and can be identified in O(N) time, where N is the
number of words (Hou et al., 2013). This finding
motivates us to adopt TPD as the global context.
Moreover, compared with other conventional co-
occurrence-based methods, such as the Apriori al-
gorithm (Agrawal et al., 1993), TPD based on the
Information Geometry (IG) framework has a solid
theoretical interpretations in statistics to guarantee
the dependence is pure.
</bodyText>
<sectionHeader confidence="0.871877" genericHeader="method">
2 Modeling Topic with TPD
</sectionHeader>
<bodyText confidence="0.999744307692308">
Compared with local context, global context can
usually capture the text topic more precisely. It
is easy to get local context by a sliding window.
We define the centered word as the current word
and the other words in the window as local con-
text words. Global context words are extracted
from all the documents in the corpus and can be
divided into two parts: a) the words in the current
document but outside of the local context window;
b) the words never appeared in the document but
in the corpus. The following example shows the
words mentioned above, and the topic (the scene
of filming) is easily captured by TPD:
</bodyText>
<listItem confidence="0.993464">
• TPD: scene camera acting movie
</listItem>
<bodyText confidence="0.995995388888889">
Text: there [is great atmosphere in the scene from the
location , the] lighting , the fog and such , but the
camera should be slowly following the killer...
The bracket stands for the local context window,
and the size of window is 5, i.e. there are five lo-
cal context words (in italics) in both sides of the
current word (in bold). Global context words are
underlined in the example.
In order to model the topic explicitly, the depen-
dence pattern should report one and only one topi-
cal meaning. TPD has a theoretical guarantee that
the dependency has an integral meaning whose un-
derlying distribution can not be conditionally fac-
torized. Formally, given a set of binary random
variables X = {X1, ... , Xn}, where Xi denotes
the occurrence (Xi = 1) or absence (Xi = 0) of
the i-th word. Then the n-order TPD over X can
be defined as follows.
</bodyText>
<sectionHeader confidence="0.518923" genericHeader="method">
DEFINITION 1. (TPD): X = {X1, ... , Xn} is
</sectionHeader>
<bodyText confidence="0.999511333333333">
of n-order Theta Pure Dependence (TPD), iff the
n-order 0 coordinate 012...n is significantly differ-
ent from zero. (Hou et al., 2013)
TPD can be effectively identified by an explicit
statistical test procedure: Log Likelihood Ratio
Test (LLRT) (Nakahara and Amari, 2002) for 0-
coordinate of IG. (Hou et al., 2013)
Here, we introduce two negative examples to
further emphasize the importance of utilizing
TPD. Example 1: can, with, of. The joint distri-
bution of this words combination can be uncon-
ditionally factorized directly, since the occurrence
of any word does not necessarily imply the occur-
rence of others. Example 2: London, Chelsea,
Sherlock Holmes. As we all know, both Chelsea
and Sherlock Holmes are closely related to Lon-
don. Chelsea and Sherlock Holmes are two rela-
tively independent topics, i.e. they are conditional
independent given London. Although the three
phrases are unconditionally dependent, their joint
distribution can be conditionally factorized. Thus
the dependency in both two examples can not be
pure.
To explain TPD and the characteristic “pure”
intuitively, let us look at a typical example of
TPD: climate, conference, Copenhagen. The co-
occurrence of the three words implies an un-
separable high-level semantic entity compared
with the two negative examples, introduced above.
In negative examples, the high frequency of words
co-occurrence can be explained as some kind of
“coincidence”, because each of them or their pair-
wise combinations has a high frequency, indepen-
dently. However, the co-occurrence of TPD words
cannot be fully explained as the random coinci-
dence of, e.g., the co-occurrence of Copenhagen
and conference (which can be any other confer-
ences in Copenhagen) and the occurrence of cli-
mate.
The word “pure” in Hou et al. (2013) means that
the joint probability distribution of these words is
significantly different from the product of lower-
order joint distributions or marginal distributions,
w.r.t all possible decompositions. More formally,
it requires that the joint distribution cannot be
factorized unconditionally (UPD) or conditionally
(CPD) in the language of graphical model. Let
xi ∈ {0, 1} denote the value of Xi. Let p(x),
x = [x1, x2, ... , xn]T, be the joint probability dis-
tribution over X. Then the definitions of UPD and
CPD are as follows:
</bodyText>
<construct confidence="0.6433896">
DEFINITION 2. (UPD): X = {X1, ... , Xn}
is of n-order Unconditional Pure Dependence
(UPD), iff it can NOT be unconditionally fac-
torized, i.e., there does NOT exist a k-partition
{C1, C2, . . . , Ck} of X, k &gt; 1, such at p(x) =
</construct>
<page confidence="0.92725">
2552
</page>
<bodyText confidence="0.716034571428572">
p(c1) * p(c2)...p(ck), where p(ci), i = 1, ... , k,
is the joint distribution over Ci. (Hou et al., 2013)
DEFINITION 3. (CPD): X = {X1, ... , Xn} is
of n-order Conditional Pure Dependence (CPD),
iff it can NOT be conditionally factorized, i.e.,
there does NOT exist C0 C X and a k-partition
{C1, C2, ..., Ck} of V = X − C0, k &gt; 1, such at
</bodyText>
<equation confidence="0.709253">
p(v|c0) = p(c1|c0) * p(c2|c0)...p(ck|c0), where
</equation>
<bodyText confidence="0.93850696969697">
p(v|c0) is the conditional joint distribution over
V given C0, and p(ci|c0), i = 1, 2, ... , k, is the
conditional joint distribution over Ci given C0. In
case that C0 is an empty set, we define p(c0) =
1. (Hou et al., 2013)
Actually, CPD is stricter than UPD, and the
dependence which just satisfies UPD is not pure
enough to model the global context. Therefore,
“pure” in our paper refers to the characteristic of
CPD. However judging CPD is NP-hard. It is
proved that a significant nonzero n-order 0 param-
eter (TPD) entails the n-order CPD/UPD in Hou
et al. (2013). The highest-order coordinate pa-
rameter in IG is a proper metric for the purity
(i.e., the unique semantics) of high-order depen-
dence. A pattern is TPD, iff the n-order 0 coor-
dinate 012...n is significantly different from zero.
Moreover, The Log Likelihood Ratio Test imple-
mented in the mixed coordinates can test whether
012...n is significantly different from zero.
Contrasting to TPD, the semantic coupling
among the associations in the two negative exam-
ples is much weaker. In conclusion, can, with, of
cannot give an explicit topic and London, Chelsea,
Sherlock Holmes includes at least two topics. the
co-occurrence of words in TPD (e.g. climate,
conference, Copenhagen) implies an un-separable
(pure) high-level semantic entity. A sufficient and
unbroken meaning of dependence can not only
supply the context but also avoid the ambiguity (or
noise) in global context. Therefore, the meaning
of pure is important in such a global context mod-
eling method.
</bodyText>
<sectionHeader confidence="0.99074" genericHeader="method">
3 Global PV-DBOW and Dependence
Vectors
</sectionHeader>
<bodyText confidence="0.995028857142857">
A version of Paragraph Vector in Le and Mikolov
(2014) PV-DBOW is extended with TPD to a
new model: Global PV-DBOW (Glo-PV-DBOW).
TPD has been extracted from the corpus before
training. Given a sequence of training words w1,
w2, w3, ... , wT and the global context glot of wt,
the objective of Glo-PV-DBOW is to maximize the
</bodyText>
<equation confidence="0.98795175">
average log probability:
[ ∑ log p(wt|wt+j)
−c&lt;j&lt;c,j̸=0
]+ log p(wt|glot) + log p(wt|doct)
</equation>
<bodyText confidence="0.998782">
where c is the local context window size. The in-
dicator of the document that the current word wt
belongs to is denoted by doct. Further, we define
p(wt|glot) in equation (2):
</bodyText>
<equation confidence="0.918066">
p(wt|glot)
[] (2)
</equation>
<bodyText confidence="0.980662833333333">
p(wt|depa t )p(wt|wa 1,wa 2, ... , wa N)
The indicator of the a-th wt’s TPD pattern is de-
noted as depat and can be trained to be a distributed
representation of TPD: dependence vector vdept .
This (N+1)-order TPD consists of N+1 words: wa1,
wa2... wa N and wt. The energy function of wt and
</bodyText>
<equation confidence="0.5794175">
wi = (wt+j, doct, depat ) is uniform as follows:
E(wt, wi) = −vwt T vwi (3)
</equation>
<bodyText confidence="0.97248">
We define the energy function of TPD words:
</bodyText>
<equation confidence="0.921111666666667">
∑N
1
E(wt, wa 1, wa 2, ... wa N) = − N
</equation>
<bodyText confidence="0.641451">
The resulting predictive distributions are given by
</bodyText>
<equation confidence="0.996704375">
exp(vwtTvwi)
p(wt|wi) = W T (5)
∑m=1 exp(vw� vwi)
p(wt|wa1,wa 2,...,wa N)
exp(1 N T
N ∑n=1 vwt vw° )
=
∑m=1 exp( N ∑n=1 vw�T vw° )
</equation>
<bodyText confidence="0.999312785714286">
Hierarchical softmax (Morin and Bengio, 2005)
is adopted to reduce the cost of computation.
The binary tree is specified with a Huffman tree,
and the Huffman code of pseudo words mi in
wt’s Huffman path is denoted as xmi. For more
about hierarchical softmax we used, please re-
fer to (Mikolov et al., 2013b). Using stochas-
tic gradient descent (SGD), distributed representa-
tions of the word, dependence and document have
been trained. The update procedure of vwi =
(vwt+j, vdoct, vdept) is as same as the procedure
described in (Mikolov et al., 2013b). Thus, the
pseudo code for training TPD words is listed indi-
vidually:
</bodyText>
<figure confidence="0.783483769230769">
1
L=
T
∑T
t=1
(1)
=
A
∏
a
T (4)
vwt vw°
n=1
(6)
2553
SGD FOR TRAINING THE TPD WORDS
1 vwt +— current word
N
2 vwa 1 ave N En=1 vw°
3 err +— 0
4 for Vmi \l\l
5 do g +— (1 — xmi — U(vmiT vwaave// * a&apos;
6 err+ = g* 1 N * vmi
7 mi+ = g * vwaave
8 for n +— 1 to N
9 do vwan + = err
</figure>
<sectionHeader confidence="0.977367" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99964484">
Apriori (not a pure dependency method) is con-
trastively adopted to implement Glo-PV-DBOW.
Glo-PV-DBOW-TPD and Glo-PV-DBOW-Apri
are all evaluated in two text classification tasks:
sentiment analysis and topic discovery. The suffix
(e.g., -2, -5) of our global method name denotes
the order of dependency (the number of words in
a dependence pattern). The order of dependency
is changed because we want to show the superi-
ority of the high-order TPD. The high-order TPD
provides the more rich and explicit global context
than the lower-order one since the high-order TPD
cannot be reduced to the random coincidence of
lower-order dependencies.
We cross-validate the hyperparameters and set
the local context window size as 10, the dimen-
sion of embeddings as 100. In sentiment anal-
ysis task, Apriori’s minimum support and TPD’s
theta 0 is respectively set as 0.004 and 1.4. While
in topic discovery task, Apriori’s minimum sup-
port and TPD’s theta 0 is around 0.020 and 2.0 re-
spectively. Since the classification accuracy of the
approaches compared is a single result, we do not
include any results for test of significance in our
method and only report the average accuracy.
</bodyText>
<subsectionHeader confidence="0.99956">
4.1 Sentiment Analysis on Movie Reviews
</subsectionHeader>
<bodyText confidence="0.999871333333333">
The binary sentiment classification on the IMDB
dataset proposed by (Maas et al., 2011) is con-
ducted. Results in Fig.1 show that global methods’
performance is more stable than PV-DBOW’s.
Moreover, TPD works much better than Apriori,
especially in the high-order dependence. Note
that TPD-5 works better than TPD-2, while Apri-
5 works worse than Apri-2. It can be explained
that the Apriori algorithm is short of an explicit
statistical test procedure to guarantee the pure de-
pendence. Therefore, the Apriori algorithm is not
suitable for generating the high-order dependence.
</bodyText>
<figure confidence="0.944825666666667">
IMDB
PV−DBOW Apri−2 Apri−5 TPD−2 TPD−5
Methods
</figure>
<figureCaption confidence="0.985348">
Figure 1: Box plot of classification accuracy over
a local method (PV-DBOW) and 4 global methods
(Apri-2/5, TPD-2/5).
</figureCaption>
<bodyText confidence="0.999717631578947">
Instead, the high-order TPD can provide the rich
and explicit global context for the model. Mean-
while, it is verified that our method is good at cap-
turing sentiment contrast.
Table 1 shows that Glo-PV-DBOW with 5-order
TPD achieves the state-of-the-art performance.
A promising result is an improvement of more
than 2% over result published in Le and Mikolov
(2014). Note that the algorithm process of Para-
graph Vector (Le and Mikolov, 2014) is much
more complex than PV-DBOW’s. Paragraph Vec-
tor includes an extra inference stage. In addition,
Paragraph Vector’s document vector is a combina-
tion of two vectors: one learned by PV-DBOW and
the other learned by Distributed Memory Model
of Paragraph Vectors (PV-DM) (Le and Mikolov,
2014). The combined document vector has 800
dimensions, while all vectors in our experiments
only have 100 dimensions.
</bodyText>
<subsectionHeader confidence="0.998994">
4.2 Topic Discovery on News
</subsectionHeader>
<bodyText confidence="0.998819230769231">
The 20 Newsgroups dataset is a collection of ap-
proximately 20,000 newsgroup documents, parti-
tioned across 20 different newsgroups. We fol-
low (Crammer et al., 2012) to create binary prob-
lems from the dataset by creating binary decision
problems of choosing between two similar groups.
Therefore, the dataset is split into two sub-datasets
as follows: comp: comp.sys.ibm.pc.hardware vs.
comp.sys.mac.hardware and sci: sci.electronics
vs. sci.med. Similarly, 1800 examples balanced
between the two labels were selected for each
problem.
The classification accuracy on each sub-dataset
</bodyText>
<figure confidence="0.964630083333333">
95
Test Classification Accuracy (perc)
94.5
94
93.5
93
92.5
92
91.5
91
90.5
90
</figure>
<page confidence="0.990455">
2554
</page>
<tableCaption confidence="0.923880333333333">
Table 1: The performance of our method com-
Table 3: Nearest neighbors of words ranking list
based on cosine similarity.
</tableCaption>
<table confidence="0.998319411764706">
pared with other approaches on the IMDB dataset.
Model Accuracy rate
BoW (bnc) (Maas et al., 2011) 87.80%
Full+Unlabeled+BoW (Maas et al., 2011) 88.89%
WRRBM (Dahl et al., 2012) 87.42%
WRRBM + BoW (bnc) (Dahl et al., 2012) 89.23%
SVM-bi (Wang and Manning, 2012) 89.16%
NBSVM-bi (Wang and Manning, 2012) 91.22%
PV-DBOW (Le and Mikolov, 2014) 90.79%
Paragraph Vector (Le and Mikolov, 2014) 92.58%
Sentence Vector + RNN-LM + NB-LM 92.57%
(Mesnil et al., 2014)
mvCNNo&amp;w (Johnson and Zhang, 2015) 93.34%
Glo-PV-DBOW-Apri-2 93.76%
Glo-PV-DBOW-Apri-5 92.41%
Glo-PV-DBOW-TPD-2 94.83%
Glo-PV-DBOW-TPD-5 95.05%
</table>
<tableCaption confidence="0.9887575">
Table 2: The performance of our method com-
pared to other approaches on 20 Newsgroup.
</tableCaption>
<table confidence="0.999230444444445">
Model Comp Sci
Confidence-weighted 94.39% 97.56%
(Crammer et al., 2012) 92.60% 98.02%
PV-DBOW
(Le and Mikolov, 2014)
Glo-PV-DBOW-Apri-2 94.56% 98.42%
Glo-PV-DBOW-Apri-5 94.43% 98.13%
Glo-PV-DBOW-TPD-2 94.59% 99.20%
Glo-PV-DBOW-TPD-5 95.47% 98.74%
</table>
<bodyText confidence="0.956296444444445">
is recorded in Table 2. Compared with
Confidence-weighted (Crammer et al., 2012) and
PV-DBOW (Le and Mikolov, 2014), our extended
models achieve the highest accuracy on each sub-
dataset. Moreover, TPD as a pure dependence
works better than Apriori when they provide the
global context for our model. The topical infor-
mation is effectively reinforced in embeddings by
incorporating TPD.
</bodyText>
<subsectionHeader confidence="0.999892">
4.3 Analysis on Word Embeddings
</subsectionHeader>
<bodyText confidence="0.999737428571429">
The cosine similarity of each word pair in 20
Newsgroups is computed. We list four center
words and their nearest neighbors in PV-DBOW
and Glo-PV-DBOW groups respectively. The
rankings are labeled in front of neighbor words,
and some notable neighbor words are in bold.
From Table 3, we can see that the statistical
information of corpus like words co-occurrence
can be mined by TPD. Therefore, the Glo-PV-
DBOW’s embeddings are context-aware and it can
help a lot for classification tasks. The top 40 near-
est neighbors of ibm are investigated, and we find
macintosh and mac appeared in the PV-DBOW
group but not in the Glo-PV-DBOW group. In
</bodyText>
<table confidence="0.998029">
Center word PV-DBOW Glo-PV-DBOW
ibm 1:aix 1:aix
2:pc 2:pc
... 3:pc’s
23:macintosh 4:austin
34:mac 5:workstations
mac 1:macintosh 1:macintosh
2:quicktime 2:apple’s
3:portable 3:quicktime
4:utilities 4:apple
5:macs 5:macs
486 1:386 1:386
2:486dx 2:cpu
3:33mhz 3:486dx
4:486dx2 4:486dx2
5:cpu 5:33mhz
Kingston 1:aix 1:aix
2:mike 2:ibm
3:sharks 3:jones
4:jones 4:sharks
5:ibm 5:mike
</table>
<bodyText confidence="0.999863888888889">
the corpus, the topic of documents is either ibm or
mac. If we perform a classification task on ”ibm
versus mac”, it will be hard to classify in the PV-
DBOW group. That is because PV-DBOW tends
to regard ibm and mac both as computers. How-
ever, the two different computer brands are distin-
guished in Glo-PV-DBOW. Further, ibm and mac
co-occur rarely in one document, and the statisti-
cal information is noted by TPD.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999960545454546">
This paper proposes to incorporate Theta Pure De-
pendence into Paragraph Vector to capture more
topical and sentimental information in the con-
text. The extended model is applied to a sen-
timent classification task and a topical detection
task. Our accuracy outperforms the state-of-the-
art result on the movie and news datasets. The ap-
proach can be improved further to fully leverage
the un-factorized sense of high-order Theta Pure
Dependence. In future, we will explore the appli-
cations of dependence distributed representation.
</bodyText>
<sectionHeader confidence="0.998816" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990231285714286">
This work is funded in part by the Chinese
863 Program (grant No. 2015AA015403), the
Key NSF Project of Chinese Tianjin (grant
No. 15JCZDJC31100), the Chinese 973 Program
(grant No. 2013CB329304 and 2014CB744604),
the Chinese NSF Project (grant No. 61272291,
61402324 and 61272265).
</bodyText>
<page confidence="0.973033">
2555
</page>
<sectionHeader confidence="0.996288" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999913564705882">
Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami.
1993. Mining association rules between sets of
items in large databases. In ACM SIGMOD Record,
volume 22, pages 207–216. ACM.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
David Maxwell Chickering, David Heckerman, and
Christopher Meek. 2004. Large-sample learning of
bayesian networks is np-hard. The Journal of Ma-
chine Learning Research, 5:1287–1330.
Ronan Collobert and Jason Weston. 2008. A uni-
fied aagrawal1993miningrchitecture for natural lan-
guage processing: Deep neural networks with mul-
titask learning. In Proceedings of the 25th interna-
tional conference on Machine learning, pages 160–
167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification for
text categorization. The Journal of Machine Learn-
ing Research, 13(1):1891–1926.
George E Dahl, Ryan P Adams, and Hugo Larochelle.
2012. Training restricted boltzmann machines on
word observations. Proceedings of International
Conference on Machine Learning.
Yuexian Hou, Xiaozhao Zhao, Dawei Song, and Wenjie
Li. 2013. Mining pure high-order word associations
via information geometry for information retrieval.
ACM Transactions on Information Systems (TOIS),
31(3):12.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Rie Johnson and Tong Zhang. 2015. Semi-supervised
learning with multi-view embedding: Theory and
application with convolutional neural networks.
arXiv preprint arXiv:1504.01255.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1188–1196.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142–150. As-
sociation for Computational Linguistics.
Gr´egoire Mesnil, Marc’Aurelio Ranzato, Tomas
Mikolov, and Yoshua Bengio. 2014. Ensemble
of generative and discriminative techniques for sen-
timent analysis of movie reviews. arXiv preprint
arXiv:1412.5335.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. ICLR Workshop.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246–252. Cite-
seer.
Hiroyuki Nakahara and Shun-ichi Amari. 2002.
Information-geometric measure for neural spikes.
Neural Computation, 14(10):2269–2316.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90–94. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.989008">
2556
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.895096">
<title confidence="0.999712">Reinforcing the Topic of Embeddings with Theta Pure Dependence</title>
<author confidence="0.998138">Yuexian Peng Wenjie Dawei</author>
<affiliation confidence="0.999733">of Computer Science and Technology, Tianjin University,</affiliation>
<address confidence="0.914947">of Computing, The Hong Kong Polytechnic University, Hong Kong</address>
<abstract confidence="0.998853882352941">For sentiment classification, it is often recognized that embedding based on distributional hypothesis is weak in capturing sentiment contrast–contrasting words may have similar local context. Based on broader context, we propose to incorporate Theta Pure Dependence (TPD) into the Paragraph Vector method to reinforce topical and sentimental information. TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution can not be conditionally factorized. Our method outperforms the state-of-the-art performance on text classification tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Tomasz Imieli´nski</author>
<author>Arun Swami</author>
</authors>
<title>Mining association rules between sets of items in large databases.</title>
<date>1993</date>
<journal>In ACM SIGMOD Record,</journal>
<volume>22</volume>
<pages>207--216</pages>
<publisher>ACM.</publisher>
<marker>Agrawal, Imieli´nski, Swami, 1993</marker>
<rawString>Rakesh Agrawal, Tomasz Imieli´nski, and Arun Swami. 1993. Mining association rules between sets of items in large databases. In ACM SIGMOD Record, volume 22, pages 207–216. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1179" citStr="Bengio et al., 2003" startWordPosition="160" endWordPosition="163">xt. Based on broader context, we propose to incorporate Theta Pure Dependence (TPD) into the Paragraph Vector method to reinforce topical and sentimental information. TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution can not be conditionally factorized. Our method outperforms the state-of-the-art performance on text classification tasks. 1 Introduction Word embeddings can be learned by training a neural probabilistic language model or a unified neural network architecture for various NLP tasks (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011). In global context-aware neural language model (Huang et al., 2012), the global context vector is a weighted average of all word embeddings of a single document/paragraph. After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained. Actually, Le and Mikolov’s Paragraph Vector (Le and Mikolov, 2014) is trained based on the log-linear neural language model (Mikolov et al., 2013a). For text classification, using a straightforward extension of language model (e.g. Le and Mikolov’s Para</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Maxwell Chickering</author>
<author>David Heckerman</author>
<author>Christopher Meek</author>
</authors>
<title>Large-sample learning of bayesian networks is np-hard.</title>
<date>2004</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>5--1287</pages>
<contexts>
<context position="4254" citStr="Chickering et al., 2004" startWordPosition="644" endWordPosition="648">ng/concept that cannot be factorized into sub dependency patterns. In the language of statistics, Conditional Pure Dependence (CPD) means that the underlying distribution of the dependency patterns cannot be factorized under certain conditions (e.g., priors, observed words, etc.). It has been proved that CPD is the high-level pure dependence in (Hou et al., 2551 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2551–2556, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2013). However, judging CPD is NP-hard (Chickering et al., 2004). Fortunately, Theta Pure Dependence (TPD) is the sufficient criteria of CPD and can be identified in O(N) time, where N is the number of words (Hou et al., 2013). This finding motivates us to adopt TPD as the global context. Moreover, compared with other conventional cooccurrence-based methods, such as the Apriori algorithm (Agrawal et al., 1993), TPD based on the Information Geometry (IG) framework has a solid theoretical interpretations in statistics to guarantee the dependence is pure. 2 Modeling Topic with TPD Compared with local context, global context can usually capture the text topic </context>
</contexts>
<marker>Chickering, Heckerman, Meek, 2004</marker>
<rawString>David Maxwell Chickering, David Heckerman, and Christopher Meek. 2004. Large-sample learning of bayesian networks is np-hard. The Journal of Machine Learning Research, 5:1287–1330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified aagrawal1993miningrchitecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1207" citStr="Collobert and Weston, 2008" startWordPosition="164" endWordPosition="168">context, we propose to incorporate Theta Pure Dependence (TPD) into the Paragraph Vector method to reinforce topical and sentimental information. TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution can not be conditionally factorized. Our method outperforms the state-of-the-art performance on text classification tasks. 1 Introduction Word embeddings can be learned by training a neural probabilistic language model or a unified neural network architecture for various NLP tasks (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011). In global context-aware neural language model (Huang et al., 2012), the global context vector is a weighted average of all word embeddings of a single document/paragraph. After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained. Actually, Le and Mikolov’s Paragraph Vector (Le and Mikolov, 2014) is trained based on the log-linear neural language model (Mikolov et al., 2013a). For text classification, using a straightforward extension of language model (e.g. Le and Mikolov’s Paragraph Vector) is considered </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified aagrawal1993miningrchitecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160– 167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1232" citStr="Collobert et al., 2011" startWordPosition="169" endWordPosition="172">porate Theta Pure Dependence (TPD) into the Paragraph Vector method to reinforce topical and sentimental information. TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution can not be conditionally factorized. Our method outperforms the state-of-the-art performance on text classification tasks. 1 Introduction Word embeddings can be learned by training a neural probabilistic language model or a unified neural network architecture for various NLP tasks (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011). In global context-aware neural language model (Huang et al., 2012), the global context vector is a weighted average of all word embeddings of a single document/paragraph. After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained. Actually, Le and Mikolov’s Paragraph Vector (Le and Mikolov, 2014) is trained based on the log-linear neural language model (Mikolov et al., 2013a). For text classification, using a straightforward extension of language model (e.g. Le and Mikolov’s Paragraph Vector) is considered not to be sensible. Embed</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification for text categorization.</title>
<date>2012</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="15957" citStr="Crammer et al., 2012" startWordPosition="2670" endWordPosition="2673">Le and Mikolov, 2014) is much more complex than PV-DBOW’s. Paragraph Vector includes an extra inference stage. In addition, Paragraph Vector’s document vector is a combination of two vectors: one learned by PV-DBOW and the other learned by Distributed Memory Model of Paragraph Vectors (PV-DM) (Le and Mikolov, 2014). The combined document vector has 800 dimensions, while all vectors in our experiments only have 100 dimensions. 4.2 Topic Discovery on News The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. We follow (Crammer et al., 2012) to create binary problems from the dataset by creating binary decision problems of choosing between two similar groups. Therefore, the dataset is split into two sub-datasets as follows: comp: comp.sys.ibm.pc.hardware vs. comp.sys.mac.hardware and sci: sci.electronics vs. sci.med. Similarly, 1800 examples balanced between the two labels were selected for each problem. The classification accuracy on each sub-dataset 95 Test Classification Accuracy (perc) 94.5 94 93.5 93 92.5 92 91.5 91 90.5 90 2554 Table 1: The performance of our method comTable 3: Nearest neighbors of words ranking list based </context>
<context position="17336" citStr="Crammer et al., 2012" startWordPosition="2877" endWordPosition="2880">., 2011) 88.89% WRRBM (Dahl et al., 2012) 87.42% WRRBM + BoW (bnc) (Dahl et al., 2012) 89.23% SVM-bi (Wang and Manning, 2012) 89.16% NBSVM-bi (Wang and Manning, 2012) 91.22% PV-DBOW (Le and Mikolov, 2014) 90.79% Paragraph Vector (Le and Mikolov, 2014) 92.58% Sentence Vector + RNN-LM + NB-LM 92.57% (Mesnil et al., 2014) mvCNNo&amp;w (Johnson and Zhang, 2015) 93.34% Glo-PV-DBOW-Apri-2 93.76% Glo-PV-DBOW-Apri-5 92.41% Glo-PV-DBOW-TPD-2 94.83% Glo-PV-DBOW-TPD-5 95.05% Table 2: The performance of our method compared to other approaches on 20 Newsgroup. Model Comp Sci Confidence-weighted 94.39% 97.56% (Crammer et al., 2012) 92.60% 98.02% PV-DBOW (Le and Mikolov, 2014) Glo-PV-DBOW-Apri-2 94.56% 98.42% Glo-PV-DBOW-Apri-5 94.43% 98.13% Glo-PV-DBOW-TPD-2 94.59% 99.20% Glo-PV-DBOW-TPD-5 95.47% 98.74% is recorded in Table 2. Compared with Confidence-weighted (Crammer et al., 2012) and PV-DBOW (Le and Mikolov, 2014), our extended models achieve the highest accuracy on each subdataset. Moreover, TPD as a pure dependence works better than Apriori when they provide the global context for our model. The topical information is effectively reinforced in embeddings by incorporating TPD. 4.3 Analysis on Word Embeddings The cos</context>
</contexts>
<marker>Crammer, Dredze, Pereira, 2012</marker>
<rawString>Koby Crammer, Mark Dredze, and Fernando Pereira. 2012. Confidence-weighted linear classification for text categorization. The Journal of Machine Learning Research, 13(1):1891–1926.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Ryan P Adams</author>
<author>Hugo Larochelle</author>
</authors>
<title>Training restricted boltzmann machines on word observations.</title>
<date>2012</date>
<booktitle>Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="16756" citStr="Dahl et al., 2012" startWordPosition="2792" endWordPosition="2795">lows: comp: comp.sys.ibm.pc.hardware vs. comp.sys.mac.hardware and sci: sci.electronics vs. sci.med. Similarly, 1800 examples balanced between the two labels were selected for each problem. The classification accuracy on each sub-dataset 95 Test Classification Accuracy (perc) 94.5 94 93.5 93 92.5 92 91.5 91 90.5 90 2554 Table 1: The performance of our method comTable 3: Nearest neighbors of words ranking list based on cosine similarity. pared with other approaches on the IMDB dataset. Model Accuracy rate BoW (bnc) (Maas et al., 2011) 87.80% Full+Unlabeled+BoW (Maas et al., 2011) 88.89% WRRBM (Dahl et al., 2012) 87.42% WRRBM + BoW (bnc) (Dahl et al., 2012) 89.23% SVM-bi (Wang and Manning, 2012) 89.16% NBSVM-bi (Wang and Manning, 2012) 91.22% PV-DBOW (Le and Mikolov, 2014) 90.79% Paragraph Vector (Le and Mikolov, 2014) 92.58% Sentence Vector + RNN-LM + NB-LM 92.57% (Mesnil et al., 2014) mvCNNo&amp;w (Johnson and Zhang, 2015) 93.34% Glo-PV-DBOW-Apri-2 93.76% Glo-PV-DBOW-Apri-5 92.41% Glo-PV-DBOW-TPD-2 94.83% Glo-PV-DBOW-TPD-5 95.05% Table 2: The performance of our method compared to other approaches on 20 Newsgroup. Model Comp Sci Confidence-weighted 94.39% 97.56% (Crammer et al., 2012) 92.60% 98.02% PV-DB</context>
</contexts>
<marker>Dahl, Adams, Larochelle, 2012</marker>
<rawString>George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012. Training restricted boltzmann machines on word observations. Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuexian Hou</author>
<author>Xiaozhao Zhao</author>
<author>Dawei Song</author>
<author>Wenjie Li</author>
</authors>
<title>Mining pure high-order word associations via information geometry for information retrieval.</title>
<date>2013</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="4416" citStr="Hou et al., 2013" startWordPosition="675" endWordPosition="678">n of the dependency patterns cannot be factorized under certain conditions (e.g., priors, observed words, etc.). It has been proved that CPD is the high-level pure dependence in (Hou et al., 2551 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2551–2556, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2013). However, judging CPD is NP-hard (Chickering et al., 2004). Fortunately, Theta Pure Dependence (TPD) is the sufficient criteria of CPD and can be identified in O(N) time, where N is the number of words (Hou et al., 2013). This finding motivates us to adopt TPD as the global context. Moreover, compared with other conventional cooccurrence-based methods, such as the Apriori algorithm (Agrawal et al., 1993), TPD based on the Information Geometry (IG) framework has a solid theoretical interpretations in statistics to guarantee the dependence is pure. 2 Modeling Topic with TPD Compared with local context, global context can usually capture the text topic more precisely. It is easy to get local context by a sliding window. We define the centered word as the current word and the other words in the window as local co</context>
<context position="6447" citStr="Hou et al., 2013" startWordPosition="1034" endWordPosition="1037">l the topic explicitly, the dependence pattern should report one and only one topical meaning. TPD has a theoretical guarantee that the dependency has an integral meaning whose underlying distribution can not be conditionally factorized. Formally, given a set of binary random variables X = {X1, ... , Xn}, where Xi denotes the occurrence (Xi = 1) or absence (Xi = 0) of the i-th word. Then the n-order TPD over X can be defined as follows. DEFINITION 1. (TPD): X = {X1, ... , Xn} is of n-order Theta Pure Dependence (TPD), iff the n-order 0 coordinate 012...n is significantly different from zero. (Hou et al., 2013) TPD can be effectively identified by an explicit statistical test procedure: Log Likelihood Ratio Test (LLRT) (Nakahara and Amari, 2002) for 0- coordinate of IG. (Hou et al., 2013) Here, we introduce two negative examples to further emphasize the importance of utilizing TPD. Example 1: can, with, of. The joint distribution of this words combination can be unconditionally factorized directly, since the occurrence of any word does not necessarily imply the occurrence of others. Example 2: London, Chelsea, Sherlock Holmes. As we all know, both Chelsea and Sherlock Holmes are closely related to L</context>
<context position="8092" citStr="Hou et al. (2013)" startWordPosition="1294" endWordPosition="1297">en. The cooccurrence of the three words implies an unseparable high-level semantic entity compared with the two negative examples, introduced above. In negative examples, the high frequency of words co-occurrence can be explained as some kind of “coincidence”, because each of them or their pairwise combinations has a high frequency, independently. However, the co-occurrence of TPD words cannot be fully explained as the random coincidence of, e.g., the co-occurrence of Copenhagen and conference (which can be any other conferences in Copenhagen) and the occurrence of climate. The word “pure” in Hou et al. (2013) means that the joint probability distribution of these words is significantly different from the product of lowerorder joint distributions or marginal distributions, w.r.t all possible decompositions. More formally, it requires that the joint distribution cannot be factorized unconditionally (UPD) or conditionally (CPD) in the language of graphical model. Let xi ∈ {0, 1} denote the value of Xi. Let p(x), x = [x1, x2, ... , xn]T, be the joint probability distribution over X. Then the definitions of UPD and CPD are as follows: DEFINITION 2. (UPD): X = {X1, ... , Xn} is of n-order Unconditional </context>
<context position="9467" citStr="Hou et al., 2013" startWordPosition="1549" endWordPosition="1552"> 2552 p(c1) * p(c2)...p(ck), where p(ci), i = 1, ... , k, is the joint distribution over Ci. (Hou et al., 2013) DEFINITION 3. (CPD): X = {X1, ... , Xn} is of n-order Conditional Pure Dependence (CPD), iff it can NOT be conditionally factorized, i.e., there does NOT exist C0 C X and a k-partition {C1, C2, ..., Ck} of V = X − C0, k &gt; 1, such at p(v|c0) = p(c1|c0) * p(c2|c0)...p(ck|c0), where p(v|c0) is the conditional joint distribution over V given C0, and p(ci|c0), i = 1, 2, ... , k, is the conditional joint distribution over Ci given C0. In case that C0 is an empty set, we define p(c0) = 1. (Hou et al., 2013) Actually, CPD is stricter than UPD, and the dependence which just satisfies UPD is not pure enough to model the global context. Therefore, “pure” in our paper refers to the characteristic of CPD. However judging CPD is NP-hard. It is proved that a significant nonzero n-order 0 parameter (TPD) entails the n-order CPD/UPD in Hou et al. (2013). The highest-order coordinate parameter in IG is a proper metric for the purity (i.e., the unique semantics) of high-order dependence. A pattern is TPD, iff the n-order 0 coordinate 012...n is significantly different from zero. Moreover, The Log Likelihood</context>
</contexts>
<marker>Hou, Zhao, Song, Li, 2013</marker>
<rawString>Yuexian Hou, Xiaozhao Zhao, Dawei Song, and Wenjie Li. 2013. Mining pure high-order word associations via information geometry for information retrieval. ACM Transactions on Information Systems (TOIS), 31(3):12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1300" citStr="Huang et al., 2012" startWordPosition="179" endWordPosition="182">einforce topical and sentimental information. TPD has a theoretical guarantee that the word dependency is pure, i.e., the dependence pattern has the integral meaning whose underlying distribution can not be conditionally factorized. Our method outperforms the state-of-the-art performance on text classification tasks. 1 Introduction Word embeddings can be learned by training a neural probabilistic language model or a unified neural network architecture for various NLP tasks (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011). In global context-aware neural language model (Huang et al., 2012), the global context vector is a weighted average of all word embeddings of a single document/paragraph. After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained. Actually, Le and Mikolov’s Paragraph Vector (Le and Mikolov, 2014) is trained based on the log-linear neural language model (Mikolov et al., 2013a). For text classification, using a straightforward extension of language model (e.g. Le and Mikolov’s Paragraph Vector) is considered not to be sensible. Embeddings learned for text classification should be very different from </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Johnson</author>
<author>Tong Zhang</author>
</authors>
<title>Semi-supervised learning with multi-view embedding: Theory and application with convolutional neural networks. arXiv preprint arXiv:1504.01255.</title>
<date>2015</date>
<contexts>
<context position="17070" citStr="Johnson and Zhang, 2015" startWordPosition="2843" endWordPosition="2846"> 90.5 90 2554 Table 1: The performance of our method comTable 3: Nearest neighbors of words ranking list based on cosine similarity. pared with other approaches on the IMDB dataset. Model Accuracy rate BoW (bnc) (Maas et al., 2011) 87.80% Full+Unlabeled+BoW (Maas et al., 2011) 88.89% WRRBM (Dahl et al., 2012) 87.42% WRRBM + BoW (bnc) (Dahl et al., 2012) 89.23% SVM-bi (Wang and Manning, 2012) 89.16% NBSVM-bi (Wang and Manning, 2012) 91.22% PV-DBOW (Le and Mikolov, 2014) 90.79% Paragraph Vector (Le and Mikolov, 2014) 92.58% Sentence Vector + RNN-LM + NB-LM 92.57% (Mesnil et al., 2014) mvCNNo&amp;w (Johnson and Zhang, 2015) 93.34% Glo-PV-DBOW-Apri-2 93.76% Glo-PV-DBOW-Apri-5 92.41% Glo-PV-DBOW-TPD-2 94.83% Glo-PV-DBOW-TPD-5 95.05% Table 2: The performance of our method compared to other approaches on 20 Newsgroup. Model Comp Sci Confidence-weighted 94.39% 97.56% (Crammer et al., 2012) 92.60% 98.02% PV-DBOW (Le and Mikolov, 2014) Glo-PV-DBOW-Apri-2 94.56% 98.42% Glo-PV-DBOW-Apri-5 94.43% 98.13% Glo-PV-DBOW-TPD-2 94.59% 99.20% Glo-PV-DBOW-TPD-5 95.47% 98.74% is recorded in Table 2. Compared with Confidence-weighted (Crammer et al., 2012) and PV-DBOW (Le and Mikolov, 2014), our extended models achieve the highest a</context>
</contexts>
<marker>Johnson, Zhang, 2015</marker>
<rawString>Rie Johnson and Tong Zhang. 2015. Semi-supervised learning with multi-view embedding: Theory and application with convolutional neural networks. arXiv preprint arXiv:1504.01255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML-14),</booktitle>
<pages>1188--1196</pages>
<contexts>
<context position="1592" citStr="Mikolov, 2014" startWordPosition="229" endWordPosition="230">classification tasks. 1 Introduction Word embeddings can be learned by training a neural probabilistic language model or a unified neural network architecture for various NLP tasks (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011). In global context-aware neural language model (Huang et al., 2012), the global context vector is a weighted average of all word embeddings of a single document/paragraph. After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained. Actually, Le and Mikolov’s Paragraph Vector (Le and Mikolov, 2014) is trained based on the log-linear neural language model (Mikolov et al., 2013a). For text classification, using a straightforward extension of language model (e.g. Le and Mikolov’s Paragraph Vector) is considered not to be sensible. Embeddings learned for text classification should be very different from that learned for language modeling. For example, language Corresponding authors: Yuexian Hou and Peng Zhang. models often calculate the probability of a sentence, therefore this is a good movie and this is a bad movie may not be discriminated from each other. In sentiment analysis task, the </context>
<context position="10866" citStr="Mikolov (2014)" startWordPosition="1778" endWordPosition="1779">wo negative examples is much weaker. In conclusion, can, with, of cannot give an explicit topic and London, Chelsea, Sherlock Holmes includes at least two topics. the co-occurrence of words in TPD (e.g. climate, conference, Copenhagen) implies an un-separable (pure) high-level semantic entity. A sufficient and unbroken meaning of dependence can not only supply the context but also avoid the ambiguity (or noise) in global context. Therefore, the meaning of pure is important in such a global context modeling method. 3 Global PV-DBOW and Dependence Vectors A version of Paragraph Vector in Le and Mikolov (2014) PV-DBOW is extended with TPD to a new model: Global PV-DBOW (Glo-PV-DBOW). TPD has been extracted from the corpus before training. Given a sequence of training words w1, w2, w3, ... , wT and the global context glot of wt, the objective of Glo-PV-DBOW is to maximize the average log probability: [ ∑ log p(wt|wt+j) −c&lt;j&lt;c,j̸=0 ]+ log p(wt|glot) + log p(wt|doct) where c is the local context window size. The indicator of the document that the current word wt belongs to is denoted by doct. Further, we define p(wt|glot) in equation (2): p(wt|glot) [] (2) p(wt|depa t )p(wt|wa 1,wa 2, ... , wa N) The </context>
<context position="15281" citStr="Mikolov (2014)" startWordPosition="2564" endWordPosition="2565">re, the Apriori algorithm is not suitable for generating the high-order dependence. IMDB PV−DBOW Apri−2 Apri−5 TPD−2 TPD−5 Methods Figure 1: Box plot of classification accuracy over a local method (PV-DBOW) and 4 global methods (Apri-2/5, TPD-2/5). Instead, the high-order TPD can provide the rich and explicit global context for the model. Meanwhile, it is verified that our method is good at capturing sentiment contrast. Table 1 shows that Glo-PV-DBOW with 5-order TPD achieves the state-of-the-art performance. A promising result is an improvement of more than 2% over result published in Le and Mikolov (2014). Note that the algorithm process of Paragraph Vector (Le and Mikolov, 2014) is much more complex than PV-DBOW’s. Paragraph Vector includes an extra inference stage. In addition, Paragraph Vector’s document vector is a combination of two vectors: one learned by PV-DBOW and the other learned by Distributed Memory Model of Paragraph Vectors (PV-DM) (Le and Mikolov, 2014). The combined document vector has 800 dimensions, while all vectors in our experiments only have 100 dimensions. 4.2 Topic Discovery on News The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, </context>
<context position="16919" citStr="Mikolov, 2014" startWordPosition="2821" endWordPosition="2822">cted for each problem. The classification accuracy on each sub-dataset 95 Test Classification Accuracy (perc) 94.5 94 93.5 93 92.5 92 91.5 91 90.5 90 2554 Table 1: The performance of our method comTable 3: Nearest neighbors of words ranking list based on cosine similarity. pared with other approaches on the IMDB dataset. Model Accuracy rate BoW (bnc) (Maas et al., 2011) 87.80% Full+Unlabeled+BoW (Maas et al., 2011) 88.89% WRRBM (Dahl et al., 2012) 87.42% WRRBM + BoW (bnc) (Dahl et al., 2012) 89.23% SVM-bi (Wang and Manning, 2012) 89.16% NBSVM-bi (Wang and Manning, 2012) 91.22% PV-DBOW (Le and Mikolov, 2014) 90.79% Paragraph Vector (Le and Mikolov, 2014) 92.58% Sentence Vector + RNN-LM + NB-LM 92.57% (Mesnil et al., 2014) mvCNNo&amp;w (Johnson and Zhang, 2015) 93.34% Glo-PV-DBOW-Apri-2 93.76% Glo-PV-DBOW-Apri-5 92.41% Glo-PV-DBOW-TPD-2 94.83% Glo-PV-DBOW-TPD-5 95.05% Table 2: The performance of our method compared to other approaches on 20 Newsgroup. Model Comp Sci Confidence-weighted 94.39% 97.56% (Crammer et al., 2012) 92.60% 98.02% PV-DBOW (Le and Mikolov, 2014) Glo-PV-DBOW-Apri-2 94.56% 98.42% Glo-PV-DBOW-Apri-5 94.43% 98.13% Glo-PV-DBOW-TPD-2 94.59% 99.20% Glo-PV-DBOW-TPD-5 95.47% 98.74% is reco</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1188–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<date>2011</date>
<contexts>
<context position="14262" citStr="Maas et al., 2011" startWordPosition="2401" endWordPosition="2404">nd set the local context window size as 10, the dimension of embeddings as 100. In sentiment analysis task, Apriori’s minimum support and TPD’s theta 0 is respectively set as 0.004 and 1.4. While in topic discovery task, Apriori’s minimum support and TPD’s theta 0 is around 0.020 and 2.0 respectively. Since the classification accuracy of the approaches compared is a single result, we do not include any results for test of significance in our method and only report the average accuracy. 4.1 Sentiment Analysis on Movie Reviews The binary sentiment classification on the IMDB dataset proposed by (Maas et al., 2011) is conducted. Results in Fig.1 show that global methods’ performance is more stable than PV-DBOW’s. Moreover, TPD works much better than Apriori, especially in the high-order dependence. Note that TPD-5 works better than TPD-2, while Apri5 works worse than Apri-2. It can be explained that the Apriori algorithm is short of an explicit statistical test procedure to guarantee the pure dependence. Therefore, the Apriori algorithm is not suitable for generating the high-order dependence. IMDB PV−DBOW Apri−2 Apri−5 TPD−2 TPD−5 Methods Figure 1: Box plot of classification accuracy over a local metho</context>
<context position="16677" citStr="Maas et al., 2011" startWordPosition="2780" endWordPosition="2783">wo similar groups. Therefore, the dataset is split into two sub-datasets as follows: comp: comp.sys.ibm.pc.hardware vs. comp.sys.mac.hardware and sci: sci.electronics vs. sci.med. Similarly, 1800 examples balanced between the two labels were selected for each problem. The classification accuracy on each sub-dataset 95 Test Classification Accuracy (perc) 94.5 94 93.5 93 92.5 92 91.5 91 90.5 90 2554 Table 1: The performance of our method comTable 3: Nearest neighbors of words ranking list based on cosine similarity. pared with other approaches on the IMDB dataset. Model Accuracy rate BoW (bnc) (Maas et al., 2011) 87.80% Full+Unlabeled+BoW (Maas et al., 2011) 88.89% WRRBM (Dahl et al., 2012) 87.42% WRRBM + BoW (bnc) (Dahl et al., 2012) 89.23% SVM-bi (Wang and Manning, 2012) 89.16% NBSVM-bi (Wang and Manning, 2012) 91.22% PV-DBOW (Le and Mikolov, 2014) 90.79% Paragraph Vector (Le and Mikolov, 2014) 92.58% Sentence Vector + RNN-LM + NB-LM 92.57% (Mesnil et al., 2014) mvCNNo&amp;w (Johnson and Zhang, 2015) 93.34% Glo-PV-DBOW-Apri-2 93.76% Glo-PV-DBOW-Apri-5 92.41% Glo-PV-DBOW-TPD-2 94.83% Glo-PV-DBOW-TPD-5 95.05% Table 2: The performance of our method compared to other approaches on 20 Newsgroup. Model Comp S</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011.</rawString>
</citation>
<citation valid="false">
<title>Learning word vectors for sentiment analysis.</title>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>142--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker></marker>
<rawString>Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 142–150. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire Mesnil</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews. arXiv preprint arXiv:1412.5335.</title>
<date>2014</date>
<contexts>
<context position="17035" citStr="Mesnil et al., 2014" startWordPosition="2838" endWordPosition="2841">94.5 94 93.5 93 92.5 92 91.5 91 90.5 90 2554 Table 1: The performance of our method comTable 3: Nearest neighbors of words ranking list based on cosine similarity. pared with other approaches on the IMDB dataset. Model Accuracy rate BoW (bnc) (Maas et al., 2011) 87.80% Full+Unlabeled+BoW (Maas et al., 2011) 88.89% WRRBM (Dahl et al., 2012) 87.42% WRRBM + BoW (bnc) (Dahl et al., 2012) 89.23% SVM-bi (Wang and Manning, 2012) 89.16% NBSVM-bi (Wang and Manning, 2012) 91.22% PV-DBOW (Le and Mikolov, 2014) 90.79% Paragraph Vector (Le and Mikolov, 2014) 92.58% Sentence Vector + RNN-LM + NB-LM 92.57% (Mesnil et al., 2014) mvCNNo&amp;w (Johnson and Zhang, 2015) 93.34% Glo-PV-DBOW-Apri-2 93.76% Glo-PV-DBOW-Apri-5 92.41% Glo-PV-DBOW-TPD-2 94.83% Glo-PV-DBOW-TPD-5 95.05% Table 2: The performance of our method compared to other approaches on 20 Newsgroup. Model Comp Sci Confidence-weighted 94.39% 97.56% (Crammer et al., 2012) 92.60% 98.02% PV-DBOW (Le and Mikolov, 2014) Glo-PV-DBOW-Apri-2 94.56% 98.42% Glo-PV-DBOW-Apri-5 94.43% 98.13% Glo-PV-DBOW-TPD-2 94.59% 99.20% Glo-PV-DBOW-TPD-5 95.47% 98.74% is recorded in Table 2. Compared with Confidence-weighted (Crammer et al., 2012) and PV-DBOW (Le and Mikolov, 2014), our ex</context>
</contexts>
<marker>Mesnil, Ranzato, Mikolov, Bengio, 2014</marker>
<rawString>Gr´egoire Mesnil, Marc’Aurelio Ranzato, Tomas Mikolov, and Yoshua Bengio. 2014. Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews. arXiv preprint arXiv:1412.5335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<journal>ICLR Workshop.</journal>
<contexts>
<context position="1671" citStr="Mikolov et al., 2013" startWordPosition="240" endWordPosition="243">aining a neural probabilistic language model or a unified neural network architecture for various NLP tasks (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011). In global context-aware neural language model (Huang et al., 2012), the global context vector is a weighted average of all word embeddings of a single document/paragraph. After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained. Actually, Le and Mikolov’s Paragraph Vector (Le and Mikolov, 2014) is trained based on the log-linear neural language model (Mikolov et al., 2013a). For text classification, using a straightforward extension of language model (e.g. Le and Mikolov’s Paragraph Vector) is considered not to be sensible. Embeddings learned for text classification should be very different from that learned for language modeling. For example, language Corresponding authors: Yuexian Hou and Peng Zhang. models often calculate the probability of a sentence, therefore this is a good movie and this is a bad movie may not be discriminated from each other. In sentiment analysis task, the semantic representation of words needs to tell word good from bad, even if the </context>
<context position="12358" citStr="Mikolov et al., 2013" startWordPosition="2056" endWordPosition="2059">is uniform as follows: E(wt, wi) = −vwt T vwi (3) We define the energy function of TPD words: ∑N 1 E(wt, wa 1, wa 2, ... wa N) = − N The resulting predictive distributions are given by exp(vwtTvwi) p(wt|wi) = W T (5) ∑m=1 exp(vw� vwi) p(wt|wa1,wa 2,...,wa N) exp(1 N T N ∑n=1 vwt vw° ) = ∑m=1 exp( N ∑n=1 vw�T vw° ) Hierarchical softmax (Morin and Bengio, 2005) is adopted to reduce the cost of computation. The binary tree is specified with a Huffman tree, and the Huffman code of pseudo words mi in wt’s Huffman path is denoted as xmi. For more about hierarchical softmax we used, please refer to (Mikolov et al., 2013b). Using stochastic gradient descent (SGD), distributed representations of the word, dependence and document have been trained. The update procedure of vwi = (vwt+j, vdoct, vdept) is as same as the procedure described in (Mikolov et al., 2013b). Thus, the pseudo code for training TPD words is listed individually: 1 L= T ∑T t=1 (1) = A ∏ a T (4) vwt vw° n=1 (6) 2553 SGD FOR TRAINING THE TPD WORDS 1 vwt +— current word N 2 vwa 1 ave N En=1 vw° 3 err +— 0 4 for Vmi \l\l 5 do g +— (1 — xmi — U(vmiT vwaave// * a&apos; 6 err+ = g* 1 N * vmi 7 mi+ = g * vwaave 8 for n +— 1 to N 9 do vwan + = err 4 Experi</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. ICLR Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1671" citStr="Mikolov et al., 2013" startWordPosition="240" endWordPosition="243">aining a neural probabilistic language model or a unified neural network architecture for various NLP tasks (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011). In global context-aware neural language model (Huang et al., 2012), the global context vector is a weighted average of all word embeddings of a single document/paragraph. After trained with all word embeddings belonging to the current paragraph, a resulting Paragraph Vector can be obtained. Actually, Le and Mikolov’s Paragraph Vector (Le and Mikolov, 2014) is trained based on the log-linear neural language model (Mikolov et al., 2013a). For text classification, using a straightforward extension of language model (e.g. Le and Mikolov’s Paragraph Vector) is considered not to be sensible. Embeddings learned for text classification should be very different from that learned for language modeling. For example, language Corresponding authors: Yuexian Hou and Peng Zhang. models often calculate the probability of a sentence, therefore this is a good movie and this is a bad movie may not be discriminated from each other. In sentiment analysis task, the semantic representation of words needs to tell word good from bad, even if the </context>
<context position="12358" citStr="Mikolov et al., 2013" startWordPosition="2056" endWordPosition="2059">is uniform as follows: E(wt, wi) = −vwt T vwi (3) We define the energy function of TPD words: ∑N 1 E(wt, wa 1, wa 2, ... wa N) = − N The resulting predictive distributions are given by exp(vwtTvwi) p(wt|wi) = W T (5) ∑m=1 exp(vw� vwi) p(wt|wa1,wa 2,...,wa N) exp(1 N T N ∑n=1 vwt vw° ) = ∑m=1 exp( N ∑n=1 vw�T vw° ) Hierarchical softmax (Morin and Bengio, 2005) is adopted to reduce the cost of computation. The binary tree is specified with a Huffman tree, and the Huffman code of pseudo words mi in wt’s Huffman path is denoted as xmi. For more about hierarchical softmax we used, please refer to (Mikolov et al., 2013b). Using stochastic gradient descent (SGD), distributed representations of the word, dependence and document have been trained. The update procedure of vwi = (vwt+j, vdoct, vdept) is as same as the procedure described in (Mikolov et al., 2013b). Thus, the pseudo code for training TPD words is listed individually: 1 L= T ∑T t=1 (1) = A ∏ a T (4) vwt vw° n=1 (6) 2553 SGD FOR TRAINING THE TPD WORDS 1 vwt +— current word N 2 vwa 1 ave N En=1 vw° 3 err +— 0 4 for Vmi \l\l 5 do g +— (1 — xmi — U(vmiT vwaave// * a&apos; 6 err+ = g* 1 N * vmi 7 mi+ = g * vwaave 8 for n +— 1 to N 9 do vwan + = err 4 Experi</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the international workshop on artificial intelligence and statistics,</booktitle>
<pages>246--252</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="12099" citStr="Morin and Bengio, 2005" startWordPosition="2008" endWordPosition="2011">of the a-th wt’s TPD pattern is denoted as depat and can be trained to be a distributed representation of TPD: dependence vector vdept . This (N+1)-order TPD consists of N+1 words: wa1, wa2... wa N and wt. The energy function of wt and wi = (wt+j, doct, depat ) is uniform as follows: E(wt, wi) = −vwt T vwi (3) We define the energy function of TPD words: ∑N 1 E(wt, wa 1, wa 2, ... wa N) = − N The resulting predictive distributions are given by exp(vwtTvwi) p(wt|wi) = W T (5) ∑m=1 exp(vw� vwi) p(wt|wa1,wa 2,...,wa N) exp(1 N T N ∑n=1 vwt vw° ) = ∑m=1 exp( N ∑n=1 vw�T vw° ) Hierarchical softmax (Morin and Bengio, 2005) is adopted to reduce the cost of computation. The binary tree is specified with a Huffman tree, and the Huffman code of pseudo words mi in wt’s Huffman path is denoted as xmi. For more about hierarchical softmax we used, please refer to (Mikolov et al., 2013b). Using stochastic gradient descent (SGD), distributed representations of the word, dependence and document have been trained. The update procedure of vwi = (vwt+j, vdoct, vdept) is as same as the procedure described in (Mikolov et al., 2013b). Thus, the pseudo code for training TPD words is listed individually: 1 L= T ∑T t=1 (1) = A ∏ a</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Nakahara</author>
<author>Shun-ichi Amari</author>
</authors>
<title>Information-geometric measure for neural spikes.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>10</issue>
<contexts>
<context position="6584" citStr="Nakahara and Amari, 2002" startWordPosition="1054" endWordPosition="1057">at the dependency has an integral meaning whose underlying distribution can not be conditionally factorized. Formally, given a set of binary random variables X = {X1, ... , Xn}, where Xi denotes the occurrence (Xi = 1) or absence (Xi = 0) of the i-th word. Then the n-order TPD over X can be defined as follows. DEFINITION 1. (TPD): X = {X1, ... , Xn} is of n-order Theta Pure Dependence (TPD), iff the n-order 0 coordinate 012...n is significantly different from zero. (Hou et al., 2013) TPD can be effectively identified by an explicit statistical test procedure: Log Likelihood Ratio Test (LLRT) (Nakahara and Amari, 2002) for 0- coordinate of IG. (Hou et al., 2013) Here, we introduce two negative examples to further emphasize the importance of utilizing TPD. Example 1: can, with, of. The joint distribution of this words combination can be unconditionally factorized directly, since the occurrence of any word does not necessarily imply the occurrence of others. Example 2: London, Chelsea, Sherlock Holmes. As we all know, both Chelsea and Sherlock Holmes are closely related to London. Chelsea and Sherlock Holmes are two relatively independent topics, i.e. they are conditional independent given London. Although th</context>
</contexts>
<marker>Nakahara, Amari, 2002</marker>
<rawString>Hiroyuki Nakahara and Shun-ichi Amari. 2002. Information-geometric measure for neural spikes. Neural Computation, 14(10):2269–2316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>90--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16840" citStr="Wang and Manning, 2012" startWordPosition="2807" endWordPosition="2810">ctronics vs. sci.med. Similarly, 1800 examples balanced between the two labels were selected for each problem. The classification accuracy on each sub-dataset 95 Test Classification Accuracy (perc) 94.5 94 93.5 93 92.5 92 91.5 91 90.5 90 2554 Table 1: The performance of our method comTable 3: Nearest neighbors of words ranking list based on cosine similarity. pared with other approaches on the IMDB dataset. Model Accuracy rate BoW (bnc) (Maas et al., 2011) 87.80% Full+Unlabeled+BoW (Maas et al., 2011) 88.89% WRRBM (Dahl et al., 2012) 87.42% WRRBM + BoW (bnc) (Dahl et al., 2012) 89.23% SVM-bi (Wang and Manning, 2012) 89.16% NBSVM-bi (Wang and Manning, 2012) 91.22% PV-DBOW (Le and Mikolov, 2014) 90.79% Paragraph Vector (Le and Mikolov, 2014) 92.58% Sentence Vector + RNN-LM + NB-LM 92.57% (Mesnil et al., 2014) mvCNNo&amp;w (Johnson and Zhang, 2015) 93.34% Glo-PV-DBOW-Apri-2 93.76% Glo-PV-DBOW-Apri-5 92.41% Glo-PV-DBOW-TPD-2 94.83% Glo-PV-DBOW-TPD-5 95.05% Table 2: The performance of our method compared to other approaches on 20 Newsgroup. Model Comp Sci Confidence-weighted 94.39% 97.56% (Crammer et al., 2012) 92.60% 98.02% PV-DBOW (Le and Mikolov, 2014) Glo-PV-DBOW-Apri-2 94.56% 98.42% Glo-PV-DBOW-Apri-5 94.43%</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90–94. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011785">
<title confidence="0.9966385">
Combining Geometric, Textual and Visual Features for Predicting
Prepositions in Image Descriptions
</title>
<author confidence="0.9984815">
Arnau Ramisa*1 Josiah Wang*2 Ying Lu3 Emmanuel Dellandrea3
Francesc Moreno-Noguer1 Robert Gaizauskas2
</author>
<affiliation confidence="0.962152">
1 Institut de Rob`otica i Inform`atica Industrial (UPC-CSIC), Barcelona, Spain
2 Department of Computer Science, University of Sheffield, UK
</affiliation>
<address confidence="0.523771">
3 LIRIS, Ecole Centrale de Lyon, France
</address>
<email confidence="0.7830575">
{aramisa, fmoreno}@iri.upc.edu {j.k.wang, r.gaizauskas}@sheffield.ac.uk
{ying.lu, emmanuel.dellandrea}@ec-lyon.fr
</email>
<sectionHeader confidence="0.989482" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999794333333333">
We investigate the role that geometric, tex-
tual and visual features play in the task
of predicting a preposition that links two
visual entities depicted in an image. The
task is an important part of the subsequent
process of generating image descriptions.
We explore the prediction of prepositions
for a pair of entities, both in the case when
the labels of such entities are known and
unknown. In all situations we found clear
evidence that all three features contribute
to the prediction task.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999655">
In recent years, there has been an increased in-
terest in the task of automatic generation of natu-
ral language image descriptions at sentence level,
compared to earlier work that annotates images
with a laundry list of terms (Duygulu et al., 2002).
The task is important in that such detailed anno-
tations are more informative and discriminative
compared to isolated textual labels, and are essen-
tial for improved text and image retrieval.
The most standard approach to generating such
descriptions involves first detecting instances of
pre-defined concepts in the image, and then rea-
soning about these concepts to generate image de-
scriptions e.g. (Kulkarni et al., 2011; Yang et al.,
2011). Our work is also based on this paradigm.
However, we assume that object instances have
already been pre-detected by visual recognisers,
and concentrate on a specific subtask of descrip-
tion generation. More specifically, given two vi-
sual entity instances where one could potentially
act as a modifier to the other, we address the prob-
lem of identifying the appropriate preposition to
connect these two entities (Figure 1). The inferred
prepositional relations will subsequently act as an
</bodyText>
<note confidence="0.666157">
*A. Ramisa and J. Wang contributed equally to this work.
</note>
<figureCaption confidence="0.870182666666667">
Figure 1: Given a subject boy and an object sled
and their location in the image, what would the
best preposition be to connect the two entities?
</figureCaption>
<bodyText confidence="0.990932260869565">
important intermediate representation towards the
eventual goal of generating image descriptions.
The main contribution of this paper is therefore
to learn to predict the most suitable preposition
given its context, and to learn this jointly from im-
ages and their descriptions. In particular, we con-
centrate on learning from (i) geometric relations
between two visual entities from image annota-
tions; (ii) textual features from textual descrip-
tions; (iii) visual features from images. Previous
work exists (Yang et al., 2011) that uses text cor-
pora to ‘guess’ the prepositions given the context
without considering the appropriate spatial rela-
tions between the entities in the image, signifying
a gap between visual content and its correspond-
ing description. For example, although person
on horse might commonly occur in text corpora,
a particular image might actually depict a person
standing beside a horse. On the other hand, work
that does consider the image content for generat-
ing prepositions (Kulkarni et al., 2011; Elliott and
Keller, 2013) map geometric relations to a limited
set of prepositions using manually defined rules,
</bodyText>
<page confidence="0.982869">
214
</page>
<note confidence="0.658218">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 214–220,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999868928571429">
not as humans would naturally use them with a
richer vocabulary. We would like to have the best
of both worlds, by considering image content as
well as textual information to select the preposi-
tion best used to express the relation between two
entities. Our hypothesis is that the combination
of geometric, textual and visual features can help
with the task of predicting the most appropriate
preposition, since incorporating geometric and vi-
sual information should help generate a relation
that is consistent with the image content, whilst
incorporating textual information should help gen-
erate a description that is consistent with natural
language.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999908476190476">
The Natural Language Processing Community has
significant interest in different aspects of prepo-
sitions. The Prepositions Project (Litkowski and
Hargraves, 2005) analysed and produced a lex-
icon of English prepositions and their senses,
and subsequently used them in the Word Sense
Disambiguation of Prepositions task in SemEval-
2007 (Litkowski and Hargraves, 2007). In
SemEval-2012, Kordjamshidi et al. (2012) intro-
duce the more fine-grained task of spatial role
labelling to detect and classify spatial relations
expressed by triples (trajector, landmark, spa-
tial indicator). In the latest edition of SemEval-
2015, the SpaceEval task (Pustejovsky et al.,
2015) introduce further tasks of identifying spatial
and motion signals, as well as spatial configura-
tions/orientation and motion relation.
In work that links prepositions more strongly
to image content, Gupta and Davis (2008) model
prepositions implicitly to disambiguate image re-
gions, rather than for predicting prepositions.
Their work also require manual annotation of
prepositional relations. In image description gen-
eration work, Kulkarni et al. (2011) manually map
spatial relations to pre-defined prepositions, whilst
Yang et al. (2011) predict prepositions from large-
scale text corpora solely based on the complement
term, with the prepositions constrained to describ-
ing scenes (on the street). Elliott and Keller (2013)
define a list of eight spatial relations and their cor-
responding prepositional term for sentence gener-
ation. Although they also present alternative mod-
els that use text corpora for descriptions that are
more human-like, they are limited to verbs and
do not cover prepositions. Le et al. (2014) exam-
ine prepositions modifying human actions (verbs),
and conclude that these relate to positional infor-
mation to a certain extent. Other related work in-
clude training classifiers for prepositions with spa-
tial relation features to improve image segmenta-
tion and detection (Fidler et al., 2013); this work
is however limited to four prepositions.
</bodyText>
<sectionHeader confidence="0.990104" genericHeader="method">
3 Task Definition
</sectionHeader>
<bodyText confidence="0.999990642857143">
We formally define the task of predicting prepo-
sitions as follows: Let P be the set of possible
prepositions. Let L be the set of possible land-
mark entities acting as the complement of a prepo-
sition, and let T be the set of possible trajector
entities modified by the prepositional phrase com-
prising a preposition and its landmark1. For exam-
ple, for the phrase person on bicycle, on would be
the preposition, bicycle the landmark, and person
the trajector. For this paper, we constrain trajector
and landmark to be entities that are visually iden-
tifiable in an image since we are interested in dis-
covering the role of visual features and geometric
configurations between two entities in the prepo-
sition prediction task.
Let D = {d1, d2,..., dN} be the set of N ob-
servations, where each di for i = 1, 2..., N is rep-
resented by di = (xi, yi, ri), where xi and yi are
the feature representations for the trajector and the
landmark entities respectively, and ri the relative
geometric feature between the two visual entities.
Given di, the objective of the preposition pre-
diction task is to produce a ranked list of preposi-
tions (p1, p2, ...p|P|) according to how likely they
are to express the appropriate spatial relation be-
tween the given trajector and landmark entities
that are either known (Section 6.1) or only repre-
sented by visual features (Section 6.2).
</bodyText>
<sectionHeader confidence="0.998124" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.9996208">
We base the preposition prediction task on two
large-scale image datasets with human authored
descriptions, namely MSCOCO (Lin et al., 2014)
and Flickr30k (Young et al., 2014; Plummer et al.,
2015). To extract instances of triples (trajector,
preposition, landmark) from image descriptions,
we used the Neural Network, transition-based de-
pendency parser of Chen and Manning (2014) as
implemented in Stanford CoreNLP (Manning et
al., 2014). Dependencies signifying prepositional
</bodyText>
<footnote confidence="0.9192655">
1The terminologies trajector and landmark are adopted
from spatial role labelling (Kordjamshidi et al., 2011)
</footnote>
<page confidence="0.996515">
215
</page>
<bodyText confidence="0.442927">
Bounding Box feature (number of dimensions)
</bodyText>
<listItem confidence="0.9994906">
• Vector (x, y) from centroid of trajector to centroid of
landmark, normalised by the size of the bounding box
enclosing both objects (2)
• Area of trajector bounding box relative to landmark (1)
• Aspect ratio of each bounding box (2)
• Area of each bounding box w.r.t. enclosing box (2)
• Intersection over union of the bounding boxes (1)
• Euclidean distance between the trajector and landmark
bounding boxes, normalised by the image size (1)
• Area of each bounding box w.r.t. the whole image (2)
</listItem>
<tableCaption confidence="0.9586545">
Table 1: Geometric features derived from bound-
ing boxes.
</tableCaption>
<bodyText confidence="0.998762413793103">
relations are retained where both the governor and
its dependent overlap with the entity mentions in
the descriptions, and where both mentions have
corresponding bounding boxes. The MSCOCO
validation set is further annotated to remove er-
rors arising from dependency parsing (notably PP
attachment errors), and is used as our clean test
set. Our final dataset comprises 8,029 training
and 3,431 test instances for MSCOCO, and 46,847
training and 20,010 test instances for Flickr30k.
Details on how the triples were extracted from
captions and matched to instances in images are
available in the supplementary material.
We consider two variants of trajector and land-
mark terms in our experiments: (i) using the
provided high level categories as terms (80 for
MSCOCO and 8 for Flickr30k); (ii) using the
terms occurring in the sentence directly, which
constitute a bigger and more realistic challenge.
For Flickr30k, the descriptive phrases may cause
data sparseness (the furry, black and white dog).
Thus, we extracted the lemmatised head word of
each phrase, using a ‘semantic head’ variant of
the head finding rules of Collins (2003) in Stan-
ford CoreNLP. Entities from the same coreference
chain are denoted with a common head noun cho-
sen by majority vote among the group, with ties
broken by the most frequent head noun in the cor-
pus, and further ties broken at random.
</bodyText>
<sectionHeader confidence="0.999707" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999927214285714">
Geometric Features: Geometric features be-
tween a trajector and a landmark entity are derived
from bounding box annotations. We defined an
11-dimensional vector of bounding box features,
covering geometric relations such as distance, ori-
entation, relative bounding box sizes and overlaps
between bounding boxes (Table 1). We chose to
use continuous features as we felt these may be
more powerful and expressive compared to dis-
crete, binned features. Despite some of these fea-
tures being correlated, we left it to the classifier to
determine the most useful features for discrimina-
tion without having to withhold any unnecessarily.
Textual features: We consider two textual fea-
tures to encode the trajector and landmark terms
wt� and wz. The first feature is a one-hot indica-
tor vector xZ and yz for the trajector and land-
mark respectively, where xzt = 1 if index t cor-
responds to the trajector term wt� and 0 elsewhere
(and similarly for landmark). As data sparseness
may be an issue, we also explore an alternative tex-
tual feature which encodes the terms as word2vec
embeddings (Mikolov et al., 2013). This encodes
each term as a vector such that semantically re-
lated terms are close in the vector space. This al-
lows information to be transferred across seman-
tically related terms during training (e.g. infor-
mation from person on boat can help predict the
preposition that mediates man and boat).
Image Features: While it is ideal to have vi-
sion systems produce a firm decision about the vi-
sual entity instance detected in an image, in real-
ity it may be beneficial to defer the decision by
allowing several possible interpretations of the in-
stance being detected. In such cases, we will not
have a single concept label for the entity, but in-
stead a high-level visual representation. For this
scenario, we extracted visual representations from
the final layer of a Convolutional Neural Network
trained on ImageNet (Krizhevsky et al., 2012), and
used them as representations for entity instances in
place of textual features.
</bodyText>
<sectionHeader confidence="0.987692" genericHeader="method">
6 Preposition Prediction
</sectionHeader>
<bodyText confidence="0.999899">
Here we highlight interesting findings from exper-
iments performed for the task of predicting prepo-
sitions for two different scenarios (Sections 6.1
and 6.2). Detailed results can be found in the sup-
plementary material.
Evaluation metrics. As there may be more than
one ‘correct’ preposition for a given context (per-
son on horse and person atop horse), we pro-
pose the mean rank of the correct preposition as
the main evaluation metric, as it accommodates
</bodyText>
<page confidence="0.997595">
216
</page>
<table confidence="0.999755333333333">
IND W2V GF IND+GF W2V+GF Baseline
MSCOCO (max rank 17) 1.45 1.43 1.72 1.44 1.42 2.14
MSCOCO (balanced) 3.20 3.10 4.60 3.00 2.90 5.40
Flickr30k (max rank 52) 1.91 1.87 2.35 1.88 1.85 2.54
Flickr30k (balanced) 11.10 9.04 15.55 10.23 8.90 15.13
MSCOCO 79.7% 80.3% 68.4% 79.8% 80.4% 40.2%
MSCOCO (balanced) 52.5% 54.2% 31.5% 52.7% 53.9% 11.9%
Flickr30k 75.4% 75.2% 58.5% 75.8% 75.4% 53.7%
Flickr30k (balanced) 24.6% 25.9% 9.0% 25.2% 26.9% 4.0%
</table>
<tableCaption confidence="0.993318">
Table 2: Top: Mean rank of the correct preposition (lower is better). Bottom: Accuracy with different
</tableCaption>
<bodyText confidence="0.956897333333333">
feature configurations. All results are with the original trajector/landmark terms from descriptions. IND
stands for Indicator Vectors, W2V for Word2Vec, and GF for Geometric Features. As baseline we rank
the prepositions by their relative frequencies in the training dataset.
</bodyText>
<figureCaption confidence="0.993741">
Figure 2: Normalised confusion matrices on the balanced test subsets for the two datasets (left:
MSCOCO, right: Flickr30k), using geometric features and word2vec with the original terms.
</figureCaption>
<figure confidence="0.883403">
Accuracy
Mean rank
</figure>
<bodyText confidence="0.9965425">
multiple possible prepositions that may be equally
valid. For completeness we also report classifica-
tion accuracy results.
Baseline. As baseline, we rank the prepositions
by their relative frequencies in the training dataset.
We found this to be a sufficiently strong baseline,
as ubiquitous prepositions such as with and in tend
to occur frequently in the dataset.
</bodyText>
<subsectionHeader confidence="0.999914">
6.1 Ranking with known entity labels
</subsectionHeader>
<bodyText confidence="0.99998115">
In this section, we focus on predicting the best
preposition given the geometric and textual fea-
tures of the trajector and landmark entities. This
simulates the scenario of a vision detector provid-
ing a firm decision on the concept label for the
detected entities. We use a multi-class logistic
regression classifier (Fan et al., 2008), and con-
catenate multiple features into a single vector. We
compare high-level categories and terms from de-
scriptions as trajector/landmark labels. Preposi-
tions are ranked in descending order of the clas-
sifier output scores.
We found a few prepositions (e.g. with) dom-
inating the datasets. Thus, we also evaluated our
models on a balanced subset where each preposi-
tion is limited to a maximum of 50 random test
samples. The training samples are weighted ac-
cording to their class frequency in order to train
non-biased classifiers to predict this balanced test
set. The results on both the original and balanced
</bodyText>
<page confidence="0.995495">
217
</page>
<table confidence="0.9997115">
Dataset Prep (known labels) Preposition Trajector Landmark
acc rank acc rank acc rank acc rank
MSCOCO 79.8% 1.46 (17) 62.9% 1.92 (17) 65.6% 4.64 (74) 44.5% 7.30 (77)
Flickr30k 67.1% 2.16 (52) 61.7% 2.28 (52) 77.3% 1.43 (8) 66.4% 1.64 (8)
</table>
<tableCaption confidence="0.87385725">
Table 3: Accuracy (acc) and mean rank (rank, with max rank in parenthesis) for each variable of the
CRF model, trained using the high-level concept labels. Columns under Prep (known labels) refer to
the results of predicting prepositions with the trajector and landmark labels fixed to the correct values.
test sets are compared.
</tableCaption>
<bodyText confidence="0.999337966666667">
As shown in Table 2, the system performed sig-
nificantly better than the baseline in most cases. In
general, geometric features perform better than the
baseline, and when combined with text features
further improve the results. In a per-preposition
analysis, the geometric features show up to 14%
improvement in the mean rank for Flickr30k.
In feature ablation tests on MSCOCO (bal-
anced), we found the y component of the trajector
to landmark vector to be important to most prepo-
sitions, especially for under, above and on. Other
important geometric features include the final two
features in Table 1 (Euclidean distance and area).
The benefit of the word2vec text feature is clear
when moving from high-level categories to origi-
nal terms from descriptions, where it consistently
improves the mean rank (up to 25%). In contrast,
the indicator vectors resulted in a less significant
improvement, if not worse performance, when us-
ing the sparse original terms.
We also evaluated the relative importance of the
trajector and the landmark, by withholding either
from the textual feature vector. We found that the
landmark plays a larger role in preposition predic-
tion as omitting the trajector produces 10%-30%
better results than omitting the landmark.
Figure 2 shows the confusion matrices of the
best-performing systems. Note that many mis-
takes arise from prepositions that are often equally
valid (e.g. predicting near instead of next to).
</bodyText>
<subsectionHeader confidence="0.999979">
6.2 Ranking with unknown entity labels
</subsectionHeader>
<bodyText confidence="0.999698666666667">
Here, we investigate the task of jointly predicting
prepositions with the entity labels given geomet-
ric and visual features (without the trajector and
landmark labels). This simulates the scenario of
a vision detector output. For this structured pre-
diction task, we use a 3-node chain CRF model2,
</bodyText>
<footnote confidence="0.9045705">
2We used the toolbox by Mark Schmidt: http://www.
cs.ubc.ca/˜schmidtm/Software/UGM.html
</footnote>
<bodyText confidence="0.9998625">
with the centre node representing the preposition
and the two end nodes representing the trajector
and landmark. We use image features for the en-
tity nodes, and geometric features for the preposi-
tion node (Section 5). Due to computational con-
straints only high-level category labels are used,
but as seen in Section 6.1, this may actually be
hurting the performance.
Table 3 shows the results of the structured
model used to predict the most likely (trajector,
preposition, landmark) combination. To facili-
tate comparison with Section 6.1, column Prep
(known labels) shows the results with the trajec-
tor and landmark labels as known conditions and
fixed to the correct values, thus only needing to
predict the preposition. The model achieved excel-
lent performance considering the added difficulty
of the task.
</bodyText>
<sectionHeader confidence="0.985621" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999940666666666">
We explored the role of geometric, textual and
visual features in learning to predict a preposi-
tion given two bounding box instances in an im-
age, and found clear evidence that all three fea-
tures play a part in the task. Our system per-
forms well even with uncertainties surrounding
the entity labels. Future work could include non-
prepositional terms like verbs, having preposi-
tions modify verbs, adding word2vec embeddings
to the structured prediction model, and providing
stronger features – whether textual, visual or geo-
metric.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999587">
This work was funded by the ERA-Net CHIST-
ERA D2K VisualSense project (Spanish MINECO
PCIN-2013-047, UK EPSRC EP/K019082/1 and
French ANR Grant ANR-12-CHRI-0002-04)
and the Spanish MINECO RobInstruct project
TIN2014-58178-R. Ying Lu was also supported
by the China Scholarship Council.
</bodyText>
<page confidence="0.997835">
218
</page>
<sectionHeader confidence="0.988705" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999035027027027">
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 740–750, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589–637, December.
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David A. Forsyth. 2002. Object recognition as ma-
chine translation: Learning a lexicon for a fixed im-
age vocabulary. In Proceedings of the European
Conference on Computer Vision, pages 97–112.
Desmond Elliott and Frank Keller. 2013. Image de-
scription using visual dependency representations.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1292–1302, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Sanja Fidler, Abhishek Sharma, and Raquel Urtasun.
2013. A sentence is worth a thousand pixels. In
Proceedings of the IEEE Conference on Computer
Vision &amp; Pattern Recognition.
Abhinav Gupta and Larry S. Davis. 2008. Beyond
nouns: Exploiting prepositions and comparative ad-
jectives for learning visual classifiers. In Proceed-
ings of the European Conference on Computer Vi-
sion, pages 16–29.
Parisa Kordjamshidi, Martijn van Otterlo, and Marie-
Francine Moens. 2011. Spatial role labeling: To-
wards extraction of spatial relations from natural
language. ACM Transactions on Speech and Lan-
guage Processing, 8(3):article 4, 36 p.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012. Semeval-2012 task 3: Spa-
tial role labeling. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics – Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 365–373, Montr´eal,
Canada, 7-8 June. Association for Computational
Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In F. Pereira, C.J.C.
Burges, L. Bottou, and K.Q. Weinberger, editors,
Advances in Neural Information Processing Systems
25, pages 1097–1105.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing image descriptions. In Proceedings of the IEEE
Conference on Computer Vision &amp; Pattern Recogni-
tion.
Dieu-Thu Le, Jasper Uijlings, and Raffaella Bernardi.
2014. TUHOI: Trento Universal Human Object In-
teraction dataset. In Proceedings of the Third Work-
shop on Vision and Language, pages 17–24, Dublin,
Ireland, August. Dublin City University and the As-
sociation for Computational Linguistics.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C. Lawrence Zitnick. 2014. Microsoft COCO:
common objects in context. CoRR, abs/1405.0312.
Kenneth C. Litkowski and Orin Hargraves. 2005.
The preposition project. In ACL-SIGSEM Workshop
on The Linguistic Dimensions of Prepositions and
Their Use in Computational Linguistic Formalisms
and Applications, pages 171–179.
Kenneth C. Litkowski and Orin Hargraves. 2007.
Semeval-2007 task 06: Word-sense disambigua-
tion of prepositions. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 24–29, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60, Baltimore, Maryland, June. Association for
Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems.
Bryan Plummer, Liwei Wang, Chris Cervantes, Juan
Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
nik. 2015. Flickr30k entities: Collecting
region-to-phrase correspondences for richer image-
to-sentence models. CoRR, abs/1505.04870.
James Pustejovsky, Parisa Kordjamshidi, Marie-
Francine Moens, Aaron Levine, Seth Dworman, and
Zachary Yocum. 2015. Semeval-2015 task 8:
Spaceeval. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval 2015),
pages 884–894, Denver, Colorado, June. Associa-
tion for Computational Linguistics.
Yezhou Yang, Ching Teo, Hal Daum´e III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
</reference>
<page confidence="0.9868">
219
</page>
<reference confidence="0.999411375">
Processing (EMNLP), pages 444–454. Association
for Computational Linguistics.
Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From image descriptions to
visual denotations: New similarity metrics for se-
mantic inference over event descriptions. Transac-
tions of the Association for Computational Linguis-
tics, 2:67–78, February.
</reference>
<page confidence="0.997592">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.852162">
<title confidence="0.999617">Combining Geometric, Textual and Visual Features for Prepositions in Image Descriptions</title>
<author confidence="0.9387735">Josiah Ying Emmanuel Robert</author>
<affiliation confidence="0.9940715">1Institut de Rob`otica i Inform`atica Industrial (UPC-CSIC), Barcelona, 2Department of Computer Science, University of Sheffield,</affiliation>
<address confidence="0.987538">3LIRIS, Ecole Centrale de Lyon, France</address>
<abstract confidence="0.999630230769231">We investigate the role that geometric, textual and visual features play in the task of predicting a preposition that links two visual entities depicted in an image. The task is an important part of the subsequent process of generating image descriptions. We explore the prediction of prepositions for a pair of entities, both in the case when the labels of such entities are known and unknown. In all situations we found clear evidence that all three features contribute to the prediction task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="8189" citStr="Chen and Manning (2014)" startWordPosition="1267" endWordPosition="1270">1, p2, ...p|P|) according to how likely they are to express the appropriate spatial relation between the given trajector and landmark entities that are either known (Section 6.1) or only represented by visual features (Section 6.2). 4 Dataset We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of Chen and Manning (2014) as implemented in Stanford CoreNLP (Manning et al., 2014). Dependencies signifying prepositional 1The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 2011) 215 Bounding Box feature (number of dimensions) • Vector (x, y) from centroid of trajector to centroid of landmark, normalised by the size of the bounding box enclosing both objects (2) • Area of trajector bounding box relative to landmark (1) • Aspect ratio of each bounding box (2) • Area of each bounding box w.r.t. enclosing box (2) • Intersection over union of the bounding boxes (1) • E</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="10136" citStr="Collins (2003)" startWordPosition="1579" endWordPosition="1580">cted from captions and matched to instances in images are available in the supplementary material. We consider two variants of trajector and landmark terms in our experiments: (i) using the provided high level categories as terms (80 for MSCOCO and 8 for Flickr30k); (ii) using the terms occurring in the sentence directly, which constitute a bigger and more realistic challenge. For Flickr30k, the descriptive phrases may cause data sparseness (the furry, black and white dog). Thus, we extracted the lemmatised head word of each phrase, using a ‘semantic head’ variant of the head finding rules of Collins (2003) in Stanford CoreNLP. Entities from the same coreference chain are denoted with a common head noun chosen by majority vote among the group, with ties broken by the most frequent head noun in the corpus, and further ties broken at random. 5 Features Geometric Features: Geometric features between a trajector and a landmark entity are derived from bounding box annotations. We defined an 11-dimensional vector of bounding box features, covering geometric relations such as distance, orientation, relative bounding box sizes and overlaps between bounding boxes (Table 1). We chose to use continuous fea</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinar Duygulu</author>
<author>Kobus Barnard</author>
<author>Nando de Freitas</author>
<author>David A Forsyth</author>
</authors>
<title>Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary.</title>
<date>2002</date>
<booktitle>In Proceedings of the European Conference on Computer Vision,</booktitle>
<pages>97--112</pages>
<marker>Duygulu, Barnard, de Freitas, Forsyth, 2002</marker>
<rawString>Pinar Duygulu, Kobus Barnard, Nando de Freitas, and David A. Forsyth. 2002. Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary. In Proceedings of the European Conference on Computer Vision, pages 97–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Image description using visual dependency representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1292--1302</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3444" citStr="Elliott and Keller, 2013" startWordPosition="524" endWordPosition="527">from textual descriptions; (iii) visual features from images. Previous work exists (Yang et al., 2011) that uses text corpora to ‘guess’ the prepositions given the context without considering the appropriate spatial relations between the entities in the image, signifying a gap between visual content and its corresponding description. For example, although person on horse might commonly occur in text corpora, a particular image might actually depict a person standing beside a horse. On the other hand, work that does consider the image content for generating prepositions (Kulkarni et al., 2011; Elliott and Keller, 2013) map geometric relations to a limited set of prepositions using manually defined rules, 214 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 214–220, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. not as humans would naturally use them with a richer vocabulary. We would like to have the best of both worlds, by considering image content as well as textual information to select the preposition best used to express the relation between two entities. Our hypothesis is that the combination of geometric, textual </context>
<context position="5780" citStr="Elliott and Keller (2013)" startWordPosition="867" endWordPosition="870">s/orientation and motion relation. In work that links prepositions more strongly to image content, Gupta and Davis (2008) model prepositions implicitly to disambiguate image regions, rather than for predicting prepositions. Their work also require manual annotation of prepositional relations. In image description generation work, Kulkarni et al. (2011) manually map spatial relations to pre-defined prepositions, whilst Yang et al. (2011) predict prepositions from largescale text corpora solely based on the complement term, with the prepositions constrained to describing scenes (on the street). Elliott and Keller (2013) define a list of eight spatial relations and their corresponding prepositional term for sentence generation. Although they also present alternative models that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. Le et al. (2014) examine prepositions modifying human actions (verbs), and conclude that these relate to positional information to a certain extent. Other related work include training classifiers for prepositions with spatial relation features to improve image segmentation and detection (Fidler et al., 2013); this work </context>
</contexts>
<marker>Elliott, Keller, 2013</marker>
<rawString>Desmond Elliott and Frank Keller. 2013. Image description using visual dependency representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="14655" citStr="Fan et al., 2008" startWordPosition="2311" endWordPosition="2314">. Baseline. As baseline, we rank the prepositions by their relative frequencies in the training dataset. We found this to be a sufficiently strong baseline, as ubiquitous prepositions such as with and in tend to occur frequently in the dataset. 6.1 Ranking with known entity labels In this section, we focus on predicting the best preposition given the geometric and textual features of the trajector and landmark entities. This simulates the scenario of a vision detector providing a firm decision on the concept label for the detected entities. We use a multi-class logistic regression classifier (Fan et al., 2008), and concatenate multiple features into a single vector. We compare high-level categories and terms from descriptions as trajector/landmark labels. Prepositions are ranked in descending order of the classifier output scores. We found a few prepositions (e.g. with) dominating the datasets. Thus, we also evaluated our models on a balanced subset where each preposition is limited to a maximum of 50 random test samples. The training samples are weighted according to their class frequency in order to train non-biased classifiers to predict this balanced test set. The results on both the original a</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanja Fidler</author>
<author>Abhishek Sharma</author>
<author>Raquel Urtasun</author>
</authors>
<title>A sentence is worth a thousand pixels.</title>
<date>2013</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition.</booktitle>
<contexts>
<context position="6368" citStr="Fidler et al., 2013" startWordPosition="960" endWordPosition="963">reet). Elliott and Keller (2013) define a list of eight spatial relations and their corresponding prepositional term for sentence generation. Although they also present alternative models that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. Le et al. (2014) examine prepositions modifying human actions (verbs), and conclude that these relate to positional information to a certain extent. Other related work include training classifiers for prepositions with spatial relation features to improve image segmentation and detection (Fidler et al., 2013); this work is however limited to four prepositions. 3 Task Definition We formally define the task of predicting prepositions as follows: Let P be the set of possible prepositions. Let L be the set of possible landmark entities acting as the complement of a preposition, and let T be the set of possible trajector entities modified by the prepositional phrase comprising a preposition and its landmark1. For example, for the phrase person on bicycle, on would be the preposition, bicycle the landmark, and person the trajector. For this paper, we constrain trajector and landmark to be entities that </context>
</contexts>
<marker>Fidler, Sharma, Urtasun, 2013</marker>
<rawString>Sanja Fidler, Abhishek Sharma, and Raquel Urtasun. 2013. A sentence is worth a thousand pixels. In Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinav Gupta</author>
<author>Larry S Davis</author>
</authors>
<title>Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers.</title>
<date>2008</date>
<booktitle>In Proceedings of the European Conference on Computer Vision,</booktitle>
<pages>16--29</pages>
<contexts>
<context position="5276" citStr="Gupta and Davis (2008)" startWordPosition="795" endWordPosition="798">them in the Word Sense Disambiguation of Prepositions task in SemEval2007 (Litkowski and Hargraves, 2007). In SemEval-2012, Kordjamshidi et al. (2012) introduce the more fine-grained task of spatial role labelling to detect and classify spatial relations expressed by triples (trajector, landmark, spatial indicator). In the latest edition of SemEval2015, the SpaceEval task (Pustejovsky et al., 2015) introduce further tasks of identifying spatial and motion signals, as well as spatial configurations/orientation and motion relation. In work that links prepositions more strongly to image content, Gupta and Davis (2008) model prepositions implicitly to disambiguate image regions, rather than for predicting prepositions. Their work also require manual annotation of prepositional relations. In image description generation work, Kulkarni et al. (2011) manually map spatial relations to pre-defined prepositions, whilst Yang et al. (2011) predict prepositions from largescale text corpora solely based on the complement term, with the prepositions constrained to describing scenes (on the street). Elliott and Keller (2013) define a list of eight spatial relations and their corresponding prepositional term for sentenc</context>
</contexts>
<marker>Gupta, Davis, 2008</marker>
<rawString>Abhinav Gupta and Larry S. Davis. 2008. Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers. In Proceedings of the European Conference on Computer Vision, pages 16–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Martijn van Otterlo</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Spatial role labeling: Towards extraction of spatial relations from natural language.</title>
<date>2011</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>8</volume>
<issue>3</issue>
<pages>p.</pages>
<marker>Kordjamshidi, van Otterlo, Moens, 2011</marker>
<rawString>Parisa Kordjamshidi, Martijn van Otterlo, and MarieFrancine Moens. 2011. Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Transactions on Speech and Language Processing, 8(3):article 4, 36 p.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Steven Bethard</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Semeval-2012 task 3: Spatial role labeling.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>365--373</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="4804" citStr="Kordjamshidi et al. (2012)" startWordPosition="725" endWordPosition="728">formation should help generate a relation that is consistent with the image content, whilst incorporating textual information should help generate a description that is consistent with natural language. 2 Related Work The Natural Language Processing Community has significant interest in different aspects of prepositions. The Prepositions Project (Litkowski and Hargraves, 2005) analysed and produced a lexicon of English prepositions and their senses, and subsequently used them in the Word Sense Disambiguation of Prepositions task in SemEval2007 (Litkowski and Hargraves, 2007). In SemEval-2012, Kordjamshidi et al. (2012) introduce the more fine-grained task of spatial role labelling to detect and classify spatial relations expressed by triples (trajector, landmark, spatial indicator). In the latest edition of SemEval2015, the SpaceEval task (Pustejovsky et al., 2015) introduce further tasks of identifying spatial and motion signals, as well as spatial configurations/orientation and motion relation. In work that links prepositions more strongly to image content, Gupta and Davis (2008) model prepositions implicitly to disambiguate image regions, rather than for predicting prepositions. Their work also require m</context>
</contexts>
<marker>Kordjamshidi, Bethard, Moens, 2012</marker>
<rawString>Parisa Kordjamshidi, Steven Bethard, and MarieFrancine Moens. 2012. Semeval-2012 task 3: Spatial role labeling. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 365–373, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>Advances in Neural Information Processing Systems 25,</booktitle>
<pages>1097--1105</pages>
<editor>In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="12323" citStr="Krizhevsky et al., 2012" startWordPosition="1945" endWordPosition="1948"> from person on boat can help predict the preposition that mediates man and boat). Image Features: While it is ideal to have vision systems produce a firm decision about the visual entity instance detected in an image, in reality it may be beneficial to defer the decision by allowing several possible interpretations of the instance being detected. In such cases, we will not have a single concept label for the entity, but instead a high-level visual representation. For this scenario, we extracted visual representations from the final layer of a Convolutional Neural Network trained on ImageNet (Krizhevsky et al., 2012), and used them as representations for entity instances in place of textual features. 6 Preposition Prediction Here we highlight interesting findings from experiments performed for the task of predicting prepositions for two different scenarios (Sections 6.1 and 6.2). Detailed results can be found in the supplementary material. Evaluation metrics. As there may be more than one ‘correct’ preposition for a given context (person on horse and person atop horse), we propose the mean rank of the correct preposition as the main evaluation metric, as it accommodates 216 IND W2V GF IND+GF W2V+GF Baseli</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. Imagenet classification with deep convolutional neural networks. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097–1105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating image descriptions.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition.</booktitle>
<contexts>
<context position="1684" citStr="Kulkarni et al., 2011" startWordPosition="245" endWordPosition="248">terest in the task of automatic generation of natural language image descriptions at sentence level, compared to earlier work that annotates images with a laundry list of terms (Duygulu et al., 2002). The task is important in that such detailed annotations are more informative and discriminative compared to isolated textual labels, and are essential for improved text and image retrieval. The most standard approach to generating such descriptions involves first detecting instances of pre-defined concepts in the image, and then reasoning about these concepts to generate image descriptions e.g. (Kulkarni et al., 2011; Yang et al., 2011). Our work is also based on this paradigm. However, we assume that object instances have already been pre-detected by visual recognisers, and concentrate on a specific subtask of description generation. More specifically, given two visual entity instances where one could potentially act as a modifier to the other, we address the problem of identifying the appropriate preposition to connect these two entities (Figure 1). The inferred prepositional relations will subsequently act as an *A. Ramisa and J. Wang contributed equally to this work. Figure 1: Given a subject boy and </context>
<context position="3417" citStr="Kulkarni et al., 2011" startWordPosition="520" endWordPosition="523"> (ii) textual features from textual descriptions; (iii) visual features from images. Previous work exists (Yang et al., 2011) that uses text corpora to ‘guess’ the prepositions given the context without considering the appropriate spatial relations between the entities in the image, signifying a gap between visual content and its corresponding description. For example, although person on horse might commonly occur in text corpora, a particular image might actually depict a person standing beside a horse. On the other hand, work that does consider the image content for generating prepositions (Kulkarni et al., 2011; Elliott and Keller, 2013) map geometric relations to a limited set of prepositions using manually defined rules, 214 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 214–220, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. not as humans would naturally use them with a richer vocabulary. We would like to have the best of both worlds, by considering image content as well as textual information to select the preposition best used to express the relation between two entities. Our hypothesis is that the combina</context>
<context position="5509" citStr="Kulkarni et al. (2011)" startWordPosition="827" endWordPosition="830">ify spatial relations expressed by triples (trajector, landmark, spatial indicator). In the latest edition of SemEval2015, the SpaceEval task (Pustejovsky et al., 2015) introduce further tasks of identifying spatial and motion signals, as well as spatial configurations/orientation and motion relation. In work that links prepositions more strongly to image content, Gupta and Davis (2008) model prepositions implicitly to disambiguate image regions, rather than for predicting prepositions. Their work also require manual annotation of prepositional relations. In image description generation work, Kulkarni et al. (2011) manually map spatial relations to pre-defined prepositions, whilst Yang et al. (2011) predict prepositions from largescale text corpora solely based on the complement term, with the prepositions constrained to describing scenes (on the street). Elliott and Keller (2013) define a list of eight spatial relations and their corresponding prepositional term for sentence generation. Although they also present alternative models that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. Le et al. (2014) examine prepositions modifying hum</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision &amp; Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dieu-Thu Le</author>
<author>Jasper Uijlings</author>
<author>Raffaella Bernardi</author>
</authors>
<title>TUHOI: Trento Universal Human Object Interaction dataset.</title>
<date>2014</date>
<booktitle>In Proceedings of the Third Workshop on Vision and Language,</booktitle>
<pages>17--24</pages>
<institution>Dublin City University and the Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="6074" citStr="Le et al. (2014)" startWordPosition="915" endWordPosition="918">description generation work, Kulkarni et al. (2011) manually map spatial relations to pre-defined prepositions, whilst Yang et al. (2011) predict prepositions from largescale text corpora solely based on the complement term, with the prepositions constrained to describing scenes (on the street). Elliott and Keller (2013) define a list of eight spatial relations and their corresponding prepositional term for sentence generation. Although they also present alternative models that use text corpora for descriptions that are more human-like, they are limited to verbs and do not cover prepositions. Le et al. (2014) examine prepositions modifying human actions (verbs), and conclude that these relate to positional information to a certain extent. Other related work include training classifiers for prepositions with spatial relation features to improve image segmentation and detection (Fidler et al., 2013); this work is however limited to four prepositions. 3 Task Definition We formally define the task of predicting prepositions as follows: Let P be the set of possible prepositions. Let L be the set of possible landmark entities acting as the complement of a preposition, and let T be the set of possible tr</context>
</contexts>
<marker>Le, Uijlings, Bernardi, 2014</marker>
<rawString>Dieu-Thu Le, Jasper Uijlings, and Raffaella Bernardi. 2014. TUHOI: Trento Universal Human Object Interaction dataset. In Proceedings of the Third Workshop on Vision and Language, pages 17–24, Dublin, Ireland, August. Dublin City University and the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsung-Yi Lin</author>
<author>Michael Maire</author>
<author>Serge Belongie</author>
<author>James Hays</author>
<author>Pietro Perona</author>
<author>Deva Ramanan</author>
<author>Piotr Doll´ar</author>
<author>C Lawrence Zitnick</author>
</authors>
<title>Microsoft COCO: common objects in context. CoRR,</title>
<date>2014</date>
<marker>Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll´ar, Zitnick, 2014</marker>
<rawString>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C. Lawrence Zitnick. 2014. Microsoft COCO: common objects in context. CoRR, abs/1405.0312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>The preposition project.</title>
<date>2005</date>
<booktitle>In ACL-SIGSEM Workshop on The Linguistic Dimensions of Prepositions and Their Use in Computational Linguistic Formalisms and Applications,</booktitle>
<pages>171--179</pages>
<contexts>
<context position="4557" citStr="Litkowski and Hargraves, 2005" startWordPosition="688" endWordPosition="691">best used to express the relation between two entities. Our hypothesis is that the combination of geometric, textual and visual features can help with the task of predicting the most appropriate preposition, since incorporating geometric and visual information should help generate a relation that is consistent with the image content, whilst incorporating textual information should help generate a description that is consistent with natural language. 2 Related Work The Natural Language Processing Community has significant interest in different aspects of prepositions. The Prepositions Project (Litkowski and Hargraves, 2005) analysed and produced a lexicon of English prepositions and their senses, and subsequently used them in the Word Sense Disambiguation of Prepositions task in SemEval2007 (Litkowski and Hargraves, 2007). In SemEval-2012, Kordjamshidi et al. (2012) introduce the more fine-grained task of spatial role labelling to detect and classify spatial relations expressed by triples (trajector, landmark, spatial indicator). In the latest edition of SemEval2015, the SpaceEval task (Pustejovsky et al., 2015) introduce further tasks of identifying spatial and motion signals, as well as spatial configurations/</context>
</contexts>
<marker>Litkowski, Hargraves, 2005</marker>
<rawString>Kenneth C. Litkowski and Orin Hargraves. 2005. The preposition project. In ACL-SIGSEM Workshop on The Linguistic Dimensions of Prepositions and Their Use in Computational Linguistic Formalisms and Applications, pages 171–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>Semeval-2007 task 06: Word-sense disambiguation of prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>24--29</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4759" citStr="Litkowski and Hargraves, 2007" startWordPosition="719" endWordPosition="722">tion, since incorporating geometric and visual information should help generate a relation that is consistent with the image content, whilst incorporating textual information should help generate a description that is consistent with natural language. 2 Related Work The Natural Language Processing Community has significant interest in different aspects of prepositions. The Prepositions Project (Litkowski and Hargraves, 2005) analysed and produced a lexicon of English prepositions and their senses, and subsequently used them in the Word Sense Disambiguation of Prepositions task in SemEval2007 (Litkowski and Hargraves, 2007). In SemEval-2012, Kordjamshidi et al. (2012) introduce the more fine-grained task of spatial role labelling to detect and classify spatial relations expressed by triples (trajector, landmark, spatial indicator). In the latest edition of SemEval2015, the SpaceEval task (Pustejovsky et al., 2015) introduce further tasks of identifying spatial and motion signals, as well as spatial configurations/orientation and motion relation. In work that links prepositions more strongly to image content, Gupta and Davis (2008) model prepositions implicitly to disambiguate image regions, rather than for predi</context>
</contexts>
<marker>Litkowski, Hargraves, 2007</marker>
<rawString>Kenneth C. Litkowski and Orin Hargraves. 2007. Semeval-2007 task 06: Word-sense disambiguation of prepositions. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 24–29, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="8247" citStr="Manning et al., 2014" startWordPosition="1276" endWordPosition="1279">the appropriate spatial relation between the given trajector and landmark entities that are either known (Section 6.1) or only represented by visual features (Section 6.2). 4 Dataset We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of Chen and Manning (2014) as implemented in Stanford CoreNLP (Manning et al., 2014). Dependencies signifying prepositional 1The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 2011) 215 Bounding Box feature (number of dimensions) • Vector (x, y) from centroid of trajector to centroid of landmark, normalised by the size of the bounding box enclosing both objects (2) • Area of trajector bounding box relative to landmark (1) • Aspect ratio of each bounding box (2) • Area of each bounding box w.r.t. enclosing box (2) • Intersection over union of the bounding boxes (1) • Euclidean distance between the trajector and landmark bound</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="11485" citStr="Mikolov et al., 2013" startWordPosition="1804" endWordPosition="1807"> being correlated, we left it to the classifier to determine the most useful features for discrimination without having to withhold any unnecessarily. Textual features: We consider two textual features to encode the trajector and landmark terms wt� and wz. The first feature is a one-hot indicator vector xZ and yz for the trajector and landmark respectively, where xzt = 1 if index t corresponds to the trajector term wt� and 0 elsewhere (and similarly for landmark). As data sparseness may be an issue, we also explore an alternative textual feature which encodes the terms as word2vec embeddings (Mikolov et al., 2013). This encodes each term as a vector such that semantically related terms are close in the vector space. This allows information to be transferred across semantically related terms during training (e.g. information from person on boat can help predict the preposition that mediates man and boat). Image Features: While it is ideal to have vision systems produce a firm decision about the visual entity instance detected in an image, in reality it may be beneficial to defer the decision by allowing several possible interpretations of the instance being detected. In such cases, we will not have a si</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Plummer</author>
<author>Liwei Wang</author>
<author>Chris Cervantes</author>
<author>Juan Caicedo</author>
<author>Julia Hockenmaier</author>
<author>Svetlana Lazebnik</author>
</authors>
<title>Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models.</title>
<date>2015</date>
<location>CoRR, abs/1505.04870.</location>
<contexts>
<context position="8006" citStr="Plummer et al., 2015" startWordPosition="1242" endWordPosition="1245">ely, and ri the relative geometric feature between the two visual entities. Given di, the objective of the preposition prediction task is to produce a ranked list of prepositions (p1, p2, ...p|P|) according to how likely they are to express the appropriate spatial relation between the given trajector and landmark entities that are either known (Section 6.1) or only represented by visual features (Section 6.2). 4 Dataset We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of Chen and Manning (2014) as implemented in Stanford CoreNLP (Manning et al., 2014). Dependencies signifying prepositional 1The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 2011) 215 Bounding Box feature (number of dimensions) • Vector (x, y) from centroid of trajector to centroid of landmark, normalised by the size of the bounding box enclosing both objects (2) • Area of trajector bou</context>
</contexts>
<marker>Plummer, Wang, Cervantes, Caicedo, Hockenmaier, Lazebnik, 2015</marker>
<rawString>Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. CoRR, abs/1505.04870.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Parisa Kordjamshidi</author>
<author>MarieFrancine Moens</author>
<author>Aaron Levine</author>
<author>Seth Dworman</author>
<author>Zachary Yocum</author>
</authors>
<date>2015</date>
<booktitle>Semeval-2015 task 8: Spaceeval. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>884--894</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado,</location>
<contexts>
<context position="5055" citStr="Pustejovsky et al., 2015" startWordPosition="763" endWordPosition="766">mmunity has significant interest in different aspects of prepositions. The Prepositions Project (Litkowski and Hargraves, 2005) analysed and produced a lexicon of English prepositions and their senses, and subsequently used them in the Word Sense Disambiguation of Prepositions task in SemEval2007 (Litkowski and Hargraves, 2007). In SemEval-2012, Kordjamshidi et al. (2012) introduce the more fine-grained task of spatial role labelling to detect and classify spatial relations expressed by triples (trajector, landmark, spatial indicator). In the latest edition of SemEval2015, the SpaceEval task (Pustejovsky et al., 2015) introduce further tasks of identifying spatial and motion signals, as well as spatial configurations/orientation and motion relation. In work that links prepositions more strongly to image content, Gupta and Davis (2008) model prepositions implicitly to disambiguate image regions, rather than for predicting prepositions. Their work also require manual annotation of prepositional relations. In image description generation work, Kulkarni et al. (2011) manually map spatial relations to pre-defined prepositions, whilst Yang et al. (2011) predict prepositions from largescale text corpora solely ba</context>
</contexts>
<marker>Pustejovsky, Kordjamshidi, Moens, Levine, Dworman, Yocum, 2015</marker>
<rawString>James Pustejovsky, Parisa Kordjamshidi, MarieFrancine Moens, Aaron Levine, Seth Dworman, and Zachary Yocum. 2015. Semeval-2015 task 8: Spaceeval. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 884–894, Denver, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
<author>Ching Teo</author>
<author>Hal Daum´e</author>
<author>Yiannis Aloimonos</author>
</authors>
<title>Corpus-guided sentence generation of natural images.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>444--454</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yang, Teo, Daum´e, Aloimonos, 2011</marker>
<rawString>Yezhou Yang, Ching Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 444–454. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--67</pages>
<contexts>
<context position="7983" citStr="Young et al., 2014" startWordPosition="1238" endWordPosition="1241">k entities respectively, and ri the relative geometric feature between the two visual entities. Given di, the objective of the preposition prediction task is to produce a ranked list of prepositions (p1, p2, ...p|P|) according to how likely they are to express the appropriate spatial relation between the given trajector and landmark entities that are either known (Section 6.1) or only represented by visual features (Section 6.2). 4 Dataset We base the preposition prediction task on two large-scale image datasets with human authored descriptions, namely MSCOCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). To extract instances of triples (trajector, preposition, landmark) from image descriptions, we used the Neural Network, transition-based dependency parser of Chen and Manning (2014) as implemented in Stanford CoreNLP (Manning et al., 2014). Dependencies signifying prepositional 1The terminologies trajector and landmark are adopted from spatial role labelling (Kordjamshidi et al., 2011) 215 Bounding Box feature (number of dimensions) • Vector (x, y) from centroid of trajector to centroid of landmark, normalised by the size of the bounding box enclosing both objects (2) </context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67–78, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
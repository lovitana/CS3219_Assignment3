<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000348">
<title confidence="0.986442">
Online Representation Learning in Recurrent Neural Language Models
</title>
<author confidence="0.994983">
Marek Rei
</author>
<affiliation confidence="0.991792">
The ALTA Institute
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.6323">
United Kingdom
</address>
<email confidence="0.998496">
marek.rei@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991125">
We investigate an extension of continuous
online learning in recurrent neural network
language models. The model keeps a sep-
arate vector representation of the current
unit of text being processed and adaptively
adjusts it after each prediction. The initial
experiments give promising results, indi-
cating that the method is able to increase
language modelling accuracy, while also
decreasing the parameters needed to store
the model along with the computation re-
quired at each step.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971390243903">
In recent years, neural network models have
shown impressive performance on many natural
language processing tasks, such as speech recogni-
tion (Chorowski et al., 2014; Graves et al., 2013),
machine translation (Kalchbrenner and Blunsom,
2013; Cho et al., 2014), text classification (Le
and Mikolov, 2014; Kalchbrenner et al., 2014) and
image description generation (Kiros et al., 2014).
One of the main advantages of these methods is
the ability to learn smooth vector representations
for words, thereby reducing the sparsity problem
inherent in any natural language dataset.
Language modelling is another task where neu-
ral networks have delivered excellent results (Ben-
gio et al., 2003; Mikolov et al., 2011). Chelba
et al. (2014) have recently benchmarked several
well-known language models by training on very
large datasets. They found that a recurrent neu-
ral network language model (RNNLM) combined
with a 9-gram MaxEnt model was able to give the
best results and lowest perplexity.
In this work we investigate a possible extension
of RNNLM, by allowing it to continue learning
and adapting during testing. The model keeps a
vector representation of the current sentence that
is being processed, and continuously modifies it
based on an error signal. We refer to this as a ver-
sion of online learning, as gradient descent is used
to optimise the vector even during testing.
The technique is inspired by work on represen-
tation learning (Collobert and Weston, 2008; Mnih
and Hinton, 2008; Mikolov et al., 2013), espe-
cially Le and Mikolov (2014) who use a related
model to learn representations for text classifica-
tion. We extend the idea to recurrent models and
apply it to the task of language modelling. Our
results indicate that by exchanging some existing
model parameters for a component using online
learning, the system is able to achieve lower per-
plexity while also reducing the necessary compu-
tation.
</bodyText>
<sectionHeader confidence="0.996033" genericHeader="introduction">
2 RNNLM
</sectionHeader>
<bodyText confidence="0.999956125">
We base our implementation of the RNNLM on
Mikolov et al. (2011), shown in Figure 1. The in-
put layer to the network consists of a 1-hot vec-
tor representing the previous word in the sequence,
and the hidden vector from the previous time step.
These are multiplied by corresponding weight ma-
trices and the resulting vectors are passed through
an activation function to calculate the hidden vec-
</bodyText>
<figureCaption confidence="0.9253555">
Figure 1: Recurrent neural network language
model (RNNLM)
</figureCaption>
<page confidence="0.939917">
238
</page>
<note confidence="0.664635">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 238–243,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.997704571428571">
tor at the current time step.1
Class-based output architecture is used to avoid
calculating the softmax over all words in the vo-
cabulary. The probability distributions over words
and classes are calculated by multiplying the hid-
den vector with the corresponding weight matrix
and applying the softmax function:
</bodyText>
<equation confidence="0.980887">
hiddent = Q(E inputt + Wh hiddent−1)
classes = softmax(Wc hiddent)
output = softmax(W (c)
o hiddent)
</equation>
<bodyText confidence="0.963098">
where Q is the logistic function and W (c)
o is the
weight matrix between the hidden layer and the
output words in class c.
Finally, we multiply the probability of the next
word belonging to class c with the output proba-
bility of the next word given the class to get the
overall probability of the next word given the pre-
vious words:
</bodyText>
<equation confidence="0.978695">
P(wt+1jwt1) Pz� classesc outputwt+1
</equation>
<bodyText confidence="0.99888255">
Negative log-probability is used as the loss
function, which optimises the network to assign
a high probability to the correct words. The net-
work is trained using gradient descent and back-
propagation through time. In the basic model, this
means unrolling the recurrent network for a fixed
number of time steps, essentially turning it into a
deep feedforward network which outputs proba-
bility distributions on different layers. Instead of
using a fixed number of steps, our implementation
unrolls each sentence from the last word to the first
word, making it more suitable for processing indi-
vidual sentences as opposed to longer texts.
In addition, we introduce a special vector to use
as the hidden vector at the start of each sentence.
The values in this vector are treated as parame-
ters and optimised during training. This allows the
network to learn a suitable starting point when no
other information is available, giving slight perfor-
mance improvements in our experiments.
</bodyText>
<sectionHeader confidence="0.982713" genericHeader="method">
3 RNNLM with online learning
</sectionHeader>
<bodyText confidence="0.856046">
We extend the RNNLM by introducing an addi-
tional document/context vector, shown as doc in
Figure 2. This vector will represent the current
</bodyText>
<footnote confidence="0.953574333333333">
1Explicit multiplication for the word vectors can be
avoided by using data structures that retrieve the correct vec-
tor in constant time.
</footnote>
<figureCaption confidence="0.8892265">
Figure 2: RNNLM with an additional document
vector for active learning
</figureCaption>
<bodyText confidence="0.997707">
document being processed, whether that is a sen-
tence, paragraph or a larger text. When calculat-
ing output probabilities over classes and words, we
also condition them on this new document vector:
</bodyText>
<equation confidence="0.9792115">
classes = softmax(Wc hiddent + Wdc doc)
output = softmax(W (c)
o hiddent + W(c)
do doc)
</equation>
<bodyText confidence="0.999974466666666">
where Wdc is the weight matrix between the docu-
ment vector and class layer, and W(c) dois the weight
matrix between the document vector and output
words in class c.
We construct the document vector by treating
the values as parameters and optimising them dur-
ing both training and testing using backpropaga-
tion. At each time step, the system first performs
a forward pass through the network and outputs
probability distributions over classes and words.
We then use the next word in the sequence to cal-
culate the error derivatives in the output and back-
propagate them back into the document vector.
The update is not able to to affect the output at
the current time step, but it will modify the doc-
ument vector which will be used in the next time
step. The same word that is used for modifying
the document vector for the next time step is also
available in the input layer of the next time step,
therefore the system receives no additional knowl-
edge as input.
We are interested in modelling individual sen-
tences, therefore at the beginning of each sentence
the document vector is reset to a specific start-
ing state, which is optimised during training and
shared between all sentences. During testing, the
values in the document vector are continuously
modified depending on the error derivatives be-
ing backpropagated from the output layer, while
all other parameters in the model stay constant.
</bodyText>
<page confidence="0.988582">
239
</page>
<bodyText confidence="0.999989813559322">
When dealing with larger texts and domain-
specific corpora, similar ideas of iterative learning
can be applied to any language model. After pro-
cessing a certain amount of data during testing, a
new model could be trained using the previously
seen testing examples as additional training data.
Since this process adds more training data which
is likely to be similar to upcoming testing exam-
ples, the system is likely to achieve a better per-
formance.
However, when dealing with independent sen-
tences, online learning becomes more difficult to
apply. Each sentence contains very little addi-
tional data, and even if the language model is
adjusted after every individual word, it only ob-
tains evidence of previous words in the sentence,
whereas these words are relatively unlikely to oc-
cur again in the same sentence. Therefore, instead
of adjusting individual word representations, our
approach learns a distributed document vector to
represent the specific unit of text that is currently
being processed. This vector is then used as addi-
tional evidence when calculating output probabil-
ities.
Le and Mikolov (2014) use a similar method
for learning vector representations of documents
and paragraphs. They construct a feedforward
language model and include a paragraph vector
as an additional vector in the input layer. The
model parameters are trained on the training set,
and when given unseen test data, the system opti-
mises the paragraph vector according to the error
signal. They use these vectors as input to a logis-
tic regression classifier and achieve state-of-the-art
performance on sentiment classification of movie
reviews. However, they did not consider the effect
of this model modification directly on the task of
language modelling.
While the system of Le and Mikolov (2014)
uses a basic feedforward language model, we ex-
tend the idea to recurrent neural network language
models, as they are currently used in state-of-
the-art language modelling systems (Chelba et al.,
2014). Attaching the document vector to the input
layer is not preferable for RNNLM, as the error
is only backpropagated into the input layer after
several time steps. When this time step is reached
and the network is unrolled to perform backprop-
agation through time, several words have already
passed without receiving any additional informa-
tion. Since our implementation performs the un-
rolling only at the end of each sentence, the up-
dates would not have any effect. Therefore, we
attach the document vector directly to the output
layer, in parallel with the recurrent hidden compo-
nent. Parameters in the document vector can then
be updated at each time step, while the unrolling
and backpropagation through time still happens at
the end of the sentence.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999685">
We constructed a dataset from English Wikipedia
to evaluate language modelling performance over
individual sentences. The text was tokenised, sen-
tence split and lowercased. The sentences were
shuffled, in order to minimise any transfer effects
between consecutive sentences, and then split into
training, development and test sets. The final sen-
tences were sampled randomly, in order to obtain
reasonable training times for the experiments. The
dataset sizes are shown in Table 2.
</bodyText>
<table confidence="0.969364">
Train Dev Test
Words 9,990,782 237,037 4,208,847
Sentences 419,278 10,000 176,564
</table>
<tableCaption confidence="0.995044">
Table 2: Dataset sizes
</tableCaption>
<bodyText confidence="0.999920958333333">
Model performance is measured using perplex-
ity, therefore lower values indicate a model which
is able to better predict the data. Special tokens
are used to mark the beginning and end of a sen-
tence. The sentence end token is also included
in the evaluation, whereas the sentence start to-
ken is only used as context in the input layer.
Any words that occur less than 30 times in the
training data were replaced by a special token for
unknown words, leaving a vocabulary of 16,514
unique words. General learning rate was set to 0.1
and decreased during training, whereas the learn-
ing rate of the document vector was fixed at 0.1
for both training and testing.
As the baseline, we use the regular RNNLM
with 100-dimensional hidden layers and word vec-
tors (M = 100). In the experiments we increase
the capacity of the model and measure how that
affects the perplexity on the datasets. First, we
increase the value of M, allowing more informa-
tion to be stored into word representations, while
also increasing the number of hidden-hidden and
hidden-output connections. As can be seen in Ta-
ble 1, this improves the overall performance of the
</bodyText>
<page confidence="0.975547">
240
</page>
<figure confidence="0.860042083333333">
Train PPL Dev PPL Test PPL
Baseline M=100 92.65 103.56 102.51
M=120 88.60 98.78 97.79
M=100, D=20 87.28 95.36 94.39
M=135 85.17 96.33 95.71
M=100, D=35 80.11 91.05 90.29
+Parameters +Operations
– –
666,960 7,400
332,300 6,000
1,167,705 13,475
581,525 10,500
</figure>
<tableCaption confidence="0.977412">
Table 1: Perplexity and additional parameters/operations for different language model configurations
</tableCaption>
<bodyText confidence="0.991755260869565">
model – setting M to 120 and 135 leads to pro-
gressively lower perplexity.
Next, instead of increasing M, we add a D-
dimensional document vector to the model and use
this for online learning. When the same num-
ber of elements is added to M or D, our results
show consistently better performance when using
the document vector. Increasing M by 35 gives
perplexity 95.71, whereas using a 35-dimensional
document vector gives perplexity 90.29. We also
performed the same experiment using only half
of the training data, and the difference was even
larger – 105.50 and 98.23 correspondingly.
One reason why online learning during model
deployment is not commonly used is because it
is computationally expensive. Continuously re-
training the model and adjusting parameters can be
very time-consuming compared to a simple feed-
forward process through the network. However,
extra computation is also needed when using a
hidden vector of size M, as opposed to using a
smaller value. When increasing the value of M to
M + X, the RNNLM will contain
</bodyText>
<equation confidence="0.944461">
X · C + 2 · X · V + 2 · X · M + X2
additional parameters and needs to perform
2 · X · M + X2 + X · C + X · E[O]
</equation>
<bodyText confidence="0.999543857142857">
additional operations at each time step.2 C is the
number of classes, V is vocabulary size, and E[O]
is the expected number of words that need to be
processed in the output layer during one step.
The corresponding number of additional param-
eters in a RNNLM model using a D-dimensional
document vector for online learning is
</bodyText>
<equation confidence="0.549044">
D + D · V + D · C
</equation>
<bodyText confidence="0.3477275">
2We only count the matrix multiplication operations, as
they take the majority of the time in a neural network lan-
guage model.
and additional operations
</bodyText>
<equation confidence="0.453521">
2 · D · E[O] + 2 · D · C
</equation>
<bodyText confidence="0.9999385">
which includes the error backpropagation at each
time step. For our experiments V = 16,514,
C = 100 and E[O] ≈ 50. Table 1 contains the ad-
ditional values for the experiments, showing that
replacing some hidden vector parameters with the
actively learned document vector leads to fewer
total parameters and fewer operations, along with
lower perplexity.
Figure 3 presents the relationship between per-
plexity and the number of additional parameters,
when increasing either M or D. The results are
averaged over 10 runs with different random ini-
tialisations. As can be seen, using a small docu-
ment vector lowers the perplexity with fewer pa-
rameters, compared to simply increasing the main
components of the network. The graph of per-
plexity with respect to additional operations in the
model also has a very similar shape.
</bodyText>
<figure confidence="0.954206857142857">
104
100
96
92
increasing M
increasing D
0 500000 1000000 1500000 2000000
</figure>
<figureCaption confidence="0.99825">
Figure 3: Perplexity as a function of additional
</figureCaption>
<bodyText confidence="0.9638956">
parameters when increasing either M or D. The
x-axis shows the number of additional parame-
ters in the model, with respect to the baseline of
M = 100, D = 0. The y-axis shows the perplex-
ity on the test set.
</bodyText>
<page confidence="0.993414">
88
84
241
</page>
<listItem confidence="0.902627">
Both Hufnagel and Marston also joined the long-standing technical death metal band Gorguts.
1. The band eventually went on to become the post-hardcore band Adair.
2. The band members originally came from different death metal bands, bonding over a common
interest in d-beat.
3. The proceeds went towards a home studio, which enabled him to concentrate on his solo output
and songs that were to become his debut mini-album ”Feeding The Wolves”.
</listItem>
<bodyText confidence="0.9429255">
The Chiefs reclaimed the title on September 29, 2014 in a Monday Night Football game against the
New England Patriots, hitting 142.2 decibels.
</bodyText>
<listItem confidence="0.874761">
1. He played in twenty-four regular season games for the Colts, all off the bench.
2. In May 2009 the Warriors announced they had re-signed him until the end of the 2011 season.
3. The team played inconsistently throughout the campaign from the outset, losing the opening
</listItem>
<bodyText confidence="0.873731333333333">
two matches before winning four consecutive games during September 1927.
He was educated at Llandovery College and Jesus College, Oxford, where he obtained an M.A.
degree.
</bodyText>
<listItem confidence="0.993226">
1. He studied at the Orthodox High School, then at the Faculty of Mathematics.
2. Kaigama studied for the priesthood at St. Augustine’s Seminary in Jos with further study in
theology in Rome.
3. Under his stewardship, Zahira College became one of the leading schools in the country.
</listItem>
<tableCaption confidence="0.99692">
Table 3: Examples of using the document vectors to find similar sentences in the development data.
</tableCaption>
<bodyText confidence="0.999920363636363">
In order to further explore the relationship be-
tween D and M, we trained a number of smaller
models with different values, under the constraint
D + M = 100. To reduce computation time, only
half of the training data was used in these experi-
ments. The lowest perplexity was achieved in the
region of D = 23 and M = 77, and making
the document vectors much smaller or larger led
to a decrease in performance. This indicates that
including the document vector does help increase
model accuracy, but as it contains no information
about the training data, this vector should be small
compared to the main model.
Intuitively, this approach works by having the
document vector capture the unique aspects of
each sentence. While the general RNNLM is a
smooth static representation of the entire training
data, the document vector is optimised to repre-
sent how each sentence differs from the main lan-
guage model. Therefore we performed a quali-
tative evaluation and found that the learned sen-
tence vectors were also very good predictors of
semantic similarity. The RNN language model
was trained on the training set, and then used to
process the development set. The last state of
the document vector of each sentence was used to
calculate cosine similarity. Table 3 contains ran-
domly sampled sentences from the development
set, together with corresponding development sen-
tences that have the highest similarity (excluding
the original sentence). Even though there is al-
most no word overlap, the retrieved sentences are
semantically very similar.
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999952944444445">
We have described a possible extension of
RNNLM which uses continuous online learning.
The model includes a separate vector to represent
the unit of text, such as a sentence, being cur-
rently processed. The vector starts in a default
state and is continuously updated using backprop-
agation, leading to a more informative representa-
tion. The modified language model achieves lower
perplexity with a more optimal use of parameters.
The idea of continuous training and adaptation
is natural and also established in biological learn-
ing processes, yet it is not widely used due to com-
putational complexity. Our experiments indicate
that by including this active learning component
in the neural network model, the system is able to
achieve higher accuracy, while also decreasing the
parameters needed to store the model and decreas-
ing the computation required.
</bodyText>
<page confidence="0.995677">
242
</page>
<sectionHeader confidence="0.995873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999699803921569">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. 3:1137–1155.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2014. One Billion Word Benchmark for Mea-
suring Progress in Statistical Language Modeling.
In INTERSPEECH 2014.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learn-
ing Phrase Representations using RNN Encoder-
Decoder for Statistical Machine Translation. June.
Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,
and Yoshua Bengio. 2014. End-to-end Continu-
ous Speech Recognition using Attention-based Re-
current NN : First Results.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. Proceed-
ings of the 25th international conference on Ma-
chine learning.
Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional LSTM. In ASRU 2013.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In EMNLP.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In The 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics. Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics.
Ryan Kiros, Ruslan Salakhutdinov, and Richard Zemel.
2014. Multimodal Neural Language Models. In
ICML 2014.
Quoc Le and Tomas Mikolov. 2014. Distributed Rep-
resentations of Sentences and Documents. In ICML,
volume 32.
Toma&amp;quot;s Mikolov, Stefan Kombrink, Anoop Deoras,
Luk´a&amp;quot;s Burget, and Jan &amp;quot;Cernock´y. 2011. RNNLM
- Recurrent neural network language modeling
toolkit. In ASRU 2011 Demo Session.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. ICLR Workshop, pages
1–12.
Andriy Mnih and Geoffrey Hinton. 2008. A Scal-
able Hierarchical Distributed Language Model. Ad-
vances in Neural Information Processing Systems,
pages 1–8.
</reference>
<page confidence="0.999037">
243
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.698731">
<title confidence="0.999845">Online Representation Learning in Recurrent Neural Language Models</title>
<author confidence="0.992304">Marek</author>
<affiliation confidence="0.9385625">The ALTA Computer University of United</affiliation>
<email confidence="0.963959">marek.rei@cl.cam.ac.uk</email>
<abstract confidence="0.992541692307692">We investigate an extension of continuous online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<pages>3--1137</pages>
<contexts>
<context position="1370" citStr="Bengio et al., 2003" startWordPosition="197" endWordPosition="201">mance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets. They found that a recurrent neural network language model (RNNLM) combined with a 9-gram MaxEnt model was able to give the best results and lowest perplexity. In this work we investigate a possible extension of RNNLM, by allowing it to continue learning and adapting during testing. The model keeps a vector representation of the current sentence that is being processed, and continuously modifies it based on an error signal. We refer to this as a version </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language Model. 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Tomas Mikolov</author>
<author>Mike Schuster</author>
<author>Qi Ge</author>
<author>Thorsten Brants</author>
<author>Phillipp Koehn</author>
<author>Tony Robinson</author>
</authors>
<title>One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling.</title>
<date>2014</date>
<booktitle>In INTERSPEECH</booktitle>
<contexts>
<context position="1415" citStr="Chelba et al. (2014)" startWordPosition="206" endWordPosition="209">sks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets. They found that a recurrent neural network language model (RNNLM) combined with a 9-gram MaxEnt model was able to give the best results and lowest perplexity. In this work we investigate a possible extension of RNNLM, by allowing it to continue learning and adapting during testing. The model keeps a vector representation of the current sentence that is being processed, and continuously modifies it based on an error signal. We refer to this as a version of online learning, as gradient descent is us</context>
<context position="9055" citStr="Chelba et al., 2014" startWordPosition="1462" endWordPosition="1465">set, and when given unseen test data, the system optimises the paragraph vector according to the error signal. They use these vectors as input to a logistic regression classifier and achieve state-of-the-art performance on sentiment classification of movie reviews. However, they did not consider the effect of this model modification directly on the task of language modelling. While the system of Le and Mikolov (2014) uses a basic feedforward language model, we extend the idea to recurrent neural network language models, as they are currently used in state-ofthe-art language modelling systems (Chelba et al., 2014). Attaching the document vector to the input layer is not preferable for RNNLM, as the error is only backpropagated into the input layer after several time steps. When this time step is reached and the network is unrolled to perform backpropagation through time, several words have already passed without receiving any additional information. Since our implementation performs the unrolling only at the end of each sentence, the updates would not have any effect. Therefore, we attach the document vector directly to the output layer, in parallel with the recurrent hidden component. Parameters in th</context>
</contexts>
<marker>Chelba, Mikolov, Schuster, Ge, Brants, Koehn, Robinson, 2014</marker>
<rawString>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2014. One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling. In INTERSPEECH 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation. June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Chorowski</author>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>End-to-end Continuous Speech Recognition using Attention-based Recurrent NN : First Results.</title>
<date>2014</date>
<contexts>
<context position="850" citStr="Chorowski et al., 2014" startWordPosition="119" endWordPosition="122"> online learning in recurrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 1 Introduction In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several </context>
</contexts>
<marker>Chorowski, Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. End-to-end Continuous Speech Recognition using Attention-based Recurrent NN : First Results.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>Proceedings of the 25th international conference on Machine learning.</booktitle>
<contexts>
<context position="2150" citStr="Collobert and Weston, 2008" startWordPosition="326" endWordPosition="329">that a recurrent neural network language model (RNNLM) combined with a 9-gram MaxEnt model was able to give the best results and lowest perplexity. In this work we investigate a possible extension of RNNLM, by allowing it to continue learning and adapting during testing. The model keeps a vector representation of the current sentence that is being processed, and continuously modifies it based on an error signal. We refer to this as a version of online learning, as gradient descent is used to optimise the vector even during testing. The technique is inspired by work on representation learning (Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2013), especially Le and Mikolov (2014) who use a related model to learn representations for text classification. We extend the idea to recurrent models and apply it to the task of language modelling. Our results indicate that by exchanging some existing model parameters for a component using online learning, the system is able to achieve lower perplexity while also reducing the necessary computation. 2 RNNLM We base our implementation of the RNNLM on Mikolov et al. (2011), shown in Figure 1. The input layer to the network consists of a 1-hot vector rep</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of the 25th international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Navdeep Jaitly</author>
<author>Abdel-rahman Mohamed</author>
</authors>
<title>Hybrid speech recognition with deep bidirectional LSTM.</title>
<date>2013</date>
<booktitle>In ASRU</booktitle>
<contexts>
<context position="872" citStr="Graves et al., 2013" startWordPosition="123" endWordPosition="126">rrent neural network language models. The model keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 1 Introduction In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language mo</context>
</contexts>
<marker>Graves, Jaitly, Mohamed, 2013</marker>
<rawString>Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. 2013. Hybrid speech recognition with deep bidirectional LSTM. In ASRU 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent Continuous Translation Models.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="925" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="129" endWordPosition="132">odel keeps a separate vector representation of the current unit of text being processed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 1 Introduction In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets. They found t</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In The 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1015" citStr="Kalchbrenner et al., 2014" startWordPosition="143" endWordPosition="146">aptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 1 Introduction In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets. They found that a recurrent neural network language model (RNNLM) combined with a 9-gram MaxEnt model </context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In The 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard Zemel</author>
</authors>
<title>Multimodal Neural Language Models.</title>
<date>2014</date>
<booktitle>In ICML</booktitle>
<contexts>
<context position="1069" citStr="Kiros et al., 2014" startWordPosition="151" endWordPosition="154">ments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 1 Introduction In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets. They found that a recurrent neural network language model (RNNLM) combined with a 9-gram MaxEnt model was able to give the best results and lowest perplexit</context>
</contexts>
<marker>Kiros, Salakhutdinov, Zemel, 2014</marker>
<rawString>Ryan Kiros, Ruslan Salakhutdinov, and Richard Zemel. 2014. Multimodal Neural Language Models. In ICML 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<date>2014</date>
<booktitle>Distributed Representations of Sentences and Documents. In ICML,</booktitle>
<volume>32</volume>
<contexts>
<context position="987" citStr="Mikolov, 2014" startWordPosition="141" endWordPosition="142">rocessed and adaptively adjusts it after each prediction. The initial experiments give promising results, indicating that the method is able to increase language modelling accuracy, while also decreasing the parameters needed to store the model along with the computation required at each step. 1 Introduction In recent years, neural network models have shown impressive performance on many natural language processing tasks, such as speech recognition (Chorowski et al., 2014; Graves et al., 2013), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014), text classification (Le and Mikolov, 2014; Kalchbrenner et al., 2014) and image description generation (Kiros et al., 2014). One of the main advantages of these methods is the ability to learn smooth vector representations for words, thereby reducing the sparsity problem inherent in any natural language dataset. Language modelling is another task where neural networks have delivered excellent results (Bengio et al., 2003; Mikolov et al., 2011). Chelba et al. (2014) have recently benchmarked several well-known language models by training on very large datasets. They found that a recurrent neural network language model (RNNLM) combined</context>
<context position="2230" citStr="Mikolov (2014)" startWordPosition="342" endWordPosition="343">as able to give the best results and lowest perplexity. In this work we investigate a possible extension of RNNLM, by allowing it to continue learning and adapting during testing. The model keeps a vector representation of the current sentence that is being processed, and continuously modifies it based on an error signal. We refer to this as a version of online learning, as gradient descent is used to optimise the vector even during testing. The technique is inspired by work on representation learning (Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2013), especially Le and Mikolov (2014) who use a related model to learn representations for text classification. We extend the idea to recurrent models and apply it to the task of language modelling. Our results indicate that by exchanging some existing model parameters for a component using online learning, the system is able to achieve lower perplexity while also reducing the necessary computation. 2 RNNLM We base our implementation of the RNNLM on Mikolov et al. (2011), shown in Figure 1. The input layer to the network consists of a 1-hot vector representing the previous word in the sequence, and the hidden vector from the prev</context>
<context position="8180" citStr="Mikolov (2014)" startWordPosition="1326" endWordPosition="1327">s, online learning becomes more difficult to apply. Each sentence contains very little additional data, and even if the language model is adjusted after every individual word, it only obtains evidence of previous words in the sentence, whereas these words are relatively unlikely to occur again in the same sentence. Therefore, instead of adjusting individual word representations, our approach learns a distributed document vector to represent the specific unit of text that is currently being processed. This vector is then used as additional evidence when calculating output probabilities. Le and Mikolov (2014) use a similar method for learning vector representations of documents and paragraphs. They construct a feedforward language model and include a paragraph vector as an additional vector in the input layer. The model parameters are trained on the training set, and when given unseen test data, the system optimises the paragraph vector according to the error signal. They use these vectors as input to a logistic regression classifier and achieve state-of-the-art performance on sentiment classification of movie reviews. However, they did not consider the effect of this model modification directly o</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In ICML, volume 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Anoop Deoras</author>
<author>Luk´as Burget</author>
<author>Jan Cernock´y</author>
</authors>
<title>RNNLM - Recurrent neural network language modeling toolkit.</title>
<date>2011</date>
<booktitle>In ASRU 2011 Demo Session.</booktitle>
<marker>Mikolov, Kombrink, Deoras, Burget, Cernock´y, 2011</marker>
<rawString>Toma&amp;quot;s Mikolov, Stefan Kombrink, Anoop Deoras, Luk´a&amp;quot;s Burget, and Jan &amp;quot;Cernock´y. 2011. RNNLM - Recurrent neural network language modeling toolkit. In ASRU 2011 Demo Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space. ICLR Workshop,</title>
<date>2013</date>
<pages>1--12</pages>
<contexts>
<context position="2196" citStr="Mikolov et al., 2013" startWordPosition="334" endWordPosition="337">LM) combined with a 9-gram MaxEnt model was able to give the best results and lowest perplexity. In this work we investigate a possible extension of RNNLM, by allowing it to continue learning and adapting during testing. The model keeps a vector representation of the current sentence that is being processed, and continuously modifies it based on an error signal. We refer to this as a version of online learning, as gradient descent is used to optimise the vector even during testing. The technique is inspired by work on representation learning (Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2013), especially Le and Mikolov (2014) who use a related model to learn representations for text classification. We extend the idea to recurrent models and apply it to the task of language modelling. Our results indicate that by exchanging some existing model parameters for a component using online learning, the system is able to achieve lower perplexity while also reducing the necessary computation. 2 RNNLM We base our implementation of the RNNLM on Mikolov et al. (2011), shown in Figure 1. The input layer to the network consists of a 1-hot vector representing the previous word in the sequence, a</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. ICLR Workshop, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>A Scalable Hierarchical Distributed Language Model.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2173" citStr="Mnih and Hinton, 2008" startWordPosition="330" endWordPosition="333">ork language model (RNNLM) combined with a 9-gram MaxEnt model was able to give the best results and lowest perplexity. In this work we investigate a possible extension of RNNLM, by allowing it to continue learning and adapting during testing. The model keeps a vector representation of the current sentence that is being processed, and continuously modifies it based on an error signal. We refer to this as a version of online learning, as gradient descent is used to optimise the vector even during testing. The technique is inspired by work on representation learning (Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2013), especially Le and Mikolov (2014) who use a related model to learn representations for text classification. We extend the idea to recurrent models and apply it to the task of language modelling. Our results indicate that by exchanging some existing model parameters for a component using online learning, the system is able to achieve lower perplexity while also reducing the necessary computation. 2 RNNLM We base our implementation of the RNNLM on Mikolov et al. (2011), shown in Figure 1. The input layer to the network consists of a 1-hot vector representing the previous </context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2008. A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems, pages 1–8.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003874">
<title confidence="0.993979">
Evaluation methods for unsupervised word embeddings
</title>
<author confidence="0.99581">
Tobias Schnabel
</author>
<affiliation confidence="0.988148">
Cornell University
</affiliation>
<address confidence="0.77252">
Ithaca, NY, 14853
</address>
<email confidence="0.997493">
tbs49@cornell.edu
</email>
<author confidence="0.967338">
Igor Labutov
</author>
<affiliation confidence="0.96723">
Cornell University
</affiliation>
<address confidence="0.759945">
Ithaca, NY, 14853
</address>
<email confidence="0.997243">
iil4@cornell.edu
</email>
<author confidence="0.9870335">
David Mimno,
Thorsten Joachims
</author>
<affiliation confidence="0.98627">
Cornell University
</affiliation>
<email confidence="0.9840265">
mimno@cornell.edu,
tj@cs.cornell.edu
</email>
<sectionHeader confidence="0.993854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918066666666">
We present a comprehensive study of eval-
uation methods for unsupervised embed-
ding techniques that obtain meaningful
representations of words from text. Differ-
ent evaluations result in different orderings
of embedding methods, calling into ques-
tion the common assumption that there is
one single optimal vector representation.
We present new evaluation techniques that
directly compare embeddings with respect
to specific queries. These methods re-
duce bias, provide greater insight, and
allow us to solicit data-driven relevance
judgments rapidly and accurately through
crowdsourcing.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974523809524">
Neural word embeddings represent meaning via
geometry. A good embedding provides vector rep-
resentations of words such that the relationship be-
tween two vectors mirrors the linguistic relation-
ship between the two words. Despite the growing
interest in vector representations of semantic in-
formation, there has been relatively little work on
direct evaluations of these models. In this work,
we explore several approaches to measuring the
quality of neural word embeddings. In particu-
lar, we perform a comprehensive analysis of eval-
uation methods and introduce novel methods that
can be implemented through crowdsourcing, pro-
viding better insights into the relative strengths of
different embeddings.
Existing schemes fall into two major categories:
extrinsic and intrinsic evaluation. In extrinsic eval-
uation, we use word embeddings as input features
to a downstream task and measure changes in per-
formance metrics specific to that task. Examples
include part-of-speech tagging and named-entity
recognition (Pennington et al., 2014). Extrinsic
evaluation only provides one way to specify the
goodness of an embedding, and it is not clear how
it connects to other measures.
Intrinsic evaluations directly test for syntactic or
semantic relationships between words (Mikolov et
al., 2013a; Baroni et al., 2014). These tasks typi-
cally involve a pre-selected set of query terms and
semantically related target words, which we refer
to as a query inventory. Methods are evaluated
by compiling an aggregate score for each method
such as a correlation coefficient, which then serves
as an absolute measure of quality. Query inven-
tories have so far been collected opportunistically
from prior work in psycholinguistics, information
retrieval (Finkelstein et al., 2002), and image anal-
ysis (Bruni et al., 2014). Because these inventories
were not constructed for word embedding evalu-
ation, they are often idiosyncratic, dominated by
specific types of queries, and poorly calibrated to
corpus statistics.
To remedy these problems, this paper makes
the following contributions. First, this is the first
paper to conduct a comprehensive study cover-
ing a wide range of evaluation criteria and popu-
lar embedding techniques. In particular, we study
how outcomes from three different evaluation cri-
teria are connected: word relatedness, coherence,
downstream performance. We show that using dif-
ferent criteria results in different relative orderings
of embeddings. These results indicate that embed-
ding methods should be compared in the context
of a specific task, e.g., linguistic insight or good
downstream performance.
Second, we study the connections between di-
rect evaluation with real users and pre-collected
offline data. We propose a new approach to evalu-
ation that focuses on direct comparison of embed-
dings with respect to individual queries rather than
overall summary scores. Because we phrase all
tasks as choice problems rather than ordinal rel-
evance tasks, we can ease the burden of the an-
</bodyText>
<page confidence="0.961644">
298
</page>
<note confidence="0.9850355">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 298–307,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999981875">
notators. We show that these evaluations can be
gathered efficiently from crowdsourcing. Our re-
sults also indicate that there is in fact strong corre-
lation between the results of automated similarity
evaluation and direct human evaluation. This re-
sult justifies the use of offline data, at least for the
similarity task.
Third, we propose a model- and data-driven ap-
proach to constructing query inventories. Rather
than picking words in an ad hoc fashion, we se-
lect query words to be diverse with respect to
their frequency, parts-of-speech and abstractness.
To facilitate systematic evaluation and compar-
ison of new embedding models, we release a
new frequency-calibrated query inventory along
with all user judgments at http://www.cs.
cornell.edu/˜schnabts/eval/.
Finally, we observe that word embeddings en-
code a surprising degree of information about
word frequency. We found this was true even in
models that explicitly reserve parameters to com-
pensate for frequency effects. This finding may
explain some of the variability across embeddings
and across evaluation methods. It also casts doubt
on the common practice of using the vanilla co-
sine similarity as a similarity measure in the em-
bedding space.
It is important to note that this work is a survey
of evaluation methods not a survey of embedding
methods. The specific example embeddings pre-
sented here were chosen as representative samples
only, and may not be optimal.
</bodyText>
<sectionHeader confidence="0.909351" genericHeader="introduction">
2 Word embeddings
</sectionHeader>
<bodyText confidence="0.999544">
We refer to a word embedding as a mapping V →
RD : w H w� that maps a word w from a vocabu-
lary V to a real-valued vector w� in an embedding
space of dimensionality D.
Following previous work (Collobert et al.,
2011; Mikolov et al., 2013a) we use the com-
monly employed cosine similarity, defined as
</bodyText>
<equation confidence="0.363282">
w1 ·~w2 for all similar-
11~w11111~w211,
</equation>
<bodyText confidence="0.999808813953488">
ity computations in the embedding space. The list
of nearest neighbors of a word w are all words
v E V \ {w}, sorted in descending order by
similarity(w, v). We will denote w as the query
word in the remainder of this paper.
All experiments in this paper are carried out
on six popular unsupervised embedding meth-
ods. These embeddings form a representative
but incomplete subset; and since we are study-
ing evaluation methods and not embeddings them-
selves, no attempt has been made to optimize these
embeddings. The first two embedding models,
the CBOW model of word2vec (Mikolov et al.,
2013a) and C&amp;W embeddings (Collobert et al.,
2011) both are motivated by a probabilistic predic-
tion approach. Given a number of context words
around a target word w, these models formulate
the embedding task as that of finding a representa-
tion that is good at predicting w from the context
representations.
The second group of models, Hellinger PCA
(Lebret and Collobert, 2014), GloVe (Pennington
et al., 2014), TSCCA (Dhillon et al., 2012) and
Sparse Random Projections (Li et al., 2006) fol-
low a reconstruction approach: word embeddings
should be able to capture as much relevant infor-
mation from the original co-occurrence matrix as
possible.
Training corpus. We tried to make the compar-
ison as fair as possible. As the C&amp;W embeddings
were only available pretrained on a November
2007 snapshot of Wikipedia, we chose the closest
available Wikipedia dump (2008-03-01) for train-
ing the other models. We tokenized the data us-
ing the Stanford tokenizer (Manning et al., 2014).
Like Collobert et al. (2011), we lower-cased all
words and replaced digits with zeros.
Details. All models embedded words into a 50-
dimensional space (D = 50). As implemented,
each method uses a different vocabulary, so we
computed the intersection of the six vocabularies
and used the resulting set of 103,647 words for all
nearest-neighbor experiments.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="method">
3 Relatedness
</sectionHeader>
<bodyText confidence="0.999876857142857">
We begin with intrinsic evaluation of relatedness
using both pre-collected human evaluations and a
novel online user study. Section 3.1 introduces the
list of datasets that is commonly used as a bench-
mark for embedding methods. There, embeddings
are evaluated individually and only their final
scores are compared, hence we refer to this sce-
nario as absolute intrinsic evaluation. We present
a new scenario, comparative intrinsic evaluation,
in which we ask people directly for their prefer-
ences among different embeddings. We demon-
strate that we can achieve the same results as of-
fline, absolute metrics using online, comparative
metrics.
</bodyText>
<equation confidence="0.905507">
similarity(w1, w2) =
</equation>
<page confidence="0.995347">
299
</page>
<subsectionHeader confidence="0.995829">
3.1 Absolute intrinsic evaluation
</subsectionHeader>
<bodyText confidence="0.9984428">
For the absolute intrinsic evaluation, we used the
same datasets and tasks as Baroni et al. (2014).
While we present results on all tasks for complete-
ness, we will mainly focus on relatedness in this
section. There are four broad categories:
</bodyText>
<listItem confidence="0.947865555555555">
• Relatedness: These datasets contain relat-
edness scores for pairs of words; the cosine
similarity of the embeddings for two words
should have high correlation (Spearman or
Pearson) with human relatedness scores.
• Analogy: This task was popularized by
Mikolov et al. (2013a). The goal is to find
a term x for a given term y so that x : y best
resembles a sample relationship a : b.
• Categorization: Here, the goal is to re-
cover a clustering of words into different cat-
egories. To do this, the corresponding word
vectors of all words in a dataset are clustered
and the purity of the returned clusters is com-
puted with respect to the labeled dataset.
• Selectional preference: The goal is to deter-
mine how typical a noun is for a verb either
as a subject or as an object (e.g., people eat,
</listItem>
<bodyText confidence="0.999408680851064">
but we rarely eat people). We follow the pro-
cedure that is outlined in Baroni et al. (2014).
Several important design questions come up
when designing reusable datasets for evaluating
relatedness. While we focus mainly on challenges
that arise in the relatedness evaluation task, many
of the questions discussed also apply to other sce-
narios.
Query inventory. How we pick the word
pairs to evaluate affects the results of the evalu-
ation. The commonly-used WordSim-353 dataset
(Finkelstein et al., 2002), for example, only tries
to have word pairs with a diverse set of similarity
scores. The more recent MEN dataset (Bruni et
al., 2014) follows a similar strategy, but restricts
queries to words that occur as annotations in an
image dataset. However, there are more important
criteria that should be considered in order to cre-
ate a diverse dataset: (i) the frequency of the words
in the English language (ii) the parts of speech of
the words and (iii) abstractness vs. concreteness
of the terms. Not only is frequency important be-
cause we want to test the quality of embeddings
on rare words, but also because it is related with
distance in the embedding space as we show later
and should be explicitly considered.
Metric aggregation. The main conceptual
shortcoming of using correlation-based metrics is
that they aggregate scores of different pairs —
even though these scores can vary greatly in the
embedding space. We can view the relatedness
task as the task of evaluating a set of rankings,
similar to ranking evaluation in Information Re-
trieval. More specifically, we have one query for
each unique query word w and rank all remaining
words v in the vocabulary accordingly. The prob-
lem now is that we usually cannot directly com-
pare scores from different rankings (Aslam and
Montague, 2001) as their scores are not guaran-
teed to have the same ranges. An even worse case
is the following scenario. Assume we use rank
correlation as our metric. As a consequence, we
need our gold ranking to define an order on all
the word pairs. However, this also means that we
somehow need to order completely unrelated word
pairs; for example, we have to decide whether
(dog, cat) is more similar than (banana, apple).
</bodyText>
<subsectionHeader confidence="0.999796">
3.2 Absolute results
</subsectionHeader>
<bodyText confidence="0.9999451">
Table 1 presents the results on 14 different datasets
for the six embedding models. We excluded ex-
amples from datasets that contained words not in
our vocabulary. For the relatedness and selective
preference tasks, the numbers in the table indicate
the correlation coefficient of human scores and the
cosine similarity times 100. The numbers for the
categorization tasks reflect the purities of the re-
sulting clusters. For the analogy task, we report
accuracy.
CBOW outperforms other embeddings on 10 of
14 datasets. CBOW especially excels at the relat-
edness and analogy tasks, but fails to surpass other
models on the selective preferences tasks. Ran-
dom projection performs worst in 13 out of the
14 tasks, being followed by Hellinger PCA. C&amp;W
and TSCCA are similar on average, but differ
across datasets. Moreover, although TSCCA and
GloVe perform similarly on most tasks, TSCCA
suffers disproportionally on the analogy tasks.
</bodyText>
<subsectionHeader confidence="0.99556">
3.3 Comparative intrinsic evaluation
</subsectionHeader>
<bodyText confidence="0.9999684">
In comparative evaluation, users give direct feed-
back on the embeddings themselves, so we do not
have to define a metric that compares scored word
pairs. Rather than defining both query and target
words, we need only choose query words since the
</bodyText>
<page confidence="0.987132">
300
</page>
<table confidence="0.998840875">
relatedness categorization sel. prefs analogy
rg ws wss wsr men toefl ap esslli batt. up mcrae an ansyn ansem average
CBOW 74.0 64.0 71.5 56.5 70.7 66.7 65.9 70.5 85.2 24.1 13.9 52.2 47.8 57.6 58.6
GloVe 63.7 54.8 65.8 49.6 64.6 69.4 64.1 65.9 77.8 27.0 18.4 42.2 44.2 39.7 53.4
TSCCA 57.8 54.4 64.7 43.3 56.7 58.3 57.5 70.5 64.2 31.0 14.4 15.5 19.0 11.1 44.2
C&amp;W 48.1 49.8 60.7 40.1 57.5 66.7 60.6 61.4 80.2 28.3 16.0 10.9 12.2 9.3 43.0
H-PCA 19.8 32.9 43.6 15.1 21.3 54.2 34.1 50.0 42.0 -2.5 3.2 3.0 2.4 3.7 23.1
Rand. Proj. 17.1 19.5 24.9 16.1 11.3 51.4 21.9 38.6 29.6 -8.5 1.2 1.0 0.3 1.9 16.2
</table>
<tableCaption confidence="0.825847">
Table 1: Results on absolute intrinsic evaluation. The best result for each dataset is highlighted in bold.
The second row contains the names of the corresponding datasets.
</tableCaption>
<bodyText confidence="0.992996425">
embeddings themselves will be used to define the
comparable target words.
Query inventory. We compiled a diverse in-
ventory of 100 query words that balance fre-
quency, part of speech (POS), and concreteness.
First, we selected 10 out of 45 broad categories
from WordNet (Miller, 1995). We then chose an
equal number of categories that mostly contained
abstract concepts and categories that referred to
concrete concepts. Among those categories, we
had one for adjectives and adverbs each, and four
for nouns and verbs each. From each category,
we drew ten random words with the restriction
that there be exactly three rare words (i.e., occur-
ring fewer than 2500 times in the training corpus)
among the ten.
Details. Our experiments were performed with
users from Amazon Mechanical Turk (MTurk)
that were native speakers of English with sufficient
experience and positive feedback on the Amazon
Mechanical Turk framework.
For each of the 100 query words in the dataset,
the nearest neighbors at ranks k ∈ {1, 5, 50} for
the six embeddings were retrieved. For each query
word and k, we presented the six words along with
the query word to the users. Each Turker was re-
quested to evaluate between 25 and 50 items per
task, where an item corresponds to the query word
and the set of 6 retrieved neighbor words from
each of the 6 embeddings. The payment was be-
tween $0.01 and $0.02 per item. The users were
then asked to pick the word that is most similar ac-
cording to their perception (the instructions were
almost identical to the WordSim-353 dataset in-
structions). Duplicate words were consolidated,
and a click was counted for all embeddings that
returned that word. An option “I don’t know the
meaning of one (or several) of the words” was also
provided as an alternative. Table 2 shows an exam-
ple instance that was given to the Turkers.
</bodyText>
<figure confidence="0.952009666666667">
Query: skillfully
(a) swiftly (b) expertly
(c) cleverly (d) pointedly
</figure>
<tableCaption confidence="0.883236">
Table 2: Example instance of comparative in-
</tableCaption>
<bodyText confidence="0.966317909090909">
trinsic evaluation task. The presented options in
this example are nearest neighbors to the query
word according to (a) C&amp;W, (b) CBOW, GloVe,
TSCCA (c) Rand. Proj. and (d) H-PCA.
The combination of 100 query words and 3
ranks yielded 300 items on which we solicited
judgements by a median of 7 Turkers (min=5,
max=14). We compare embeddings by average
win ratio, where the win ratio was how many times
raters chose embedding e divided by the number
of total ratings for item i.
</bodyText>
<subsectionHeader confidence="0.972406">
3.4 Comparative results
</subsectionHeader>
<bodyText confidence="0.999703294117647">
Overall comparative results replicate previous re-
sults. Figure 1(a) shows normalized win ratio
scores for each embedding across 3 conditions
corresponding to the frequency of the query word
in the training corpus. The scores were normal-
ized to sum to one in each condition to emphasize
relative differences. CBOW in general performed
the best and random projection the worst (p-value
&lt; 0.05 for all pairs except H-PCA and C&amp;W in
comparing un-normalized score differences for the
ALL-FREQ condition with a randomized permuta-
tion test). The novel comparative evaluations cor-
respond both in rank and in relative margins to
those shown in Table 1.
Unlike previous results, we can now show
differences beyond the nearest neighbors. Fig-
ure 1(b) presents the same results, but this time
</bodyText>
<page confidence="0.985156">
301
</page>
<figure confidence="0.999757866666666">
Score
(a) Normalized scores by global word frequency.
(c) Normalized scores by part of speech.
Rand. Proj H-PCA C&amp;W TSCCA GloVe CBOW
(b) Normalized scores by nearest neighbor rank k.
(d) Normalized scores by category.
Rand. Proj H-PCA C&amp;W TSCCA GloVe CBOW
Score
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
freq 2500
freq &gt; 2500
all freq
Rand. Proj H-PCA C&amp;W TSCCA GloVe CBOW
Score
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
adj POS
adv POS
noun POS
verb POS
0.40
0.30
0.20
0.10
0.00
0.35
0.25
0.15
0.05
1 NN
5 NN
50 NN
Rand. Proj H-PCA C&amp;W TSCCA GloVe CBOW
Score
0.40
0.30
0.20
0.10
0.00
0.35
0.25
0.15
0.05
abstract
concrete
</figure>
<figureCaption confidence="0.999987">
Figure 1: Direct comparison task
</figureCaption>
<bodyText confidence="0.999973594594595">
broken up by the rank k of the neighbors that were
compared. CBOW has its strengths especially at
rank k = 1. For neighbors that appear after that,
CBOW does not necessarily produce better em-
beddings. In fact, it even does worse for k = 50
than GloVe. It is important to note, however, that
we cannot make absolute statements about how
performance behaves across different values of k
since each assessment is always relative to the
quality of all other embeddings.
We balanced our query inventory also with re-
spect to parts of speech and abstractness vs. con-
creteness. Figure 1(c) shows the relative per-
formances of all embeddings for the four POS
classes (adjectives, adverbs, nouns and verbs).
While most embeddings show relatively homoge-
neous behaviour across the four classes, GloVe
suffers disproportionally on adverbs. Moving on
to Figure 1(d), we can see a similar behavior for
TSCCA: Its performance is much lower on con-
crete words than on abstract ones. This differ-
ence may be important, as recent related work
finds that simply differentiating between general
and specific terms explains much of the observed
variation between embedding methods in hierar-
chical classification tasks (Levy et al., 2015b). We
take the two observations above as evidence that a
more fine-grained analysis is necessary in discern-
ing different embedding methods.
As a by-product, we observed that there was
no embedding method that consistently performed
best on all of the four different absolute evaluation
tasks. However, we would like to reiterate that our
goal is not to identify one best method, but rather
point out that different evaluations (e.g., changing
the rank k of the nearest neighbors in the compar-
ison task) result in different outcomes.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="method">
4 Coherence
</sectionHeader>
<bodyText confidence="0.992382111111111">
In the relatedness task we measure whether a pair
of semantically similar words are near each other
in the embedding space. In this novel coherence
task we assess whether groups of words in a small
neighborhood in the embedding space are mutu-
ally related. Previous work has used this property
for qualitative evaluation using visualizations of
2D projections (Turian et al., 2010), but we are not
aware of any work using local neighborhoods for
</bodyText>
<page confidence="0.997381">
302
</page>
<bodyText confidence="0.999862333333334">
quantitative evaluation. Good embeddings should
have coherent neighborhoods for each word, so
inserting a word not belonging to this neighbor-
hood should be easy to spot. Similar to Chang et
al. (2009), we presented Turkers with four words,
three of which are close neighbors and one of
which is an “intruder.” For each of the 100 words
in our query set of Section 3.3, we retrieved the
two nearest neighbors. These words along with the
query word defined the set of (supposedly) good
options. Table 3 shows an example instance that
was given to the Turkers.
</bodyText>
<figure confidence="0.9791045">
(a) finally (b) eventually
(c) immediately (d) put
</figure>
<tableCaption confidence="0.844523">
Table 3: Example instance of intrusion task. The
query word is option (a), intruder is (d).
</tableCaption>
<bodyText confidence="0.99871934375">
To normalize for frequency-based effects, we
computed the average frequency avg of the three
words in this set and chose the intruder word to be
the first word that had a frequency of avg ± 500
starting at rank 100 of the list of nearest neighbors.
Results. In total, we solicited judgments on 600
items (100 query words for each of the 6 em-
beddings) from a median of 7 Turkers (min=4,
max=11) per item, where each Turker evaluated
between 25 and 50 items per task. Figure 2 shows
the results of the intrusion experiment. The evalu-
ation measure is micro-averaged precision for an
embedding across 100 query words, where per-
item precision is defined as the number of raters
that discovered the intruder divided the total num-
ber of raters of item i. Random guessing would
achieve an average precision of 0.25.
All embeddings perform better than guessing,
indicating that there is at least some coherent
structure captured in all of them. However,
the best performing embeddings at this task are
TSCCA, CBOW and GloVe (the precision mean
differences were not significant under a random
permutation test), while TSCCA attains greater
precision (p &lt; 0.05) in relation to C&amp;W, H-PCA
and random projection embeddings. These re-
sults are in contrast to the direct comparison study,
where the performance of TSCCA was found to be
significantly worse than that of CBOW. However,
the order of the last three embeddings remains un-
changed, implying that performance on the intru-
sion task and performance on the direct compari-
</bodyText>
<figureCaption confidence="0.974155">
Figure 2: Intrusion task: average precision by
global word frequency.
</figureCaption>
<bodyText confidence="0.9944144">
son task are correlated. CBOW and C&amp;W seem
to do equally well on rare and frequent words,
whereas the other models’ performance suffers on
rare words.
Discussion. Evaluation of set-based properties
of embeddings may produce different results from
item-based evaluation: rankings we got from the
intrusion task did not match the rankings we ob-
tained from the relatedness task. Pairwise similar-
ities seem to be only part of the information that is
encoded in word embeddings, so looking at more
global measures is necessary for a better under-
standing of differences between embeddings.
We choose intruder words based on similar but
lower-ranked words, so an embedding could score
well on this task by doing an unusually bad job
at returning less-closely related words. However,
the results in Figure 1(b) suggest that there is lit-
tle differences at higher ranks (rank 50) between
embeddings.
</bodyText>
<sectionHeader confidence="0.995197" genericHeader="method">
5 Extrinsic Tasks
</sectionHeader>
<bodyText confidence="0.999670916666667">
Extrinsic evaluations measure the contribution of
a word embedding model to a specific task. There
is an implicit assumption in the use of such eval-
uations that there is a consistent, global ranking
of word embedding quality, and that higher qual-
ity embeddings will necessarily improve results on
any downstream task. We find that this assumption
does not hold: different tasks favor different em-
beddings. Although these evaluations are useful
in characterizing the relative strengths of different
models, we do not recommend that they be used as
a proxy for a general notion of embedding quality.
</bodyText>
<figure confidence="0.999451615384615">
Rand. Proj H-PCA C&amp;W GloVe CBOW TSCCA
Outlier Detection Precision
0.7
0.6
0.5
0.4
0.3
0.1
0.0
0.2
freq 2500
freq &gt; 2500
all freq
</figure>
<page confidence="0.996966">
303
</page>
<table confidence="0.999297375">
dev test p-value
Baseline 94.18 93.78 0.000
Rand. Proj. 94.33 93.90 0.006
GloVe 94.28 93.93 0.015
H-PCA 94.48 93.96 0.029
C&amp;W 94.53 94.12
CBOW 94.32 93.93 0.012
TSCCA 94.53 94.09 0.357
</table>
<tableCaption confidence="0.989331666666667">
Table 4: F1 chunking results using different word
embeddings as features. The p-values are with re-
spect to the best performing method.
</tableCaption>
<table confidence="0.99958025">
test p-value
BOW (baseline) 88.90 7.45·10−14
Rand. Proj. 62.95 7.47·10−12
GloVe 74.87 5.00·10−2
H-PCA 69.45 6.06·10−11
C&amp;W 72.37 1.29·10−7
CBOW 75.78
TSCCA 75.02 7.28·10−4
</table>
<tableCaption confidence="0.991566">
Table 5: F1 sentiment analysis results using differ-
</tableCaption>
<bodyText confidence="0.997285214285714">
ent word embeddings as features. The p-values are
with respect to the best performing embedding.
Noun phrase chunking. First we use a noun
phrase chunking task similar to that used by Turian
et al. (2010). The only difference is that we nor-
malize all word vectors to unit length, rather than
scaling them with some custom factor, before giv-
ing them to the conditional random field (CRF)
model as input. We expect that this task will be
more sensitive to syntactic information than to se-
mantic information.
Sentiment classification. Second we use a re-
cently released dataset for binary sentiment clas-
sification by Maas et al. (2011). The dataset con-
tains 50K movie reviews with a balanced distribu-
tion of binary polarity labels. We evaluate the rel-
ative performance of word embeddings at this task
as follows: we generate embedding-only features
for each review by computing a linear combina-
tion of word embeddings weighted by the num-
ber of times that the word appeared in the review
(using the same bag-of-words features as Maas
et al. (2011)). A LIBLINEAR logistic regression
model (Fan et al., 2008) with the default parame-
ters is trained and evaluated using 10 fold cross-
validation. A vanilla bag of words feature set is
the baseline (denoted as BOW here). We expect
that this task will be more sensitive to semantic
information than syntactic information.
Results. Table 4 shows the average F1-scores
for the chunking task. The p-values were com-
puted using randomization (Yeh, 2000) on the sen-
tence level. First, we can observe that adding word
vectors as features results in performance lifts with
all embeddings when compared to the baseline.
The performance of C&amp;W and TSCCA is statis-
tically not significant, and C&amp;W does better than
all the remaining methods at the p = 0.05 level.
Surprisingly, although the performance of Ran-
dom Projections is still last, the gap to GloVe and
CBOW is now very small. Table 5 shows results
on the sentiment analysis task. We recover a sim-
ilar order of embeddings as in the absolute intrin-
sic evaluation, however, the order of TSCCA and
GloVe is now reversed.
Discussion. Performance on downstream tasks
is not consistent across tasks, and may not be con-
sistent with intrinsic evaluations. Comparing per-
formance across tasks may provide insight into
the information encoded by an embedding, but
we should not expect any specific task to act as
a proxy for abstract quality. Furthermore, if good
downstream performance is really the goal of an
embedding, we recommend that embeddings be
trained specifically to optimize a specific objective
(Lebret and Collobert, 2014).
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999978166666667">
We find consistent differences between word em-
beddings, despite the fact that they are operating
on the same input data and optimizing arguably
very similar objective functions (Pennington et al.,
2014; Levy and Goldberg, 2014). Recent work
suggests that many apparent performance differ-
ences on specific tasks are due to a lack of hyper-
parameter optimization (Levy et al., 2015a). Dif-
ferent algorithms are, in fact, encoding surpris-
ingly different information that may or may not
align with our desired use cases. For example,
we find that embeddings encode differing degrees
of information about word frequency, even after
length normalization. This result is surprising for
two reasons. First, many algorithms reserve dis-
tinct “intercept” parameters to absorb frequency-
based effects. Second, we expect that the ge-
ometry of the embedding space will be primar-
</bodyText>
<page confidence="0.998462">
304
</page>
<figureCaption confidence="0.955513">
Figure 3: Embeddings can accurately predict
</figureCaption>
<bodyText confidence="0.991835297297298">
whether a word is frequent or rare.
ily driven by semantics: the relatively small num-
ber of frequent words should be evenly distributed
through the space, while large numbers of rare,
specific words should cluster around related, but
more frequent, words.
We trained a logistic regression model to predict
word frequency categories based on word vectors.
The linear classifier was trained to put words ei-
ther in a frequent or rare category, with thresholds
varying from 100 to 50,000. At each threshold fre-
quency, we sampled the training sets to ensure a
consistent balance of the label distribution across
all frequencies. We used length-normalized em-
beddings, as rare words might have shorter vec-
tors resulting from fewer updates during training
(Turian et al., 2010). We report the mean accuracy
and standard deviation (1σ) using five-fold cross-
validation at each threshold frequency in Figure 3.
All word embeddings do better than random,
suggesting that they contain some frequency in-
formation. GloVe and TSCCA achieve nearly
100% accuracy on thresholds up to 1000. Unlike
all other embeddings, accuracy for C&amp;W embed-
dings increases for larger threshold values. Fur-
ther investigation revealed that the weight vector
direction changes gradually with the threshold fre-
quency — indicating that frequency seems to be
encoded in a smooth way in the embedding space.
Although GloVe and CBOW are the two best
performing embeddings on the intrinsic tasks, they
differ vastly in the amount of frequency informa-
tion they encode. As a consequence, we can con-
clude that most of the differences in frequency pre-
diction are not due to intrinsic properties of natu-
ral language: it is not the case that frequent words
naturally have only frequent neighbors.
</bodyText>
<figure confidence="0.8449575">
105
104
100 101 102 103
Nearest neighbor rank
</figure>
<figureCaption confidence="0.913173">
Figure 4: Avg. word rank by frequency in train-
ing corpus vs. nearest-neighbor rank in the C&amp;W
embedding space.
</figureCaption>
<bodyText confidence="0.99919964">
Word frequency information in the embedding
space also affects cosine similarity. For each of the
words in the WordSim-353 dataset, we queried for
the k = 1000 nearest neighbors. We then looked
up their frequency ranks in the training corpus and
averaged those ranks over all the query words. We
found a strong correlation between the frequency
of a word and its position in the ranking of near-
est neighbors in our experiments. Figure 4 shows
a power law relationship for C&amp;W embeddings
between a word’s nearest neighbor rank (w.r.t. a
query) and the word’s frequency rank in the train-
ing corpus (nn-rank ∼ 1000 · corpus-rank0.17).
This is a concern: the frequency of a word in the
language plays a critical role in word processing
of humans as well (Cattell, 1886). As a conse-
quence, we need to explicitly consider word fre-
quency as a factor in the experiment design. Also,
the above results mean that the commonly-used
cosine similarity in the embedding space for the
intrinsic tasks gets polluted by frequency-based
effects. We believe that further research should
address how to better measure linguistic relation-
ships between words in the embedding space, e.g.,
by learning a custom metric.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.999827625">
Mikolov et al. (2013b) demonstrate that cer-
tain linguistic regularities exist in the embedding
space. The authors show that by doing simple
vector arithmetic in the embedding space, one
can solve various syntactic and semantic analogy
tasks. This is different to previous work, which
phrased the analogy task as a classification prob-
lem (Turney, 2008). Surprisingly, word embed-
</bodyText>
<figure confidence="0.998094133333333">
CCA
C&amp;W
CBOW
GloVe
Rand. Proj
H-PCA
102 103 104 105
Word frequency
Accuracy 1.0
0.9
0.8
0.7
0.6
0.5
Avg. rank by corpus frequency
</figure>
<page confidence="0.996273">
305
</page>
<bodyText confidence="0.999921977272727">
dings seem to capture even more complex linguis-
tic properties. Chen et al. (2013) show that word
embeddings even contain information about re-
gional spellings (UK vs. US), noun gender and
sentiment polarity.
Previous work in evaluation for word embed-
dings can be divided into intrinsic and extrin-
sic evaluations. Intrinsic evaluations measure the
quality of word vectors by directly measuring
correlation between semantic relatedness and ge-
ometric relatedness, usually through inventories
of query terms. Focusing on intrinsic measures,
Baroni et al. (2014) compare word embeddings
against distributional word vectors on a variety of
query inventories and tasks. Faruqui and Dyer
(2014) provide a website that allows the automatic
evaluation of embeddings on a number of query
inventories. Gao et al. (2014) publish an improved
query inventory for the analogical reasoning task.
Finally, Tsvetkov et al. (2015) propose a new in-
trinsic measure that better correlates with extrinsic
performance. However, all these evaluations are
done on precollected inventories and mostly lim-
ited to local metrics like relatedness.
Extrinsic evaluations use embeddings as fea-
tures in models for other tasks, such as semantic
role labeling or part-of-speech tagging (Collobert
et al., 2011), and improve the performance of ex-
isting systems (Turian et al., 2010). However, they
have been less successful at other tasks such as
parsing (Andreas and Klein, 2014).
More work has been done in unsupervised se-
mantic modeling in the context of topic models.
One example is the word intrusion task (Chang et
al., 2009), in which annotators are asked to iden-
tify a random word inserted into the set of high
probability words for a given topic. Word embed-
dings do not produce interpretable dimensions, so
we cannot directly use this method, but we present
a related task based on nearest neighbors. Manual
evaluation is expensive and time-consuming, but
other work establishes that automated evaluations
can closely model human intuitions (Newman et
al., 2010).
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999977722222222">
There are many factors that affect word embed-
ding quality. Standard aggregate evaluations,
while useful, do not present a complete or con-
sistent picture. Factors such as word frequency
play a significant and previously unacknowledged
role. Word frequency also interferes with the
commonly-used cosine similarity measure. We
present a novel evaluation framework based on di-
rect comparisons between embeddings that pro-
vides more fine-grained analysis and supports sim-
ple, crowdsourced relevance judgments. We also
present a novel Coherence task that measures our
intuition that neighborhoods in the embedding
space should be semantically or syntactically re-
lated. We find that extrinsic evaluations, although
useful for highlighting specific aspects of embed-
ding performance, should not be used as a proxy
for generic quality.
</bodyText>
<sectionHeader confidence="0.998145" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9808392">
This research was funded in part through NSF
Award IIS-1513692. We would like to thank
Alexandra Schofield, Adith Swaminathan and all
other members of the NLP seminar for their help-
ful feedback.
</bodyText>
<sectionHeader confidence="0.997533" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855866666667">
Jacob Andreas and Dan Klein. 2014. How much do
word embeddings encode about syntax? In ACL:
Short Papers, pages 822–827.
Javed Aslam and Mark Montague. 2001. Models for
metasearch. In SIGIR, pages 276–284.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238–247.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. JAIR,
49:1–47.
James McKeen Cattell. 1886. The time taken up by
cerebral operations. Mind,, (42):220–242.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In
NIPS, pages 288–296.
Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and
Steven Skiena. 2013. The expressive power of word
embeddings. arXiv preprint: 1408.3456.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JLMR, 12:2493–2537.
Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster,
and Lyle H. Ungar. 2012. Two step CCA: A
new spectral method for estimating vector models
of words. In ICML, pages 1551–1558.
</reference>
<page confidence="0.990149">
306
</page>
<reference confidence="0.999759788732395">
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. JLMR,
9:1871–1874.
Manaal Faruqui and Chris Dyer. 2014. Community
evaluation and exchange of word vectors at word-
vectors.org. In ACL: System Demonstrations.
Lev Finkelstein, Ehud Rivlin Zach Solan Gadi Wolf-
man Evgeniy Gabrilovich, Yossi Matias, and Eytan
Ruppin. 2002. Placing search in context: The con-
cept revisited. TOIS, 20(1):116–131, January.
Bin Gao, Jiang Bian, and Tie-Yan Liu. 2014. Wor-
dRep: A benchmark for research on learning word
representations. ICML Workshop on Knowledge-
Powered Deep Learning for Text Mining.
R´emi Lebret and Ronan Collobert. 2014. Word em-
beddings through Hellinger PCA. In EACL, pages
482–490.
Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In NIPS,
pages 2177–2185.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015a.
Improving distributional similarity with lessons
learned from word embeddings. TACL.
Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015b. Do supervised distributional meth-
ods really learn lexical inference relations? In
NAACL.
Ping Li, Trevor J Hastie, and Kenneth W Church.
2006. Very sparse random projections. In KDD,
pages 287–296.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analysis.
In HLT-ACL, pages 142–150.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In ACL: System Demon-
strations, pages 55–60.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.
Tomas Mikolov, Wen-Tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751.
George A Miller. 1995. WordNet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of topic
coherence. In HLT-NAACL, pages 100–108.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In EMNLP.
Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation of
word vector representations by subspace alignment.
In EMNLP.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL, pages 384–
394.
Peter D. Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In
COLING, pages 905–912.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL,
pages 947–953.
</reference>
<page confidence="0.998594">
307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.188675">
<title confidence="0.999833">Evaluation methods for unsupervised word embeddings</title>
<author confidence="0.999897">Tobias Schnabel</author>
<affiliation confidence="0.999998">Cornell University</affiliation>
<address confidence="0.99997">Ithaca, NY, 14853</address>
<email confidence="0.997801">tbs49@cornell.edu</email>
<author confidence="0.891123">Igor Labutov</author>
<affiliation confidence="0.789281">Cornell Ithaca, NY,</affiliation>
<email confidence="0.99504">iil4@cornell.edu</email>
<author confidence="0.804209">David Thorsten</author>
<affiliation confidence="0.520078">Cornell</affiliation>
<email confidence="0.998206">tj@cs.cornell.edu</email>
<abstract confidence="0.9963788125">We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text. Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation. We present new evaluation techniques that directly compare embeddings with respect to specific queries. These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>How much do word embeddings encode about syntax?</title>
<date>2014</date>
<booktitle>In ACL: Short Papers,</booktitle>
<pages>822--827</pages>
<contexts>
<context position="32906" citStr="Andreas and Klein, 2014" startWordPosition="5374" endWordPosition="5377"> improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new intrinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness. Extrinsic evaluations use embeddings as features in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of existing systems (Turian et al., 2010). However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014). More work has been done in unsupervised semantic modeling in the context of topic models. One example is the word intrusion task (Chang et al., 2009), in which annotators are asked to identify a random word inserted into the set of high probability words for a given topic. Word embeddings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. Manual evaluation is expensive and time-consuming, but other work establishes that automated evaluations can closely model human intuitions (Newman et al., 2010). 8 Concl</context>
</contexts>
<marker>Andreas, Klein, 2014</marker>
<rawString>Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax? In ACL: Short Papers, pages 822–827.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javed Aslam</author>
<author>Mark Montague</author>
</authors>
<title>Models for metasearch.</title>
<date>2001</date>
<booktitle>In SIGIR,</booktitle>
<pages>276--284</pages>
<contexts>
<context position="11317" citStr="Aslam and Montague, 2001" startWordPosition="1794" endWordPosition="1797">later and should be explicitly considered. Metric aggregation. The main conceptual shortcoming of using correlation-based metrics is that they aggregate scores of different pairs — even though these scores can vary greatly in the embedding space. We can view the relatedness task as the task of evaluating a set of rankings, similar to ranking evaluation in Information Retrieval. More specifically, we have one query for each unique query word w and rank all remaining words v in the vocabulary accordingly. The problem now is that we usually cannot directly compare scores from different rankings (Aslam and Montague, 2001) as their scores are not guaranteed to have the same ranges. An even worse case is the following scenario. Assume we use rank correlation as our metric. As a consequence, we need our gold ranking to define an order on all the word pairs. However, this also means that we somehow need to order completely unrelated word pairs; for example, we have to decide whether (dog, cat) is more similar than (banana, apple). 3.2 Absolute results Table 1 presents the results on 14 different datasets for the six embedding models. We excluded examples from datasets that contained words not in our vocabulary. Fo</context>
</contexts>
<marker>Aslam, Montague, 2001</marker>
<rawString>Javed Aslam and Mark Montague. 2001. Models for metasearch. In SIGIR, pages 276–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="2191" citStr="Baroni et al., 2014" startWordPosition="306" endWordPosition="309"> embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic evaluation, we use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014). These tasks typically involve a pre-selected set of query terms and semantically related target words, which we refer to as a query inventory. Methods are evaluated by compiling an aggregate score for each method such as a correlation coefficient, which then serves as an absolute measure of quality. Query inventories have so far been collected opportunistically from prior work in psycholinguistics, information retrieval (Finkelstein et al., 2002), and image analysis (Bruni et al., 2014). Because these inventories were not constructed for word embedding evaluation, they are often idiosyncrati</context>
<context position="8585" citStr="Baroni et al. (2014)" startWordPosition="1326" endWordPosition="1329">ommonly used as a benchmark for embedding methods. There, embeddings are evaluated individually and only their final scores are compared, hence we refer to this scenario as absolute intrinsic evaluation. We present a new scenario, comparative intrinsic evaluation, in which we ask people directly for their preferences among different embeddings. We demonstrate that we can achieve the same results as offline, absolute metrics using online, comparative metrics. similarity(w1, w2) = 299 3.1 Absolute intrinsic evaluation For the absolute intrinsic evaluation, we used the same datasets and tasks as Baroni et al. (2014). While we present results on all tasks for completeness, we will mainly focus on relatedness in this section. There are four broad categories: • Relatedness: These datasets contain relatedness scores for pairs of words; the cosine similarity of the embeddings for two words should have high correlation (Spearman or Pearson) with human relatedness scores. • Analogy: This task was popularized by Mikolov et al. (2013a). The goal is to find a term x for a given term y so that x : y best resembles a sample relationship a : b. • Categorization: Here, the goal is to recover a clustering of words into</context>
<context position="32021" citStr="Baroni et al. (2014)" startWordPosition="5238" endWordPosition="5241">uracy 1.0 0.9 0.8 0.7 0.6 0.5 Avg. rank by corpus frequency 305 dings seem to capture even more complex linguistic properties. Chen et al. (2013) show that word embeddings even contain information about regional spellings (UK vs. US), noun gender and sentiment polarity. Previous work in evaluation for word embeddings can be divided into intrinsic and extrinsic evaluations. Intrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and geometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks. Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new intrinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness. Extrinsic evaluations use embeddings as f</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>JAIR,</journal>
<pages>49--1</pages>
<contexts>
<context position="2684" citStr="Bruni et al., 2014" startWordPosition="383" endWordPosition="386">c evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014). These tasks typically involve a pre-selected set of query terms and semantically related target words, which we refer to as a query inventory. Methods are evaluated by compiling an aggregate score for each method such as a correlation coefficient, which then serves as an absolute measure of quality. Query inventories have so far been collected opportunistically from prior work in psycholinguistics, information retrieval (Finkelstein et al., 2002), and image analysis (Bruni et al., 2014). Because these inventories were not constructed for word embedding evaluation, they are often idiosyncratic, dominated by specific types of queries, and poorly calibrated to corpus statistics. To remedy these problems, this paper makes the following contributions. First, this is the first paper to conduct a comprehensive study covering a wide range of evaluation criteria and popular embedding techniques. In particular, we study how outcomes from three different evaluation criteria are connected: word relatedness, coherence, downstream performance. We show that using different criteria results</context>
<context position="10155" citStr="Bruni et al., 2014" startWordPosition="1599" endWordPosition="1602"> people). We follow the procedure that is outlined in Baroni et al. (2014). Several important design questions come up when designing reusable datasets for evaluating relatedness. While we focus mainly on challenges that arise in the relatedness evaluation task, many of the questions discussed also apply to other scenarios. Query inventory. How we pick the word pairs to evaluate affects the results of the evaluation. The commonly-used WordSim-353 dataset (Finkelstein et al., 2002), for example, only tries to have word pairs with a diverse set of similarity scores. The more recent MEN dataset (Bruni et al., 2014) follows a similar strategy, but restricts queries to words that occur as annotations in an image dataset. However, there are more important criteria that should be considered in order to create a diverse dataset: (i) the frequency of the words in the English language (ii) the parts of speech of the words and (iii) abstractness vs. concreteness of the terms. Not only is frequency important because we want to test the quality of embeddings on rare words, but also because it is related with distance in the embedding space as we show later and should be explicitly considered. Metric aggregation. </context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. JAIR, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James McKeen Cattell</author>
</authors>
<title>The time taken up by cerebral operations.</title>
<date>1886</date>
<tech>Mind,, (42):220–242.</tech>
<contexts>
<context position="30508" citStr="Cattell, 1886" startWordPosition="5002" endWordPosition="5003">k = 1000 nearest neighbors. We then looked up their frequency ranks in the training corpus and averaged those ranks over all the query words. We found a strong correlation between the frequency of a word and its position in the ranking of nearest neighbors in our experiments. Figure 4 shows a power law relationship for C&amp;W embeddings between a word’s nearest neighbor rank (w.r.t. a query) and the word’s frequency rank in the training corpus (nn-rank ∼ 1000 · corpus-rank0.17). This is a concern: the frequency of a word in the language plays a critical role in word processing of humans as well (Cattell, 1886). As a consequence, we need to explicitly consider word frequency as a factor in the experiment design. Also, the above results mean that the commonly-used cosine similarity in the embedding space for the intrinsic tasks gets polluted by frequency-based effects. We believe that further research should address how to better measure linguistic relationships between words in the embedding space, e.g., by learning a custom metric. 7 Related work Mikolov et al. (2013b) demonstrate that certain linguistic regularities exist in the embedding space. The authors show that by doing simple vector arithme</context>
</contexts>
<marker>Cattell, 1886</marker>
<rawString>James McKeen Cattell. 1886. The time taken up by cerebral operations. Mind,, (42):220–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David Blei</author>
</authors>
<title>Reading tea leaves: How humans interpret topic models.</title>
<date>2009</date>
<booktitle>In NIPS,</booktitle>
<pages>288--296</pages>
<contexts>
<context position="20042" citStr="Chang et al. (2009)" startWordPosition="3273" endWordPosition="3276">sure whether a pair of semantically similar words are near each other in the embedding space. In this novel coherence task we assess whether groups of words in a small neighborhood in the embedding space are mutually related. Previous work has used this property for qualitative evaluation using visualizations of 2D projections (Turian et al., 2010), but we are not aware of any work using local neighborhoods for 302 quantitative evaluation. Good embeddings should have coherent neighborhoods for each word, so inserting a word not belonging to this neighborhood should be easy to spot. Similar to Chang et al. (2009), we presented Turkers with four words, three of which are close neighbors and one of which is an “intruder.” For each of the 100 words in our query set of Section 3.3, we retrieved the two nearest neighbors. These words along with the query word defined the set of (supposedly) good options. Table 3 shows an example instance that was given to the Turkers. (a) finally (b) eventually (c) immediately (d) put Table 3: Example instance of intrusion task. The query word is option (a), intruder is (d). To normalize for frequency-based effects, we computed the average frequency avg of the three words </context>
<context position="33057" citStr="Chang et al., 2009" startWordPosition="5401" endWordPosition="5404">rinsic performance. However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness. Extrinsic evaluations use embeddings as features in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of existing systems (Turian et al., 2010). However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014). More work has been done in unsupervised semantic modeling in the context of topic models. One example is the word intrusion task (Chang et al., 2009), in which annotators are asked to identify a random word inserted into the set of high probability words for a given topic. Word embeddings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. Manual evaluation is expensive and time-consuming, but other work establishes that automated evaluations can closely model human intuitions (Newman et al., 2010). 8 Conclusions There are many factors that affect word embedding quality. Standard aggregate evaluations, while useful, do not present a complete or consistent</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. In NIPS, pages 288–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanqing Chen</author>
<author>Bryan Perozzi</author>
<author>Rami Al-Rfou</author>
<author>Steven Skiena</author>
</authors>
<title>The expressive power of word embeddings. arXiv preprint:</title>
<date>2013</date>
<pages>1408--3456</pages>
<contexts>
<context position="31546" citStr="Chen et al. (2013)" startWordPosition="5169" endWordPosition="5172">om metric. 7 Related work Mikolov et al. (2013b) demonstrate that certain linguistic regularities exist in the embedding space. The authors show that by doing simple vector arithmetic in the embedding space, one can solve various syntactic and semantic analogy tasks. This is different to previous work, which phrased the analogy task as a classification problem (Turney, 2008). Surprisingly, word embedCCA C&amp;W CBOW GloVe Rand. Proj H-PCA 102 103 104 105 Word frequency Accuracy 1.0 0.9 0.8 0.7 0.6 0.5 Avg. rank by corpus frequency 305 dings seem to capture even more complex linguistic properties. Chen et al. (2013) show that word embeddings even contain information about regional spellings (UK vs. US), noun gender and sentiment polarity. Previous work in evaluation for word embeddings can be divided into intrinsic and extrinsic evaluations. Intrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and geometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks. Faruqui and Dyer (2</context>
</contexts>
<marker>Chen, Perozzi, Al-Rfou, Skiena, 2013</marker>
<rawString>Yanqing Chen, Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2013. The expressive power of word embeddings. arXiv preprint: 1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<pages>12--2493</pages>
<publisher>JLMR,</publisher>
<contexts>
<context position="5732" citStr="Collobert et al., 2011" startWordPosition="863" endWordPosition="866">dings and across evaluation methods. It also casts doubt on the common practice of using the vanilla cosine similarity as a similarity measure in the embedding space. It is important to note that this work is a survey of evaluation methods not a survey of embedding methods. The specific example embeddings presented here were chosen as representative samples only, and may not be optimal. 2 Word embeddings We refer to a word embedding as a mapping V → RD : w H w� that maps a word w from a vocabulary V to a real-valued vector w� in an embedding space of dimensionality D. Following previous work (Collobert et al., 2011; Mikolov et al., 2013a) we use the commonly employed cosine similarity, defined as w1 ·~w2 for all similar11~w11111~w211, ity computations in the embedding space. The list of nearest neighbors of a word w are all words v E V \ {w}, sorted in descending order by similarity(w, v). We will denote w as the query word in the remainder of this paper. All experiments in this paper are carried out on six popular unsupervised embedding methods. These embeddings form a representative but incomplete subset; and since we are studying evaluation methods and not embeddings themselves, no attempt has been m</context>
<context position="7446" citStr="Collobert et al. (2011)" startWordPosition="1149" endWordPosition="1152">oVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) follow a reconstruction approach: word embeddings should be able to capture as much relevant information from the original co-occurrence matrix as possible. Training corpus. We tried to make the comparison as fair as possible. As the C&amp;W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for training the other models. We tokenized the data using the Stanford tokenizer (Manning et al., 2014). Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros. Details. All models embedded words into a 50- dimensional space (D = 50). As implemented, each method uses a different vocabulary, so we computed the intersection of the six vocabularies and used the resulting set of 103,647 words for all nearest-neighbor experiments. 3 Relatedness We begin with intrinsic evaluation of relatedness using both pre-collected human evaluations and a novel online user study. Section 3.1 introduces the list of datasets that is commonly used as a benchmark for embedding methods. There, embeddings are evaluate</context>
<context position="32737" citStr="Collobert et al., 2011" startWordPosition="5346" endWordPosition="5349">s and tasks. Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new intrinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness. Extrinsic evaluations use embeddings as features in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of existing systems (Turian et al., 2010). However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014). More work has been done in unsupervised semantic modeling in the context of topic models. One example is the word intrusion task (Chang et al., 2009), in which annotators are asked to identify a random word inserted into the set of high probability words for a given topic. Word embeddings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. M</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. JLMR, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Jordan Rodu</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Two step CCA: A new spectral method for estimating vector models of words.</title>
<date>2012</date>
<booktitle>In ICML,</booktitle>
<pages>1551--1558</pages>
<contexts>
<context position="6882" citStr="Dhillon et al., 2012" startWordPosition="1057" endWordPosition="1060">evaluation methods and not embeddings themselves, no attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&amp;W embeddings (Collobert et al., 2011) both are motivated by a probabilistic prediction approach. Given a number of context words around a target word w, these models formulate the embedding task as that of finding a representation that is good at predicting w from the context representations. The second group of models, Hellinger PCA (Lebret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) follow a reconstruction approach: word embeddings should be able to capture as much relevant information from the original co-occurrence matrix as possible. Training corpus. We tried to make the comparison as fair as possible. As the C&amp;W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for training the other models. We tokenized the data using the Stanford tokenizer (Manning et al., 2014). Like Collobert et al. (2011), we lower-cased all words and repla</context>
</contexts>
<marker>Dhillon, Rodu, Foster, Ungar, 2012</marker>
<rawString>Paramveer S. Dhillon, Jordan Rodu, Dean P. Foster, and Lyle H. Ungar. 2012. Two step CCA: A new spectral method for estimating vector models of words. In ICML, pages 1551–1558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>JLMR,</journal>
<pages>9--1871</pages>
<contexts>
<context position="25407" citStr="Fan et al., 2008" startWordPosition="4166" endWordPosition="4169">semantic information. Sentiment classification. Second we use a recently released dataset for binary sentiment classification by Maas et al. (2011). The dataset contains 50K movie reviews with a balanced distribution of binary polarity labels. We evaluate the relative performance of word embeddings at this task as follows: we generate embedding-only features for each review by computing a linear combination of word embeddings weighted by the number of times that the word appeared in the review (using the same bag-of-words features as Maas et al. (2011)). A LIBLINEAR logistic regression model (Fan et al., 2008) with the default parameters is trained and evaluated using 10 fold crossvalidation. A vanilla bag of words feature set is the baseline (denoted as BOW here). We expect that this task will be more sensitive to semantic information than syntactic information. Results. Table 4 shows the average F1-scores for the chunking task. The p-values were computed using randomization (Yeh, 2000) on the sentence level. First, we can observe that adding word vectors as features results in performance lifts with all embeddings when compared to the baseline. The performance of C&amp;W and TSCCA is statistically no</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. JLMR, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Community evaluation and exchange of word vectors at wordvectors.org. In ACL: System Demonstrations.</title>
<date>2014</date>
<contexts>
<context position="32150" citStr="Faruqui and Dyer (2014)" startWordPosition="5257" endWordPosition="5260"> Chen et al. (2013) show that word embeddings even contain information about regional spellings (UK vs. US), noun gender and sentiment polarity. Previous work in evaluation for word embeddings can be divided into intrinsic and extrinsic evaluations. Intrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and geometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks. Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new intrinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness. Extrinsic evaluations use embeddings as features in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Community evaluation and exchange of word vectors at wordvectors.org. In ACL: System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
</authors>
<title>Ehud Rivlin Zach Solan Gadi Wolfman Evgeniy Gabrilovich, Yossi Matias, and Eytan Ruppin.</title>
<date>2002</date>
<journal>TOIS,</journal>
<volume>20</volume>
<issue>1</issue>
<marker>Finkelstein, 2002</marker>
<rawString>Lev Finkelstein, Ehud Rivlin Zach Solan Gadi Wolfman Evgeniy Gabrilovich, Yossi Matias, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. TOIS, 20(1):116–131, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Gao</author>
<author>Jiang Bian</author>
<author>Tie-Yan Liu</author>
</authors>
<title>WordRep: A benchmark for research on learning word representations. ICML Workshop on KnowledgePowered Deep Learning for Text Mining.</title>
<date>2014</date>
<contexts>
<context position="32271" citStr="Gao et al. (2014)" startWordPosition="5277" endWordPosition="5280">ntiment polarity. Previous work in evaluation for word embeddings can be divided into intrinsic and extrinsic evaluations. Intrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and geometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks. Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new intrinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness. Extrinsic evaluations use embeddings as features in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of existing systems (Turian et al., 2010). However, they have been less successful at other tasks such a</context>
</contexts>
<marker>Gao, Bian, Liu, 2014</marker>
<rawString>Bin Gao, Jiang Bian, and Tie-Yan Liu. 2014. WordRep: A benchmark for research on learning word representations. ICML Workshop on KnowledgePowered Deep Learning for Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´emi Lebret</author>
<author>Ronan Collobert</author>
</authors>
<title>Word embeddings through Hellinger PCA.</title>
<date>2014</date>
<booktitle>In EACL,</booktitle>
<pages>482--490</pages>
<contexts>
<context position="6819" citStr="Lebret and Collobert, 2014" startWordPosition="1047" endWordPosition="1050">rm a representative but incomplete subset; and since we are studying evaluation methods and not embeddings themselves, no attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&amp;W embeddings (Collobert et al., 2011) both are motivated by a probabilistic prediction approach. Given a number of context words around a target word w, these models formulate the embedding task as that of finding a representation that is good at predicting w from the context representations. The second group of models, Hellinger PCA (Lebret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) follow a reconstruction approach: word embeddings should be able to capture as much relevant information from the original co-occurrence matrix as possible. Training corpus. We tried to make the comparison as fair as possible. As the C&amp;W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for training the other models. We tokenized the data using the Stanford tokenizer (Manning et al., 2014). L</context>
<context position="26916" citStr="Lebret and Collobert, 2014" startWordPosition="4415" endWordPosition="4418">ar order of embeddings as in the absolute intrinsic evaluation, however, the order of TSCCA and GloVe is now reversed. Discussion. Performance on downstream tasks is not consistent across tasks, and may not be consistent with intrinsic evaluations. Comparing performance across tasks may provide insight into the information encoded by an embedding, but we should not expect any specific task to act as a proxy for abstract quality. Furthermore, if good downstream performance is really the goal of an embedding, we recommend that embeddings be trained specifically to optimize a specific objective (Lebret and Collobert, 2014). 6 Discussion We find consistent differences between word embeddings, despite the fact that they are operating on the same input data and optimizing arguably very similar objective functions (Pennington et al., 2014; Levy and Goldberg, 2014). Recent work suggests that many apparent performance differences on specific tasks are due to a lack of hyperparameter optimization (Levy et al., 2015a). Different algorithms are, in fact, encoding surprisingly different information that may or may not align with our desired use cases. For example, we find that embeddings encode differing degrees of infor</context>
</contexts>
<marker>Lebret, Collobert, 2014</marker>
<rawString>R´emi Lebret and Ronan Collobert. 2014. Word embeddings through Hellinger PCA. In EACL, pages 482–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="27158" citStr="Levy and Goldberg, 2014" startWordPosition="4452" endWordPosition="4455">ns. Comparing performance across tasks may provide insight into the information encoded by an embedding, but we should not expect any specific task to act as a proxy for abstract quality. Furthermore, if good downstream performance is really the goal of an embedding, we recommend that embeddings be trained specifically to optimize a specific objective (Lebret and Collobert, 2014). 6 Discussion We find consistent differences between word embeddings, despite the fact that they are operating on the same input data and optimizing arguably very similar objective functions (Pennington et al., 2014; Levy and Goldberg, 2014). Recent work suggests that many apparent performance differences on specific tasks are due to a lack of hyperparameter optimization (Levy et al., 2015a). Different algorithms are, in fact, encoding surprisingly different information that may or may not align with our desired use cases. For example, we find that embeddings encode differing degrees of information about word frequency, even after length normalization. This result is surprising for two reasons. First, many algorithms reserve distinct “intercept” parameters to absorb frequencybased effects. Second, we expect that the geometry of t</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In NIPS, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving distributional similarity with lessons learned from word embeddings.</title>
<date>2015</date>
<publisher>TACL.</publisher>
<contexts>
<context position="18847" citStr="Levy et al., 2015" startWordPosition="3078" endWordPosition="3081">lative performances of all embeddings for the four POS classes (adjectives, adverbs, nouns and verbs). While most embeddings show relatively homogeneous behaviour across the four classes, GloVe suffers disproportionally on adverbs. Moving on to Figure 1(d), we can see a similar behavior for TSCCA: Its performance is much lower on concrete words than on abstract ones. This difference may be important, as recent related work finds that simply differentiating between general and specific terms explains much of the observed variation between embedding methods in hierarchical classification tasks (Levy et al., 2015b). We take the two observations above as evidence that a more fine-grained analysis is necessary in discerning different embedding methods. As a by-product, we observed that there was no embedding method that consistently performed best on all of the four different absolute evaluation tasks. However, we would like to reiterate that our goal is not to identify one best method, but rather point out that different evaluations (e.g., changing the rank k of the nearest neighbors in the comparison task) result in different outcomes. 4 Coherence In the relatedness task we measure whether a pair of s</context>
<context position="27309" citStr="Levy et al., 2015" startWordPosition="4477" endWordPosition="4480">a proxy for abstract quality. Furthermore, if good downstream performance is really the goal of an embedding, we recommend that embeddings be trained specifically to optimize a specific objective (Lebret and Collobert, 2014). 6 Discussion We find consistent differences between word embeddings, despite the fact that they are operating on the same input data and optimizing arguably very similar objective functions (Pennington et al., 2014; Levy and Goldberg, 2014). Recent work suggests that many apparent performance differences on specific tasks are due to a lack of hyperparameter optimization (Levy et al., 2015a). Different algorithms are, in fact, encoding surprisingly different information that may or may not align with our desired use cases. For example, we find that embeddings encode differing degrees of information about word frequency, even after length normalization. This result is surprising for two reasons. First, many algorithms reserve distinct “intercept” parameters to absorb frequencybased effects. Second, we expect that the geometry of the embedding space will be primar304 Figure 3: Embeddings can accurately predict whether a word is frequent or rare. ily driven by semantics: the relat</context>
</contexts>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015a. Improving distributional similarity with lessons learned from word embeddings. TACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Steffen Remus</author>
<author>Chris Biemann</author>
<author>Ido Dagan</author>
</authors>
<title>Do supervised distributional methods really learn lexical inference relations?</title>
<date>2015</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="18847" citStr="Levy et al., 2015" startWordPosition="3078" endWordPosition="3081">lative performances of all embeddings for the four POS classes (adjectives, adverbs, nouns and verbs). While most embeddings show relatively homogeneous behaviour across the four classes, GloVe suffers disproportionally on adverbs. Moving on to Figure 1(d), we can see a similar behavior for TSCCA: Its performance is much lower on concrete words than on abstract ones. This difference may be important, as recent related work finds that simply differentiating between general and specific terms explains much of the observed variation between embedding methods in hierarchical classification tasks (Levy et al., 2015b). We take the two observations above as evidence that a more fine-grained analysis is necessary in discerning different embedding methods. As a by-product, we observed that there was no embedding method that consistently performed best on all of the four different absolute evaluation tasks. However, we would like to reiterate that our goal is not to identify one best method, but rather point out that different evaluations (e.g., changing the rank k of the nearest neighbors in the comparison task) result in different outcomes. 4 Coherence In the relatedness task we measure whether a pair of s</context>
<context position="27309" citStr="Levy et al., 2015" startWordPosition="4477" endWordPosition="4480">a proxy for abstract quality. Furthermore, if good downstream performance is really the goal of an embedding, we recommend that embeddings be trained specifically to optimize a specific objective (Lebret and Collobert, 2014). 6 Discussion We find consistent differences between word embeddings, despite the fact that they are operating on the same input data and optimizing arguably very similar objective functions (Pennington et al., 2014; Levy and Goldberg, 2014). Recent work suggests that many apparent performance differences on specific tasks are due to a lack of hyperparameter optimization (Levy et al., 2015a). Different algorithms are, in fact, encoding surprisingly different information that may or may not align with our desired use cases. For example, we find that embeddings encode differing degrees of information about word frequency, even after length normalization. This result is surprising for two reasons. First, many algorithms reserve distinct “intercept” parameters to absorb frequencybased effects. Second, we expect that the geometry of the embedding space will be primar304 Figure 3: Embeddings can accurately predict whether a word is frequent or rare. ily driven by semantics: the relat</context>
</contexts>
<marker>Levy, Remus, Biemann, Dagan, 2015</marker>
<rawString>Omer Levy, Steffen Remus, Chris Biemann, and Ido Dagan. 2015b. Do supervised distributional methods really learn lexical inference relations? In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ping Li</author>
<author>Trevor J Hastie</author>
<author>Kenneth W Church</author>
</authors>
<title>Very sparse random projections.</title>
<date>2006</date>
<booktitle>In KDD,</booktitle>
<pages>287--296</pages>
<contexts>
<context position="6930" citStr="Li et al., 2006" startWordPosition="1065" endWordPosition="1068">attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&amp;W embeddings (Collobert et al., 2011) both are motivated by a probabilistic prediction approach. Given a number of context words around a target word w, these models formulate the embedding task as that of finding a representation that is good at predicting w from the context representations. The second group of models, Hellinger PCA (Lebret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) follow a reconstruction approach: word embeddings should be able to capture as much relevant information from the original co-occurrence matrix as possible. Training corpus. We tried to make the comparison as fair as possible. As the C&amp;W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for training the other models. We tokenized the data using the Stanford tokenizer (Manning et al., 2014). Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros. Details. All models embed</context>
</contexts>
<marker>Li, Hastie, Church, 2006</marker>
<rawString>Ping Li, Trevor J Hastie, and Kenneth W Church. 2006. Very sparse random projections. In KDD, pages 287–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In HLT-ACL,</booktitle>
<pages>142--150</pages>
<contexts>
<context position="24937" citStr="Maas et al. (2011)" startWordPosition="4087" endWordPosition="4090">ddings as features. The p-values are with respect to the best performing embedding. Noun phrase chunking. First we use a noun phrase chunking task similar to that used by Turian et al. (2010). The only difference is that we normalize all word vectors to unit length, rather than scaling them with some custom factor, before giving them to the conditional random field (CRF) model as input. We expect that this task will be more sensitive to syntactic information than to semantic information. Sentiment classification. Second we use a recently released dataset for binary sentiment classification by Maas et al. (2011). The dataset contains 50K movie reviews with a balanced distribution of binary polarity labels. We evaluate the relative performance of word embeddings at this task as follows: we generate embedding-only features for each review by computing a linear combination of word embeddings weighted by the number of times that the word appeared in the review (using the same bag-of-words features as Maas et al. (2011)). A LIBLINEAR logistic regression model (Fan et al., 2008) with the default parameters is trained and evaluated using 10 fold crossvalidation. A vanilla bag of words feature set is the bas</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In HLT-ACL, pages 142–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In ACL: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="7416" citStr="Manning et al., 2014" startWordPosition="1144" endWordPosition="1147">ret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) follow a reconstruction approach: word embeddings should be able to capture as much relevant information from the original co-occurrence matrix as possible. Training corpus. We tried to make the comparison as fair as possible. As the C&amp;W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for training the other models. We tokenized the data using the Stanford tokenizer (Manning et al., 2014). Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros. Details. All models embedded words into a 50- dimensional space (D = 50). As implemented, each method uses a different vocabulary, so we computed the intersection of the six vocabularies and used the resulting set of 103,647 words for all nearest-neighbor experiments. 3 Relatedness We begin with intrinsic evaluation of relatedness using both pre-collected human evaluations and a novel online user study. Section 3.1 introduces the list of datasets that is commonly used as a benchmark for embedding methods. </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In ACL: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2168" citStr="Mikolov et al., 2013" startWordPosition="302" endWordPosition="305"> strengths of different embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic evaluation, we use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014). These tasks typically involve a pre-selected set of query terms and semantically related target words, which we refer to as a query inventory. Methods are evaluated by compiling an aggregate score for each method such as a correlation coefficient, which then serves as an absolute measure of quality. Query inventories have so far been collected opportunistically from prior work in psycholinguistics, information retrieval (Finkelstein et al., 2002), and image analysis (Bruni et al., 2014). Because these inventories were not constructed for word embedding evaluation, they</context>
<context position="5754" citStr="Mikolov et al., 2013" startWordPosition="867" endWordPosition="870">ion methods. It also casts doubt on the common practice of using the vanilla cosine similarity as a similarity measure in the embedding space. It is important to note that this work is a survey of evaluation methods not a survey of embedding methods. The specific example embeddings presented here were chosen as representative samples only, and may not be optimal. 2 Word embeddings We refer to a word embedding as a mapping V → RD : w H w� that maps a word w from a vocabulary V to a real-valued vector w� in an embedding space of dimensionality D. Following previous work (Collobert et al., 2011; Mikolov et al., 2013a) we use the commonly employed cosine similarity, defined as w1 ·~w2 for all similar11~w11111~w211, ity computations in the embedding space. The list of nearest neighbors of a word w are all words v E V \ {w}, sorted in descending order by similarity(w, v). We will denote w as the query word in the remainder of this paper. All experiments in this paper are carried out on six popular unsupervised embedding methods. These embeddings form a representative but incomplete subset; and since we are studying evaluation methods and not embeddings themselves, no attempt has been made to optimize these </context>
<context position="9002" citStr="Mikolov et al. (2013" startWordPosition="1393" endWordPosition="1396">te metrics using online, comparative metrics. similarity(w1, w2) = 299 3.1 Absolute intrinsic evaluation For the absolute intrinsic evaluation, we used the same datasets and tasks as Baroni et al. (2014). While we present results on all tasks for completeness, we will mainly focus on relatedness in this section. There are four broad categories: • Relatedness: These datasets contain relatedness scores for pairs of words; the cosine similarity of the embeddings for two words should have high correlation (Spearman or Pearson) with human relatedness scores. • Analogy: This task was popularized by Mikolov et al. (2013a). The goal is to find a term x for a given term y so that x : y best resembles a sample relationship a : b. • Categorization: Here, the goal is to recover a clustering of words into different categories. To do this, the corresponding word vectors of all words in a dataset are clustered and the purity of the returned clusters is computed with respect to the labeled dataset. • Selectional preference: The goal is to determine how typical a noun is for a verb either as a subject or as an object (e.g., people eat, but we rarely eat people). We follow the procedure that is outlined in Baroni et al</context>
<context position="30974" citStr="Mikolov et al. (2013" startWordPosition="5075" endWordPosition="5078"> · corpus-rank0.17). This is a concern: the frequency of a word in the language plays a critical role in word processing of humans as well (Cattell, 1886). As a consequence, we need to explicitly consider word frequency as a factor in the experiment design. Also, the above results mean that the commonly-used cosine similarity in the embedding space for the intrinsic tasks gets polluted by frequency-based effects. We believe that further research should address how to better measure linguistic relationships between words in the embedding space, e.g., by learning a custom metric. 7 Related work Mikolov et al. (2013b) demonstrate that certain linguistic regularities exist in the embedding space. The authors show that by doing simple vector arithmetic in the embedding space, one can solve various syntactic and semantic analogy tasks. This is different to previous work, which phrased the analogy task as a classification problem (Turney, 2008). Surprisingly, word embedCCA C&amp;W CBOW GloVe Rand. Proj H-PCA 102 103 104 105 Word frequency Accuracy 1.0 0.9 0.8 0.7 0.6 0.5 Avg. rank by corpus frequency 305 dings seem to capture even more complex linguistic properties. Chen et al. (2013) show that word embeddings e</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013a. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-Tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="2168" citStr="Mikolov et al., 2013" startWordPosition="302" endWordPosition="305"> strengths of different embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic evaluation, we use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014). These tasks typically involve a pre-selected set of query terms and semantically related target words, which we refer to as a query inventory. Methods are evaluated by compiling an aggregate score for each method such as a correlation coefficient, which then serves as an absolute measure of quality. Query inventories have so far been collected opportunistically from prior work in psycholinguistics, information retrieval (Finkelstein et al., 2002), and image analysis (Bruni et al., 2014). Because these inventories were not constructed for word embedding evaluation, they</context>
<context position="5754" citStr="Mikolov et al., 2013" startWordPosition="867" endWordPosition="870">ion methods. It also casts doubt on the common practice of using the vanilla cosine similarity as a similarity measure in the embedding space. It is important to note that this work is a survey of evaluation methods not a survey of embedding methods. The specific example embeddings presented here were chosen as representative samples only, and may not be optimal. 2 Word embeddings We refer to a word embedding as a mapping V → RD : w H w� that maps a word w from a vocabulary V to a real-valued vector w� in an embedding space of dimensionality D. Following previous work (Collobert et al., 2011; Mikolov et al., 2013a) we use the commonly employed cosine similarity, defined as w1 ·~w2 for all similar11~w11111~w211, ity computations in the embedding space. The list of nearest neighbors of a word w are all words v E V \ {w}, sorted in descending order by similarity(w, v). We will denote w as the query word in the remainder of this paper. All experiments in this paper are carried out on six popular unsupervised embedding methods. These embeddings form a representative but incomplete subset; and since we are studying evaluation methods and not embeddings themselves, no attempt has been made to optimize these </context>
<context position="9002" citStr="Mikolov et al. (2013" startWordPosition="1393" endWordPosition="1396">te metrics using online, comparative metrics. similarity(w1, w2) = 299 3.1 Absolute intrinsic evaluation For the absolute intrinsic evaluation, we used the same datasets and tasks as Baroni et al. (2014). While we present results on all tasks for completeness, we will mainly focus on relatedness in this section. There are four broad categories: • Relatedness: These datasets contain relatedness scores for pairs of words; the cosine similarity of the embeddings for two words should have high correlation (Spearman or Pearson) with human relatedness scores. • Analogy: This task was popularized by Mikolov et al. (2013a). The goal is to find a term x for a given term y so that x : y best resembles a sample relationship a : b. • Categorization: Here, the goal is to recover a clustering of words into different categories. To do this, the corresponding word vectors of all words in a dataset are clustered and the purity of the returned clusters is computed with respect to the labeled dataset. • Selectional preference: The goal is to determine how typical a noun is for a verb either as a subject or as an object (e.g., people eat, but we rarely eat people). We follow the procedure that is outlined in Baroni et al</context>
<context position="30974" citStr="Mikolov et al. (2013" startWordPosition="5075" endWordPosition="5078"> · corpus-rank0.17). This is a concern: the frequency of a word in the language plays a critical role in word processing of humans as well (Cattell, 1886). As a consequence, we need to explicitly consider word frequency as a factor in the experiment design. Also, the above results mean that the commonly-used cosine similarity in the embedding space for the intrinsic tasks gets polluted by frequency-based effects. We believe that further research should address how to better measure linguistic relationships between words in the embedding space, e.g., by learning a custom metric. 7 Related work Mikolov et al. (2013b) demonstrate that certain linguistic regularities exist in the embedding space. The authors show that by doing simple vector arithmetic in the embedding space, one can solve various syntactic and semantic analogy tasks. This is different to previous work, which phrased the analogy task as a classification problem (Turney, 2008). Surprisingly, word embedCCA C&amp;W CBOW GloVe Rand. Proj H-PCA 102 103 104 105 Word frequency Accuracy 1.0 0.9 0.8 0.7 0.6 0.5 Avg. rank by corpus frequency 305 dings seem to capture even more complex linguistic properties. Chen et al. (2013) show that word embeddings e</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-Tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746– 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="14021" citStr="Miller, 1995" startWordPosition="2258" endWordPosition="2259">PCA 19.8 32.9 43.6 15.1 21.3 54.2 34.1 50.0 42.0 -2.5 3.2 3.0 2.4 3.7 23.1 Rand. Proj. 17.1 19.5 24.9 16.1 11.3 51.4 21.9 38.6 29.6 -8.5 1.2 1.0 0.3 1.9 16.2 Table 1: Results on absolute intrinsic evaluation. The best result for each dataset is highlighted in bold. The second row contains the names of the corresponding datasets. embeddings themselves will be used to define the comparable target words. Query inventory. We compiled a diverse inventory of 100 query words that balance frequency, part of speech (POS), and concreteness. First, we selected 10 out of 45 broad categories from WordNet (Miller, 1995). We then chose an equal number of categories that mostly contained abstract concepts and categories that referred to concrete concepts. Among those categories, we had one for adjectives and adverbs each, and four for nouns and verbs each. From each category, we drew ten random words with the restriction that there be exactly three rare words (i.e., occurring fewer than 2500 times in the training corpus) among the ten. Details. Our experiments were performed with users from Amazon Mechanical Turk (MTurk) that were native speakers of English with sufficient experience and positive feedback on t</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. WordNet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic evaluation of topic coherence.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>100--108</pages>
<contexts>
<context position="33497" citStr="Newman et al., 2010" startWordPosition="5471" endWordPosition="5474">ing (Andreas and Klein, 2014). More work has been done in unsupervised semantic modeling in the context of topic models. One example is the word intrusion task (Chang et al., 2009), in which annotators are asked to identify a random word inserted into the set of high probability words for a given topic. Word embeddings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. Manual evaluation is expensive and time-consuming, but other work establishes that automated evaluations can closely model human intuitions (Newman et al., 2010). 8 Conclusions There are many factors that affect word embedding quality. Standard aggregate evaluations, while useful, do not present a complete or consistent picture. Factors such as word frequency play a significant and previously unacknowledged role. Word frequency also interferes with the commonly-used cosine similarity measure. We present a novel evaluation framework based on direct comparisons between embeddings that provides more fine-grained analysis and supports simple, crowdsourced relevance judgments. We also present a novel Coherence task that measures our intuition that neighbor</context>
</contexts>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In HLT-NAACL, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1916" citStr="Pennington et al., 2014" startWordPosition="263" endWordPosition="266">l approaches to measuring the quality of neural word embeddings. In particular, we perform a comprehensive analysis of evaluation methods and introduce novel methods that can be implemented through crowdsourcing, providing better insights into the relative strengths of different embeddings. Existing schemes fall into two major categories: extrinsic and intrinsic evaluation. In extrinsic evaluation, we use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task. Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014). Extrinsic evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures. Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014). These tasks typically involve a pre-selected set of query terms and semantically related target words, which we refer to as a query inventory. Methods are evaluated by compiling an aggregate score for each method such as a correlation coefficient, which then serves as an absolute measure of quality. Query inventories have</context>
<context position="6852" citStr="Pennington et al., 2014" startWordPosition="1052" endWordPosition="1055">ubset; and since we are studying evaluation methods and not embeddings themselves, no attempt has been made to optimize these embeddings. The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&amp;W embeddings (Collobert et al., 2011) both are motivated by a probabilistic prediction approach. Given a number of context words around a target word w, these models formulate the embedding task as that of finding a representation that is good at predicting w from the context representations. The second group of models, Hellinger PCA (Lebret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) follow a reconstruction approach: word embeddings should be able to capture as much relevant information from the original co-occurrence matrix as possible. Training corpus. We tried to make the comparison as fair as possible. As the C&amp;W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for training the other models. We tokenized the data using the Stanford tokenizer (Manning et al., 2014). Like Collobert et al. (2011), we l</context>
<context position="27132" citStr="Pennington et al., 2014" startWordPosition="4448" endWordPosition="4451"> with intrinsic evaluations. Comparing performance across tasks may provide insight into the information encoded by an embedding, but we should not expect any specific task to act as a proxy for abstract quality. Furthermore, if good downstream performance is really the goal of an embedding, we recommend that embeddings be trained specifically to optimize a specific objective (Lebret and Collobert, 2014). 6 Discussion We find consistent differences between word embeddings, despite the fact that they are operating on the same input data and optimizing arguably very similar objective functions (Pennington et al., 2014; Levy and Goldberg, 2014). Recent work suggests that many apparent performance differences on specific tasks are due to a lack of hyperparameter optimization (Levy et al., 2015a). Different algorithms are, in fact, encoding surprisingly different information that may or may not align with our desired use cases. For example, we find that embeddings encode differing degrees of information about word frequency, even after length normalization. This result is surprising for two reasons. First, many algorithms reserve distinct “intercept” parameters to absorb frequencybased effects. Second, we exp</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Manaal Faruqui</author>
<author>Wang Ling</author>
<author>Guillaume Lample</author>
<author>Chris Dyer</author>
</authors>
<title>Evaluation of word vector representations by subspace alignment.</title>
<date>2015</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="32374" citStr="Tsvetkov et al. (2015)" startWordPosition="5292" endWordPosition="5295">d extrinsic evaluations. Intrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and geometric relatedness, usually through inventories of query terms. Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks. Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new intrinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness. Extrinsic evaluations use embeddings as features in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of existing systems (Turian et al., 2010). However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014). More work has been done in unsupervised semantic modeling in the c</context>
</contexts>
<marker>Tsvetkov, Faruqui, Ling, Lample, Dyer, 2015</marker>
<rawString>Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guillaume Lample, and Chris Dyer. 2015. Evaluation of word vector representations by subspace alignment. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="19773" citStr="Turian et al., 2010" startWordPosition="3228" endWordPosition="3231">would like to reiterate that our goal is not to identify one best method, but rather point out that different evaluations (e.g., changing the rank k of the nearest neighbors in the comparison task) result in different outcomes. 4 Coherence In the relatedness task we measure whether a pair of semantically similar words are near each other in the embedding space. In this novel coherence task we assess whether groups of words in a small neighborhood in the embedding space are mutually related. Previous work has used this property for qualitative evaluation using visualizations of 2D projections (Turian et al., 2010), but we are not aware of any work using local neighborhoods for 302 quantitative evaluation. Good embeddings should have coherent neighborhoods for each word, so inserting a word not belonging to this neighborhood should be easy to spot. Similar to Chang et al. (2009), we presented Turkers with four words, three of which are close neighbors and one of which is an “intruder.” For each of the 100 words in our query set of Section 3.3, we retrieved the two nearest neighbors. These words along with the query word defined the set of (supposedly) good options. Table 3 shows an example instance that</context>
<context position="24510" citStr="Turian et al. (2010)" startWordPosition="4015" endWordPosition="4018">4.53 94.12 CBOW 94.32 93.93 0.012 TSCCA 94.53 94.09 0.357 Table 4: F1 chunking results using different word embeddings as features. The p-values are with respect to the best performing method. test p-value BOW (baseline) 88.90 7.45·10−14 Rand. Proj. 62.95 7.47·10−12 GloVe 74.87 5.00·10−2 H-PCA 69.45 6.06·10−11 C&amp;W 72.37 1.29·10−7 CBOW 75.78 TSCCA 75.02 7.28·10−4 Table 5: F1 sentiment analysis results using different word embeddings as features. The p-values are with respect to the best performing embedding. Noun phrase chunking. First we use a noun phrase chunking task similar to that used by Turian et al. (2010). The only difference is that we normalize all word vectors to unit length, rather than scaling them with some custom factor, before giving them to the conditional random field (CRF) model as input. We expect that this task will be more sensitive to syntactic information than to semantic information. Sentiment classification. Second we use a recently released dataset for binary sentiment classification by Maas et al. (2011). The dataset contains 50K movie reviews with a balanced distribution of binary polarity labels. We evaluate the relative performance of word embeddings at this task as foll</context>
<context position="28608" citStr="Turian et al., 2010" startWordPosition="4682" endWordPosition="4685">pace, while large numbers of rare, specific words should cluster around related, but more frequent, words. We trained a logistic regression model to predict word frequency categories based on word vectors. The linear classifier was trained to put words either in a frequent or rare category, with thresholds varying from 100 to 50,000. At each threshold frequency, we sampled the training sets to ensure a consistent balance of the label distribution across all frequencies. We used length-normalized embeddings, as rare words might have shorter vectors resulting from fewer updates during training (Turian et al., 2010). We report the mean accuracy and standard deviation (1σ) using five-fold crossvalidation at each threshold frequency in Figure 3. All word embeddings do better than random, suggesting that they contain some frequency information. GloVe and TSCCA achieve nearly 100% accuracy on thresholds up to 1000. Unlike all other embeddings, accuracy for C&amp;W embeddings increases for larger threshold values. Further investigation revealed that the weight vector direction changes gradually with the threshold frequency — indicating that frequency seems to be encoded in a smooth way in the embedding space. Alt</context>
<context position="32808" citStr="Turian et al., 2010" startWordPosition="5358" endWordPosition="5361">omatic evaluation of embeddings on a number of query inventories. Gao et al. (2014) publish an improved query inventory for the analogical reasoning task. Finally, Tsvetkov et al. (2015) propose a new intrinsic measure that better correlates with extrinsic performance. However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness. Extrinsic evaluations use embeddings as features in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of existing systems (Turian et al., 2010). However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014). More work has been done in unsupervised semantic modeling in the context of topic models. One example is the word intrusion task (Chang et al., 2009), in which annotators are asked to identify a random word inserted into the set of high probability words for a given topic. Word embeddings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors. Manual evaluation is expensive and time-consuming, but other work establ</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In ACL, pages 384– 394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In COLING,</booktitle>
<pages>905--912</pages>
<contexts>
<context position="31305" citStr="Turney, 2008" startWordPosition="5128" endWordPosition="5129">edding space for the intrinsic tasks gets polluted by frequency-based effects. We believe that further research should address how to better measure linguistic relationships between words in the embedding space, e.g., by learning a custom metric. 7 Related work Mikolov et al. (2013b) demonstrate that certain linguistic regularities exist in the embedding space. The authors show that by doing simple vector arithmetic in the embedding space, one can solve various syntactic and semantic analogy tasks. This is different to previous work, which phrased the analogy task as a classification problem (Turney, 2008). Surprisingly, word embedCCA C&amp;W CBOW GloVe Rand. Proj H-PCA 102 103 104 105 Word frequency Accuracy 1.0 0.9 0.8 0.7 0.6 0.5 Avg. rank by corpus frequency 305 dings seem to capture even more complex linguistic properties. Chen et al. (2013) show that word embeddings even contain information about regional spellings (UK vs. US), noun gender and sentiment polarity. Previous work in evaluation for word embeddings can be divided into intrinsic and extrinsic evaluations. Intrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and geo</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008. A uniform approach to analogies, synonyms, antonyms, and associations. In COLING, pages 905–912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In ACL,</booktitle>
<pages>947--953</pages>
<contexts>
<context position="25792" citStr="Yeh, 2000" startWordPosition="4231" endWordPosition="4232">linear combination of word embeddings weighted by the number of times that the word appeared in the review (using the same bag-of-words features as Maas et al. (2011)). A LIBLINEAR logistic regression model (Fan et al., 2008) with the default parameters is trained and evaluated using 10 fold crossvalidation. A vanilla bag of words feature set is the baseline (denoted as BOW here). We expect that this task will be more sensitive to semantic information than syntactic information. Results. Table 4 shows the average F1-scores for the chunking task. The p-values were computed using randomization (Yeh, 2000) on the sentence level. First, we can observe that adding word vectors as features results in performance lifts with all embeddings when compared to the baseline. The performance of C&amp;W and TSCCA is statistically not significant, and C&amp;W does better than all the remaining methods at the p = 0.05 level. Surprisingly, although the performance of Random Projections is still last, the gap to GloVe and CBOW is now very small. Table 5 shows results on the sentiment analysis task. We recover a similar order of embeddings as in the absolute intrinsic evaluation, however, the order of TSCCA and GloVe i</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In ACL, pages 947–953.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
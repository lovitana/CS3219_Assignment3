<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000095">
<title confidence="0.6646525">
Towards the Extraction of Customer-to-Customer Suggestions from
Reviews
</title>
<author confidence="0.933837">
Sapna Negi
</author>
<affiliation confidence="0.9502835">
Insight Centre for Data Analytics
National University of Ireland Galway
</affiliation>
<author confidence="0.969072">
Paul Buitelaar
</author>
<affiliation confidence="0.989992">
Insight Centre for Data Analytics
National University of Ireland Galway
</affiliation>
<email confidence="0.812853">
{firstname.lastname}@insight-centre.org
</email>
<sectionHeader confidence="0.990591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890043478261">
State of the art in opinion mining mainly
focuses on positive and negative senti-
ment summarisation of online customer
reviews. We observe that reviewers tend
to provide advice, recommendations and
tips to the fellow customers on a variety
of points of interest. In this work, we
target the automatic detection of sugges-
tion expressing sentences in customer re-
views. This is a novel problem, and there-
fore to begin with, requires a well formed
problem definition and benchmark dataset.
This work provides a 3- fold contribu-
tion, namely, problem definition, bench-
mark dataset, and an approach for detec-
tion of suggestions for the customers. The
problem is framed as a sentence classifi-
cation problem, and a set of linguistically
motivated features are proposed. Analy-
sis of the nature of suggestions, and clas-
sification errors, highlight challenges and
research opportunities associated with this
problem.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998563901960784">
Opinion mining mainly deals with the summari-
sation of opinions on the basis of their sentiment
polarities (Liu, 2012). However, on a closer obser-
vation of such opinionated text, we can discover
other facets of opinions. For example, a hotel re-
view sentence like, Make sure you bring plenty
of sun tan lotion-very expensive in the gift shop,
would be labeled as a neutral sentiment in cur-
rent opinion mining methodologies, since it would
only be interested in collecting opinions about the
hotel. In the case of aspect based sentiment anal-
ysis, the sentence does not comprise of hotel re-
lated aspects, and thus will again be labeled as
neutral/objective. While such sentences are gen-
erally ignored in sentiment based opinion sum-
marisation, these can be very useful information
to extract from the reviews. In hotel reviews, such
suggestions range from tips and advice on the re-
viewed entity, to suggestions and recommenda-
tions about the neighbourhood, transportation, and
things to do. Similarly, in product reviews sugges-
tions can be about how to make a better use the
product, accessories which go with them, or avail-
ability of better deals. We refer to such sentences
as customer-to-customer suggestions (CTC).
Another type of suggestions, which can appear in
the reviews, are the ones aiming at manufactur-
ers or service providers, suggesting new features
and improvements in products or services. For ex-
ample, An electric kettle in the room would have
been a useful addition. Recently, there have been
some works on extracting the suggestions for im-
provements from reviews (section 3), but they did
not focus on CTC suggestions. Also, suggestions
for improvement discuss only about the reviewed
entity and its aspects, unlike suggestions to cus-
tomers, which might also include other topics of
interest.
Suggestion mining and retrieval can be a poten-
tial new research area emerging from this kind of
research. Industrial importance of suggestions to
customers can be validated from the sections like
‘Room Tips’ (Figure 1) on TripAdvisor1. Simi-
larly, Yelp also features ‘tips’ 2 (Figure 2) related
to a business, and defines it as ‘key information’.
Such tips are often suggestions, or some impor-
tant information, a user wants to convey to others.
These tips are manually entered by the users, in
addition to the reviews. We note that using sug-
gestion mining, such information can be automat-
ically extracted from a large number of already ex-
</bodyText>
<footnote confidence="0.9989605">
1http://www.tripadvisor.com/
2http://www.yelp-support.com/article/What-are-tips
</footnote>
<page confidence="0.94293">
2159
</page>
<note confidence="0.9959445">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2159–2167,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.9999495">
Figure 1: Room Tips on TripAdvisor
Figure 2: Tips on Yelp
</figureCaption>
<bodyText confidence="0.977228095238095">
isting reviews. The recommendation type of sug-
gestions are of great importance in the case, when
there is no dedicated reviews available for small
shops, cafe, restaurants etc. in the vicinity of a
hotel/business. Suggestions extracted from a large
number of reviews, can also be seen as a kind of
summarisation, an alternative or complementary
to sentiment summarisation over the reviews.
In order to perform suggestion mining, expres-
sions of suggestions should to be detected in a
given text. The presence of a variety of linguistic
strategies in the suggestion expressing sentences
makes this task interesting from a computational
linguistics perspective. Section 2 discusses this in
more detail. The detection of suggestions in text
goes beyond the scope of sentiment polarity detec-
tion, and opens up new problems and challenges
in the areas of subjectivity analysis, social media
analysis, and extra-propositional semantics.
Our main contributions through this work can be
listed as:
</bodyText>
<listItem confidence="0.999508714285714">
• Proposition of the task of detection of CTC
suggestions.
• A well formed problem definition and scope.
• Preparation of benchmark dataset for two do-
mains of reviews, hotel and electronics.
• An approach to automatically detect CTC
suggestions from review texts.
</listItem>
<sectionHeader confidence="0.845099" genericHeader="introduction">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.99913025">
Since suggestion mining is a young problem, there
is a need for problem analysis and definition. As
indicated in the previous section, the tasks under
suggestion mining may vary. Below, we propose
three parameters whose value would help define
such tasks, and the values for these parameters in
the context of the current task of detection of CTC
suggestions.
</bodyText>
<listItem confidence="0.941735">
• Who is the suggestion aimed at?
</listItem>
<bodyText confidence="0.9910514">
As we explained in section 1, suggestions can be
aimed at one of the two kinds of stakeholders,
customers and service providers. In this work,
we perform the detection of suggestions for cus-
tomers only.
</bodyText>
<listItem confidence="0.9810235">
• What should be the textual unit of sugges-
tion?
</listItem>
<bodyText confidence="0.999912">
The previous works on extraction of suggestions
for product improvement considered sentences
as the unit of suggestions. In this work, we
also consider sentences as units of suggestion.
However, we observe that sentences might miss
the context, or refer to something mentioned in
the previous sentence. Furthermore, punctua-
tion marks are often erroneously used in social
media text, so automatic sentence split does not
work well with such text. The approach used in
this work aims at the detection of expressions
of suggestions, regardless of the presence or ab-
sence of context in the same sentence. There-
fore, we currently ignore the problems asso-
ciated with using sentences as a unit for sug-
gestions. The datasets used in this work are
sentiment analysis review datasets from other
works, in which reviews are already split into
sentences. For future works, we assume that
context can be determined by the neighbouring
text once the expression of suggestion is suc-
cessfully detected.
</bodyText>
<listItem confidence="0.9708145">
• What kind of text should be considered as a
suggestion?
</listItem>
<bodyText confidence="0.9856622">
Oxford dictionary defines suggestion as, An
idea or plan put forward for consideration.
Some of the listed synonyms of suggestions are
proposal, proposition, recommendation, advice,
hint, tip, clue. In a general scenario, this defini-
</bodyText>
<page confidence="0.893086">
2160
</page>
<bodyText confidence="0.875061727272727">
Sentence Category % Annotators Suggestion
Expression
Concierge is not available 24 hours Inform 70 Implicit
We did not have breakfast at the hotel but opted to grab Tell 80 Implicit
breads/pastries/coffee from the many food-outlets in the main stations.
Room was very quiet despite being close to the elevator. Remark 50 Implicit
The staff was nice and friendly Praise 60 Implicit
Double bed quite narrow and not as comfortable as expected. Disappoint 60 Implicit
The view from the 7th floor was amazing. Fascinate 50 Implicit
If you do end up here, be sure to specify a room at the back of the hotel. Suggest 100 Explicit
I recommend going for a Trabi Safari Recommend 90 Explicit
</bodyText>
<tableCaption confidence="0.997582">
Table 1: Sentences Which Were Perceived as CTC Suggestions by Laymen
</tableCaption>
<bodyText confidence="0.976936388888889">
tion of suggestion easily distinguishes sugges-
tions from other kind of text. However, when
suggestions are required to be identified in the
space of customer reviews, there seems to be a
tendency to consider most of the sentences as
suggestions to other customers. This observa-
tion is based on a preliminary data annotation
task, which was meant for data analysis and de-
velopment of annotation guidelines. Asher et.
al (2009) define 20 types of opinion expres-
sions, including suggestions and recommenda-
tions, which appear in opinionated text. In order
to form a representative sample, we chose 20
sentences from reviews, corresponding to each
of these types, and asked 10 people (layman) to
decide if these sentences are CTC suggestions
or not. Table 1 shows sentences for which 5 or
more people agreed of CTC Suggestion label.
Asher et. al. (2009) observe that these types are
not uniformly distributed in the reviews. Ac-
cording to them, suggestions and recommenda-
tions constitute about 10% of the statements,
while judgements (blame, praise) and senti-
ments (love, fascinate, hate, disappoint, sad)
constitute about 80%. Later, when we annotate
the review datasets for CTC sugggestions, they
also shows a smaller percentage of suggestions
(see table 2).
Except the suggest and recommend categories,
suggestions in rest of the categories are in an
implicit form and need to be inferred. Since
human beings can inherently infer suggestions,
the layman annotators considered both implicit
and explicit form of suggestions as suggestions.
However, in a real case scenario, humans can-
not go through the large amount of reviews and
infer suggestions from all of them. In this work,
we aim to automatically detect and extract the
explicit expressions of suggestions, rather than
inferring them.
For the ease of defining the scope of our work,
we propose two form of suggestions:
Explicit: Directly suggests/recommends an en-
tity or action,
Implicit: Only provides the information from
which a suggestion can be inferred, but do not
authoritatively suggest anything.
Lastly, we frame the problem of CTC sugges-
tion detection problem as a sentence classification
problem:
Given a set S of statements {s1,s2,s3,...,sn}, pre-
dict a label li for each of statement in S, where li
E {CTC suggestion, non CTC suggestion}, where
CTC suggestion should be explicitly expressed.
</bodyText>
<sectionHeader confidence="0.999769" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.996877">
Only a few attempts have been made to study sug-
gestion mining, and there is an unavailability of
benchmark datasets. Therefore, suggestion min-
ing still remains a young area of study.
</bodyText>
<listItem confidence="0.998202055555556">
• Suggestion Mining from Customer Reviews
As mentioned in section 1, there have been
some attempts to extract suggestions for im-
provements in products from customer reviews.
Ramanand et. al. (2010) used manually for-
mulated patterns to extract wishes regarding
improvements in products. Brun and Hagege
(2013) also used manually formulated rules to
extract suggestions for improvements from the
product reviews. Negi and Buitelaar (2015)
studied linguistic nature of suggestions and
wishes for improvements and performed exper-
iments in order to assert that these contain sub-
junctive mood. These works do not acknowl-
edge the fact that reviews can also contain sug-
gestions for other customers. One major draw-
back of previous works on customer reviews is
the public unavailability of evaluation datasets.
</listItem>
<page confidence="0.747683">
2161
</page>
<listItem confidence="0.929726">
• Other Domains
</listItem>
<bodyText confidence="0.999858869565218">
Two other lines of work extracted suggestions
from domains other than reviews. Dong et. al.
(2013) performed detection of suggestions for
product improvement using tweets. They used a
statistical classifier, with features comprising of
bag of words, automatically extracted sugges-
tion templates using sequential pattern mining
and hashtags. (Wicaksono and Myaeng, 2012;
Wicaksono and Myaeng, 2013) extracted advice
from discussion threads. They also used a su-
pervised classification approach, where some of
the features were domain dependent, like the
similarity between original query post and a
given sentence. They do not make any distinc-
tion between implicit and explicit advice, since
there is less ambiguity in the domain of discus-
sion forum.
None of the previous works study the complex
and interesting nature of suggestions in opinion-
ated text, and the relationship between suggestions
and sentiments. Also, there is an unavailability
of benchmark datasets of suggestions customer re-
views.
</bodyText>
<sectionHeader confidence="0.994643" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.99978716">
Since there is no available dataset of suggestions
for customers, we prepare new datasets for this
task. We consider two kinds of reviews for this
task, hotel reviews and electronic product reviews.
Hotel: Wachsmuth et al. (2014) provide a large
dataset of TripAdvisor hotel reviews, where re-
views are segmented into statement so that each
statement has only one sentiment label. State-
ments have been manually tagged with positive,
negative, conflict and neutral sentiments. We take
a smaller subset of these reviews, where each
statement is an instance of our dataset. Each state-
ment also bears a unique identity no., which is
constituted of hotel identity no., statement num-
ber and review identity no. Therefore, the reviews
belonging to same hotel, and the statements of the
same review can be identified.
Electronics: Hu et. al.(2004) provide a dataset
consisting of reviews of electronic products,
which is also already split into sentences, and the
corresponding sentiment for each sentence is man-
ually tagged.
In the next section, we further annotate the sen-
tences from these two datasets, for the current
task.
</bodyText>
<table confidence="0.999742">
Agreement Hotel(8050 sentences) Electronics(3782 sen-
tences)
Phase 1
Confidence CTC Sugg. CTC Sugg.
&gt;= 0.6 3220 1488
&gt;= 0.7 1046 604
&gt;= 0.8 1024 562
&gt;= 0.9 1020 558
1 1019 553
Phase 2
kappa CTC Sugg.(Explicit) CTC Sugg.(Explicit)
0.86 407 273
</table>
<tableCaption confidence="0.975269">
Table 2: Statistics of Phase 1 and Phase 2 Annota-
tions
</tableCaption>
<subsectionHeader confidence="0.966785">
4.1 Dataset Preparation
</subsectionHeader>
<bodyText confidence="0.999823529411764">
We performed a two phase annotation using both
crowdsourced and expert annotations. This re-
duced the number of statements to be annotated
by experts.
Phase 1 - Crowdsourced Annotations: The
Crowdflower3 platform was used for crowd-
sourced annotations. The platform provides a
set of management and analytics tools for qual-
ity management, as well as for interaction with
the annotators. In order to qualify for the anno-
tation task, a Crowdflower worker was required to
obtain a score of 7/10 out of 10 test statements.
The annotators were asked to choose one label out
of ‘Suggestion to Customers’ or ‘Other Statement’
for each sentence. The definition of suggestion
was left entirely to the understanding of the an-
notators. For each sentence, Crowdflower selects
the answer with the highest confidence score4. We
set the system not to seek more than 3 annotations
for a statement if one of the labels attained a confi-
dence score of 0.6 or more. At least 3, and at most
5 annotators labeled each statement. Confidence
score for each label is the weighted sum of the
trust scores of annotators who chose that label5.
Trust score is determined by annotator’s score in
the test questions. Table 2 shows the variation in
the number of statements tagged as suggestions
with the corresponding confidence scores. These
suggestions are a mixture of both implicit and ex-
plicit types, since the definition of suggestions was
not restricted for the annotators. We observed that
with the increase of confidence score, the ratio of
explicit suggestions among the tagged suggestions
increases.
</bodyText>
<footnote confidence="0.9943905">
3http://www.crowdflower.com/
4https://success.crowdflower.com/hc/en-
us/articles/201855939-Get-Results-How-to-Calculate-a-
Confidence-Score
5https://success.crowdflower.com/hc/en-
us/articles/202703305-Getting-Started-Glossary-of-Terms
</footnote>
<page confidence="0.994354">
2162
</page>
<bodyText confidence="0.999428545454545">
Phase 2 - Expert Annotations: Since we target
the extraction of explicitly expressed suggestions,
two expert annotators further classified the sen-
tences which were finally labelled as ‘Suggestion
to Customers’ by the Crowdflower platform, as ex-
plicit CTC suggestions and implicit CTC sugges-
tions. Therefore, the number of sentences which
experts annotated was much smaller than the total
number of sentences in the two datasets. The key
points of annotation guidelines for identifying the
explicit suggestions are:
</bodyText>
<listItem confidence="0.88879525">
1. The intention of giving a suggestion and
the suggested action or recommended entity
should be explicitly stated in the sentence.
For example:
</listItem>
<bodyText confidence="0.970386">
[[Try]intention[the cup cakes at the bakery
next door]entity]action.
Other explicit forms of this suggestion could
be: I recommend the cup cakes at the bakery
next door, or, You should definitely taste the
cup cakes from the bakery next door. Implicit
form could be, The cup cakes from the bakery
next door were delicious.
2. The suggestion should have the intention of
benefiting the customers, and should not be
mere sarcasm or joke. For example, If the
player doesn’t work now, you can run it over
with your car.
A kappa value of 0.86 was calculated between the
annotations performed by the expert annotators.
The datasets from both phase 1 and phase 2 anno-
tations are freely available for research. The final
dataset has 3 kinds of labels, Implicit CTC, Ex-
plicit CTC, and others. Therefore, this dataset is
also usable by the works which intend to extract
implicit suggestions as well.
</bodyText>
<sectionHeader confidence="0.981004" genericHeader="method">
5 Suggestion Detection
</sectionHeader>
<bodyText confidence="0.981158666666667">
We frame the task of CTC suggestion detection as
a text classification problem. Our approach per-
forms binary classification in a supervised fash-
ion. Explicit CTC suggestions belong to the posi-
tive class, and rest of the sentences to the negative
class.
Heuristic Features: A general notion about the
task is that suggestions contain some distinc-
tive keywords like, suggest, recommend etc, and
should be easily detectable using them. Therefore,
we use a set of manually selected features in order
to test this notion.
</bodyText>
<listItem confidence="0.865363944444444">
• Suggestion keywords: These include the
verbs: suggestion, advise, request, ask, warn,
recommend, do, do not; their corresponding
nouns: suggestion, advice, request, warning,
tip, recommendation, and the synonyms of
these words obtained from WordNet. Sug-
gestion keywords constitute a single feature,
which is binary in nature, and its value is de-
termined by the presence of any of the sug-
gestion keywords in the sentence.
• POS tag VB: Base form of Verb (VB) ap-
pears to be frequently used in the predicates
of CTC suggestions.
Generic Features: Standard uni, bi, tri-grams,
and uni, bi, tri-grams of Part of Speech tags (Penn
Tree bank tagset). We consider the best perform-
ing set of Heuristics and generic feature types as
the baseline for this task (see Table 3).
</listItem>
<subsectionHeader confidence="0.966058">
5.1 Special Features
</subsectionHeader>
<bodyText confidence="0.981783516129032">
We suggest a set of complementary features,
which are motivated by the linguistic analysis of
explicit CTC suggestions.
1. Imperative Mood Sequential Patterns: Im-
perative mood expressions are often present in
explicit suggestions. Wicaksono et. al. (2013)
also observed the presence of imperative mood
in advice sentences. They used an imperative
mood detector, based on a set of handmade
rules to determine whether a sentence is imper-
ative.
In our case, the statements often contain more
than one clause, and more than one mood ex-
pression. This feature aims to detect if any part
of a given statement bears atleast one expres-
sion of imperative mood. We aim to identify
sequential patterns of linguistic elements (POS
tags in our case) which mark the imperative
mood, and check if these patterns are present
in a given statement. We automatically extract
these features (patterns) using sequential pat-
tern mining. We prepare a small dataset of 200
example sentences of imperative mood. These
sentences were short sentence of lengths be-
tween 3-8 words, and are manually collected
from websites related to English grammar, and
linguistic research papers on mood and modal-
ity.
Sequential Pattern Mining - Background:
State of the art sequential pattern mining algo-
rithms require the dataset to be converted into
</bodyText>
<page confidence="0.89969">
2163
</page>
<table confidence="0.9995568">
Features Hotel Electronics
Precision Recall F score Precision Recall F score
Heuristic 0.260 0.484 0.338 0.216 0.505 0.303
Unigrams 0.492 0.528 0.509 0.527 0.565 0.540
Uni,bi-grams 0.513 0.577 0.543 0.571 0.602 0.586
Uni, bi, tri-grams 0.519 0.545 0.532 0.562 0.605 0.570
uni, bi-grams + unigrams POS tags 0.539 0.555 0.547 0.634 0.565 0.595
uni, bi-grams + uni, bi-grams POS tags 0.568 0.491 0.527 0.662 0.515 0.575
uni, bi-grams + uni, bi, tri-grams POS 0.593 0.469 0.524 0.625 0.518 0.556
tags
</table>
<tableCaption confidence="0.999217">
Table 3: Performance of Heuristic and Generic Features in a 10-Fold Cross Validation
</tableCaption>
<figure confidence="0.773131333333333">
Support Sequence
0.69 &lt;VB&gt;&lt;NN&gt;
0.60 &lt; V B &gt;&lt; PRP &gt;
0.54 &lt;VB&gt;&lt;VB&gt;&lt;NN&gt;
0.51 &lt;RB&gt;&lt;VB&gt;
0.70 &lt;MD&gt;&lt;VB&gt;
</figure>
<tableCaption confidence="0.868953">
Table 4: Sequential POS Patterns obtained from
Imperative Mood Dataset
</tableCaption>
<bodyText confidence="0.982740642857143">
an ordered list of events. An event is a non-
empty unordered collection of items, which in
turn is the smallest unit of sequences. The
output of these algorithms are sequential pat-
terns, which comprise of sequences of items,
and is defined to be frequent if its support (a
measure of frequency) is equal or more than
a user-defined threshold. (a1, a2, ..., aq) is a
sequence, where ai is an event. A sequence
(a1, a2, ..., an), is a subsequence of another
sequence (b1, b2, ...bm), if there exist integers
i1 &lt; i2 &lt; ... &lt; in such that a1 C_ bi1,
a2 C_ bi2, ..., an C_ bin. For example, given
a sequence (AB,E,ACD), where B is an item
and AB, E and ACD are events. (B,AC) is a
subsequence, since B C_ AB and AC C_ ACD
and the order of events is preserved. (AB,E) is
not a subsequence of the sequence (ABE). In
our case, each POS tag is an event, and all the
events are of length 1 item.
We use Clospan (Yan et al., 2003) algorithm
to obtain the patterns of POS tags appearing
in imperative mood sentences. Closed pattern
mining algorithms produce a significantly less
number of sequences than the older methods,
while preserving the same expressive power.
We use 2-3 length sequences with a relative
support greater than 0.5, where maximum rela-
tive support is 1 (see Table 4).
Sequential patterns and n-grams: Sequen-
tial patterns are different from continuous n-
grams in the sense that these are similar to the
templates with place-holders, or skip n-grams,
where the items in a pattern are ordered but
should not necessarily be immediate to each
other.
The part of speech tags avoid sparse nature of
word based patterns, and at the same time cap-
tures the language usage model of imperative
sentences. However, the pos tags tend to repeat
in longer texts, therefore we limit the number of
placeholders which could appear between two
items of a pattern to 2. Each pattern is treated
as a separate feature, whose feature value is bi-
nary depending on its presence or absence in a
given sentence.
2. Sentiment Features: Given that suggestions
are being extracted from customer reviews,
which are otherwise mostly used for sentiment
analysis, a relation between sentiments and
suggestions can be suspected. It can be ob-
served From the Figure 3, 4 suggestions do not
seem to always carry one particular sentiment,
but different sentiments at different instances.
We compare three types of sentiment related
features:
</bodyText>
<listItem confidence="0.9647135">
a) Manually tagged sentiments: These anno-
tations were provided with the used sentiment
analysis datasets.
b) Sentiwordnet score summation: SentiWord-
Net (Esuli and Sebastiani, 2006) sentiment
score summation of all the words in a sentence.
No sense disambiguation is performed, and all
synset scores are summed up for each word.
c) Normalised sentiwordnet score summation:
These scores are the sum of all the sentiment
scores of the words in a given sentence, nor-
malised over the number of words carrying
non-neutral sentiment score.
3. Information about the subject/s of a state-
ment: This feature captures the presence of
nsubj dependency (Marneffe and Manning,
</listItem>
<page confidence="0.965244">
2164
</page>
<table confidence="0.999759444444444">
Features Hotel Electronics
Precision Recall F score AUC Precision Recall F score AUC
Baseline (best generic features) 0.539 0.555 0.547 0.763 0.634 0.565 0.595 0.817
+ patterns 0.542 0.511 0.526 0.743 0.607 0.604 0.616 0.790
+ sentiments (manual) 0.529 0.536 0.532 0.754 0.581 0.630 0.605 0.797
+ sentiments (score) 0.537 0.541 0.539 0.757 0.563 0.593 0.578 0.779
+ sentiments (normalised score) 0.543 0.561 0.550 0.774 0.645 0.593 0.618 0.784
+ nsubj 0.559 0.538 0.548 0.757 0.597 0.586 0.591 0.778
Baseline + special (all) 0.580 0.512 0.567 0.781 0.645 0.621 0.640 0.790
</table>
<tableCaption confidence="0.999154">
Table 5: Performance of Special Features in a 10 Fold Cross Validation
</tableCaption>
<figureCaption confidence="0.98954175">
Figure 3: Manually labelled Sentiment Distribu-
tion of Electronics Dataset
Figure 4: Manually labelled Sentiment Distribu-
tion of Hotel Dataset
</figureCaption>
<bodyText confidence="0.999795611111111">
2008), and if present, the pair of POS tags of
the arguments of this dependency. Often a re-
viewer addresses the reader when giving a sug-
gestion. For the sentence, Ifyou do end up here,
be sure to specify a room at the back of the
hotel, the nsubj dependency is nsubj(do, you).
The feature value would be VBP-PRP in this
case. On the other hand, this suggestion could
also have been, Be sure to specify a room at the
back of the hotel.. In this case the feature value
would be null. If more than one nsubj depen-
dency is present, the POS pair of each of them
will be included in the feature value.
Experimental Setup: We use the Stanford Parser
(Toutanova et al., 2003) for obtaining part of
speech and dependency information. Stemming
did not effect the results. Stopwords were used
using a customised stopword list. We employ the
LibSVM library (Chang and Lin, 2011) for Sup-
port Vector Machine classifiers (SVM), as imple-
mented in Weka machine learning toolkit (Hall et
al., 2009). The parameter values of SVM classi-
fiers are: SVM type = C-SVC, Kernel Function
= Radial Basis Function. A 10 fold cross valida-
tion is performed in order to evaluate the classifier
model. Features are ranked using the feature se-
lection algorithm InfoGain (Mitchell, 1997). The
other attribute selection algorithms provided with
the Weka toolkit were also experimented with, but
InfoGain consistently performed best in all the
runs. Only the features which posses a positive
information gain value are retained. The best per-
forming run has a feature vector of size 300. The
problem of having a imbalanced dataset is handled
by using a higher class weight of 5:1 (Akbani et
al., 2004) for positive class.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999274333333333">
We evaluate the proposed features using 10-fold
cross validation. As indicated in section 5, we
consider best performing set of generic features
as the baseline, which is: uni, bi grams and uni-
grams of pos tags. Table 5 summarises the classi-
fier performance with the addition of special fea-
tures, measured in Precision, Recall, F1 score, and
ROC Area Under Curve. The results indicate that
special features improve the F score in both the
domains. However, some of the generic features
produced better precision and recall values (Table
3). Imperative patterns improve the baseline re-
sults for electronics dataset, but not for the hotel
dataset. Also, the special features produce better
improvement over baseline for electronics dataset.
We attribute this to the smaller size of electronics
dataset.
Table 7 shows how the top ranked features change
on addition of special features. Heuristic fea-
tures tend to appear as some of the top ranked
generic features. On the addition of special fea-
tures, some of the special features replace the top
general features. This validates the importance of
proposed special features. Normalised sentiment
</bodyText>
<page confidence="0.968227">
2165
</page>
<table confidence="0.9971797">
# Sentence Classifier Human
1 Buy this for the storage and price, avoid it if you know nothing about computers. 1 1
2 whichever camera you buy, add upto about 200 dollars for an additional memory card i bought a 256 mb card, usb 0 1
card reader,camera bag and the warranty.
3 Looks sort of like picasa software (google it if you dont know ) in the interface and is as easy to install and operate as 0 1
g2
4 If you cant make an Italian meal don’t advertise you can. 1 0
5 It would have been better to have some sort of window on the carrying case , so you could see the display without 1 0
opening it
6 If you have a lot of money to waste, make sure you book this hotel 1 0
</table>
<tableCaption confidence="0.973461">
Table 6: Error Analysis of the Best Performing Feature Set (0 = Negative Class, 1 = Positive Class)
</tableCaption>
<table confidence="0.999254">
Type Domain Top Features
Baseline Hotel you, VB, if you, recommend,
if, want, you are, highly recom-
mend, I would
Elec. you, VB, recommend, if you,
if, buy, recommend this, we, I
highly recommend, don’t, get
Baseline Hotel you, VB, &lt; V B, NN &gt;, VBD,
+Special &lt; V B, V B, NN &gt;, recom-
mend, if you, root(Root,VB),
norm. sentiment score, suggest
Elec. you, VB, &lt; VB, NN &gt;, rec-
ommend, if you, nsubj, buy, sug-
gest, &lt; V B, PRP &gt;, norm.
sentiment score
</table>
<tableCaption confidence="0.967234">
Table 7: Some of the Top Ranked Features in Dif-
ferent Runs
</tableCaption>
<bodyText confidence="0.9998458">
score prove to be better features than the other two
types of sentiment features including the manu-
ally labelled sentiments. This indicates that this
method of sentiment calculation is capturing some
universally used phrasing for suggestions, where
real sentiment of the sentence fails to capture it.
Observed Challenges: Table 6 shows some in-
stances of Type 1 and Type 2 errors in the best
performing feature set. Our error analysis reveals
the challenges associated with this task.
</bodyText>
<listItem confidence="0.5431550625">
1. Non-CTC Suggestions: Example #4 gives a
suggestion for the improvement of the hotel
restaurant. Similarly, #5 is a suggestion to the
product manufacturer instead of fellow cus-
tomers. These kind of suggestions at times
possess features similar to CTC suggestions.
2. Complex sentences: Often, suggestion is
only expressed in one part of a very long
sentence (#2,#3,). This might generate erro-
neous rank for features; also part of speech
taggers tend to perform poorly for such sen-
tences.
3. Sarcasm: The surface form of #6 is a sugges-
tion, but it is a sarcasm.
4. Biased Datasets: Explicit CTC suggestion
expressing sentences occur sparsely, which
</listItem>
<bodyText confidence="0.997978875">
is unfavourable for supervised learning ap-
proaches because of increased cost of data
annotation task, and imbalanced class repre-
sentation.
A general observation is that the text in the form of
suggestions may not always be a suggestion, and
vice versa. Therefore, syntactic and lexical fea-
tures seem to be ineffective in many cases.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999969833333333">
This work serves as an introduction and analysis
of the problem of the extraction of customer-to-
customer suggestions from reviews. The task is
useful for a number of practical applications. We
observed that the layman perception of suggestion
is very wide, especially in the case of reviews.
Therefore, we defined and limited the scope of our
work to explicit suggestions. Following this, we
prepared a well-investigated benchmark dataset,
which is freely available for research purposes.
A pilot approach to the problem is presented,
which analyses the performance of standard text
classification features, and tests a set of comple-
mentary/special features. The special features im-
proved the baseline results for both service and
product reviews.
Analysis of classification errors highlighted the
challenges associated with the task. The classi-
fication results have scope for improvement, and
therefore the task calls for advanced semantic fea-
tures and dedicated models, which will be our fu-
ture direction. Furthermore, the relation between
sentiments and suggestions seem to be worth in-
vestigating.
</bodyText>
<sectionHeader confidence="0.978231" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9385964">
This work has been funded by the European
Unions Horizon 2020 programme under grant
agreement No 644632 MixedEmotions, and the
Science Foundation Ireland under Grant Number
SFI/12/RC/2289 (Insight Center).
</bodyText>
<page confidence="0.989245">
2166
</page>
<sectionHeader confidence="0.993804" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948376623377">
Rehan Akbani, Stephen Kwek, and Nathalie Japkow-
icz. 2004. Applying support vector machines to
imbalanced datasets. In Machine Learning: ECML
2004, Lecture Notes in Computer Science, pages
39–50. Springer Berlin Heidelberg.
Nicholas Asher, Farah Benamara, and Yannick Math-
ieu. 2009. Appraisal of Opinion Expressions in Dis-
course. Lingvistic Investigationes, 31.2:279–292.
Caroline Brun and Caroline Hagege. 2013. Suggestion
mining: Detecting suggestions for improvement in
users comments. Research in Computing Science.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1–27:27.
Li Dong, Furu Wei, Yajuan Duan, Xiaohua Liu, Ming
Zhou, and Ke Xu. 2013. The automated acquisition
of suggestions from tweets. In AAAI. AAAI Press.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06, pages 417–422.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software, an update.
SIGKDD Explorations, 11:10–18.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.
Marie-Catherine De Marneffe and Christopher D.
Manning. 2008. Stanford typed dependencies man-
ual.
Thomas M. Mitchell. 1997. Machine Learning.
McGraw-Hill, Inc., New York, NY, USA, 1 edition.
Sapna Negi and Paul Buitelaar. 2015. Curse or boon?
presence of subjunctive mood in opinionated text. In
Proceedings of the 11th International Conference on
Computational Semantics, pages 101–106, London,
UK, April. Association for Computational Linguis-
tics.
Janardhanan Ramanand, Krishna Bhavsar, and Niran-
jan Pedanekar. 2010. Wishful thinking - finding
suggestions and ’buy’ wishes from product reviews.
In Proceedings of the NAACL HLT 2010 Workshop
on Computational Approaches to Analysis and Gen-
eration of Emotion in Text, pages 54–61, Los Ange-
les, CA, June. Association for Computational Lin-
guistics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In In Proceedings of HLT-NAACL 2003, pages 252–
259.
Henning Wachsmuth, Martin Trenkmann, Benno Stein,
Gregor Engels, and Tsvetomira Palakarska. 2014.
A review corpus for argumentation analysis. In
Proceedings of the 15th International Conference
on Computational Linguistics and Intelligent Text
Processing, volume 8404 of LNCS, pages 115–127,
Kathmandu, Nepal. Springer.
Alfan Farizki Wicaksono and Sung-Hyon Myaeng.
2012. Mining advices from weblogs. In Proceed-
ings of the 21st ACM International Conference on
Information and Knowledge Management, CIKM
’12, pages 2347–2350, New York, NY, USA. ACM.
Alfan Farizki Wicaksono and Sung-Hyon Myaeng.
2013. Automatic extraction of advice-revealing sen-
tences foradvice mining from online forums. In K-
CAP, pages 97–104. ACM.
Xifeng Yan, Jiawei Han, and Ramin Afshar. 2003.
Clospan: Mining closed sequential patterns in large
datasets. In In SDM, pages 166–177.
</reference>
<page confidence="0.994744">
2167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.740129">
<title confidence="0.994833">Towards the Extraction of Customer-to-Customer Suggestions from Reviews</title>
<author confidence="0.99177">Sapna Negi</author>
<affiliation confidence="0.9900525">Insight Centre for Data National University of Ireland Galway</affiliation>
<author confidence="0.989966">Paul Buitelaar</author>
<affiliation confidence="0.9873855">Insight Centre for Data National University of Ireland Galway</affiliation>
<abstract confidence="0.989482041666667">State of the art in opinion mining mainly focuses on positive and negative sentiment summarisation of online customer reviews. We observe that reviewers tend to provide advice, recommendations and tips to the fellow customers on a variety of points of interest. In this work, we target the automatic detection of suggestion expressing sentences in customer reviews. This is a novel problem, and therefore to begin with, requires a well formed problem definition and benchmark dataset. This work provides a 3fold contribution, namely, problem definition, benchmark dataset, and an approach for detection of suggestions for the customers. The problem is framed as a sentence classification problem, and a set of linguistically motivated features are proposed. Analysis of the nature of suggestions, and classification errors, highlight challenges and research opportunities associated with this problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rehan Akbani</author>
<author>Stephen Kwek</author>
<author>Nathalie Japkowicz</author>
</authors>
<title>Applying support vector machines to imbalanced datasets.</title>
<date>2004</date>
<booktitle>In Machine Learning: ECML 2004, Lecture Notes in Computer Science,</booktitle>
<pages>39--50</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="26044" citStr="Akbani et al., 2004" startWordPosition="4184" endWordPosition="4187">, Kernel Function = Radial Basis Function. A 10 fold cross validation is performed in order to evaluate the classifier model. Features are ranked using the feature selection algorithm InfoGain (Mitchell, 1997). The other attribute selection algorithms provided with the Weka toolkit were also experimented with, but InfoGain consistently performed best in all the runs. Only the features which posses a positive information gain value are retained. The best performing run has a feature vector of size 300. The problem of having a imbalanced dataset is handled by using a higher class weight of 5:1 (Akbani et al., 2004) for positive class. 6 Results and Discussion We evaluate the proposed features using 10-fold cross validation. As indicated in section 5, we consider best performing set of generic features as the baseline, which is: uni, bi grams and unigrams of pos tags. Table 5 summarises the classifier performance with the addition of special features, measured in Precision, Recall, F1 score, and ROC Area Under Curve. The results indicate that special features improve the F score in both the domains. However, some of the generic features produced better precision and recall values (Table 3). Imperative pa</context>
</contexts>
<marker>Akbani, Kwek, Japkowicz, 2004</marker>
<rawString>Rehan Akbani, Stephen Kwek, and Nathalie Japkowicz. 2004. Applying support vector machines to imbalanced datasets. In Machine Learning: ECML 2004, Lecture Notes in Computer Science, pages 39–50. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Farah Benamara</author>
<author>Yannick Mathieu</author>
</authors>
<date>2009</date>
<booktitle>Appraisal of Opinion Expressions in Discourse. Lingvistic Investigationes,</booktitle>
<pages>31--2</pages>
<marker>Asher, Benamara, Mathieu, 2009</marker>
<rawString>Nicholas Asher, Farah Benamara, and Yannick Mathieu. 2009. Appraisal of Opinion Expressions in Discourse. Lingvistic Investigationes, 31.2:279–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Brun</author>
<author>Caroline Hagege</author>
</authors>
<title>Suggestion mining: Detecting suggestions for improvement in users comments.</title>
<date>2013</date>
<journal>Research in Computing Science.</journal>
<contexts>
<context position="10806" citStr="Brun and Hagege (2013)" startWordPosition="1712" endWordPosition="1715">atement in S, where li E {CTC suggestion, non CTC suggestion}, where CTC suggestion should be explicitly expressed. 3 Related Work Only a few attempts have been made to study suggestion mining, and there is an unavailability of benchmark datasets. Therefore, suggestion mining still remains a young area of study. • Suggestion Mining from Customer Reviews As mentioned in section 1, there have been some attempts to extract suggestions for improvements in products from customer reviews. Ramanand et. al. (2010) used manually formulated patterns to extract wishes regarding improvements in products. Brun and Hagege (2013) also used manually formulated rules to extract suggestions for improvements from the product reviews. Negi and Buitelaar (2015) studied linguistic nature of suggestions and wishes for improvements and performed experiments in order to assert that these contain subjunctive mood. These works do not acknowledge the fact that reviews can also contain suggestions for other customers. One major drawback of previous works on customer reviews is the public unavailability of evaluation datasets. 2161 • Other Domains Two other lines of work extracted suggestions from domains other than reviews. Dong et</context>
</contexts>
<marker>Brun, Hagege, 2013</marker>
<rawString>Caroline Brun and Caroline Hagege. 2013. Suggestion mining: Detecting suggestions for improvement in users comments. Research in Computing Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Libsvm: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="25247" citStr="Chang and Lin, 2011" startWordPosition="4052" endWordPosition="4055"> dependency is nsubj(do, you). The feature value would be VBP-PRP in this case. On the other hand, this suggestion could also have been, Be sure to specify a room at the back of the hotel.. In this case the feature value would be null. If more than one nsubj dependency is present, the POS pair of each of them will be included in the feature value. Experimental Setup: We use the Stanford Parser (Toutanova et al., 2003) for obtaining part of speech and dependency information. Stemming did not effect the results. Stopwords were used using a customised stopword list. We employ the LibSVM library (Chang and Lin, 2011) for Support Vector Machine classifiers (SVM), as implemented in Weka machine learning toolkit (Hall et al., 2009). The parameter values of SVM classifiers are: SVM type = C-SVC, Kernel Function = Radial Basis Function. A 10 fold cross validation is performed in order to evaluate the classifier model. Features are ranked using the feature selection algorithm InfoGain (Mitchell, 1997). The other attribute selection algorithms provided with the Weka toolkit were also experimented with, but InfoGain consistently performed best in all the runs. Only the features which posses a positive information</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: A library for support vector machines. ACM Trans. Intell. Syst. Technol., 2(3):27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Dong</author>
<author>Furu Wei</author>
<author>Yajuan Duan</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Ke Xu</author>
</authors>
<title>The automated acquisition of suggestions from tweets. In AAAI.</title>
<date>2013</date>
<publisher>AAAI Press.</publisher>
<marker>Dong, Wei, Duan, Liu, Zhou, Xu, 2013</marker>
<rawString>Li Dong, Furu Wei, Yajuan Duan, Xiaohua Liu, Ming Zhou, and Ke Xu. 2013. The automated acquisition of suggestions from tweets. In AAAI. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="23108" citStr="Esuli and Sebastiani, 2006" startWordPosition="3692" endWordPosition="3695">absence in a given sentence. 2. Sentiment Features: Given that suggestions are being extracted from customer reviews, which are otherwise mostly used for sentiment analysis, a relation between sentiments and suggestions can be suspected. It can be observed From the Figure 3, 4 suggestions do not seem to always carry one particular sentiment, but different sentiments at different instances. We compare three types of sentiment related features: a) Manually tagged sentiments: These annotations were provided with the used sentiment analysis datasets. b) Sentiwordnet score summation: SentiWordNet (Esuli and Sebastiani, 2006) sentiment score summation of all the words in a sentence. No sense disambiguation is performed, and all synset scores are summed up for each word. c) Normalised sentiwordnet score summation: These scores are the sum of all the sentiment scores of the words in a given sentence, normalised over the number of words carrying non-neutral sentiment score. 3. Information about the subject/s of a statement: This feature captures the presence of nsubj dependency (Marneffe and Manning, 2164 Features Hotel Electronics Precision Recall F score AUC Precision Recall F score AUC Baseline (best generic featu</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software, an update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<pages>11--10</pages>
<contexts>
<context position="25361" citStr="Hall et al., 2009" startWordPosition="4071" endWordPosition="4074">ould also have been, Be sure to specify a room at the back of the hotel.. In this case the feature value would be null. If more than one nsubj dependency is present, the POS pair of each of them will be included in the feature value. Experimental Setup: We use the Stanford Parser (Toutanova et al., 2003) for obtaining part of speech and dependency information. Stemming did not effect the results. Stopwords were used using a customised stopword list. We employ the LibSVM library (Chang and Lin, 2011) for Support Vector Machine classifiers (SVM), as implemented in Weka machine learning toolkit (Hall et al., 2009). The parameter values of SVM classifiers are: SVM type = C-SVC, Kernel Function = Radial Basis Function. A 10 fold cross validation is performed in order to evaluate the classifier model. Features are ranked using the feature selection algorithm InfoGain (Mitchell, 1997). The other attribute selection algorithms provided with the Weka toolkit were also experimented with, but InfoGain consistently performed best in all the runs. Only the features which posses a positive information gain value are retained. The best performing run has a feature vector of size 300. The problem of having a imbala</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software, an update. SIGKDD Explorations, 11:10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1327" citStr="Liu, 2012" startWordPosition="196" endWordPosition="197">es a well formed problem definition and benchmark dataset. This work provides a 3- fold contribution, namely, problem definition, benchmark dataset, and an approach for detection of suggestions for the customers. The problem is framed as a sentence classification problem, and a set of linguistically motivated features are proposed. Analysis of the nature of suggestions, and classification errors, highlight challenges and research opportunities associated with this problem. 1 Introduction Opinion mining mainly deals with the summarisation of opinions on the basis of their sentiment polarities (Liu, 2012). However, on a closer observation of such opinionated text, we can discover other facets of opinions. For example, a hotel review sentence like, Make sure you bring plenty of sun tan lotion-very expensive in the gift shop, would be labeled as a neutral sentiment in current opinion mining methodologies, since it would only be interested in collecting opinions about the hotel. In the case of aspect based sentiment analysis, the sentence does not comprise of hotel related aspects, and thus will again be labeled as neutral/objective. While such sentences are generally ignored in sentiment based o</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2008</date>
<note>Stanford typed dependencies manual.</note>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D. Manning. 2008. Stanford typed dependencies manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<volume>1</volume>
<pages>edition.</pages>
<publisher>McGraw-Hill, Inc.,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="25633" citStr="Mitchell, 1997" startWordPosition="4118" endWordPosition="4119">arser (Toutanova et al., 2003) for obtaining part of speech and dependency information. Stemming did not effect the results. Stopwords were used using a customised stopword list. We employ the LibSVM library (Chang and Lin, 2011) for Support Vector Machine classifiers (SVM), as implemented in Weka machine learning toolkit (Hall et al., 2009). The parameter values of SVM classifiers are: SVM type = C-SVC, Kernel Function = Radial Basis Function. A 10 fold cross validation is performed in order to evaluate the classifier model. Features are ranked using the feature selection algorithm InfoGain (Mitchell, 1997). The other attribute selection algorithms provided with the Weka toolkit were also experimented with, but InfoGain consistently performed best in all the runs. Only the features which posses a positive information gain value are retained. The best performing run has a feature vector of size 300. The problem of having a imbalanced dataset is handled by using a higher class weight of 5:1 (Akbani et al., 2004) for positive class. 6 Results and Discussion We evaluate the proposed features using 10-fold cross validation. As indicated in section 5, we consider best performing set of generic feature</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Thomas M. Mitchell. 1997. Machine Learning. McGraw-Hill, Inc., New York, NY, USA, 1 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sapna Negi</author>
<author>Paul Buitelaar</author>
</authors>
<title>Curse or boon? presence of subjunctive mood in opinionated text.</title>
<date>2015</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Semantics,</booktitle>
<pages>101--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>London, UK,</location>
<contexts>
<context position="10934" citStr="Negi and Buitelaar (2015)" startWordPosition="1730" endWordPosition="1733"> Work Only a few attempts have been made to study suggestion mining, and there is an unavailability of benchmark datasets. Therefore, suggestion mining still remains a young area of study. • Suggestion Mining from Customer Reviews As mentioned in section 1, there have been some attempts to extract suggestions for improvements in products from customer reviews. Ramanand et. al. (2010) used manually formulated patterns to extract wishes regarding improvements in products. Brun and Hagege (2013) also used manually formulated rules to extract suggestions for improvements from the product reviews. Negi and Buitelaar (2015) studied linguistic nature of suggestions and wishes for improvements and performed experiments in order to assert that these contain subjunctive mood. These works do not acknowledge the fact that reviews can also contain suggestions for other customers. One major drawback of previous works on customer reviews is the public unavailability of evaluation datasets. 2161 • Other Domains Two other lines of work extracted suggestions from domains other than reviews. Dong et. al. (2013) performed detection of suggestions for product improvement using tweets. They used a statistical classifier, with f</context>
</contexts>
<marker>Negi, Buitelaar, 2015</marker>
<rawString>Sapna Negi and Paul Buitelaar. 2015. Curse or boon? presence of subjunctive mood in opinionated text. In Proceedings of the 11th International Conference on Computational Semantics, pages 101–106, London, UK, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janardhanan Ramanand</author>
<author>Krishna Bhavsar</author>
<author>Niranjan Pedanekar</author>
</authors>
<title>Wishful thinking - finding suggestions and ’buy’ wishes from product reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>54--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, CA,</location>
<marker>Ramanand, Bhavsar, Pedanekar, 2010</marker>
<rawString>Janardhanan Ramanand, Krishna Bhavsar, and Niranjan Pedanekar. 2010. Wishful thinking - finding suggestions and ’buy’ wishes from product reviews. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 54–61, Los Angeles, CA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="25048" citStr="Toutanova et al., 2003" startWordPosition="4021" endWordPosition="4024">of the arguments of this dependency. Often a reviewer addresses the reader when giving a suggestion. For the sentence, Ifyou do end up here, be sure to specify a room at the back of the hotel, the nsubj dependency is nsubj(do, you). The feature value would be VBP-PRP in this case. On the other hand, this suggestion could also have been, Be sure to specify a room at the back of the hotel.. In this case the feature value would be null. If more than one nsubj dependency is present, the POS pair of each of them will be included in the feature value. Experimental Setup: We use the Stanford Parser (Toutanova et al., 2003) for obtaining part of speech and dependency information. Stemming did not effect the results. Stopwords were used using a customised stopword list. We employ the LibSVM library (Chang and Lin, 2011) for Support Vector Machine classifiers (SVM), as implemented in Weka machine learning toolkit (Hall et al., 2009). The parameter values of SVM classifiers are: SVM type = C-SVC, Kernel Function = Radial Basis Function. A 10 fold cross validation is performed in order to evaluate the classifier model. Features are ranked using the feature selection algorithm InfoGain (Mitchell, 1997). The other att</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In In Proceedings of HLT-NAACL 2003, pages 252– 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henning Wachsmuth</author>
<author>Martin Trenkmann</author>
<author>Benno Stein</author>
<author>Gregor Engels</author>
<author>Tsvetomira Palakarska</author>
</authors>
<title>A review corpus for argumentation analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>8404</volume>
<pages>115--127</pages>
<publisher>Springer.</publisher>
<location>Kathmandu, Nepal.</location>
<contexts>
<context position="12554" citStr="Wachsmuth et al. (2014)" startWordPosition="1980" endWordPosition="1983">. They do not make any distinction between implicit and explicit advice, since there is less ambiguity in the domain of discussion forum. None of the previous works study the complex and interesting nature of suggestions in opinionated text, and the relationship between suggestions and sentiments. Also, there is an unavailability of benchmark datasets of suggestions customer reviews. 4 Data Since there is no available dataset of suggestions for customers, we prepare new datasets for this task. We consider two kinds of reviews for this task, hotel reviews and electronic product reviews. Hotel: Wachsmuth et al. (2014) provide a large dataset of TripAdvisor hotel reviews, where reviews are segmented into statement so that each statement has only one sentiment label. Statements have been manually tagged with positive, negative, conflict and neutral sentiments. We take a smaller subset of these reviews, where each statement is an instance of our dataset. Each statement also bears a unique identity no., which is constituted of hotel identity no., statement number and review identity no. Therefore, the reviews belonging to same hotel, and the statements of the same review can be identified. Electronics: Hu et. </context>
</contexts>
<marker>Wachsmuth, Trenkmann, Stein, Engels, Palakarska, 2014</marker>
<rawString>Henning Wachsmuth, Martin Trenkmann, Benno Stein, Gregor Engels, and Tsvetomira Palakarska. 2014. A review corpus for argumentation analysis. In Proceedings of the 15th International Conference on Computational Linguistics and Intelligent Text Processing, volume 8404 of LNCS, pages 115–127, Kathmandu, Nepal. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfan Farizki Wicaksono</author>
<author>Sung-Hyon Myaeng</author>
</authors>
<title>Mining advices from weblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12,</booktitle>
<pages>2347--2350</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11688" citStr="Wicaksono and Myaeng, 2012" startWordPosition="1844" endWordPosition="1847">ntain subjunctive mood. These works do not acknowledge the fact that reviews can also contain suggestions for other customers. One major drawback of previous works on customer reviews is the public unavailability of evaluation datasets. 2161 • Other Domains Two other lines of work extracted suggestions from domains other than reviews. Dong et. al. (2013) performed detection of suggestions for product improvement using tweets. They used a statistical classifier, with features comprising of bag of words, automatically extracted suggestion templates using sequential pattern mining and hashtags. (Wicaksono and Myaeng, 2012; Wicaksono and Myaeng, 2013) extracted advice from discussion threads. They also used a supervised classification approach, where some of the features were domain dependent, like the similarity between original query post and a given sentence. They do not make any distinction between implicit and explicit advice, since there is less ambiguity in the domain of discussion forum. None of the previous works study the complex and interesting nature of suggestions in opinionated text, and the relationship between suggestions and sentiments. Also, there is an unavailability of benchmark datasets of </context>
</contexts>
<marker>Wicaksono, Myaeng, 2012</marker>
<rawString>Alfan Farizki Wicaksono and Sung-Hyon Myaeng. 2012. Mining advices from weblogs. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pages 2347–2350, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfan Farizki Wicaksono</author>
<author>Sung-Hyon Myaeng</author>
</authors>
<title>Automatic extraction of advice-revealing sentences foradvice mining from online forums.</title>
<date>2013</date>
<booktitle>In KCAP,</booktitle>
<pages>97--104</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11717" citStr="Wicaksono and Myaeng, 2013" startWordPosition="1848" endWordPosition="1851">e works do not acknowledge the fact that reviews can also contain suggestions for other customers. One major drawback of previous works on customer reviews is the public unavailability of evaluation datasets. 2161 • Other Domains Two other lines of work extracted suggestions from domains other than reviews. Dong et. al. (2013) performed detection of suggestions for product improvement using tweets. They used a statistical classifier, with features comprising of bag of words, automatically extracted suggestion templates using sequential pattern mining and hashtags. (Wicaksono and Myaeng, 2012; Wicaksono and Myaeng, 2013) extracted advice from discussion threads. They also used a supervised classification approach, where some of the features were domain dependent, like the similarity between original query post and a given sentence. They do not make any distinction between implicit and explicit advice, since there is less ambiguity in the domain of discussion forum. None of the previous works study the complex and interesting nature of suggestions in opinionated text, and the relationship between suggestions and sentiments. Also, there is an unavailability of benchmark datasets of suggestions customer reviews.</context>
</contexts>
<marker>Wicaksono, Myaeng, 2013</marker>
<rawString>Alfan Farizki Wicaksono and Sung-Hyon Myaeng. 2013. Automatic extraction of advice-revealing sentences foradvice mining from online forums. In KCAP, pages 97–104. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xifeng Yan</author>
<author>Jiawei Han</author>
<author>Ramin Afshar</author>
</authors>
<title>Clospan: Mining closed sequential patterns in large datasets.</title>
<date>2003</date>
<booktitle>In In SDM,</booktitle>
<pages>166--177</pages>
<contexts>
<context position="21438" citStr="Yan et al., 2003" startWordPosition="3425" endWordPosition="3428">l or more than a user-defined threshold. (a1, a2, ..., aq) is a sequence, where ai is an event. A sequence (a1, a2, ..., an), is a subsequence of another sequence (b1, b2, ...bm), if there exist integers i1 &lt; i2 &lt; ... &lt; in such that a1 C_ bi1, a2 C_ bi2, ..., an C_ bin. For example, given a sequence (AB,E,ACD), where B is an item and AB, E and ACD are events. (B,AC) is a subsequence, since B C_ AB and AC C_ ACD and the order of events is preserved. (AB,E) is not a subsequence of the sequence (ABE). In our case, each POS tag is an event, and all the events are of length 1 item. We use Clospan (Yan et al., 2003) algorithm to obtain the patterns of POS tags appearing in imperative mood sentences. Closed pattern mining algorithms produce a significantly less number of sequences than the older methods, while preserving the same expressive power. We use 2-3 length sequences with a relative support greater than 0.5, where maximum relative support is 1 (see Table 4). Sequential patterns and n-grams: Sequential patterns are different from continuous ngrams in the sense that these are similar to the templates with place-holders, or skip n-grams, where the items in a pattern are ordered but should not necessa</context>
</contexts>
<marker>Yan, Han, Afshar, 2003</marker>
<rawString>Xifeng Yan, Jiawei Han, and Ramin Afshar. 2003. Clospan: Mining closed sequential patterns in large datasets. In In SDM, pages 166–177.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
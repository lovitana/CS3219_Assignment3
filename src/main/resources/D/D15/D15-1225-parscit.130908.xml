<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000760">
<title confidence="0.998081">
An Unsupervised Bayesian Modelling Approach to Storyline Detection
from News Articles
</title>
<author confidence="0.876346">
Deyu Zhoutt Haiyang Xut Yulan He§
</author>
<affiliation confidence="0.821601">
t School of Computer Science and Engineering, Southeast University, China
t State Key Laboratory for Novel Software Technology, Nanjing University, China
§ School of Engineering and Applied Science, Aston University, UK
</affiliation>
<email confidence="0.99222">
d.zhou@seu.edu.cn, h.xu@seu.edu.cn, y.he@cantab.net
</email>
<sectionHeader confidence="0.994745" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999563727272727">
Storyline detection from news articles
aims at summarizing events described un-
der a certain news topic and revealing how
those events evolve over time. It is a dif-
ficult task because it requires first the de-
tection of events from news articles pub-
lished in different time periods and then
the construction of storylines by linking
events into coherent news stories. More-
over, each storyline has different hierarchi-
cal structures which are dependent across
epochs. Existing approaches often ignore
the dependency of hierarchical structures
in storyline generation. In this paper, we
propose an unsupervised Bayesian model,
called dynamic storyline detection model,
to extract structured representations and
evolution patterns of storylines. The pro-
posed model is evaluated on a large scale
news corpus. Experimental results show
that our proposed model outperforms sev-
eral baseline approaches.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972714285715">
The rapid development of online news media sites
is accompanied by the generation of tremendous
news reports. Facing such massive amount of
news articles, it is crucial to develop an automat-
ed tool which can provide a temporal summary of
events and their evolutions related to a topic from
news reports. Therefore, storyline detection, aim-
ing at summarising the development of certain re-
lated events, has been studied in order to help read-
ers quickly understand the major events reported
in news articles. It has attracted great attention re-
cently. Kawamae (2011) proposed a trend analy-
sis model which used the difference between tem-
poral words and other words in each documen-
t to detect topic evolution over time. Ahmed et
al. (2011) proposed a unified framework to group
temporally and topically related news articles in-
to same storylines in order to reveal the tempo-
ral evolution of events. Tang and Yang (2012) de-
veloped a topic-user-trend model, which incorpo-
rates user interests into the generative process of
web contents. Radinsky and Horvitz (2013) built
storylines based on text clustering and entity en-
tropy to predict future events. Huang and Huang
(2013) developed a mixture-event-aspect model to
model sub-events into local and global aspects and
utilize an optimization method to generate story-
lines. Wang et al. (2013) proposed an evolutionary
multi-branch tree clustering method for streaming
text data in which the tree construction is casted
as an online posterior estimation problem by con-
sidering both the current tree and the previous tree
simultaneously.
With the fast development of social media plat-
forms, newsworthy events are widely scattered not
only on traditional news media but also on social
media (Zhou et al., 2015). For example, Twit-
ter, one of the most widely adopted social medi-
a platforms, appears to cover nearly all newswire
events (Petrovic et al., 2013). Therefore, ap-
proaches have also been proposed for storyline
summarization on social media. Given a user in-
put query of an ongoing event, Lin et al. (2012) ex-
tracted the storyline of an event by first obtaining
relevant tweets and then generating storylines via
graph optimization. In (Li and Li, 2013), an evo-
lutionary hierarchical Dirichlet process was pro-
posed to capture the topic evolution pattern in sto-
ryline summarization.
However, most of the aforementioned ap-
proaches do not represent events in the form of
structured representation. More importantly, they
ignore the dependency of the hierarchical struc-
tures of events at different epochs in a storyline.
In this paper, we propose a dynamic storyline de-
tection model to overcome the above limitations.
</bodyText>
<page confidence="0.87953">
1943
</page>
<note confidence="0.653406">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1943–1948,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999082181818182">
We assume that each document could belong to
one storyline s, which is modelled as a joint dis-
tribution over some named entities e and a set of
topics z. Furthermore, to link events at different
epochs and detect different types of storylines, the
weighted sum of storyline distribution of previous
epochs is employed as the prior of the current s-
toryline distribution. The proposed model is eval-
uated on a large scale news corpus. Experimental
results show that our proposed model outperforms
several baseline approaches.
</bodyText>
<sectionHeader confidence="0.991945" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.9978336">
To model the generation of a storyline in con-
secutive time periods for a stream of documents,
we propose an unsupervised latent variable mod-
el, called dynamic storyline detection model (DS-
DM), The graphical model of DSDM is shown in
</bodyText>
<figureCaption confidence="0.9803255">
Figure 1.
Figure 1: The Dynamic
</figureCaption>
<figure confidence="0.869153724137931">
Storyline Detection mod-
el.
low:
For each time period t from 1 to T:
a distribution over storylines
For each storyline s E
• Draw
atsDirichlet(γts).•
{1...S}:–
6tsDirichlet(αts).–
wtsDirichlet(ϵts).–
tscp
βs
•
–
Multinomial(irts).
– For each named entity e E {1...Ed}:
* Choose a named entity e
Multinomial(wts).
– For other word positions n E {1...Nd}:
* Choose a topic zn Multinomial(Bts).
* Choose a word wn Multinomial(Wts,z).
Draw a distribution over topics
Draw a distribution over named entities
For each topic k E {1 ... K�, draw a word distri-
bution
,k Dirichlet(
).
For each document d E {1...D}:
</figure>
<subsectionHeader confidence="0.729232">
Choose a storyline indicator std
</subsectionHeader>
<bodyText confidence="0.99538425">
We define an evolutionary matrix of storyline
indicator s and topic z, σts,z,m, where each colum-
n σts,z,m denotes storyline-topic-word distribution
of storyline indicator s and topic z at epoch m,
an evolutionary topic matrix of storyline indicator
s, τts, where each column τts,m denotes storyline-
topic distribution of storyline indicator at epoch
m, an evolutionary entity matrix of storyline in-
dicator s, υts, where each column υts,m denotes
storyline-entity distribution of storyline indicator
s.
We attach a vector of M + 1 weights
</bodyText>
<equation confidence="0.954326">
=
ff t M t E
l
s,z,m} m=0�
s,z,m &gt; �
mM=0
=
</equation>
<bodyText confidence="0.998846888888889">
with its components representing the weights that
each
contributes to calculating the priors of
We do it similarly for
and
The Dirich-
let prior for the storyline-topic-word distribution,
the storyline-topic distribution and the storyline-
entity distri
</bodyText>
<equation confidence="0.956404333333333">
µts,z
µ
µ
,
µts,z,m
1),
σts,z,m
φts,z.
θts
ωts.
bution, respectively, at epoch t are:
µts,z,m X σt (1)
s,z,m
M
m=0
µts,m X τt (2)
s,m
m=0
µts,m X υt (3)
s,m
m=0
</equation>
<bodyText confidence="0.999983421052632">
In this model, we assume that the storyline-
topic-word, storyline-topic and storyline-entity
probabilities at timet are dependent on the
previous storyline-topic-word, storyline-topic and
storyline-entity distributions in the last M epochs.
For a certain period of time, we assume that each
document could belong to one storyline s, which is
modelled as a joint distribution over some named
entities
and a set of topics z. This assumption es-
sentially encourages documents published around
similar time that involve the same named entities
and discuss similar topics to be grouped into the
same storyline. As the storyline distribution is
shared across documents with the same named en-
tities and similar topics, it essentially preserves the
ambiguity that for example, documents compris-
ing the same person and location may or may not
belong to the same story
</bodyText>
<equation confidence="0.9654696">
e
line.
The generative process of DSDM is shown be-
M
M
</equation>
<bodyText confidence="0.9935764">
In our experiments, the weight parameters are
set to be the same regardless of storylines or top-
ics. They are only dependent on the time win-
dow using an
exponential decay function, µm =
</bodyText>
<figure confidence="0.987315708333333">
6
M
µ
R
w
S K
�
µM
w e
Nd Ed
z
0 H
S
T
s
D
( S
E
M
βt s,z =
αt =
s
ϵt =
s
</figure>
<page confidence="0.962323">
1944
</page>
<bodyText confidence="0.997926411764706">
exp(−0.5×m) where m stands for the mth epoch
counting backwards in the past M epochs. That
is, more recent documents would have a relatively
stronger influence on the model parameters in the
current epoch compared to earlier documents. It is
also possible to estimate the weights directly from
data. We leave it as our future work.
The storyline-topic-word distribution cpts,z, the
storyline-topic distribution Bts and the storyline-
entity distribution wt s at the current epoch t are
generated from the Dirichlet distribution param-
eterized by Qts,z, αts, cts, cpts,z ∼ Dir(Qts,z), cpts,k ∼
Dir(αts),wts ∼ Dir(cts). With this formulation,
we can ensure that the mean of the Dirichlet pa-
rameter for the current epoch becomes proportion-
al to the weighted sum of the word, topic distribu-
tion, and entity distribution at previous epochs.
</bodyText>
<sectionHeader confidence="0.99769" genericHeader="method">
3 Inference and Parameter Estimation
</sectionHeader>
<bodyText confidence="0.990952428571429">
We use collapsed Gibbs sampling (Griffiths and
Steyvers, 2004) to infer the parameters of the mod-
el, given observed data D. Gibbs sampling is a
Markov chain Monte Carlo method which allows
us repeatedly sample from a Markov chain whose
stationary distribution is the posterior of interest,
st
d and ztd,n here, from the distribution over that
variable given the current values of all other vari-
ables and the data. Such samples can be used to
empirically estimate the target distribution. Let-
ting the subscript −d denote the quantity that ex-
cludes counts in document d, the conditional pos-
terior for sd is:
</bodyText>
<equation confidence="0.9927011875">
P(std = j|st−d, z, w, A) ∝ {Nj}−d + γ
D−d + Sγ
77�77 (d)
11b3�1 Nj,e − b + cje
7�7n(d)
l ib 1 nj,k − b + αt
j,k
j,k
(d) K
Hnbj 1 nj − b + Ek=1 αtj,k
7 7
llbjjd)
1vnj,k,v − b + Q�k,v
,
7�7n(d)
l ib 1 nj,k − b + EV, 1 Qtj,k,v
</equation>
<bodyText confidence="0.9984367">
where Nj denotes the number of documents as-
signed to storyline indicator j in the whole corpus,
D is the total number of documents, nj,e is the
number of times named entity e is assigned with
storyline indicator j, nEj denotes the total number
of named entities with storyline indicator j in the
document collection, nj,k is the number of times
words with topic label k with storyline indicator j,
nj is the total number of words (excluding named
entities) in the corpus with storyline indicator j,
nj,k,v is the number of words v with storyline in-
dicator j and topic label k in the document col-
lection, counts with (d) notation denote the counts
relating to document d only.
Letting the index x = (d, n) denote nth word in
document d and the subscript −x denote a quantity
that excludes data from the nth word position in
document d. We only sample a topic zx if the nth
word is not a named entity based on the following
conditional posterior:
</bodyText>
<equation confidence="0.999356">
P �zt x � k�sd � j, ��
{ntj,k}−x + αtj,k
a , at k x
{nj}−x + ∑K {nt ,k}−x + ∑Vv= 1 βt j,k,v
</equation>
<bodyText confidence="0.960716416666667">
Once the latent variables s and z are known,
we can easily estimate the model parameters
π, O, cp, ψ, w. We set the hyperparameters α =
γ = 0.1, Q = c = 0.01 for the current epoch
(i.e., m = 0), and gather statistics in the previous
7 epochs (i.e., M = 7) to set the Dirichlet priors
for the storyline-topic-word distribution cpts,z, the
storyline-topic distribution Bts and the storyline-
entity distribution wt s in the current epoch t, and
run Gibbs sampler for 1000 iterations and stop the
iteration once the log-likelihood of the training da-
ta converges under the learned model.
</bodyText>
<sectionHeader confidence="0.999445" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.94958">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999974461538462">
We crawled and parsed the GDELT Even-
t Databasel containing news articles published in
May 2014. We manually annotated one-week da-
ta containing 101,654 documents and identified 77
storylines for evaluation. We also report the result-
s of our model on the one-month data containing
526,587 documents. But we only report the preci-
sion and not recall of the storylines extracted since
it is time consuming to identify all the true story-
lines in such a large dataset. In our experiments,
we used the Stanford Named Entity Recognizer
for identifying the named entities. In addition, we
removed common stopwords and only kept tokens
</bodyText>
<equation confidence="0.9742830625">
lhttp://data.gdeltproject.org/events/
index.html
1 nEj − b + EE e=1 ctj,e
nE(d)
Hb
E
× H
e=1
K
× H
k=1
V
× H
v=1
{t /�t
nj,k,wn }−x + Nj,k,v
</equation>
<page confidence="0.949989">
1945
</page>
<bodyText confidence="0.997256">
which are verbs, nouns, or adjectives in these news
articles.
</bodyText>
<subsectionHeader confidence="0.994194">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.9990255">
We chose the following three methods as the base-
line approaches.
</bodyText>
<listItem confidence="0.968167095238095">
1. K-Means + Cosine Similarity (KMCS): the
method first applies K-Means to cluster news
documents for each day, then link storylines
detected in different days based on the cosine
similarity measurement.
2. LDA + Cosine Similarity (LDCS): the
method first splits news documents on a daily
basis, then applies the Latent Dirichlet Allo-
cation (LDA) model to detect the latent story-
lines for the documents in each day, in which
each storyline is modelled as a joint distribu-
tion over named entities and words, and final-
ly links storylines detected in different days
using the cosine similarity measurement.
3. Dynamic LDA (DLDA)2: this is the dynam-
ic LDA (Blei and Lafferty, 2006) where the
topic-word distributions are linked across e-
pochs based on the Markovian assumption.
That is, the topic-word distribution at the cur-
rent epoch is only influenced by the topic-
word distribution in the previous epoch.
</listItem>
<subsectionHeader confidence="0.998888">
4.3 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.9999378">
To evaluate the performance of the proposed ap-
proach, we use precision, recall and F-score which
are commonly used in evaluating information ex-
traction systems. The precision is calculated based
on the following criteria: 1) The entities and key-
words extracted refer to the same storyline. 2) The
duration of the storyline is correct. We assume that
the start date (or end date) of a storyline is the pub-
lication date of the first (or last) news article about
it.
</bodyText>
<subsectionHeader confidence="0.999259">
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99991475">
The proposed model is compared against the base-
line approaches on the annotated one-week da-
ta which consist of 77 storylines. The number
of storylines, 5, and the number of topics, K,
are both set to 100. The number of historical e-
pochs, M, which is taken into account for set-
ting the Dirichlet priors for the storyline-topic-
word, the storyline-topic and the storyline-entity
</bodyText>
<footnote confidence="0.9947305">
2Topic number is set to 100 for both DLDA and LDCS.
Cluster number is also set to 100 for KMCS.
</footnote>
<bodyText confidence="0.986928666666667">
distributions, is set to 7. The evaluation results of
our proposed approach in comparison to the three
baselines are presented in Table 1.
</bodyText>
<table confidence="0.999447">
Method Precision Recall F-score
KMCS 22.73 32.47 26.74
LDCS 34.29 31.17 32.66
DLDA 62.67 61.03 61.84
DSDM 70.67 68.80 69.27
</table>
<tableCaption confidence="0.713196333333333">
Table 1: Performance comparison of the storyline
extraction results in terms of Precision (%), Recall
(%) and F-score (%).
</tableCaption>
<bodyText confidence="0.999366133333333">
It can be observed from Table 1 that simply
using K-means to cluster news articles in each
day and linking similar stories across differen-
t days in hoping of identifying storylines gives
the worst results. Using LDA to detect stories
in each day improves the precision dramatically.
The dynamic LDA model assumes topics (or sto-
ries) in the current epoch evolves from the previ-
ous epoch and further improves the storyline de-
tection results significantly. Our proposed mod-
el aims to capture the long distance dependen-
cies in which the statistics gathered in the past 7
days are taken into account to set the Dirichlet pri-
ors of the storyline-topic-word, storyline-topic and
storyline-entity distributions in the current epoch.
It gives the best performance and outperforms dy-
namic LDA by nearly 7% in F-measure.
To study the impact of the number of topics on
the performance of the proposed model, we con-
ducted experiments on the one-month data with d-
ifferent number of topics varying between 100 and
200. In all these experiments, the number of story-
lines, 5, is set to 200, based on the speculation that
about 40 storylines in the annotated one-week data
last for one month and about 40 new storylines oc-
cur each week. Table 2 shows the precision of the
proposed method under different number of topic-
s. It can be observed that the performance of the
proposed approach is quite stable across different
number of topics.
</bodyText>
<table confidence="0.997642">
K 100 150 200
Precision 69.6% 70.2% 69.9%
</table>
<tableCaption confidence="0.9888765">
Table 2: The precision of our method with various
number (K) of topics.
</tableCaption>
<page confidence="0.990796">
1946
</page>
<figureCaption confidence="0.99811">
Figure 2: Storyline about the patent infringement case between Apple and Samsung was extracted by the
proposed Model.
</figureCaption>
<table confidence="0.912319227272727">
Entity: Topic: Entity: Topic: Entity: Topic:
Samsung Patent Apple Patent Apple Patent
Apple Infringe Samsung Jury Samsung Jury
Google Verdict Silicon Company Google Company
U.S. Jury Valley Verdict U.S. Device
San Jose Damage Google Device Lenovo Update
California Allegiance Mr Farage Amount California Wearable
Company San Jose Factory Video
Entity: Topic: Entity: Topic: Entity: Topic:
Samsung Patent Apple Patent Samsung Patent
Apple Trial Samsung Award Apple Infringe
Google Jury Google Jury Google Company
U.S. Report U.S. Verdict Amazon Win
Mr Farage User San Jose Phone U.S. Jury
Official Mr Farage Appeal Steve Jobs Verdict
Supply Trial Agreement
May 6
May 2
May 4
May 1
May 3
May 5
</table>
<subsectionHeader confidence="0.94981">
4.5 Structured Browsing
</subsectionHeader>
<bodyText confidence="0.99988993939394">
We illustrate the evolution of storylines by using
structured browsing, from which the structured in-
formation (entity, topic, keywords) about story-
lines and the duration of storylines can be easily
observed. Figure 2 shows the storyline about “The
patent infringement case between Apple and
Samsung”. It can be observed that in the first two
days, the hierarchical structure consists of entities
(Apple, Samsung) and keywords (trial, patent, in-
fringe). The case has gained significant attention
in the next three days when US jury orders Sam-
sung to pay Apple $119.6 million. It can be ob-
served that the stories in the next three days also
consist of entities (Apple, Samsung), but with d-
ifferent keywords (award, patent, win). The last
day’s story gives an overall summary and consists
of entities (Apple, Samsung) and keywords (jury,
patent, company).
To further investigate the storylines detected by
the proposed model, we randomly selected three
detected storylines. The first one is about “the
patent infringement case between Apple and
Samsung”. It is a short-term storyline lasting for
6 day as shown in Figure 3. The second one is
about “India election”, which is a long-term sto-
ryline lasting for one month. The third one is about
“Pistorius shoot Steenkamp”, which is an inter-
mittent storyline, lasting for a total of 22 days but
with no relevant news reports in certain days as
shown in Figure 3. It can be observed that the pro-
posed model can detect not only continuous but
also intermittent storylines, which further demon-
strates the advantage of the proposed model.
</bodyText>
<figure confidence="0.385956">
Date of storylines
</figure>
<figureCaption confidence="0.9702695">
Figure 3: The number of documents on each day
relating to the three storylines.
</figureCaption>
<sectionHeader confidence="0.995244" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999927153846154">
In this paper, we have proposed an unsupervised
Bayesian model to extract storylines from news
corpus. Experimental results show that our pro-
posed model is able to extract both continuous and
intermittent storylines and outperforms a number
of baselines. In future work, we will consider
modelling background topics explicitly and inves-
tigating more principled ways in setting the weight
parameters of the statistics gathered in the histor-
ical epochs. Moreover, we will also explore the
impact of different scale of the dependencies from
historical epochs on the distributions of the current
epoch.
</bodyText>
<sectionHeader confidence="0.998024" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9989204">
This work was funded by the National Natural Sci-
ence Foundation of China (61528302), State Edu-
cation Ministry, the Fundamental Research Funds
for the Central Universities, and the Innovate UK
under the grant number 101779.
</bodyText>
<figure confidence="0.984087857142857">
400
Apple VS Samsung
India Election
Pistorius shoot Steenkamp
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Number of documents related to the storyline
350
300
250
200
150
100
50
</figure>
<page confidence="0.969991">
1947
</page>
<sectionHeader confidence="0.990509" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999882183333333">
Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing,
Alexander J Smola, and Choon Hui Teo. 2011. U-
nified analysis of streaming news. In Proceedings
of the 20th international conference on World wide
web, pages 267–276. ACM.
David M Blei and John D Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd interna-
tional conference on Machine learning, pages 113–
120. ACM.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Sciences 101 (Suppl. 1), pages 5228–
5235.
Lifu Huang and Lian’en Huang. 2013. Optimized
event storyline generation based on mixture-event-
aspect model. In Proceedings of the 2013 Confer-
ence on Empirical Methods on Natural Language
Processing, pages 726–735. ACL.
Noriaki Kawamae. 2011. Trend analysis model: trend
consists of temporal words, topics, and timestamp-
s. In Proceedings of the fourth ACM international
conference on Web search and data mining, pages
317–326. ACM.
Jiwei Li and Sujian Li. 2013. Evolutionary hierarchi-
cal dirichlet process for timeline summarization. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 556–
560. ACL.
Chen Lin, Chun Lin, Jingxuan Li, Dingding Wang,
Yang Chen, and Tao Li. 2012. Generating even-
t storylines from microblogs. In Proceedings of the
21st ACM international conference on Information
and knowledge management, pages 175–184. ACM.
Saˇsa Petrovic, Miles Osborne, Richard McCreadie,
Craig Macdonald, Iadh Ounis, and Luke Shrimpton.
2013. Can twitter replace newswire for breaking
news? In Proceedings of the 7th International AAAI
Conference on Weblogs and Social Media.
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings of the
sixth ACM international conference on Web search
and data mining, pages 255–264. ACM.
Xuning Tang and Christopher C Yang. 2012. TUT: a
statistical model for detecting trends, topics and user
interests in social media. In Proceedings of the 21st
ACM international conference on Information and
knowledge management, pages 972–981. ACM.
Xiting Wang, Shixia Liu, Yangqiu Song, and Bain-
ing Guo. 2013. Mining evolutionary multi-branch
trees from text streams. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 722–730.
ACM.
Deyu Zhou, Liangyu Chen, and Yulan He. 2015.
An unsupervised framework of exploring events
on twitter: Filtering, extraction and categorization.
In Proceedings of the Twenty-Ninth AAAI Confer-
ence on Artificial Intelligence (AAAI 2015) , Austin,
Texas, USA, January 25 C 30, 2015, pages 2468–
2474.
</reference>
<page confidence="0.995207">
1948
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732661">
<title confidence="0.992745">An Unsupervised Bayesian Modelling Approach to Storyline</title>
<author confidence="0.810155">from News Articles</author>
<affiliation confidence="0.985176333333333">of Computer Science and Engineering, Southeast University, Key Laboratory for Novel Software Technology, Nanjing University, of Engineering and Applied Science, Aston University,</affiliation>
<email confidence="0.979176">d.zhou@seu.edu.cn,h.xu@seu.edu.cn,y.he@cantab.net</email>
<abstract confidence="0.99863352173913">Storyline detection from news articles aims at summarizing events described under a certain news topic and revealing how those events evolve over time. It is a difficult task because it requires first the detection of events from news articles published in different time periods and then the construction of storylines by linking events into coherent news stories. Moreover, each storyline has different hierarchical structures which are dependent across epochs. Existing approaches often ignore the dependency of hierarchical structures in storyline generation. In this paper, we propose an unsupervised Bayesian model, called dynamic storyline detection model, to extract structured representations and evolution patterns of storylines. The proposed model is evaluated on a large scale news corpus. Experimental results show that our proposed model outperforms several baseline approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Qirong Ho</author>
<author>Jacob Eisenstein</author>
<author>Eric Xing</author>
<author>Alexander J Smola</author>
<author>Choon Hui Teo</author>
</authors>
<title>Unified analysis of streaming news.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web,</booktitle>
<pages>267--276</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2043" citStr="Ahmed et al. (2011)" startWordPosition="307" endWordPosition="310">g such massive amount of news articles, it is crucial to develop an automated tool which can provide a temporal summary of events and their evolutions related to a topic from news reports. Therefore, storyline detection, aiming at summarising the development of certain related events, has been studied in order to help readers quickly understand the major events reported in news articles. It has attracted great attention recently. Kawamae (2011) proposed a trend analysis model which used the difference between temporal words and other words in each document to detect topic evolution over time. Ahmed et al. (2011) proposed a unified framework to group temporally and topically related news articles into same storylines in order to reveal the temporal evolution of events. Tang and Yang (2012) developed a topic-user-trend model, which incorporates user interests into the generative process of web contents. Radinsky and Horvitz (2013) built storylines based on text clustering and entity entropy to predict future events. Huang and Huang (2013) developed a mixture-event-aspect model to model sub-events into local and global aspects and utilize an optimization method to generate storylines. Wang et al. (2013)</context>
</contexts>
<marker>Ahmed, Ho, Eisenstein, Xing, Smola, Teo, 2011</marker>
<rawString>Amr Ahmed, Qirong Ho, Jacob Eisenstein, Eric Xing, Alexander J Smola, and Choon Hui Teo. 2011. Unified analysis of streaming news. In Proceedings of the 20th international conference on World wide web, pages 267–276. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Dynamic topic models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning,</booktitle>
<pages>113--120</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12710" citStr="Blei and Lafferty, 2006" startWordPosition="2140" endWordPosition="2143">st applies K-Means to cluster news documents for each day, then link storylines detected in different days based on the cosine similarity measurement. 2. LDA + Cosine Similarity (LDCS): the method first splits news documents on a daily basis, then applies the Latent Dirichlet Allocation (LDA) model to detect the latent storylines for the documents in each day, in which each storyline is modelled as a joint distribution over named entities and words, and finally links storylines detected in different days using the cosine similarity measurement. 3. Dynamic LDA (DLDA)2: this is the dynamic LDA (Blei and Lafferty, 2006) where the topic-word distributions are linked across epochs based on the Markovian assumption. That is, the topic-word distribution at the current epoch is only influenced by the topicword distribution in the previous epoch. 4.3 Evaluation Metric To evaluate the performance of the proposed approach, we use precision, recall and F-score which are commonly used in evaluating information extraction systems. The precision is calculated based on the following criteria: 1) The entities and keywords extracted refer to the same storyline. 2) The duration of the storyline is correct. We assume that th</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>David M Blei and John D Lafferty. 2006. Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning, pages 113– 120. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences</booktitle>
<volume>101</volume>
<pages>5228--5235</pages>
<contexts>
<context position="8708" citStr="Griffiths and Steyvers, 2004" startWordPosition="1408" endWordPosition="1411">leave it as our future work. The storyline-topic-word distribution cpts,z, the storyline-topic distribution Bts and the storylineentity distribution wt s at the current epoch t are generated from the Dirichlet distribution parameterized by Qts,z, αts, cts, cpts,z ∼ Dir(Qts,z), cpts,k ∼ Dir(αts),wts ∼ Dir(cts). With this formulation, we can ensure that the mean of the Dirichlet parameter for the current epoch becomes proportional to the weighted sum of the word, topic distribution, and entity distribution at previous epochs. 3 Inference and Parameter Estimation We use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) to infer the parameters of the model, given observed data D. Gibbs sampling is a Markov chain Monte Carlo method which allows us repeatedly sample from a Markov chain whose stationary distribution is the posterior of interest, st d and ztd,n here, from the distribution over that variable given the current values of all other variables and the data. Such samples can be used to empirically estimate the target distribution. Letting the subscript −d denote the quantity that excludes counts in document d, the conditional posterior for sd is: P(std = j|st−d, z, w, A) ∝ {Nj}−d + γ D−d + Sγ 77�77 (d)</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy of Sciences 101 (Suppl. 1), pages 5228– 5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lifu Huang</author>
<author>Lian’en Huang</author>
</authors>
<title>Optimized event storyline generation based on mixture-eventaspect model.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>726--735</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2476" citStr="Huang and Huang (2013)" startWordPosition="376" endWordPosition="379">ly. Kawamae (2011) proposed a trend analysis model which used the difference between temporal words and other words in each document to detect topic evolution over time. Ahmed et al. (2011) proposed a unified framework to group temporally and topically related news articles into same storylines in order to reveal the temporal evolution of events. Tang and Yang (2012) developed a topic-user-trend model, which incorporates user interests into the generative process of web contents. Radinsky and Horvitz (2013) built storylines based on text clustering and entity entropy to predict future events. Huang and Huang (2013) developed a mixture-event-aspect model to model sub-events into local and global aspects and utilize an optimization method to generate storylines. Wang et al. (2013) proposed an evolutionary multi-branch tree clustering method for streaming text data in which the tree construction is casted as an online posterior estimation problem by considering both the current tree and the previous tree simultaneously. With the fast development of social media platforms, newsworthy events are widely scattered not only on traditional news media but also on social media (Zhou et al., 2015). For example, Twi</context>
</contexts>
<marker>Huang, Huang, 2013</marker>
<rawString>Lifu Huang and Lian’en Huang. 2013. Optimized event storyline generation based on mixture-eventaspect model. In Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing, pages 726–735. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriaki Kawamae</author>
</authors>
<title>Trend analysis model: trend consists of temporal words, topics, and timestamps.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining,</booktitle>
<pages>317--326</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1872" citStr="Kawamae (2011)" startWordPosition="278" endWordPosition="279">rforms several baseline approaches. 1 Introduction The rapid development of online news media sites is accompanied by the generation of tremendous news reports. Facing such massive amount of news articles, it is crucial to develop an automated tool which can provide a temporal summary of events and their evolutions related to a topic from news reports. Therefore, storyline detection, aiming at summarising the development of certain related events, has been studied in order to help readers quickly understand the major events reported in news articles. It has attracted great attention recently. Kawamae (2011) proposed a trend analysis model which used the difference between temporal words and other words in each document to detect topic evolution over time. Ahmed et al. (2011) proposed a unified framework to group temporally and topically related news articles into same storylines in order to reveal the temporal evolution of events. Tang and Yang (2012) developed a topic-user-trend model, which incorporates user interests into the generative process of web contents. Radinsky and Horvitz (2013) built storylines based on text clustering and entity entropy to predict future events. Huang and Huang (2</context>
</contexts>
<marker>Kawamae, 2011</marker>
<rawString>Noriaki Kawamae. 2011. Trend analysis model: trend consists of temporal words, topics, and timestamps. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 317–326. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Sujian Li</author>
</authors>
<title>Evolutionary hierarchical dirichlet process for timeline summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>556--560</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3507" citStr="Li and Li, 2013" startWordPosition="542" endWordPosition="545"> the fast development of social media platforms, newsworthy events are widely scattered not only on traditional news media but also on social media (Zhou et al., 2015). For example, Twitter, one of the most widely adopted social media platforms, appears to cover nearly all newswire events (Petrovic et al., 2013). Therefore, approaches have also been proposed for storyline summarization on social media. Given a user input query of an ongoing event, Lin et al. (2012) extracted the storyline of an event by first obtaining relevant tweets and then generating storylines via graph optimization. In (Li and Li, 2013), an evolutionary hierarchical Dirichlet process was proposed to capture the topic evolution pattern in storyline summarization. However, most of the aforementioned approaches do not represent events in the form of structured representation. More importantly, they ignore the dependency of the hierarchical structures of events at different epochs in a storyline. In this paper, we propose a dynamic storyline detection model to overcome the above limitations. 1943 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1943–1948, Lisbon, Portugal, 17-21 Septe</context>
</contexts>
<marker>Li, Li, 2013</marker>
<rawString>Jiwei Li and Sujian Li. 2013. Evolutionary hierarchical dirichlet process for timeline summarization. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 556– 560. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Lin</author>
<author>Chun Lin</author>
<author>Jingxuan Li</author>
<author>Dingding Wang</author>
<author>Yang Chen</author>
<author>Tao Li</author>
</authors>
<title>Generating event storylines from microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>175--184</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3360" citStr="Lin et al. (2012)" startWordPosition="518" endWordPosition="521">ree construction is casted as an online posterior estimation problem by considering both the current tree and the previous tree simultaneously. With the fast development of social media platforms, newsworthy events are widely scattered not only on traditional news media but also on social media (Zhou et al., 2015). For example, Twitter, one of the most widely adopted social media platforms, appears to cover nearly all newswire events (Petrovic et al., 2013). Therefore, approaches have also been proposed for storyline summarization on social media. Given a user input query of an ongoing event, Lin et al. (2012) extracted the storyline of an event by first obtaining relevant tweets and then generating storylines via graph optimization. In (Li and Li, 2013), an evolutionary hierarchical Dirichlet process was proposed to capture the topic evolution pattern in storyline summarization. However, most of the aforementioned approaches do not represent events in the form of structured representation. More importantly, they ignore the dependency of the hierarchical structures of events at different epochs in a storyline. In this paper, we propose a dynamic storyline detection model to overcome the above limit</context>
</contexts>
<marker>Lin, Lin, Li, Wang, Chen, Li, 2012</marker>
<rawString>Chen Lin, Chun Lin, Jingxuan Li, Dingding Wang, Yang Chen, and Tao Li. 2012. Generating event storylines from microblogs. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 175–184. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Petrovic</author>
<author>Miles Osborne</author>
<author>Richard McCreadie</author>
<author>Craig Macdonald</author>
<author>Iadh Ounis</author>
<author>Luke Shrimpton</author>
</authors>
<title>Can twitter replace newswire for breaking news?</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="3204" citStr="Petrovic et al., 2013" startWordPosition="491" endWordPosition="494">ptimization method to generate storylines. Wang et al. (2013) proposed an evolutionary multi-branch tree clustering method for streaming text data in which the tree construction is casted as an online posterior estimation problem by considering both the current tree and the previous tree simultaneously. With the fast development of social media platforms, newsworthy events are widely scattered not only on traditional news media but also on social media (Zhou et al., 2015). For example, Twitter, one of the most widely adopted social media platforms, appears to cover nearly all newswire events (Petrovic et al., 2013). Therefore, approaches have also been proposed for storyline summarization on social media. Given a user input query of an ongoing event, Lin et al. (2012) extracted the storyline of an event by first obtaining relevant tweets and then generating storylines via graph optimization. In (Li and Li, 2013), an evolutionary hierarchical Dirichlet process was proposed to capture the topic evolution pattern in storyline summarization. However, most of the aforementioned approaches do not represent events in the form of structured representation. More importantly, they ignore the dependency of the hie</context>
</contexts>
<marker>Petrovic, Osborne, McCreadie, Macdonald, Ounis, Shrimpton, 2013</marker>
<rawString>Saˇsa Petrovic, Miles Osborne, Richard McCreadie, Craig Macdonald, Iadh Ounis, and Luke Shrimpton. 2013. Can twitter replace newswire for breaking news? In Proceedings of the 7th International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eric Horvitz</author>
</authors>
<title>Mining the web to predict future events.</title>
<date>2013</date>
<booktitle>In Proceedings of the sixth ACM international conference on Web search and data mining,</booktitle>
<pages>255--264</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2366" citStr="Radinsky and Horvitz (2013)" startWordPosition="358" endWordPosition="361">help readers quickly understand the major events reported in news articles. It has attracted great attention recently. Kawamae (2011) proposed a trend analysis model which used the difference between temporal words and other words in each document to detect topic evolution over time. Ahmed et al. (2011) proposed a unified framework to group temporally and topically related news articles into same storylines in order to reveal the temporal evolution of events. Tang and Yang (2012) developed a topic-user-trend model, which incorporates user interests into the generative process of web contents. Radinsky and Horvitz (2013) built storylines based on text clustering and entity entropy to predict future events. Huang and Huang (2013) developed a mixture-event-aspect model to model sub-events into local and global aspects and utilize an optimization method to generate storylines. Wang et al. (2013) proposed an evolutionary multi-branch tree clustering method for streaming text data in which the tree construction is casted as an online posterior estimation problem by considering both the current tree and the previous tree simultaneously. With the fast development of social media platforms, newsworthy events are wide</context>
</contexts>
<marker>Radinsky, Horvitz, 2013</marker>
<rawString>Kira Radinsky and Eric Horvitz. 2013. Mining the web to predict future events. In Proceedings of the sixth ACM international conference on Web search and data mining, pages 255–264. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuning Tang</author>
<author>Christopher C Yang</author>
</authors>
<title>TUT: a statistical model for detecting trends, topics and user interests in social media.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM international conference on Information and knowledge management,</booktitle>
<pages>972--981</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2223" citStr="Tang and Yang (2012)" startWordPosition="337" endWordPosition="340"> reports. Therefore, storyline detection, aiming at summarising the development of certain related events, has been studied in order to help readers quickly understand the major events reported in news articles. It has attracted great attention recently. Kawamae (2011) proposed a trend analysis model which used the difference between temporal words and other words in each document to detect topic evolution over time. Ahmed et al. (2011) proposed a unified framework to group temporally and topically related news articles into same storylines in order to reveal the temporal evolution of events. Tang and Yang (2012) developed a topic-user-trend model, which incorporates user interests into the generative process of web contents. Radinsky and Horvitz (2013) built storylines based on text clustering and entity entropy to predict future events. Huang and Huang (2013) developed a mixture-event-aspect model to model sub-events into local and global aspects and utilize an optimization method to generate storylines. Wang et al. (2013) proposed an evolutionary multi-branch tree clustering method for streaming text data in which the tree construction is casted as an online posterior estimation problem by consider</context>
</contexts>
<marker>Tang, Yang, 2012</marker>
<rawString>Xuning Tang and Christopher C Yang. 2012. TUT: a statistical model for detecting trends, topics and user interests in social media. In Proceedings of the 21st ACM international conference on Information and knowledge management, pages 972–981. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiting Wang</author>
<author>Shixia Liu</author>
<author>Yangqiu Song</author>
<author>Baining Guo</author>
</authors>
<title>Mining evolutionary multi-branch trees from text streams.</title>
<date>2013</date>
<booktitle>In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>722--730</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2643" citStr="Wang et al. (2013)" startWordPosition="401" endWordPosition="404">hmed et al. (2011) proposed a unified framework to group temporally and topically related news articles into same storylines in order to reveal the temporal evolution of events. Tang and Yang (2012) developed a topic-user-trend model, which incorporates user interests into the generative process of web contents. Radinsky and Horvitz (2013) built storylines based on text clustering and entity entropy to predict future events. Huang and Huang (2013) developed a mixture-event-aspect model to model sub-events into local and global aspects and utilize an optimization method to generate storylines. Wang et al. (2013) proposed an evolutionary multi-branch tree clustering method for streaming text data in which the tree construction is casted as an online posterior estimation problem by considering both the current tree and the previous tree simultaneously. With the fast development of social media platforms, newsworthy events are widely scattered not only on traditional news media but also on social media (Zhou et al., 2015). For example, Twitter, one of the most widely adopted social media platforms, appears to cover nearly all newswire events (Petrovic et al., 2013). Therefore, approaches have also been </context>
</contexts>
<marker>Wang, Liu, Song, Guo, 2013</marker>
<rawString>Xiting Wang, Shixia Liu, Yangqiu Song, and Baining Guo. 2013. Mining evolutionary multi-branch trees from text streams. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 722–730. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyu Zhou</author>
<author>Liangyu Chen</author>
<author>Yulan He</author>
</authors>
<title>An unsupervised framework of exploring events on twitter: Filtering, extraction and categorization.</title>
<date>2015</date>
<journal>C</journal>
<booktitle>In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2015) ,</booktitle>
<volume>30</volume>
<pages>2468--2474</pages>
<location>Austin, Texas, USA,</location>
<contexts>
<context position="3058" citStr="Zhou et al., 2015" startWordPosition="466" endWordPosition="469">uture events. Huang and Huang (2013) developed a mixture-event-aspect model to model sub-events into local and global aspects and utilize an optimization method to generate storylines. Wang et al. (2013) proposed an evolutionary multi-branch tree clustering method for streaming text data in which the tree construction is casted as an online posterior estimation problem by considering both the current tree and the previous tree simultaneously. With the fast development of social media platforms, newsworthy events are widely scattered not only on traditional news media but also on social media (Zhou et al., 2015). For example, Twitter, one of the most widely adopted social media platforms, appears to cover nearly all newswire events (Petrovic et al., 2013). Therefore, approaches have also been proposed for storyline summarization on social media. Given a user input query of an ongoing event, Lin et al. (2012) extracted the storyline of an event by first obtaining relevant tweets and then generating storylines via graph optimization. In (Li and Li, 2013), an evolutionary hierarchical Dirichlet process was proposed to capture the topic evolution pattern in storyline summarization. However, most of the a</context>
</contexts>
<marker>Zhou, Chen, He, 2015</marker>
<rawString>Deyu Zhou, Liangyu Chen, and Yulan He. 2015. An unsupervised framework of exploring events on twitter: Filtering, extraction and categorization. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2015) , Austin, Texas, USA, January 25 C 30, 2015, pages 2468– 2474.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000067">
<title confidence="0.990401">
ReVal: A Simple and Effective Machine Translation Evaluation Metric
Based on Recurrent Neural Networks
</title>
<author confidence="0.996039">
Rohit Gupta1, Constantin Or˘asan1, Josef van Genabith2
</author>
<affiliation confidence="0.957557">
&apos;Research Group in Computational Linguistics, University of Wolverhampton, UK
2Saarland University and German Research Center for Artificial Intelligence (DFKI), Germany
</affiliation>
<email confidence="0.972964">
{r.gupta, c.orasan}@wlv.ac.uk
josef.van genabith@dfki.de
</email>
<sectionHeader confidence="0.993731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998034375">
Many state-of-the-art Machine Translation
(MT) evaluation metrics are complex, in-
volve extensive external resources (e.g.
for paraphrasing) and require tuning to
achieve best results. We present a simple
alternative approach based on dense vec-
tor spaces and recurrent neural networks
(RNNs), in particular Long Short Term
Memory (LSTM) networks. For WMT-14,
our new metric scores best for two out of
five language pairs, and overall best and
second best on all language pairs, using
Spearman and Pearson correlation, respec-
tively. We also show how training data is
computed automatically from WMT ranks
data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999810054054054">
Deep learning approaches have turned out to be
successful in many NLP applications such as para-
phrasing (Mikolov et al., 2013b; Socher et al.,
2011), sentiment analysis (Socher et al., 2013b),
parsing (Socher et al., 2013a) and machine trans-
lation (Mikolov et al., 2013a). While dense vec-
tor space representations such as those obtained
through Deep Neural Networks (DNNs) or RNNs
are able to capture semantic similarity for words
(Mikolov et al., 2013b), segments (Socher et al.,
2011) and documents (Le and Mikolov, 2014)
naturally, traditional MT evaluation metrics can
only achieve this using resources like WordNet
and paraphrase databases. This paper presents a
novel, efficient and compact MT evaluation mea-
sure based on RNNs. Our metric is simple in the
sense that it does not require much machinery and
resources apart from the dense word vectors. This
cannot be said of most of the state-of-the-art MT
evaluation metrics, which tend to be complex and
require extensive feature engineering. Our metric
is based on RNNs and particularly on Tree Long
Short Term Memory (Tree-LSTM) networks (Tai
et al., 2015). LSTM (Hochreiter and Schmidhu-
ber, 1997) is a sequence learning technique which
uses a memory cell to preserve a state over a long
period of time. This enables distributed represen-
tations of sentences using distributed representa-
tions of words. Tree-LSTM is a recent approach,
which is an extension of the simple LSTM frame-
work (Zaremba and Sutskever, 2014). To provide
the required training data, we also show how to
automatically convert the WMT-13 (Bojar et al.,
2013) human evaluation rankings into similarity
scores between the reference and the translation.
Our metric including training data is available at
https://github.com/rohitguptacs/ReVal.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999911">
Many metrics have been proposed for MT eval-
uation. Earlier popular metrics are based on n-
gram counts (e.g. BLEU (Papineni et al., 2002)
and NIST (Doddington, 2002)) or word error rate.
Other popular metrics like METEOR (Denkowski
and Lavie, 2014) and TERp (Snover et al., 2008)
also use external resources like WordNet and para-
phrase databases. However, system-level cor-
relation with human judgements for these met-
rics remains below 0.90 Pearson correlation co-
efficient (as per WMT-14 results, BLEU-0.888,
NIST-0.867, METEOR-0.829, TER-0.826, WER-
0.821).
Recent best-performing metrics in the WMT-14
metric shared task (Mach´acek and Bojar, 2014)
used a combination of different metrics. The
top performing system DISKOTK-PARTY-TUNED
(Joty et al., 2014) in the WMT-14 task uses five
different discourse metrics and twelve different
metrics from the ASIYA MT evaluation toolkit
(Gim´enez and M`arquez, 2010). The metric com-
putes the number of common sub-trees between
a reference and a translation using a convolution
</bodyText>
<page confidence="0.887973">
1066
</page>
<note confidence="0.647349">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1066–1072,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999853333333333">
tree kernel (Collins and Duffy, 2001). The basic
version of the metric does not perform well but
in combination with the other 12 metrics from
the ASIYA toolkit obtained the best results for
the WMT-14 metric shared task. Another top
performing metric LAYERED (Gautam and Bhat-
tacharyya, 2014), uses linear interpolation of dif-
ferent metrics. LAYERED uses BLEU and TER
to capture lexical similarity, Hamming score and
Kendall Tau Distance (Birch and Osborne, 2011)
to identify syntactic similarity, and dependency
parsing (De Marneffe et al., 2006) and the Univer-
sal Networking Language1 for semantic similarity.
Recently, Guzm´an et al. (2015) presented a metric
based on word embeddings and neural networks.
However, this metric is limited to ranking the
available systems and does not provide an absolute
score.
In this paper we propose a compact MT eval-
uation metric. We hypothesize that our model
learns different notions of similarity (which other
metrics tend to capture using different metrics)
using input, output and forget gates of an LSTM
architecture.
</bodyText>
<sectionHeader confidence="0.945365" genericHeader="method">
3 LSTMs and Tree-LSTMs
</sectionHeader>
<bodyText confidence="0.999348458333334">
Recurrent Neural Networks allow processing of
arbitrary length sequences, but early RNNs had
the problem of vanishing and exploding gradi-
ents (Bengio et al., 1994). RNNs with LSTM
(Hochreiter and Schmidhuber, 1997) tackle this
problem by introducing a memory cell composed
of a unit called constant error carousel (CEC) with
multiplicative input and output gate units. Input
gates protect against irrelevant inputs and output
gates against current irrelevant memory contents.
This architecture is capable of capturing important
pieces of information seen in a bigger context.
Tree-LSTM is an extension of simple LSTM. A
typical LSTM processes the information sequen-
tially whereas Tree-LSTM architectures enable
sentence representation through a syntactic struc-
ture. Equation (1) represents the composition of
a hidden state vector for an LSTM architecture.
For a simple LSTM, ct represents the memory cell
and ot the output gate at time step t in a sequence.
For Tree-LSTM, ct represents the memory cell
and ot represents the output gate corresponding
to node t in a tree. The structural processing of
Tree-LSTM makes it better suited to representing
</bodyText>
<footnote confidence="0.927971">
1http://www.undl.org/unlsys/unl/unl2005/UW.htm
</footnote>
<bodyText confidence="0.97078325">
sentences. For example, dependency tree structure
captures syntactic features and model parameters
the importance of words (content vs. function
words).
</bodyText>
<equation confidence="0.962344">
ht = ot ⊙ tanh ct (1)
</equation>
<figureCaption confidence="0.96887025">
Figure 1 shows simple LSTM and Tree-LSTM
architectures.
Figure 1: Tree-LSTM (left) and simple LSTM
(right)
</figureCaption>
<sectionHeader confidence="0.990774" genericHeader="method">
4 Evaluation Metric
</sectionHeader>
<bodyText confidence="0.996886">
We represent both the reference (href) and the
translation (htra) using an LSTM and predict the
similarity score y� based on a neural network which
considers both distance and angle between href
and htra:
</bodyText>
<equation confidence="0.999231571428571">
hx = href ⊙ htra
h+ = |href − htra|
(W(�)h� + W(+)h+ + b(h))
hs = σ
(W(p)hs + b(p))
�pθ = softmax
y�= rT pθ
</equation>
<bodyText confidence="0.9977564">
where, σ is a sigmoid function, pθ is the estimated
probability distribution vector and rT = [1 2...K].
The cost function J(θ) is defined over probability
distributions p and pθ using regularised Kullback-
Leibler (KL) divergence.
</bodyText>
<equation confidence="0.977068">
KL (p(i)
p0 ) + 2 ||θ||22 (3)
</equation>
<bodyText confidence="0.99990775">
In Equation 3, i represents the index of each train-
ing pair, n is the number of training pairs and p is
the sparse target distribution such that y = rTp is
defined as follows:
</bodyText>
<figure confidence="0.772084851851852">
{ y − ⌊y⌋, j = ⌊y⌋ + 1
⌊y⌋ − y + 1, j = ⌊y⌋
0
y1
y1
y2
y3
y4
y3
x1
y4
x4
x2
y2
x1
y5
x5
x2
x4
x3
(2)
1
J(θ) = n
∑n
i=1
pj =
otherwise
</figure>
<page confidence="0.847805">
1067
</page>
<bodyText confidence="0.970786722222222">
for 1 G j G K, where, y E [1, K] is the similarity
score of a training pair. For example, for y = 2.7,
pT = [0 0.3 0.7 0 0]. In our case, the similarity
score y is a value between 1 and 5.
For our work, we use glove word vectors (Pen-
nington et al., 2014) and the simple LSTM, the
dependency Tree-LSTM and neural network im-
plementations by Tai et al. (2015). 2 The system
uses the scientific computing framework Torch3.
Training is performed on the data computed in
Section 5. The system uses a mini batch size
of 25 with learning rate 0.05 and regularization
strength 0.0001. The compositional parameters
for our Tree-LSTM systems with memory di-
mensions 150 and 300 are 203,400 and 541,800,
respectively. The training is performed for 10
epochs. System-level scores are computed by ag-
gregating and normalising segment-level scores.
</bodyText>
<sectionHeader confidence="0.9818255" genericHeader="method">
5 Computing Similarity Scores from
WMT Rankings
</sectionHeader>
<bodyText confidence="0.999978148148148">
As we do not have access to any dataset which
provides scores to segments on the basis of trans-
lation quality, we used the WMT-13 ranks corpus
to automatically derive training data. This corpus
is a by-product of the manual systems evaluation
carried out in the WMT-13 evaluation. In the eval-
uation, the annotators are presented with a source
segment, the output of five systems and a reference
translation. The annotators are given the following
instructions: “You are shown a source sentence
followed by several candidate translations. Your
task is to rank the translations from best to worst
(ties are allowed)”. Using the WMT-13 ranked
corpus, we derived a corpus where the reference
and corresponding translations are assigned simi-
larity scores. The fact that ties are allowed makes
it more suitable to generate similarity scores. If
all translations are bad, annotators can mark all
as rank 5 and if all translations are accurate, an-
notators can mark all as rank 1. The selection of
the WMT-13 corpus over other WMT workshops
is motivated by the fact that it is the largest among
them. It contains ten times more ranks than WMT-
12 and three to four times more than WMT-14.
This also makes it possible to obtain enough refer-
ence translation pairs which are evaluated several
times.
</bodyText>
<footnote confidence="0.999894666666667">
2The adapted code for MT evaluation scenarios is avail-
able at https://github.com/rohitguptacs/ReVal.
3http://torch.ch
</footnote>
<bodyText confidence="0.99282275">
Our hypothesis is that if a translation is given
a certain rank many times, this reflects its simi-
larity score with the reference. A better ranked
translation among many systems will be close to
the reference whereas a worse ranked translation
among many systems will be dissimilar from the
reference. To remove noisy pairs, we collect ref-
erence translation pairs below a certain variance
only. We determined appropriate variance values
using Algorithm 1 below for n = 3, 4, 5, 6, 7 and
&gt; 8, separately. The computed variance values are
given in Table 1.
</bodyText>
<table confidence="0.5171085">
n 3 4 5 6 7 &gt;8
Var 0.65 1.0 1.2 1.2 1.3 0.85
</table>
<tableCaption confidence="0.645708">
Table 1: Variances computed using Algorithm 1
Algorithm 1 Variance Computation
</tableCaption>
<listItem confidence="0.837244363636364">
1: procedure GETVARIANCE(judgements)
2: V, v — —1, 0.25 &gt; Initialise N
3: for v &lt; max do
4: prs — pairs with variance below v
5: score .— kendall(prs, judgements)
6: if score &gt; 0.78 then
7: V v
8: v — v + 0.05
9: else
10: break
11: Return V &gt; Return variance
</listItem>
<bodyText confidence="0.999119391304348">
In Algorithm 1, the kendall function calculates
Kendall tau correlation using the WMT-13 hu-
man judgements. We select a set for which the
correlation coefficient is greater than 0.78.4 The
correlation is computed using the annotations for
which scores are available in the corpus (prs). In
other words, the corpus acts as a scoring function
for the available reference translation pairs, which
gives a similarity score between a reference and a
translation. We selected pairs below the variance
values obtained for n = 4, 5, 6, 7 and &gt; 8. Finally,
all the pairs are merged to obtain a set (L). Apart
from this set, we created three other sets for our
experiments. The last two also use the SICK data
(Marelli et al., 2014) which was developed for
evaluating semantic similarity. All four sets are
described below:
L: contains the set generated by selecting the
pairs ranked four or more times and filtering
the segments based on the variance
LNF: contains the set generated by selecting
the pairs ranked four or more times without
any filtering depending on the variance
</bodyText>
<footnote confidence="0.990435">
4The score was decided so that we obtain around 10K
pairs which are annotated at least four times.
</footnote>
<page confidence="0.882513">
1068
</page>
<table confidence="0.9123514">
L+Sick: Added 4500 sentence pairs from
the SICK training set to Set L in the training
set and 500 pairs in the development set.
XL+Sick: Added also the pairs ranked three
times to Set L+Sick.
Train Dev Test
L 9559 1000 1000
LNF 17855 1000 1000
L+Sick 14059 1500 1000
XL+Sick 21356 1500 1000
</table>
<tableCaption confidence="0.724084333333333">
Table 2: Derived Corpus statistics
Table 2 shows the number of pairs extracted for
each set to train our LSTM based models.5
</tableCaption>
<sectionHeader confidence="0.999829" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.996739548780488">
We evaluate our approach trained on the four dif-
ferent datasets obtained from WMT-13 (as given
in Table 2) on WMT-14. Table 3 shows system-
level Pearson correlation obtained on different lan-
guage pairs as well as average Pearson correlation
(PAvg) over all language pairs. The last column
of the table also shows average Spearman corre-
lation (SAvg). The 95% confidence level scores
are obtained using bootstrap resampling as used in
the WMT-2014 metric task evaluation. The scores
in bold show best scores overall and the scores in
bold italic show best scores in our variants.
In Table 3 and Table 4, the first section
(L+Sick(lstm)) shows the results obtained us-
ing simple LSTM (layer 1, hidden dimension
50, memory dimension 150, compositional pa-
rameters 203400). The second section shows
the scores of our Tree-LSTM metric trained
on different training sets and dimensions. Di-
mensions are shown in brackets, e.g L(50,150)
shows the results on set ‘L’ with the hidden
dimension 50 and the memory dimension 150.
L+Sick(mix) shows results of combining the two
systems: L+Sick(50,150) and L+Sick(100,150).
For the sentences longer than 20 words, the sys-
tem uses scores of L+Sick(100,150) and scores
of L+Sick(50,150) for the rest. The third sec-
tion shows the best three overall systems from the
WMT-14 metric task. The fourth section in Table
3 shows the systems from the WMT-14 task which
obtained best results for certain languages but do
5For testing our approach we use WMT-12 and WMT-14
rankings instead of the test sets in this table.
not preform well overall. The last section in Tables
3 and 4 shows systems implementing BLEU (or
variants for the segment level) and METEOR in
the WMT-14 metric task.
Tables 3 and 4 contain a deluge of evaluation
data, mainly to explore the effect of different
training data and model parameter settings for our
models. The main messages can be summarised
as follows: 1. Tree LSTM models significantly
outperform the LSTM model (L+Sick(lstm) and
L+Sick(50,150) have the same data and parameter
settings). 2. For Tree-LSTM models different
parameter settings have only a minor impact on
performance (in fact only for a few language pairs
(e.g. hi-en at system-level, L+Sick(100, 300) and
L+Sick(100,150)) results are statistically signifi-
cantly different). This is reassuring as it indicates
that the metric is not overly sensitive to exten-
sive and delicate parameter tuning. 3. For the
system level evaluation Tree-LSTM models are
fully competitive with the best of the current com-
plex models that combine many different metrics,
substantial external resources and may require a
significant amount of feature engineering and tun-
ing. 4. For the segment level evaluation our met-
ric outperforms BLEU based approaches and the
other three systems6 but lags behind some other
approaches. We investigate this further below.
Tables 3 and 4 show that set L is able to obtain
similar results compared to set LNF even though
we filter out almost half of the pairs. Table 3 shows
that for L+Sick(50, 150) and L+Sick(mix), we ob-
tained an average second best Pearson correlation
and best Spearman correlation coefficient. We also
obtained better results for the Russian-English and
Czech-English language pairs compared to any
other systems in the WMT-14 task.
We also evaluate our setting L-Sick(50,150) on
the WMT-12 task dataset. Our metric performs
best for two out of four language pairs and best
overall at the system level with 0.950 and 0.926
Pearson and Spearman correlation coefficient, re-
spectively. At the segment level, we obtained
0.222 Kendall tau correlation which was better
than seven out of the total ten metrics in the WMT-
12 task.
One of the reasons for the difference in
segment-level and system-level correlations is that
Kendall Tau segment-level correlation is calcu-
</bodyText>
<footnote confidence="0.996022">
6These three systems are not given in this paper. Please
refer (Mach´acek and Bojar, 2014) for results of these systems.
</footnote>
<page confidence="0.920096">
1069
</page>
<table confidence="0.99994225">
Test cs-en de-en fr-en hi-en ru-en PAvg SAvg
L+Sick(lstm) .922 f .051 .882 f .028 .974 f .009 .898 f .011 .863 f .023 .908 f .024 .872 f .060
LNF(50,150) .972 f .032 .900 f .026 .974 f .009 .900 f .011 .882 f .021 .925 f .020 .913 f .045
L(50,150) .988 f .022 .897 f .027 .978 f .008 .905 f .010 .875 f .022 .929 f .018 .904 f .042
L+Sick(50,150) .993 f .017 .904 f .025 .978 f .008 .908 f .010 .881 f .022 .933 f .016 .915 f .042
L+Sick(100,300) .993 f .018 .907 f .025 .973 f .009 .866 f .012 .890 f .020 .926 f .017 .902 f .050
XL+Sick(100,300) .913 f .054 .917 f .024 .978 f .008 .904 f .010 .884 f .022 .919 f .024 .889 f .055
L+Sick(100,150) .994 f .016 .911 f .025 .975 f .009 .923 f .010 .870 f .022 .935 f .016 .904 f .049
L+Sick(mix) .994 f .017 .906 f .025 .979 f .008 .918 f .010 .881 f .022 .935 f .016 .919 f .045
DISCOTK-PARTY-TUNED .975 f .031 .943 f .020 .977 f .009 .956 f .007 .870 f .022 .944 f .018 .912 f .043
LAYERED .941 f .045 .893 f .026 .973 f .009 .976 f .006 .854 f .023 .927 f .022 .894 f .047
DISCOTK-PARTY .983 f .025 .921 f .024 .970 f .010 .862 f .015 .856 f .023 .918 f .019 .856 f .046
REDSYS .989 f .021 .898 f .026 .981 f .008 .676 f .022 .814 f .026 .872 f .021 .786 f .047
REDSYSSENT .993 f .018 .910 f .024 .980 f .008 .644 f .023 .807 f .027 .867 f .020 .771 f .043
BLEU .909 f 0.54 .832 f .034 .952 f .012 .956 f .007 .789 f .027 .888 f .027 .833 f .058
METEOR .980 f .029 .927 f .022 .975 f .009 .457 f .027 .805 f .026 .829 f .023 .788 f .046
</table>
<tableCaption confidence="0.961555">
Table 3: Results: System-Level Correlations on WMT-14
</tableCaption>
<table confidence="0.9999422">
Test cs-en de-en fr-en hi-en ru-en Average Avg wmt12
L+Sick(lstm) .204 f .015 .232 f .014 .289 f .013 .319 f .013 .236 f .012 .256 f .013 .254 f .013
NFL(50,150) .228 f .015 .288 f .014 .318 f .014 .341 f .014 .271 f .012 .289 f .014 .287 f .014
L(50,150) .225 f .015 .272 f .014 .328 f .013 .346 f .013 .280 f .011 .290 f .013 .287 f .013
L+Sick(50,150) .243 f .016 .274 f .013 .333 f .013 .360 f .014 .278 f .011 .298 f .013 .295 f .014
L+Sick(100,300) .233 f .014 .286 f .014 .343 f .014 .358 f .013 .281 f .011 .300 f .013 .297 f .013
XL+Sick(100,300) .252 f .014 .279 f .014 .347 f .013 .367 f .013 .274 f .011 .304 f .013 .301 f .013
L+Sick(100,150) .243 f .016 .274 f .014 .329 f .013 .368 f .012 .276 f .011 .298 f .013 .295 f .013
L+Sick(mix) .243 f .016 .276 f .013 .338 f .013 .358 f .013 .273 f .011 .298 f .013 .295 f .013
DISCOTK-PARTY-TUNED .328 f .014 .380 f .014 .433 f .013 .434 f .013 .355 f .010 .386 f .013 .386 f .013
BEER .284 f .015 .337 f .014 .417 f .013 .438 f .014 .333 f .011 .362 f .013 .358 f .013
REDCOMBSENT .284 f .015 .338 f .013 .406 f .012 .417 f .014 .336 f .011 .356 f .013 .346 f .013
METEOR .282 f .015 .334 f .014 .406 f .012 ..420 f .013 .329 f .010 .354 f .013 .341 f .013
BLEU NRC .226 f .014 .272 f .014 .382 f .013 .322 f .013 .269 f .011 .294 f .013 .267 f .013
SENTBLEU .213 f .016 .271 f .014 .378 f .013 .300 f .013 .263 f .011 .285 f .013 .258 f .014
</table>
<tableCaption confidence="0.999114">
Table 4: Results: Segment-Level Correlations on WMT-14
</tableCaption>
<bodyText confidence="0.93610965625">
lated based on rankings and does not consider
the amount of difference between scores. Here is
an example similar to that given in (Hopkins and
May, 2013). Suppose four systems produce the
translations T0, T1, T2 and T3. Suppose we have
two metrics M1 and M2 and they produce scores
and rankings as follows. GS represents the correct
ranking and scores; Scores are in a scale [0, 1]
with a higher score indicating a better translation:
M1: T0 (0.10), T3 (0.71), T1 (0.72), T2 (0.73)
M2: T1 (0.71), T0 (0.72), T2 (0.73), T3 (0.74)
GS: T0 (0.10), T1 (0.71), T2 (0.72), T3 (0.73)
Certainly, M1 produces better scores and rank-
ing than M2. But, Kendall Tau segment-level
correlation is higher for M2. (There are four
concordant pairs in the M1 rank and five in the
M2 rank.) Therefore, if a metric does not scale
well as per the quality of translations, it may still
obtain a good Kendall Tau segment-level corre-
lation and a better metric may end up getting a
low correlation. Another reason for the discrep-
ancy between segment and system-level scores
may be a low agreement on annotations. For
the WMT-14 dataset, inter-annotator and intra-
annotator agreement were 0.367 and 0.522. These
problems should not occur with Pearson corre-
lation at the system level because system-level
scores are calculated using more sophisticated ap-
proaches (Koehn, 2012; Hopkins and May, 2013;
Sakaguchi et al., 2014). For example, Hopkins and
May (2013) model the differences among annota-
tors by adding random Gaussian noise.
</bodyText>
<sectionHeader confidence="0.998102" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999929714285714">
We conclude that our dense-vector-space-based
ReVal metric is simple, elegant and effective with
state-of-the-art results. ReVal is fully competitive
with the best of the current complex alternative
approaches that involve system combination, ex-
tensive external resources, feature engineering and
tuning.
</bodyText>
<sectionHeader confidence="0.969673" genericHeader="references">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999464714285714">
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Unions Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement no. 317471 and the EC- funded
project QT21 under Horizon 2020, ICT 17, grant
agreement no. 645452.
</bodyText>
<page confidence="0.986677">
1070
</page>
<note confidence="0.7966732">
Mark Hopkins and Jonathan May. 2013. Models of
translation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1416–1424, Sofia, Bulgaria.
</note>
<table confidence="0.5645354">
References
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. IEEE Transactions on
Neural Networks, 5(2):157–166.
</table>
<reference confidence="0.999007191919192">
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing metrics for MT. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1027–1035. Association for Computational
Linguistics.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut,
and Lucia Specia. 2013. Findings of the 2013
Workshop on Statistical Machine Translation. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 1–44, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems, pages 625–632.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449–454.
Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the EACL
2014 Workshop on Statistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138–145. Morgan Kauf-
mann Publishers Inc.
Shubham Gautam and Pushpak Bhattacharyya. 2014.
Layered: Metric for machine translation evaluation.
In Proceedings of the Ninth Workshop on Statistical
Machine Translation.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Linguistic
measures for automatic machine translation evalua-
tion. Machine Translation, 24(3-4):209–240.
Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, and
Preslav Nakov. 2015. Pairwise neural machine
translation evaluation. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 805–814, Beijing,
China. Association for Computational Linguistics.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Shafiq Joty, Francisco Guzm´an, Llu´ıs M`arquez, and
Preslav Nakov. 2014. DiscoTK: Using Discourse
Structure for Machine Translation Evaluation. In
Proceedings of the Ninth Workshop on Statistical
Machine Translation.
Philipp Koehn. 2012. Simulating human judgment
in machine translation evaluation campaigns. In
Proceedings of the Ninth International Workshop
on Spoken Language Translation, pages 179–184,
Hong Kong.
Quoc Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of The 31st International Conference on
Machine Learning, pages 1188–1196.
Matouˇs Mach´acek and Ondrej Bojar. 2014. Results of
the WMT-14 metrics shared task. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A sick cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC 2014.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting Similarities among Languages
for Machine Translation. CoRR, pages 1–10.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the ACL, pages 311–318.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), pages 1532–1543.
Keisuke Sakaguchi, Matt Post, and Benjamin
Van Durme. 2014. Efficient elicitation of
annotations for human evaluation of machine
translation. In In Proceedings of the Ninth
Workshop on Statistical Machine Translation,
Baltimore, Maryland.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2008. TERp system description.
In MetricsMATR workshop atAMTA. Citeseer.
</reference>
<page confidence="0.85944">
1071
</page>
<reference confidence="0.999170625">
Richard Socher, Eh Huang, and Jeffrey Pennington.
2011. Dynamic Pooling and Unfolding Recursive
Autoencoders for Paraphrase Detection. In Ad-
vances in Neural Information Processing Systems,
pages 801–809.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing With Composi-
tional Vector Grammars. In Proceedings of the ACL,
pages 455–465.
Richard Socher, Alex Perelygin, and Jy Wu. 2013b.
Recursive deep models for semantic composition-
ality over a sentiment treebank. In Proceedings of
EMNLP, pages 1631–1642.
Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China. Association for
Computational Linguistics.
Wojciech Zaremba and Ilya Sutskever. 2014. Learning
to execute. arXiv preprint arXiv:1410.4615.
</reference>
<page confidence="0.996071">
1072
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.469529">
<title confidence="0.9998465">ReVal: A Simple and Effective Machine Translation Evaluation Based on Recurrent Neural Networks</title>
<author confidence="0.999899">Constantin Josef van</author>
<affiliation confidence="0.922858">Group in Computational Linguistics, University of Wolverhampton, University and German Research Center for Artificial Intelligence (DFKI),</affiliation>
<email confidence="0.586209">josef.vangenabith@dfki.de</email>
<abstract confidence="0.996917">Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve best results. We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>Reordering metrics for MT.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>1027--1035</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4482" citStr="Birch and Osborne, 2011" startWordPosition="670" endWordPosition="673"> on Empirical Methods in Natural Language Processing, pages 1066–1072, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tree kernel (Collins and Duffy, 2001). The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 metric shared task. Another top performing metric LAYERED (Gautam and Bhattacharyya, 2014), uses linear interpolation of different metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance (Birch and Osborne, 2011) to identify syntactic similarity, and dependency parsing (De Marneffe et al., 2006) and the Universal Networking Language1 for semantic similarity. Recently, Guzm´an et al. (2015) presented a metric based on word embeddings and neural networks. However, this metric is limited to ranking the available systems and does not provide an absolute score. In this paper we propose a compact MT evaluation metric. We hypothesize that our model learns different notions of similarity (which other metrics tend to capture using different metrics) using input, output and forget gates of an LSTM architecture.</context>
</contexts>
<marker>Birch, Osborne, 2011</marker>
<rawString>Alexandra Birch and Miles Osborne. 2011. Reordering metrics for MT. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1027–1035. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2605" citStr="Bojar et al., 2013" startWordPosition="393" endWordPosition="396"> extensive feature engineering. Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks (Tai et al., 2015). LSTM (Hochreiter and Schmidhuber, 1997) is a sequence learning technique which uses a memory cell to preserve a state over a long period of time. This enables distributed representations of sentences using distributed representations of words. Tree-LSTM is a recent approach, which is an extension of the simple LSTM framework (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitguptacs/ReVal. 2 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements f</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>625--632</pages>
<contexts>
<context position="4056" citStr="Collins and Duffy, 2001" startWordPosition="602" endWordPosition="605">and Bojar, 2014) used a combination of different metrics. The top performing system DISKOTK-PARTY-TUNED (Joty et al., 2014) in the WMT-14 task uses five different discourse metrics and twelve different metrics from the ASIYA MT evaluation toolkit (Gim´enez and M`arquez, 2010). The metric computes the number of common sub-trees between a reference and a translation using a convolution 1066 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1066–1072, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tree kernel (Collins and Duffy, 2001). The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 metric shared task. Another top performing metric LAYERED (Gautam and Bhattacharyya, 2014), uses linear interpolation of different metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance (Birch and Osborne, 2011) to identify syntactic similarity, and dependency parsing (De Marneffe et al., 2006) and the Universal Networking Language1 for semantic similarity. Recently, Guzm´an et al. </context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Advances in Neural Information Processing Systems, pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor universal: Language specific translation evaluation for any target language.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="3049" citStr="Denkowski and Lavie, 2014" startWordPosition="458" endWordPosition="461">extension of the simple LSTM framework (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitguptacs/ReVal. 2 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER0.821). Recent best-performing metrics in the WMT-14 metric shared task (Mach´acek and Bojar, 2014) used a combination of different metrics. The top performing system DISKOTK-PARTY-TUNED (Joty et al., 2014) in the WMT-14 task uses five different discourse metrics and twelve different metrics from th</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="2966" citStr="Doddington, 2002" startWordPosition="447" endWordPosition="448">ted representations of words. Tree-LSTM is a recent approach, which is an extension of the simple LSTM framework (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitguptacs/ReVal. 2 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER0.821). Recent best-performing metrics in the WMT-14 metric shared task (Mach´acek and Bojar, 2014) used a combination of different metrics. The top performing system DISKOTK-PARTY-TUNED (Joty et al., 2014) in the WMT</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138–145. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubham Gautam</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Layered: Metric for machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="4311" citStr="Gautam and Bhattacharyya, 2014" startWordPosition="643" endWordPosition="647">Gim´enez and M`arquez, 2010). The metric computes the number of common sub-trees between a reference and a translation using a convolution 1066 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1066–1072, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tree kernel (Collins and Duffy, 2001). The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 metric shared task. Another top performing metric LAYERED (Gautam and Bhattacharyya, 2014), uses linear interpolation of different metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance (Birch and Osborne, 2011) to identify syntactic similarity, and dependency parsing (De Marneffe et al., 2006) and the Universal Networking Language1 for semantic similarity. Recently, Guzm´an et al. (2015) presented a metric based on word embeddings and neural networks. However, this metric is limited to ranking the available systems and does not provide an absolute score. In this paper we propose a compact MT evaluation metric. We hypothesize that o</context>
</contexts>
<marker>Gautam, Bhattacharyya, 2014</marker>
<rawString>Shubham Gautam and Pushpak Bhattacharyya. 2014. Layered: Metric for machine translation evaluation. In Proceedings of the Ninth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic measures for automatic machine translation evaluation.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<pages>24--3</pages>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Linguistic measures for automatic machine translation evaluation. Machine Translation, 24(3-4):209–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Guzm´an</author>
<author>Shafiq Joty</author>
<author>Llu´ıs M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>Pairwise neural machine translation evaluation.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),</booktitle>
<pages>805--814</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China.</location>
<marker>Guzm´an, Joty, M`arquez, Nakov, 2015</marker>
<rawString>Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, and Preslav Nakov. 2015. Pairwise neural machine translation evaluation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 805–814, Beijing, China. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="2174" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="321" endWordPosition="325"> naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks (Tai et al., 2015). LSTM (Hochreiter and Schmidhuber, 1997) is a sequence learning technique which uses a memory cell to preserve a state over a long period of time. This enables distributed representations of sentences using distributed representations of words. Tree-LSTM is a recent approach, which is an extension of the simple LSTM framework (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitg</context>
<context position="5320" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="799" endWordPosition="802"> on word embeddings and neural networks. However, this metric is limited to ranking the available systems and does not provide an absolute score. In this paper we propose a compact MT evaluation metric. We hypothesize that our model learns different notions of similarity (which other metrics tend to capture using different metrics) using input, output and forget gates of an LSTM architecture. 3 LSTMs and Tree-LSTMs Recurrent Neural Networks allow processing of arbitrary length sequences, but early RNNs had the problem of vanishing and exploding gradients (Bengio et al., 1994). RNNs with LSTM (Hochreiter and Schmidhuber, 1997) tackle this problem by introducing a memory cell composed of a unit called constant error carousel (CEC) with multiplicative input and output gate units. Input gates protect against irrelevant inputs and output gates against current irrelevant memory contents. This architecture is capable of capturing important pieces of information seen in a bigger context. Tree-LSTM is an extension of simple LSTM. A typical LSTM processes the information sequentially whereas Tree-LSTM architectures enable sentence representation through a syntactic structure. Equation (1) represents the composition of a hid</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Francisco Guzm´an</author>
<author>Llu´ıs M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>DiscoTK: Using Discourse Structure for Machine Translation Evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation.</booktitle>
<marker>Joty, Guzm´an, M`arquez, Nakov, 2014</marker>
<rawString>Shafiq Joty, Francisco Guzm´an, Llu´ıs M`arquez, and Preslav Nakov. 2014. DiscoTK: Using Discourse Structure for Machine Translation Evaluation. In Proceedings of the Ninth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Simulating human judgment in machine translation evaluation campaigns.</title>
<date>2012</date>
<booktitle>In Proceedings of the Ninth International Workshop on Spoken Language Translation,</booktitle>
<pages>179--184</pages>
<location>Hong Kong.</location>
<contexts>
<context position="20602" citStr="Koehn, 2012" startWordPosition="3558" endWordPosition="3559">e M1 rank and five in the M2 rank.) Therefore, if a metric does not scale well as per the quality of translations, it may still obtain a good Kendall Tau segment-level correlation and a better metric may end up getting a low correlation. Another reason for the discrepancy between segment and system-level scores may be a low agreement on annotations. For the WMT-14 dataset, inter-annotator and intraannotator agreement were 0.367 and 0.522. These problems should not occur with Pearson correlation at the system level because system-level scores are calculated using more sophisticated approaches (Koehn, 2012; Hopkins and May, 2013; Sakaguchi et al., 2014). For example, Hopkins and May (2013) model the differences among annotators by adding random Gaussian noise. 7 Conclusion We conclude that our dense-vector-space-based ReVal metric is simple, elegant and effective with state-of-the-art results. ReVal is fully competitive with the best of the current complex alternative approaches that involve system combination, extensive external resources, feature engineering and tuning. Acknowledgement The research leading to these results has received funding from the People Programme (Marie Curie Actions) o</context>
</contexts>
<marker>Koehn, 2012</marker>
<rawString>Philipp Koehn. 2012. Simulating human judgment in machine translation evaluation campaigns. In Proceedings of the Ninth International Workshop on Spoken Language Translation, pages 179–184, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of The 31st International Conference on Machine Learning,</booktitle>
<pages>1188--1196</pages>
<contexts>
<context position="1541" citStr="Mikolov, 2014" startWordPosition="223" endWordPosition="224"> also show how training data is computed automatically from WMT ranks data. 1 Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine translation (Mikolov et al., 2013a). While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks (Tai et al., 2015). LSTM (</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of The 31st International Conference on Machine Learning, pages 1188–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matouˇs Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT-14 metrics shared task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation.</booktitle>
<marker>Mach´acek, Bojar, 2014</marker>
<rawString>Matouˇs Mach´acek and Ondrej Bojar. 2014. Results of the WMT-14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>A sick cure for the evaluation of compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="11436" citStr="Marelli et al., 2014" startWordPosition="1864" endWordPosition="1867">ments. We select a set for which the correlation coefficient is greater than 0.78.4 The correlation is computed using the annotations for which scores are available in the corpus (prs). In other words, the corpus acts as a scoring function for the available reference translation pairs, which gives a similarity score between a reference and a translation. We selected pairs below the variance values obtained for n = 4, 5, 6, 7 and &gt; 8. Finally, all the pairs are merged to obtain a set (L). Apart from this set, we created three other sets for our experiments. The last two also use the SICK data (Marelli et al., 2014) which was developed for evaluating semantic similarity. All four sets are described below: L: contains the set generated by selecting the pairs ranked four or more times and filtering the segments based on the variance LNF: contains the set generated by selecting the pairs ranked four or more times without any filtering depending on the variance 4The score was decided so that we obtain around 10K pairs which are annotated at least four times. 1068 L+Sick: Added 4500 sentence pairs from the SICK training set to Set L in the training set and 500 pairs in the development set. XL+Sick: Added also</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A sick cure for the evaluation of compositional distributional semantic models. In Proceedings of LREC 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting Similarities among Languages for Machine Translation. CoRR,</title>
<date>2013</date>
<pages>1--10</pages>
<contexts>
<context position="1143" citStr="Mikolov et al., 2013" startWordPosition="158" endWordPosition="161">aphrasing) and require tuning to achieve best results. We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data. 1 Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine translation (Mikolov et al., 2013a). While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based </context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a. Exploiting Similarities among Languages for Machine Translation. CoRR, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1143" citStr="Mikolov et al., 2013" startWordPosition="158" endWordPosition="161">aphrasing) and require tuning to achieve best results. We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data. 1 Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine translation (Mikolov et al., 2013a). While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="2938" citStr="Papineni et al., 2002" startWordPosition="441" endWordPosition="444">tions of sentences using distributed representations of words. Tree-LSTM is a recent approach, which is an extension of the simple LSTM framework (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitguptacs/ReVal. 2 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER0.821). Recent best-performing metrics in the WMT-14 metric shared task (Mach´acek and Bojar, 2014) used a combination of different metrics. The top performing system DISKOTK-PARTY-TUNED (J</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>1532--1543</pages>
<contexts>
<context position="7747" citStr="Pennington et al., 2014" startWordPosition="1235" endWordPosition="1239">Leibler (KL) divergence. KL (p(i) p0 ) + 2 ||θ||22 (3) In Equation 3, i represents the index of each training pair, n is the number of training pairs and p is the sparse target distribution such that y = rTp is defined as follows: { y − ⌊y⌋, j = ⌊y⌋ + 1 ⌊y⌋ − y + 1, j = ⌊y⌋ 0 y1 y1 y2 y3 y4 y3 x1 y4 x4 x2 y2 x1 y5 x5 x2 x4 x3 (2) 1 J(θ) = n ∑n i=1 pj = otherwise 1067 for 1 G j G K, where, y E [1, K] is the similarity score of a training pair. For example, for y = 2.7, pT = [0 0.3 0.7 0 0]. In our case, the similarity score y is a value between 1 and 5. For our work, we use glove word vectors (Pennington et al., 2014) and the simple LSTM, the dependency Tree-LSTM and neural network implementations by Tai et al. (2015). 2 The system uses the scientific computing framework Torch3. Training is performed on the data computed in Section 5. The system uses a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001. The compositional parameters for our Tree-LSTM systems with memory dimensions 150 and 300 are 203,400 and 541,800, respectively. The training is performed for 10 epochs. System-level scores are computed by aggregating and normalising segment-level scores. 5 Computing Similarity</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), pages 1532–1543.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Keisuke Sakaguchi</author>
<author>Matt Post</author>
</authors>
<location>and Benjamin</location>
<marker>Sakaguchi, Post, </marker>
<rawString>Keisuke Sakaguchi, Matt Post, and Benjamin</rawString>
</citation>
<citation valid="true">
<authors>
<author>Van Durme</author>
</authors>
<title>Efficient elicitation of annotations for human evaluation of machine translation. In</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, Maryland.</location>
<marker>Van Durme, 2014</marker>
<rawString>Van Durme. 2014. Efficient elicitation of annotations for human evaluation of machine translation. In In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>TERp system description.</title>
<date>2008</date>
<booktitle>In MetricsMATR workshop atAMTA. Citeseer.</booktitle>
<contexts>
<context position="3080" citStr="Snover et al., 2008" startWordPosition="464" endWordPosition="467">k (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitguptacs/ReVal. 2 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use external resources like WordNet and paraphrase databases. However, system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER0.821). Recent best-performing metrics in the WMT-14 metric shared task (Mach´acek and Bojar, 2014) used a combination of different metrics. The top performing system DISKOTK-PARTY-TUNED (Joty et al., 2014) in the WMT-14 task uses five different discourse metrics and twelve different metrics from the ASIYA MT evaluation toolkit (</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2008</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2008. TERp system description. In MetricsMATR workshop atAMTA. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eh Huang</author>
<author>Jeffrey Pennington</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="1166" citStr="Socher et al., 2011" startWordPosition="162" endWordPosition="165">tuning to achieve best results. We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data. 1 Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine translation (Mikolov et al., 2013a). While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is </context>
</contexts>
<marker>Socher, Huang, Pennington, 2011</marker>
<rawString>Richard Socher, Eh Huang, and Jeffrey Pennington. 2011. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing With Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>455--465</pages>
<contexts>
<context position="1207" citStr="Socher et al., 2013" startWordPosition="168" endWordPosition="171"> a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data. 1 Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine translation (Mikolov et al., 2013a). While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is simple in the sense that it does not requ</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013a. Parsing With Compositional Vector Grammars. In Proceedings of the ACL, pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jy Wu</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="1207" citStr="Socher et al., 2013" startWordPosition="168" endWordPosition="171"> a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data. 1 Introduction Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing (Mikolov et al., 2013b; Socher et al., 2011), sentiment analysis (Socher et al., 2013b), parsing (Socher et al., 2013a) and machine translation (Mikolov et al., 2013a). While dense vector space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words (Mikolov et al., 2013b), segments (Socher et al., 2011) and documents (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is simple in the sense that it does not requ</context>
</contexts>
<marker>Socher, Perelygin, Wu, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, and Jy Wu. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, pages 1631–1642.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),</booktitle>
<pages>1556--1566</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China.</location>
<contexts>
<context position="2133" citStr="Tai et al., 2015" startWordPosition="316" endWordPosition="319">ts (Le and Mikolov, 2014) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation measure based on RNNs. Our metric is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks (Tai et al., 2015). LSTM (Hochreiter and Schmidhuber, 1997) is a sequence learning technique which uses a memory cell to preserve a state over a long period of time. This enables distributed representations of sentences using distributed representations of words. Tree-LSTM is a recent approach, which is an extension of the simple LSTM framework (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data </context>
<context position="7849" citStr="Tai et al. (2015)" startWordPosition="1253" endWordPosition="1256"> pair, n is the number of training pairs and p is the sparse target distribution such that y = rTp is defined as follows: { y − ⌊y⌋, j = ⌊y⌋ + 1 ⌊y⌋ − y + 1, j = ⌊y⌋ 0 y1 y1 y2 y3 y4 y3 x1 y4 x4 x2 y2 x1 y5 x5 x2 x4 x3 (2) 1 J(θ) = n ∑n i=1 pj = otherwise 1067 for 1 G j G K, where, y E [1, K] is the similarity score of a training pair. For example, for y = 2.7, pT = [0 0.3 0.7 0 0]. In our case, the similarity score y is a value between 1 and 5. For our work, we use glove word vectors (Pennington et al., 2014) and the simple LSTM, the dependency Tree-LSTM and neural network implementations by Tai et al. (2015). 2 The system uses the scientific computing framework Torch3. Training is performed on the data computed in Section 5. The system uses a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001. The compositional parameters for our Tree-LSTM systems with memory dimensions 150 and 300 are 203,400 and 541,800, respectively. The training is performed for 10 epochs. System-level scores are computed by aggregating and normalising segment-level scores. 5 Computing Similarity Scores from WMT Rankings As we do not have access to any dataset which provides scores to segments on</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1556–1566, Beijing, China. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Zaremba</author>
<author>Ilya Sutskever</author>
</authors>
<title>Learning to execute. arXiv preprint arXiv:1410.4615.</title>
<date>2014</date>
<contexts>
<context position="2491" citStr="Zaremba and Sutskever, 2014" startWordPosition="374" endWordPosition="377">rd vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks (Tai et al., 2015). LSTM (Hochreiter and Schmidhuber, 1997) is a sequence learning technique which uses a memory cell to preserve a state over a long period of time. This enables distributed representations of sentences using distributed representations of words. Tree-LSTM is a recent approach, which is an extension of the simple LSTM framework (Zaremba and Sutskever, 2014). To provide the required training data, we also show how to automatically convert the WMT-13 (Bojar et al., 2013) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitguptacs/ReVal. 2 Related Work Many metrics have been proposed for MT evaluation. Earlier popular metrics are based on ngram counts (e.g. BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)) or word error rate. Other popular metrics like METEOR (Denkowski and Lavie, 2014) and TERp (Snover et al., 2008) also use e</context>
</contexts>
<marker>Zaremba, Sutskever, 2014</marker>
<rawString>Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
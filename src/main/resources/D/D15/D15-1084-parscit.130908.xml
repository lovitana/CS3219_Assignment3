<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000138">
<title confidence="0.98086">
Knowledge Base Unification via Sense Embeddings and Disambiguation
</title>
<author confidence="0.99864">
Claudio Delli Bovi Luis Espinosa-Anke Roberto Navigli
</author>
<affiliation confidence="0.992643333333333">
Department of Department of Information and Department of
Computer Science Communication Technologies Computer Science
Sapienza University of Rome Universitat Pompeu Fabra Sapienza University of Rome
</affiliation>
<email confidence="0.993355">
dellibovi@di.uniroma1.it luis.espinosa@upf.edu navigli@di.uniroma1.it
</email>
<sectionHeader confidence="0.997311" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976578947368">
We present KB-UNIFY, a novel approach
for integrating the output of different
Open Information Extraction systems into
a single unified and fully disambiguated
knowledge repository. KB-UNIFY con-
sists of three main steps: (1) disambigua-
tion of relation argument pairs via a sense-
based vector representation and a large
unified sense inventory; (2) ranking of se-
mantic relations according to their degree
of specificity; (3) cross-resource relation
alignment and merging based on the se-
mantic similarity of domains and ranges.
We tested KB-UNIFY on a set of four
heterogeneous knowledge bases, obtain-
ing high-quality results. We discuss and
provide evaluations at each stage, and re-
lease output and evaluation data for the use
and scrutiny of the community1.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998906235294118">
The breakthrough of the Open Information Ex-
traction (OIE) paradigm opened up a research
area where Web-scale unconstrained Information
Extraction systems are developed to acquire and
formalize large quantities of knowledge. How-
ever, while successful, to date most state-of-the-
art OIE systems have been developed with their
own type inventories, and no portable ontologi-
cal structure. In fact, OIE systems can be very
different in nature. Early approaches (Etzioni et
al., 2008; Wu and Weld, 2010; Fader et al., 2011)
focused on extracting a large number of relations
from massive unstructured corpora, mostly rely-
ing on dependencies at the level of surface text.
Systems like NELL (Carlson et al., 2010) com-
bine a hand-crafted taxonomy of entities and re-
lations with self-supervised large-scale extraction
</bodyText>
<footnote confidence="0.883319">
1http://lcl.uniroma1.it/kb-unify
</footnote>
<bodyText confidence="0.999656690476191">
from the Web, but they require additional process-
ing for linking and integration (Dutta et al., 2014).
More recent work has focused, instead, on
deeper language understanding, especially at the
level of syntax and semantics (Nakashole et al.,
2012; Moro and Navigli, 2013). By leveraging
semantic analysis, knowledge gathered from un-
structured text can be adequately integrated and
used to enrich existing knowledge bases, such
as YAGO (Mahdisoltani et al., 2015), FREEBASE
(Bollacker et al., 2008) and DBPEDIA (Lehmann
et al., 2014). A large amount of reliable struc-
tured knowledge is crucial for OIE approaches
based on distant supervision (Mintz et al., 2009;
Riedel et al., 2010), even when multi-instance
multi-learning algorithms (Surdeanu et al., 2012)
or matrix factorization techniques (Riedel et al.,
2013; Fan et al., 2014) come into play to deal
with noisy extractions. For this reason a recent
trend of research has focused on Knowledge Base
(KB) completion (Nickel et al., 2012; Bordes et
al., 2013), exploiting the fact that distantly super-
vised OIE and structured knowledge can comple-
ment each other. However, the majority of integra-
tion approaches nowadays are not designed to deal
with many different resources at the same time.
We propose an approach where the key idea is to
bring together knowledge drawn from an arbitrary
number of OIE systems, regardless of whether
these systems provide links to some general-
purpose inventory, come with their own ad-hoc
structure, or have no structure at all. Knowledge
from each source, in the form of (subject, predi-
cate, object) triples, is disambiguated and linked
to a single large sense inventory. This enables us
to discover alignments at a semantic level between
relations from different KBs, and to generate a
unified, fully disambiguated KB of entities and
semantic relations. KB-UNIFY achieves state-
of-the-art disambiguation and provides a general,
resource-independent representation of semantic
relations, suitable for any kind of KB.
</bodyText>
<page confidence="0.97003">
726
</page>
<note confidence="0.9851375">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 726–736,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.998212285714286">
The remainder of this paper is structured as fol-
lows: Section 2 reviews relevant related work;
Sections 3, 4, 5 and 6 describe in detail each stage
of the approach; Sections 7 and 8 describe the ex-
periments carried out and the results obtained; and
finally Section 9 summarizes our findings and dis-
cusses potential directions for future work.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999981563636364">
The integration of knowledge drawn from dif-
ferent sources has received much attention over
the last decade. Among the most notable examples
are resources like BabelNet (Navigli and Ponzetto,
2012), UBY (Gurevych et al., 2012) and YAGO
(Mahdisoltani et al., 2015). While great effort
has been put into aligning knowledge at the con-
cept level, most approaches do not tackle the prob-
lem of integrating heterogeneous knowledge at the
relation level, nor do they exploit effectively the
huge amount of information harvested with OIE
systems, even when this information is unambigu-
ously linked to a structured resource, as in (Nakas-
hole et al., 2012), or (Moro and Navigli, 2013).
In fact, as the number of resources increases, KB
alignment is already becoming an emergent re-
search field: Dutta et al. (2014) describe a method
for linking arguments in NELL triples to DBPE-
DIA by combining First Order Logic and Markov
Networks; Grycner and Weikum (2014) seman-
tify PATTY’s pattern synsets and connect them
to WordNet verbs; Lin et al. (2012) propose a
method to propagate FREEBASE types across RE-
VERB and deal with the problem of unlinkable
entities. All these approaches achieve very com-
petitive results in their respective settings, but un-
like the approach being proposed here, they limit
the task to 1-to-1 alignments. A few contributions
have tried to broaden the scope and include dif-
ferent resources at the same time, but with rather
different goals from ours. For example, Riedel et
al. (2013) propose a universal schema that inte-
grates structured data with OIE data by learning
latent feature vectors for entities and relations; the
KNOWLEDGE VAULT (Dong et al., 2014) uses a
graph-based probabilistic framework where prior
knowledge from existing resources (e.g. FREE-
BASE) improves Web extractions by predicting
their reliability. However, in both cases the main
objective is distantly supervised extraction from
unstructured text, rather than KB unification. A re-
cent trend of research focuses on learning embed-
ding models for structured knowledge and their
application to tasks like relation extraction and KB
completion (Socher et al., 2013; Weston et al.,
2013; Bordes et al., 2013). These approaches,
however, leverage embeddings at surface level,
which are suboptimal for our task, as will be dis-
cussed in Section 3. Since we require a com-
mon semantic framework for KB unification, we
use vector representations based on word senses,
which are mapped to a very large sense inventory.
This shared sense inventory, then, constitutes the
common ground in which disambiguation, align-
ment and final unification occurs.
</bodyText>
<sectionHeader confidence="0.990389" genericHeader="method">
3 Knowledge Base Unification: Overview
</sectionHeader>
<bodyText confidence="0.999651157894737">
KB-UNIFY takes as input a set of KBs K =
{KB1, ..., KBn} and outputs a single, unified and
fully disambiguated KB, denoted as KB∗. For
our purposes we can define a KB KBi as a triple
(Ei, Ri, Ti), where Ei is a set of entities, Ri is
a set of semantic relations, and Ti is a set of
triples (facts) (ed, r, eg) with subject and object
ed, eg E Ei and predicate r E Ri. Depending
on the nature of each KBi, entities in Ei might
be disambiguated and linked to an external in-
ventory (e.g. the entity Washington linked to the
Wikipedia page GEORGE WASHINGTON), or un-
linked and only available as ambiguous mentions
(e.g. the bare word washington might refer to the
president, the city or the state). We can thus par-
tition K into a subset of linked resources KD, and
one of unlinked resources KU. In order to align
very different and heterogeneous KBs at the se-
mantic level, KB-UNIFY exploits:
</bodyText>
<listItem confidence="0.975303555555556">
• A unified sense inventory S, which acts as
a superset for the inventories of individual
KBs. We choose BabelNet (Navigli and
Ponzetto, 2012) for this purpose: by merg-
ing complementary knowledge from different
resources (e.g. Wikipedia, WordNet, Wiki-
data and Wiktionary, among others), Babel-
Net provides a wide coverage of entities and
concepts whilst at the same time enabling
convenient inter-resource mappings for KBi
in KD. For instance, each Wikipedia page (or
Wikidata item) has a corresponding synset in
BabelNet, which enables a one-to-one map-
ping between BabelNet’s synsets and entries
in, e.g., DBPEDIA or FREEBASE;
• A vector space model VS that enables a se-
mantic representation for every item in S.
Current distributional models, like word em-
</listItem>
<page confidence="0.996214">
727
</page>
<figureCaption confidence="0.999772">
Figure 1: Unification algorithm workflow
</figureCaption>
<bodyText confidence="0.999799464285714">
beddings (Mikolov et al., 2013), are not suit-
able to our setting: they are constrained to
surface word forms, and hence they inher-
ently retain ambiguity of polysemous words
and entity mentions. We thus leverage
SENSEMBED (Iacobacci et al., 2015), a novel
semantically-enhanced approach to embed-
dings. SENSEMBED is trained on a large
annotated corpus and produces continuous
representations for individual word senses
(sense embeddings), according to an under-
lying sense inventory.
Figure 1 illustrates the workflow of our KB uni-
fication approach. Entities coming from any
KBi E KD can be directly (and unambiguously)
mapped to the corresponding entries in 5 via Ba-
belNet inter-resource linking (Figure 1(a)): in the
above example, the entity Washington linked to
the Wikipedia page GEORGE WASHINGTON is in-
cluded in the BabelNet synset Washington4bn.
In contrast, unlinked (and potentially ambiguous)
entities need an explicit disambiguation step (Fig-
ure 1(b)) connecting them to appropriate entries,
i.e. synsets, in 5: this is the case, in the above
example, for the ambiguous mention washington
that has to be linked to either the president, the
city or the state. Therefore, our approach com-
prises two successive stages:
</bodyText>
<listItem confidence="0.999858222222222">
• A disambiguation stage (Section 5) where
all KBi E K are linked to 5, either by
inter-resource mapping (Figure 1(a)) or dis-
ambiguation (Figure 1(b)), and all Ei are
merged into a unified set of entities E*. As
a result of this process we obtain a set KS
comprising all the KBs in K redefined using
the common sense inventory 5;
• An alignment stage (Section 6, Figure 1(c))
</listItem>
<bodyText confidence="0.9943542">
where, for each pair of KBs KBSi , KBSj E
KS, we compare any relation pair (ri, rj),
ri E RSi and rj E RSj , in order to identify
cross-resource alignments and merge rela-
tions sharing equivalent semantics into rela-
tion clusters (relation synsets). This process
yields a unified set of relation synsets R*.
The overall result is KB* _ (E*, R*, T*),
where T* is the set of all disambiguated
triples redefined over E* and R*.
</bodyText>
<sectionHeader confidence="0.995869" genericHeader="method">
4 Background
</sectionHeader>
<bodyText confidence="0.999942179487179">
The disambiguation stage of our approach is
based on the interplay between two core compo-
nents: a vector space model VS, as introduced
in Section 3, which provides an unambiguous se-
mantic representation for each item in 5; and a
Word Sense Disambiguation/Entity Linking sys-
tem, working on the same sense inventory 5,
which discovers and disambiguates concepts and
entity mentions within a given input text. In this
section we briefly describe our choice for these
two components: SENSEMBED (Iacobacci et al.,
2015) and BABELFY (Moro et al., 2014).
SENSEMBED is a knowledge-based approach
for obtaining latent continuous representations of
individual word senses. Unlike other sense-based
embeddings approaches, like (Huang et al., 2012),
which address the inherent polysemy of word-
level representations relying solely on text cor-
pora, SENSEMBED exploits the structured knowl-
edge of a large sense inventory along with the dis-
tributional information gathered from text corpora.
In order to do this, SENSEMBED requires a sense-
annotated corpus; for each target word sense, then,
a representation is computed by maximizing the
log likelihood of the word sense with respect to
its context within the annotated text, similarly to
the word-based embeddings model. Following Ia-
cobacci et al. (2015), we trained SENSEMBED us-
ing the English Wikipedia and, as sense inventory,
BabelNet.
BABELFY2 is a joint state-of-the-art approach
to multilingual Entity Linking and Word Sense
Disambiguation. Given the BabelNet lexical-
ized semantic network as underlying structure,
BABELFY first models each concept in the net-
work through its corresponding semantic signa-
ture by leveraging a graph random walk algorithm.
Then, given an input text, the generated seman-
tic signatures are used to construct a subgraph
</bodyText>
<footnote confidence="0.988816">
2http://babelfy.org
</footnote>
<page confidence="0.994278">
728
</page>
<bodyText confidence="0.999961727272727">
of the semantic network representing the mean-
ing of the content words in that text. BABELFY
then searches this subgraph for the intended sense
of each content word, by means of a densest-
subgraph heuristic that identifies high-coherence
interpretations. Given its unified approach that
covers concepts and named entities alike, and its
flexibility in disambiguating both bag-of-words
and proper text, BABELFY constitutes the most
convenient choice for linking relation triples to a
high-coverage sense inventory like BabelNet.
</bodyText>
<sectionHeader confidence="0.996535" genericHeader="method">
5 Disambiguation
</sectionHeader>
<bodyText confidence="0.939807307692308">
In the disambiguation phase (Figure 1(b)), all
KBi ∈ KU are linked to the unified sense in-
ventory S and added to the set of redefined KBs
KS. As explained in Section 3, while each KB
in KD can be unambiguously redefined via Babel-
Net inter-resource links and added to KS, KBs in
KU require an explicit disambiguation step. Given
KBi ∈ KU, our disambiguation module (Figure
2) takes as input its set of unlinked triples Ti and
outputs a set TiS ⊆ Ti of disambiguated triples
with subject-object pairs linked to S. The triples in
TiS, together with their corresponding entity sets
and relation sets, constitute the redefined KBSi
which is then added to KS. However, applying
a straightforward approach that disambiguates all
triples in isolation might lead to very imprecise re-
sults, due to the lack of available context for each
individual triple. We thus devised a disambigua-
tion strategy that comprises three successive steps:
1. We identify a set of high-confidence seeds
from Ti (Section 5.1), i.e. triples hed, r, egi
where subject ed and object eg are highly
semantically related, and disambiguate them
using the senses that maximize their similar-
ity in our vector space VS;
2. We use the seeds to generate a ranking of
</bodyText>
<figureCaption confidence="0.996703">
Figure 2: Disambiguation algorithm workflow
</figureCaption>
<bodyText confidence="0.972795777777778">
the relations in Ri according to their degree
of specificity (Section 5.2). We represent
each r ∈ Ri in our vector space VS and as-
sign higher specificity to relations whose ar-
guments are closer in VS;
3. We finally disambiguate the remaining non-
seed triples in Ti (Section 5.3) starting from
the most specific relations, and jointly using
all participating argument pairs as context.
</bodyText>
<subsectionHeader confidence="0.992526">
5.1 Identifying Seed Argument Pairs
</subsectionHeader>
<bodyText confidence="0.995325666666667">
The first stage of our disambiguation approach
aims at extracting reliable seeds from Ti, i.e.
triples hed, r, egi where subject ed and object eg
can be confidently disambiguated without addi-
tional context. In order to do this we leverage
the sense embeddings associated with each can-
didate disambiguation for ed and eg. We consider
all the available senses for both ed and eg in S,
namely sd = {s1 d, ..., sm d } and sg = {s1 g, ..., sm/
</bodyText>
<equation confidence="0.806267">
g },
</equation>
<bodyText confidence="0.84996">
and the corresponding sets of sense embeddings
</bodyText>
<equation confidence="0.970508428571429">
d = { 1d, m/
v vd, ..., vdm } and vg = {v1g, ..., vg }. We
then select, among all possible pairs of senses, the
pair hs∗d, s∗gi
between the corresponding embeddings hv∗d, v∗gi:
∗ ∗ vd · vg
hvd,vgi = argmaJCvd∈vd, v9∈v9 kvdk kvgk (1)
</equation>
<bodyText confidence="0.999626894736842">
For each disambiguated triple hs∗d, r, s∗gi, the co-
sine similarity value associated with hv∗d, v∗gi rep-
resents the disambiguation confidence ζdis. We
rank all such triples according to their confidence,
and select those above a given threshold δdis. The
underlying assumption is that, for high-confidence
subject-object pairs, the embeddings associated
with the correct senses s∗d and s∗ g will be closest
in VS compared to any other candidate pair. In-
tuitively, the more the relation r between ed and
eg is semantically well defined, the more this as-
sumption is justified. As an example, consider the
triple hArmstrong, worked for, NASAi: among all
the possible senses for Armstrong (the astronaut,
the jazz musician the cyclist, etc.) and NASA (the
space agency, the racing organization, a Swedish
band, etc.) we expect the vectors corresponding to
the astronaut and the space agency to be closest in
the vector space model VS.
</bodyText>
<subsectionHeader confidence="0.99791">
5.2 Relation Specificity Ranking
</subsectionHeader>
<bodyText confidence="0.985145333333333">
The assumption that, given an ambiguous
subject-object pair, correct argument senses are
that maximizes the cosine similarity
</bodyText>
<page confidence="0.992173">
729
</page>
<bodyText confidence="0.959133375">
the closest pair in the vector space (Section 5.1) is
easily verifiable for general relations (e.g. is a, is
part of). However, as a semantic relation becomes
specific, its arguments are less guaranteed to be
semantically related (e.g. is a professor in the uni-
versity of) and a disambiguation approach based
exclusively on similarity is prone to errors. On
the other hand, specific relations tend to narrow
down the scope of possible entity types occurring
as subject and object. In the above example, is a
professor in the university of requires entity pairs
with professors as subjects and cities as objects.
Our disambiguation strategy should thus vary ac-
cording to the specificity of the relations taken into
account. In order to consider this observation in
our disambiguation pipeline, we first need to es-
timate the degree of specificity for each relation
in the relation set Ri of the target KB to be dis-
ambiguated. Given Ri and a set of seeds from the
previous stage (Section 5.1), we apply a specificity
ranking policy and sort relations in Ri from the
most general to the most specific. We compute the
generality Gen(r) of a given relation r by looking
at the spatial dispersion of the sense embeddings
associated with its seed subjects and objects. Let
vD (vG) be the set of sense embeddings associated
with the domain (range) seed arguments of r. For
both vD and vG, we compute the corresponding
centroid vectors µD and µG as:
µk = 1 k |Y lvl, k E {D,G} (2)
v∈vk
Then, the variances σ2D and σ2G are given by:
</bodyText>
<equation confidence="0.941722">
�σ2k = 1(1 − cos (v, µk
|vk |v∈vk
</equation>
<bodyText confidence="0.994297">
with k E {D, G} as before. We finally compute
Gen(r) as the average of σ2D and σ2G. The result
of this procedure is a relation specificity ranking
that associates each relation r with its general-
ity Gen(r). Intuitively, we expect more general
relations to show higher variance (hence higher
Gen(r)), as their subjects and objects are likely
to be rather disperse throughout the vector space;
instead, arguments of very specific relations are
more likely to be clustered together in compact re-
gions, yielding lower values of Gen(r).
</bodyText>
<subsectionHeader confidence="0.997048">
5.3 Disambiguation with Relation Context
</subsectionHeader>
<bodyText confidence="0.99758764">
In the third step, both the specificity ranking
and the seeds are exploited to disambiguate the
remaining triples in Ti. To do this we leverage
BABELFY (Moro et al., 2014) (introduced in Sec-
tion 4). As we observed in Section 5.2, spe-
cific relations impose constraints on their subject-
object types and tend to show compact domains
and ranges in the vector space. Therefore, given
a triple (ed, r, eg), knowing that r is specific en-
ables us to put together all the triples in Ti where
r occurs, and use them to provide meaningful con-
text for disambiguation. If r is general, instead, its
subject-object types are less constrained and addi-
tional triples do not guarantee to provide semanti-
cally related context.
At this stage, our algorithm takes as input the
set of triples Ti, along with the associated disam-
biguation seeds (Section 5.1), the specificity rank-
ing (Section 5.2) and a specificity threshold Sspec.
Ti is first partitioned into two subsets: T spec
i , com-
prising all the triples for which Gen(r) &lt; Sspec,
and Tgen i= Ti \ T spec
i . We then employ two dif-
ferent disambiguation strategies:
</bodyText>
<listItem confidence="0.903466">
• For each distinct relation r occurring in
T spec
</listItem>
<bodyText confidence="0.594523">
i , we first retrieve the subset T spec
</bodyText>
<equation confidence="0.8624915">
i,r ⊂
T spec
</equation>
<bodyText confidence="0.997597166666667">
i of triples where r occurs, and then dis-
ambiguate Tspec i,ras a whole with BABELFY.
For each triple in Tspec
i,r , context is provided
by all the remaining triples along with the
disambiguated seeds extracted for r.
</bodyText>
<listItem confidence="0.545633">
• We disambiguate the remaining triples in
Tgen
</listItem>
<bodyText confidence="0.999832333333333">
i one by one in isolation with BABELFY,
providing for each triple only the predicate
string r as additional context.
</bodyText>
<sectionHeader confidence="0.996175" genericHeader="method">
6 Cross-Resource Relation Alignment
</sectionHeader>
<bodyText confidence="0.999982176470588">
After disambiguation (Section 5) each KB in
K is linked to the unified sense inventory 5 and
added to KS. However, each KBSi E KS still pro-
vides its own relation set RSi ⊆ Ri. Instead, in the
unified KB∗, relations with equivalent semantics
should be considered as part of a single relation
synset even when they come from different KBs.
Therefore, at this stage, we apply an alignment al-
gorithm to identify pairs of relations from different
KBs having equivalent semantics. We exploit the
fact that each relation r is now defined over entity
pairs linked to 5, and we generate a semantic rep-
resentation of r in the vector space VS based on
the centroid vectors of its domain and range. Due
to representing the semantics of relations on this
common ground, we can compare them by com-
puting their domain and range similarity in VS. We
</bodyText>
<equation confidence="0.906445">
))2 (3)
</equation>
<page confidence="0.947835">
730
</page>
<bodyText confidence="0.997179">
first consider each KBSi E KS and, for each rela-
tion ri in RSi , we compute the corresponding cen-
troid vectors µri d and µrig using formula (2). Then,
for each pair of KBs (KBSi , KBSj ) E KS × KS,
we compare all relation pairs (ri, rj) E RSi × RSj
by computing the cosine similarity between do-
main centroids sD and between range centroids
</bodyText>
<equation confidence="0.9617928">
sG:
ri rj
µk · µk
sk __ I µri I I I I µkj11
(4)
</equation>
<bodyText confidence="0.999994111111111">
where µrk denotes the centroid associated with re-
lation r and k E {D, G}. The average of sD and
sG gives us an alignment confidence ζalign for the
pair (ri, rj). If confidence is above a given thresh-
old δalign then ri and rj are merged into the same
relation synset. Relations for which no alignment
is found are turned into singleton relation synsets.
As a result of this alignment procedure we obtain
the unified set of relations R∗.
</bodyText>
<sectionHeader confidence="0.998853" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9591055">
The setting for our experimental evaluation was
the following:
</bodyText>
<listItem confidence="0.928772789473684">
• We used BabelNet 3.03 as our unified sense
inventory for the unification procedure as
well as the underlying inventory for both BA-
BELFY and SENSEMBED. Currently, Babel-
Net contains around 14M synsets and repre-
sents the largest single multilingual reposi-
tory of entities and concepts;
• We selected PATTY (Nakashole et al., 2012)
and WISENET (Moro and Navigli, 2013)
as linked resources. We used PATTY with
FREEBASE types and pattern synsets derived
from Wikipedia, and WISENET 2.0 with
Wikipedia relational phrases;
• We selected NELL (Carlson et al., 2010) and
REVERB (Fader et al., 2011) as unlinked
resources. We used KB beliefs updated to
November 2014 for the former, and the set
of relation instances from ClueWeb09 for the
latter.
</listItem>
<bodyText confidence="0.998406">
Comparative statistics in Table 1 show that the
input KBs are rather different in nature: NELL
is based on 298 predefined relations and contains
beliefs for about 2 million entities. The distri-
bution of entities over relations is however very
</bodyText>
<footnote confidence="0.978818">
3http://babelnet.org
</footnote>
<table confidence="0.998433">
KU KD
NELL REVERB PATTY WISENET
# relations 298 1 299 844 1 631 531 245 935
# triples 2 245 050 14 728 268 15 802 946 2 271 807
# entities 1 996 021 3 327 425 1 087 907 1 636 307
</table>
<tableCaption confidence="0.999742">
Table 1: Statistics on the input KBs
</tableCaption>
<figureCaption confidence="0.996762666666667">
Figure 3: Precision (left) and coverage (right) of disam-
biguated seeds at different values of δdis for (a) the whole set
of triples in PATTY and (b) the subset of ambiguous triples
</figureCaption>
<bodyText confidence="0.999638875">
skewed, with 80.33% of the triples being instances
of the generalizations relationship. In con-
trast, REVERB contains a highly sparse relation
set (1,299,844 distinct relations) and more than
3 million distinct entities. PATTY features the
largest (and, together with WISENET, sparsest)
set of triples, with 1,631,531 distinct relations and
less than 10 triples per relation on average.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="method">
8 Experiments
</sectionHeader>
<subsectionHeader confidence="0.945168">
8.1 Disambiguation
</subsectionHeader>
<bodyText confidence="0.999980923076923">
We tested our disambiguation approach exper-
imentally in terms of both disambiguated seed
quality (Section 8.1.1) and overall disambiguation
performance (Section 8.1.2). We created a de-
velopment set by extracting a subset of 6 million
triples from the largest linked KB in our experi-
mental setup, i.e. PATTY. Triples in PATTY are
automatically linked to YAGO, which is in turn
linked to WordNet and DBPEDIA. Since both re-
sources are also linked by BabelNet, we mapped
the original triples to the BabelNet sense inventory
and used them to tune our disambiguation module.
We also provide two baseline approaches: (1) di-
</bodyText>
<page confidence="0.995217">
731
</page>
<table confidence="0.9995635">
SENSEMBED Baseline
(dis 0.5-0.7 0.7-0.9 0.9-1.0 0.5-0.7 0.7-0.9 0.9-1.0
PATTY .980 .980 1.000 .793 .780 1.000
WISENET .958 .960 .973 .726 .786 .791
NELL .955 .995 1.000 .800 .770 .885
REVERB .930 .940 .950 .775 .725 .920
</table>
<tableCaption confidence="0.750575">
Table 2: Disambiguation precision for all KBs
</tableCaption>
<table confidence="0.9996475">
Sspec = 0.8 Sspec = 0.5 Sspec = 0.3
all only seeds all only seeds all only seeds
PATTY 62.15 26.60 52.49 24.06 40.75 21.41
WISENET 60.00 37.46 54.44 22.26 53.58 16.62
NELL 76.97 62.98 50.95 20.71 44.70 4.36
REVERB 41.20 38.57 25.14 23.70 13.37 12.75
</table>
<tableCaption confidence="0.999842">
Table 3: Coverage results (%) for all KBs
</tableCaption>
<bodyText confidence="0.99886725">
rect disambiguation on individual triples with BA-
BELFY alone (without the seeds) and (2) direct
disambiguation of the seeds only (without BA-
BELFY).
</bodyText>
<sectionHeader confidence="0.351227" genericHeader="method">
8.1.1 Results: Disambiguated Seeds
</sectionHeader>
<bodyText confidence="0.9999749375">
We tuned our disambiguation algorithm by
studying the quality of the disambiguated seeds
(Section 5.1) extracted from the surface text triples
of PATTY. Figure 3 shows precision and cover-
age for increasing values of the confidence thresh-
old Sdis. We computed precision by checking
each disambiguated seed against the correspond-
ing linked triple in the development set, and cov-
erage as the ratio of covered triples. We analyzed
results for both the whole set of triples in PATTY
(Fig. 3a) and the subset of ambiguous triples (Fig.
3b), i.e. those triples whose subjects and objects
have at least two candidate senses each in the Ba-
belNet inventory. In both cases, precision of dis-
ambiguated seeds increases rapidly with Sdis, sta-
bilizing above 90% with Sdis &gt; 0.25. Coverage
displays the opposite behavior, decreasing expo-
nentially with more confident outcomes, from 6
million triples to less than a thousand (for seeds
with confidence Sdis &gt; 0.95). As a result, we
chose Sdis = 0.25 as optimal threshold value
throughout the rest of the evaluations.
In addition, we manually evaluated the disam-
biguated seeds extracted from both linked KBs
(PATTY and WISENET) and unlinked KBs (NELL
and REVERB). For each KB, we extracted up to
three random samples of 150 triples according to
different levels of confidence (dis: the first sam-
ple included extraction with 0.5 &lt; (dis &lt; 0.7,
the second with 0.7 &lt; (dis &lt; 0.9, and the third
with (dis ≥ 0.9. Each sample was evaluated by
two human judges: for each disambiguated triple
</bodyText>
<table confidence="0.8655014">
KB-UNIFY Dutta et al. Baseline
all only seeds (α = 0.5)
Precision .852 .957 .931 .749
Recall .875 .117 .799 .608
F-score .864 .197 .857 .671
</table>
<tableCaption confidence="0.998837">
Table 4: Disambiguation results over NELL gold standard
</tableCaption>
<bodyText confidence="0.999602071428571">
(ed, r, eg), we presented our judges with the sur-
face text arguments ed, eg and the relation string r,
along with the two BabelNet synsets correspond-
ing to the disambiguated arguments s∗d, s∗g, and we
asked whether the association of each subject and
object with the proposed BabelNet synset was cor-
rect. We then estimated precision as the average
proportion of correctly disambiguated triples. For
each sample we compared disambiguation preci-
sion using SENSEMBED, as in Section 5.1, against
the first baseline with BABELFY alone. Results,
reported in Table 2, show that our approach consis-
tently outperforms the baseline and provides high
precision over all samples and KBs.
</bodyText>
<sectionHeader confidence="0.4544725" genericHeader="method">
8.1.2 Results: Disambiguation with Relation
Context
</sectionHeader>
<bodyText confidence="0.999967785714286">
We then evaluated the overall disambiguation
output after specificity ranking (Section 5.2) and
disambiguation with relation context using BA-
BELFY (Section 5.3). We analyzed three config-
urations of the disambiguation pipeline, namely
Sspec E {0.8, 0.5, 0.3}. We ran the algorithm over
both linked and unlinked KBs of our experimen-
tal setup, and computed the coverage for each KB
as the overall ratio of disambiguated triples. Re-
sults are reported in Table 3 and compared to the
coverage obtained from the disambiguated seeds
only: context-aware disambiguation substantially
increases coverage over all KBs. Table 3 also
shows that a restrictive Sspec results in lower cover-
age values, due to the increased number of triples
disambiguated without context.
Finally, we evaluated the quality of disam-
biguation on a publicly available dataset (Dutta
et al., 2014) comprising manual annotations for
NELL. This dataset provides a gold standard
of 1200 triples whose subjects and objects are
manually assigned a proper DBpedia URI. We
again used BabelNet’s inter-resource links to ex-
press DBpedia annotations with our sense inven-
tory and then sought, for each annotated triple in
the dataset, the corresponding triple in our disam-
biguated version of NELL with Sdis = 0.25 and
Sspec = 0.8. We then repeated this process con-
</bodyText>
<page confidence="0.99303">
732
</page>
<figureCaption confidence="0.995571">
Figure 4: Average argument similarity against Gen(r)
</figureCaption>
<table confidence="0.999046333333333">
NELL REVERB PATTY WISENET
Precision .660 .715 .625 .750
Cohen’s kappa - .430 .620 .600
</table>
<tableCaption confidence="0.998722">
Table 5: Specificity ranking evaluation
</tableCaption>
<bodyText confidence="0.999948461538462">
sidering only the disambiguated seeds instead of
the whole disambiguation pipeline. In line with
(Dutta et al., 2014), we computed precision, recall
and F-score for each setting. Results are reported
in Table 4 and compared against those of Dutta et
al. (2014) and against our first baseline with BA-
BELFY alone. KB-UNIFY achieves the best result,
showing that a baseline based on state-of-the-art
disambiguation is negatively affected by the lack
of context for each individual triple. In contrast,
an approach that relies only on the disambiguated
seeds affords very high precision, but suffers from
dramatically lower coverage.
</bodyText>
<subsectionHeader confidence="0.999586">
8.2 Specificity Ranking
</subsectionHeader>
<bodyText confidence="0.999935375">
We evaluated the specificity ranking (Section
5.2) generated by KB-UNIFY for all KBs of our
experimental setup. First of all, we empirically
validated our scoring function Gen(r) over each
resource: for each relation we computed the aver-
age cosine similarity among all its domain argu-
ments ¯sD and among all its range arguments ¯sG.
We then plotted the average s¯ of ¯sD and ¯sG against
Gen(r) for each relation r (Figure 4). As observed
in Section 5.2, the average similarity among do-
main and range arguments decreases for increas-
ing values of Gen(r), indicating that more gen-
eral relations allow less semantically constrained
subject-object types. We then used human judge-
ment to assess the quality of our specificity rank-
ings. First, each ranking was split into four quar-
</bodyText>
<table confidence="0.9997033">
NELL
High Gen(r) agent created
at location
Low Gen(r) person in economic sector
restaurant in city
REVERB
High Gen(r) is for
is in
Low Gen(r) enter Taurus in
carry oxygen to
PATTY
High Gen(r) located in
later served to
Low Gen(r) starting pitcher who played
league coach for
WISENET
High Gen(r) include
is a type of
Low Gen(r) lobe-finned fish lived during
took part in the Eurovision contest
</table>
<tableCaption confidence="0.9804725">
Table 6: Examples of general and specific relations for all
KBs
</tableCaption>
<bodyText confidence="0.999902583333333">
tiles, and two human evaluators were presented
with a sample from the top quartile (i.e. a relation
falling into the most general category) and a sam-
ple from the bottom quartile (i.e. a relation falling
into the most specific category). We shuffled each
relation pair, showed it to our human judges, and
then asked which of the two relations they consid-
ered to be the more specific. Ranking precision
was computed by considering those pairs where
human choice agreed with the ranking. Finally,
we computed inter-annotator agreement on each
specificity ranking (except for NELL, due to the
small sample size) with Cohen’s kappa coefficient
(Cohen, 1968). Results for each ranking are re-
ported in Table 5, while some examples of general
and specific relations for each KB are shown in
Table 6. Disagreement between human choice and
ranking is higher in NELL (where the set of rela-
tions is quite small compared to other KBs) and in
PATTY (due to a sparser set of relations, biased
towards very specific patterns). Inter-annotator
agreement is instead lower for REVERB, where
unconstrained Web harvesting often results in am-
biguous relation strings.
</bodyText>
<subsectionHeader confidence="0.975913">
8.3 Alignment
</subsectionHeader>
<bodyText confidence="0.9997944">
Due to the novelty of our approach, and hence
the lack of widely accepted gold standards and
testbeds, we evaluated our cross-resource relation
alignment algorithm (Section 6) by exploiting hu-
man judgement once again. Given the results of
</bodyText>
<page confidence="0.995624">
733
</page>
<table confidence="0.9998985">
δalign PATTY-WISENET PATTY-REVERB NELL-REVERB
0.7 0.9 0.7 0.9 0.7 0.9
Prec. .68 .80 .58 .74 .61 .75
# Align. 128k 1.2k 47k 643 2.6k 88
PATTY-NELL WISENET-NELL WISENET-REVERB
δalign 0.7 0.9 0.7 0.9 0.7 0.9
Prec. .66 1.00 .70 .84 .59 .87
# Align. 2.6k 57 381 34 9.9k 169
</table>
<tableCaption confidence="0.95337">
Table 7: Cross-resource alignment evaluation
</tableCaption>
<table confidence="0.986253222222222">
PATTY-WISENET ζalign
portrayed ’s character 0.84
debuted in first appeared in 0.86
PATTY-REVERB ζalign
language in is spoken in 0.81
mostly known for plays the role of 0.70
NELL-REVERB ζalign
bookwriter is a novel by 0.88
personleadscity is the mayor of 0.60
NELL-PATTY ζalign
worksfor was hired by 0.72
riveremptiesintoriver tributary of 0.89
NELL-WISENET ζalign
animaleatfood feeds on 0.72
teamhomestadium play their home games at 0.88
REVERB-WISENET ζalign
has a selection of offers 0.82
had grown up in was born and raised in 0.85
</table>
<tableCaption confidence="0.998768">
Table 8: Examples of cross resource relation alignments
</tableCaption>
<bodyText confidence="0.99991608">
Section 8.1, we considered the top 10k frequent
relations for each KB and ran the algorithm over
each possible pair of KBs with two different con-
figurations: δalign = 0.7 and δalign = 0.9. From
each pair of KBs (KBi, KBj) we obtained a list
of candidate alignments, i.e. pairs of relations
(ri, rj) where ri E KBi and rj E KBj. From
each list we then extracted a random sample of
150 candidate alignments. We showed each align-
ment4 (ri, rj) to two human judges, and asked
whether ri and rj represented the same relation.
The problem was presented in terms of paraphras-
ing: for each pair, we asked whether exchanging
ri and rj within a sentence would have changed
that sentence’s meaning. In line with Section 8.2
we computed precision based on the agreement
between human choice and automatic alignments.
Results are reported in Table 7. Our alignment al-
gorithm shows high precision in all pairings where
δalign = 0.9. Alignment reliability decreases for
lower δalign, as relation pairs where ri is a gener-
alization of rj (or vice versa) tend to have similar
centroids in VS. The same holds for pairs where ri
is the negation of rj (or vice versa). Even though
we could have utilized measures based on rela-
</bodyText>
<footnote confidence="0.749591666666667">
4In the case of relation synsets, such as PATTY and
WISENET, we selected up to three random relation strings
from each synset.
</footnote>
<bodyText confidence="0.9999">
tion string similarity (Dutta et al., 2015) to reduce
wrong alignments in these cases, by relying on a
purely semantic criterion we removed any prior as-
sumption on the format of input KBs. Some exam-
ples of alignments are shown in Table 8.
To conclude, we report statistics regarding the
unified KB* produced from the initial set of re-
sources in our experimental setup (cf. Section
7). We validated our thresholds for high-precision,
and selected δdis = 0.25, δspec = 0.8 and δalign
= 0.8. Our alignment algorithm produced 56,673
confident alignments, out of which 2,207 relation
synsets were derived, with an average size of 16.82
individual relations per synset. As a result, we ob-
tained a unified KB* comprising 24,221,856 dis-
ambiguated triples defined over 1,952,716 distinct
entities and 2,675,296 distinct relations.
</bodyText>
<sectionHeader confidence="0.993717" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999985142857143">
We have presented KB-UNIFY, a novel, gen-
eral approach for disambiguating and seamlessly
unifying KBs produced by different OIE sys-
tems. KB-UNIFY represents entities and relations
using a shared semantic representation, leverag-
ing a unified sense inventory together with a
semantically-enhanced vector space model and a
disambiguation algorithm. This enables us to dis-
ambiguate unlinked resources (like NELL and RE-
VERB) with high precision and coverage, and to
discover relation-level cross-resource alignments
effectively. One of the key features of our strat-
egy is its generality: by representing each KB on
a common ground, we need no prior assumption
on the nature and format of the knowledge it en-
codes. We tested our approach experimentally on
a set of four very different KBs, both linked and
unlinked, and we evaluated disambiguation and
alignment results extensively at every stage, ex-
ploiting both human evaluations and public gold
standard datasets (when available). This work
opens compelling avenues for future work. We
plan to further exploit sense-enhanced unified rep-
resentations of relations in various ways: provid-
ing an ontological structure for the unified KB,
exploring complementary approaches for captur-
ing semantic relation alignments, and incorporat-
ing multilinguality.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997646333333333">
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
</bodyText>
<page confidence="0.996995">
734
</page>
<sectionHeader confidence="0.996377" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999968371428572">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A Col-
laboratively Created Graph Database For Structur-
ing Human Knowledge. In Proceedings of SIG-
MOD, pages 1247–1250.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating Embeddings for Modeling Multi-
relational Data. In Advances in NIPS, volume 26,
pages 2787–2795.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an Architecture for Never-
Ending Language Learning. In Proceedings of
AAAI, pages 1306–1313.
Jacob Cohen. 1968. Weighted Kappa: Nominal Scale
Agreement Provision for Scaled Disagreement or
Partial Credit. Psychological Bulletin, 70(4):213–
220.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowl-
edge Vault: A Web-scale Approach to Probabilistic
Knowledge Fusion. In Proceedings of the SIGKDD,
pages 601–610.
Arnab Dutta, Christian Meilicke, and Simone Paolo
Ponzetto. 2014. A Probabilistic Approach for Inte-
grating Heterogeneous Knowledge Sources. In Pro-
ceedings of ESWC, pages 286–301.
Arnab Dutta, Christian Meilicke, and Heiner Stucken-
schmidt. 2015. Enriching Structured Knowledge
with Open Information. In Proceedings of WWW,
pages 267–277.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open Information Extraction
from the Web. Commun. ACM, 51(12):68–74.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of EMNLP, pages 1535–
1545.
Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu,
Thomas Fang Zheng, and Edward Y. Chang. 2014.
Distant Supervision for Relation Extraction with
Matrix Completion. In Proceedings of ACL, pages
839–849.
Adam Grycner and Gerhard Weikum. 2014. HARPY:
Hypernyms and Alignment of Relational Para-
phrases. In Proceedings of ACL, pages 2195–2204.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. Uby: A large-scale unified
lexical-semantic resource based on LMF. In Pro-
ceedings of ACL, pages 580–590.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL, pages
873–882.
Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. SensEmbed: Learning
Sense Embeddings for Word and Relational Simi-
larity. In Proceedings of ACL, pages 95–105.
Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N. Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, S¨oren Auer, and Christian Bizer. 2014. DB-
pedia - A Large-scale, Multilingual Knowledge Base
Extracted from Wikipedia. Semantic Web Journal,
pages 1–29.
Thomas Lin, Mausam, and Oren Etzioni. 2012. No
Noun Phrase Left Behind: Detecting and Typing
Unlinkable Entities. In Proceedings of EMNLP-
CoNLL, pages 893–903.
Farzaneh Mahdisoltani, Joanna Biega, and Fabian M.
Suchanek. 2015. YAGO3: A Knowledge Base from
Multilingual Wikipedias. In CIDR.
Tomas Mikolov, Kal Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at ICLR.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant Supervision for Relation Extrac-
tion Without Labeled Data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.
Andrea Moro and Roberto Navigli. 2013. Integrating
Syntactic and Semantic Analysis into the Open In-
formation Extraction Paradigm. In Proceedings of
IJCAI, pages 2148–2154.
Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: a Unified Approach. TACL, 2:231–
244.
Ndapandula Nakashole, Gerhard Weikum, and
Fabian M. Suchanek. 2012. PATTY: A Taxonomy
of Relational Patterns with Semantic Types. In
Proceedings of EMNLP-CoNLL, pages 1135–1145.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The Automatic Construction, Evaluation
and Application of a Wide-Coverage Multilingual
Semantic Network. Artificial Intelligence, 193:217–
250.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing YAGO: Scalable Ma-
chine Learning for Linked Data. In Proceedings of
WWW, pages 271–280.
</reference>
<page confidence="0.981051">
735
</page>
<reference confidence="0.999811083333333">
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling Relations and Their Mentions
Without Labeled Text. In Proceedings of ECML-
PKDD, pages 148–163.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation Extraction
with Matrix Factorization and Universal Schemas.
In Proceedings of NAACL, pages 74–84.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with Neural
Tensor Networks for Knowledge Base Completion.
In Advances in NIPS, pages 926–934.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
Multi-label Learning for Relation Extraction. In
Proceedings of EMNLP-CoNLL, pages 455–465.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting Language
and Knowledge Bases with Embedding Models for
Relation Extraction. In Proceedings of EMNLP,
pages 1366–1371.
Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction using Wikipedia. In Proceedings ofACL,
pages 118–127.
</reference>
<page confidence="0.998559">
736
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872827">
<title confidence="0.999885">Knowledge Base Unification via Sense Embeddings and Disambiguation</title>
<author confidence="0.99998">Claudio Delli Bovi Luis Espinosa-Anke Roberto Navigli</author>
<affiliation confidence="0.997259333333333">Department of Department of Information and Department of Computer Science Communication Technologies Computer Sapienza University of Rome Universitat Pompeu Fabra Sapienza University of Rome</affiliation>
<email confidence="0.989442">dellibovi@di.uniroma1.itluis.espinosa@upf.edunavigli@di.uniroma1.it</email>
<abstract confidence="0.99436075">present a novel approach for integrating the output of different Open Information Extraction systems into a single unified and fully disambiguated repository. consists of three main steps: (1) disambiguation of relation argument pairs via a sensebased vector representation and a large unified sense inventory; (2) ranking of semantic relations according to their degree of specificity; (3) cross-resource relation alignment and merging based on the semantic similarity of domains and ranges. tested a set of four heterogeneous knowledge bases, obtaining high-quality results. We discuss and provide evaluations at each stage, and release output and evaluation data for the use scrutiny of the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A Collaboratively Created Graph Database For Structuring Human Knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGMOD,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="2509" citStr="Bollacker et al., 2008" startWordPosition="360" endWordPosition="363">hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can comp</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A Collaboratively Created Graph Database For Structuring Human Knowledge. In Proceedings of SIGMOD, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Alberto GarciaDuran</author>
<author>Jason Weston</author>
<author>Oksana Yakhnenko</author>
</authors>
<title>Translating Embeddings for Modeling Multirelational Data.</title>
<date>2013</date>
<booktitle>In Advances in NIPS,</booktitle>
<volume>26</volume>
<pages>2787--2795</pages>
<contexts>
<context position="3024" citStr="Bordes et al., 2013" startWordPosition="443" endWordPosition="446">ich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE systems, regardless of whether these systems provide links to some generalpurpose inventory, come with their own ad-hoc structure, or have no structure at all. Knowledge from each source, in the form of (subject, predicate, object) triples, is disambiguat</context>
<context position="6760" citStr="Bordes et al., 2013" startWordPosition="1034" endWordPosition="1037">y learning latent feature vectors for entities and relations; the KNOWLEDGE VAULT (Dong et al., 2014) uses a graph-based probabilistic framework where prior knowledge from existing resources (e.g. FREEBASE) improves Web extractions by predicting their reliability. However, in both cases the main objective is distantly supervised extraction from unstructured text, rather than KB unification. A recent trend of research focuses on learning embedding models for structured knowledge and their application to tasks like relation extraction and KB completion (Socher et al., 2013; Weston et al., 2013; Bordes et al., 2013). These approaches, however, leverage embeddings at surface level, which are suboptimal for our task, as will be discussed in Section 3. Since we require a common semantic framework for KB unification, we use vector representations based on word senses, which are mapped to a very large sense inventory. This shared sense inventory, then, constitutes the common ground in which disambiguation, alignment and final unification occurs. 3 Knowledge Base Unification: Overview KB-UNIFY takes as input a set of KBs K = {KB1, ..., KBn} and outputs a single, unified and fully disambiguated KB, denoted as K</context>
</contexts>
<marker>Bordes, Usunier, GarciaDuran, Weston, Yakhnenko, 2013</marker>
<rawString>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multirelational Data. In Advances in NIPS, volume 26, pages 2787–2795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an Architecture for NeverEnding Language Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>1306--1313</pages>
<contexts>
<context position="1875" citStr="Carlson et al., 2010" startWordPosition="269" endWordPosition="272">earch area where Web-scale unconstrained Information Extraction systems are developed to acquire and formalize large quantities of knowledge. However, while successful, to date most state-of-theart OIE systems have been developed with their own type inventories, and no portable ontological structure. In fact, OIE systems can be very different in nature. Early approaches (Etzioni et al., 2008; Wu and Weld, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015),</context>
<context position="22882" citStr="Carlson et al., 2010" startWordPosition="3767" endWordPosition="3770">The setting for our experimental evaluation was the following: • We used BabelNet 3.03 as our unified sense inventory for the unification procedure as well as the underlying inventory for both BABELFY and SENSEMBED. Currently, BabelNet contains around 14M synsets and represents the largest single multilingual repository of entities and concepts; • We selected PATTY (Nakashole et al., 2012) and WISENET (Moro and Navigli, 2013) as linked resources. We used PATTY with FREEBASE types and pattern synsets derived from Wikipedia, and WISENET 2.0 with Wikipedia relational phrases; • We selected NELL (Carlson et al., 2010) and REVERB (Fader et al., 2011) as unlinked resources. We used KB beliefs updated to November 2014 for the former, and the set of relation instances from ClueWeb09 for the latter. Comparative statistics in Table 1 show that the input KBs are rather different in nature: NELL is based on 298 predefined relations and contains beliefs for about 2 million entities. The distribution of entities over relations is however very 3http://babelnet.org KU KD NELL REVERB PATTY WISENET # relations 298 1 299 844 1 631 531 245 935 # triples 2 245 050 14 728 268 15 802 946 2 271 807 # entities 1 996 021 3 327 </context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an Architecture for NeverEnding Language Learning. In Proceedings of AAAI, pages 1306–1313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>Weighted Kappa: Nominal Scale Agreement Provision for Scaled Disagreement or Partial Credit.</title>
<date>1968</date>
<journal>Psychological Bulletin,</journal>
<volume>70</volume>
<issue>4</issue>
<pages>220</pages>
<contexts>
<context position="31982" citStr="Cohen, 1968" startWordPosition="5265" endWordPosition="5266">sented with a sample from the top quartile (i.e. a relation falling into the most general category) and a sample from the bottom quartile (i.e. a relation falling into the most specific category). We shuffled each relation pair, showed it to our human judges, and then asked which of the two relations they considered to be the more specific. Ranking precision was computed by considering those pairs where human choice agreed with the ranking. Finally, we computed inter-annotator agreement on each specificity ranking (except for NELL, due to the small sample size) with Cohen’s kappa coefficient (Cohen, 1968). Results for each ranking are reported in Table 5, while some examples of general and specific relations for each KB are shown in Table 6. Disagreement between human choice and ranking is higher in NELL (where the set of relations is quite small compared to other KBs) and in PATTY (due to a sparser set of relations, biased towards very specific patterns). Inter-annotator agreement is instead lower for REVERB, where unconstrained Web harvesting often results in ambiguous relation strings. 8.3 Alignment Due to the novelty of our approach, and hence the lack of widely accepted gold standards and</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>Jacob Cohen. 1968. Weighted Kappa: Nominal Scale Agreement Provision for Scaled Disagreement or Partial Credit. Psychological Bulletin, 70(4):213– 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Dong</author>
<author>Evgeniy Gabrilovich</author>
<author>Geremy Heitz</author>
<author>Wilko Horn</author>
<author>Ni Lao</author>
<author>Kevin Murphy</author>
<author>Thomas Strohmann</author>
<author>Shaohua Sun</author>
<author>Wei Zhang</author>
</authors>
<title>Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion.</title>
<date>2014</date>
<booktitle>In Proceedings of the SIGKDD,</booktitle>
<pages>601--610</pages>
<contexts>
<context position="6241" citStr="Dong et al., 2014" startWordPosition="957" endWordPosition="960"> to propagate FREEBASE types across REVERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the approach being proposed here, they limit the task to 1-to-1 alignments. A few contributions have tried to broaden the scope and include different resources at the same time, but with rather different goals from ours. For example, Riedel et al. (2013) propose a universal schema that integrates structured data with OIE data by learning latent feature vectors for entities and relations; the KNOWLEDGE VAULT (Dong et al., 2014) uses a graph-based probabilistic framework where prior knowledge from existing resources (e.g. FREEBASE) improves Web extractions by predicting their reliability. However, in both cases the main objective is distantly supervised extraction from unstructured text, rather than KB unification. A recent trend of research focuses on learning embedding models for structured knowledge and their application to tasks like relation extraction and KB completion (Socher et al., 2013; Weston et al., 2013; Bordes et al., 2013). These approaches, however, leverage embeddings at surface level, which are subo</context>
</contexts>
<marker>Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, Zhang, 2014</marker>
<rawString>Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion. In Proceedings of the SIGKDD, pages 601–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnab Dutta</author>
<author>Christian Meilicke</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>A Probabilistic Approach for Integrating Heterogeneous Knowledge Sources.</title>
<date>2014</date>
<booktitle>In Proceedings of ESWC,</booktitle>
<pages>286--301</pages>
<contexts>
<context position="2112" citStr="Dutta et al., 2014" startWordPosition="301" endWordPosition="304">r own type inventories, and no portable ontological structure. In fact, OIE systems can be very different in nature. Early approaches (Etzioni et al., 2008; Wu and Weld, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-</context>
<context position="5374" citStr="Dutta et al. (2014)" startWordPosition="814" endWordPosition="817">nzetto, 2012), UBY (Gurevych et al., 2012) and YAGO (Mahdisoltani et al., 2015). While great effort has been put into aligning knowledge at the concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for linking arguments in NELL triples to DBPEDIA by combining First Order Logic and Markov Networks; Grycner and Weikum (2014) semantify PATTY’s pattern synsets and connect them to WordNet verbs; Lin et al. (2012) propose a method to propagate FREEBASE types across REVERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the approach being proposed here, they limit the task to 1-to-1 alignments. A few contributions have tried to broaden the scope and include different resources at </context>
<context position="28805" citStr="Dutta et al., 2014" startWordPosition="4747" endWordPosition="4750">E {0.8, 0.5, 0.3}. We ran the algorithm over both linked and unlinked KBs of our experimental setup, and computed the coverage for each KB as the overall ratio of disambiguated triples. Results are reported in Table 3 and compared to the coverage obtained from the disambiguated seeds only: context-aware disambiguation substantially increases coverage over all KBs. Table 3 also shows that a restrictive Sspec results in lower coverage values, due to the increased number of triples disambiguated without context. Finally, we evaluated the quality of disambiguation on a publicly available dataset (Dutta et al., 2014) comprising manual annotations for NELL. This dataset provides a gold standard of 1200 triples whose subjects and objects are manually assigned a proper DBpedia URI. We again used BabelNet’s inter-resource links to express DBpedia annotations with our sense inventory and then sought, for each annotated triple in the dataset, the corresponding triple in our disambiguated version of NELL with Sdis = 0.25 and Sspec = 0.8. We then repeated this process con732 Figure 4: Average argument similarity against Gen(r) NELL REVERB PATTY WISENET Precision .660 .715 .625 .750 Cohen’s kappa - .430 .620 .600 </context>
</contexts>
<marker>Dutta, Meilicke, Ponzetto, 2014</marker>
<rawString>Arnab Dutta, Christian Meilicke, and Simone Paolo Ponzetto. 2014. A Probabilistic Approach for Integrating Heterogeneous Knowledge Sources. In Proceedings of ESWC, pages 286–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnab Dutta</author>
<author>Christian Meilicke</author>
<author>Heiner Stuckenschmidt</author>
</authors>
<title>Enriching Structured Knowledge with Open Information.</title>
<date>2015</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>267--277</pages>
<contexts>
<context position="35014" citStr="Dutta et al., 2015" startWordPosition="5776" endWordPosition="5779"> between human choice and automatic alignments. Results are reported in Table 7. Our alignment algorithm shows high precision in all pairings where δalign = 0.9. Alignment reliability decreases for lower δalign, as relation pairs where ri is a generalization of rj (or vice versa) tend to have similar centroids in VS. The same holds for pairs where ri is the negation of rj (or vice versa). Even though we could have utilized measures based on rela4In the case of relation synsets, such as PATTY and WISENET, we selected up to three random relation strings from each synset. tion string similarity (Dutta et al., 2015) to reduce wrong alignments in these cases, by relying on a purely semantic criterion we removed any prior assumption on the format of input KBs. Some examples of alignments are shown in Table 8. To conclude, we report statistics regarding the unified KB* produced from the initial set of resources in our experimental setup (cf. Section 7). We validated our thresholds for high-precision, and selected δdis = 0.25, δspec = 0.8 and δalign = 0.8. Our alignment algorithm produced 56,673 confident alignments, out of which 2,207 relation synsets were derived, with an average size of 16.82 individual r</context>
</contexts>
<marker>Dutta, Meilicke, Stuckenschmidt, 2015</marker>
<rawString>Arnab Dutta, Christian Meilicke, and Heiner Stuckenschmidt. 2015. Enriching Structured Knowledge with Open Information. In Proceedings of WWW, pages 267–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<date>2008</date>
<booktitle>Open Information Extraction from the Web. Commun. ACM,</booktitle>
<pages>51--12</pages>
<contexts>
<context position="1648" citStr="Etzioni et al., 2008" startWordPosition="231" endWordPosition="234">iscuss and provide evaluations at each stage, and release output and evaluation data for the use and scrutiny of the community1. 1 Introduction The breakthrough of the Open Information Extraction (OIE) paradigm opened up a research area where Web-scale unconstrained Information Extraction systems are developed to acquire and formalize large quantities of knowledge. However, while successful, to date most state-of-theart OIE systems have been developed with their own type inventories, and no portable ontological structure. In fact, OIE systems can be very different in nature. Early approaches (Etzioni et al., 2008; Wu and Weld, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et</context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S. Weld. 2008. Open Information Extraction from the Web. Commun. ACM, 51(12):68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying Relations for Open Information Extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1535--1545</pages>
<contexts>
<context position="1688" citStr="Fader et al., 2011" startWordPosition="239" endWordPosition="242">age, and release output and evaluation data for the use and scrutiny of the community1. 1 Introduction The breakthrough of the Open Information Extraction (OIE) paradigm opened up a research area where Web-scale unconstrained Information Extraction systems are developed to acquire and formalize large quantities of knowledge. However, while successful, to date most state-of-theart OIE systems have been developed with their own type inventories, and no portable ontological structure. In fact, OIE systems can be very different in nature. Early approaches (Etzioni et al., 2008; Wu and Weld, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By </context>
<context position="22914" citStr="Fader et al., 2011" startWordPosition="3773" endWordPosition="3776">valuation was the following: • We used BabelNet 3.03 as our unified sense inventory for the unification procedure as well as the underlying inventory for both BABELFY and SENSEMBED. Currently, BabelNet contains around 14M synsets and represents the largest single multilingual repository of entities and concepts; • We selected PATTY (Nakashole et al., 2012) and WISENET (Moro and Navigli, 2013) as linked resources. We used PATTY with FREEBASE types and pattern synsets derived from Wikipedia, and WISENET 2.0 with Wikipedia relational phrases; • We selected NELL (Carlson et al., 2010) and REVERB (Fader et al., 2011) as unlinked resources. We used KB beliefs updated to November 2014 for the former, and the set of relation instances from ClueWeb09 for the latter. Comparative statistics in Table 1 show that the input KBs are rather different in nature: NELL is based on 298 predefined relations and contains beliefs for about 2 million entities. The distribution of entities over relations is however very 3http://babelnet.org KU KD NELL REVERB PATTY WISENET # relations 298 1 299 844 1 631 531 245 935 # triples 2 245 050 14 728 268 15 802 946 2 271 807 # entities 1 996 021 3 327 425 1 087 907 1 636 307 Table 1:</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying Relations for Open Information Extraction. In Proceedings of EMNLP, pages 1535– 1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miao Fan</author>
<author>Deli Zhao</author>
<author>Qiang Zhou</author>
<author>Zhiyuan Liu</author>
<author>Thomas Fang Zheng</author>
<author>Edward Y Chang</author>
</authors>
<title>Distant Supervision for Relation Extraction with Matrix Completion.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>839--849</pages>
<contexts>
<context position="2845" citStr="Fan et al., 2014" startWordPosition="412" endWordPosition="415">antics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE systems, regardless of whether these systems provide links to some generalpu</context>
</contexts>
<marker>Fan, Zhao, Zhou, Liu, Zheng, Chang, 2014</marker>
<rawString>Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu, Thomas Fang Zheng, and Edward Y. Chang. 2014. Distant Supervision for Relation Extraction with Matrix Completion. In Proceedings of ACL, pages 839–849.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Grycner</author>
<author>Gerhard Weikum</author>
</authors>
<title>HARPY: Hypernyms and Alignment of Relational Paraphrases.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>2195--2204</pages>
<contexts>
<context position="5519" citStr="Grycner and Weikum (2014)" startWordPosition="838" endWordPosition="841"> the concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for linking arguments in NELL triples to DBPEDIA by combining First Order Logic and Markov Networks; Grycner and Weikum (2014) semantify PATTY’s pattern synsets and connect them to WordNet verbs; Lin et al. (2012) propose a method to propagate FREEBASE types across REVERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the approach being proposed here, they limit the task to 1-to-1 alignments. A few contributions have tried to broaden the scope and include different resources at the same time, but with rather different goals from ours. For example, Riedel et al. (2013) propose a universal schema that integrates structured</context>
</contexts>
<marker>Grycner, Weikum, 2014</marker>
<rawString>Adam Grycner and Gerhard Weikum. 2014. HARPY: Hypernyms and Alignment of Relational Paraphrases. In Proceedings of ACL, pages 2195–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Judith Eckle-Kohler</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Christian Wirth</author>
</authors>
<title>Uby: A large-scale unified lexical-semantic resource based on LMF.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>580--590</pages>
<contexts>
<context position="4797" citStr="Gurevych et al., 2012" startWordPosition="718" endWordPosition="721">ciation for Computational Linguistics. The remainder of this paper is structured as follows: Section 2 reviews relevant related work; Sections 3, 4, 5 and 6 describe in detail each stage of the approach; Sections 7 and 8 describe the experiments carried out and the results obtained; and finally Section 9 summarizes our findings and discusses potential directions for future work. 2 Related Work The integration of knowledge drawn from different sources has received much attention over the last decade. Among the most notable examples are resources like BabelNet (Navigli and Ponzetto, 2012), UBY (Gurevych et al., 2012) and YAGO (Mahdisoltani et al., 2015). While great effort has been put into aligning knowledge at the concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for </context>
</contexts>
<marker>Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, Wirth, 2012</marker>
<rawString>Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M. Meyer, and Christian Wirth. 2012. Uby: A large-scale unified lexical-semantic resource based on LMF. In Proceedings of ACL, pages 580–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>873--882</pages>
<contexts>
<context position="11693" citStr="Huang et al., 2012" startWordPosition="1854" endWordPosition="1857">VS, as introduced in Section 3, which provides an unambiguous semantic representation for each item in 5; and a Word Sense Disambiguation/Entity Linking system, working on the same sense inventory 5, which discovers and disambiguates concepts and entity mentions within a given input text. In this section we briefly describe our choice for these two components: SENSEMBED (Iacobacci et al., 2015) and BABELFY (Moro et al., 2014). SENSEMBED is a knowledge-based approach for obtaining latent continuous representations of individual word senses. Unlike other sense-based embeddings approaches, like (Huang et al., 2012), which address the inherent polysemy of wordlevel representations relying solely on text corpora, SENSEMBED exploits the structured knowledge of a large sense inventory along with the distributional information gathered from text corpora. In order to do this, SENSEMBED requires a senseannotated corpus; for each target word sense, then, a representation is computed by maximizing the log likelihood of the word sense with respect to its context within the annotated text, similarly to the word-based embeddings model. Following Iacobacci et al. (2015), we trained SENSEMBED using the English Wikipe</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Proceedings of ACL, pages 873–882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ignacio Iacobacci</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SensEmbed: Learning Sense Embeddings for Word and Relational Similarity.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>95--105</pages>
<contexts>
<context position="9166" citStr="Iacobacci et al., 2015" startWordPosition="1441" endWordPosition="1444">n KD. For instance, each Wikipedia page (or Wikidata item) has a corresponding synset in BabelNet, which enables a one-to-one mapping between BabelNet’s synsets and entries in, e.g., DBPEDIA or FREEBASE; • A vector space model VS that enables a semantic representation for every item in S. Current distributional models, like word em727 Figure 1: Unification algorithm workflow beddings (Mikolov et al., 2013), are not suitable to our setting: they are constrained to surface word forms, and hence they inherently retain ambiguity of polysemous words and entity mentions. We thus leverage SENSEMBED (Iacobacci et al., 2015), a novel semantically-enhanced approach to embeddings. SENSEMBED is trained on a large annotated corpus and produces continuous representations for individual word senses (sense embeddings), according to an underlying sense inventory. Figure 1 illustrates the workflow of our KB unification approach. Entities coming from any KBi E KD can be directly (and unambiguously) mapped to the corresponding entries in 5 via BabelNet inter-resource linking (Figure 1(a)): in the above example, the entity Washington linked to the Wikipedia page GEORGE WASHINGTON is included in the BabelNet synset Washington</context>
<context position="11471" citStr="Iacobacci et al., 2015" startWordPosition="1824" endWordPosition="1827"> KB* _ (E*, R*, T*), where T* is the set of all disambiguated triples redefined over E* and R*. 4 Background The disambiguation stage of our approach is based on the interplay between two core components: a vector space model VS, as introduced in Section 3, which provides an unambiguous semantic representation for each item in 5; and a Word Sense Disambiguation/Entity Linking system, working on the same sense inventory 5, which discovers and disambiguates concepts and entity mentions within a given input text. In this section we briefly describe our choice for these two components: SENSEMBED (Iacobacci et al., 2015) and BABELFY (Moro et al., 2014). SENSEMBED is a knowledge-based approach for obtaining latent continuous representations of individual word senses. Unlike other sense-based embeddings approaches, like (Huang et al., 2012), which address the inherent polysemy of wordlevel representations relying solely on text corpora, SENSEMBED exploits the structured knowledge of a large sense inventory along with the distributional information gathered from text corpora. In order to do this, SENSEMBED requires a senseannotated corpus; for each target word sense, then, a representation is computed by maximiz</context>
</contexts>
<marker>Iacobacci, Pilehvar, Navigli, 2015</marker>
<rawString>Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli. 2015. SensEmbed: Learning Sense Embeddings for Word and Relational Similarity. In Proceedings of ACL, pages 95–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Lehmann</author>
<author>Robert Isele</author>
<author>Max Jakob</author>
<author>Anja Jentzsch</author>
<author>Dimitris Kontokostas</author>
<author>Pablo N Mendes</author>
<author>Sebastian Hellmann</author>
<author>Mohamed Morsey</author>
<author>Patrick van Kleef</author>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
</authors>
<title>DBpedia - A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia. Semantic Web Journal,</title>
<date>2014</date>
<pages>1--29</pages>
<marker>Lehmann, Isele, Jakob, Jentzsch, Kontokostas, Mendes, Hellmann, Morsey, van Kleef, Auer, Bizer, 2014</marker>
<rawString>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, S¨oren Auer, and Christian Bizer. 2014. DBpedia - A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia. Semantic Web Journal, pages 1–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>893--903</pages>
<contexts>
<context position="5606" citStr="Lin et al. (2012)" startWordPosition="853" endWordPosition="856">ledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for linking arguments in NELL triples to DBPEDIA by combining First Order Logic and Markov Networks; Grycner and Weikum (2014) semantify PATTY’s pattern synsets and connect them to WordNet verbs; Lin et al. (2012) propose a method to propagate FREEBASE types across REVERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the approach being proposed here, they limit the task to 1-to-1 alignments. A few contributions have tried to broaden the scope and include different resources at the same time, but with rather different goals from ours. For example, Riedel et al. (2013) propose a universal schema that integrates structured data with OIE data by learning latent feature vectors for entities and relations; the </context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2012. No Noun Phrase Left Behind: Detecting and Typing Unlinkable Entities. In Proceedings of EMNLPCoNLL, pages 893–903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farzaneh Mahdisoltani</author>
<author>Joanna Biega</author>
<author>Fabian M Suchanek</author>
</authors>
<title>YAGO3: A Knowledge Base from Multilingual Wikipedias.</title>
<date>2015</date>
<booktitle>In CIDR.</booktitle>
<contexts>
<context position="2474" citStr="Mahdisoltani et al., 2015" startWordPosition="355" endWordPosition="358">NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OI</context>
<context position="4834" citStr="Mahdisoltani et al., 2015" startWordPosition="724" endWordPosition="727">tics. The remainder of this paper is structured as follows: Section 2 reviews relevant related work; Sections 3, 4, 5 and 6 describe in detail each stage of the approach; Sections 7 and 8 describe the experiments carried out and the results obtained; and finally Section 9 summarizes our findings and discusses potential directions for future work. 2 Related Work The integration of knowledge drawn from different sources has received much attention over the last decade. Among the most notable examples are resources like BabelNet (Navigli and Ponzetto, 2012), UBY (Gurevych et al., 2012) and YAGO (Mahdisoltani et al., 2015). While great effort has been put into aligning knowledge at the concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for linking arguments in NELL triples to </context>
</contexts>
<marker>Mahdisoltani, Biega, Suchanek, 2015</marker>
<rawString>Farzaneh Mahdisoltani, Joanna Biega, and Fabian M. Suchanek. 2015. YAGO3: A Knowledge Base from Multilingual Wikipedias. In CIDR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kal Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="8952" citStr="Mikolov et al., 2013" startWordPosition="1406" endWordPosition="1409">resources (e.g. Wikipedia, WordNet, Wikidata and Wiktionary, among others), BabelNet provides a wide coverage of entities and concepts whilst at the same time enabling convenient inter-resource mappings for KBi in KD. For instance, each Wikipedia page (or Wikidata item) has a corresponding synset in BabelNet, which enables a one-to-one mapping between BabelNet’s synsets and entries in, e.g., DBPEDIA or FREEBASE; • A vector space model VS that enables a semantic representation for every item in S. Current distributional models, like word em727 Figure 1: Unification algorithm workflow beddings (Mikolov et al., 2013), are not suitable to our setting: they are constrained to surface word forms, and hence they inherently retain ambiguity of polysemous words and entity mentions. We thus leverage SENSEMBED (Iacobacci et al., 2015), a novel semantically-enhanced approach to embeddings. SENSEMBED is trained on a large annotated corpus and produces continuous representations for individual word senses (sense embeddings), according to an underlying sense inventory. Figure 1 illustrates the workflow of our KB unification approach. Entities coming from any KBi E KD can be directly (and unambiguously) mapped to the </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kal Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant Supervision for Relation Extraction Without Labeled Data.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="2672" citStr="Mintz et al., 2009" startWordPosition="387" endWordPosition="390">rocessing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant Supervision for Relation Extraction Without Labeled Data. In Proceedings of ACLIJCNLP, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Roberto Navigli</author>
</authors>
<title>Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>2148--2154</pages>
<contexts>
<context position="2283" citStr="Moro and Navigli, 2013" startWordPosition="327" endWordPosition="330">d, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy ext</context>
<context position="5245" citStr="Moro and Navigli, 2013" startWordPosition="792" endWordPosition="795">sources has received much attention over the last decade. Among the most notable examples are resources like BabelNet (Navigli and Ponzetto, 2012), UBY (Gurevych et al., 2012) and YAGO (Mahdisoltani et al., 2015). While great effort has been put into aligning knowledge at the concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for linking arguments in NELL triples to DBPEDIA by combining First Order Logic and Markov Networks; Grycner and Weikum (2014) semantify PATTY’s pattern synsets and connect them to WordNet verbs; Lin et al. (2012) propose a method to propagate FREEBASE types across REVERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the approach being proposed here, </context>
<context position="22690" citStr="Moro and Navigli, 2013" startWordPosition="3737" endWordPosition="3740">et. Relations for which no alignment is found are turned into singleton relation synsets. As a result of this alignment procedure we obtain the unified set of relations R∗. 7 Experimental Setup The setting for our experimental evaluation was the following: • We used BabelNet 3.03 as our unified sense inventory for the unification procedure as well as the underlying inventory for both BABELFY and SENSEMBED. Currently, BabelNet contains around 14M synsets and represents the largest single multilingual repository of entities and concepts; • We selected PATTY (Nakashole et al., 2012) and WISENET (Moro and Navigli, 2013) as linked resources. We used PATTY with FREEBASE types and pattern synsets derived from Wikipedia, and WISENET 2.0 with Wikipedia relational phrases; • We selected NELL (Carlson et al., 2010) and REVERB (Fader et al., 2011) as unlinked resources. We used KB beliefs updated to November 2014 for the former, and the set of relation instances from ClueWeb09 for the latter. Comparative statistics in Table 1 show that the input KBs are rather different in nature: NELL is based on 298 predefined relations and contains beliefs for about 2 million entities. The distribution of entities over relations </context>
</contexts>
<marker>Moro, Navigli, 2013</marker>
<rawString>Andrea Moro and Roberto Navigli. 2013. Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm. In Proceedings of IJCAI, pages 2148–2154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Alessandro Raganato</author>
<author>Roberto Navigli</author>
</authors>
<title>Entity Linking meets Word Sense Disambiguation: a Unified Approach. TACL,</title>
<date>2014</date>
<pages>2--231</pages>
<contexts>
<context position="11503" citStr="Moro et al., 2014" startWordPosition="1830" endWordPosition="1833">set of all disambiguated triples redefined over E* and R*. 4 Background The disambiguation stage of our approach is based on the interplay between two core components: a vector space model VS, as introduced in Section 3, which provides an unambiguous semantic representation for each item in 5; and a Word Sense Disambiguation/Entity Linking system, working on the same sense inventory 5, which discovers and disambiguates concepts and entity mentions within a given input text. In this section we briefly describe our choice for these two components: SENSEMBED (Iacobacci et al., 2015) and BABELFY (Moro et al., 2014). SENSEMBED is a knowledge-based approach for obtaining latent continuous representations of individual word senses. Unlike other sense-based embeddings approaches, like (Huang et al., 2012), which address the inherent polysemy of wordlevel representations relying solely on text corpora, SENSEMBED exploits the structured knowledge of a large sense inventory along with the distributional information gathered from text corpora. In order to do this, SENSEMBED requires a senseannotated corpus; for each target word sense, then, a representation is computed by maximizing the log likelihood of the wo</context>
<context position="19118" citStr="Moro et al., 2014" startWordPosition="3091" endWordPosition="3094"> specificity ranking that associates each relation r with its generality Gen(r). Intuitively, we expect more general relations to show higher variance (hence higher Gen(r)), as their subjects and objects are likely to be rather disperse throughout the vector space; instead, arguments of very specific relations are more likely to be clustered together in compact regions, yielding lower values of Gen(r). 5.3 Disambiguation with Relation Context In the third step, both the specificity ranking and the seeds are exploited to disambiguate the remaining triples in Ti. To do this we leverage BABELFY (Moro et al., 2014) (introduced in Section 4). As we observed in Section 5.2, specific relations impose constraints on their subjectobject types and tend to show compact domains and ranges in the vector space. Therefore, given a triple (ed, r, eg), knowing that r is specific enables us to put together all the triples in Ti where r occurs, and use them to provide meaningful context for disambiguation. If r is general, instead, its subject-object types are less constrained and additional triples do not guarantee to provide semantically related context. At this stage, our algorithm takes as input the set of triples</context>
</contexts>
<marker>Moro, Raganato, Navigli, 2014</marker>
<rawString>Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking meets Word Sense Disambiguation: a Unified Approach. TACL, 2:231– 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian M Suchanek</author>
</authors>
<title>PATTY: A Taxonomy of Relational Patterns with Semantic Types.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1135--1145</pages>
<contexts>
<context position="2258" citStr="Nakashole et al., 2012" startWordPosition="323" endWordPosition="326">et al., 2008; Wu and Weld, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into pl</context>
<context position="5216" citStr="Nakashole et al., 2012" startWordPosition="786" endWordPosition="790">owledge drawn from different sources has received much attention over the last decade. Among the most notable examples are resources like BabelNet (Navigli and Ponzetto, 2012), UBY (Gurevych et al., 2012) and YAGO (Mahdisoltani et al., 2015). While great effort has been put into aligning knowledge at the concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. (2014) describe a method for linking arguments in NELL triples to DBPEDIA by combining First Order Logic and Markov Networks; Grycner and Weikum (2014) semantify PATTY’s pattern synsets and connect them to WordNet verbs; Lin et al. (2012) propose a method to propagate FREEBASE types across REVERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the a</context>
<context position="22653" citStr="Nakashole et al., 2012" startWordPosition="3731" endWordPosition="3734">re merged into the same relation synset. Relations for which no alignment is found are turned into singleton relation synsets. As a result of this alignment procedure we obtain the unified set of relations R∗. 7 Experimental Setup The setting for our experimental evaluation was the following: • We used BabelNet 3.03 as our unified sense inventory for the unification procedure as well as the underlying inventory for both BABELFY and SENSEMBED. Currently, BabelNet contains around 14M synsets and represents the largest single multilingual repository of entities and concepts; • We selected PATTY (Nakashole et al., 2012) and WISENET (Moro and Navigli, 2013) as linked resources. We used PATTY with FREEBASE types and pattern synsets derived from Wikipedia, and WISENET 2.0 with Wikipedia relational phrases; • We selected NELL (Carlson et al., 2010) and REVERB (Fader et al., 2011) as unlinked resources. We used KB beliefs updated to November 2014 for the former, and the set of relation instances from ClueWeb09 for the latter. Comparative statistics in Table 1 show that the input KBs are rather different in nature: NELL is based on 298 predefined relations and contains beliefs for about 2 million entities. The dis</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian M. Suchanek. 2012. PATTY: A Taxonomy of Relational Patterns with Semantic Types. In Proceedings of EMNLP-CoNLL, pages 1135–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<pages>250</pages>
<contexts>
<context position="4768" citStr="Navigli and Ponzetto, 2012" startWordPosition="713" endWordPosition="716"> 17-21 September 2015. c�2015 Association for Computational Linguistics. The remainder of this paper is structured as follows: Section 2 reviews relevant related work; Sections 3, 4, 5 and 6 describe in detail each stage of the approach; Sections 7 and 8 describe the experiments carried out and the results obtained; and finally Section 9 summarizes our findings and discusses potential directions for future work. 2 Related Work The integration of knowledge drawn from different sources has received much attention over the last decade. Among the most notable examples are resources like BabelNet (Navigli and Ponzetto, 2012), UBY (Gurevych et al., 2012) and YAGO (Mahdisoltani et al., 2015). While great effort has been put into aligning knowledge at the concept level, most approaches do not tackle the problem of integrating heterogeneous knowledge at the relation level, nor do they exploit effectively the huge amount of information harvested with OIE systems, even when this information is unambiguously linked to a structured resource, as in (Nakashole et al., 2012), or (Moro and Navigli, 2013). In fact, as the number of resources increases, KB alignment is already becoming an emergent research field: Dutta et al. </context>
<context position="8262" citStr="Navigli and Ponzetto, 2012" startWordPosition="1299" endWordPosition="1302">i, entities in Ei might be disambiguated and linked to an external inventory (e.g. the entity Washington linked to the Wikipedia page GEORGE WASHINGTON), or unlinked and only available as ambiguous mentions (e.g. the bare word washington might refer to the president, the city or the state). We can thus partition K into a subset of linked resources KD, and one of unlinked resources KU. In order to align very different and heterogeneous KBs at the semantic level, KB-UNIFY exploits: • A unified sense inventory S, which acts as a superset for the inventories of individual KBs. We choose BabelNet (Navigli and Ponzetto, 2012) for this purpose: by merging complementary knowledge from different resources (e.g. Wikipedia, WordNet, Wikidata and Wiktionary, among others), BabelNet provides a wide coverage of entities and concepts whilst at the same time enabling convenient inter-resource mappings for KBi in KD. For instance, each Wikipedia page (or Wikidata item) has a corresponding synset in BabelNet, which enables a one-to-one mapping between BabelNet’s synsets and entries in, e.g., DBPEDIA or FREEBASE; • A vector space model VS that enables a semantic representation for every item in S. Current distributional models</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network. Artificial Intelligence, 193:217– 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Nickel</author>
<author>Volker Tresp</author>
<author>Hans-Peter Kriegel</author>
</authors>
<title>Factorizing YAGO: Scalable Machine Learning for Linked Data.</title>
<date>2012</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>271--280</pages>
<contexts>
<context position="3002" citStr="Nickel et al., 2012" startWordPosition="439" endWordPosition="442">rated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE systems, regardless of whether these systems provide links to some generalpurpose inventory, come with their own ad-hoc structure, or have no structure at all. Knowledge from each source, in the form of (subject, predicate, object) t</context>
</contexts>
<marker>Nickel, Tresp, Kriegel, 2012</marker>
<rawString>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2012. Factorizing YAGO: Scalable Machine Learning for Linked Data. In Proceedings of WWW, pages 271–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling Relations and Their Mentions Without Labeled Text.</title>
<date>2010</date>
<booktitle>In Proceedings of ECMLPKDD,</booktitle>
<pages>148--163</pages>
<contexts>
<context position="2694" citStr="Riedel et al., 2010" startWordPosition="391" endWordPosition="394">g and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling Relations and Their Mentions Without Labeled Text. In Proceedings of ECMLPKDD, pages 148–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation Extraction with Matrix Factorization and Universal Schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>74--84</pages>
<contexts>
<context position="2826" citStr="Riedel et al., 2013" startWordPosition="408" endWordPosition="411">vel of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE systems, regardless of whether these systems provide link</context>
<context position="6065" citStr="Riedel et al. (2013)" startWordPosition="929" endWordPosition="932">combining First Order Logic and Markov Networks; Grycner and Weikum (2014) semantify PATTY’s pattern synsets and connect them to WordNet verbs; Lin et al. (2012) propose a method to propagate FREEBASE types across REVERB and deal with the problem of unlinkable entities. All these approaches achieve very competitive results in their respective settings, but unlike the approach being proposed here, they limit the task to 1-to-1 alignments. A few contributions have tried to broaden the scope and include different resources at the same time, but with rather different goals from ours. For example, Riedel et al. (2013) propose a universal schema that integrates structured data with OIE data by learning latent feature vectors for entities and relations; the KNOWLEDGE VAULT (Dong et al., 2014) uses a graph-based probabilistic framework where prior knowledge from existing resources (e.g. FREEBASE) improves Web extractions by predicting their reliability. However, in both cases the main objective is distantly supervised extraction from unstructured text, rather than KB unification. A recent trend of research focuses on learning embedding models for structured knowledge and their application to tasks like relati</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation Extraction with Matrix Factorization and Universal Schemas. In Proceedings of NAACL, pages 74–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Reasoning with Neural Tensor Networks for Knowledge Base Completion.</title>
<date>2013</date>
<booktitle>In Advances in NIPS,</booktitle>
<pages>926--934</pages>
<contexts>
<context position="6717" citStr="Socher et al., 2013" startWordPosition="1026" endWordPosition="1029">integrates structured data with OIE data by learning latent feature vectors for entities and relations; the KNOWLEDGE VAULT (Dong et al., 2014) uses a graph-based probabilistic framework where prior knowledge from existing resources (e.g. FREEBASE) improves Web extractions by predicting their reliability. However, in both cases the main objective is distantly supervised extraction from unstructured text, rather than KB unification. A recent trend of research focuses on learning embedding models for structured knowledge and their application to tasks like relation extraction and KB completion (Socher et al., 2013; Weston et al., 2013; Bordes et al., 2013). These approaches, however, leverage embeddings at surface level, which are suboptimal for our task, as will be discussed in Section 3. Since we require a common semantic framework for KB unification, we use vector representations based on word senses, which are mapped to a very large sense inventory. This shared sense inventory, then, constitutes the common ground in which disambiguation, alignment and final unification occurs. 3 Knowledge Base Unification: Overview KB-UNIFY takes as input a set of KBs K = {KB1, ..., KBn} and outputs a single, unifi</context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning with Neural Tensor Networks for Knowledge Base Completion. In Advances in NIPS, pages 926–934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance Multi-label Learning for Relation Extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>455--465</pages>
<contexts>
<context position="2770" citStr="Surdeanu et al., 2012" startWordPosition="400" endWordPosition="403">ead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro and Navigli, 2013). By leveraging semantic analysis, knowledge gathered from unstructured text can be adequately integrated and used to enrich existing knowledge bases, such as YAGO (Mahdisoltani et al., 2015), FREEBASE (Bollacker et al., 2008) and DBPEDIA (Lehmann et al., 2014). A large amount of reliable structured knowledge is crucial for OIE approaches based on distant supervision (Mintz et al., 2009; Riedel et al., 2010), even when multi-instance multi-learning algorithms (Surdeanu et al., 2012) or matrix factorization techniques (Riedel et al., 2013; Fan et al., 2014) come into play to deal with noisy extractions. For this reason a recent trend of research has focused on Knowledge Base (KB) completion (Nickel et al., 2012; Bordes et al., 2013), exploiting the fact that distantly supervised OIE and structured knowledge can complement each other. However, the majority of integration approaches nowadays are not designed to deal with many different resources at the same time. We propose an approach where the key idea is to bring together knowledge drawn from an arbitrary number of OIE s</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multi-instance Multi-label Learning for Relation Extraction. In Proceedings of EMNLP-CoNLL, pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Oksana Yakhnenko</author>
<author>Nicolas Usunier</author>
</authors>
<title>Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1366--1371</pages>
<contexts>
<context position="6738" citStr="Weston et al., 2013" startWordPosition="1030" endWordPosition="1033"> data with OIE data by learning latent feature vectors for entities and relations; the KNOWLEDGE VAULT (Dong et al., 2014) uses a graph-based probabilistic framework where prior knowledge from existing resources (e.g. FREEBASE) improves Web extractions by predicting their reliability. However, in both cases the main objective is distantly supervised extraction from unstructured text, rather than KB unification. A recent trend of research focuses on learning embedding models for structured knowledge and their application to tasks like relation extraction and KB completion (Socher et al., 2013; Weston et al., 2013; Bordes et al., 2013). These approaches, however, leverage embeddings at surface level, which are suboptimal for our task, as will be discussed in Section 3. Since we require a common semantic framework for KB unification, we use vector representations based on word senses, which are mapped to a very large sense inventory. This shared sense inventory, then, constitutes the common ground in which disambiguation, alignment and final unification occurs. 3 Knowledge Base Unification: Overview KB-UNIFY takes as input a set of KBs K = {KB1, ..., KBn} and outputs a single, unified and fully disambig</context>
</contexts>
<marker>Weston, Bordes, Yakhnenko, Usunier, 2013</marker>
<rawString>Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction. In Proceedings of EMNLP, pages 1366–1371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open Information Extraction using Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>118--127</pages>
<contexts>
<context position="1667" citStr="Wu and Weld, 2010" startWordPosition="235" endWordPosition="238">luations at each stage, and release output and evaluation data for the use and scrutiny of the community1. 1 Introduction The breakthrough of the Open Information Extraction (OIE) paradigm opened up a research area where Web-scale unconstrained Information Extraction systems are developed to acquire and formalize large quantities of knowledge. However, while successful, to date most state-of-theart OIE systems have been developed with their own type inventories, and no portable ontological structure. In fact, OIE systems can be very different in nature. Early approaches (Etzioni et al., 2008; Wu and Weld, 2010; Fader et al., 2011) focused on extracting a large number of relations from massive unstructured corpora, mostly relying on dependencies at the level of surface text. Systems like NELL (Carlson et al., 2010) combine a hand-crafted taxonomy of entities and relations with self-supervised large-scale extraction 1http://lcl.uniroma1.it/kb-unify from the Web, but they require additional processing for linking and integration (Dutta et al., 2014). More recent work has focused, instead, on deeper language understanding, especially at the level of syntax and semantics (Nakashole et al., 2012; Moro an</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open Information Extraction using Wikipedia. In Proceedings ofACL, pages 118–127.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
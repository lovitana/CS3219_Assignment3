<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.996836">
System Combination for Machine Translation through Paraphrasing
</title>
<author confidence="0.997047">
Wei-Yun Ma
</author>
<affiliation confidence="0.994509">
Institute of Information science
</affiliation>
<address confidence="0.6967785">
Academia Sinica
Taipei 115, Taiwan
</address>
<email confidence="0.994153">
ma@iis.sinica.edu.tw
</email>
<author confidence="0.998005">
Kathleen McKeown
</author>
<affiliation confidence="0.9964975">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.98765">
New York, NY 10027, USA
</address>
<email confidence="0.999181">
kathy@cs.columbia.edu
</email>
<sectionHeader confidence="0.983054" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946391304348">
In this paper, we propose a paraphrasing
model to address the task of system com-
bination for machine translation. We dy-
namically learn hierarchical paraphrases
from target hypotheses and form a syn-
chronous context-free grammar to guide
a series of transformations of target hy-
potheses into fused translations. The
model is able to exploit phrasal and struc-
tural system-weighted consensus and also
to utilize existing information about word
ordering present in the target hypotheses.
In addition, to consider a diverse set of
plausible fused translations, we develop a
hybrid combination architecture, where
we paraphrase every target hypothesis us-
ing different fusing techniques to obtain
fused translations for each target, and
then make the final selection among all
fused translations. Our experimental re-
sults show that our approach can achieve
a significant improvement over combina-
tion baselines.
</bodyText>
<sectionHeader confidence="0.995107" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9898188">
In the past several years, many machine transla-
tion (MT) combination approaches have been
developed. Word-level combination approaches,
such as the confusion network decoding model,
have been quite successful (Matusov et al., 2006;
</bodyText>
<note confidence="0.987897666666667">
Rosti et al., 2007a; He et al. 2008; Karakos et al.
2008; Chen et al. 2009a; Narsale 2010; Leusch
2011; Freitag et al. 2014).
</note>
<bodyText confidence="0.988581130434783">
In addition to word-level combination ap-
proaches, some phrase-level combination ap-
proaches have also recently been developed; the
goal is to retain coherence and consistency be-
tween the words in a phrase. The most common
phrase-level combination approaches are re-
decoding methods: by constructing a new phrase
table from each MT system’s source-to-target
phrase alignments, the source sentence can also
be re-decoded using the new translation table
(Rosti et al., 2007b; Huang and Papineni, 2007;
Chen et al., 2007; Chen et al., 2009b). One prob-
lem with these approaches is that, just with a new
phrase table, existing information about word
ordering present in the target hypotheses is not
utilized; thus the approaches are likely to make
new mistakes of word reordering which do not
appear in the target hypotheses of MT engines.
Huang and Papineni (2007) attacked this issue
through a reordering cost function that encour-
ages search along with decoding paths from all
MT engines’ decoders.
Another phrase-level combination approach
relies on a lattice decoding model to carry out the
combination (Feng et al 2009; Du and Way 2010;
Ma and McKeown 2012). In a lattice, each edge
is associated with a phrase (a single word or a
sequence of words) rather than a single word.
The construction of the lattice is based on the
extraction of phrase pairs from word alignments
between a selected best MT system hypothesis
(the backbone) and the other translation hypothe-
ses. One challenge of the lattice decoding model
is that it is difficult to consider structural consen-
sus among target hypotheses from multiple MT
engines, i.e, the consensus among occurrences of
discontinuous words.
In this paper, we propose another phrase-level
combination approach – a paraphrasing model
using hierarchical paraphrases (paraphrases con-
tain subparaphrases), to fuse target hypotheses.
We dynamically learn hierarchical paraphrases
from target hypotheses without any syntactic an-
notations and form a synchronous context-free
grammar (SCFG) (Aho and Ullman 1969) to
1053
</bodyText>
<note confidence="0.846991">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1053–1058,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9981584375">
guide a series of transformations of target hy-
potheses into fused translations. Through these
structural transformations, the paraphrasing
model is able to exploit phrasal and structural
system-weighted consensus and also able to uti-
lize existing information about word ordering
present in the target hypotheses. In addition, to
consider a diverse set of plausible fused transla-
tions, we develop a hybrid combination architec-
ture, where we paraphrase every target hypothe-
sis using different fusing techniques to obtain
fused translations for each target, and then make
the final selection among all fused translations
through a sentence-level selection-based model.
In short, compared with other related work,
our approach features the following advantages:
</bodyText>
<listItem confidence="0.998342357142857">
1. It can consider structural system-weighted
consensus among target hypotheses from
multiple MT engines through its hierar-
chical paraphrases, which non-hierarchical
paraphrases are not able to do.
2. It can utilize existing information about
word ordering present in the target hy-
potheses.
3. It can retain coherence and consistency
between the words in a phrase.
4. The hybrid combination architecture ena-
bles us to consider a diverse set of plausi-
ble fused translations produced by differ-
ent fusing techniques.
</listItem>
<sectionHeader confidence="0.747047" genericHeader="method">
2 Hybrid Combination Architecture
</sectionHeader>
<bodyText confidence="0.999979944444445">
In the context of system combination, discrimi-
native reranking or post editing, MT researchers
(Rosti et al., 2007a; Huang and Papineni, 2007;
Devlin and Matsoukas, 2012, Matusov et al.,
2008; Gimpel et al., 2013) have recently shown
many positive results if more diverse translations
are considered. Inspired by them, we develop a
hybrid combination architecture in order to con-
sider more diverse fused translations. We para-
phrase every target hypothesis to obtain the cor-
responding fused translation, and then make the
final selection among all fused translations
through a sentence-level selection-based model,
shown in Figure 1. In the architecture, different
fusing techniques can be used to generate fused
translations for the further sentence-level selec-
tion, enabling us to exploit more sophisticated
information of the whole sentence.
</bodyText>
<figure confidence="0.617951">
Best Translation
</figure>
<figureCaption confidence="0.98262">
Figure 1. An example of hybrid combination architecture
</figureCaption>
<sectionHeader confidence="0.989773" genericHeader="method">
3 Paraphrasing Model
</sectionHeader>
<bodyText confidence="0.999983083333333">
In this section, we introduce our paraphrasing
model. For each single target hypothesis, we ex-
tract a set of hierarchical paraphrases from mon-
olingual word alignments between the hypothesis
and other hypotheses. Each set of hierarchical
paraphrases forms a synchronous context-free
grammar to guide a series of transformations of
that target hypothesis into a fused translation.
Any monolingual word aligner can be used to
produce the monolingual word alignments. In
our system, we adopt TERp (Snover et al. 2009),
one of the state-of-the-art alignment tools, to
serve this purpose. TERp is an extension of TER
(Snover et al. 2006). Both TERp and TER are
automatic evaluation metrics for MT, based on
measuring the ratio of the number of edit opera-
tions between the reference sentence and the MT
system hypothesis. The edit operations of TERp
include TER’s Matches, Insertions, Deletions,
Substitutions and Shifts—as well as three new
edit operations: Stem Matches, Synonym Match-
es and Paraphrases. A valuable side product of
TERp is the monolingual word alignment. A
constructed example is shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.999159">
3.1 Hierarchical Paraphrase Extraction
</subsectionHeader>
<bodyText confidence="0.993491846153846">
We first introduce our notation. For a given sen-
tence i, we use to denote the target hypothe-
sis from MT system h, use to denoteEh
attached with related word positions, use to
denote a phrase within Eh , and use to denote
eh attached with related word positions. For in-
stance, If is “you buy the book”, thenEph
would be “you1 buy2 the3 book4”. If is “the
book”, then is “the3 book4”.
For a given sentence i, a MT system h and a
MT system k, we use a SCFG denoted by to
represent the set of hierarchical paraphrases
learned from and Epk . Adapting (Chiang
</bodyText>
<figure confidence="0.991306282051282">
Para-
phrasing
model
Translation from
MT System 1
Fusion
output
Lattice
decoding
Fusion
output
Sentence-level Selection-based Model
Para-
phrasing
model
Translation from
MT System 2
Fusion
output
Lattice
decoding
Fusion
output
...
Para-
phrasing
model
Translation from
MT System N
Fusion
output
Lattice
decoding
Fusion
output
Translations
from all MT
Systems
1054
</figure>
<bodyText confidence="0.994534142857143">
2007), we design the following rules to obtain
Qh,k , based on the monolingual word alignment,
obtained by a aligner, such as TERp.
ed hierarchical paraphrases to paraphrase -
“you1 buy2 the3 book4” are shown in Table 1.
Because of limited space, only part of the para-
phrases, i.e, part of the rules of , are shown.
</bodyText>
<listItem confidence="0.96410625">
• If is consistent1 with the mono-
lingual word alignment, then x -&gt;&lt; eph , ek &gt;
is added to .
• If is in , and is
</listItem>
<bodyText confidence="0.9614560625">
consistent with monolingual word alignment
such that and a = aleka2 , then
X --&gt;&lt; y,Xay2 ,a,Xaa2 &gt; is added to , where
a is an index.
Please note that for each extracted hierarchical
paraphrase - , would include
information of word positions while would not.
For a certain target hypothesis - , our goal
is to paraphrase it to get the fusion output by us-
ing a set of hierarchical paraphrases, denoted by
Qhi . Thus we create the union of all related hier-
archical paraphrases learned from and other
target hypotheses. Two special “glue” rules -_+&lt;s1x21s1x2&gt;
and are also added
to Qi . The process can be represented formally in
the following:
</bodyText>
<equation confidence="0.987013">
N
Qh — i = Qh,ki  {S  S1 X2, S1 X2 ,� S &lt; X1, X1 }
k1
</equation>
<bodyText confidence="0.978705">
where N is the total number of MT systems.
</bodyText>
<figure confidence="0.6383868">
3.1.1 An Example
E1 i
: you buy the book
: the books that you bought
: the book that you bring
</figure>
<figureCaption confidence="0.990265">
Figure 2. A constructed example of a sentence - “你買的書
(the book that you bought)” and its translations from three
</figureCaption>
<bodyText confidence="0.920616">
MT systems –E1i, and , and word alignments be-
tween E2 and E,&apos; , and between and , obtained
through TERp.
We use a Chinese-to-English example in Figure
2 to illustrate the extraction process. The extract-
</bodyText>
<table confidence="0.986321210526316">
1 This means that words in a legal paraphrase are not
aligned to words outside of the paraphrase, and should
include at least one pair of words aligned with each
other.
ofrulesofQZ /&amp;quot;/&apos;7 L 2,2 ?in
1i/n� L&apos;2,1 ? Q2,3
?part
X -+&lt; yod , you&gt; (a) ✓ ✓ ✓
X-+&lt;yodbuy&apos;&apos; ,youbuy&gt; (b) ✓
X -+&lt; you&apos; buy2 , youbougbt &gt; (c) ✓
X -+&lt; yod buy2 , youbring &gt; (d) ✓
X -+&lt; book&apos;, book &gt; (e) ✓ ✓
X -+&lt; book&apos;, books&gt; (f) ✓
X -+&lt; the&apos; book&apos; , the book &gt; (g) ✓ ✓
X -+&lt; the&apos; book&apos; , thebooks&gt; (h) ✓
X _+&lt; yod buy2 the book° , thebooks that youbought &gt; (3, ✓
X -+&lt; X, the&apos; book , thebooks tbat X1 &gt; (j) ✓
X _+&lt; you` buy2 the&apos; X1, the X, that youbougbt &gt; (k) ✓
X -+&lt; X1 the X2, theX2 that X, &gt; (l) ✓ ✓
</table>
<tableCaption confidence="0.9517145">
Table 1. Part of extracted hierarchical paraphrases to para-
phrase EPZ , i.e, part of the rules of .
</tableCaption>
<bodyText confidence="0.982093923076923">
Note that, in Table 1, the rules (j), (k) and (l)
can be regarded as structural paraphrases, and
they utilize existing information about word or-
dering present in the target hypotheses. Since
rule (l) is included in both and Q2 3 , we can
say that rule (l) has more structural consensus
than rule (j) and (k). And rule (l) also models the
word reordering through reversing the order of
X1 and X2. By the example, we can see the rea-
son why our model is able to exploit structural
consensus and also to utilize existing information
about word ordering present in the target hypoth-
eses.
</bodyText>
<subsectionHeader confidence="0.99775">
3.2 Decoding
</subsectionHeader>
<bodyText confidence="0.999080125">
Given a certain target hypothesis - , and its
set of hierarchical paraphrases - , the decoder
aims to paraphrase using by performing a
search for the single most probable derivation via
the CKY algorithm with a Viterbi approximation.
The derivation is the paraphrased result, i.e, the
fusion result indicated in Figure 1. The single
most probable derivation can be represented as
</bodyText>
<figure confidence="0.989312454545455">
I 1 if qhj E Qh
0 otherwise
(Eh)
*length
p *Jl *log(LM(Eh))w
arg
p EPh) arg max
Eh Ah
max log
(Ei
h
J
1
( *f(q k))
k1
ak
�,i
h �
E2i
Ei
3
1055
</figure>
<bodyText confidence="0.999455">
where is the jth paraphrase in used to
generate Eh , J is the number of paraphrases used
to generate . N is the total number of MT sys-
tems. is the weight of MT system k, in charge
of the system-weighted consensus. AP is phrase
penalty. is the LM weight and is word
penalty. All weights are trained discriminatively
for Bleu score using Minimum Error Rate Train-
ing (MERT) procedure (Och 2004).
The ideal result of paraphrasing is shown
in the following, which is supposed to be gener-
ated with a higher chance if, regardless of system
weights. That is because of the use of the rules
with higher degree of structural consensus, such
as (l) and (e).
</bodyText>
<equation confidence="0.849418083333333">
 
S 1
X , X 
2 2
the X that X using rule (l)

4 3
you buy the X , the X that you bought using rule (c)
1 2 3 
4 4
you buy the book , the book that you bought using rule (e)
1 2 3 4 
</equation>
<sectionHeader confidence="0.548808" genericHeader="method">
4 Sentence-Level Selection-based Model
</sectionHeader>
<equation confidence="0.831999636363636">
g max
az
mal * log(LM(Ef )) +
f f
\
m *log( 1- TER
ma
(Ef,Em)))+�\mak *log(1-TER(Ef,Ek)))
m=1 k=1
Our experiments are conducted and report
ed on
</equation>
<bodyText confidence="0.971351888888889">
three datasets: The first dataset includes Chinese-
English system translations and reference trans-
lations from DARPA GALE 2008 (GALE Chi-
Eng). The second dataset includes Chinese-
English system translations and reference trans-
om NIST 2008 (NIST Ara-Eng).
lations and from NIST 2008 (NIST Chi-Eng).
And the third dataset includes Arabic-English
system translations and reference translations
Eng Dataset. And
performs the
best in Bleu score.
Two combination baselines are implemented
for comparison: one is an implementation based
on confusion network decoding, and the other is
Lattice Decoding from (Ma and McKeown 2012),
both of which are using TERp to obtain word
alignments between a selected backbone hypoth-
esis and other target hypotheses. The former uses
these word alignments to construct a confusion
network while the latter extracts phrases which
are consistent with these word alignments to
construct a lattice. For both baselines, backbone
hypotheses are selected sentence by sentence
based on system-weighted consensus among
“rwth-pbt-sh”
slation of all MT systems.
</bodyText>
<sectionHeader confidence="0.602435" genericHeader="evaluation">
5.1 Results
</sectionHeader>
<bodyText confidence="0.9509654">
LD represents Lattice Decoding (Ma an
d McKe-
own 2012); PARA represents paraphrasing mod-
el proposed in this paper; Backbone_* represents
that * is carried out on selected backbones, in
contrast with the hybrid combination architecture.
Arch_LD represents that only lattice decoding is
carried out using hybrid combination architecture.
Arch_PARA represents that only paraphrasing
model is carried out using hybrid combination
</bodyText>
<figure confidence="0.87817375">
rule
using
glue
 X3 the3 X4,


azg max log p (Eif )
M N =
MTSystem# TuneSent# TestSent#
GALE Chi-Eng 5 422 422
NIST Chi-Eng 5 524 788
NIST Ara-Eng 5 592 717
</figure>
<tableCaption confidence="0.990797">
Table 2. Experimental setting
</tableCaption>
<table confidence="0.948510466666667">
MT System Approach Bleu
nrc phrase-based SMT 30.95
rwth-pbt-aml phrase-based SMT + 31.83
source reordering
mentation 31.78
phrase-based SMT + word seg-
rwth-pbt-jx
dering + rescoring 32.63
phrase-based SMT + source reor-
rwth-pbt-sh
sri-hpbt hierarc hical phrase-based SMT 32.00
and fr
Table 3: Techniques of top five MT of GALE Chi-Eng Da-
+ source eordering
taset
</table>
<bodyText confidence="0.974575347826087">
For a given sentence i and its M multiple fusion
outputs -E f
&lt; f &lt; M generated by the para-
phrasing model or the lattice decoding model, the
goal here is to select the best one among them, as
shown in Figure 1 (For the case shown in the
figure, M is
The idea is to compare system-
weighted consensus among all fusion outputs and
translations from all MT systems, and then select
the one with the highest consensus. We adopt
Minimum Bayes Risk (MBR) decoding (Kumar
an
,
2N).
d Byrne, 2004; Sim et al., 2007) to serve our
purpose and develop the following TER-based
MBR:
where TER is Translation Tdit Ratio.wm is the
fusion weight specific to a certain MT system
and a certain fusion model,wk is the weight of
MT system k an
d is the LM weight. All
</bodyText>
<table confidence="0.639854666666667">
weights are trained discriminatively for Bleu
score using MERT.
5 Experiments
</table>
<tableCaption confidence="0.7503715">
Table 3 lists distinguishing machine transla-
tion approaches of top five MT of GALE Chi-
</tableCaption>
<table confidence="0.737577625">
tran
In Table 4, CN represents confusion network;
1056
architecture. Arch_LD_PARA represents that
LD and PARA are both carried out using hybrid
combination architecture, which is the example
shown in Figure 2.
GALE NIST NIST
Chi-Eng Chi-Eng Ara-Eng
Best MT system 32.63 30.16 48.40
Backbone_CN (baseline) 33.04 31.21 48.56
Backbone_LD (baseline) 33.16 32.65 49.33
Backbone_PARA 33.09 32.59 49.46
Arch_LD 33.24 32.66 50.48
Arch_PARA 33.32 32.90 50.20
Arch_LD_PARA 33.72 33.42 50.44
</table>
<tableCaption confidence="0.999822">
Table 4. Experimental results in Bleu score
</tableCaption>
<bodyText confidence="0.999843555555556">
From Table 4, we can first observe that, for
the three datasets, Backbone_PARA and Back-
bone_LD outperform Backbone_CN, which
shows the advantage of using phrases over words
in combination. However, Backbone_PARA
does not show improvement over Backbone_LD.
The reason could be that selected backbones al-
ready have a high level of quality and fewer
words need to be replaced or re-ordered in con-
trast with other target hypotheses.
We find that Arch_PARA performs better than
Backbone_PARA, and Arch_LD performs better
than Backbone_LD. This observation supports
our claim that it is beneficial to consider more
diverse sets of plausible fused translations.
Arch_LD_PARA achieves the best perfor-
mance among all techniques used in this paper. It
not only supports our claim, but also brings a
conclusion that the paraphrasing model and lat-
tice decoding can compensate for the weaknesses
of the other in our architecture.
Since the paraphrasing model uses hierarchical
paraphrases to carry out the fusion, it is able to
make a bigger degree of word-reordering or
structural change on the input hypothesis in
comparison with lattice decoding. We suppose
that when more word-reordering and structural
changes are needed, paraphrasing model can
bring more benefits than lattice decoding. Be-
cause the quality of a given translation hypothe-
sis is highly related to word reordering and struc-
tural change, it can be expected that when a
poorly translated hypothesis is paraphrased, par-
aphrasing model can bring more benefits than
lattice decoding. In order to obtain the evidence
to support this hypothesis, we carried out the fol-
lowing experiment on NIST Chi-Eng Dataset.
For each MT system from the selected top 5
system A-E, we paraphrase its translations using
the paraphrasing model and lattice decoding sep-
arately, aiming to compare the performances of
the two models on each MT system. In other
words, we do not first do backbone selection.
Every MT system’s translation is regarded as a
backbone. The results are shown in Table 5.
</bodyText>
<table confidence="0.999626428571428">
MT Lattice Decod- Paraphrasing
ing model
Sys A 30.16 32.17 31.76
Sys B 30.06 31.93 31.72
Sys C 28.15 30.66 31.00
Sys D 29.94 31.86 31.46
Sys E 29.52 31.52 31.92
</table>
<tableCaption confidence="0.9076465">
Table 5. The Bleu score of each MT system, the Bleu
score of paraphrasing each MT system using lattice decod-
ing and the Bleu score of paraphrasing each MT system
using paraphrasing model.
</tableCaption>
<bodyText confidence="0.999961333333333">
Among the five MT systems, “Sys C” and
“Sys D” perform poorer than the other three MT
systems. When we paraphrase the two systems,
we find that paraphrasing model outperforms
lattice decoding. These results support our hy-
pothesis that when more word-reordering and
structural changes are needed, paraphrasing
model can bring more benefits than lattice de-
coding.
</bodyText>
<sectionHeader confidence="0.998615" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999979">
We view MT combination as a paraphrasing pro-
cess using a set of hierarchical paraphrases, in
which more complicated paraphrasing phenome-
na are able to be modeled, such as phrasal and
structural consensus. Existing information about
word ordering present in the target hypotheses
are also considered. The experimental results
show that our approach can achieve a significant
improvement over combination baselines.
There are many possibilities for enriching the
simple framework. Many ideas from recent
translation developments can be borrowed and
modified for combination. Our future work aims
to incorporate syntactic or semantic information
into our paraphrasing framework.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997639333333333">
We would like to thank the anonymous review-
ers for their valuable comments and suggestions.
This work is supported by the National Science
Foundation via Grant No. 0910778 entitled
“Richer Representations for Machine Transla-
tion”.
</bodyText>
<page confidence="0.724336">
1057
</page>
<sectionHeader confidence="0.763158" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.995584160377358">
A. V. Aho and J. D. Ullman. 1969. Syntax directed
translations and the pushdown assembler. Journal
of Com-puter and System Sciences, 3:37–56.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison.
2007. Multi-engine machine translation with an
open-source SMT decoder. In Proceedings of
WMT07
Boxing Chen, Min Zhang and Aiti Aw. 2009a. A
Comparative Study of Hypothesis Alignment and
its Improvement for Machine Translation System
Combination. In: Proceedings of ACL-IJCNLP. pp.
1067-1074. Singapore. August.
Yu Chen, Michael Jellinghaus, Andreas Eisele, Yi
Zhang, Sabine Hunsicker, Silke Theison, Christian
Federmann, Hans Uszkoreit. 2009b. Combining
Multi-Engine Translations with Moses. In Proceed-
ings of the Fourth Workshop on Statistical Ma-
chine Translation
David Chiang. Hierarchical phrase-based translation.
2007. Computational Linguistics, 33(2):201–228.
J. Devlin and S. Matsoukas. 2012. Trait-based hy-
pothesis selection for machine translation. In Proc.
of NAACL
Jinhua Du and Andy Way. 2010. Using TERp to
Augment the System Combination for SMT. In
Proceedings of the Ninth Conference of the Asso-
ciation for Machine Translation (AMTA2010)
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and
Yajuan Lu. 2009 Lattice-based System Combina-
tion for Statistical Machine Translation. In Pro-
ceedings of ACL
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open source machine translation sys-
tem combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 29–32, Gothenburg, Sweden, April.
Association for Computational Linguistics.
Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory
Shakhnarovich. 2013. A Systematic Exploration of
Diversity in Machine Translation. In Proc. of
EMNLP
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Ngu-
yen, and Robert Moore. 2008. Indirect-hmm-based
hypothesis alignment for computing outputs from
machine translation systems. In Proceedings of
EMNLP
Fei Huang and Kishore Papineni. 2007. Hierarchical
System Combination for Machine Translation. In
Proceed-ings of EMNLP-CoNLL
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation sys-
tem combination using ITG-based alignments. In
Proceedings of ACL-HLT
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of HLT
Wei-Yun Ma and Kathleen McKeown. 2012. Phrase-
level System Combination for Machine Translation
Based on Target-to-Target Decoding. In Proceed-
ings of the 10th Biennial Conference of the Asso-
ciation for Ma-chine Translation in the Americas
(AMTA), San Diego, CA.
Gregor Leusch, Markus Freitag, and Hermann Ney.
The RWTH System Combination System for
WMT 2011. 2011. In Proceedings of the Sixth
Workshop on Statistical Machine Translation
Evgeny Matusov, Nicola Ueffing, and Hermann Ney
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced
hypotheses alignment. In Proceedings of EACL
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D.
Dechelotte, M. Federico, M. Kolss, Y. S. Lee, J. B.
Marino, M. Paulik, S. Roukos, H. Schwenk, and H.
Ney. 2008. System combination for machine trans-
lation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222–1237, September.
Sushant Narsale. JHU System Combination Scheme
for WMT 2010. 2010. In Proceedings of the Fifth
Workshop on Statistical Machine Translation
Franz Josef Och. 2004. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of ACL
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system
combination for machine translation. In Proceed-
ings of ACL
Antti-Veikko I. Rosti, Necip F. Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
J. Dorr. 2007b. Combining outputs from multiple
machine translation systems. In Proceedings of
NAACL-HLT
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine
translation system combination. In Proceedings of
ICASSP
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.
Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
of AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. TER-Plus: Paraphrase, Semantic, and
Alignment Enhancements to Translation Edit Rate.
Machine Translation, 23(2–3):117–127.
</reference>
<page confidence="0.773642">
1058
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327081">
<title confidence="0.999489">System Combination for Machine Translation through Paraphrasing</title>
<author confidence="0.830837">Wei-Yun</author>
<affiliation confidence="0.7211435">Institute of Information Academia</affiliation>
<address confidence="0.719944">Taipei 115, Taiwan</address>
<email confidence="0.800216">ma@iis.sinica.edu.tw</email>
<author confidence="0.984975">Kathleen</author>
<affiliation confidence="0.999898">Department of Computer</affiliation>
<address confidence="0.9042165">Columbia New York, NY 10027, USA</address>
<email confidence="0.999841">kathy@cs.columbia.edu</email>
<abstract confidence="0.998827541666667">In this paper, we propose a paraphrasing model to address the task of system combination for machine translation. We dynamically learn hierarchical paraphrases from target hypotheses and form a synchronous context-free grammar to guide a series of transformations of target hypotheses into fused translations. The model is able to exploit phrasal and structural system-weighted consensus and also to utilize existing information about word ordering present in the target hypotheses. In addition, to consider a diverse set of plausible fused translations, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>Journal of Com-puter and System Sciences,</journal>
<pages>3--37</pages>
<contexts>
<context position="3605" citStr="Aho and Ullman 1969" startWordPosition="544" endWordPosition="547">ckbone) and the other translation hypotheses. One challenge of the lattice decoding model is that it is difficult to consider structural consensus among target hypotheses from multiple MT engines, i.e, the consensus among occurrences of discontinuous words. In this paper, we propose another phrase-level combination approach – a paraphrasing model using hierarchical paraphrases (paraphrases contain subparaphrases), to fuse target hypotheses. We dynamically learn hierarchical paraphrases from target hypotheses without any syntactic annotations and form a synchronous context-free grammar (SCFG) (Aho and Ullman 1969) to 1053 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1053–1058, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. guide a series of transformations of target hypotheses into fused translations. Through these structural transformations, the paraphrasing model is able to exploit phrasal and structural system-weighted consensus and also able to utilize existing information about word ordering present in the target hypotheses. In addition, to consider a diverse set of plausible fused translations, we develop </context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>A. V. Aho and J. D. Ullman. 1969. Syntax directed translations and the pushdown assembler. Journal of Com-puter and System Sciences, 3:37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Chen</author>
<author>Andreas Eisele</author>
<author>Christian Federmann</author>
<author>Eva Hasler</author>
<author>Michael Jellinghaus</author>
<author>Silke Theison</author>
</authors>
<title>Multi-engine machine translation with an open-source SMT decoder.</title>
<date>2007</date>
<booktitle>In Proceedings of WMT07</booktitle>
<contexts>
<context position="2077" citStr="Chen et al., 2007" startWordPosition="304" endWordPosition="307"> et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 200</context>
</contexts>
<marker>Chen, Eisele, Federmann, Hasler, Jellinghaus, Theison, 2007</marker>
<rawString>Yu Chen, Andreas Eisele, Christian Federmann, Eva Hasler, Michael Jellinghaus, and Silke Theison. 2007. Multi-engine machine translation with an open-source SMT decoder. In Proceedings of WMT07</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
</authors>
<title>A Comparative Study of Hypothesis Alignment and its Improvement for Machine Translation System Combination. In:</title>
<date>2009</date>
<booktitle>Proceedings of ACL-IJCNLP.</booktitle>
<pages>1067--1074</pages>
<contexts>
<context position="1511" citStr="Chen et al. 2009" startWordPosition="217" endWordPosition="220">e paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem</context>
</contexts>
<marker>Chen, Zhang, Aw, 2009</marker>
<rawString>Boxing Chen, Min Zhang and Aiti Aw. 2009a. A Comparative Study of Hypothesis Alignment and its Improvement for Machine Translation System Combination. In: Proceedings of ACL-IJCNLP. pp. 1067-1074. Singapore. August.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yu Chen</author>
<author>Michael Jellinghaus</author>
<author>Andreas Eisele</author>
<author>Yi Zhang</author>
<author>Sabine Hunsicker</author>
</authors>
<title>Silke Theison, Christian Federmann, Hans Uszkoreit. 2009b. Combining Multi-Engine Translations with Moses.</title>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation</booktitle>
<marker>Chen, Jellinghaus, Eisele, Zhang, Hunsicker, </marker>
<rawString>Yu Chen, Michael Jellinghaus, Andreas Eisele, Yi Zhang, Sabine Hunsicker, Silke Theison, Christian Federmann, Hans Uszkoreit. 2009b. Combining Multi-Engine Translations with Moses. In Proceedings of the Fourth Workshop on Statistical Machine Translation</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. Hierarchical phrase-based translation. 2007. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Devlin</author>
<author>S Matsoukas</author>
</authors>
<title>Trait-based hypothesis selection for machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL</booktitle>
<contexts>
<context position="5284" citStr="Devlin and Matsoukas, 2012" startWordPosition="788" endWordPosition="791">le MT engines through its hierarchical paraphrases, which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translations for the further sentence-level selection, en</context>
</contexts>
<marker>Devlin, Matsoukas, 2012</marker>
<rawString>J. Devlin and S. Matsoukas. 2012. Trait-based hypothesis selection for machine translation. In Proc. of NAACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinhua Du</author>
<author>Andy Way</author>
</authors>
<title>Using TERp to Augment the System Combination for SMT.</title>
<date>2010</date>
<booktitle>In Proceedings of the Ninth Conference of the Association for Machine Translation (AMTA2010)</booktitle>
<contexts>
<context position="2695" citStr="Du and Way 2010" startWordPosition="406" endWordPosition="409">hen et al., 2009b). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 2009; Du and Way 2010; Ma and McKeown 2012). In a lattice, each edge is associated with a phrase (a single word or a sequence of words) rather than a single word. The construction of the lattice is based on the extraction of phrase pairs from word alignments between a selected best MT system hypothesis (the backbone) and the other translation hypotheses. One challenge of the lattice decoding model is that it is difficult to consider structural consensus among target hypotheses from multiple MT engines, i.e, the consensus among occurrences of discontinuous words. In this paper, we propose another phrase-level combi</context>
</contexts>
<marker>Du, Way, 2010</marker>
<rawString>Jinhua Du and Andy Way. 2010. Using TERp to Augment the System Combination for SMT. In Proceedings of the Ninth Conference of the Association for Machine Translation (AMTA2010)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Yang Liu</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
<author>Yajuan Lu</author>
</authors>
<title>Lattice-based System Combination for Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="2678" citStr="Feng et al 2009" startWordPosition="402" endWordPosition="405">n et al., 2007; Chen et al., 2009b). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 2009; Du and Way 2010; Ma and McKeown 2012). In a lattice, each edge is associated with a phrase (a single word or a sequence of words) rather than a single word. The construction of the lattice is based on the extraction of phrase pairs from word alignments between a selected best MT system hypothesis (the backbone) and the other translation hypotheses. One challenge of the lattice decoding model is that it is difficult to consider structural consensus among target hypotheses from multiple MT engines, i.e, the consensus among occurrences of discontinuous words. In this paper, we propose another p</context>
</contexts>
<marker>Feng, Liu, Mi, Liu, Lu, 2009</marker>
<rawString>Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Yajuan Lu. 2009 Lattice-based System Combination for Statistical Machine Translation. In Proceedings of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Freitag</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open source machine translation system combination.</title>
<date>2014</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>29--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="1561" citStr="Freitag et al. 2014" startWordPosition="225" endWordPosition="228">ferent fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem with these approaches is that, just with a new ph</context>
</contexts>
<marker>Freitag, Huck, Ney, 2014</marker>
<rawString>Markus Freitag, Matthias Huck, and Hermann Ney. 2014. Jane: Open source machine translation system combination. In Conference of the European Chapter of the Association for Computational Linguistics, pages 29–32, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Dhruv Batra</author>
<author>Chris Dyer</author>
<author>Gregory Shakhnarovich</author>
</authors>
<title>A Systematic Exploration of Diversity in Machine Translation.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP</booktitle>
<contexts>
<context position="5328" citStr="Gimpel et al., 2013" startWordPosition="796" endWordPosition="799">, which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translations for the further sentence-level selection, enabling us to exploit more sophisticated info</context>
</contexts>
<marker>Gimpel, Batra, Dyer, Shakhnarovich, 2013</marker>
<rawString>Kevin Gimpel, Dhruv Batra, Chris Dyer, and Gregory Shakhnarovich. 2013. A Systematic Exploration of Diversity in Machine Translation. In Proc. of EMNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmm-based hypothesis alignment for computing outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="1472" citStr="He et al. 2008" startWordPosition="209" endWordPosition="212">rid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al.,</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmm-based hypothesis alignment for computing outputs from machine translation systems. In Proceedings of EMNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Kishore Papineni</author>
</authors>
<title>Hierarchical System Combination for Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceed-ings of EMNLP-CoNLL</booktitle>
<contexts>
<context position="2058" citStr="Huang and Papineni, 2007" startWordPosition="300" endWordPosition="303">6; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combinat</context>
<context position="5256" citStr="Huang and Papineni, 2007" startWordPosition="784" endWordPosition="787">get hypotheses from multiple MT engines through its hierarchical paraphrases, which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translations for the further </context>
</contexts>
<marker>Huang, Papineni, 2007</marker>
<rawString>Fei Huang and Kishore Papineni. 2007. Hierarchical System Combination for Machine Translation. In Proceed-ings of EMNLP-CoNLL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine translation system combination using ITG-based alignments.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT</booktitle>
<contexts>
<context position="1493" citStr="Karakos et al. 2008" startWordPosition="213" endWordPosition="216">architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer. 2008. Machine translation system combination using ITG-based alignments. In Proceedings of ACL-HLT</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT</booktitle>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of HLT</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Yun Ma</author>
<author>Kathleen McKeown</author>
</authors>
<title>Phraselevel System Combination for Machine Translation Based on Target-to-Target Decoding.</title>
<date>2012</date>
<booktitle>In Proceedings of the 10th Biennial Conference of the Association for Ma-chine Translation in the Americas (AMTA),</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="2717" citStr="Ma and McKeown 2012" startWordPosition="410" endWordPosition="413">). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 2009; Du and Way 2010; Ma and McKeown 2012). In a lattice, each edge is associated with a phrase (a single word or a sequence of words) rather than a single word. The construction of the lattice is based on the extraction of phrase pairs from word alignments between a selected best MT system hypothesis (the backbone) and the other translation hypotheses. One challenge of the lattice decoding model is that it is difficult to consider structural consensus among target hypotheses from multiple MT engines, i.e, the consensus among occurrences of discontinuous words. In this paper, we propose another phrase-level combination approach – a pa</context>
<context position="13243" citStr="Ma and McKeown 2012" startWordPosition="2226" endWordPosition="2229">n three datasets: The first dataset includes ChineseEnglish system translations and reference translations from DARPA GALE 2008 (GALE ChiEng). The second dataset includes ChineseEnglish system translations and reference transom NIST 2008 (NIST Ara-Eng). lations and from NIST 2008 (NIST Chi-Eng). And the third dataset includes Arabic-English system translations and reference translations Eng Dataset. And performs the best in Bleu score. Two combination baselines are implemented for comparison: one is an implementation based on confusion network decoding, and the other is Lattice Decoding from (Ma and McKeown 2012), both of which are using TERp to obtain word alignments between a selected backbone hypothesis and other target hypotheses. The former uses these word alignments to construct a confusion network while the latter extracts phrases which are consistent with these word alignments to construct a lattice. For both baselines, backbone hypotheses are selected sentence by sentence based on system-weighted consensus among “rwth-pbt-sh” slation of all MT systems. 5.1 Results LD represents Lattice Decoding (Ma an d McKeown 2012); PARA represents paraphrasing model proposed in this paper; Backbone_* repre</context>
</contexts>
<marker>Ma, McKeown, 2012</marker>
<rawString>Wei-Yun Ma and Kathleen McKeown. 2012. Phraselevel System Combination for Machine Translation Based on Target-to-Target Decoding. In Proceedings of the 10th Biennial Conference of the Association for Ma-chine Translation in the Americas (AMTA), San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Markus Freitag</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH System Combination System for WMT</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation</booktitle>
<marker>Leusch, Freitag, Ney, 2011</marker>
<rawString>Gregor Leusch, Markus Freitag, and Hermann Ney. The RWTH System Combination System for WMT 2011. 2011. In Proceedings of the Sixth Workshop on Statistical Machine Translation</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="1435" citStr="Matusov et al., 2006" startWordPosition="201" endWordPosition="204">usible fused translations, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; H</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proceedings of EACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>G Leusch</author>
<author>R E Banchs</author>
<author>N Bertoldi</author>
<author>D Dechelotte</author>
<author>M Federico</author>
<author>M Kolss</author>
<author>Y S Lee</author>
<author>J B Marino</author>
<author>M Paulik</author>
<author>S Roukos</author>
<author>H Schwenk</author>
<author>H Ney</author>
</authors>
<title>System combination for machine translation of spoken and written language.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>16</volume>
<issue>7</issue>
<contexts>
<context position="5306" citStr="Matusov et al., 2008" startWordPosition="792" endWordPosition="795">erarchical paraphrases, which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translations for the further sentence-level selection, enabling us to exploit m</context>
</contexts>
<marker>Matusov, Leusch, Banchs, Bertoldi, Dechelotte, Federico, Kolss, Lee, Marino, Paulik, Roukos, Schwenk, Ney, 2008</marker>
<rawString>E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee, J. B. Marino, M. Paulik, S. Roukos, H. Schwenk, and H. Ney. 2008. System combination for machine translation of spoken and written language. IEEE Transactions on Audio, Speech and Language Processing, 16(7):1222–1237, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sushant Narsale</author>
</authors>
<title>JHU System Combination Scheme for WMT</title>
<date>2010</date>
<booktitle>In Proceedings of the Fifth Workshop on Statistical Machine Translation</booktitle>
<contexts>
<context position="1526" citStr="Narsale 2010" startWordPosition="221" endWordPosition="222">target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem with these app</context>
</contexts>
<marker>Narsale, 2010</marker>
<rawString>Sushant Narsale. JHU System Combination Scheme for WMT 2010. 2010. In Proceedings of the Fifth Workshop on Statistical Machine Translation</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="11995" citStr="Och 2004" startWordPosition="2002" endWordPosition="2003">ure 1. The single most probable derivation can be represented as I 1 if qhj E Qh 0 otherwise (Eh) *length p *Jl *log(LM(Eh))w arg p EPh) arg max Eh Ah max log (Ei h J 1 ( *f(q k)) k1 ak �,i h � E2i Ei 3 1055 where is the jth paraphrase in used to generate Eh , J is the number of paraphrases used to generate . N is the total number of MT systems. is the weight of MT system k, in charge of the system-weighted consensus. AP is phrase penalty. is the LM weight and is word penalty. All weights are trained discriminatively for Bleu score using Minimum Error Rate Training (MERT) procedure (Och 2004). The ideal result of paraphrasing is shown in the following, which is supposed to be generated with a higher chance if, regardless of system weights. That is because of the use of the rules with higher degree of structural consensus, such as (l) and (e).   S 1 X , X  2 2 the X that X using rule (l)  4 3 you buy the X , the X that you bought using rule (c) 1 2 3  4 4 you buy the book , the book that you bought using rule (e) 1 2 3 4  4 Sentence-Level Selection-based Model g max az mal * log(LM(Ef )) + f f \ m *log( 1- TER ma (Ef,Em)))+�\mak *log(1-TER(Ef,Ek))) m=1 k=1 Our experiments a</context>
</contexts>
<marker>Och, 2004</marker>
<rawString>Franz Josef Och. 2004. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1455" citStr="Rosti et al., 2007" startWordPosition="205" endWordPosition="208">ons, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2</context>
<context position="5229" citStr="Rosti et al., 2007" startWordPosition="780" endWordPosition="783">d consensus among target hypotheses from multiple MT engines through its hierarchical paraphrases, which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused tr</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard Schwartz. 2007a. Improved word-level system combination for machine translation. In Proceedings of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Necip F Ayan</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="1455" citStr="Rosti et al., 2007" startWordPosition="205" endWordPosition="208">ons, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2</context>
<context position="5229" citStr="Rosti et al., 2007" startWordPosition="780" endWordPosition="783">d consensus among target hypotheses from multiple MT engines through its hierarchical paraphrases, which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused tr</context>
</contexts>
<marker>Rosti, Ayan, Xiang, Matsoukas, Schwartz, Dorr, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Necip F. Ayan, Bing Xiang, Spyros Matsoukas, Richard Schwartz, and Bonnie J. Dorr. 2007b. Combining outputs from multiple machine translation systems. In Proceedings of NAACL-HLT</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William J Byrne</author>
<author>Mark J F Gales</author>
<author>Hichem Sahbi</author>
<author>Phil C Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context position="15196" citStr="Sim et al., 2007" startWordPosition="2551" endWordPosition="2554">cal phrase-based SMT 32.00 and fr Table 3: Techniques of top five MT of GALE Chi-Eng Da+ source eordering taset For a given sentence i and its M multiple fusion outputs -E f &lt; f &lt; M generated by the paraphrasing model or the lattice decoding model, the goal here is to select the best one among them, as shown in Figure 1 (For the case shown in the figure, M is The idea is to compare systemweighted consensus among all fusion outputs and translations from all MT systems, and then select the one with the highest consensus. We adopt Minimum Bayes Risk (MBR) decoding (Kumar an , 2N). d Byrne, 2004; Sim et al., 2007) to serve our purpose and develop the following TER-based MBR: where TER is Translation Tdit Ratio.wm is the fusion weight specific to a certain MT system and a certain fusion model,wk is the weight of MT system k an d is the LM weight. All weights are trained discriminatively for Bleu score using MERT. 5 Experiments Table 3 lists distinguishing machine translation approaches of top five MT of GALE Chitran In Table 4, CN represents confusion network; 1056 architecture. Arch_LD_PARA represents that LD and PARA are both carried out using hybrid combination architecture, which is the example show</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William J. Byrne, Mark J.F. Gales, Hichem Sahbi, and Phil C. Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In Proceedings of ICASSP</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="6684" citStr="Snover et al. 2006" startWordPosition="1000" endWordPosition="1003">ion, we introduce our paraphrasing model. For each single target hypothesis, we extract a set of hierarchical paraphrases from monolingual word alignments between the hypothesis and other hypotheses. Each set of hierarchical paraphrases forms a synchronous context-free grammar to guide a series of transformations of that target hypothesis into a fused translation. Any monolingual word aligner can be used to produce the monolingual word alignments. In our system, we adopt TERp (Snover et al. 2009), one of the state-of-the-art alignment tools, to serve this purpose. TERp is an extension of TER (Snover et al. 2006). Both TERp and TER are automatic evaluation metrics for MT, based on measuring the ratio of the number of edit operations between the reference sentence and the MT system hypothesis. The edit operations of TERp include TER’s Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and Paraphrases. A valuable side product of TERp is the monolingual word alignment. A constructed example is shown in Figure 2. 3.1 Hierarchical Paraphrase Extraction We first introduce our notation. For a given sentence i, we use to denote the targ</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>TER-Plus: Paraphrase, Semantic, and Alignment Enhancements to Translation Edit Rate. Machine Translation,</title>
<date>2009</date>
<pages>23--2</pages>
<contexts>
<context position="6566" citStr="Snover et al. 2009" startWordPosition="980" endWordPosition="983">e sentence. Best Translation Figure 1. An example of hybrid combination architecture 3 Paraphrasing Model In this section, we introduce our paraphrasing model. For each single target hypothesis, we extract a set of hierarchical paraphrases from monolingual word alignments between the hypothesis and other hypotheses. Each set of hierarchical paraphrases forms a synchronous context-free grammar to guide a series of transformations of that target hypothesis into a fused translation. Any monolingual word aligner can be used to produce the monolingual word alignments. In our system, we adopt TERp (Snover et al. 2009), one of the state-of-the-art alignment tools, to serve this purpose. TERp is an extension of TER (Snover et al. 2006). Both TERp and TER are automatic evaluation metrics for MT, based on measuring the ratio of the number of edit operations between the reference sentence and the MT system hypothesis. The edit operations of TERp include TER’s Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and Paraphrases. A valuable side product of TERp is the monolingual word alignment. A constructed example is shown in Figure 2. 3.1</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009. TER-Plus: Paraphrase, Semantic, and Alignment Enhancements to Translation Edit Rate. Machine Translation, 23(2–3):117–127.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
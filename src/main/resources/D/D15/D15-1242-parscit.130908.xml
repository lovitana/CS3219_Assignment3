<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.086138">
<title confidence="0.933366">
Specializing Word Embeddings for Similarity or Relatedness
</title>
<author confidence="0.993294">
Douwe Kiela, Felix Hill and Stephen Clark
</author>
<affiliation confidence="0.994982">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.98079">
douwe.kiela|felix.hill|stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.99722" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977076923077">
We demonstrate the advantage of special-
izing semantic word embeddings for either
similarity or relatedness. We compare two
variants of retrofitting and a joint-learning
approach, and find that all three yield spe-
cialized semantic spaces that capture hu-
man intuitions regarding similarity and re-
latedness better than unspecialized spaces.
We also show that using specialized spaces
in NLP tasks and applications leads to clear
improvements, for document classification
and synonym selection, which rely on ei-
ther similarity or relatedness but not both.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953677419355">
Most current models of semantic word representa-
tion exploit the distributional hypothesis: the idea
that words occurring in similar contexts have sim-
ilar meanings (Harris, 1954; Turney and Pantel,
2010; Clark, 2015). Such representations (or em-
beddings) can reflect human intuitions about simi-
larity and relatedness (Turney, 2006; Agirre et al.,
2009), and have been applied to a wide variety
of NLP tasks, including bilingual lexicon induc-
tion (Mikolov et al., 2013b), sentiment analysis
(Socher et al., 2013) and named entity recognition
(Turian et al., 2010; Guo et al., 2014).
Arguably, one of the reasons behind the popu-
larity of word embeddings is that they are “gen-
eral purpose”: they can be used in a variety of
tasks without modification. Although this behav-
ior is sometimes desirable, it may in other cases be
detrimental to downstream performance. For ex-
ample, when classifying documents by topic, we
are particularly interested in related words rather
than similar ones: knowing that dog is associated
with cat is much more informative of the topic
than knowing that it is a synonym of canine. Con-
versely, if our embeddings indicate that table is
closely related to chair, that does not mean we
should translate table into French as chaise.
This distinction between “genuine” similarity
and associative similarity (i.e., relatedness) is
well-known in cognitive science (Tversky, 1977).
In NLP, however, semantic spaces are generally
evaluated on how well they capture both similar-
ity and relatedness, even though, for many word
combinations (such as car and petrol), these two
objectives are mutually incompatible (Hill et al.,
2014b). In part, this oversight stems from the dis-
tributional hypothesis itself: car and petrol do not
have the same, or even very similar, meanings,
but these two words may well occur in similar
contexts. Corpus-driven approaches based on the
distributional hypothesis therefore generally learn
embeddings that capture both similarity and relat-
edness reasonably well, but neither perfectly.
In this work we demonstrate the advantage of
specializing semantic spaces for either similar-
ity or relatedness. Specializing for similarity is
achieved by learning from both a corpus and a
thesaurus, and for relatedness by learning from
both a corpus and a collection of psychological as-
sociation norms. We also compare the recently-
introduced technique of graph-based retrofitting
(Faruqui et al., 2015) with a skip-gram retrofitting
and a skip-gram joint-learning approach. All three
methods yield specialized semantic spaces that
capture human intuitions regarding similarity and
relatedness significantly better than unspecialized
spaces, in one case yielding state-of-the-art results
for word similarity. More importantly, we show
clear improvements in downstream tasks and ap-
plications: specialized similarity spaces improve
synonym detection, while association spaces work
better than both general-purpose and similarity-
specialized spaces for document classification.
</bodyText>
<sectionHeader confidence="0.991503" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.99980625">
The underlying assumption of our approach is
that, during training, word embeddings can be
“nudged” in a particular direction by includ-
ing information from an additional semantic data
</bodyText>
<page confidence="0.948715">
2044
</page>
<note confidence="0.652102">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2044–2048,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999496166666667">
source. For directing embeddings towards genuine
similarity, we use the MyThes thesaurus devel-
oped by the OpenOffice.org project1. It contains
synonyms for almost 80,000 words in English. For
directing embeddings towards relatedness, we use
the University of South Florida (USF) free asso-
ciation norms (Nelson et al., 2004). This dataset
contains scores for free association (an experi-
mental measure of cognitive association) of over
10,000 concept words. For raw text data we use a
dump of the English Wikipedia plus newswire text
(8 billion words in total)2.
</bodyText>
<subsectionHeader confidence="0.999514">
2.1 Evaluations (Intrinsic and Extrinsic)
</subsectionHeader>
<bodyText confidence="0.999986466666667">
For instrinsic comparisons with human judge-
ments, we evaluate on SimLex (Hill et al.,
2014b) (999 pairwise comparisons), which explic-
itly measures similarity, and MEN (Bruni et al.,
2014) (3000 comparisons), which explicitly mea-
sures relatedness. We also consider two down-
stream tasks and applications. In the TOEFL
synonym selection task (Landauer and Dumais,
1997), the objective is to select the correct syn-
onym for a target word from a multiple-choice set
of possible answers. For a more extrinsic evalua-
tion, we use a document classification task based
on the Reuters Corpus Volume 1 (RCV1) (Lewis
et al., 2004). This dataset consists of over 800,000
manually categorized news articles.3
</bodyText>
<subsectionHeader confidence="0.9993">
2.2 Joint Learning
</subsectionHeader>
<bodyText confidence="0.999760666666667">
The standard skip-gram training objective for a se-
quence of training words w1, w2, ..., wT and a con-
text size c is the log-likelihood criterion:
</bodyText>
<equation confidence="0.99886975">
T
T
1 � Jθ(wt) = T
t=1 t=1
</equation>
<bodyText confidence="0.744169">
where p(wt+j|wt) is obtained via the softmax:
</bodyText>
<equation confidence="0.952226333333333">
T
uwt+j vwt
Ew&apos; expuTw&apos;vwt
</equation>
<bodyText confidence="0.997618">
where uw and vw are the context and target vec-
tor representations for word w, respectively, and
w1 ranges over the full vocabulary (Mikolov et al.,
</bodyText>
<footnote confidence="0.826880833333333">
1https://www.openoffice.org/lingucomponent/thesaurus.html
2The script for obtaining this corpus is available from
http://word2vec.googlecode.com/svn/trunk/demo-train-big-model-v1.sh
3We exclude articles with multiple topic labels in order to
avoid multi-class document classification. The dataset con-
tains a total of 78 topic labels and 33,226 news articles.
</footnote>
<bodyText confidence="0.999947571428571">
2013a). For our joint learning approach, we sup-
plement the skip-gram objective with additional
contexts (synonyms or free-associates) from an
external data source. In the sampling condition,
for target word wt, we modify the objective to in-
clude an additional context wa sampled uniformly
from the set of additional contexts Awt:
</bodyText>
<equation confidence="0.965817">
(Jθ(wt) + [wa — UAwt] log p(wa|wt))
</equation>
<bodyText confidence="0.9975785">
In the all condition, all additional contexts for a
target word are added at each occurrence:
</bodyText>
<equation confidence="0.982868">
⎛ ⎞
�
⎝Jθ(wt) + log p(wa|wt) ⎠
waEAwt
</equation>
<bodyText confidence="0.9999335">
The set of additional contexts Awt contains the
relevant contexts for a word wt; e.g., for the word
dog, Adog for the thesaurus is the set of all syn-
onyms of dog in the thesaurus.
</bodyText>
<subsectionHeader confidence="0.997862">
2.3 Retrofitting
</subsectionHeader>
<bodyText confidence="0.999764363636364">
Faruqui et al. (2015) introduced retrofitting as a
post-hoc graph-based learning objective that im-
proves learned word embeddings. We experi-
ment with their method, calling it graph-based
retrofitting. In addition, we introduce a similar ap-
proach that instead uses the same objective func-
tion that was used to learn the original skip-gram
embeddings. In other words, we first train a stan-
dard skip-gram model, and then learn from the ad-
ditional contexts in a second training stage as if
they form a separate corpus:
</bodyText>
<equation confidence="0.9937595">
1 T � log p(wa|wt)
T t=1 waEAwt
</equation>
<bodyText confidence="0.99830825">
We call this approach skip-gram retrofitting. In
all cases, our embeddings have 300 dimensions,
which has been found to work well (Mikolov et
al., 2013a; Baroni et al., 2014)
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
3 Results for Intrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.9999804">
We compare standard skip-gram embeddings with
retrofitted and jointly learned specialized embed-
dings, as well as with “fitted” embeddings that
were randomly initialized and learned only from
the additional semantic resource. In each case, the
</bodyText>
<equation confidence="0.998816083333333">
1
T
� log p(wt+j|wt)
−c&lt;j&lt;c
p(wt+j|wt) =
exp
1
T
T
t=1
1 T
T t=1
</equation>
<page confidence="0.936229">
2045
</page>
<table confidence="0.9998605">
Method SimLex-999 MEN
Skip-gram 0.31 0.68
Fit-Norms 0.08 0.14
Fit-Thesaurus 0.26 0.14
Joint-Norms-Sampled 0.43 0.72
Joint-Norms-All 0.42 0.67
Joint-Thesaurus-Sampled 0.38 0.69
Joint-Thesaurus-All 0.44 0.60
GB-Retrofit-Norms 0.32 0.71
GB-Retrofit-Thesaurus 0.38 0.68
SG-Retrofit-Norms 0.35 0.71
SG-Retrofit-Thesaurus 0.47 0.69
</table>
<tableCaption confidence="0.9553215">
Table 1: Spearman ρ on a genuine similarity
(SimLex-999) and relatedness (MEN) dataset.
</tableCaption>
<bodyText confidence="0.999647625">
training algorithm was run for a single iteration
(results from more iterations are presented later).
As shown in Table 1, embeddings that were
specialized for similarity using a thesaurus per-
form better on SimLex-999, and those special-
ized for relatedness using association data per-
form better on MEN. Fitting, or learning only from
the additional semantic resource without access
to raw text, does not perform well. Skip-gram
retrofitting with the thesaurus performs best on
SimLex-999; joint learning with sampling from
the USF norms performs best on MEN, although
the two retrofitting approaches are close. There
is an interesting difference between the two joint
learning approaches: while sampling a single
free associate as additional context works best
for relatedness, presenting all additional contexts
(synonyms) works best for similarity. In both
cases, skip-gram retrofitting matches or outper-
forms graph-based retrofitting.
More training iterations All the results above
were obtained using a single training iteration.
When retrofitting, however, it is easy to learn from
multiple iterations of the thesaurus or the USF
norms. The results are shown in Figure 1, where
the dashed lines are the joint learning and standard
skip-gram results for comparison with retrofitting
scores. As would be expected, too many iterations
leads to overfitting on the semantic resource, with
performance eventually decreasing after the ini-
tial increase. The results show that retrofitting is
particularly useful for similarity, as indicated by
the large increase in performance on SimLex-999.
The highest performance obtained, at 5 iterations,
is a Spearman ρs correlation of 0.53, which, as far
as we know, matches the current state-of-the-art.4
For relatedness-specific embeddings, the effect
is less clear: joint learning performs compara-
tively much better. Retrofitting does outperform
it, at around 2-10 iterations on the USF norms,
but the improvement is marginal. The highest
retrofitting score is 0.74; the highest joint learn-
ing score is 0.72. Both are highly competitive re-
sults on MEN, and outperform e.g. GloVe at 0.71
(Pennington et al., 2014). Joint learning with a
thesaurus, however, leads to poor performance on
MEN, as expected: the embeddings get dragged
away from relatedness and towards similarity.
</bodyText>
<subsectionHeader confidence="0.995047">
3.1 Curriculum learning?
</subsectionHeader>
<bodyText confidence="0.999996">
The fact that joint learning works better when sup-
plementing raw text input with free associates, but
skip-gram retrofitting works better with additional
thesaurus information, could be due to curriculum
learning effects (Bengio et al., 2009). Unlike the
USF norms, many of the words from the thesaurus
are unusual and have low frequency. This suggests
that the thesaurus is more ‘advanced’ (from the
perspective of the learning model) than the USF
norms as an information source. Its information
may be detrimental to model optimization when
encountered early in training (in the joint learning
condition) because the model has not acquired the
basic concepts on which it builds. However, with
retrofitting the model first acquires good represen-
tations for frequent words from the raw text, after
which it can better understand, and learn from, the
information in the thesaurus.
</bodyText>
<sectionHeader confidence="0.953696" genericHeader="method">
4 Downstream Tasks and Applications
</sectionHeader>
<subsectionHeader confidence="0.956207">
4.1 TOEFL Synonym Task
</subsectionHeader>
<bodyText confidence="0.999865916666667">
Unsupervised synonym selection has many appli-
cations including the generation of thesauri and
other lexical resources from raw text (Kageura et
al., 2000). In the well-known TOEFL evalua-
tion (Freitag et al., 2005) models are required to
identify true synonyms to question words from a
selection of possible answers. To test our models
on this task, for each question in the dataset, we
rank the multiple-choice answers according to the
cosine similarity between their word embeddings
and that of the target word, and choose the highest-
ranked option.
</bodyText>
<footnote confidence="0.9865775">
4Hill et al. (2014a) obtain a score of 0.52 using neural
translation embeddings.
</footnote>
<page confidence="0.989899">
2046
</page>
<figureCaption confidence="0.999134">
Figure 1: Varying the number of iterations when retrofitting
</figureCaption>
<table confidence="0.9996089">
Method TOEFL Doc
Skip-gram 77.50 83.96
Joint-Norms-Sampled 78.75 84.46
Joint-Norms-All 66.25 84.82
Joint-Thesaurus-Sampled 81.25 83.90
Joint-Thesaurus-All 80.00 83.56
GB-Retrofit-Norms 80.00 80.58
GB-Retrofit-Thesaurus 83.75 80.24
SG-Retrofit-Norms 80.00 84.56
SG-Retrofit-Thesaurus 88.75 84.55
</table>
<tableCaption confidence="0.993063">
Table 2: TOEFL synonym selection and docu-
</tableCaption>
<bodyText confidence="0.978187555555555">
ment classification accuracy (percentage of cor-
rectly answered questions/correctly classified doc-
uments).
As Table 2 shows, similarity-specialized em-
beddings perform much better than standard em-
beddings and relatedness-specialized embeddings.
Retrofitting outperforms joint learning, and skip-
gram retrofitting matches or outperforms graph-
based retrofitting.
</bodyText>
<subsectionHeader confidence="0.995896">
4.2 Document Classification
</subsectionHeader>
<bodyText confidence="0.999992916666667">
To investigate how well the various semantic
spaces perform on document classification, we
first construct document-level representations by
summing the vector representations for all words
in a given document. After setting aside a small
development set for tuning the hyperparameters of
the supervised algorithm, we train a support vec-
tor machine (SVM) classifier with a linear kernel
and evaluate document topic classification accu-
racy using ten-fold cross-validation.
The results are reported in the rightmost col-
umn of Table 2. Relatedness-specialized embed-
dings perform better on document topic classi-
fication than similarity embeddings, except with
graph-based retrofitting, which in fact performs
below the standard skip-gram model. The joint-
learning model with all relevant free association
norms presented as context for each target word is
the best performing model. The differences in the
table appear small, but the dataset contains more
than 10,000 documents, so every percentage point
is worth more than 100 documents. Joint learning
while presenting all relevant association norms for
each target word performs best on this task.
</bodyText>
<sectionHeader confidence="0.996463" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999944375">
We have demonstrated the advantage of special-
izing embeddings for the tasks of genuine simi-
larity and relatedness. In doing so, we compared
two retrofitting methods and a joint learning ap-
proach. Specialized embeddings outperform stan-
dard embeddings by a large margin on instrinsic
similarity and relatedness evaluations. We showed
that the difference in how embeddings are spe-
cialized carries to downstream NLP tasks, demon-
strating that similarity embeddings are better at
the TOEFL synonym selection task and related-
ness embeddings at a document topic classifica-
tion task. Lastly, we varied the number of itera-
tions that we use for retrofitting, showing that per-
formance could be improved even further by going
over several iterations of the semantic resource.
</bodyText>
<sectionHeader confidence="0.998846" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.887076">
DK is supported by EPSRC grant EP/I037512/1.
FH is supported by St Johns College, Cambridge.
SC is supported by ERC Starting Grant DisCoTex
</bodyText>
<page confidence="0.981656">
2047
</page>
<bodyText confidence="0.91421875">
(306920) and EPSRC grant EP/I037512/1. We
thank Yoshua Bengio, Kyunghyun Cho and Ivan
Vuli´c for useful discussions and the anonymous
reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.99511" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999731852631579">
Eneko Agirre, Enrique Alfonseca, Keith B. Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In NAACL,
pages 19–27.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In ACL, pages
238–247.
Yoshua Bengio, J´erˆome Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Proceedings of the 26th annual international confer-
ence on machine learning, pages 41–48. ACM.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artifical Intelligence Research, 49:1–47.
Stephen Clark. 2015. Vector Space Models of Lexical
Meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, chapter 16.
Wiley-Blackwell, Oxford.
Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
Proceedings of NAACL.
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distribu-
tional representations of synonymy. In Proceedings
of the Ninth Conference on Computational Natural
Language Learning, pages 25–32.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 110–120.
Zelig Harris. 1954. Distributional Structure. Word,
10(23):146—162.
Felix Hill, Kyunghyun Cho, S´ebastien Jean, Coline
Devin, and Yoshua Bengio. 2014a. Embedding
word similarity with neural machine translation.
CoRR, abs/1412.6448.
Felix Hill, Roi Reichart, and Anna Korhonen.
2014b. SimLex-999: Evaluating semantic mod-
els with (genuine) similarity estimation. CoRR,
abs/1408.3456.
Kyo Kageura, Keita Tsuji, and Akiko N Aizawa. 2000.
Automatic thesaurus generation through multiple
filtering. In Proceedings of the 18th conference
on Computational linguistics-Volume 1, pages 397–
403.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. RCV1: A new benchmark collection for
text categorization research. The Journal of Ma-
chine Learning Research, 5:361–397.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word repre-
sentations in vector space. In Proceedings of ICLR,
Scottsdale, Arizona, USA.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages
for machine translation. In Proceedings of ICLR,
Scottsdale, Arizona, USA.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The University of South Florida
free association, rhyme, and word fragment norms.
Behavior Research Methods, Instruments, &amp; Com-
puters, 36(3):402–407.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12:1532–1543.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, Seattle, WA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL, pages 384–394.
Peter D. Turney and Patrick Pantel. 2010. From
Frequency to Meaning: vector space models of se-
mantics. Journal of Artifical Intelligence Research,
37(1):141–188, January.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379–416.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4).
</reference>
<page confidence="0.994052">
2048
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928905">
<title confidence="0.99976">Specializing Word Embeddings for Similarity or Relatedness</title>
<author confidence="0.989898">Douwe Kiela</author>
<author confidence="0.989898">Felix Hill</author>
<author confidence="0.989898">Stephen</author>
<affiliation confidence="0.9946085">Computer University of</affiliation>
<email confidence="0.993088">douwe.kiela|felix.hill|stephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.996658285714286">We demonstrate the advantage of specializing semantic word embeddings for either similarity or relatedness. We compare two variants of retrofitting and a joint-learning approach, and find that all three yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness better than unspecialized spaces. We also show that using specialized spaces and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith B Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1122" citStr="Agirre et al., 2009" startWordPosition="153" endWordPosition="156">dness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related </context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith B. Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In NAACL, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="7654" citStr="Baroni et al., 2014" startWordPosition="1163" endWordPosition="1166">oves learned word embeddings. We experiment with their method, calling it graph-based retrofitting. In addition, we introduce a similar approach that instead uses the same objective function that was used to learn the original skip-gram embeddings. In other words, we first train a standard skip-gram model, and then learn from the additional contexts in a second training stage as if they form a separate corpus: 1 T � log p(wa|wt) T t=1 waEAwt We call this approach skip-gram retrofitting. In all cases, our embeddings have 300 dimensions, which has been found to work well (Mikolov et al., 2013a; Baroni et al., 2014) 3 Results for Intrinsic Evaluation We compare standard skip-gram embeddings with retrofitted and jointly learned specialized embeddings, as well as with “fitted” embeddings that were randomly initialized and learned only from the additional semantic resource. In each case, the 1 T � log p(wt+j|wt) −c&lt;j&lt;c p(wt+j|wt) = exp 1 T T t=1 1 T T t=1 2045 Method SimLex-999 MEN Skip-gram 0.31 0.68 Fit-Norms 0.08 0.14 Fit-Thesaurus 0.26 0.14 Joint-Norms-Sampled 0.43 0.72 Joint-Norms-All 0.42 0.67 Joint-Thesaurus-Sampled 0.38 0.69 Joint-Thesaurus-All 0.44 0.60 GB-Retrofit-Norms 0.32 0.71 GB-Retrofit-Thesa</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>J´erˆome Louradour</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>Curriculum learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th annual international conference on machine learning,</booktitle>
<pages>41--48</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11001" citStr="Bengio et al., 2009" startWordPosition="1658" endWordPosition="1661">al. The highest retrofitting score is 0.74; the highest joint learning score is 0.72. Both are highly competitive results on MEN, and outperform e.g. GloVe at 0.71 (Pennington et al., 2014). Joint learning with a thesaurus, however, leads to poor performance on MEN, as expected: the embeddings get dragged away from relatedness and towards similarity. 3.1 Curriculum learning? The fact that joint learning works better when supplementing raw text input with free associates, but skip-gram retrofitting works better with additional thesaurus information, could be due to curriculum learning effects (Bengio et al., 2009). Unlike the USF norms, many of the words from the thesaurus are unusual and have low frequency. This suggests that the thesaurus is more ‘advanced’ (from the perspective of the learning model) than the USF norms as an information source. Its information may be detrimental to model optimization when encountered early in training (in the joint learning condition) because the model has not acquired the basic concepts on which it builds. However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn fr</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41–48. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="4962" citStr="Bruni et al., 2014" startWordPosition="727" endWordPosition="730">ords in English. For directing embeddings towards relatedness, we use the University of South Florida (USF) free association norms (Nelson et al., 2004). This dataset contains scores for free association (an experimental measure of cognitive association) of over 10,000 concept words. For raw text data we use a dump of the English Wikipedia plus newswire text (8 billion words in total)2. 2.1 Evaluations (Intrinsic and Extrinsic) For instrinsic comparisons with human judgements, we evaluate on SimLex (Hill et al., 2014b) (999 pairwise comparisons), which explicitly measures similarity, and MEN (Bruni et al., 2014) (3000 comparisons), which explicitly measures relatedness. We also consider two downstream tasks and applications. In the TOEFL synonym selection task (Landauer and Dumais, 1997), the objective is to select the correct synonym for a target word from a multiple-choice set of possible answers. For a more extrinsic evaluation, we use a document classification task based on the Reuters Corpus Volume 1 (RCV1) (Lewis et al., 2004). This dataset consists of over 800,000 manually categorized news articles.3 2.2 Joint Learning The standard skip-gram training objective for a sequence of training words </context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artifical Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector Space Models of Lexical Meaning.</title>
<date>2015</date>
<booktitle>Handbook of Contemporary Semantics, chapter 16. Wiley-Blackwell,</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<location>Oxford.</location>
<contexts>
<context position="986" citStr="Clark, 2015" startWordPosition="135" endWordPosition="136">pproach, and find that all three yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other case</context>
</contexts>
<marker>Clark, 2015</marker>
<rawString>Stephen Clark. 2015. Vector Space Models of Lexical Meaning. In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics, chapter 16. Wiley-Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Jesse Dodge</author>
<author>Sujay K Jauhar</author>
<author>Chris Dyer</author>
<author>Eduard Hovy</author>
<author>Noah A Smith</author>
</authors>
<title>Retrofitting word vectors to semantic lexicons.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="3206" citStr="Faruqui et al., 2015" startWordPosition="480" endWordPosition="483">well occur in similar contexts. Corpus-driven approaches based on the distributional hypothesis therefore generally learn embeddings that capture both similarity and relatedness reasonably well, but neither perfectly. In this work we demonstrate the advantage of specializing semantic spaces for either similarity or relatedness. Specializing for similarity is achieved by learning from both a corpus and a thesaurus, and for relatedness by learning from both a corpus and a collection of psychological association norms. We also compare the recentlyintroduced technique of graph-based retrofitting (Faruqui et al., 2015) with a skip-gram retrofitting and a skip-gram joint-learning approach. All three methods yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness significantly better than unspecialized spaces, in one case yielding state-of-the-art results for word similarity. More importantly, we show clear improvements in downstream tasks and applications: specialized similarity spaces improve synonym detection, while association spaces work better than both general-purpose and similarityspecialized spaces for document classification. 2 Approach The underlying ass</context>
<context position="6955" citStr="Faruqui et al. (2015)" startWordPosition="1046" endWordPosition="1049">contexts (synonyms or free-associates) from an external data source. In the sampling condition, for target word wt, we modify the objective to include an additional context wa sampled uniformly from the set of additional contexts Awt: (Jθ(wt) + [wa — UAwt] log p(wa|wt)) In the all condition, all additional contexts for a target word are added at each occurrence: ⎛ ⎞ � ⎝Jθ(wt) + log p(wa|wt) ⎠ waEAwt The set of additional contexts Awt contains the relevant contexts for a word wt; e.g., for the word dog, Adog for the thesaurus is the set of all synonyms of dog in the thesaurus. 2.3 Retrofitting Faruqui et al. (2015) introduced retrofitting as a post-hoc graph-based learning objective that improves learned word embeddings. We experiment with their method, calling it graph-based retrofitting. In addition, we introduce a similar approach that instead uses the same objective function that was used to learn the original skip-gram embeddings. In other words, we first train a standard skip-gram model, and then learn from the additional contexts in a second training stage as if they form a separate corpus: 1 T � log p(wa|wt) T t=1 waEAwt We call this approach skip-gram retrofitting. In all cases, our embeddings </context>
</contexts>
<marker>Faruqui, Dodge, Jauhar, Dyer, Hovy, Smith, 2015</marker>
<rawString>Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris Dyer, Eduard Hovy, and Noah A Smith. 2015. Retrofitting word vectors to semantic lexicons. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
<author>Matthias Blume</author>
<author>John Byrnes</author>
<author>Edmond Chow</author>
<author>Sadik Kapadia</author>
<author>Richard Rohwer</author>
<author>Zhiqiang Wang</author>
</authors>
<title>New experiments in distributional representations of synonymy.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="11911" citStr="Freitag et al., 2005" startWordPosition="1802" endWordPosition="1805">zation when encountered early in training (in the joint learning condition) because the model has not acquired the basic concepts on which it builds. However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn from, the information in the thesaurus. 4 Downstream Tasks and Applications 4.1 TOEFL Synonym Task Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option. 4Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings. 2046 Figure 1: Varying the number of iterations when retrofitting Method TOEFL Doc Skip-gram 77.50 83.96 Joint-Norms-Sampled 78.75 84.46 Joint-Norms-All 66.25 84.82 Joint-Thesaurus-S</context>
</contexts>
<marker>Freitag, Blume, Byrnes, Chow, Kapadia, Rohwer, Wang, 2005</marker>
<rawString>Dayne Freitag, Matthias Blume, John Byrnes, Edmond Chow, Sadik Kapadia, Richard Rohwer, and Zhiqiang Wang. 2005. New experiments in distributional representations of synonymy. In Proceedings of the Ninth Conference on Computational Natural Language Learning, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting embedding features for simple semi-supervised learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>110--120</pages>
<contexts>
<context position="1350" citStr="Guo et al., 2014" startWordPosition="191" endWordPosition="194">atedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is closely related to c</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi-supervised learning. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional Structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="947" citStr="Harris, 1954" startWordPosition="129" endWordPosition="130"> of retrofitting and a joint-learning approach, and find that all three yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is so</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zelig Harris. 1954. Distributional Structure. Word, 10(23):146—162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Kyunghyun Cho</author>
<author>S´ebastien Jean</author>
<author>Coline Devin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Embedding word similarity with neural machine translation.</title>
<date>2014</date>
<location>CoRR, abs/1412.6448.</location>
<contexts>
<context position="2415" citStr="Hill et al., 2014" startWordPosition="361" endWordPosition="364">uch more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is closely related to chair, that does not mean we should translate table into French as chaise. This distinction between “genuine” similarity and associative similarity (i.e., relatedness) is well-known in cognitive science (Tversky, 1977). In NLP, however, semantic spaces are generally evaluated on how well they capture both similarity and relatedness, even though, for many word combinations (such as car and petrol), these two objectives are mutually incompatible (Hill et al., 2014b). In part, this oversight stems from the distributional hypothesis itself: car and petrol do not have the same, or even very similar, meanings, but these two words may well occur in similar contexts. Corpus-driven approaches based on the distributional hypothesis therefore generally learn embeddings that capture both similarity and relatedness reasonably well, but neither perfectly. In this work we demonstrate the advantage of specializing semantic spaces for either similarity or relatedness. Specializing for similarity is achieved by learning from both a corpus and a thesaurus, and for rela</context>
<context position="4865" citStr="Hill et al., 2014" startWordPosition="713" endWordPosition="716">es thesaurus developed by the OpenOffice.org project1. It contains synonyms for almost 80,000 words in English. For directing embeddings towards relatedness, we use the University of South Florida (USF) free association norms (Nelson et al., 2004). This dataset contains scores for free association (an experimental measure of cognitive association) of over 10,000 concept words. For raw text data we use a dump of the English Wikipedia plus newswire text (8 billion words in total)2. 2.1 Evaluations (Intrinsic and Extrinsic) For instrinsic comparisons with human judgements, we evaluate on SimLex (Hill et al., 2014b) (999 pairwise comparisons), which explicitly measures similarity, and MEN (Bruni et al., 2014) (3000 comparisons), which explicitly measures relatedness. We also consider two downstream tasks and applications. In the TOEFL synonym selection task (Landauer and Dumais, 1997), the objective is to select the correct synonym for a target word from a multiple-choice set of possible answers. For a more extrinsic evaluation, we use a document classification task based on the Reuters Corpus Volume 1 (RCV1) (Lewis et al., 2004). This dataset consists of over 800,000 manually categorized news articles</context>
<context position="12266" citStr="Hill et al. (2014" startWordPosition="1861" endWordPosition="1864">am Tasks and Applications 4.1 TOEFL Synonym Task Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option. 4Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings. 2046 Figure 1: Varying the number of iterations when retrofitting Method TOEFL Doc Skip-gram 77.50 83.96 Joint-Norms-Sampled 78.75 84.46 Joint-Norms-All 66.25 84.82 Joint-Thesaurus-Sampled 81.25 83.90 Joint-Thesaurus-All 80.00 83.56 GB-Retrofit-Norms 80.00 80.58 GB-Retrofit-Thesaurus 83.75 80.24 SG-Retrofit-Norms 80.00 84.56 SG-Retrofit-Thesaurus 88.75 84.55 Table 2: TOEFL synonym selection and document classification accuracy (percentage of correctly answered questions/correctly classified documents). As Table 2 shows, similarity-</context>
</contexts>
<marker>Hill, Cho, Jean, Devin, Bengio, 2014</marker>
<rawString>Felix Hill, Kyunghyun Cho, S´ebastien Jean, Coline Devin, and Yoshua Bengio. 2014a. Embedding word similarity with neural machine translation. CoRR, abs/1412.6448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>SimLex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="2415" citStr="Hill et al., 2014" startWordPosition="361" endWordPosition="364">uch more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is closely related to chair, that does not mean we should translate table into French as chaise. This distinction between “genuine” similarity and associative similarity (i.e., relatedness) is well-known in cognitive science (Tversky, 1977). In NLP, however, semantic spaces are generally evaluated on how well they capture both similarity and relatedness, even though, for many word combinations (such as car and petrol), these two objectives are mutually incompatible (Hill et al., 2014b). In part, this oversight stems from the distributional hypothesis itself: car and petrol do not have the same, or even very similar, meanings, but these two words may well occur in similar contexts. Corpus-driven approaches based on the distributional hypothesis therefore generally learn embeddings that capture both similarity and relatedness reasonably well, but neither perfectly. In this work we demonstrate the advantage of specializing semantic spaces for either similarity or relatedness. Specializing for similarity is achieved by learning from both a corpus and a thesaurus, and for rela</context>
<context position="4865" citStr="Hill et al., 2014" startWordPosition="713" endWordPosition="716">es thesaurus developed by the OpenOffice.org project1. It contains synonyms for almost 80,000 words in English. For directing embeddings towards relatedness, we use the University of South Florida (USF) free association norms (Nelson et al., 2004). This dataset contains scores for free association (an experimental measure of cognitive association) of over 10,000 concept words. For raw text data we use a dump of the English Wikipedia plus newswire text (8 billion words in total)2. 2.1 Evaluations (Intrinsic and Extrinsic) For instrinsic comparisons with human judgements, we evaluate on SimLex (Hill et al., 2014b) (999 pairwise comparisons), which explicitly measures similarity, and MEN (Bruni et al., 2014) (3000 comparisons), which explicitly measures relatedness. We also consider two downstream tasks and applications. In the TOEFL synonym selection task (Landauer and Dumais, 1997), the objective is to select the correct synonym for a target word from a multiple-choice set of possible answers. For a more extrinsic evaluation, we use a document classification task based on the Reuters Corpus Volume 1 (RCV1) (Lewis et al., 2004). This dataset consists of over 800,000 manually categorized news articles</context>
<context position="12266" citStr="Hill et al. (2014" startWordPosition="1861" endWordPosition="1864">am Tasks and Applications 4.1 TOEFL Synonym Task Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option. 4Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings. 2046 Figure 1: Varying the number of iterations when retrofitting Method TOEFL Doc Skip-gram 77.50 83.96 Joint-Norms-Sampled 78.75 84.46 Joint-Norms-All 66.25 84.82 Joint-Thesaurus-Sampled 81.25 83.90 Joint-Thesaurus-All 80.00 83.56 GB-Retrofit-Norms 80.00 80.58 GB-Retrofit-Thesaurus 83.75 80.24 SG-Retrofit-Norms 80.00 84.56 SG-Retrofit-Thesaurus 88.75 84.55 Table 2: TOEFL synonym selection and document classification accuracy (percentage of correctly answered questions/correctly classified documents). As Table 2 shows, similarity-</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014b. SimLex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyo Kageura</author>
<author>Keita Tsuji</author>
<author>Akiko N Aizawa</author>
</authors>
<title>Automatic thesaurus generation through multiple filtering.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics-Volume 1,</booktitle>
<pages>397--403</pages>
<contexts>
<context position="11852" citStr="Kageura et al., 2000" startWordPosition="1792" endWordPosition="1795"> source. Its information may be detrimental to model optimization when encountered early in training (in the joint learning condition) because the model has not acquired the basic concepts on which it builds. However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn from, the information in the thesaurus. 4 Downstream Tasks and Applications 4.1 TOEFL Synonym Task Unsupervised synonym selection has many applications including the generation of thesauri and other lexical resources from raw text (Kageura et al., 2000). In the well-known TOEFL evaluation (Freitag et al., 2005) models are required to identify true synonyms to question words from a selection of possible answers. To test our models on this task, for each question in the dataset, we rank the multiple-choice answers according to the cosine similarity between their word embeddings and that of the target word, and choose the highestranked option. 4Hill et al. (2014a) obtain a score of 0.52 using neural translation embeddings. 2046 Figure 1: Varying the number of iterations when retrofitting Method TOEFL Doc Skip-gram 77.50 83.96 Joint-Norms-Sample</context>
</contexts>
<marker>Kageura, Tsuji, Aizawa, 2000</marker>
<rawString>Kyo Kageura, Keita Tsuji, and Akiko N Aizawa. 2000. Automatic thesaurus generation through multiple filtering. In Proceedings of the 18th conference on Computational linguistics-Volume 1, pages 397– 403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="5141" citStr="Landauer and Dumais, 1997" startWordPosition="753" endWordPosition="756">ins scores for free association (an experimental measure of cognitive association) of over 10,000 concept words. For raw text data we use a dump of the English Wikipedia plus newswire text (8 billion words in total)2. 2.1 Evaluations (Intrinsic and Extrinsic) For instrinsic comparisons with human judgements, we evaluate on SimLex (Hill et al., 2014b) (999 pairwise comparisons), which explicitly measures similarity, and MEN (Bruni et al., 2014) (3000 comparisons), which explicitly measures relatedness. We also consider two downstream tasks and applications. In the TOEFL synonym selection task (Landauer and Dumais, 1997), the objective is to select the correct synonym for a target word from a multiple-choice set of possible answers. For a more extrinsic evaluation, we use a document classification task based on the Reuters Corpus Volume 1 (RCV1) (Lewis et al., 2004). This dataset consists of over 800,000 manually categorized news articles.3 2.2 Joint Learning The standard skip-gram training objective for a sequence of training words w1, w2, ..., wT and a context size c is the log-likelihood criterion: T T 1 � Jθ(wt) = T t=1 t=1 where p(wt+j|wt) is obtained via the softmax: T uwt+j vwt Ew&apos; expuTw&apos;vwt where uw </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="5391" citStr="Lewis et al., 2004" startWordPosition="797" endWordPosition="800">c) For instrinsic comparisons with human judgements, we evaluate on SimLex (Hill et al., 2014b) (999 pairwise comparisons), which explicitly measures similarity, and MEN (Bruni et al., 2014) (3000 comparisons), which explicitly measures relatedness. We also consider two downstream tasks and applications. In the TOEFL synonym selection task (Landauer and Dumais, 1997), the objective is to select the correct synonym for a target word from a multiple-choice set of possible answers. For a more extrinsic evaluation, we use a document classification task based on the Reuters Corpus Volume 1 (RCV1) (Lewis et al., 2004). This dataset consists of over 800,000 manually categorized news articles.3 2.2 Joint Learning The standard skip-gram training objective for a sequence of training words w1, w2, ..., wT and a context size c is the log-likelihood criterion: T T 1 � Jθ(wt) = T t=1 t=1 where p(wt+j|wt) is obtained via the softmax: T uwt+j vwt Ew&apos; expuTw&apos;vwt where uw and vw are the context and target vector representations for word w, respectively, and w1 ranges over the full vocabulary (Mikolov et al., 1https://www.openoffice.org/lingucomponent/thesaurus.html 2The script for obtaining this corpus is available fr</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. 2004. RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of ICLR,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="1237" citStr="Mikolov et al., 2013" startWordPosition="173" endWordPosition="176">ads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than </context>
<context position="7631" citStr="Mikolov et al., 2013" startWordPosition="1159" endWordPosition="1162">ing objective that improves learned word embeddings. We experiment with their method, calling it graph-based retrofitting. In addition, we introduce a similar approach that instead uses the same objective function that was used to learn the original skip-gram embeddings. In other words, we first train a standard skip-gram model, and then learn from the additional contexts in a second training stage as if they form a separate corpus: 1 T � log p(wa|wt) T t=1 waEAwt We call this approach skip-gram retrofitting. In all cases, our embeddings have 300 dimensions, which has been found to work well (Mikolov et al., 2013a; Baroni et al., 2014) 3 Results for Intrinsic Evaluation We compare standard skip-gram embeddings with retrofitted and jointly learned specialized embeddings, as well as with “fitted” embeddings that were randomly initialized and learned only from the additional semantic resource. In each case, the 1 T � log p(wt+j|wt) −c&lt;j&lt;c p(wt+j|wt) = exp 1 T T t=1 1 T T t=1 2045 Method SimLex-999 MEN Skip-gram 0.31 0.68 Fit-Norms 0.08 0.14 Fit-Thesaurus 0.26 0.14 Joint-Norms-Sampled 0.43 0.72 Joint-Norms-All 0.42 0.67 Joint-Thesaurus-Sampled 0.38 0.69 Joint-Thesaurus-All 0.44 0.60 GB-Retrofit-Norms 0.32</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of ICLR, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of ICLR,</booktitle>
<location>Scottsdale, Arizona, USA.</location>
<contexts>
<context position="1237" citStr="Mikolov et al., 2013" startWordPosition="173" endWordPosition="176">ads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than </context>
<context position="7631" citStr="Mikolov et al., 2013" startWordPosition="1159" endWordPosition="1162">ing objective that improves learned word embeddings. We experiment with their method, calling it graph-based retrofitting. In addition, we introduce a similar approach that instead uses the same objective function that was used to learn the original skip-gram embeddings. In other words, we first train a standard skip-gram model, and then learn from the additional contexts in a second training stage as if they form a separate corpus: 1 T � log p(wa|wt) T t=1 waEAwt We call this approach skip-gram retrofitting. In all cases, our embeddings have 300 dimensions, which has been found to work well (Mikolov et al., 2013a; Baroni et al., 2014) 3 Results for Intrinsic Evaluation We compare standard skip-gram embeddings with retrofitted and jointly learned specialized embeddings, as well as with “fitted” embeddings that were randomly initialized and learned only from the additional semantic resource. In each case, the 1 T � log p(wt+j|wt) −c&lt;j&lt;c p(wt+j|wt) = exp 1 T T t=1 1 T T t=1 2045 Method SimLex-999 MEN Skip-gram 0.31 0.68 Fit-Norms 0.08 0.14 Fit-Thesaurus 0.26 0.14 Joint-Norms-Sampled 0.43 0.72 Joint-Norms-All 0.42 0.67 Joint-Thesaurus-Sampled 0.38 0.69 Joint-Thesaurus-All 0.44 0.60 GB-Retrofit-Norms 0.32</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. In Proceedings of ICLR, Scottsdale, Arizona, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The University of South Florida free association, rhyme, and word fragment norms.</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="4495" citStr="Nelson et al., 2004" startWordPosition="654" endWordPosition="657"> be “nudged” in a particular direction by including information from an additional semantic data 2044 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2044–2048, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. source. For directing embeddings towards genuine similarity, we use the MyThes thesaurus developed by the OpenOffice.org project1. It contains synonyms for almost 80,000 words in English. For directing embeddings towards relatedness, we use the University of South Florida (USF) free association norms (Nelson et al., 2004). This dataset contains scores for free association (an experimental measure of cognitive association) of over 10,000 concept words. For raw text data we use a dump of the English Wikipedia plus newswire text (8 billion words in total)2. 2.1 Evaluations (Intrinsic and Extrinsic) For instrinsic comparisons with human judgements, we evaluate on SimLex (Hill et al., 2014b) (999 pairwise comparisons), which explicitly measures similarity, and MEN (Bruni et al., 2014) (3000 comparisons), which explicitly measures relatedness. We also consider two downstream tasks and applications. In the TOEFL syno</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 2004</marker>
<rawString>Douglas L Nelson, Cathy L McEvoy, and Thomas A Schreiber. 2004. The University of South Florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, &amp; Computers, 36(3):402–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12--1532</pages>
<contexts>
<context position="10570" citStr="Pennington et al., 2014" startWordPosition="1595" endWordPosition="1598">ndicated by the large increase in performance on SimLex-999. The highest performance obtained, at 5 iterations, is a Spearman ρs correlation of 0.53, which, as far as we know, matches the current state-of-the-art.4 For relatedness-specific embeddings, the effect is less clear: joint learning performs comparatively much better. Retrofitting does outperform it, at around 2-10 iterations on the USF norms, but the improvement is marginal. The highest retrofitting score is 0.74; the highest joint learning score is 0.72. Both are highly competitive results on MEN, and outperform e.g. GloVe at 0.71 (Pennington et al., 2014). Joint learning with a thesaurus, however, leads to poor performance on MEN, as expected: the embeddings get dragged away from relatedness and towards similarity. 3.1 Curriculum learning? The fact that joint learning works better when supplementing raw text input with free associates, but skip-gram retrofitting works better with additional thesaurus information, could be due to curriculum learning effects (Bengio et al., 2009). Unlike the USF norms, many of the words from the thesaurus are unusual and have low frequency. This suggests that the thesaurus is more ‘advanced’ (from the perspectiv</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="1281" citStr="Socher et al., 2013" startWordPosition="179" endWordPosition="182">sification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conv</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="1331" citStr="Turian et al., 2010" startWordPosition="187" endWordPosition="190">her similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is c</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="972" citStr="Turney and Pantel, 2010" startWordPosition="131" endWordPosition="134">ng and a joint-learning approach, and find that all three yield specialized semantic spaces that capture human intuitions regarding similarity and relatedness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: vector space models of semantics. Journal of Artifical Intelligence Research, 37(1):141–188, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1100" citStr="Turney, 2006" startWordPosition="151" endWordPosition="152">ity and relatedness better than unspecialized spaces. We also show that using specialized spaces in NLP tasks and applications leads to clear improvements, for document classification and synonym selection, which rely on either similarity or relatedness but not both. 1 Introduction Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014). Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification. Although this behavior is sometimes desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly </context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
</authors>
<title>Features of similarity.</title>
<date>1977</date>
<journal>Psychological Review,</journal>
<volume>84</volume>
<issue>4</issue>
<contexts>
<context position="2167" citStr="Tversky, 1977" startWordPosition="324" endWordPosition="325">s desirable, it may in other cases be detrimental to downstream performance. For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine. Conversely, if our embeddings indicate that table is closely related to chair, that does not mean we should translate table into French as chaise. This distinction between “genuine” similarity and associative similarity (i.e., relatedness) is well-known in cognitive science (Tversky, 1977). In NLP, however, semantic spaces are generally evaluated on how well they capture both similarity and relatedness, even though, for many word combinations (such as car and petrol), these two objectives are mutually incompatible (Hill et al., 2014b). In part, this oversight stems from the distributional hypothesis itself: car and petrol do not have the same, or even very similar, meanings, but these two words may well occur in similar contexts. Corpus-driven approaches based on the distributional hypothesis therefore generally learn embeddings that capture both similarity and relatedness reas</context>
</contexts>
<marker>Tversky, 1977</marker>
<rawString>Amos Tversky. 1977. Features of similarity. Psychological Review, 84(4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.925158">
Estimation of Discourse Segmentation Labels from Crowd Data
</title>
<author confidence="0.9808">
Ziheng Huang Jialu Zhong Rebecca J. Passonneau
</author>
<affiliation confidence="0.927936">
Department of Statistics Department of Computer Science Center for Computational
Columbia University Columbia University Learning Systems
zh2220@columbia.edu jialu.zhong@columbia.edu Columbia University
</affiliation>
<email confidence="0.982359">
becky@ccls.columbia.edu
</email>
<sectionHeader confidence="0.997191" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999685">
For annotation tasks involving independent
judgments, probabilistic models have been
used to infer ground truth labels from data
where a crowd of many annotators labels the
same items. Such models have been shown to
produce results superior to taking the majority
vote, but have not been applied to sequential
data. We present two methods to infer ground
truth labels from sequential annotations where
we assume judgments are not independent,
based on the observation that an annotator’s
segments all tend to be several utterances long.
The data consists of crowd labels for anno-
tation of discourse segment boundaries. The
new methods extend Hidden Markov Models
to relax the independence assumption. The
two methods are distinct, so positive labels
proposed by both are taken to be ground truth.
In addition, results of the models are checked
using metrics that test whether an annotator’s
accuracy relative to a given model remains
consistent across different conversations.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968194029851">
A single, spontaneous, spoken interaction can consist
of multiple activities, such as to plan a future event,
to complain about a past situation, or to carry out a
transaction that might consist of subtasks. Speakers
shift from one activity to the next with more or less
awareness and explicit demarcation. To treat such con-
versational activities as a sequence of discrete units is
a convenient oversimplification that is often resorted
to (Bokaei et al., 2015; Galley et al., 2003; Passon-
neau and Litman, 1997). Systems that provide auto-
mated access to spoken language data often rely on
segmentation of spoken discourse into sequential units
for summarization (Wang and Cardie, 2012; Dielmann
and Renals, 2005) or information retrieval (Ward et al.,
2015). Research on the organization of spoken dis-
course also relies directly or indirectly on identifica-
tion of such units to detect agreement among partici-
pants (Hillard et al., 2003; Somasundaran et al., 2007;
Germesin and Wilson, 2009), multiparty meeting ac-
tion items (Purver et al., 2007), decisions (Fern´andez
et al., 2008), or answers to questions (Sun and Chai,
2007; Bosma, 2005). To support such research, there
is a need for annotation methods to segment conversa-
tional interaction into sequential, multi-utterance units.
We present and compare two methods to derive such
data from crowdsourced annotations.
Crowdsourced annotation, where each item is la-
beled by a crowd of many independent annotators, is
becoming more common in natural language process-
ing. Examples include word sense (Bruce and Wiebe,
1999; Snow et al., 2008; Passonneau and Carpenter,
2014), named entities (Finin et al., 2010), and sev-
eral other tasks in (Snow et al., 2008), including tex-
tual entailment. Three advantages to corpus annotation
through application of a probabilistic model to crowd-
sourced labels, rather than reliance on interannotator
agreement computed for a small number of trained an-
notators, are higher quality, lower cost, and a poste-
rior probability for each ground truth label (Sheng et
al., 2008; Snow et al., 2008; Passonneau and Carpen-
ter, 2014). The latter serves as a confidence measure,
which contrasts with interannotator agreement mea-
sures and with majority-voted labels, neither of which
provides quality information for the ground truth la-
bels on individual items. Previous work has demon-
strated that model estimation of ground truth labels
from crowd labels produces results superior to the
crowd’s majority vote, due to differences among anno-
tators in the quality of their labels (Dawid and Skene,
1979; Snow et al., 2008; Passonneau and Carpenter,
2014). No previous work, however, provides model-
based estimation of labels for sequential annotation
from crowd labels.
For the discourse segmentation data presented here,
annotators were presented with audio files of conversa-
tions and corresponding transcriptions into utterances.
The annotation task was to identify each utterance that
completes a discourse segment spanning one or more
utterances, based on the speakers’ conversational ac-
tivities or intentions, as in (Passonneau and Litman,
1997). The annotations from y annotators for a conver-
sation with x utterances can be represented as a y x x
matrix, with cell values nzj E {0, 1} to represent the
binary segment boundary label assigned by annotator
yz at utterance xj. Figure 1 illustrates part of such a
matrix. The eight annotators for this conversation are
on the y-axis and utterances 80 through 180 are on the
</bodyText>
<page confidence="0.938923">
2190
</page>
<note confidence="0.9959085">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2190–2200,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.98657525">
Figure 1: Annotation labels from eight annotators (A-G, I)
on utterances 80 through 180 of a sample conversation. Ver-
tical bars represent positive labels, with a different color for
each annotator. Annotator H did not do this conversation.
</figureCaption>
<bodyText confidence="0.9999743125">
x-axis. Colored bars represent positive labels, and each
color represents a distinct annotator. The label distri-
bution shown here is typical of our dataset: an annota-
tor’s positive labels are typically separated by several
utterances, and annotators agree much more often on
non-boundaries than on boundaries. Full consensus on
a positive label is rare, but does occur. Here, all eight
annotators assigned a positive label at utterance 120,
six at utterance 178, and five at utterance 140.
Our work assumes that unobserved true labels con-
dition the annotators’ observed labels, and can be mod-
eled as hidden states in a Markov-type process. Be-
cause an annotator rarely assigns positive labels for ad-
jacent utterances, we assume that neither the true labels
nor the observations are conditionally independent, and
therefore are not generated by a simple Markov pro-
cess. Our first model adapts the Double Chain Markov
Model (Berchtold, 1999), designed to account for such
cases. We then propose a second model that assumes
that each annotator’s labels are drawn from a Bernoulli
distribution, that annotator performance is a parame-
ter of the model, and that the state transitions are con-
ditioned by an empirical distribution of discourse seg-
ment lengths. The two methods are quite distinct. Each
thus serves as an evaluation of the other. The seg-
ment boundaries proposed by both models include all
the majority vote cases, and in addition, cases voted
on by a minority of relatively accurate annotators. We
take segment boundaries proposed by both methods as
ground truth. To further assess the results of the mod-
els, we assume that an annotator’s accuracies should be
consistent across the conversations she annotates.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999553">
Previous work on annotation of discourse into lin-
ear segments has used a variety of methods to de-
rive ground truth segment boundaries. In (Passonneau
and Litman, 1997), seven annotators annotated narra-
tive monologues for segments based on speaker inten-
tion. Agreement levels for ground truth boundaries
were based on statistical significance using Cochran’s
Q. In (Galley et al., 2003), three annotators segmented
the ICSI meeting corpus into topical units, and ma-
jority agreement was taken as ground truth. A func-
tional segmentation of meetings from the AMI mul-
tiparty meeting corpus based on involved participants
was segmented by one annotator and finalized by a sec-
ond annotator (Bokaei et al., 2015). Task-based seg-
mentation of patron-librarian interactions (Passonneau
et al., 2011) measured agreement among two annota-
tors using Krippendorff’s Alpha at an average of 0.77
(Krippendorff, 1980). The annotation task here mostly
closely resembles (Passonneau and Litman, 1997), and
uses a similar number of annotators. No prior work,
however, applies a probabilistic model to crowd labels
for discourse segmentation.
Estimation of ground truth from crowd labels has
been applied to many tasks, but is especially useful
where judgments are subjective, making ground truth
difficult to arrive at. Application areas include dis-
ease prevalence estimation (Albert and Dodd, 2008),
identification of craters in images of Venus (Smyth et
al., 1995), curation of biological data (Rzhetsky et al.,
2009), computer vision (Whitehill et al., 2009), pa-
tient history (Dawid and Skene, 1979), and clinical re-
ports (2010). Smyth et al. (1995), Rogers et al., and
(2010) and Raykar et al. (2010) discuss the advan-
tages of probabilistically annotated corpora over ma-
jority vote. Much of this work is motivated by the ob-
servation that annotators have different accuracies, and
the fact that when annotators have known accuracies
it can be shown that a majority of inaccurate annota-
tors can be wrong (Raykar et al., 2010; Passonneau and
Carpenter, 2014). Equally important, information from
inaccurate annotators informs the model inference. For
example, an inaccurate annotator might be biased to-
wards label m whenever the true label is z.
Dawid and Skene (1979) present a joint model of
true labels, observed labels, and annotator perfor-
mance. Perhaps its first application to NLP data was the
Bruce and Wiebe (1999) investigation of word sense.
It has also been applied to more fine-grained word
sense with a direct comparison to trained annotator la-
bels in (Passonneau and Carpenter, 2014). Snow et al.
(2008) showed that application of the same model to
noisy crowd annotations produced data of equal qual-
ity to five distinct published gold standards. Hovy et
al. (2013) apply a simple and effective model to iden-
tify untrustworthy annotators and test it on the same
datasets used in (Snow et al., 2008). As they point out,
when ties occur among an even number of annotators,
it’s necessary to resort to a tie-breaking procedure, e.g.,
for utterance 155 in Figure 1 where four annotators as-
sign a positive label and four do not.
In experiments on an existing dataset of word sense
annotation, Dligach et al. (2010) compare singly anno-
tated data with doubly annotated adjudicated data, us-
ing trained annotators. They find that with the same
amount of data, machine learning performance im-
proves with the doubly annotated adjudicated data by
</bodyText>
<page confidence="0.9977">
2191
</page>
<figureCaption confidence="0.985907666666667">
Figure 2: The annotation interface presented the audio control button on the upper left and the transcript below, with a scroll
bar (not shown). Utterances from the two speakers are on the right and left sides, respectively. Each utterance had a checkbox;
when selected, a textbox appeared to allow annotators to enter their segment descriptions.
</figureCaption>
<bodyText confidence="0.999859727272727">
a small amount, but that investing in more singly an-
notated labels leads to greater improvements. Their re-
sults on trained annotators, however, would not apply
to our use case involving untrained annotators. In pre-
vious work, we found the cost per ground truth label
of singly annotated data with trained annotators to be
more than twice that for multiply annotated data with
twenty untrained annotators (Passonneau and Carpen-
ter, 2014). Half that many would have been sufficient
for the Dawid &amp; Skene model used there, which would
reduce the cost by half again as much.1
</bodyText>
<sectionHeader confidence="0.976024" genericHeader="method">
3 Data and Annotation Task
</sectionHeader>
<bodyText confidence="0.999970344827586">
The data consists of digital recordings and transcripts
of fifty telephone calls between family members and
friends who were native speakers of Tagalog. These
were collected for the Babel program, sponsored by
the Intelligence Advanced Research Projects Activ-
ity (IARPA). The calls ranged in length from about
seven to ten minutes (µ = 9.67 minutes, Q=0.68 min-
utes). Transcripts provided by IARPA had an average
of 364.66 utterances (min=239; max=475; Q=60.80).
The annotations were collected using Amazon Me-
chanical Turk. The task name and instructions were
in English. The instructions were provided through a
short video and text. Proficiency in Tagalog was as-
sessed through a vocabulary test. Those who passed
the vocabulary test were paid to do an initial annotation
so we could ensure they understood the task. The ini-
tial task was based on a short Tagalog conversation that
had been translated, annotated by a bilingual speaker
of Tagalog and English, and verified by Passonneau.
Annotators who understood the task and whose labels
and descriptions seemed reasonable were admitted into
the pool of annotators. A pool of nine annotators com-
pleted the qualifications. Each conversation was anno-
tated by at least five annotators. Altogether, annotators
assigned 5,567 labels to 164,097 utterances. Annota-
tors’ segments had a mean length of 21.85 utterances
with a high standard deviation (Q = 19.32).
The interface designed for the annotation task is
shown in Figure 2. Through the interface, annotators
</bodyText>
<footnote confidence="0.9947405">
1Twenty labels per item were collected in order to provide
tight estimates for item difficulty. This, however, requires a
model with a parameter for item difficulty, which had not yet
been implemented for this data.
</footnote>
<bodyText confidence="0.99953525">
could read the transcript of a recorded conversation,
and could play, pause or stop the audio. Each utterance
had a checkbox for assigning a positive label if the an-
notator judged it to be the end of a segment. As shown,
selection of a checkbox opened a text box for the anno-
tator to enter a brief description of the segment. Table 1
in section 8 illustrates the descriptions assigned by six
annotators to several segments.
</bodyText>
<sectionHeader confidence="0.996908" genericHeader="method">
4 Assumptions
</sectionHeader>
<bodyText confidence="0.999988611111111">
Given the many labels from annotators, our goal is to
estimate a ground truth label for each utterance posi-
tion, where the label values represent a binary classi-
fication of segment boundaries. Our two models each
assume there is a hidden true label that conditions an
annotator’s observed labels, and that can be estimated
from the observed labels. How well the estimated
ground truth fits the data thus depends on how well
the model assumptions accord with the phenomenon
of interest. The models do not account for annotator
differences in the level of granularity they apply; cf.
the contrast between lumpers and splitters in taxonomic
classification of the natural world (Branch, 2014). Fur-
ther, neither model takes linguistic features into ac-
count that annotators consider in deciding on segments,
such as speaker attitude towards utterance content or
speaker role in the conversational activity (Niekrasz
and Moore, 2009). We find, however, much agree-
ment between the two models on the proposed segment
boundaries, and leave for future work the question of
whether more complex models could accoount for dif-
ferences in granularity or utterance features.
As discussed in section 2, we assume that annotators
are not equally accurate, and that a probabilistic model
based on the distribution of observed labels can do bet-
ter than majority vote. Inspired by the type of prob-
abilistic model proposed in (Dawid and Skene, 1979)
and extended in (Bruce and Wiebe, 1999; Passonneau
and Carpenter, 2014), annotator accuracy is a parame-
ter of our second model. As described in detail in sub-
sequent sections, the two models proposed here rely on
distinct assumptions and inference methods. They nev-
ertheless propose many of the same labels. We take
each model to provide independent evidence for the
ground truth labels, thus the final labels are those voted
on by both models.
</bodyText>
<page confidence="0.996063">
2192
</page>
<figureCaption confidence="0.99833625">
Figure 3: Graphical model of Double Chain Dynamic Hid-
den Markov Model for a conversation with m annotators and
n utterances. The xt are the hidden states, and the yjl are the
observed labels from annotator l at utterance j.
</figureCaption>
<bodyText confidence="0.994195">
In addition, we assume that annotators’ accuracies
should be relatively consistent across conversations,
and we measure how well each model’s results support
this assumption. We base the assumption on the obser-
vation that the annotation task is the same for all con-
versations, and an annotator’s relative ability to do the
task should not change significantly. The annotators all
had the same initial training, and did about the same
number of conversations. The conversations all had
similar conditions of collection, similar participants,
and similarly mundane topics and conversational activ-
ities that most annotators would be familiar with. The
subjects that were discussed included parties, watch-
ing tv, siblings, money, jobs, spouses, medical issues,
birthdays, and so on.
ances. ytl = 1 represents that the lth annotator
annotates tth utterance to a true boundary and 0
otherwise;
</bodyText>
<listItem confidence="0.78152075">
• Θ is a vector of parameters. To be more specific,
the elements are:
– the probability of the initial hidden state:
πx1, x1 ∈ {0, 1}. Note π0 + π1 = 1.
</listItem>
<bodyText confidence="0.98615380952381">
– the probabilities of the initial emission ma-
trix. Note that the initial emission matrix is
a 2 × 2 matrix: γl ∈ {cx1,y1l}, x1, y1l ∈
{0, 1}, l ∈ {1, 2, ... , m}. For annotator l,
cx1,y1l is the probability of emitting from x1
to y1l.
– the transition matrix between hidden states,
A ∈ {axt−1,xt}, xt−1, xt ∈ {0,1}, t ∈
{2, 3, ... , n}. axt−1,xt is the probability of
transitioning from xt−1 to xt.
– the emission matrices, Bl ∈ {bxt,y(t−1)l,ytl},
xt, y(t−1)l, ytl ∈ {0,1}, l ∈ {1, 2, . . . , m},
t ∈ {2, 3, ... , n}. Note that the emis-
sion matrix is a 2 × 2 × 2 matrix as each
observed state depends on current hidden
state as well as the previous observation, i.e.,
bxt,y(t−1)l,ytl is the probability of emitting
from xt to ytl and transitioning from y(t−1)l
to ytl.
A graphical sketch of the DCD HMM model is shown
in Figure 3. The target function F = P(x, y|Θ) is:
</bodyText>
<equation confidence="0.7161495">
x1 x2 x3 xn−1 xn
y11 y21 y31 yn−11 yn1
y12 y22 y32 yn−12 yn2
y1m y2m y3m yn−1m ynm
</equation>
<table confidence="0.426430333333333">
5 Double Chain Dynamic Hidden F = πx1 HM cx1,y1l n axt−1,xt HM n bxt,y(t−1)l,ytl
Markov Model l=1 H l=1 H
t=2 t=2
</table>
<bodyText confidence="0.998733055555555">
The first model we propose combines the Double
Chain Markov Model (Berchtold, 1999) and dynamic
Bayesian networks (Martinez and Sucar, 2008). The
double chaining involves the dependence of observa-
tions on immediately prior observations. Figure 3
shows that for all ytl, t ≥ 2, observation ytl depends
on observation y(t−1)l. The emission matrix at the first
utterance x1 is thus a 2×2 matrix, while all subsequent
emission matrices are 2 × 2 × 2. As in (Martinez and
Sucar, 2008), the observed states can be regarded as a
composition of m independent chains, where m is the
number of annotators for the conversation. Also, the
lth annotator’s observation at the tth utterance depends
not only on the same hidden state xt, but also on the
last observation y(t−1)l.
Assume in a conversation, there are m annotators
and n utterances. The model Θ = {π, γ, A, B} can
be described as follows:
</bodyText>
<listItem confidence="0.9971874">
• a set of hidden states, i.e the true labels: xt ∈
{0,1}, t ∈ {1, 2, ... , n}. xt = 1 represents the
tth utterance is a true boundary and 0 otherwise;
• a set of observed variables: ytl ∈ {0, 1}, l ∈
{1, 2, ... , m} annotators, t ∈ {1, 2, ... , n} utter-
</listItem>
<bodyText confidence="0.9989945">
We can derive a marginal distribution over y and have
the likelihood as:
</bodyText>
<equation confidence="0.9842545">
L(Θ) = P (y|Θ) = � P(x, y|Θ)
x
</equation>
<bodyText confidence="0.999359736842105">
Our goal is to find the parameters (Θ) that maximize
the above function. Bayes Net Toolbox for Matlab
(Murphy, 2001) is used for the inference. Expectation-
Maximization (EM) with Junction Tree inference for
the E-step is used for learning the parameters. The
Junction Tree Algorithm is a method to calculate
marginals by propagation on the graph. It runs as fol-
lows: 1) Initialize: Pick a proper root and initialize all
variables; 2) Collect: Pass message from each child of a
node through separators to the parent node and update
the node with collected evidence; 3) Distribute: Send
back message to each child of the node through separa-
tors and update the child with distributed evidence; 4)
Normalize: Normalize cliques connected by a separa-
tor so they agree with each other: e.g., for {AB} and
{BC}, if we have EA{AB} = EC{AB}, propaga-
tion is complete.
After convergence from EM, junction tree propaga-
tion is again used for inference, and the model produces
</bodyText>
<page confidence="0.920092">
2193
</page>
<bodyText confidence="0.9998735">
a probability for each ground truth label. We take the
label to be positive if the posterior probability is greater
than 0.5; as shown in section 8, probabilities tend to be
very high or very low.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="method">
6 Interval-dependent HMM
</sectionHeader>
<bodyText confidence="0.99991105">
The second model, Interval-dependent HMM, imposes
a constraint on the state transitions between two posi-
tive labels based on the empirical distribution of inter-
vals between observed labels. Initially, we examined
known distributions. The Poisson, for example, repre-
sents the probability of events in an interval as an av-
erage rate. The model based on the Poisson did not
perform particularly well. Histograms of interval sizes
from different conversations have similar shapes, how-
ever, as illustrated in Figure 4. Although more of the
probability is towards 20 to 40 utterances in Figure 4a,
and between 15 and 35 utterances in Figure 4b, we as-
sume these small differences in the two distributions
are mainly due to sampling variation. As discussed in
preceding sections, the model we present here assumes
that the probability of a true label at time ti is a func-
tion of the interval length ti − tj, where tj is the most
recent time of a true label. The observed data for all
annotators on all conversations provides a set of time
intervals to construct the empirical distribution.
</bodyText>
<figure confidence="0.998083">
(a) First sample conversation
(b) Second sample conversation
</figure>
<figureCaption confidence="0.9738665">
Figure 4: Histograms of interval lengths between all ob-
served labels for two conversations.
</figureCaption>
<bodyText confidence="0.999939333333333">
To assess whether we have sufficient data to reliably
construct the empirical distribution, we performed fifty
iterations of random divisions of the data into two sam-
ples. For each pair of samples, we measured the max-
imum distance between pairs of cumulative distribu-
tion function (CDF) curves, and used the two-sample
Kolmogorov-Smirnov test to measure the goodness of
fit of the two curves. Figure 5 shows an example com-
parison of two CDF curves which have a maximum gap
</bodyText>
<figureCaption confidence="0.92416">
Figure 5: A plot of two CDF curves for a random split of
the data. The curves are almost identical; the maximum gap
is 0.0175. A two sample K-S test has a p-value of 0.79.
</figureCaption>
<bodyText confidence="0.999839083333333">
of 0.0175 and a K-S p-value of 0.7866. The mean max-
imum distance between pairs of curves was 0.014, with
a standard deviation of 0.009, both of which are quite
small. The p-values for the K-S test ranged from 0.4 to
0.96, which fail to reject the hypothesis that the pairs of
samples are from the same distribution. While the two
measures are not conclusive evidence that we have suf-
ficient data to construct the empirical distribution, they
are supportive. Further, reliance on estimates of the
empirical distribution are preferable to a known distri-
bution that does not fit the data, such as the Poisson.
The model can be described as follows:
</bodyText>
<listItem confidence="0.9999006">
• the observations Yij E 0, 1, i E 1, 2, , N, j E
1,2,•••,J;
• the true labels Zi E 0, 1, i E 1, 2, • • • , N;
• the 2 x 2 annotator performance matrices Bj;
• the initial state probability π
</listItem>
<bodyText confidence="0.999842">
Given N utterances, J annotators, the initial state prob-
ability π and four cells in each annotator’s performance
matrix Bj, where Bj11 represents the true positives
(the probability that given a ground truth positive la-
bel, annotator aj assigns a positive label), Bj10 rep-
resents false negatives, Bj01 represents false positives,
and Bj00 represents true negatives. π = 1 is the proba-
bility that the first hidden state is a boundary and π = 2
means it is not. Our objective is to find the param-
eter vector θ = (π, B) that maximize the likelihood
P(Y |θ), and to use this θ to estimate the true labels Z.
</bodyText>
<equation confidence="0.9126855625">
To solve:
�� J
log [P(Y |θ)] = Argmaxlog P(Y, Z|θ)
θ Z
we use expectation-maximization (EM).
E step First, we should find the lower bound off our
optimization object: Argθmaxlog I EP(Y, Z|θ)] ; by Z
Argmax
θ
2194
Jensen’s inequality, we have:
log XP(Y, Z|θ)= logX#
P(Y, Z|θ) Qθ (Z)
Z Z Qθ (Z)
~P (Y, Z|θ) �
Qθ(Z) log Qθ(Z)
</equation>
<bodyText confidence="0.999540166666667">
After this transformation, Xt+1 is independent to all
Xk for any k &lt; t provided that Xt is given. With
X as the new hidden state, we can estimate the HMM
parameter by adding some constraints. Replacing the Z
in the object function with X, we can rewrite the object
function as:
</bodyText>
<equation confidence="0.99809625">
X
&gt;
Z
X=
X
N
X
t=1
Qθ(Z) is a function of θ which satisfies that
Qθ(Z) = 1. The equality holds if and only if
P(Y, Z|θ)
Qθ(Z)
</equation>
<bodyText confidence="0.835224">
Note that c is a constant. In the E step we need to
calculate the Q function to maintain the equality. By
straightforward algebra, we get Qθ = P(Z|Y, θ).
</bodyText>
<equation confidence="0.995056035714286">
P(Y, X|θ(n)) log [P(Y, X|θ)]
&amp;quot; N−1X
P(Y,X|θ(n)) LogP(X1) + log P(Xt+1|Xt)+
t=1
#log P(Yt|Xt)
N−1
P(Y,X|θ(n)) &amp;quot;log [πX1] + X log [AXt,Xt+1] +
t=1
X
X
X=
X
P
Z
= c for all Z
M step In this part, we should maximize our lower N #log [BXt,Yt]
bound: X
t=1
�
X ~P (Y, Z|θ)
Argmax Qθ(n)(Z) log Qθ(n)(Z)
θ
Z
Since log [Qθ(Z)] is a term not related to θ,
P(Z|Y, θ) oc P(Z, Y |θ). Our problem becomes:
X
Argmax P(Y, Z|θ(n)) log [P(Y, Z|θ)]
θ Z
</equation>
<bodyText confidence="0.99582116">
θ(n) is the parameter we get from the last iteration, and
the Q function is fixed in this M step. We cannot use the
forward-backward algorithm to optimize, because the
first order Markov property does not hold: P(Zi = 1)
is a function of the last positive label Zj = 1 at time
j such that j &lt; i, and for all k such that j &lt; k &lt;
i, Zk = 0. To make use of the Markov property, we
rely on a hidden variable Ui to save the interval length
between i and j. The hidden parameter space is then
expanded to Xt = (Zt, Ut), where Ut denotes the size
of the interval between the current position ti and the
most recent tj with a positive label. If the true label
Zti = 0, then Uti = ti − tj, and if Zti = 1, then
Uti = 0. This gives t + 1 possible states for each t: the
t states for Zt = 0, and one state for Zt = 1.
In this problem, given a length N conversation, there
are N +1 hidden states at each moment. Xt = 1 means
(Zt = 1, Ut = 0), Xt = 2 means (Zt = 0, Ut = 1),
Xt = 3 means (Zt = 0, Ut = 2), and so on.
The transition matrix at each t for the cases repre-
sented by P(Xt = k|Xt−1 = l), which is with size
(t + 1) x (t + 2), will necessarily be very sparse. For
example, given an empirical function f(n) = P(x =
n|x &gt; n), the transition matrix from t = 4 to t = 5 can
be written:
</bodyText>
<equation confidence="0.9863285">
f(1) 1 − f(1) 1 0 1 0 1 0 1 0
⎛ f(2) 0 − f(2) 0 0 0 ⎞
⎜ ⎜ ⎜ ⎝ 0 0 − f(3) 0 0
0 0 0 − f(4) ⎠⎟⎟⎟
0 0 0 0 0
− f(5)
</equation>
<bodyText confidence="0.990329833333333">
The object is split into three independent parts: the first
part is for the initial state distribution π, the second for
the transition probability matrix A, and the third is the
emission matrix B. For the first term, because in the
moment t = 1, Xt can just be 1 or 2, we have the
optimization problem:
</bodyText>
<equation confidence="0.99113325">
2
P(Y, X1 = i|θ(n)) log [πi]
s.t π1 + π2 = 1
π3 = π4 = ... = πN+1 = 0
</equation>
<bodyText confidence="0.75307">
We can easily solve this optimization problem by the
Lagrange multiplier: we have the update formula:
</bodyText>
<equation confidence="0.995825166666667">
π(n+1) = P (X1 = 1|Y, θ(n))
1
π(n+1) = P(X1 = 2|Y, θ(n))
2
π(n+1) = 0 for i &gt; 2
i
</equation>
<bodyText confidence="0.997845285714286">
Both can be solved by the traditional forward-backward
algorithm after this transformation. θ(n) is the parame-
ter set we get from the last iteration.
The second term can be ignored, since we use the
known empirical distribution as the transition matrix; it
is therefore a constant term.
The third term can be rewritten as:
</bodyText>
<equation confidence="0.9746812">
P(Y, X|θ(n))log [BXt,Yt]
I(Yt,j = k)P(Xt = i,Y |θ(n))
log [Bj,i,k]
X
i=1
Argmax
π
XN
t=1
=
</equation>
<page confidence="0.902415777777778">
XN
t=1
X1
k=0
N+1X
i=1
XJ
j=1
2195
</page>
<bodyText confidence="0.753874">
So our problem is:
</bodyText>
<equation confidence="0.975917">
Argmax N J N+1X X1 I(Yt,j = k)
B X X i=1 k=0
t=1 j=1
P(Xt = i,Y |θ(n)) logBj,i,k
s.t X1 Bj,i,k = 1 For all i, j
k=0
Bj,i1,k = Bj,i2,k For all j, k and i1, i2 &gt; 2
</equation>
<bodyText confidence="0.999225">
The second constraint here means that, if this is not a
true boundary, a given annotator j will have the same
emission matrix no matter what U is. This optimization
can also be solved by Lagrange multiplier, where the
update formula is as follows. For i = 1:
</bodyText>
<equation confidence="0.99867575">
N _
B� Z+1)k = Pt=1 P(N,Zt = 1|θ( ))I (Yt,j = k)
Pt=1 P(Y, Zt = 1|θ(n))
For any i =� 1, the matrix B is the same given j:
N
B(n+1) _ Pt=1 P(Y, Zt 1|θ(n))I(Yt,j = k)
j,i#1,k - N
Pt=1 P(Y, Zt =� 1|θ(n))
</equation>
<bodyText confidence="0.9997525">
Now we have the update function for θ. After con-
vergence, we will have π and B. It is straightforward
to transfer these parameters for the new space to our
original HMM problem. This completes the M step.
</bodyText>
<sectionHeader confidence="0.958409" genericHeader="method">
7 Model Checking
</sectionHeader>
<bodyText confidence="0.999879127659575">
No ground truth labels are available to evaluate our
models. We check the model results, however, in three
ways. One, we consider labels proposed by both mod-
els to be stronger evidence than labels proposed only
by one. Two, we measure the consistency of annotators
on the assumption that the same annotator should have
relatively consistent performance across conversations,
relative to the same model. The third way we can check
the models is to examine the descriptive labels that an-
notators assign to segments to determine whether de-
scriptions for the same segment from different annota-
tors are consistent. In this section, we describe the two
consistency metrics.
We measure how consistently the label quality from
annotator ai surpasses that for aj, i =� j, for all pairs
of annotators using a metric to measure inconsistency
and strength of inconsistency (I&amp;SI) (de Vries, 1998).
We also apply a variant we refer to as Directional Con-
sistency (DC), which takes into account how often an-
notator ai surpasses annotator aj. To measure anno-
tators’ performance relative to the inferred true labels,
we use F-score, the harmonic mean of recall and preci-
sion. Recall is the ratio of true positives to the sum of
true positives and false negatives; precision is the ratio
of true positives to the sum of true positives and false
positives. A square matrix of annotator dominance is
first constructed to give a count of how many conver-
sations there are where ai has a higher F measure than
aj, i =� j. A linear dominance ordering &gt; of all anno-
tators has an inconsistency score I that is incremented
by 1 for each pair of annotators where ai &gt; aj in the
linear ordering and (ai, aj) =� (aj, ai) in the matrix. I
is minimal if no other ordering has fewer inconsisten-
cies. The strength of the inconsistency IS for a linear
ordering is incremented by the difference in rank be-
tween ai and aj for every inconsistent pair in the linear
ordering. The I&amp;SI method finds an ordering that mini-
mizes I and SI. To check the results of our models, we
compare the I&amp;SI value of the dominance matrix asso-
ciated with the model results against a simulated ran-
dom matrix. If the model results are significantly more
consistent than the simulation, the model produces a
consistent ranking of annotators.
We propose a Directional Consistency index (DC E
[0, 1]) which considers the number of times ai has a
higher F measure than aj (Leiva et al., 2008). Where
X is the dominance matrix:
</bodyText>
<equation confidence="0.9923765">
DC = Pi=1 Pnj=i+1  |xij − x ji |
N
Xn
j=1,j#i
</equation>
<bodyText confidence="0.999656">
DC values closer to zero indicate less consistency in
differences among annotators, and the converse for val-
ues closer to 1. High DC values for the results of our
models thus indicates better performance of the model
in predicting consistent annotator behavior.
</bodyText>
<sectionHeader confidence="0.999805" genericHeader="method">
8 Results and Model Checking
</sectionHeader>
<bodyText confidence="0.999986730769231">
The results consist of the true labels assigned by each
model to each conversation, and estimates of the anno-
tators’ performance relative to the model’s ground truth
labels. Note that as the conversation is not a parame-
ter of either model, after estimation of the empirical
distribution of segment lengths, the data for each con-
versation is treated separately.
To provide a concrete illustration, we first review the
data for a typical conversation. Table 1 presents the
segments derived from both models for an extract from
conversation 945, which had six annotators, and the an-
notator’s segment descriptions. We selected a conver-
sation with an even number of annotators to illustrate
that an arbitrary choice must be made, given a 50/50
vote split. We take ties as true positives to provide a
more conservative baseline. We first discuss this exam-
ple conversation in detail to explain the kinds of cases
where the models differ from majority voting. Then we
present summary results on the fifty conversations for
majority voting compared with the two models.
In Table 1 a description at n gives the annotator’s in-
terpretation of the kind of conversational activity that
ends with n. When annotators agree on a positive la-
bel that ends a segment, they might not agree on the
utterance that starts the segment, so their descriptions
will not necessarily be about the same segments. From
</bodyText>
<equation confidence="0.948664">
Xn
i=1
N=
xij
</equation>
<page confidence="0.986668">
2196
</page>
<table confidence="0.6437595">
Utt Description
191 C: S1 and S2 are talking about the status of their
children’s studies
I: S1 and S2 are talking about their children’s
education
216 A: S1 and S2 spoke about their children’s studies
E: S2 then shared that he’s going to Laguna-
Muntinlupa tomorrow. S1 said that S2 has
many orders. S2 shared that he’s striving
hard in order for her kids to graduate
college.... The two laughed at each other
about S1’s children not getting traits from S1
I: S1 and S2 are talking about who their children
took after
217 C: S1 and S2 are joking about the traits their
children got from them
</table>
<bodyText confidence="0.8959666">
D: They are talking about S1s daughter that she is
good at academics and that she got her being
smart from her mom and nothing from S1. S1
said even if she got nothing from him as long
as she will just study hard its okay
</bodyText>
<subsectionHeader confidence="0.378675">
241 A: S1 and S2 spoke about their time of sleeeping
</subsectionHeader>
<bodyText confidence="0.886850875">
C: S1 and S2 tell each other what time they
usually go to sleep
D: They are talking about the time that they
go to sleep. S2 said sometimes by ten,
eleven or twelve midnight. S1 said sometimes
he goes out one in the morning. Sometimes
he goes to sleep at ten or eleven
in the evening too
</bodyText>
<tableCaption confidence="0.985262428571429">
Table 1: Annotator descriptions for conversation 945 for a
sequence of four segment boundaries hypothesized by both
models. A description from annotator ai at utterance n indi-
cates ai assigned a positive label, and gives the annotator’s
interpretation of the kind of interaction that ended at utter-
ance n. Underlined utterance numbers indicate cases where
at least six annotators assigned a positive label.
</tableCaption>
<bodyText confidence="0.999750470588235">
the table, however, we see a a pattern that is consistent
for most of the data: abstracting over the descriptions
gives a good indication of what’s going on in the seg-
ments that are defined by the positive labels assigned
by both models. The descriptions from C and I at 191,
for example, describe the first segment as the speak-
ers talking about their children’s education. A’s similar
description at 216 indicates that A ended the segment
later than C and I. E and I describe the second segment
as being about the children, including who they take
after. C’s description about who the children take after
occurs at a later utterance. The third segment goes into
detail about the children’s traits, and the fourth is about
what time the speakers go to sleep.
Across all fifty conversations, ID HMM assigns
more positive labels than the majority, and DCD HMM
assigns more than ID HMM. Totals for each labeling
</bodyText>
<subsectionHeader confidence="0.363548">
Method Total
</subsectionHeader>
<bodyText confidence="0.405229">
Majority
</bodyText>
<sectionHeader confidence="0.471896" genericHeader="method">
ID HMM
DCD HMM
</sectionHeader>
<tableCaption confidence="0.994673">
Table 2: Total positive labels assigned by each method.
</tableCaption>
<table confidence="0.999858421052632">
Utt Annotators DCD HMM ID HMM
11 2 (A,I) 1.00 0.99
42 3 (B,E,I) 1.00 1.00
43 3 (A,C,D) 1.00 1.00
67 6 (A,B,C,D,E,I) 1.00 1.00
114 2 (A,C) 1.00 0.98
126 2 (D,E) 1.00 1.00
127 1 (C) 0.63 0.02
144 2 (A,D) 1.00 0.98
147 2 (C,I) 0.90 0.65
191 2 (C,I) 0.90 0.73
216 3 (A,E,I) 1.00 1.00
217 2 (C,D) 1.00 0.66
241 3 (A,C,D) 1.00 1.00
276 3 (B,C,D) 1.00 1.00
282 1 (A) 1.00 0.29
300 5 (A,B,D,E,I) 1.00 1.00
356 2 (C,I) 0.98 0.27
357 4 (A,B,D,E) 1.00 1.00
</table>
<tableCaption confidence="0.994147666666667">
Table 3: Comparison of positive predictions from majority
voting (N=8, underlined; ties are taken as positive), DDC
HMM (N=18), and the ID HMM (N=15) for conversation
945. Probabilities in bold are for boundaries proposed by
only one model; italics are for probabilities below the 0.5
threshold to be considered true boundaries.
</tableCaption>
<bodyText confidence="0.9978778125">
method are in Table 2. Wherever the majority vote
predicts a true label, both models always do. If ID
HMM posits a boundary at an utterance, DCD HMM
also does, but DCD HMM predicts additional ones.
Because all the ID HMM labels are also identified by
DCD HMM, these are the final labels we propose.
Table 3 shows the positive labels predicted for con-
versation 945 by majority vote, and by our two models.
Column one is the utterance number, and again, under-
lining indicates cases where the voted baseline would
assign a positive label. Column two lists the annota-
tors who assigned a positive label, and columns three
and four show the posteriors assigned by the two mod-
els; for all utterances not listed in the table, the posteri-
ors are below 0.5. Low posteriors for ID HMM where
DCD HMM proposed a boundary are in italics.
</bodyText>
<table confidence="0.999640857142857">
Ann Maj DCD HMM ID HMM
A 0.68 0.71 0.68
B 0.57 0.36 0.38
C 0.40 0.63 0.46
D 0.59 0.56 0.55
E 0.73 0.50 0.52
I 0.43 0.55 0.49
</table>
<tableCaption confidence="0.9735575">
Table 4: F-measure for annotators in conversation 945 for
majority vote labels and both models; recall that the true la-
bels for each model are different, and that DCD HMM hy-
pothesizes more true labels than ID HMM.
</tableCaption>
<bodyText confidence="0.993121">
For each model, the annotator can be ranked by the
F-scores relative to the model predictions. When one
of the models agrees with a minority of annotators,
</bodyText>
<page confidence="0.99270625">
683
991
1324
2197
</page>
<table confidence="0.99741225">
Model I&amp;SI DC
Majority I=1, SI=3, p=0.008 p=0.0600
DCD HMM I=2, SI=5, p=0.02 p=0.0014
ID HMM I=0, SI=0, p=0 p=0.0001
</table>
<tableCaption confidence="0.999329">
Table 5: Consistency of annotators
</tableCaption>
<bodyText confidence="0.999968826086956">
the minority consists of the annotators considered by
the model to have higher performance, as given by F-
measure. The three sets of F-scores for the six anno-
tators in 945 are shown in Table 4. Annotator perfor-
mance given the two models is very similar; the Pear-
son correlation is 0.80. F-scores based on the major-
ity baseline, however, do not correlate well with DCD
(p = −0.5) or ID (p = 0.49). In eight cases where
DCD posits a true label for conversation 945, and only
2 annotators voted positive, the pair never includes B,
the least accurate annotator by DCD (see Table 4),
and always includes one of the top three annotators
(A,C,D). In the two cases where only one annotator
voted positive, it was A or C, one of the two top DCD
HMM annotators. Both models consider A to be the
best annotator. C is relatively good in the DCD HMM
model and relatively poor in the ID HMM model.
I&amp;SI tests whether there exists a linear ordering
of the annotators such that their relative performance
across conversations is consistent. DC tests whether an
ordering ai &gt; aj is based on relatively more frequent
dominance of ai over aj. Table 5 shows that major-
ity vote and the two models produce results that lead
to high I&amp;SI consistency, based on the statistically sig-
nificant p-values. The majority vote p-value for DC,
however, is not statistically significant. By the more
stringent DC measure, the labels from the two HMM
variants are superior to the majority vote labels.
The list of descriptions from annotators at utterance
n represents the semantics of the hypothesized segment
ending at n. Semantic consistency for a given segment
serves as another check on the output of the model, be-
cause the human descriptions of the activity within the
segment do not conflict. In general, this is the case for
both models, but less so for DCD HMM. For conversa-
tion 945 illustrated in Table 3, there are three positive
labels proposed by DCD HMM that are missing from
the ID HMM predictions. These are at 127 where only
annotator C had a positive label, 282 where only anno-
tator A had a positive label, and 356 where annotators
C and I had a positive label. Annotators B, C and D, for
example, describe a segment ending at utterance 276 as
the speakers discussing Facebook, whereas annotator
A locates the end of the Facebook segment at utterance
282. The DCD HMM model posits a true label at 276
but not at 282, in contast to ID HMM.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="method">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999980857142857">
The two models for estimating ground truth labels from
crowd labels advance previous work on probabilistic
models for annotation by handling sequential data. We
have argued that for our data, the Markov assumption
must be relaxed. The two models handle this in distinct
ways. The first model assumes that each state can be
decomposed into multiple aspects, and that states and
observations are conditionally dependent on the previ-
ous point in time. The second model builds in a pa-
rameter for annotator performance, as in previous work
that adopts the Dawid and Skene (1979) model. Both
assign more ground truth labels than majority voting,
and avoid the problem with the majority vote method
of ties where there are an even number of annotators.
The results of the two models are very similar, but
DCD HMM hypothesizes more boundaries, and there-
fore ranks some annotators differently.
Here we check the models by comparing them to
each other, through analysis of each annotator’s con-
sistency across multiple conversations, and through in-
spection of the semantics of annotators’ descriptions.
Our future work will use the models generatively to
predict a subset of the data for a given annotator, based
on a model fit to all but the held out data. To do so, we
would extend the models with an additional parameter
for the conversation, to account for the observation that
while all conversations seem to fit the same empirical
distribution, there are differences across conversations.
</bodyText>
<sectionHeader confidence="0.994788" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999954761904762">
Annotation and machine learning of discourse segmen-
tation covers several types of units, including topical
segments (Galley et al., 2003), meeting units in which
action items are identified or decisions made (Purver et
al., 2007; Fern´andez et al., 2008), transaction subtasks
for ordering library books (Passonneau et al., 2014), or
speaker involvement (Bokaei et al., 2015). This work
relies on manual transcription, and draws on many
sources of knowledge for machine learned models, in-
cluding turn-taking, prosody, and linguistic features.
The segmentation annotation can be linear (Galley et
al., 2003; Bokaei et al., 2015; Passonneau and Litman,
1997; Passonneau et al., 2014) or hierarchical (Purver
et al., 2007; Fern´andez et al., 2008; Passonneau et al.,
2011). The differences in methods and results across
this body of work, points to a need for more datasets
for research on the organization of discourse into ac-
tivity units. The results presented here support this re-
search agenda by providing a reliable and cost-effective
method to estimate ground truth discourse segment la-
bels from crowd labels.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997402">
The authors thank Bob Carpenter for discussions dur-
ing the early stages of the data analysis, and for helpful
feedback on the paper. We thank the IARPA Babel pro-
gram manager, Mary Harper, for giving us permission
to annotate the Babel data.
</bodyText>
<page confidence="0.991257">
2198
</page>
<sectionHeader confidence="0.995835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985395045454546">
Paul S. Albert and Lori E. Dodd. 2008. On estimating
diagnostic accuracy from studies with multiple raters
and partial gold standard evaluation. Journal of the
American Statistical Association, 103(481):61–73.
Andr´e Berchtold. 1999. The Double Chain Markov
Model. Communications in Statistics: Theory and
Methods, 28(348):2569–2589.
Mohammad Hadi Bokaei, Hossein Sameti, and Yang
Liu. 2015. Linear discourse segmentation of multi-
party meetings based on local and global informa-
tion. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 23(11):1879–1891.
Wauter Bosma. 2005. Extending answers using dis-
course structure. In RANLP Workshop on Crossing
Barriers in Text Summarization Research.
Glenn Branch. 2014. Whence lumpers and split-
ters? http://ncse.com/blog/2014/11/
whence-lumpers-splitters-0016004,
December.
Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recog-
nizing subjectivity: a case study of manual tagging.
Natural Language Engineering, 1(1):1–16.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Journal of the Royal Statistical Society.
Series C (Applied Statistics), 28(1):20–28.
Han de Vries. 1998. Finding a dominance order most
consistent with a linear hierarchy: a new procedure
and review. Animal Behavior, 55:827–843.
Alfred Dielmann and Steve Renals. 2005. Multistream
dynamic Bayesian network for meeting segmenta-
tion. In Samy Bengio and Herv Bourlard, editors,
Machine Learning for Multimodal Interaction, vol-
ume 3361 of Lecture Notes in Computer Science,
pages 76–86. Springer Berlin Heidelberg.
Dmitriy Dligach, Rodney D. Nielsen, and Martha
Palmer. 2010. To annotate more accurately or to an-
notate more. In Proceedings of the Fourth Linguistic
Annotation Workshop (LAW IV), pages 64–72.
Raquel Fern´andez, Matthew Frampton, Patrick Ehlen,
Matthew Purver, and Stanley Peters. 2008. Mod-
elling and detecting decisions in multi-party dia-
logue. In Proceedings of the 9th SIGdial Workshop
on Discourse and Dialogue.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with
crowdsourcing. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, CSLDAMT
’10, pages 80–88, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 562–569,
Sapporo, Japan, July. Association for Computational
Linguistics.
Sebastian Germesin and Theresa Wilson. 2009.
Agreement detection in multiparty conversation. In
Proceedings of the 2009 International Conference
on Multimodal Interfaces, ICMI-MLMI ’09, pages
7–14, New York, NY, USA. ACM.
Dustin Hillard, Mari Ostendorf, and Elizabeth
Shriberg. 2003. Detection of agreement vs. dis-
agreement in meetings: Training with unlabeled
data. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy: Companion Volume of the Proceedings of HLT-
NAACL 2003–short Papers - Volume 2, NAACL-
Short ’03, pages 34–36, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with MACE. In Proceedings of the 2013 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT), pages 1120–1130, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Klaus Krippendorff. 1980. Content analysis: An intro-
duction to its methodology. Sage Publications, Bev-
erly Hills, CA.
David Leiva, Antonio Solanas, and Lluis Salafranca.
2008. Testing reciprocity in social interactions: A
comparison between the directional consistency and
skew-symmetry statistics. Behavior Research Meth-
ods, 40(2):626–634.
Miriam Martinez and L. Enrique Sucar. 2008. Learn-
ing dynamic Naive Bayesian classifiers. In Proceed-
ings of the Twenty-First International FLAIRS Con-
ference, pages 655–659.
Kevin Murphy. 2001. Bayes Net toolbox for Matlab.
Computing Science and Statistics, 33(2):1024–1034.
John Niekrasz and Johanna Moore. 2009. Participant
subjectivity and involvement as a basis for discourse
segmentation. In Proceedings of the SIGDIAL 2009
Conference, pages 54–61, London, UK, September.
Association for Computational Linguistics.
Rebecca J. Passonneau and Bob Carpenter. 2014. The
benefits of a model of annotation. Transactions
of the Association for Computational Linguistics,
2:311326.
Rebecca J. Passonneau and Diane Litman. 1997.
Discourse segmentation by human and automated
means. Computational Linguistics, 23.1:103–139.
Special Issue on Empirical Studies in Discourse In-
terpretation and Generation.
</reference>
<page confidence="0.925626">
2199
</page>
<reference confidence="0.999860097222222">
Rebecca J. Passonneau, Irene Alvarado, Phil Crone,
and Simon Jerome. 2011. PARADISE-style evalua-
tion of a human-human library corpus. In Proceed-
ings of the SIGDIAL 2011 Conference, pages 325–
331, Portland, Oregon, June. Association for Com-
putational Linguistics.
Rebecca J. Passonneau, Boxuan Guan, Cho Ho Ye-
ung, Yuan Du, and Emma Conner. 2014. Aspectual
properties of conversational activities. In Proceed-
ings of the 15th Annual Meeting of the Special Inter-
est Group on Discourse and Dialogue (SIGDIAL),
pages 228–237, Philadelphia, PA, U.S.A., June. As-
sociation for Computational Linguistics.
Matthew Purver, John Dowding, John Niekrasz,
Patrick Ehlen, Sharareh Noorbaloochi, and Stanley
Peters. 2007. Detecting and summarizing action
items in multi-party dialogue. In Proceedings of the
8th SIGdial Workshop on Discourse and Dialogue.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
Journal of Machine Learning Research, 11:1297–
1322.
Simon Rogers, Mark Girolami, and Tamara Polajnar.
2010. Semi-parametric analysis of multi-rater data.
Statistical Computing, 20:317–334.
Andrey Rzhetsky, Hagit Shatkay, and W. John Wilbur.
2009. How to get the most out of your curation ef-
fort. PLoS Computational Biology., 5(5):1–13.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? Improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of the Fourteenth ACM Inter-
national Conference on Knowledge Discovery and
Data Mining (KDD).
Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro
Perona, and Pierre Baldi. 1995. Inferring ground
truth from subjectively-labeled images of Venus. In
Advances in Neural Information Processing Systems
7, pages 1085–1092. MIT Press.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of Empirical
Methods in Natural Language Processing (EMNLP),
pages 254–263, Honolulu.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
meetings. In Proceedings of the 8th SIGdial Work-
shop on Discourse and Dialogue (SIGdial 07), page
2634.
Mingyu Sun and Joyce Y. Chai. 2007. Discourse pro-
cessing for context question answering based on lin-
guistic knowledge. Know.-Based Syst., 20(6):511–
526, August.
Lu Wang and Claire Cardie. 2012. Focused meet-
ing summarization via unsupervised relation extrac-
tion. In Proceedings of the 13th Annual Meeting of
the Special Interest Group on Discourse and Dia-
logue, SIGDIAL ’12, pages 304–313, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Nigel G. Ward, Steven D. Werner, Fernando Garcia,
and Emilio Sanchis. 2015. A prosody-based vector-
space model of dialog activity for information re-
trieval. Speech Communication, 28:85–96.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In Proceedings
of the 24th Annual Conference on Advances in Neu-
ral Information Processing Systems.
</reference>
<page confidence="0.985582">
2200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953929">
<title confidence="0.998253">Estimation of Discourse Segmentation Labels from Crowd Data</title>
<author confidence="0.996304">Ziheng Jialu Zhong Rebecca J</author>
<affiliation confidence="0.988295">Department of Department of Computer Science Columbia University Center for Columbia University jialu.zhong@columbia.edu Learning zh2220@columbia.edu Columbia University</affiliation>
<email confidence="0.999328">becky@ccls.columbia.edu</email>
<abstract confidence="0.999724565217391">For annotation tasks involving independent judgments, probabilistic models have been used to infer ground truth labels from data where a crowd of many annotators labels the same items. Such models have been shown to produce results superior to taking the majority vote, but have not been applied to sequential data. We present two methods to infer ground truth labels from sequential annotations where we assume judgments are not independent, based on the observation that an annotator’s segments all tend to be several utterances long. The data consists of crowd labels for annotation of discourse segment boundaries. The new methods extend Hidden Markov Models to relax the independence assumption. The two methods are distinct, so positive labels proposed by both are taken to be ground truth. In addition, results of the models are checked using metrics that test whether an annotator’s accuracy relative to a given model remains consistent across different conversations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul S Albert</author>
<author>Lori E Dodd</author>
</authors>
<title>On estimating diagnostic accuracy from studies with multiple raters and partial gold standard evaluation.</title>
<date>2008</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>103</volume>
<issue>481</issue>
<contexts>
<context position="8381" citStr="Albert and Dodd, 2008" startWordPosition="1289" endWordPosition="1292">actions (Passonneau et al., 2011) measured agreement among two annotators using Krippendorff’s Alpha at an average of 0.77 (Krippendorff, 1980). The annotation task here mostly closely resembles (Passonneau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identification of craters in images of Venus (Smyth et al., 1995), curation of biological data (Rzhetsky et al., 2009), computer vision (Whitehill et al., 2009), patient history (Dawid and Skene, 1979), and clinical reports (2010). Smyth et al. (1995), Rogers et al., and (2010) and Raykar et al. (2010) discuss the advantages of probabilistically annotated corpora over majority vote. Much of this work is motivated by the observation that annotators have different accuracies, and the fact that when annotators have known accuracies it can be shown that a majority of inaccurate annotators can be</context>
</contexts>
<marker>Albert, Dodd, 2008</marker>
<rawString>Paul S. Albert and Lori E. Dodd. 2008. On estimating diagnostic accuracy from studies with multiple raters and partial gold standard evaluation. Journal of the American Statistical Association, 103(481):61–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Berchtold</author>
</authors>
<title>The Double Chain Markov Model.</title>
<date>1999</date>
<booktitle>Communications in Statistics: Theory and Methods,</booktitle>
<pages>28--348</pages>
<contexts>
<context position="6210" citStr="Berchtold, 1999" startWordPosition="952" endWordPosition="953">us on a positive label is rare, but does occur. Here, all eight annotators assigned a positive label at utterance 120, six at utterance 178, and five at utterance 140. Our work assumes that unobserved true labels condition the annotators’ observed labels, and can be modeled as hidden states in a Markov-type process. Because an annotator rarely assigns positive labels for adjacent utterances, we assume that neither the true labels nor the observations are conditionally independent, and therefore are not generated by a simple Markov process. Our first model adapts the Double Chain Markov Model (Berchtold, 1999), designed to account for such cases. We then propose a second model that assumes that each annotator’s labels are drawn from a Bernoulli distribution, that annotator performance is a parameter of the model, and that the state transitions are conditioned by an empirical distribution of discourse segment lengths. The two methods are quite distinct. Each thus serves as an evaluation of the other. The segment boundaries proposed by both models include all the majority vote cases, and in addition, cases voted on by a minority of relatively accurate annotators. We take segment boundaries proposed b</context>
<context position="17838" citStr="Berchtold, 1999" startWordPosition="2882" endWordPosition="2883">mission matrix is a 2 × 2 × 2 matrix as each observed state depends on current hidden state as well as the previous observation, i.e., bxt,y(t−1)l,ytl is the probability of emitting from xt to ytl and transitioning from y(t−1)l to ytl. A graphical sketch of the DCD HMM model is shown in Figure 3. The target function F = P(x, y|Θ) is: x1 x2 x3 xn−1 xn y11 y21 y31 yn−11 yn1 y12 y22 y32 yn−12 yn2 y1m y2m y3m yn−1m ynm 5 Double Chain Dynamic Hidden F = πx1 HM cx1,y1l n axt−1,xt HM n bxt,y(t−1)l,ytl Markov Model l=1 H l=1 H t=2 t=2 The first model we propose combines the Double Chain Markov Model (Berchtold, 1999) and dynamic Bayesian networks (Martinez and Sucar, 2008). The double chaining involves the dependence of observations on immediately prior observations. Figure 3 shows that for all ytl, t ≥ 2, observation ytl depends on observation y(t−1)l. The emission matrix at the first utterance x1 is thus a 2×2 matrix, while all subsequent emission matrices are 2 × 2 × 2. As in (Martinez and Sucar, 2008), the observed states can be regarded as a composition of m independent chains, where m is the number of annotators for the conversation. Also, the lth annotator’s observation at the tth utterance depends</context>
</contexts>
<marker>Berchtold, 1999</marker>
<rawString>Andr´e Berchtold. 1999. The Double Chain Markov Model. Communications in Statistics: Theory and Methods, 28(348):2569–2589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Hadi Bokaei</author>
<author>Hossein Sameti</author>
<author>Yang Liu</author>
</authors>
<title>Linear discourse segmentation of multiparty meetings based on local and global information.</title>
<date>2015</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>23</volume>
<issue>11</issue>
<contexts>
<context position="1795" citStr="Bokaei et al., 2015" startWordPosition="262" endWordPosition="265">re checked using metrics that test whether an annotator’s accuracy relative to a given model remains consistent across different conversations. 1 Introduction A single, spontaneous, spoken interaction can consist of multiple activities, such as to plan a future event, to complain about a past situation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fe</context>
<context position="7708" citStr="Bokaei et al., 2015" startWordPosition="1192" endWordPosition="1195">methods to derive ground truth segment boundaries. In (Passonneau and Litman, 1997), seven annotators annotated narrative monologues for segments based on speaker intention. Agreement levels for ground truth boundaries were based on statistical significance using Cochran’s Q. In (Galley et al., 2003), three annotators segmented the ICSI meeting corpus into topical units, and majority agreement was taken as ground truth. A functional segmentation of meetings from the AMI multiparty meeting corpus based on involved participants was segmented by one annotator and finalized by a second annotator (Bokaei et al., 2015). Task-based segmentation of patron-librarian interactions (Passonneau et al., 2011) measured agreement among two annotators using Krippendorff’s Alpha at an average of 0.77 (Krippendorff, 1980). The annotation task here mostly closely resembles (Passonneau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Applic</context>
<context position="41426" citStr="Bokaei et al., 2015" startWordPosition="7210" endWordPosition="7213">so, we would extend the models with an additional parameter for the conversation, to account for the observation that while all conversations seem to fit the same empirical distribution, there are differences across conversations. 10 Conclusion Annotation and machine learning of discourse segmentation covers several types of units, including topical segments (Galley et al., 2003), meeting units in which action items are identified or decisions made (Purver et al., 2007; Fern´andez et al., 2008), transaction subtasks for ordering library books (Passonneau et al., 2014), or speaker involvement (Bokaei et al., 2015). This work relies on manual transcription, and draws on many sources of knowledge for machine learned models, including turn-taking, prosody, and linguistic features. The segmentation annotation can be linear (Galley et al., 2003; Bokaei et al., 2015; Passonneau and Litman, 1997; Passonneau et al., 2014) or hierarchical (Purver et al., 2007; Fern´andez et al., 2008; Passonneau et al., 2011). The differences in methods and results across this body of work, points to a need for more datasets for research on the organization of discourse into activity units. The results presented here support th</context>
</contexts>
<marker>Bokaei, Sameti, Liu, 2015</marker>
<rawString>Mohammad Hadi Bokaei, Hossein Sameti, and Yang Liu. 2015. Linear discourse segmentation of multiparty meetings based on local and global information. IEEE Transactions on Audio, Speech and Language Processing, 23(11):1879–1891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wauter Bosma</author>
</authors>
<title>Extending answers using discourse structure.</title>
<date>2005</date>
<booktitle>In RANLP Workshop on Crossing Barriers in Text Summarization Research.</booktitle>
<contexts>
<context position="2476" citStr="Bosma, 2005" startWordPosition="371" endWordPosition="372">vide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment.</context>
</contexts>
<marker>Bosma, 2005</marker>
<rawString>Wauter Bosma. 2005. Extending answers using discourse structure. In RANLP Workshop on Crossing Barriers in Text Summarization Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Branch</author>
</authors>
<title>Whence lumpers and splitters? http://ncse.com/blog/2014/11/</title>
<date>2014</date>
<pages>0016004</pages>
<contexts>
<context position="14234" citStr="Branch, 2014" startWordPosition="2249" endWordPosition="2250">th label for each utterance position, where the label values represent a binary classification of segment boundaries. Our two models each assume there is a hidden true label that conditions an annotator’s observed labels, and that can be estimated from the observed labels. How well the estimated ground truth fits the data thus depends on how well the model assumptions accord with the phenomenon of interest. The models do not account for annotator differences in the level of granularity they apply; cf. the contrast between lumpers and splitters in taxonomic classification of the natural world (Branch, 2014). Further, neither model takes linguistic features into account that annotators consider in deciding on segments, such as speaker attitude towards utterance content or speaker role in the conversational activity (Niekrasz and Moore, 2009). We find, however, much agreement between the two models on the proposed segment boundaries, and leave for future work the question of whether more complex models could accoount for differences in granularity or utterance features. As discussed in section 2, we assume that annotators are not equally accurate, and that a probabilistic model based on the distri</context>
</contexts>
<marker>Branch, 2014</marker>
<rawString>Glenn Branch. 2014. Whence lumpers and splitters? http://ncse.com/blog/2014/11/ whence-lumpers-splitters-0016004, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca F Bruce</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Recognizing subjectivity: a case study of manual tagging.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2908" citStr="Bruce and Wiebe, 1999" startWordPosition="434" endWordPosition="437"> al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment. Three advantages to corpus annotation through application of a probabilistic model to crowdsourced labels, rather than reliance on interannotator agreement computed for a small number of trained annotators, are higher quality, lower cost, and a posterior probability for each ground truth label (Sheng et al., 2008; Snow et al., 2008; Passonneau and Carpenter, 2014). The latter serves as a confidence measure, which contrasts with</context>
<context position="9406" citStr="Bruce and Wiebe (1999)" startWordPosition="1456" endWordPosition="1459">rk is motivated by the observation that annotators have different accuracies, and the fact that when annotators have known accuracies it can be shown that a majority of inaccurate annotators can be wrong (Raykar et al., 2010; Passonneau and Carpenter, 2014). Equally important, information from inaccurate annotators informs the model inference. For example, an inaccurate annotator might be biased towards label m whenever the true label is z. Dawid and Skene (1979) present a joint model of true labels, observed labels, and annotator performance. Perhaps its first application to NLP data was the Bruce and Wiebe (1999) investigation of word sense. It has also been applied to more fine-grained word sense with a direct comparison to trained annotator labels in (Passonneau and Carpenter, 2014). Snow et al. (2008) showed that application of the same model to noisy crowd annotations produced data of equal quality to five distinct published gold standards. Hovy et al. (2013) apply a simple and effective model to identify untrustworthy annotators and test it on the same datasets used in (Snow et al., 2008). As they point out, when ties occur among an even number of annotators, it’s necessary to resort to a tie-bre</context>
<context position="15012" citStr="Bruce and Wiebe, 1999" startWordPosition="2372" endWordPosition="2375">content or speaker role in the conversational activity (Niekrasz and Moore, 2009). We find, however, much agreement between the two models on the proposed segment boundaries, and leave for future work the question of whether more complex models could accoount for differences in granularity or utterance features. As discussed in section 2, we assume that annotators are not equally accurate, and that a probabilistic model based on the distribution of observed labels can do better than majority vote. Inspired by the type of probabilistic model proposed in (Dawid and Skene, 1979) and extended in (Bruce and Wiebe, 1999; Passonneau and Carpenter, 2014), annotator accuracy is a parameter of our second model. As described in detail in subsequent sections, the two models proposed here rely on distinct assumptions and inference methods. They nevertheless propose many of the same labels. We take each model to provide independent evidence for the ground truth labels, thus the final labels are those voted on by both models. 2192 Figure 3: Graphical model of Double Chain Dynamic Hidden Markov Model for a conversation with m annotators and n utterances. The xt are the hidden states, and the yjl are the observed label</context>
</contexts>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recognizing subjectivity: a case study of manual tagging. Natural Language Engineering, 1(1):1–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dawid</author>
<author>A M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the EM algorithm.</title>
<date>1979</date>
<journal>Journal of the Royal Statistical Society. Series C (Applied Statistics),</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="3909" citStr="Dawid and Skene, 1979" startWordPosition="592" endWordPosition="595">higher quality, lower cost, and a posterior probability for each ground truth label (Sheng et al., 2008; Snow et al., 2008; Passonneau and Carpenter, 2014). The latter serves as a confidence measure, which contrasts with interannotator agreement measures and with majority-voted labels, neither of which provides quality information for the ground truth labels on individual items. Previous work has demonstrated that model estimation of ground truth labels from crowd labels produces results superior to the crowd’s majority vote, due to differences among annotators in the quality of their labels (Dawid and Skene, 1979; Snow et al., 2008; Passonneau and Carpenter, 2014). No previous work, however, provides modelbased estimation of labels for sequential annotation from crowd labels. For the discourse segmentation data presented here, annotators were presented with audio files of conversations and corresponding transcriptions into utterances. The annotation task was to identify each utterance that completes a discourse segment spanning one or more utterances, based on the speakers’ conversational activities or intentions, as in (Passonneau and Litman, 1997). The annotations from y annotators for a conversatio</context>
<context position="8584" citStr="Dawid and Skene, 1979" startWordPosition="1321" endWordPosition="1324">eau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identification of craters in images of Venus (Smyth et al., 1995), curation of biological data (Rzhetsky et al., 2009), computer vision (Whitehill et al., 2009), patient history (Dawid and Skene, 1979), and clinical reports (2010). Smyth et al. (1995), Rogers et al., and (2010) and Raykar et al. (2010) discuss the advantages of probabilistically annotated corpora over majority vote. Much of this work is motivated by the observation that annotators have different accuracies, and the fact that when annotators have known accuracies it can be shown that a majority of inaccurate annotators can be wrong (Raykar et al., 2010; Passonneau and Carpenter, 2014). Equally important, information from inaccurate annotators informs the model inference. For example, an inaccurate annotator might be biased t</context>
<context position="14973" citStr="Dawid and Skene, 1979" startWordPosition="2365" endWordPosition="2368">h as speaker attitude towards utterance content or speaker role in the conversational activity (Niekrasz and Moore, 2009). We find, however, much agreement between the two models on the proposed segment boundaries, and leave for future work the question of whether more complex models could accoount for differences in granularity or utterance features. As discussed in section 2, we assume that annotators are not equally accurate, and that a probabilistic model based on the distribution of observed labels can do better than majority vote. Inspired by the type of probabilistic model proposed in (Dawid and Skene, 1979) and extended in (Bruce and Wiebe, 1999; Passonneau and Carpenter, 2014), annotator accuracy is a parameter of our second model. As described in detail in subsequent sections, the two models proposed here rely on distinct assumptions and inference methods. They nevertheless propose many of the same labels. We take each model to provide independent evidence for the ground truth labels, thus the final labels are those voted on by both models. 2192 Figure 3: Graphical model of Double Chain Dynamic Hidden Markov Model for a conversation with m annotators and n utterances. The xt are the hidden sta</context>
<context position="40126" citStr="Dawid and Skene (1979)" startWordPosition="7003" endWordPosition="7006">t not at 282, in contast to ID HMM. 9 Discussion The two models for estimating ground truth labels from crowd labels advance previous work on probabilistic models for annotation by handling sequential data. We have argued that for our data, the Markov assumption must be relaxed. The two models handle this in distinct ways. The first model assumes that each state can be decomposed into multiple aspects, and that states and observations are conditionally dependent on the previous point in time. The second model builds in a parameter for annotator performance, as in previous work that adopts the Dawid and Skene (1979) model. Both assign more ground truth labels than majority voting, and avoid the problem with the majority vote method of ties where there are an even number of annotators. The results of the two models are very similar, but DCD HMM hypothesizes more boundaries, and therefore ranks some annotators differently. Here we check the models by comparing them to each other, through analysis of each annotator’s consistency across multiple conversations, and through inspection of the semantics of annotators’ descriptions. Our future work will use the models generatively to predict a subset of the data </context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. P. Dawid and A. M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han de Vries</author>
</authors>
<title>Finding a dominance order most consistent with a linear hierarchy: a new procedure and review. Animal Behavior,</title>
<date>1998</date>
<pages>55--827</pages>
<marker>de Vries, 1998</marker>
<rawString>Han de Vries. 1998. Finding a dominance order most consistent with a linear hierarchy: a new procedure and review. Animal Behavior, 55:827–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred Dielmann</author>
<author>Steve Renals</author>
</authors>
<title>Multistream dynamic Bayesian network for meeting segmentation.</title>
<date>2005</date>
<booktitle>In Samy Bengio and Herv Bourlard, editors, Machine Learning for Multimodal Interaction,</booktitle>
<volume>3361</volume>
<pages>76--86</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="2047" citStr="Dielmann and Renals, 2005" startWordPosition="301" endWordPosition="304">to plan a future event, to complain about a past situation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two</context>
</contexts>
<marker>Dielmann, Renals, 2005</marker>
<rawString>Alfred Dielmann and Steve Renals. 2005. Multistream dynamic Bayesian network for meeting segmentation. In Samy Bengio and Herv Bourlard, editors, Machine Learning for Multimodal Interaction, volume 3361 of Lecture Notes in Computer Science, pages 76–86. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Rodney D Nielsen</author>
<author>Martha Palmer</author>
</authors>
<title>To annotate more accurately or to annotate more.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Linguistic Annotation Workshop (LAW IV),</booktitle>
<pages>64--72</pages>
<contexts>
<context position="10207" citStr="Dligach et al. (2010)" startWordPosition="1593" endWordPosition="1596">w et al. (2008) showed that application of the same model to noisy crowd annotations produced data of equal quality to five distinct published gold standards. Hovy et al. (2013) apply a simple and effective model to identify untrustworthy annotators and test it on the same datasets used in (Snow et al., 2008). As they point out, when ties occur among an even number of annotators, it’s necessary to resort to a tie-breaking procedure, e.g., for utterance 155 in Figure 1 where four annotators assign a positive label and four do not. In experiments on an existing dataset of word sense annotation, Dligach et al. (2010) compare singly annotated data with doubly annotated adjudicated data, using trained annotators. They find that with the same amount of data, machine learning performance improves with the doubly annotated adjudicated data by 2191 Figure 2: The annotation interface presented the audio control button on the upper left and the transcript below, with a scroll bar (not shown). Utterances from the two speakers are on the right and left sides, respectively. Each utterance had a checkbox; when selected, a textbox appeared to allow annotators to enter their segment descriptions. a small amount, but th</context>
</contexts>
<marker>Dligach, Nielsen, Palmer, 2010</marker>
<rawString>Dmitriy Dligach, Rodney D. Nielsen, and Martha Palmer. 2010. To annotate more accurately or to annotate more. In Proceedings of the Fourth Linguistic Annotation Workshop (LAW IV), pages 64–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Fern´andez</author>
<author>Matthew Frampton</author>
<author>Patrick Ehlen</author>
<author>Matthew Purver</author>
<author>Stanley Peters</author>
</authors>
<title>Modelling and detecting decisions in multi-party dialogue.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<marker>Fern´andez, Frampton, Ehlen, Purver, Peters, 2008</marker>
<rawString>Raquel Fern´andez, Matthew Frampton, Patrick Ehlen, Matthew Purver, and Stanley Peters. 2008. Modelling and detecting decisions in multi-party dialogue. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>Will Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10,</booktitle>
<pages>80--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2997" citStr="Finin et al., 2010" startWordPosition="448" endWordPosition="451">7), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment. Three advantages to corpus annotation through application of a probabilistic model to crowdsourced labels, rather than reliance on interannotator agreement computed for a small number of trained annotators, are higher quality, lower cost, and a posterior probability for each ground truth label (Sheng et al., 2008; Snow et al., 2008; Passonneau and Carpenter, 2014). The latter serves as a confidence measure, which contrasts with interannotator agreement measures and with majority-voted labels, neither of which provi</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in twitter data with crowdsourcing. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10, pages 80–88, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen R McKeown</author>
<author>Eric FoslerLussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>562--569</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1816" citStr="Galley et al., 2003" startWordPosition="266" endWordPosition="269">ics that test whether an annotator’s accuracy relative to a given model remains consistent across different conversations. 1 Introduction A single, spontaneous, spoken interaction can consist of multiple activities, such as to plan a future event, to complain about a past situation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008</context>
<context position="7389" citStr="Galley et al., 2003" startWordPosition="1140" endWordPosition="1143">rs. We take segment boundaries proposed by both methods as ground truth. To further assess the results of the models, we assume that an annotator’s accuracies should be consistent across the conversations she annotates. 2 Related Work Previous work on annotation of discourse into linear segments has used a variety of methods to derive ground truth segment boundaries. In (Passonneau and Litman, 1997), seven annotators annotated narrative monologues for segments based on speaker intention. Agreement levels for ground truth boundaries were based on statistical significance using Cochran’s Q. In (Galley et al., 2003), three annotators segmented the ICSI meeting corpus into topical units, and majority agreement was taken as ground truth. A functional segmentation of meetings from the AMI multiparty meeting corpus based on involved participants was segmented by one annotator and finalized by a second annotator (Bokaei et al., 2015). Task-based segmentation of patron-librarian interactions (Passonneau et al., 2011) measured agreement among two annotators using Krippendorff’s Alpha at an average of 0.77 (Krippendorff, 1980). The annotation task here mostly closely resembles (Passonneau and Litman, 1997), and </context>
<context position="41188" citStr="Galley et al., 2003" startWordPosition="7174" endWordPosition="7177">ations, and through inspection of the semantics of annotators’ descriptions. Our future work will use the models generatively to predict a subset of the data for a given annotator, based on a model fit to all but the held out data. To do so, we would extend the models with an additional parameter for the conversation, to account for the observation that while all conversations seem to fit the same empirical distribution, there are differences across conversations. 10 Conclusion Annotation and machine learning of discourse segmentation covers several types of units, including topical segments (Galley et al., 2003), meeting units in which action items are identified or decisions made (Purver et al., 2007; Fern´andez et al., 2008), transaction subtasks for ordering library books (Passonneau et al., 2014), or speaker involvement (Bokaei et al., 2015). This work relies on manual transcription, and draws on many sources of knowledge for machine learned models, including turn-taking, prosody, and linguistic features. The segmentation annotation can be linear (Galley et al., 2003; Bokaei et al., 2015; Passonneau and Litman, 1997; Passonneau et al., 2014) or hierarchical (Purver et al., 2007; Fern´andez et al.</context>
</contexts>
<marker>Galley, McKeown, FoslerLussier, Jing, 2003</marker>
<rawString>Michel Galley, Kathleen R. McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 562–569, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Germesin</author>
<author>Theresa Wilson</author>
</authors>
<title>Agreement detection in multiparty conversation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 International Conference on Multimodal Interfaces, ICMI-MLMI ’09,</booktitle>
<pages>7--14</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2325" citStr="Germesin and Wilson, 2009" startWordPosition="345" endWordPosition="348">discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 20</context>
</contexts>
<marker>Germesin, Wilson, 2009</marker>
<rawString>Sebastian Germesin and Theresa Wilson. 2009. Agreement detection in multiparty conversation. In Proceedings of the 2009 International Conference on Multimodal Interfaces, ICMI-MLMI ’09, pages 7–14, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dustin Hillard</author>
<author>Mari Ostendorf</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Detection of agreement vs. disagreement in meetings: Training with unlabeled data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Companion Volume of the Proceedings of HLTNAACL 2003–short Papers -</booktitle>
<volume>2</volume>
<pages>34--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2270" citStr="Hillard et al., 2003" startWordPosition="337" endWordPosition="340"> such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples inc</context>
</contexts>
<marker>Hillard, Ostendorf, Shriberg, 2003</marker>
<rawString>Dustin Hillard, Mari Ostendorf, and Elizabeth Shriberg. 2003. Detection of agreement vs. disagreement in meetings: Training with unlabeled data. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Companion Volume of the Proceedings of HLTNAACL 2003–short Papers - Volume 2, NAACLShort ’03, pages 34–36, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Ashish Vaswani</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning whom to trust with MACE.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>1120--1130</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="9763" citStr="Hovy et al. (2013)" startWordPosition="1515" endWordPosition="1518"> inaccurate annotator might be biased towards label m whenever the true label is z. Dawid and Skene (1979) present a joint model of true labels, observed labels, and annotator performance. Perhaps its first application to NLP data was the Bruce and Wiebe (1999) investigation of word sense. It has also been applied to more fine-grained word sense with a direct comparison to trained annotator labels in (Passonneau and Carpenter, 2014). Snow et al. (2008) showed that application of the same model to noisy crowd annotations produced data of equal quality to five distinct published gold standards. Hovy et al. (2013) apply a simple and effective model to identify untrustworthy annotators and test it on the same datasets used in (Snow et al., 2008). As they point out, when ties occur among an even number of annotators, it’s necessary to resort to a tie-breaking procedure, e.g., for utterance 155 in Figure 1 where four annotators assign a positive label and four do not. In experiments on an existing dataset of word sense annotation, Dligach et al. (2010) compare singly annotated data with doubly annotated adjudicated data, using trained annotators. They find that with the same amount of data, machine learni</context>
</contexts>
<marker>Hovy, Berg-Kirkpatrick, Vaswani, Hovy, 2013</marker>
<rawString>Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with MACE. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1120–1130, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content analysis: An introduction to its methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="7902" citStr="Krippendorff, 1980" startWordPosition="1220" endWordPosition="1221">ground truth boundaries were based on statistical significance using Cochran’s Q. In (Galley et al., 2003), three annotators segmented the ICSI meeting corpus into topical units, and majority agreement was taken as ground truth. A functional segmentation of meetings from the AMI multiparty meeting corpus based on involved participants was segmented by one annotator and finalized by a second annotator (Bokaei et al., 2015). Task-based segmentation of patron-librarian interactions (Passonneau et al., 2011) measured agreement among two annotators using Krippendorff’s Alpha at an average of 0.77 (Krippendorff, 1980). The annotation task here mostly closely resembles (Passonneau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identification of craters in images of Venus (Smyth et al., 1995), curation of biological data (Rzhetsky et al., 2009),</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content analysis: An introduction to its methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Leiva</author>
<author>Antonio Solanas</author>
<author>Lluis Salafranca</author>
</authors>
<title>Testing reciprocity in social interactions: A comparison between the directional consistency and skew-symmetry statistics.</title>
<date>2008</date>
<journal>Behavior Research Methods,</journal>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="30514" citStr="Leiva et al., 2008" startWordPosition="5291" endWordPosition="5294">linear ordering is incremented by the difference in rank between ai and aj for every inconsistent pair in the linear ordering. The I&amp;SI method finds an ordering that minimizes I and SI. To check the results of our models, we compare the I&amp;SI value of the dominance matrix associated with the model results against a simulated random matrix. If the model results are significantly more consistent than the simulation, the model produces a consistent ranking of annotators. We propose a Directional Consistency index (DC E [0, 1]) which considers the number of times ai has a higher F measure than aj (Leiva et al., 2008). Where X is the dominance matrix: DC = Pi=1 Pnj=i+1 |xij − x ji | N Xn j=1,j#i DC values closer to zero indicate less consistency in differences among annotators, and the converse for values closer to 1. High DC values for the results of our models thus indicates better performance of the model in predicting consistent annotator behavior. 8 Results and Model Checking The results consist of the true labels assigned by each model to each conversation, and estimates of the annotators’ performance relative to the model’s ground truth labels. Note that as the conversation is not a parameter of eit</context>
</contexts>
<marker>Leiva, Solanas, Salafranca, 2008</marker>
<rawString>David Leiva, Antonio Solanas, and Lluis Salafranca. 2008. Testing reciprocity in social interactions: A comparison between the directional consistency and skew-symmetry statistics. Behavior Research Methods, 40(2):626–634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Martinez</author>
<author>L Enrique Sucar</author>
</authors>
<title>Learning dynamic Naive Bayesian classifiers.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty-First International FLAIRS Conference,</booktitle>
<pages>655--659</pages>
<contexts>
<context position="17895" citStr="Martinez and Sucar, 2008" startWordPosition="2888" endWordPosition="2891">erved state depends on current hidden state as well as the previous observation, i.e., bxt,y(t−1)l,ytl is the probability of emitting from xt to ytl and transitioning from y(t−1)l to ytl. A graphical sketch of the DCD HMM model is shown in Figure 3. The target function F = P(x, y|Θ) is: x1 x2 x3 xn−1 xn y11 y21 y31 yn−11 yn1 y12 y22 y32 yn−12 yn2 y1m y2m y3m yn−1m ynm 5 Double Chain Dynamic Hidden F = πx1 HM cx1,y1l n axt−1,xt HM n bxt,y(t−1)l,ytl Markov Model l=1 H l=1 H t=2 t=2 The first model we propose combines the Double Chain Markov Model (Berchtold, 1999) and dynamic Bayesian networks (Martinez and Sucar, 2008). The double chaining involves the dependence of observations on immediately prior observations. Figure 3 shows that for all ytl, t ≥ 2, observation ytl depends on observation y(t−1)l. The emission matrix at the first utterance x1 is thus a 2×2 matrix, while all subsequent emission matrices are 2 × 2 × 2. As in (Martinez and Sucar, 2008), the observed states can be regarded as a composition of m independent chains, where m is the number of annotators for the conversation. Also, the lth annotator’s observation at the tth utterance depends not only on the same hidden state xt, but also on the la</context>
</contexts>
<marker>Martinez, Sucar, 2008</marker>
<rawString>Miriam Martinez and L. Enrique Sucar. 2008. Learning dynamic Naive Bayesian classifiers. In Proceedings of the Twenty-First International FLAIRS Conference, pages 655–659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Murphy</author>
</authors>
<title>Bayes Net toolbox for Matlab.</title>
<date>2001</date>
<journal>Computing Science and Statistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="19116" citStr="Murphy, 2001" startWordPosition="3123" endWordPosition="3124">vation y(t−1)l. Assume in a conversation, there are m annotators and n utterances. The model Θ = {π, γ, A, B} can be described as follows: • a set of hidden states, i.e the true labels: xt ∈ {0,1}, t ∈ {1, 2, ... , n}. xt = 1 represents the tth utterance is a true boundary and 0 otherwise; • a set of observed variables: ytl ∈ {0, 1}, l ∈ {1, 2, ... , m} annotators, t ∈ {1, 2, ... , n} utterWe can derive a marginal distribution over y and have the likelihood as: L(Θ) = P (y|Θ) = � P(x, y|Θ) x Our goal is to find the parameters (Θ) that maximize the above function. Bayes Net Toolbox for Matlab (Murphy, 2001) is used for the inference. ExpectationMaximization (EM) with Junction Tree inference for the E-step is used for learning the parameters. The Junction Tree Algorithm is a method to calculate marginals by propagation on the graph. It runs as follows: 1) Initialize: Pick a proper root and initialize all variables; 2) Collect: Pass message from each child of a node through separators to the parent node and update the node with collected evidence; 3) Distribute: Send back message to each child of the node through separators and update the child with distributed evidence; 4) Normalize: Normalize cl</context>
</contexts>
<marker>Murphy, 2001</marker>
<rawString>Kevin Murphy. 2001. Bayes Net toolbox for Matlab. Computing Science and Statistics, 33(2):1024–1034.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Niekrasz</author>
<author>Johanna Moore</author>
</authors>
<title>Participant subjectivity and involvement as a basis for discourse segmentation.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<pages>54--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>London, UK,</location>
<contexts>
<context position="14472" citStr="Niekrasz and Moore, 2009" startWordPosition="2282" endWordPosition="2285">that can be estimated from the observed labels. How well the estimated ground truth fits the data thus depends on how well the model assumptions accord with the phenomenon of interest. The models do not account for annotator differences in the level of granularity they apply; cf. the contrast between lumpers and splitters in taxonomic classification of the natural world (Branch, 2014). Further, neither model takes linguistic features into account that annotators consider in deciding on segments, such as speaker attitude towards utterance content or speaker role in the conversational activity (Niekrasz and Moore, 2009). We find, however, much agreement between the two models on the proposed segment boundaries, and leave for future work the question of whether more complex models could accoount for differences in granularity or utterance features. As discussed in section 2, we assume that annotators are not equally accurate, and that a probabilistic model based on the distribution of observed labels can do better than majority vote. Inspired by the type of probabilistic model proposed in (Dawid and Skene, 1979) and extended in (Bruce and Wiebe, 1999; Passonneau and Carpenter, 2014), annotator accuracy is a p</context>
</contexts>
<marker>Niekrasz, Moore, 2009</marker>
<rawString>John Niekrasz and Johanna Moore. 2009. Participant subjectivity and involvement as a basis for discourse segmentation. In Proceedings of the SIGDIAL 2009 Conference, pages 54–61, London, UK, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Bob Carpenter</author>
</authors>
<title>The benefits of a model of annotation.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--311326</pages>
<contexts>
<context position="2960" citStr="Passonneau and Carpenter, 2014" startWordPosition="442" endWordPosition="445">ltiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment. Three advantages to corpus annotation through application of a probabilistic model to crowdsourced labels, rather than reliance on interannotator agreement computed for a small number of trained annotators, are higher quality, lower cost, and a posterior probability for each ground truth label (Sheng et al., 2008; Snow et al., 2008; Passonneau and Carpenter, 2014). The latter serves as a confidence measure, which contrasts with interannotator agreement measures and with majority</context>
<context position="9041" citStr="Passonneau and Carpenter, 2014" startWordPosition="1398" endWordPosition="1401">n images of Venus (Smyth et al., 1995), curation of biological data (Rzhetsky et al., 2009), computer vision (Whitehill et al., 2009), patient history (Dawid and Skene, 1979), and clinical reports (2010). Smyth et al. (1995), Rogers et al., and (2010) and Raykar et al. (2010) discuss the advantages of probabilistically annotated corpora over majority vote. Much of this work is motivated by the observation that annotators have different accuracies, and the fact that when annotators have known accuracies it can be shown that a majority of inaccurate annotators can be wrong (Raykar et al., 2010; Passonneau and Carpenter, 2014). Equally important, information from inaccurate annotators informs the model inference. For example, an inaccurate annotator might be biased towards label m whenever the true label is z. Dawid and Skene (1979) present a joint model of true labels, observed labels, and annotator performance. Perhaps its first application to NLP data was the Bruce and Wiebe (1999) investigation of word sense. It has also been applied to more fine-grained word sense with a direct comparison to trained annotator labels in (Passonneau and Carpenter, 2014). Snow et al. (2008) showed that application of the same mod</context>
<context position="11221" citStr="Passonneau and Carpenter, 2014" startWordPosition="1754" endWordPosition="1758">ces from the two speakers are on the right and left sides, respectively. Each utterance had a checkbox; when selected, a textbox appeared to allow annotators to enter their segment descriptions. a small amount, but that investing in more singly annotated labels leads to greater improvements. Their results on trained annotators, however, would not apply to our use case involving untrained annotators. In previous work, we found the cost per ground truth label of singly annotated data with trained annotators to be more than twice that for multiply annotated data with twenty untrained annotators (Passonneau and Carpenter, 2014). Half that many would have been sufficient for the Dawid &amp; Skene model used there, which would reduce the cost by half again as much.1 3 Data and Annotation Task The data consists of digital recordings and transcripts of fifty telephone calls between family members and friends who were native speakers of Tagalog. These were collected for the Babel program, sponsored by the Intelligence Advanced Research Projects Activity (IARPA). The calls ranged in length from about seven to ten minutes (µ = 9.67 minutes, Q=0.68 minutes). Transcripts provided by IARPA had an average of 364.66 utterances (min</context>
<context position="15045" citStr="Passonneau and Carpenter, 2014" startWordPosition="2376" endWordPosition="2379"> in the conversational activity (Niekrasz and Moore, 2009). We find, however, much agreement between the two models on the proposed segment boundaries, and leave for future work the question of whether more complex models could accoount for differences in granularity or utterance features. As discussed in section 2, we assume that annotators are not equally accurate, and that a probabilistic model based on the distribution of observed labels can do better than majority vote. Inspired by the type of probabilistic model proposed in (Dawid and Skene, 1979) and extended in (Bruce and Wiebe, 1999; Passonneau and Carpenter, 2014), annotator accuracy is a parameter of our second model. As described in detail in subsequent sections, the two models proposed here rely on distinct assumptions and inference methods. They nevertheless propose many of the same labels. We take each model to provide independent evidence for the ground truth labels, thus the final labels are those voted on by both models. 2192 Figure 3: Graphical model of Double Chain Dynamic Hidden Markov Model for a conversation with m annotators and n utterances. The xt are the hidden states, and the yjl are the observed labels from annotator l at utterance j</context>
</contexts>
<marker>Passonneau, Carpenter, 2014</marker>
<rawString>Rebecca J. Passonneau and Bob Carpenter. 2014. The benefits of a model of annotation. Transactions of the Association for Computational Linguistics, 2:311326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane Litman</author>
</authors>
<title>Discourse segmentation by human and automated means.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<booktitle>Special Issue on Empirical Studies in Discourse Interpretation and Generation.</booktitle>
<pages>23--1</pages>
<contexts>
<context position="1846" citStr="Passonneau and Litman, 1997" startWordPosition="270" endWordPosition="274"> an annotator’s accuracy relative to a given model remains consistent across different conversations. 1 Introduction A single, spontaneous, spoken interaction can consist of multiple activities, such as to plan a future event, to complain about a past situation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Su</context>
<context position="4456" citStr="Passonneau and Litman, 1997" startWordPosition="670" endWordPosition="673">ifferences among annotators in the quality of their labels (Dawid and Skene, 1979; Snow et al., 2008; Passonneau and Carpenter, 2014). No previous work, however, provides modelbased estimation of labels for sequential annotation from crowd labels. For the discourse segmentation data presented here, annotators were presented with audio files of conversations and corresponding transcriptions into utterances. The annotation task was to identify each utterance that completes a discourse segment spanning one or more utterances, based on the speakers’ conversational activities or intentions, as in (Passonneau and Litman, 1997). The annotations from y annotators for a conversation with x utterances can be represented as a y x x matrix, with cell values nzj E {0, 1} to represent the binary segment boundary label assigned by annotator yz at utterance xj. Figure 1 illustrates part of such a matrix. The eight annotators for this conversation are on the y-axis and utterances 80 through 180 are on the 2190 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2190–2200, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Annotation lab</context>
<context position="7171" citStr="Passonneau and Litman, 1997" startWordPosition="1108" endWordPosition="1111"> quite distinct. Each thus serves as an evaluation of the other. The segment boundaries proposed by both models include all the majority vote cases, and in addition, cases voted on by a minority of relatively accurate annotators. We take segment boundaries proposed by both methods as ground truth. To further assess the results of the models, we assume that an annotator’s accuracies should be consistent across the conversations she annotates. 2 Related Work Previous work on annotation of discourse into linear segments has used a variety of methods to derive ground truth segment boundaries. In (Passonneau and Litman, 1997), seven annotators annotated narrative monologues for segments based on speaker intention. Agreement levels for ground truth boundaries were based on statistical significance using Cochran’s Q. In (Galley et al., 2003), three annotators segmented the ICSI meeting corpus into topical units, and majority agreement was taken as ground truth. A functional segmentation of meetings from the AMI multiparty meeting corpus based on involved participants was segmented by one annotator and finalized by a second annotator (Bokaei et al., 2015). Task-based segmentation of patron-librarian interactions (Pas</context>
<context position="41706" citStr="Passonneau and Litman, 1997" startWordPosition="7252" endWordPosition="7255">ing of discourse segmentation covers several types of units, including topical segments (Galley et al., 2003), meeting units in which action items are identified or decisions made (Purver et al., 2007; Fern´andez et al., 2008), transaction subtasks for ordering library books (Passonneau et al., 2014), or speaker involvement (Bokaei et al., 2015). This work relies on manual transcription, and draws on many sources of knowledge for machine learned models, including turn-taking, prosody, and linguistic features. The segmentation annotation can be linear (Galley et al., 2003; Bokaei et al., 2015; Passonneau and Litman, 1997; Passonneau et al., 2014) or hierarchical (Purver et al., 2007; Fern´andez et al., 2008; Passonneau et al., 2011). The differences in methods and results across this body of work, points to a need for more datasets for research on the organization of discourse into activity units. The results presented here support this research agenda by providing a reliable and cost-effective method to estimate ground truth discourse segment labels from crowd labels. Acknowledgments The authors thank Bob Carpenter for discussions during the early stages of the data analysis, and for helpful feedback on the </context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca J. Passonneau and Diane Litman. 1997. Discourse segmentation by human and automated means. Computational Linguistics, 23.1:103–139. Special Issue on Empirical Studies in Discourse Interpretation and Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Irene Alvarado</author>
<author>Phil Crone</author>
<author>Simon Jerome</author>
</authors>
<title>PARADISE-style evaluation of a human-human library corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGDIAL 2011 Conference,</booktitle>
<pages>325--331</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="7792" citStr="Passonneau et al., 2011" startWordPosition="1202" endWordPosition="1205">97), seven annotators annotated narrative monologues for segments based on speaker intention. Agreement levels for ground truth boundaries were based on statistical significance using Cochran’s Q. In (Galley et al., 2003), three annotators segmented the ICSI meeting corpus into topical units, and majority agreement was taken as ground truth. A functional segmentation of meetings from the AMI multiparty meeting corpus based on involved participants was segmented by one annotator and finalized by a second annotator (Bokaei et al., 2015). Task-based segmentation of patron-librarian interactions (Passonneau et al., 2011) measured agreement among two annotators using Krippendorff’s Alpha at an average of 0.77 (Krippendorff, 1980). The annotation task here mostly closely resembles (Passonneau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identific</context>
<context position="41820" citStr="Passonneau et al., 2011" startWordPosition="7270" endWordPosition="7273">g units in which action items are identified or decisions made (Purver et al., 2007; Fern´andez et al., 2008), transaction subtasks for ordering library books (Passonneau et al., 2014), or speaker involvement (Bokaei et al., 2015). This work relies on manual transcription, and draws on many sources of knowledge for machine learned models, including turn-taking, prosody, and linguistic features. The segmentation annotation can be linear (Galley et al., 2003; Bokaei et al., 2015; Passonneau and Litman, 1997; Passonneau et al., 2014) or hierarchical (Purver et al., 2007; Fern´andez et al., 2008; Passonneau et al., 2011). The differences in methods and results across this body of work, points to a need for more datasets for research on the organization of discourse into activity units. The results presented here support this research agenda by providing a reliable and cost-effective method to estimate ground truth discourse segment labels from crowd labels. Acknowledgments The authors thank Bob Carpenter for discussions during the early stages of the data analysis, and for helpful feedback on the paper. We thank the IARPA Babel program manager, Mary Harper, for giving us permission to annotate the Babel data.</context>
</contexts>
<marker>Passonneau, Alvarado, Crone, Jerome, 2011</marker>
<rawString>Rebecca J. Passonneau, Irene Alvarado, Phil Crone, and Simon Jerome. 2011. PARADISE-style evaluation of a human-human library corpus. In Proceedings of the SIGDIAL 2011 Conference, pages 325– 331, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Boxuan Guan</author>
<author>Cho Ho Yeung</author>
<author>Yuan Du</author>
<author>Emma Conner</author>
</authors>
<title>Aspectual properties of conversational activities.</title>
<date>2014</date>
<booktitle>In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>228--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, PA, U.S.A.,</location>
<contexts>
<context position="41380" citStr="Passonneau et al., 2014" startWordPosition="7203" endWordPosition="7206">n a model fit to all but the held out data. To do so, we would extend the models with an additional parameter for the conversation, to account for the observation that while all conversations seem to fit the same empirical distribution, there are differences across conversations. 10 Conclusion Annotation and machine learning of discourse segmentation covers several types of units, including topical segments (Galley et al., 2003), meeting units in which action items are identified or decisions made (Purver et al., 2007; Fern´andez et al., 2008), transaction subtasks for ordering library books (Passonneau et al., 2014), or speaker involvement (Bokaei et al., 2015). This work relies on manual transcription, and draws on many sources of knowledge for machine learned models, including turn-taking, prosody, and linguistic features. The segmentation annotation can be linear (Galley et al., 2003; Bokaei et al., 2015; Passonneau and Litman, 1997; Passonneau et al., 2014) or hierarchical (Purver et al., 2007; Fern´andez et al., 2008; Passonneau et al., 2011). The differences in methods and results across this body of work, points to a need for more datasets for research on the organization of discourse into activit</context>
</contexts>
<marker>Passonneau, Guan, Yeung, Du, Conner, 2014</marker>
<rawString>Rebecca J. Passonneau, Boxuan Guan, Cho Ho Yeung, Yuan Du, and Emma Conner. 2014. Aspectual properties of conversational activities. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 228–237, Philadelphia, PA, U.S.A., June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>John Dowding</author>
<author>John Niekrasz</author>
<author>Patrick Ehlen</author>
<author>Sharareh Noorbaloochi</author>
<author>Stanley Peters</author>
</authors>
<title>Detecting and summarizing action items in multi-party dialogue.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="2380" citStr="Purver et al., 2007" startWordPosition="354" endWordPosition="357">en resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Fi</context>
<context position="41279" citStr="Purver et al., 2007" startWordPosition="7189" endWordPosition="7192"> will use the models generatively to predict a subset of the data for a given annotator, based on a model fit to all but the held out data. To do so, we would extend the models with an additional parameter for the conversation, to account for the observation that while all conversations seem to fit the same empirical distribution, there are differences across conversations. 10 Conclusion Annotation and machine learning of discourse segmentation covers several types of units, including topical segments (Galley et al., 2003), meeting units in which action items are identified or decisions made (Purver et al., 2007; Fern´andez et al., 2008), transaction subtasks for ordering library books (Passonneau et al., 2014), or speaker involvement (Bokaei et al., 2015). This work relies on manual transcription, and draws on many sources of knowledge for machine learned models, including turn-taking, prosody, and linguistic features. The segmentation annotation can be linear (Galley et al., 2003; Bokaei et al., 2015; Passonneau and Litman, 1997; Passonneau et al., 2014) or hierarchical (Purver et al., 2007; Fern´andez et al., 2008; Passonneau et al., 2011). The differences in methods and results across this body o</context>
</contexts>
<marker>Purver, Dowding, Niekrasz, Ehlen, Noorbaloochi, Peters, 2007</marker>
<rawString>Matthew Purver, John Dowding, John Niekrasz, Patrick Ehlen, Sharareh Noorbaloochi, and Stanley Peters. 2007. Detecting and summarizing action items in multi-party dialogue. In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas C Raykar</author>
<author>Shipeng Yu</author>
<author>Linda H Zhao</author>
<author>Gerardo Hermosillo Valadez</author>
<author>Charles Florin</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
</authors>
<title>Learning from crowds.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<pages>1322</pages>
<contexts>
<context position="8686" citStr="Raykar et al. (2010)" startWordPosition="1340" endWordPosition="1343">ilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identification of craters in images of Venus (Smyth et al., 1995), curation of biological data (Rzhetsky et al., 2009), computer vision (Whitehill et al., 2009), patient history (Dawid and Skene, 1979), and clinical reports (2010). Smyth et al. (1995), Rogers et al., and (2010) and Raykar et al. (2010) discuss the advantages of probabilistically annotated corpora over majority vote. Much of this work is motivated by the observation that annotators have different accuracies, and the fact that when annotators have known accuracies it can be shown that a majority of inaccurate annotators can be wrong (Raykar et al., 2010; Passonneau and Carpenter, 2014). Equally important, information from inaccurate annotators informs the model inference. For example, an inaccurate annotator might be biased towards label m whenever the true label is z. Dawid and Skene (1979) present a joint model of true labe</context>
</contexts>
<marker>Raykar, Yu, Zhao, Valadez, Florin, Bogoni, Moy, 2010</marker>
<rawString>Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. 2010. Learning from crowds. Journal of Machine Learning Research, 11:1297– 1322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Rogers</author>
<author>Mark Girolami</author>
<author>Tamara Polajnar</author>
</authors>
<title>Semi-parametric analysis of multi-rater data.</title>
<date>2010</date>
<journal>Statistical Computing,</journal>
<pages>20--317</pages>
<marker>Rogers, Girolami, Polajnar, 2010</marker>
<rawString>Simon Rogers, Mark Girolami, and Tamara Polajnar. 2010. Semi-parametric analysis of multi-rater data. Statistical Computing, 20:317–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrey Rzhetsky</author>
<author>Hagit Shatkay</author>
<author>W John Wilbur</author>
</authors>
<title>How to get the most out of your curation effort.</title>
<date>2009</date>
<journal>PLoS Computational Biology.,</journal>
<volume>5</volume>
<issue>5</issue>
<contexts>
<context position="8501" citStr="Rzhetsky et al., 2009" startWordPosition="1308" endWordPosition="1311">77 (Krippendorff, 1980). The annotation task here mostly closely resembles (Passonneau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identification of craters in images of Venus (Smyth et al., 1995), curation of biological data (Rzhetsky et al., 2009), computer vision (Whitehill et al., 2009), patient history (Dawid and Skene, 1979), and clinical reports (2010). Smyth et al. (1995), Rogers et al., and (2010) and Raykar et al. (2010) discuss the advantages of probabilistically annotated corpora over majority vote. Much of this work is motivated by the observation that annotators have different accuracies, and the fact that when annotators have known accuracies it can be shown that a majority of inaccurate annotators can be wrong (Raykar et al., 2010; Passonneau and Carpenter, 2014). Equally important, information from inaccurate annotators </context>
</contexts>
<marker>Rzhetsky, Shatkay, Wilbur, 2009</marker>
<rawString>Andrey Rzhetsky, Hagit Shatkay, and W. John Wilbur. 2009. How to get the most out of your curation effort. PLoS Computational Biology., 5(5):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S Sheng</author>
<author>Foster Provost</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Get another label? Improving data quality and data mining using multiple, noisy labelers.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fourteenth ACM International Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="3391" citStr="Sheng et al., 2008" startWordPosition="511" endWordPosition="514">many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment. Three advantages to corpus annotation through application of a probabilistic model to crowdsourced labels, rather than reliance on interannotator agreement computed for a small number of trained annotators, are higher quality, lower cost, and a posterior probability for each ground truth label (Sheng et al., 2008; Snow et al., 2008; Passonneau and Carpenter, 2014). The latter serves as a confidence measure, which contrasts with interannotator agreement measures and with majority-voted labels, neither of which provides quality information for the ground truth labels on individual items. Previous work has demonstrated that model estimation of ground truth labels from crowd labels produces results superior to the crowd’s majority vote, due to differences among annotators in the quality of their labels (Dawid and Skene, 1979; Snow et al., 2008; Passonneau and Carpenter, 2014). No previous work, however, p</context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeirotis. 2008. Get another label? Improving data quality and data mining using multiple, noisy labelers. In Proceedings of the Fourteenth ACM International Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
<author>Usama Fayyad</author>
<author>Michael Burl</author>
<author>Pietro Perona</author>
<author>Pierre Baldi</author>
</authors>
<title>Inferring ground truth from subjectively-labeled images of Venus.</title>
<date>1995</date>
<booktitle>In Advances in Neural Information Processing Systems 7,</booktitle>
<pages>1085--1092</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8448" citStr="Smyth et al., 1995" startWordPosition="1300" endWordPosition="1303">ors using Krippendorff’s Alpha at an average of 0.77 (Krippendorff, 1980). The annotation task here mostly closely resembles (Passonneau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identification of craters in images of Venus (Smyth et al., 1995), curation of biological data (Rzhetsky et al., 2009), computer vision (Whitehill et al., 2009), patient history (Dawid and Skene, 1979), and clinical reports (2010). Smyth et al. (1995), Rogers et al., and (2010) and Raykar et al. (2010) discuss the advantages of probabilistically annotated corpora over majority vote. Much of this work is motivated by the observation that annotators have different accuracies, and the fact that when annotators have known accuracies it can be shown that a majority of inaccurate annotators can be wrong (Raykar et al., 2010; Passonneau and Carpenter, 2014). Equal</context>
</contexts>
<marker>Smyth, Fayyad, Burl, Perona, Baldi, 1995</marker>
<rawString>Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. 1995. Inferring ground truth from subjectively-labeled images of Venus. In Advances in Neural Information Processing Systems 7, pages 1085–1092. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>254--263</pages>
<location>Honolulu.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 254–263, Honolulu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Josef Ruppenhofer</author>
<author>Janyce Wiebe</author>
</authors>
<title>Detecting arguing and sentiment in meetings.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue (SIGdial 07),</booktitle>
<pages>2634</pages>
<contexts>
<context position="2297" citStr="Somasundaran et al., 2007" startWordPosition="341" endWordPosition="344">ctivities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and </context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2007</marker>
<rawString>Swapna Somasundaran, Josef Ruppenhofer, and Janyce Wiebe. 2007. Detecting arguing and sentiment in meetings. In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue (SIGdial 07), page 2634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingyu Sun</author>
<author>Joyce Y Chai</author>
</authors>
<title>Discourse processing for context question answering based on linguistic knowledge.</title>
<date>2007</date>
<journal>Know.-Based Syst.,</journal>
<volume>20</volume>
<issue>6</issue>
<pages>526</pages>
<contexts>
<context position="2462" citStr="Sun and Chai, 2007" startWordPosition="367" endWordPosition="370">7). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsourced annotations. Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textu</context>
</contexts>
<marker>Sun, Chai, 2007</marker>
<rawString>Mingyu Sun and Joyce Y. Chai. 2007. Discourse processing for context question answering based on linguistic knowledge. Know.-Based Syst., 20(6):511– 526, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Claire Cardie</author>
</authors>
<title>Focused meeting summarization via unsupervised relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12,</booktitle>
<pages>304--313</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2019" citStr="Wang and Cardie, 2012" startWordPosition="297" endWordPosition="300">le activities, such as to plan a future event, to complain about a past situation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units</context>
</contexts>
<marker>Wang, Cardie, 2012</marker>
<rawString>Lu Wang and Claire Cardie. 2012. Focused meeting summarization via unsupervised relation extraction. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12, pages 304–313, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel G Ward</author>
<author>Steven D Werner</author>
<author>Fernando Garcia</author>
<author>Emilio Sanchis</author>
</authors>
<title>A prosody-based vectorspace model of dialog activity for information retrieval.</title>
<date>2015</date>
<journal>Speech Communication,</journal>
<pages>28--85</pages>
<contexts>
<context position="2092" citStr="Ward et al., 2015" startWordPosition="308" endWordPosition="311">ation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, 2007; Bosma, 2005). To support such research, there is a need for annotation methods to segment conversational interaction into sequential, multi-utterance units. We present and compare two methods to derive such data from crowdsource</context>
</contexts>
<marker>Ward, Werner, Garcia, Sanchis, 2015</marker>
<rawString>Nigel G. Ward, Steven D. Werner, Fernando Garcia, and Emilio Sanchis. 2015. A prosody-based vectorspace model of dialog activity for information retrieval. Speech Communication, 28:85–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Paul Ruvolo</author>
<author>Tingfan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier Movellan</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<booktitle>In Proceedings of the 24th Annual Conference on Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="8543" citStr="Whitehill et al., 2009" startWordPosition="1314" endWordPosition="1317">ask here mostly closely resembles (Passonneau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identification of craters in images of Venus (Smyth et al., 1995), curation of biological data (Rzhetsky et al., 2009), computer vision (Whitehill et al., 2009), patient history (Dawid and Skene, 1979), and clinical reports (2010). Smyth et al. (1995), Rogers et al., and (2010) and Raykar et al. (2010) discuss the advantages of probabilistically annotated corpora over majority vote. Much of this work is motivated by the observation that annotators have different accuracies, and the fact that when annotators have known accuracies it can be shown that a majority of inaccurate annotators can be wrong (Raykar et al., 2010; Passonneau and Carpenter, 2014). Equally important, information from inaccurate annotators informs the model inference. For example, </context>
</contexts>
<marker>Whitehill, Ruvolo, Wu, Bergsma, Movellan, 2009</marker>
<rawString>Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Proceedings of the 24th Annual Conference on Advances in Neural Information Processing Systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
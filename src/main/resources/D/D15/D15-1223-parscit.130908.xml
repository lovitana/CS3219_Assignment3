<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.989962">
Krimping texts for better summarization
</title>
<author confidence="0.950843">
Marina Litvak
</author>
<affiliation confidence="0.864357">
Sami Shamoon College
</affiliation>
<address confidence="0.7413235">
of Engineering,
Beer Sheva, Israel
</address>
<email confidence="0.997626">
marinal@sce.ac.il
</email>
<author confidence="0.82863">
Natalia Vanetik
</author>
<affiliation confidence="0.756725">
Sami Shamoon College
</affiliation>
<address confidence="0.692182">
of Engineering,
Beer Sheva, Israel
</address>
<email confidence="0.99682">
natalyav@sce.ac.il
</email>
<author confidence="0.993745">
Mark Last
</author>
<affiliation confidence="0.928371">
Ben-Gurion University
of the Negev,
</affiliation>
<address confidence="0.44734">
Beer Sheva, Israel
</address>
<email confidence="0.997816">
mlast@bgu.ac.il
</email>
<sectionHeader confidence="0.993879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998752173913">
Automated text summarization is aimed at
extracting essential information from orig-
inal text and presenting it in a minimal,
often predefined, number of words. In
this paper, we introduce a new approach
for unsupervised extractive summariza-
tion, based on the Minimum Description
Length (MDL) principle, using the Krimp
dataset compression algorithm (Vreeken
et al., 2011). Our approach represents a
text as a transactional dataset, with sen-
tences as transactions, and then describes
it by itemsets that stand for frequent se-
quences of words. The summary is then
compiled from sentences that compress
(and as such, best describe) the document.
The problem of summarization is reduced
to the maximal coverage, following the
assumption that a summary that best de-
scribes the original text, should cover most
of the word sequences describing the doc-
ument. We solve it by a greedy algorithm
and present the evaluation results.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947321428572">
Many unsupervised approaches for extractive text
summarization follow the maximal coverage prin-
ciple (Takamura and Okumura, 2009; Gillick and
Favre, 2009), where the extract that maximally
covers the information contained in the source
text, is selected. Since the exhaustive solution de-
mands an exponential number of tests, approxima-
tion techniques, such as a greedy approach or a
global optimization of a target function, are uti-
lized. It is quite common to measure text infor-
mativeness by the frequency of its components–
words, phrases, concepts, and so on.
A different approach that received much less atten-
tion is based on the Minimum Description Length
(MDL) principle, defining the best summary as the
one that leads to the best compression of the text
by providing its shortest and most concise descrip-
tion. The MDL principle is widely useful in com-
pression techniques of non-textual data, such as
summarization of query results for OLAP appli-
cations. (Lakshmanan et al., 2002; Bu et al., 2005)
However, only a few works on text summarization
using MDL can be found in the literature. Authors
of (Nomoto and Matsumoto, 2001) used K-means
clustering extended with MDL principle for find-
ing diverse topics in the summarized text. Nomoto
in (2004) also extended the C4.5 classifier with
MDL for learning rhetorical relations. In (Nguyen
et al., 2015) the problem of micro-review summa-
rization is formulated within the MDL framework,
where the authors view the tips as being encoded
by snippets, and seek to find a collection of snip-
pets that produce the encoding with the minimum
number of bits.
This paper introduces a new MDL-based approach
for extracting relevant sentences into a summary.
The approach represents documents as a sequen-
tial transactional dataset and then compresses it by
replacing frequent sequences of words by codes.
The summary is then compiled from sentences that
best compress (or describe) the document content.
The intuition behind this approach says that a sum-
mary that best describes the original text should
cover its most frequent word sequences. As such,
the problem of summarization is very naturally re-
duced to the maximal coverage problem. We solve
it by the greedy method which ranks sentences by
their coverage of best compressing frequent word
sequences and selects the top-ranked sentences to
a summary. There are a few works that applied
the common data mining techniques for calculat-
ing frequent itemsets from transactional data to
the text summarization task (Baralis et al., 2012;
Agarwal et al., 2011; Dalal and Zaveri, 2013),
but none of them followed the MDL principle.
The comparative results on three different corpora
</bodyText>
<page confidence="0.941565">
1931
</page>
<note confidence="0.6516525">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1931–1935,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999844">
show that our approach outperforms other unsu-
pervised state-of-the-art summarizers.
</bodyText>
<sectionHeader confidence="0.989348" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.9999970625">
The proposed summarization methodology, de-
noted by Gamp1, is based on the MDL princi-
ple that is defined formally as follows (Mitchell,
1997). Given a set of models M, a model M ∈ M
is considered the best if it minimizes L(M) +
L(D|M) where L(M) is the bit length of the de-
scription of M and L(D|M) is the bit length of
the dataset D encoded with M.
In our approach, we first represent an input text
as a transactional dataset. Then, using the Krimp
dataset compression algorithm (Vreeken et al.,
2011), we build the MDL for this dataset using
its frequent sequential itemsets (word sequences).
The sentences that cover most frequent word se-
quences are chosen to a summary. The following
subsections describe our methodology in detail.
</bodyText>
<subsectionHeader confidence="0.994287">
2.1 Problem statement
</subsectionHeader>
<bodyText confidence="0.992217821428572">
We are given a single text or a collection of texts
about the same topic, composed of a set of sen-
tences S1, ... , Sn over terms t1, ... , tm. The
word limit W is defined for the final summary.
We represent a text as a sequential transactional
dataset. Such a dataset consists of transactions
(sentences), denoted by T1, ... , Tn, and unique
items (terms2) I1, ... , Im. Items are unique across
the entire dataset. The number n of transactions
is called the size of a dataset. Transaction Ti is
a sequence of items from I1, ... , Im, denoted by
Ti = (Ii1,... , Iik); the same item may appear in
different places within the same transaction. Sup-
port of an item sequence s in the dataset is the ra-
tio of transactions containing it as a subsequence
to the dataset size n, i.e., supp(s) = JITEDJsCT}J .
n
Given a support bound Supp ∈ [0, 1], a sequence
s is called frequent if supp(s) ≥ Supp.
According to the MDL principle, we are inter-
ested in the minimal size of a compressed dataset
D|CT after frequent sequences in D are encoded
with the compressing set–codes from the Coding
Table CT, where shorter codes are assigned to
more frequent sequences. The description length
of non-encoded terms is assumed proportional to
their length (number of characters). We rank sen-
tences by their coverage of the best compressing
</bodyText>
<footnote confidence="0.988649666666667">
1abbreviation of two words: GAp and kriMP
2normalized words following tokenization, stemming,
and stop-word removal
</footnote>
<bodyText confidence="0.99984325">
set, which is the number of CT members in the
sentence. The sentences with the highest cover-
age score are added to the summary as long as its
length does not exceed W.
</bodyText>
<subsectionHeader confidence="0.99988">
2.2 Krimping text
</subsectionHeader>
<bodyText confidence="0.99957876">
The purpose of the Krimp algorithm (Vreeken et
al., 2011) is to use frequent sets (or sequences)
to compress a transactional database in order to
achieve MDL for that database. Let FreqSeq be
the set of all frequent sequences in the database. A
collection CT of sequences from FreqSeq (called
the Coding Table) is called best when it minimizes
L(CT) + L(D|CT). We are interested in both ex-
act and inexact sequences, allowing a sequence to
have gaps inside it as long as the ratio of sequence
length to sequence length with gaps does not ex-
ceed a pre-set parameter Gap ∈ (0, 1]. Sequences
with gaps make sense in text data, as phrasing of
the same fact or entity in different sentences may
differ. In order to encode the database, every mem-
ber s ∈ CT is associated with its binary prefix
code c (such as Huffman codes for 4 members: 0,
10, 110, 111), so that the most frequent code has
the shortest length. We use an upper bound C on
the number of encoded sequences in the coding ta-
ble CT, in order to limit document compression.
Krimp-based extractive summarization (see Algo-
rithm 1) is given a document D with n sentences
and m unique terms. The algorithm parameters
are described in Table 1:
</bodyText>
<table confidence="0.999025888888889">
# note description affects
1 W summary words limit summary length
2 Supp minimal support bound – number of frequent
minimal fraction of word sequences
sentences containing |FreqSeq|,
a frequent sequence compression rate
3 C maximal number of codes as in 2
4 Gap maximal allowed
sequence gap ratio as in 2
</table>
<tableCaption confidence="0.920374">
Table 1: Parameters of Algorithm 1.
The algorithm consists of the following steps.
</tableCaption>
<listItem confidence="0.5103415">
1. We find all frequent term sequences in the
document using Apriori-TID algorithm (R
</listItem>
<bodyText confidence="0.980236166666667">
and R, 1994) for the given Supp and Gap
and store them in set FreqSeq, which is kept
in Standard Candidate Order3. The coding ta-
ble CT is initialized to contain all single nor-
malized terms and their frequencies. CT is
always kept in Standard Cover Order4 (Steps
</bodyText>
<footnote confidence="0.9241135">
3first, sorted by increasing support, then by decreasing se-
quence length, then lexicographically
4first, sorted by decreasing sequence length, then by de-
creasing support, and finally, in lexicographical order
</footnote>
<page confidence="0.978087">
1932
</page>
<listItem confidence="0.773352392857143">
1 and 2 in Algorithm 1).
2. We repeatedly choose frequent sequences
from the set FreqSeq so that the size of the
encoded dataset is minimal, with every se-
lected sequence replaced by its code. Selec-
tion is done by computing the decrease in the
size of the encoding when each one of the se-
quences is considered to be a candidate to be
added to CT (Step 3 in Algorithm 1).
3. The summary is constructed by incremen-
tally adding sentences with the highest cov-
erage of encoded term sequences (Step 4 in
Algorithm 1) that are not covered by previ-
ously selected sentences. The sentences are
selected in the greedy manner as long as the
word limit W is not exceeded.
Algorithm 1: Gamp: Krimp-based extractive
summarization with gaps
Input: (1) A document, containing sentences
S1, ... , Sn after tokenization,
stemming and stop-word removal;
(2) normalized terms T1, ... , Tm;
(3) summary size W
(3) limit C on the number of codes to use;
(4) minimal support bound Supp;
(5) maximal gap ratio Gap.
Output: Extractive summary Summary
/* STEP 1: Frequent sequence mining */
</listItem>
<bodyText confidence="0.6228456">
FreqSeq t— inexact frequent sequences of terms
from {T1, ... , Tm} appearing in at least Supp fraction
of sentences and having a gap ratio of at least Gap;
Sort FreqSeq according to Standard Candidate Order;
/* STEP 2: Initialize the coding table */
Add all terms T1, ... , Tm and their support to CT;
Keep CT always sorted according to Standard Cover
Order;
Initialize prefix codes according to the order of
sequences in CT; /* STEP 3: Find the best encoding */
</bodyText>
<equation confidence="0.914801357142857">
EncodedData t— PrefixEncoding({S1, ... , Sn}, CT);
CodeCount t— 0;
while CodeCount &lt; B and FreqSeq =� 0 do
BestCode t— c E FreqSeq such that
L(CT U {c}) +
L(PrefixEncoding({S1, ... , Sn}, CT U {c})) is
minimal;
CT t— CT U {BestCode};
FreqSeq t— FreqSeq \ {BestCode};
Remove supersets of BestCode from FreqSeq;
CodeCount++;
end
Summary t— 0; /* STEP 4: Build the summary */
while #words(Summary) &lt; W do
</equation>
<bodyText confidence="0.903178333333333">
Find the sentence Si that covers the largest set T of
terms in CT and add it to Summary;
Remove terms of T from CT;
</bodyText>
<footnote confidence="0.2690205">
end
return Summary
</footnote>
<note confidence="0.366223833333333">
Example 2.1 Let dataset D contain following
three sentences (taken from the ”House of cards”
TV show):
S1 = A hunter must stalk his prey until the hunter becomes the hunted.
S2 = Then the prey becomes the predator.
S3 =Then the predator and the hunter fight.
</note>
<bodyText confidence="0.9982055">
After stemming, tokenization and stop-word re-
moval we obtain unique (stemmed) terms:
</bodyText>
<equation confidence="0.99999125">
t1 = hunter, t2 = must, t3 = stalk, t4 = prei,
t5 = becom, t6 = hunt, t7 = predat, t8 =fight.
supp(t1) = supp(t4) = supp(t5) = supp(t7) = 32 .
supp(t2) = supp(t3) = supp(t8) = 13.
</equation>
<bodyText confidence="0.564366">
Now we can view sentences as the following se-
quences of normalized terms.
</bodyText>
<equation confidence="0.999861333333333">
S1 = (t1,t2,t3,t4,t1,t5,t4)
S2 = (t4, t5, t7)
S3 = (t7, t1, t8)
</equation>
<bodyText confidence="0.898379347826087">
Initial coding table CT will contain all fre-
quent single terms in Standard Cover Order:
t5, t1, t7, t4, t8, t2, t3. Let the minimal support
bound be 23, i.e., to be frequent a sequence must
appear in at least 2 sentences, and let the gap ra-
tio be 12. Also, let the limit C be 4, meaning that
only the first four entries of the coding table will
be used for encoding. There exists a frequent se-
quence (t4, t5) that appears twice in the text, once
in S1 with a gap and once in S2 without a gap. We
add it to the coding table according to the Stan-
dard Cover Order, generate prefix codes for the
first four entries, and get
seq supp code code len
(prei, becom) 2/3 0 1
becom 2/3 10 2
CT = hunter 2/3 110 3
predat 2/3 111 3
prei 2/3 − −
... ... − −
Now S1 covers 3 out of 4 entries in CT, while S2
and S3 cover 2 terms each. If our summary is to
contain just one sentence, we select S1.
</bodyText>
<sectionHeader confidence="0.889579" genericHeader="method">
3 Experimental settings and results
</sectionHeader>
<bodyText confidence="0.999979916666667">
We performed experiments on three corpora from
the Document Understanding Conference (DUC):
2002, 2004, and 2007 (duc, 2002 2007), summa-
rized in Table 2. DUC 2002 and 2007 each con-
tains a set of single documents, and DUC 2004
contains 50 sets of 10 related documents. We
generated summaries per single documents in the
DUC 2002 and 2007 corpora, and per each set
of related documents (by considering each set
of documents as one meta-document) in DUC
2004, following the corresponding length restric-
tions. The summarization quality was measured
</bodyText>
<page confidence="0.973884">
1933
</page>
<table confidence="0.913781">
corpus # documents # summaries summ len doc size (KB) type
DUC’02 533 2-3 100 1-20 SD
DUC’04 50 4 100 20-89 MD (50x10)
DUC’07 23 4 250 28-131 SD
</table>
<tableCaption confidence="0.999457">
Table 2: Corpora statistics.
</tableCaption>
<bodyText confidence="0.999812515151515">
by the ROUGE-1 and ROUGE-2 (Lin, 2004) re-
call scores, with the word limit indicated in Ta-
ble 2, without stemming and stopword removal.
The results of the introduced algorithm (Gamp)
may be affected by several input parameters: min-
imum support (Supp), codes limit (C), and the
maximal gap allowed between frequent words
(Gap). In order to find the best algorithm set-
tings for a general case, we performed experi-
ments that explored the impact of these parame-
ters on the summarization results. First, we exper-
imented with different values of support count in
the range of [2,10]. The results show that we get
the best summaries using the sequences that occur
in at least four document sentences.
A limit on the number of codes is an additional
parameter.We explored the impact of this param-
eter on the quality of generated summaries. As
we could conclude from our experiments, the best
summarization results are obtained if this param-
eter is set to the maximal number of words in the
summary, W. Consequently, we used 100 codes
for summarizing DUC 2002 and DUC 2004 docu-
ments and 250 codes for DUC 2007 documents.
The maximal gap ratio defines a pattern for gener-
ating the frequent sequences and has a direct effect
on their structure and number. Our experiments
showed that allowing a small gap between words
of a sequence helps to improve slightly the rank-
ing of sentences, but the improvement is not sig-
nificant. Thus we used Gap = 0.8 in comparative
experiments for all corpora. The resulting settings
for each corpus are shown in Table 3.
</bodyText>
<note confidence="0.771956666666667">
Corpus Supp Max. codes Gap
DUC 2002 and 2004 4 100 0.8
DUC 2007 4 250 0.8
</note>
<tableCaption confidence="0.999006">
Table 3: Best settings.
</tableCaption>
<bodyText confidence="0.99999">
We compared the Gamp algorithm with the two
known unsupervised state-of-the-art summarizers
denoted by Gillick (Gillick and Favre, 2009) and
McDonald (McDonald, 2007). As a baseline, we
used a very simple approach that takes first sen-
tences to a summary (denoted by TopK). Table 4
contains the results of comparative evaluations.
The best scores are shown in bold. Gamp out-
performed the other methods on all datasets (us-
ing ROUGE-1 score). The difference between the
scores of Gamp and Gillick (second best system)
on DUC 2007 is highly significant according to
the Wilcoxon matched pairs test. Based on the
same test, the difference of scores obtained on the
DUC 2004 is not statistically significant. On the
DUC 2002, Gamp is ranked first, with insignifi-
cant difference from the second best (McDonald’s)
scores. Based on this result, we can conclude
that MDL-based summarization using frequent se-
quences works better on long documents or multi-
document domain. Intuitively, it is a very logical
conclusion, because single short documents do not
contain a sufficient number of frequent sequences.
It is noteworthy that, in addition to the greedy ap-
proach, we also evaluated the global optimization
with maximizing coverage and minimizing redun-
dancy using Linear Programming (LP). However,
experimental results did not provide any improve-
ment over the greedy approach. Therefore, we re-
port only the results of the greedy solution.
</bodyText>
<table confidence="0.9996095">
ROUGE-1 Recall ROUGE-2 Recall
Algorithm DUC’02 DUC’04 DUC’07 DUC’02 DUC’04 DUC’07
Gamp 0.4421 0.3440 0.3959 0.1941 0.0829 0.0942
Gillick 0.4207 0.3314 0.3518 0.1773 0.0753 0.0650
McDonald 0.4391 0.2955 0.3500 0.1981 0.0556 0.0672
TopK 0.4322 0.2973 0.3525 0.1867 0.0606 0.0706
</table>
<tableCaption confidence="0.999267">
Table 4: Comparative results.
</tableCaption>
<sectionHeader confidence="0.998318" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999965875">
In this paper, we introduce a new approach for
summarizing text documents based on their Min-
imal Description Length. We describe documents
using frequent sequences of their words. The sen-
tences with the highest coverage of the best com-
pressing set are selected to a summary. The ex-
perimental results show that this approach outper-
forms other unsupervised state-of-the-art methods
when summarizing long documents or sets of re-
lated documents. We would not recommend using
our approach for summarizing single short doc-
uments which do not contain enough content for
providing a high-quality description. In the future,
we intend to apply the MDL method to keyword
extraction, headline generation, and other related
tasks.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9985515">
This work was partially supported by the U.S. De-
partment of the Navy, Office of Naval Research.
</bodyText>
<page confidence="0.995746">
1934
</page>
<sectionHeader confidence="0.990014" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998474">
Nitin Agarwal, Kiran Gvr, Ravi Shankar Reddy, and
Carolyn Penstein Ros. 2011. Scisumm: A multi-
document summarization system for scientific articles.
In Proceedings of the ACL-HLT 2011 System Demon-
strations, pages 115–120.
Elena Baralis, Luca Cagliero, Saima Jabeen, and
Alessandro Fiori. 2012. Multi-document summariza-
tion exploiting frequent itemsets. In Proceedings of the
27th Annual ACM Symposium on Applied Computing,
SAC ’12, pages 782–786.
Shaofeng Bu, Laks V. S. Lakshmanan, and Raymond T.
Ng. 2005. Mdl summarization with holes. In Proceed-
ings of the 31st International Conference on Very Large
Data Bases, VLDB ’05, pages 433–444.
Mita K. Dalal and Mukesh A. Zaveri. 2013. Semisu-
pervised learning based opinion summarization and
classification for online product reviews. Applied Com-
putational Intelligence and Soft Computing, 2013.
2002–2007. Document Understanding Conference.
http://duc.nist.gov.
Dan Gillick and Benoit Favre. 2009. A Scalable
Global Model for Summarization. In Proceedings of
the NAACL HLT Workshop on Integer Linear Program-
ming for Natural Language Processing, pages 10–18.
Laks V. S. Lakshmanan, Raymond T. Ng, Chris-
tine Xing Wang, Xiaodong Zhou, and Theodore J.
Johnson. 2002. The generalized mdl approach for
summarization. In Proceedings of the 28th Interna-
tional Conference on Very Large Data Bases, VLDB
’02, pages 766–777.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of summaries. In Proceedings of the
Workshop on Text Summarization Branches Out (WAS
2004), pages 25–26.
R. McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Advances
in Information Retrieval, pages 557–564.
Thomas M. Mitchell. 1997. Machine Learning.
McGraw-Hill, Inc., New York, NY, USA, 1 edition.
Thanh-Son Nguyen, Hady W. Lauw, and Panayiotis
Tsaparas. 2015. Review synthesis for micro-review
summarization. In Proceedings of the Eighth ACM In-
ternational Conference on Web Search and Data Min-
ing, WSDM ’15, pages 169–178.
Tadashi Nomoto and Yuji Matsumoto. 2001. A new
approach to unsupervised text summarization. In Pro-
ceedings of the 24th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ’01, pages 26–34.
Tadashi Nomoto. 2004. Machine Learning Ap-
proaches to Rhetorical Parsing and Open-Domain Text
Summarization. Ph.D. thesis, Nara Institute of Science
and Technology.
Agrawal R and Srikant R. 1994. Fast algorithms for
mining association rules. In 20th International Con-
ference on Very Large Databases, pages 487–499.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In EACL ’09: Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics, pages 781–
789.
Jilles Vreeken, Matthijs Leeuwen, and Arno Siebes.
2011. Krimp: Mining itemsets that compress. Data
Min. Knowl. Discov., 23(1):169–214, July.
</reference>
<page confidence="0.994484">
1935
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.015187">
<title confidence="0.998079">Krimping texts for better summarization</title>
<author confidence="0.960655">Marina Sami Shamoon</author>
<title confidence="0.318252">of</title>
<author confidence="0.928172">Beer Sheva</author>
<email confidence="0.971618">marinal@sce.ac.il</email>
<author confidence="0.5711965">Natalia Sami Shamoon</author>
<title confidence="0.291196">of</title>
<author confidence="0.91582">Beer Sheva</author>
<email confidence="0.989457">natalyav@sce.ac.il</email>
<author confidence="0.889617">Mark</author>
<degree confidence="0.616874">Ben-Gurion of the</degree>
<author confidence="0.917883">Beer Sheva</author>
<email confidence="0.996506">mlast@bgu.ac.il</email>
<abstract confidence="0.997519791666667">Automated text summarization is aimed at extracting essential information from original text and presenting it in a minimal, often predefined, number of words. In this paper, we introduce a new approach for unsupervised extractive summarization, based on the Minimum Description Length (MDL) principle, using the Krimp dataset compression algorithm (Vreeken et al., 2011). Our approach represents a text as a transactional dataset, with sentences as transactions, and then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nitin Agarwal</author>
<author>Kiran Gvr</author>
<author>Ravi Shankar Reddy</author>
<author>Carolyn Penstein Ros</author>
</authors>
<title>Scisumm: A multidocument summarization system for scientific articles.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL-HLT 2011 System Demonstrations,</booktitle>
<pages>115--120</pages>
<contexts>
<context position="3796" citStr="Agarwal et al., 2011" startWordPosition="592" endWordPosition="595">ment content. The intuition behind this approach says that a summary that best describes the original text should cover its most frequent word sequences. As such, the problem of summarization is very naturally reduced to the maximal coverage problem. We solve it by the greedy method which ranks sentences by their coverage of best compressing frequent word sequences and selects the top-ranked sentences to a summary. There are a few works that applied the common data mining techniques for calculating frequent itemsets from transactional data to the text summarization task (Baralis et al., 2012; Agarwal et al., 2011; Dalal and Zaveri, 2013), but none of them followed the MDL principle. The comparative results on three different corpora 1931 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1931–1935, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. show that our approach outperforms other unsupervised state-of-the-art summarizers. 2 Methodology The proposed summarization methodology, denoted by Gamp1, is based on the MDL principle that is defined formally as follows (Mitchell, 1997). Given a set of models M, a model M ∈ </context>
</contexts>
<marker>Agarwal, Gvr, Reddy, Ros, 2011</marker>
<rawString>Nitin Agarwal, Kiran Gvr, Ravi Shankar Reddy, and Carolyn Penstein Ros. 2011. Scisumm: A multidocument summarization system for scientific articles. In Proceedings of the ACL-HLT 2011 System Demonstrations, pages 115–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Baralis</author>
<author>Luca Cagliero</author>
<author>Saima Jabeen</author>
<author>Alessandro Fiori</author>
</authors>
<title>Multi-document summarization exploiting frequent itemsets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 27th Annual ACM Symposium on Applied Computing, SAC ’12,</booktitle>
<pages>782--786</pages>
<contexts>
<context position="3774" citStr="Baralis et al., 2012" startWordPosition="588" endWordPosition="591">(or describe) the document content. The intuition behind this approach says that a summary that best describes the original text should cover its most frequent word sequences. As such, the problem of summarization is very naturally reduced to the maximal coverage problem. We solve it by the greedy method which ranks sentences by their coverage of best compressing frequent word sequences and selects the top-ranked sentences to a summary. There are a few works that applied the common data mining techniques for calculating frequent itemsets from transactional data to the text summarization task (Baralis et al., 2012; Agarwal et al., 2011; Dalal and Zaveri, 2013), but none of them followed the MDL principle. The comparative results on three different corpora 1931 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1931–1935, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. show that our approach outperforms other unsupervised state-of-the-art summarizers. 2 Methodology The proposed summarization methodology, denoted by Gamp1, is based on the MDL principle that is defined formally as follows (Mitchell, 1997). Given a set of </context>
</contexts>
<marker>Baralis, Cagliero, Jabeen, Fiori, 2012</marker>
<rawString>Elena Baralis, Luca Cagliero, Saima Jabeen, and Alessandro Fiori. 2012. Multi-document summarization exploiting frequent itemsets. In Proceedings of the 27th Annual ACM Symposium on Applied Computing, SAC ’12, pages 782–786.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaofeng Bu</author>
<author>Laks V S Lakshmanan</author>
<author>Raymond T Ng</author>
</authors>
<title>Mdl summarization with holes.</title>
<date>2005</date>
<booktitle>In Proceedings of the 31st International Conference on Very Large Data Bases, VLDB ’05,</booktitle>
<pages>433--444</pages>
<contexts>
<context position="2249" citStr="Bu et al., 2005" startWordPosition="343" endWordPosition="346">l optimization of a target function, are utilized. It is quite common to measure text informativeness by the frequency of its components– words, phrases, concepts, and so on. A different approach that received much less attention is based on the Minimum Description Length (MDL) principle, defining the best summary as the one that leads to the best compression of the text by providing its shortest and most concise description. The MDL principle is widely useful in compression techniques of non-textual data, such as summarization of query results for OLAP applications. (Lakshmanan et al., 2002; Bu et al., 2005) However, only a few works on text summarization using MDL can be found in the literature. Authors of (Nomoto and Matsumoto, 2001) used K-means clustering extended with MDL principle for finding diverse topics in the summarized text. Nomoto in (2004) also extended the C4.5 classifier with MDL for learning rhetorical relations. In (Nguyen et al., 2015) the problem of micro-review summarization is formulated within the MDL framework, where the authors view the tips as being encoded by snippets, and seek to find a collection of snippets that produce the encoding with the minimum number of bits. T</context>
</contexts>
<marker>Bu, Lakshmanan, Ng, 2005</marker>
<rawString>Shaofeng Bu, Laks V. S. Lakshmanan, and Raymond T. Ng. 2005. Mdl summarization with holes. In Proceedings of the 31st International Conference on Very Large Data Bases, VLDB ’05, pages 433–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mita K Dalal</author>
<author>Mukesh A Zaveri</author>
</authors>
<title>Semisupervised learning based opinion summarization and classification for online product reviews.</title>
<date>2013</date>
<booktitle>Applied Computational Intelligence and Soft Computing,</booktitle>
<contexts>
<context position="3821" citStr="Dalal and Zaveri, 2013" startWordPosition="596" endWordPosition="599">ition behind this approach says that a summary that best describes the original text should cover its most frequent word sequences. As such, the problem of summarization is very naturally reduced to the maximal coverage problem. We solve it by the greedy method which ranks sentences by their coverage of best compressing frequent word sequences and selects the top-ranked sentences to a summary. There are a few works that applied the common data mining techniques for calculating frequent itemsets from transactional data to the text summarization task (Baralis et al., 2012; Agarwal et al., 2011; Dalal and Zaveri, 2013), but none of them followed the MDL principle. The comparative results on three different corpora 1931 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1931–1935, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. show that our approach outperforms other unsupervised state-of-the-art summarizers. 2 Methodology The proposed summarization methodology, denoted by Gamp1, is based on the MDL principle that is defined formally as follows (Mitchell, 1997). Given a set of models M, a model M ∈ M is considered the best </context>
</contexts>
<marker>Dalal, Zaveri, 2013</marker>
<rawString>Mita K. Dalal and Mukesh A. Zaveri. 2013. Semisupervised learning based opinion summarization and classification for online product reviews. Applied Computational Intelligence and Soft Computing, 2013.</rawString>
</citation>
<citation valid="false">
<title>Document Understanding Conference.</title>
<note>http://duc.nist.gov.</note>
<marker></marker>
<rawString>2002–2007. Document Understanding Conference. http://duc.nist.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A Scalable Global Model for Summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="1400" citStr="Gillick and Favre, 2009" startWordPosition="203" endWordPosition="206"> then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results. 1 Introduction Many unsupervised approaches for extractive text summarization follow the maximal coverage principle (Takamura and Okumura, 2009; Gillick and Favre, 2009), where the extract that maximally covers the information contained in the source text, is selected. Since the exhaustive solution demands an exponential number of tests, approximation techniques, such as a greedy approach or a global optimization of a target function, are utilized. It is quite common to measure text informativeness by the frequency of its components– words, phrases, concepts, and so on. A different approach that received much less attention is based on the Minimum Description Length (MDL) principle, defining the best summary as the one that leads to the best compression of th</context>
<context position="14878" citStr="Gillick and Favre, 2009" startWordPosition="2566" endWordPosition="2569">for generating the frequent sequences and has a direct effect on their structure and number. Our experiments showed that allowing a small gap between words of a sequence helps to improve slightly the ranking of sentences, but the improvement is not significant. Thus we used Gap = 0.8 in comparative experiments for all corpora. The resulting settings for each corpus are shown in Table 3. Corpus Supp Max. codes Gap DUC 2002 and 2004 4 100 0.8 DUC 2007 4 250 0.8 Table 3: Best settings. We compared the Gamp algorithm with the two known unsupervised state-of-the-art summarizers denoted by Gillick (Gillick and Favre, 2009) and McDonald (McDonald, 2007). As a baseline, we used a very simple approach that takes first sentences to a summary (denoted by TopK). Table 4 contains the results of comparative evaluations. The best scores are shown in bold. Gamp outperformed the other methods on all datasets (using ROUGE-1 score). The difference between the scores of Gamp and Gillick (second best system) on DUC 2007 is highly significant according to the Wilcoxon matched pairs test. Based on the same test, the difference of scores obtained on the DUC 2004 is not statistically significant. On the DUC 2002, Gamp is ranked f</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A Scalable Global Model for Summarization. In Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laks V S Lakshmanan</author>
<author>Raymond T Ng</author>
<author>Christine Xing Wang</author>
<author>Xiaodong Zhou</author>
<author>Theodore J Johnson</author>
</authors>
<title>The generalized mdl approach for summarization.</title>
<date>2002</date>
<booktitle>In Proceedings of the 28th International Conference on Very Large Data Bases, VLDB ’02,</booktitle>
<pages>766--777</pages>
<contexts>
<context position="2231" citStr="Lakshmanan et al., 2002" startWordPosition="339" endWordPosition="342">reedy approach or a global optimization of a target function, are utilized. It is quite common to measure text informativeness by the frequency of its components– words, phrases, concepts, and so on. A different approach that received much less attention is based on the Minimum Description Length (MDL) principle, defining the best summary as the one that leads to the best compression of the text by providing its shortest and most concise description. The MDL principle is widely useful in compression techniques of non-textual data, such as summarization of query results for OLAP applications. (Lakshmanan et al., 2002; Bu et al., 2005) However, only a few works on text summarization using MDL can be found in the literature. Authors of (Nomoto and Matsumoto, 2001) used K-means clustering extended with MDL principle for finding diverse topics in the summarized text. Nomoto in (2004) also extended the C4.5 classifier with MDL for learning rhetorical relations. In (Nguyen et al., 2015) the problem of micro-review summarization is formulated within the MDL framework, where the authors view the tips as being encoded by snippets, and seek to find a collection of snippets that produce the encoding with the minimum</context>
</contexts>
<marker>Lakshmanan, Ng, Wang, Zhou, Johnson, 2002</marker>
<rawString>Laks V. S. Lakshmanan, Raymond T. Ng, Christine Xing Wang, Xiaodong Zhou, and Theodore J. Johnson. 2002. The generalized mdl approach for summarization. In Proceedings of the 28th International Conference on Very Large Data Bases, VLDB ’02, pages 766–777.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out (WAS</booktitle>
<pages>25--26</pages>
<contexts>
<context position="13141" citStr="Lin, 2004" startWordPosition="2268" endWordPosition="2269">. DUC 2002 and 2007 each contains a set of single documents, and DUC 2004 contains 50 sets of 10 related documents. We generated summaries per single documents in the DUC 2002 and 2007 corpora, and per each set of related documents (by considering each set of documents as one meta-document) in DUC 2004, following the corresponding length restrictions. The summarization quality was measured 1933 corpus # documents # summaries summ len doc size (KB) type DUC’02 533 2-3 100 1-20 SD DUC’04 50 4 100 20-89 MD (50x10) DUC’07 23 4 250 28-131 SD Table 2: Corpora statistics. by the ROUGE-1 and ROUGE-2 (Lin, 2004) recall scores, with the word limit indicated in Table 2, without stemming and stopword removal. The results of the introduced algorithm (Gamp) may be affected by several input parameters: minimum support (Supp), codes limit (C), and the maximal gap allowed between frequent words (Gap). In order to find the best algorithm settings for a general case, we performed experiments that explored the impact of these parameters on the summarization results. First, we experimented with different values of support count in the range of [2,10]. The results show that we get the best summaries using the seq</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004), pages 25–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="14908" citStr="McDonald, 2007" startWordPosition="2572" endWordPosition="2573">nd has a direct effect on their structure and number. Our experiments showed that allowing a small gap between words of a sequence helps to improve slightly the ranking of sentences, but the improvement is not significant. Thus we used Gap = 0.8 in comparative experiments for all corpora. The resulting settings for each corpus are shown in Table 3. Corpus Supp Max. codes Gap DUC 2002 and 2004 4 100 0.8 DUC 2007 4 250 0.8 Table 3: Best settings. We compared the Gamp algorithm with the two known unsupervised state-of-the-art summarizers denoted by Gillick (Gillick and Favre, 2009) and McDonald (McDonald, 2007). As a baseline, we used a very simple approach that takes first sentences to a summary (denoted by TopK). Table 4 contains the results of comparative evaluations. The best scores are shown in bold. Gamp outperformed the other methods on all datasets (using ROUGE-1 score). The difference between the scores of Gamp and Gillick (second best system) on DUC 2007 is highly significant according to the Wilcoxon matched pairs test. Based on the same test, the difference of scores obtained on the DUC 2004 is not statistically significant. On the DUC 2002, Gamp is ranked first, with insignificant diffe</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>R. McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Advances in Information Retrieval, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<volume>1</volume>
<pages>edition.</pages>
<publisher>McGraw-Hill, Inc.,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="4357" citStr="Mitchell, 1997" startWordPosition="673" endWordPosition="674">ation task (Baralis et al., 2012; Agarwal et al., 2011; Dalal and Zaveri, 2013), but none of them followed the MDL principle. The comparative results on three different corpora 1931 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1931–1935, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. show that our approach outperforms other unsupervised state-of-the-art summarizers. 2 Methodology The proposed summarization methodology, denoted by Gamp1, is based on the MDL principle that is defined formally as follows (Mitchell, 1997). Given a set of models M, a model M ∈ M is considered the best if it minimizes L(M) + L(D|M) where L(M) is the bit length of the description of M and L(D|M) is the bit length of the dataset D encoded with M. In our approach, we first represent an input text as a transactional dataset. Then, using the Krimp dataset compression algorithm (Vreeken et al., 2011), we build the MDL for this dataset using its frequent sequential itemsets (word sequences). The sentences that cover most frequent word sequences are chosen to a summary. The following subsections describe our methodology in detail. 2.1 P</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Thomas M. Mitchell. 1997. Machine Learning. McGraw-Hill, Inc., New York, NY, USA, 1 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thanh-Son Nguyen</author>
<author>Hady W Lauw</author>
<author>Panayiotis Tsaparas</author>
</authors>
<title>Review synthesis for micro-review summarization.</title>
<date>2015</date>
<booktitle>In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM ’15,</booktitle>
<pages>169--178</pages>
<contexts>
<context position="2602" citStr="Nguyen et al., 2015" startWordPosition="400" endWordPosition="403"> compression of the text by providing its shortest and most concise description. The MDL principle is widely useful in compression techniques of non-textual data, such as summarization of query results for OLAP applications. (Lakshmanan et al., 2002; Bu et al., 2005) However, only a few works on text summarization using MDL can be found in the literature. Authors of (Nomoto and Matsumoto, 2001) used K-means clustering extended with MDL principle for finding diverse topics in the summarized text. Nomoto in (2004) also extended the C4.5 classifier with MDL for learning rhetorical relations. In (Nguyen et al., 2015) the problem of micro-review summarization is formulated within the MDL framework, where the authors view the tips as being encoded by snippets, and seek to find a collection of snippets that produce the encoding with the minimum number of bits. This paper introduces a new MDL-based approach for extracting relevant sentences into a summary. The approach represents documents as a sequential transactional dataset and then compresses it by replacing frequent sequences of words by codes. The summary is then compiled from sentences that best compress (or describe) the document content. The intuitio</context>
</contexts>
<marker>Nguyen, Lauw, Tsaparas, 2015</marker>
<rawString>Thanh-Son Nguyen, Hady W. Lauw, and Panayiotis Tsaparas. 2015. Review synthesis for micro-review summarization. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM ’15, pages 169–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A new approach to unsupervised text summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’01,</booktitle>
<pages>26--34</pages>
<contexts>
<context position="2379" citStr="Nomoto and Matsumoto, 2001" startWordPosition="365" endWordPosition="368">of its components– words, phrases, concepts, and so on. A different approach that received much less attention is based on the Minimum Description Length (MDL) principle, defining the best summary as the one that leads to the best compression of the text by providing its shortest and most concise description. The MDL principle is widely useful in compression techniques of non-textual data, such as summarization of query results for OLAP applications. (Lakshmanan et al., 2002; Bu et al., 2005) However, only a few works on text summarization using MDL can be found in the literature. Authors of (Nomoto and Matsumoto, 2001) used K-means clustering extended with MDL principle for finding diverse topics in the summarized text. Nomoto in (2004) also extended the C4.5 classifier with MDL for learning rhetorical relations. In (Nguyen et al., 2015) the problem of micro-review summarization is formulated within the MDL framework, where the authors view the tips as being encoded by snippets, and seek to find a collection of snippets that produce the encoding with the minimum number of bits. This paper introduces a new MDL-based approach for extracting relevant sentences into a summary. The approach represents documents </context>
</contexts>
<marker>Nomoto, Matsumoto, 2001</marker>
<rawString>Tadashi Nomoto and Yuji Matsumoto. 2001. A new approach to unsupervised text summarization. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’01, pages 26–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadashi Nomoto</author>
</authors>
<title>Machine Learning Approaches to Rhetorical Parsing and Open-Domain Text Summarization.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Nara Institute of Science and Technology.</institution>
<marker>Nomoto, 2004</marker>
<rawString>Tadashi Nomoto. 2004. Machine Learning Approaches to Rhetorical Parsing and Open-Domain Text Summarization. Ph.D. thesis, Nara Institute of Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Agrawal</author>
<author>R Srikant</author>
</authors>
<title>Fast algorithms for mining association rules.</title>
<date>1994</date>
<booktitle>In 20th International Conference on Very Large Databases,</booktitle>
<pages>487--499</pages>
<marker>Agrawal, Srikant, 1994</marker>
<rawString>Agrawal R and Srikant R. 1994. Fast algorithms for mining association rules. In 20th International Conference on Very Large Databases, pages 487–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>781--789</pages>
<contexts>
<context position="1374" citStr="Takamura and Okumura, 2009" startWordPosition="199" endWordPosition="202">ntences as transactions, and then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results. 1 Introduction Many unsupervised approaches for extractive text summarization follow the maximal coverage principle (Takamura and Okumura, 2009; Gillick and Favre, 2009), where the extract that maximally covers the information contained in the source text, is selected. Since the exhaustive solution demands an exponential number of tests, approximation techniques, such as a greedy approach or a global optimization of a target function, are utilized. It is quite common to measure text informativeness by the frequency of its components– words, phrases, concepts, and so on. A different approach that received much less attention is based on the Minimum Description Length (MDL) principle, defining the best summary as the one that leads to </context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 781– 789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jilles Vreeken</author>
<author>Matthijs Leeuwen</author>
<author>Arno Siebes</author>
</authors>
<title>Krimp: Mining itemsets that compress.</title>
<date>2011</date>
<journal>Data Min. Knowl. Discov.,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="680" citStr="Vreeken et al., 2011" startWordPosition="90" endWordPosition="93">Shamoon College of Engineering, Beer Sheva, Israel marinal@sce.ac.il Natalia Vanetik Sami Shamoon College of Engineering, Beer Sheva, Israel natalyav@sce.ac.il Mark Last Ben-Gurion University of the Negev, Beer Sheva, Israel mlast@bgu.ac.il Abstract Automated text summarization is aimed at extracting essential information from original text and presenting it in a minimal, often predefined, number of words. In this paper, we introduce a new approach for unsupervised extractive summarization, based on the Minimum Description Length (MDL) principle, using the Krimp dataset compression algorithm (Vreeken et al., 2011). Our approach represents a text as a transactional dataset, with sentences as transactions, and then describes it by itemsets that stand for frequent sequences of words. The summary is then compiled from sentences that compress (and as such, best describe) the document. The problem of summarization is reduced to the maximal coverage, following the assumption that a summary that best describes the original text, should cover most of the word sequences describing the document. We solve it by a greedy algorithm and present the evaluation results. 1 Introduction Many unsupervised approaches for e</context>
<context position="4718" citStr="Vreeken et al., 2011" startWordPosition="741" endWordPosition="744">tational Linguistics. show that our approach outperforms other unsupervised state-of-the-art summarizers. 2 Methodology The proposed summarization methodology, denoted by Gamp1, is based on the MDL principle that is defined formally as follows (Mitchell, 1997). Given a set of models M, a model M ∈ M is considered the best if it minimizes L(M) + L(D|M) where L(M) is the bit length of the description of M and L(D|M) is the bit length of the dataset D encoded with M. In our approach, we first represent an input text as a transactional dataset. Then, using the Krimp dataset compression algorithm (Vreeken et al., 2011), we build the MDL for this dataset using its frequent sequential itemsets (word sequences). The sentences that cover most frequent word sequences are chosen to a summary. The following subsections describe our methodology in detail. 2.1 Problem statement We are given a single text or a collection of texts about the same topic, composed of a set of sentences S1, ... , Sn over terms t1, ... , tm. The word limit W is defined for the final summary. We represent a text as a sequential transactional dataset. Such a dataset consists of transactions (sentences), denoted by T1, ... , Tn, and unique it</context>
<context position="6649" citStr="Vreeken et al., 2011" startWordPosition="1082" endWordPosition="1085">–codes from the Coding Table CT, where shorter codes are assigned to more frequent sequences. The description length of non-encoded terms is assumed proportional to their length (number of characters). We rank sentences by their coverage of the best compressing 1abbreviation of two words: GAp and kriMP 2normalized words following tokenization, stemming, and stop-word removal set, which is the number of CT members in the sentence. The sentences with the highest coverage score are added to the summary as long as its length does not exceed W. 2.2 Krimping text The purpose of the Krimp algorithm (Vreeken et al., 2011) is to use frequent sets (or sequences) to compress a transactional database in order to achieve MDL for that database. Let FreqSeq be the set of all frequent sequences in the database. A collection CT of sequences from FreqSeq (called the Coding Table) is called best when it minimizes L(CT) + L(D|CT). We are interested in both exact and inexact sequences, allowing a sequence to have gaps inside it as long as the ratio of sequence length to sequence length with gaps does not exceed a pre-set parameter Gap ∈ (0, 1]. Sequences with gaps make sense in text data, as phrasing of the same fact or en</context>
</contexts>
<marker>Vreeken, Leeuwen, Siebes, 2011</marker>
<rawString>Jilles Vreeken, Matthijs Leeuwen, and Arno Siebes. 2011. Krimp: Mining itemsets that compress. Data Min. Knowl. Discov., 23(1):169–214, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
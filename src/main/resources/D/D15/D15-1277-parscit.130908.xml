<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003835">
<title confidence="0.995976">
Can Symbol Grounding Improve Low-Level NLP?
Word Segmentation as a Case Study
</title>
<author confidence="0.860932">
Hirotaka Kameko†, Shinsuke Mori‡, and Yoshimasa Tsuruoka††Graduate School of Engineering, The University of Tokyo
</author>
<affiliation confidence="0.64536">
Hongo, Bunkyo-ku, Tokyo, Japan
</affiliation>
<email confidence="0.704775">
{kameko, tsuruoka}@logos.t.u-tokyo.ac.jp
</email>
<affiliation confidence="0.578134">
$Academic Center for Computing and Media Studies, Kyoto University
</affiliation>
<address confidence="0.574925">
Yoshida Honmachi, Sakyo-ku, Kyoto, Japan
</address>
<email confidence="0.996119">
forest@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.997359" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918857142857">
We propose a novel framework for im-
proving a word segmenter using informa-
tion acquired from symbol grounding. We
generate a term dictionary in three steps:
generating a pseudo-stochastically seg-
mented corpus, building a symbol ground-
ing model to enumerate word candidates,
and filtering them according to the ground-
ing scores. We applied our method to
game records of Japanese chess with com-
mentaries. The experimental results show
that the accuracy of a word segmenter can
be improved by incorporating the gener-
ated dictionary.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999759461538462">
Today we can easily obtain a large amount of
text associated with multi-modal information, and
there is a growing interest in the use of non-
textual information in the natural language pro-
cessing (NLP) community. Many of these studies
aim to output natural language sentences from a
nonlinguistic modality, such as image (Farhadi et
al., 2010; Yang et al., 2011; Rohrbach et al., 2013).
Kiros et al. (2014) showed that multi-modal infor-
mation improves the performance of a language
model.
Inspired by these studies, we explore a method
for improving the performance of a low-level NLP
task using multi-modal information. In this work,
we focus on the task of word segmentation (WS)
in Japanese. WS is often performed as the first
processing step for languages without clear word
boundaries, and it is as important as part-of-speech
(POS) tagging in English. We assume that a
large set of pairs of non-textual data and sentences
describing them is available as the information
source. In our experiments, the pairs consist of
game states in Shogi (Japanese chess) and textual
comments on them, which were made by Shogi
experts. We enumerate substrings (character se-
quences) in the sentences and match them with
Shogi states by a neural network model. The ra-
tionale here is that substrings which match with
non-language data well tend to be real words.
Our method consists of three steps (see Figure
1). First, we segment commentary sentences for a
game state in various ways to produce word can-
didates. Then, we match them with game states of
a Shogi playing program. Finally, we compile the
symbol grounding results at all states and incorpo-
rate them to an automatic WS. To the best of our
knowledge, this is the first result reporting a per-
formance improvement in an NLP task by symbol
grounding.
</bodyText>
<sectionHeader confidence="0.90881" genericHeader="introduction">
2 Stochastically Segmented Corpus
</sectionHeader>
<bodyText confidence="0.999869285714286">
Before symbol grounding, we need to segment
the text into words that include probable candi-
date words. For this purpose, we use a stochasti-
cally segmented corpus (SSC) (Mori and Takuma,
2004). Then we propose to simulate it by a normal
(deterministically) segmented corpus to avoid the
problem of computational cost.
</bodyText>
<subsectionHeader confidence="0.998225">
2.1 Stochastically Segmented Corpora
</subsectionHeader>
<bodyText confidence="0.967763769230769">
An SSC is defined as a combination of a raw
corpus Cr (hereafter referred to as the character
sequence xnr
1 ) and word boundary probabilities
of the form Pi, which is the probability that a
word boundary exists between two characters xi
and xi+1. These probabilities are estimated by a
model based on logistic regression (LR) (Fan et
al., 2008) trained on a manually segmented cor-
pus by referring to the surrounding charactersl.
Since there are word boundaries before the first
character and after the last character of the corpus,
P0 = Pnr = 1. The expected frequency of a word
</bodyText>
<footnote confidence="0.619517">
&apos;In the experiment we used the same features as those
used in Neubig et al., (2011). .
</footnote>
<page confidence="0.918837">
2298
</page>
<note confidence="0.934193">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2298–2303,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.999654">
Figure 1: Overview of our method.
</figureCaption>
<equation confidence="0.932547666666667">
w in an SSC is calculated as follows: fr(w) =
∑icO Pi
{∏k=1−(1
− Pi+j) } Pi+k, where O =
{i xi+k
i+1 = w} is the set of all the occurrences
</equation>
<bodyText confidence="0.934474">
of the string matching with w2.
</bodyText>
<subsectionHeader confidence="0.960572">
2.2 Pseudo-Stochastically Segmented
Corpora
</subsectionHeader>
<bodyText confidence="0.999754222222222">
The computational cost (in terms of both time
and space) for calculating the expected frequen-
cies in an SSC is very high3, so it is not a prac-
tical approach for symbol grounding. In this
work, we approximate an SSC using a determinis-
tically segmented corpus, which we call a pseudo-
stochastically segmented corpus (pSSC). The fol-
lowing is the process we use to produce a pSSC
from an SSC.
</bodyText>
<listItem confidence="0.9993376">
• For i = 1 to nr − 1
1. output a character xi,
2. generate a random number 0 &lt; p &lt; 1,
3. output a word boundary if p &lt; Pi or
output nothing otherwise.
</listItem>
<bodyText confidence="0.999008375">
Now we have a corpus in the same format as
a standard segmented corpus with variable (non-
constant) segmentation, where xi and xi+1 are
segmented with the probability of Pi. We exe-
cute the above procedure m times and divide the
counts by m. The law of large numbers guarantees
that the approximation errors decrease to 0 when
m _* 00.
</bodyText>
<sectionHeader confidence="0.972539" genericHeader="method">
3 Symbol Grounding
</sectionHeader>
<bodyText confidence="0.998947">
As the target of symbol grounding, we use states
(piece positions) of a Shogi game and commen-
</bodyText>
<footnote confidence="0.9462042">
2For a detailed explanation and a mathematical proof of
this method, please refer to Mori and Takuma (2004) .
3This is because an SSC has many words and word frag-
ments. Additionally, word 1-gram frequencies must be cal-
culated using floating point numbers instead of integers.
</footnote>
<bodyText confidence="0.9997815">
taries associated with them. We should note, how-
ever, that our framework is general and applica-
ble to different types of combinations such as im-
age/description pairs (Regneri et al., 2013).
</bodyText>
<subsectionHeader confidence="0.998558">
3.1 Game Commentary
</subsectionHeader>
<bodyText confidence="0.999964">
The Japanese language is one of the languages
without clear word boundaries and we need an au-
tomatic WS as the first step of NLP. In Shogi, there
are many professional players and many commen-
taries about game states are available.
</bodyText>
<subsectionHeader confidence="0.999756">
3.2 Grounding Words
</subsectionHeader>
<bodyText confidence="0.993185555555556">
We build a symbol grounding model using a Shogi
commentary dataset. We use a set of pairs of a
Shogi state Si and a commentary sentence Ci as
the training set. A Shogi state Si is converted into
a feature vector f(Si). We generate m (in our ex-
periment, m = 4) pSSC C′ i from Ci. C′ i contains
m corpora of the same text body but with differ-
ent word segmentation, C′ij (j = 1, ... , m). We
treat these as m pairs of a feature vector of Shogi
state f(Si) and a sequence of words C′ij. We train
a model which predicts words in C′ij using f(Si)
as input.
We use a multi-layer perceptron as the predic-
tion model. The input is a vector of the features
of a state. The hidden layer is a 100-dimensional
vector and is activated by a bipolar sigmoid func-
tion. Its output is a d-dimensional real-valued vec-
tor, each of whose elements indicates whether a
word in the vocabulary of d words appears in the
commentary or not. The output layer is activated
by a binary sigmoid function.
We use features of Shogi states which a com-
puter Shogi program called Gekisashi (Tsuruoka
et al., 2002) uses to evaluate the states in game
tree search as input. The features of Shogi states
used in this experiment are below:
a) Positions of pieces (e.g. my rook is at 2h).
</bodyText>
<page confidence="0.955483">
2299
</page>
<listItem confidence="0.9661728">
b) Pieces captured (e.g. the opponent has a
bishop).
c) Combinations of a) and b) (e.g. my king is at
7h and the opponent’s rook is at 7b).
d) Other heuristic features.
</listItem>
<bodyText confidence="0.998034636363636">
Among them, a), b) and c) occupy the majority.
Unlike normal symbol grounding, the vocabu-
lary contains many word candidates appearing in
the pSSC generated from the commentaries. Some
are real words and some are wrong fragments.
These wrong fragments will appear more or less
randomly in the commentaries than real words.
The perceptron therefore cannot acquire strong re-
lation between states and fragments and the output
values of the perceptron will be smaller than those
of real words.
</bodyText>
<sectionHeader confidence="0.920062" genericHeader="method">
4 Word Segmentation Using Symbol
Grounding Result
</sectionHeader>
<bodyText confidence="0.999989666666667">
This section describes a baseline automatic word
segmenter and a method for incorporating the
symbol grounding result to it.
</bodyText>
<subsectionHeader confidence="0.98623">
4.1 Baseline Word Segmenter
</subsectionHeader>
<bodyText confidence="0.999963625">
Among many Japanese WS and morphological an-
alyzers (word segmentation and POS tagging), we
adopt pointwise WS (Neubig et al., 2011), because
it is the only word segmenter which is capable of
adding new words without POS information.
The input of the pointwise WS is an unseg-
mented character sequence x = x1x2 · · · xk. The
word segmenter decides if there is a word bound-
ary ti = 1 or not ti = 0 by using support vector
machines (SVMs) (Fan et al., 2008). The features
are character n-grams and character type n-grams
(n = 1, 2, 3) around the decision points in a win-
dow with a width of 6 characters. Additional fea-
tures are triggered if character n-grams in the win-
dow match with character sequences in the dictio-
nary.
</bodyText>
<subsectionHeader confidence="0.9927755">
4.2 Training a Word Segmenter with
Grounded Words
</subsectionHeader>
<bodyText confidence="0.999830444444444">
As a first trial for incorporating symbol ground-
ing results to an NLP task, we propose to gener-
ate a dictionary based on the symbol grounding
result. We can expect that the word candidates
that are given high scores by the perceptron in the
symbol grounding result have strong relationship
to the positions. In other words, we can make a
good dictionary by selecting word candidates in
descending order of the scores. As a method for
</bodyText>
<tableCaption confidence="0.998907">
Table 1: Corpus specifications.
</tableCaption>
<table confidence="0.999412">
#sent. #words #char.
Training
BCCWJ 56,753 1,324,951 1,911,660
Newspaper 8,164 240,097 361,843
Conversation 11,700 147,809 197,941
Develepment 170 2,501 3,340
Shogi-dev.
Test
BCCWJ-test 6,025 148,929 212,261
Shogi-test 3,299 24,966 32,481
</table>
<bodyText confidence="0.985477777777778">
taking all the occurrences into account, we test the
following three functions:
sum: the summation of the scores of all the out-
put vectors,
ave: the average of them,
max: the maximum in them.
First, we acquire a V-dimensional real-valued vec-
tor for each Shogi state Si as the result of symbol
grounding. Then, for each candidate in C-, we
get the element of the vector which corresponds to
the candidate as the score of the candidate. After
that, we get the summation of, the average of, or
the maximum in the scores of the same candidate
over the whole dataset.
Finally we select the top R percent of word can-
didates in descending order of the value of sum,
ave, or max and add them to the WS dictionary
and retrain the model.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9997215">
We conducted word segmentation experiments in
the following settings.
</bodyText>
<subsectionHeader confidence="0.949737">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999763066666667">
The annotated corpus we used to build the base-
line word segmenter is the manually annotated
part (core data) of the Balanced Corpus of Con-
temporary Written Japanese (BCCWJ) (Maekawa,
2008), plus newspaper articles and daily conver-
sation sentences. We also used a 234,652-word
dictionary (UniDic) provided with the BCCWJ.
A small portion of the BCCWJ core data is re-
served for testing. In addition, we manually seg-
mented sentences randomly obtained from Shogi
commentaries. We divided these sentences into
two parts: a development set and a test set. Ta-
ble 1 shows the details of these corpora.
To make a pSSC, we prepared 33,151 pairs of
a Shogi position and a commentary sentence. The
</bodyText>
<page confidence="0.995938">
2300
</page>
<tableCaption confidence="0.996771">
Table 2: WS accuracy on BCCWJ.
</tableCaption>
<table confidence="0.999902333333333">
Recall Prec. F-meas.
Baseline 98.99 99.06 99.03
+ Sym.Gro. 99.03 99.01 99.02
</table>
<tableCaption confidence="0.98311">
Table 3: WS accuracy on Shogi commentaries.
</tableCaption>
<table confidence="0.999530666666667">
Recall Prec. F-meas.
Baseline 90.12 91.43 90.77
+ Sym.Gro. 90.60 91.66 91.13
</table>
<bodyText confidence="0.93658575">
sentences are converted into pSSC m = 4 times
by an LR word segmentation model trained from
the training data in Table 1 and sent to the symbol
grounding module.
</bodyText>
<subsectionHeader confidence="0.998821">
5.2 Word Segmentation Systems
</subsectionHeader>
<bodyText confidence="0.999270071428572">
We built the following two word segmentation
models (Neubig et al., 2011) to evaluate our
framework.
Baseline: The model is trained from training
data shown in Table 1 and UniDic.
+Sym.Gro.: The model is trained from the lan-
guage resources for the Baseline and the
symbol grounding result.
To decide the function and the value of R for
+Sym.Gro. (see Section 4.2), we measured the
accuracies on the development set of all the com-
binations. The best combination was sum and
R = 0.0114. In this case, 127 words were added
to the dictionary.
</bodyText>
<subsectionHeader confidence="0.917528">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99888946875">
Following the standard in word segmentation ex-
periments, the evaluation criteria are recall, preci-
sion, and F-measure (their harmonic mean).
Table 2 and 3 show WS accuracies on BCCWJ-
test and Shogi-test, respectively. The difference in
accuracy of the baseline method on BCCWJ-test
and Shogi-test shows that WS of Shogi commen-
taries is very difficult. Like many other domains,
Shogi commentaries contain many special words
and expressions, which decrease the accuracy.
When we compare the F-measures on Shogi-
test (Table 3), +Sym.Gro. outperforms Baseline.
The improvement is statistically significant (at 5%
level). The error reduction ratio is comparable to a
natural annotation case (Liu et al., 2014), despite
the fact that our method is unsupervised except for
4In addition we measured the accuracies on the test set of
all the combinations and found that the same function and the
value of the parameter are the best. This indicates the stability
of the function and the parameter.
a hyperparameter. Thus we can say that WS im-
provement by symbol grounding is as valuable as
the annotation additions.
From a close look at the comparison of the re-
call and the precision, we see that the improve-
ment in the recall is higher than that of the preci-
sion. This result shows that the symbol grounding
successfully acquired new words with a few erro-
neous words. As the final remark, the result on the
general domain (Table 2) shows that our frame-
work does not cause a severe performance degra-
dation in the general domain.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999734894736842">
The NLP task we focus on in this paper is word
segmentation. One of the first empirical methods
was based on a hidden Markov model (Nagata,
1994). In parallel, there were attempts at solv-
ing Chinese word segmentation in a similar way
(Sproat and Chang, 1996). These methods take
words as the modeling unit.
Recently, Neubig et al. (2011) have presented
a method for directly deciding whether there is a
word boundary or not at each point between char-
acters. For Chinese word segmentation, there are
some attempts at tagging characters with BIES
tags (Xue, 2003) by a sequence labeller such as
CRFs (Lafferty et al., 2001), where B, I, E, and
S means the beginning of a word, intermediate of
a word, the end of a word, and a single charac-
ter word, respectively. The pointwise WS can be
seen as character tagging with the BI tag system,
in which there is no constraint between neighbor-
ing tags. For Japanese WS, our preliminary exper-
iments showed that the combination of the BI tag
system with SVMs is slightly better than the BIES
tag system with CRFs. This is another reason why
we used the former in this paper. Our extension of
word segmentation is, however, applicable to the
BIES/CRFs combination as well.
The method we describe in this paper is un-
supervised and requires a small amount of anno-
tated data to tune the hyperparameter. From this
viewpoint, the approach based on natural annota-
tion (Yang and Vozila, 2014; Jiang et al., 2013;
Liu et al., 2014) may come to readers’ mind. In
these studies, tags in hyper-texts were regarded
as partial annotations and used to improve WS
performance using CRFs trainable from such data
(Tsuboi et al., 2008). Mori and Nagao (1996) pro-
posed a method for extracting new words from a
large amount of raw text. Murawaki and Kuro-
</bodyText>
<page confidence="0.95912">
2301
</page>
<bodyText confidence="0.99923325">
hashi (2008) proposed an online method in a sim-
ilar setting. In contrast to these studies, this paper
proposes to use other modalities, game states as
the first trial, than languages.
</bodyText>
<sectionHeader confidence="0.990502" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999972157894737">
We have described an unsupervised method for
improving word segmentation based on symbol
grounding results. To extract word candidates
from raw sentences, we first segment sentences
stochastically, and then match the word candidate
sequences with game states that are described by
the sentences. Finally, we selected word candi-
dates referring to the grounding scores. The exper-
imental results showed that we can improve word
segmentation by using symbol grounding results.
Our framework is general and it is worth testing
on other NLP tasks. As future work, we will apply
other deep neural network models to our approach.
It is interesting to apply the symbol grounding re-
sults to an embedding model-based word segmen-
tation approach (Ma and Hinrichs, 2015). It is also
interesting to extend our method to deal with other
types of non-textual information such as images
and economic indices.
</bodyText>
<sectionHeader confidence="0.952444" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99469175">
We thank the anonymous reviewers for their help-
ful comments and suggestions. This work was
supported by JSPS Grants-in-Aid for Scientific
Research Grant Number 26540190.
</bodyText>
<sectionHeader confidence="0.999094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998113355263158">
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from images.
In Proceedings of the 11th European Conference on
Computer Vision, pages 15–29.
Wenbin Jiang, Meng Sun, Yajuan Lu, Yating Yang, and
Qun Liu. 2013. Discriminative learning with natu-
ral annotations: Word segmentation as a case study.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 761–
769.
Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel.
2014. Multimodal neural language models. In Pro-
ceedings of the 31st International Conference on
Machine Learning, pages 595–603.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289.
Yijia Liu, Yue Zhang, Wangxiang Che, Ting Liu, and
Fan Wu. 2014. Domain adaptation for CRF-based
Chinese word segmentation using free annotations.
In Proceedings of the 2014 Conference on Empiri-
cal Methods in Natural Language Processing, pages
864–874.
Jianqiang Ma and Erhard Hinrichs. 2015. Accurate
linear-time Chinese word segmentation via embed-
ding matching. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1733–1743.
Kikuo Maekawa. 2008. Balanced corpus of con-
temporary written Japanese. In Proceedings of the
6th Workshop on Asian Language Resources, pages
101–102.
Shinsuke Mori and Makoto Nagao. 1996. Word ex-
traction from corpora and its part-of-speech estima-
tion using distributional analysis. In Proceedings of
the 16th International Conference on Computational
Linguistics, pages 1119–1122.
Shinsuke Mori and Daisuke Takuma. 2004. Word
n-gram probability estimation from a Japanese raw
corpus. In Proceedings of the Eighth International
Conference on Speech and Language Processing,
pages 1037–1040.
Yugo Murawaki and Sadao Kurohashi. 2008. Online
acquisition of Japanese unknown morphemes using
morphological constraints. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 429–437.
Masaaki Nagata. 1994. A stochastic Japanese mor-
phological analyzer using a forward-DP backward-
A* N-best search algorithm. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 201–207.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics, pages 529–533.
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Com-
putational Linguistics, 1:25–36.
Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater,
Manfred Pinkal, and Bernt Schiele. 2013. Translat-
ing video content to natural language descriptions.
In Proceedings of the 14th International Conference
on Computer Vision, pages 433–440.
</reference>
<page confidence="0.857469">
2302
</page>
<reference confidence="0.999177730769231">
Richard Sproat and Chilin Shih William Gale Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377–404.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations.
In Proceedings of the 22nd International Conference
on Computational Linguistics, pages 897–904.
Yoshimasa Tsuruoka, Daisaku Yokoyama, and Takashi
Chikayama. 2002. Game-tree search algorithm
based on realization probability. ICGA Journal,
25(3):145–152.
N. Xue. 2003. Chinese word segmentation as charac-
ter tagging. International Journal of Computational
Linguistics and Chinese, 8(1):29–48.
Fan Yang and Paul Vozila. 2014. Semi-supervised
Chinese word segmentation using partial-label
learning with conditional random fields. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing, pages 90–98.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 444–454.
</reference>
<page confidence="0.965265">
2303
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.202057">
<title confidence="0.995815">Can Symbol Grounding Improve Low-Level</title>
<author confidence="0.833818">Word Segmentation as a Case Study</author>
<affiliation confidence="0.724428333333333">Shinsuke and Yoshimasa School of Engineering, The University of Hongo, Bunkyo-ku, Tokyo, Center for Computing and Media Studies, Kyoto</affiliation>
<address confidence="0.451718">Yoshida Honmachi, Sakyo-ku, Kyoto,</address>
<email confidence="0.991478">forest@i.kyoto-u.ac.jp</email>
<abstract confidence="0.9941688">We propose a novel framework for improving a word segmenter using information acquired from symbol grounding. We generate a term dictionary in three steps: generating a pseudo-stochastically segmented corpus, building a symbol grounding model to enumerate word candidates, and filtering them according to the grounding scores. We applied our method to game records of Japanese chess with commentaries. The experimental results show that the accuracy of a word segmenter can be improved by incorporating the generated dictionary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="3478" citStr="Fan et al., 2008" startWordPosition="552" endWordPosition="555">ble candidate words. For this purpose, we use a stochastically segmented corpus (SSC) (Mori and Takuma, 2004). Then we propose to simulate it by a normal (deterministically) segmented corpus to avoid the problem of computational cost. 2.1 Stochastically Segmented Corpora An SSC is defined as a combination of a raw corpus Cr (hereafter referred to as the character sequence xnr 1 ) and word boundary probabilities of the form Pi, which is the probability that a word boundary exists between two characters xi and xi+1. These probabilities are estimated by a model based on logistic regression (LR) (Fan et al., 2008) trained on a manually segmented corpus by referring to the surrounding charactersl. Since there are word boundaries before the first character and after the last character of the corpus, P0 = Pnr = 1. The expected frequency of a word &apos;In the experiment we used the same features as those used in Neubig et al., (2011). . 2298 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2298–2303, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Overview of our method. w in an SSC is calculated as follows: fr(w) </context>
<context position="8538" citStr="Fan et al., 2008" startWordPosition="1461" endWordPosition="1464">ing Result This section describes a baseline automatic word segmenter and a method for incorporating the symbol grounding result to it. 4.1 Baseline Word Segmenter Among many Japanese WS and morphological analyzers (word segmentation and POS tagging), we adopt pointwise WS (Neubig et al., 2011), because it is the only word segmenter which is capable of adding new words without POS information. The input of the pointwise WS is an unsegmented character sequence x = x1x2 · · · xk. The word segmenter decides if there is a word boundary ti = 1 or not ti = 0 by using support vector machines (SVMs) (Fan et al., 2008). The features are character n-grams and character type n-grams (n = 1, 2, 3) around the decision points in a window with a width of 6 characters. Additional features are triggered if character n-grams in the window match with character sequences in the dictionary. 4.2 Training a Word Segmenter with Grounded Words As a first trial for incorporating symbol grounding results to an NLP task, we propose to generate a dictionary based on the symbol grounding result. We can expect that the word candidates that are given high scores by the perceptron in the symbol grounding result have strong relatio</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: Generating sentences from images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European Conference on Computer Vision,</booktitle>
<pages>15--29</pages>
<contexts>
<context position="1289" citStr="Farhadi et al., 2010" startWordPosition="186" endWordPosition="189">idates, and filtering them according to the grounding scores. We applied our method to game records of Japanese chess with commentaries. The experimental results show that the accuracy of a word segmenter can be improved by incorporating the generated dictionary. 1 Introduction Today we can easily obtain a large amount of text associated with multi-modal information, and there is a growing interest in the use of nontextual information in the natural language processing (NLP) community. Many of these studies aim to output natural language sentences from a nonlinguistic modality, such as image (Farhadi et al., 2010; Yang et al., 2011; Rohrbach et al., 2013). Kiros et al. (2014) showed that multi-modal information improves the performance of a language model. Inspired by these studies, we explore a method for improving the performance of a low-level NLP task using multi-modal information. In this work, we focus on the task of word segmentation (WS) in Japanese. WS is often performed as the first processing step for languages without clear word boundaries, and it is as important as part-of-speech (POS) tagging in English. We assume that a large set of pairs of non-textual data and sentences describing the</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: Generating sentences from images. In Proceedings of the 11th European Conference on Computer Vision, pages 15–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Meng Sun</author>
<author>Yajuan Lu</author>
<author>Yating Yang</author>
<author>Qun Liu</author>
</authors>
<title>Discriminative learning with natural annotations: Word segmentation as a case study.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="15033" citStr="Jiang et al., 2013" startWordPosition="2569" endWordPosition="2572">tag system, in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for improving word segment</context>
</contexts>
<marker>Jiang, Sun, Lu, Yang, Liu, 2013</marker>
<rawString>Wenbin Jiang, Meng Sun, Yajuan Lu, Yating Yang, and Qun Liu. 2013. Discriminative learning with natural annotations: Word segmentation as a case study. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761– 769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Ruslan Salakhutdinov</author>
<author>Rich Zemel</author>
</authors>
<title>Multimodal neural language models.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="1353" citStr="Kiros et al. (2014)" startWordPosition="198" endWordPosition="201">applied our method to game records of Japanese chess with commentaries. The experimental results show that the accuracy of a word segmenter can be improved by incorporating the generated dictionary. 1 Introduction Today we can easily obtain a large amount of text associated with multi-modal information, and there is a growing interest in the use of nontextual information in the natural language processing (NLP) community. Many of these studies aim to output natural language sentences from a nonlinguistic modality, such as image (Farhadi et al., 2010; Yang et al., 2011; Rohrbach et al., 2013). Kiros et al. (2014) showed that multi-modal information improves the performance of a language model. Inspired by these studies, we explore a method for improving the performance of a low-level NLP task using multi-modal information. In this work, we focus on the task of word segmentation (WS) in Japanese. WS is often performed as the first processing step for languages without clear word boundaries, and it is as important as part-of-speech (POS) tagging in English. We assume that a large set of pairs of non-textual data and sentences describing them is available as the information source. In our experiments, th</context>
</contexts>
<marker>Kiros, Salakhutdinov, Zemel, 2014</marker>
<rawString>Ryan Kiros, Ruslan Salakhutdinov, and Rich Zemel. 2014. Multimodal neural language models. In Proceedings of the 31st International Conference on Machine Learning, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="14213" citStr="Lafferty et al., 2001" startWordPosition="2422" endWordPosition="2425"> The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model (Nagata, 1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way (Sproat and Chang, 1996). These methods take words as the modeling unit. Recently, Neubig et al. (2011) have presented a method for directly deciding whether there is a word boundary or not at each point between characters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags (Xue, 2003) by a sequence labeller such as CRFs (Lafferty et al., 2001), where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single character word, respectively. The pointwise WS can be seen as character tagging with the BI tag system, in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijia Liu</author>
<author>Yue Zhang</author>
<author>Wangxiang Che</author>
<author>Ting Liu</author>
<author>Fan Wu</author>
</authors>
<title>Domain adaptation for CRF-based Chinese word segmentation using free annotations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>864--874</pages>
<contexts>
<context position="12762" citStr="Liu et al., 2014" startWordPosition="2166" endWordPosition="2169">recall, precision, and F-measure (their harmonic mean). Table 2 and 3 show WS accuracies on BCCWJtest and Shogi-test, respectively. The difference in accuracy of the baseline method on BCCWJ-test and Shogi-test shows that WS of Shogi commentaries is very difficult. Like many other domains, Shogi commentaries contain many special words and expressions, which decrease the accuracy. When we compare the F-measures on Shogitest (Table 3), +Sym.Gro. outperforms Baseline. The improvement is statistically significant (at 5% level). The error reduction ratio is comparable to a natural annotation case (Liu et al., 2014), despite the fact that our method is unsupervised except for 4In addition we measured the accuracies on the test set of all the combinations and found that the same function and the value of the parameter are the best. This indicates the stability of the function and the parameter. a hyperparameter. Thus we can say that WS improvement by symbol grounding is as valuable as the annotation additions. From a close look at the comparison of the recall and the precision, we see that the improvement in the recall is higher than that of the precision. This result shows that the symbol grounding succe</context>
<context position="15052" citStr="Liu et al., 2014" startWordPosition="2573" endWordPosition="2576"> there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for improving word segmentation based on symb</context>
</contexts>
<marker>Liu, Zhang, Che, Liu, Wu, 2014</marker>
<rawString>Yijia Liu, Yue Zhang, Wangxiang Che, Ting Liu, and Fan Wu. 2014. Domain adaptation for CRF-based Chinese word segmentation using free annotations. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 864–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianqiang Ma</author>
<author>Erhard Hinrichs</author>
</authors>
<title>Accurate linear-time Chinese word segmentation via embedding matching.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1733--1743</pages>
<marker>Ma, Hinrichs, 2015</marker>
<rawString>Jianqiang Ma and Erhard Hinrichs. 2015. Accurate linear-time Chinese word segmentation via embedding matching. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, pages 1733–1743.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
</authors>
<title>Balanced corpus of contemporary written Japanese.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Workshop on Asian Language Resources,</booktitle>
<pages>101--102</pages>
<contexts>
<context position="10570" citStr="Maekawa, 2008" startWordPosition="1807" endWordPosition="1808">didate as the score of the candidate. After that, we get the summation of, the average of, or the maximum in the scores of the same candidate over the whole dataset. Finally we select the top R percent of word candidates in descending order of the value of sum, ave, or max and add them to the WS dictionary and retrain the model. 5 Evaluation We conducted word segmentation experiments in the following settings. 5.1 Corpora The annotated corpus we used to build the baseline word segmenter is the manually annotated part (core data) of the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), plus newspaper articles and daily conversation sentences. We also used a 234,652-word dictionary (UniDic) provided with the BCCWJ. A small portion of the BCCWJ core data is reserved for testing. In addition, we manually segmented sentences randomly obtained from Shogi commentaries. We divided these sentences into two parts: a development set and a test set. Table 1 shows the details of these corpora. To make a pSSC, we prepared 33,151 pairs of a Shogi position and a commentary sentence. The 2300 Table 2: WS accuracy on BCCWJ. Recall Prec. F-meas. Baseline 98.99 99.06 99.03 + Sym.Gro. 99.03 9</context>
</contexts>
<marker>Maekawa, 2008</marker>
<rawString>Kikuo Maekawa. 2008. Balanced corpus of contemporary written Japanese. In Proceedings of the 6th Workshop on Asian Language Resources, pages 101–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Makoto Nagao</author>
</authors>
<title>Word extraction from corpora and its part-of-speech estimation using distributional analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>1119--1122</pages>
<contexts>
<context position="15270" citStr="Mori and Nagao (1996)" startWordPosition="2609" endWordPosition="2612">is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for improving word segmentation based on symbol grounding results. To extract word candidates from raw sentences, we first segment sentences stochastically, and then match the word candidate sequences with game states that are described by the sentences. Finally,</context>
</contexts>
<marker>Mori, Nagao, 1996</marker>
<rawString>Shinsuke Mori and Makoto Nagao. 1996. Word extraction from corpora and its part-of-speech estimation using distributional analysis. In Proceedings of the 16th International Conference on Computational Linguistics, pages 1119–1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Daisuke Takuma</author>
</authors>
<title>Word n-gram probability estimation from a Japanese raw corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth International Conference on Speech and Language Processing,</booktitle>
<pages>1037--1040</pages>
<contexts>
<context position="2970" citStr="Mori and Takuma, 2004" startWordPosition="468" endWordPosition="471"> First, we segment commentary sentences for a game state in various ways to produce word candidates. Then, we match them with game states of a Shogi playing program. Finally, we compile the symbol grounding results at all states and incorporate them to an automatic WS. To the best of our knowledge, this is the first result reporting a performance improvement in an NLP task by symbol grounding. 2 Stochastically Segmented Corpus Before symbol grounding, we need to segment the text into words that include probable candidate words. For this purpose, we use a stochastically segmented corpus (SSC) (Mori and Takuma, 2004). Then we propose to simulate it by a normal (deterministically) segmented corpus to avoid the problem of computational cost. 2.1 Stochastically Segmented Corpora An SSC is defined as a combination of a raw corpus Cr (hereafter referred to as the character sequence xnr 1 ) and word boundary probabilities of the form Pi, which is the probability that a word boundary exists between two characters xi and xi+1. These probabilities are estimated by a model based on logistic regression (LR) (Fan et al., 2008) trained on a manually segmented corpus by referring to the surrounding charactersl. Since t</context>
<context position="5343" citStr="Mori and Takuma (2004)" startWordPosition="889" endWordPosition="892"> 1, 3. output a word boundary if p &lt; Pi or output nothing otherwise. Now we have a corpus in the same format as a standard segmented corpus with variable (nonconstant) segmentation, where xi and xi+1 are segmented with the probability of Pi. We execute the above procedure m times and divide the counts by m. The law of large numbers guarantees that the approximation errors decrease to 0 when m _* 00. 3 Symbol Grounding As the target of symbol grounding, we use states (piece positions) of a Shogi game and commen2For a detailed explanation and a mathematical proof of this method, please refer to Mori and Takuma (2004) . 3This is because an SSC has many words and word fragments. Additionally, word 1-gram frequencies must be calculated using floating point numbers instead of integers. taries associated with them. We should note, however, that our framework is general and applicable to different types of combinations such as image/description pairs (Regneri et al., 2013). 3.1 Game Commentary The Japanese language is one of the languages without clear word boundaries and we need an automatic WS as the first step of NLP. In Shogi, there are many professional players and many commentaries about game states are a</context>
</contexts>
<marker>Mori, Takuma, 2004</marker>
<rawString>Shinsuke Mori and Daisuke Takuma. 2004. Word n-gram probability estimation from a Japanese raw corpus. In Proceedings of the Eighth International Conference on Speech and Language Processing, pages 1037–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yugo Murawaki</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Online acquisition of Japanese unknown morphemes using morphological constraints.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>429--437</pages>
<marker>Murawaki, Kurohashi, 2008</marker>
<rawString>Yugo Murawaki and Sadao Kurohashi. 2008. Online acquisition of Japanese unknown morphemes using morphological constraints. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 429–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-DP backwardA* N-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="13737" citStr="Nagata, 1994" startWordPosition="2344" endWordPosition="2345">s valuable as the annotation additions. From a close look at the comparison of the recall and the precision, we see that the improvement in the recall is higher than that of the precision. This result shows that the symbol grounding successfully acquired new words with a few erroneous words. As the final remark, the result on the general domain (Table 2) shows that our framework does not cause a severe performance degradation in the general domain. 6 Related Work The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model (Nagata, 1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way (Sproat and Chang, 1996). These methods take words as the modeling unit. Recently, Neubig et al. (2011) have presented a method for directly deciding whether there is a word boundary or not at each point between characters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags (Xue, 2003) by a sequence labeller such as CRFs (Lafferty et al., 2001), where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single character word</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backwardA* N-best search algorithm. In Proceedings of the 15th International Conference on Computational Linguistics, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yosuke Nakata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust, adaptable Japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>529--533</pages>
<contexts>
<context position="3796" citStr="Neubig et al., (2011)" startWordPosition="609" endWordPosition="612">w corpus Cr (hereafter referred to as the character sequence xnr 1 ) and word boundary probabilities of the form Pi, which is the probability that a word boundary exists between two characters xi and xi+1. These probabilities are estimated by a model based on logistic regression (LR) (Fan et al., 2008) trained on a manually segmented corpus by referring to the surrounding charactersl. Since there are word boundaries before the first character and after the last character of the corpus, P0 = Pnr = 1. The expected frequency of a word &apos;In the experiment we used the same features as those used in Neubig et al., (2011). . 2298 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2298–2303, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Overview of our method. w in an SSC is calculated as follows: fr(w) = ∑icO Pi {∏k=1−(1 − Pi+j) } Pi+k, where O = {i xi+k i+1 = w} is the set of all the occurrences of the string matching with w2. 2.2 Pseudo-Stochastically Segmented Corpora The computational cost (in terms of both time and space) for calculating the expected frequencies in an SSC is very high3, so it is not a practica</context>
<context position="8216" citStr="Neubig et al., 2011" startWordPosition="1396" endWordPosition="1399">some are wrong fragments. These wrong fragments will appear more or less randomly in the commentaries than real words. The perceptron therefore cannot acquire strong relation between states and fragments and the output values of the perceptron will be smaller than those of real words. 4 Word Segmentation Using Symbol Grounding Result This section describes a baseline automatic word segmenter and a method for incorporating the symbol grounding result to it. 4.1 Baseline Word Segmenter Among many Japanese WS and morphological analyzers (word segmentation and POS tagging), we adopt pointwise WS (Neubig et al., 2011), because it is the only word segmenter which is capable of adding new words without POS information. The input of the pointwise WS is an unsegmented character sequence x = x1x2 · · · xk. The word segmenter decides if there is a word boundary ti = 1 or not ti = 0 by using support vector machines (SVMs) (Fan et al., 2008). The features are character n-grams and character type n-grams (n = 1, 2, 3) around the decision points in a window with a width of 6 characters. Additional features are triggered if character n-grams in the window match with character sequences in the dictionary. 4.2 Training</context>
<context position="11567" citStr="Neubig et al., 2011" startWordPosition="1973" endWordPosition="1976">he details of these corpora. To make a pSSC, we prepared 33,151 pairs of a Shogi position and a commentary sentence. The 2300 Table 2: WS accuracy on BCCWJ. Recall Prec. F-meas. Baseline 98.99 99.06 99.03 + Sym.Gro. 99.03 99.01 99.02 Table 3: WS accuracy on Shogi commentaries. Recall Prec. F-meas. Baseline 90.12 91.43 90.77 + Sym.Gro. 90.60 91.66 91.13 sentences are converted into pSSC m = 4 times by an LR word segmentation model trained from the training data in Table 1 and sent to the symbol grounding module. 5.2 Word Segmentation Systems We built the following two word segmentation models (Neubig et al., 2011) to evaluate our framework. Baseline: The model is trained from training data shown in Table 1 and UniDic. +Sym.Gro.: The model is trained from the language resources for the Baseline and the symbol grounding result. To decide the function and the value of R for +Sym.Gro. (see Section 4.2), we measured the accuracies on the development set of all the combinations. The best combination was sum and R = 0.0114. In this case, 127 words were added to the dictionary. 5.3 Results and Discussion Following the standard in word segmentation experiments, the evaluation criteria are recall, precision, and</context>
<context position="13929" citStr="Neubig et al. (2011)" startWordPosition="2374" endWordPosition="2377">on. This result shows that the symbol grounding successfully acquired new words with a few erroneous words. As the final remark, the result on the general domain (Table 2) shows that our framework does not cause a severe performance degradation in the general domain. 6 Related Work The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model (Nagata, 1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way (Sproat and Chang, 1996). These methods take words as the modeling unit. Recently, Neubig et al. (2011) have presented a method for directly deciding whether there is a word boundary or not at each point between characters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags (Xue, 2003) by a sequence labeller such as CRFs (Lafferty et al., 2001), where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single character word, respectively. The pointwise WS can be seen as character tagging with the BI tag system, in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments</context>
</contexts>
<marker>Neubig, Nakata, Mori, 2011</marker>
<rawString>Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable Japanese morphological analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 529–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Marcus Rohrbach</author>
<author>Dominikus Wetzel</author>
<author>Stefan Thater</author>
<author>Bernt Schiele</author>
<author>Manfred Pinkal</author>
</authors>
<title>Grounding action descriptions in videos.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--25</pages>
<contexts>
<context position="5700" citStr="Regneri et al., 2013" startWordPosition="948" endWordPosition="951">oximation errors decrease to 0 when m _* 00. 3 Symbol Grounding As the target of symbol grounding, we use states (piece positions) of a Shogi game and commen2For a detailed explanation and a mathematical proof of this method, please refer to Mori and Takuma (2004) . 3This is because an SSC has many words and word fragments. Additionally, word 1-gram frequencies must be calculated using floating point numbers instead of integers. taries associated with them. We should note, however, that our framework is general and applicable to different types of combinations such as image/description pairs (Regneri et al., 2013). 3.1 Game Commentary The Japanese language is one of the languages without clear word boundaries and we need an automatic WS as the first step of NLP. In Shogi, there are many professional players and many commentaries about game states are available. 3.2 Grounding Words We build a symbol grounding model using a Shogi commentary dataset. We use a set of pairs of a Shogi state Si and a commentary sentence Ci as the training set. A Shogi state Si is converted into a feature vector f(Si). We generate m (in our experiment, m = 4) pSSC C′ i from Ci. C′ i contains m corpora of the same text body bu</context>
</contexts>
<marker>Regneri, Rohrbach, Wetzel, Thater, Schiele, Pinkal, 2013</marker>
<rawString>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics, 1:25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Rohrbach</author>
<author>Wei Qiu</author>
<author>Ivan Titov</author>
<author>Stefan Thater</author>
<author>Manfred Pinkal</author>
<author>Bernt Schiele</author>
</authors>
<title>Translating video content to natural language descriptions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th International Conference on Computer Vision,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="1332" citStr="Rohrbach et al., 2013" startWordPosition="194" endWordPosition="197">he grounding scores. We applied our method to game records of Japanese chess with commentaries. The experimental results show that the accuracy of a word segmenter can be improved by incorporating the generated dictionary. 1 Introduction Today we can easily obtain a large amount of text associated with multi-modal information, and there is a growing interest in the use of nontextual information in the natural language processing (NLP) community. Many of these studies aim to output natural language sentences from a nonlinguistic modality, such as image (Farhadi et al., 2010; Yang et al., 2011; Rohrbach et al., 2013). Kiros et al. (2014) showed that multi-modal information improves the performance of a language model. Inspired by these studies, we explore a method for improving the performance of a low-level NLP task using multi-modal information. In this work, we focus on the task of word segmentation (WS) in Japanese. WS is often performed as the first processing step for languages without clear word boundaries, and it is as important as part-of-speech (POS) tagging in English. We assume that a large set of pairs of non-textual data and sentences describing them is available as the information source. I</context>
</contexts>
<marker>Rohrbach, Qiu, Titov, Thater, Pinkal, Schiele, 2013</marker>
<rawString>Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele. 2013. Translating video content to natural language descriptions. In Proceedings of the 14th International Conference on Computer Vision, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih William Gale Nancy Chang</author>
</authors>
<title>A stochastic finite-state wordsegmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="13850" citStr="Sproat and Chang, 1996" startWordPosition="2361" endWordPosition="2364">sion, we see that the improvement in the recall is higher than that of the precision. This result shows that the symbol grounding successfully acquired new words with a few erroneous words. As the final remark, the result on the general domain (Table 2) shows that our framework does not cause a severe performance degradation in the general domain. 6 Related Work The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model (Nagata, 1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way (Sproat and Chang, 1996). These methods take words as the modeling unit. Recently, Neubig et al. (2011) have presented a method for directly deciding whether there is a word boundary or not at each point between characters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags (Xue, 2003) by a sequence labeller such as CRFs (Lafferty et al., 2001), where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single character word, respectively. The pointwise WS can be seen as character tagging with the BI tag system, in which there is no co</context>
</contexts>
<marker>Sproat, Chang, 1996</marker>
<rawString>Richard Sproat and Chilin Shih William Gale Nancy Chang. 1996. A stochastic finite-state wordsegmentation algorithm for Chinese. Computational Linguistics, 22(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
<author>Hisashi Kashima</author>
<author>Shinsuke Mori</author>
<author>Hiroki Oda</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Training conditional random fields using incomplete annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>897--904</pages>
<contexts>
<context position="15247" citStr="Tsuboi et al., 2008" startWordPosition="2605" endWordPosition="2608">ystem with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for improving word segmentation based on symbol grounding results. To extract word candidates from raw sentences, we first segment sentences stochastically, and then match the word candidate sequences with game states that are described by </context>
</contexts>
<marker>Tsuboi, Kashima, Mori, Oda, Matsumoto, 2008</marker>
<rawString>Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki Oda, and Yuji Matsumoto. 2008. Training conditional random fields using incomplete annotations. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 897–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Daisaku Yokoyama</author>
<author>Takashi Chikayama</author>
</authors>
<title>Game-tree search algorithm based on realization probability.</title>
<date>2002</date>
<journal>ICGA Journal,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="7048" citStr="Tsuruoka et al., 2002" startWordPosition="1201" endWordPosition="1204">and a sequence of words C′ij. We train a model which predicts words in C′ij using f(Si) as input. We use a multi-layer perceptron as the prediction model. The input is a vector of the features of a state. The hidden layer is a 100-dimensional vector and is activated by a bipolar sigmoid function. Its output is a d-dimensional real-valued vector, each of whose elements indicates whether a word in the vocabulary of d words appears in the commentary or not. The output layer is activated by a binary sigmoid function. We use features of Shogi states which a computer Shogi program called Gekisashi (Tsuruoka et al., 2002) uses to evaluate the states in game tree search as input. The features of Shogi states used in this experiment are below: a) Positions of pieces (e.g. my rook is at 2h). 2299 b) Pieces captured (e.g. the opponent has a bishop). c) Combinations of a) and b) (e.g. my king is at 7h and the opponent’s rook is at 7b). d) Other heuristic features. Among them, a), b) and c) occupy the majority. Unlike normal symbol grounding, the vocabulary contains many word candidates appearing in the pSSC generated from the commentaries. Some are real words and some are wrong fragments. These wrong fragments will</context>
</contexts>
<marker>Tsuruoka, Yokoyama, Chikayama, 2002</marker>
<rawString>Yoshimasa Tsuruoka, Daisaku Yokoyama, and Takashi Chikayama. 2002. Game-tree search algorithm based on realization probability. ICGA Journal, 25(3):145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<journal>International Journal of Computational Linguistics and Chinese,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="14153" citStr="Xue, 2003" startWordPosition="2413" endWordPosition="2414">egradation in the general domain. 6 Related Work The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model (Nagata, 1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way (Sproat and Chang, 1996). These methods take words as the modeling unit. Recently, Neubig et al. (2011) have presented a method for directly deciding whether there is a word boundary or not at each point between characters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags (Xue, 2003) by a sequence labeller such as CRFs (Lafferty et al., 2001), where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single character word, respectively. The pointwise WS can be seen as character tagging with the BI tag system, in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, </context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>N. Xue. 2003. Chinese word segmentation as character tagging. International Journal of Computational Linguistics and Chinese, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Yang</author>
<author>Paul Vozila</author>
</authors>
<title>Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>90--98</pages>
<contexts>
<context position="15013" citStr="Yang and Vozila, 2014" startWordPosition="2565" endWordPosition="2568">er tagging with the BI tag system, in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for im</context>
</contexts>
<marker>Yang, Vozila, 2014</marker>
<rawString>Fan Yang and Paul Vozila. 2014. Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 90–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>444--454</pages>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
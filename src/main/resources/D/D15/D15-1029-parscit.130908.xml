<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.175538">
<title confidence="0.988496">
Pre-Computable Multi-Layer Neural Network Language Models
</title>
<author confidence="0.997695">
Jacob Devlin Chris Quirk Arul Menezes
</author>
<affiliation confidence="0.989321">
Microsoft Research Microsoft Research Microsoft Research
</affiliation>
<address confidence="0.976372">
Redmond, WA, USA Redmond, WA, USA Redmond, WA, USA
</address>
<email confidence="0.99503">
jdevlin@microsoft.com chrisq@microsoft.com arulm@microsoft.com
</email>
<sectionHeader confidence="0.993822" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947884615385">
In the last several years, neural network
models have significantly improved accu-
racy in a number of NLP tasks. How-
ever, one serious drawback that has im-
peded their adoption in production sys-
tems is the slow runtime speed of neu-
ral network models compared to alternate
models, such as maximum entropy classi-
fiers. In Devlin et al. (2014), the authors
presented a simple technique for speeding
up feed-forward embedding-based neural
network models, where the dot product be-
tween each word embedding and part of
the first hidden layer are pre-computed of-
fline. However, this technique cannot be
used for hidden layers beyond the first. In
this paper, we explore a neural network
architecture where the embedding layer
feeds into multiple hidden layers that are
placed “next to” one another so that each
can be pre-computed independently. On
a large scale language modeling task, this
architecture achieves a 10x speedup at run-
time and a significant reduction in perplex-
ity when compared to a standard multi-
layer network.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999869673076923">
Neural network models have become extremely
popular in the last several years for a wide va-
riety of NLP tasks, including language model-
ing (Schwenk, 2007), sentiment analysis (Socher
et al., 2013), translation modeling (Devlin et al.,
2014), and many others (Collobert et al., 2011).
However, a serious drawback of neural network
models is their slow speeds in training and test
time (runtime) relative to alternative models such
as maximum entropy (Berger et al., 1996) or back-
off models (Kneser and Ney, 1995).
One popular application of neural network
models in NLP is using neural network language
models (NNLMs) as an additional feature in an
existing machine translation (MT) or automatic
speech recognition (ASR) engines. NNLMs are
particularly costly in this scenario, since decoding
a single sentence typically requires tens of thou-
sands or more n-gram lookups. Although we will
focus on this particular scenario in this paper, it
is important to note that the techniques presented
generalize to any feed-forward embedding-based
neural network model.
One popular technique for improving the run-
time speed of NNLMs involves training the net-
work to be “approximately normalized,” so that
the softmax normalizer does not have to be com-
puted after training. Two algorithms have been
proposed to achieve this: (1) noise-contrastive es-
timation (NCE) (Mnih and Teh, 2012; Vaswani et
al., 2013) and (2) explicit self-normalization (De-
vlin et al., 2014), which is used in this paper.
However, even with self-normalized networks,
computing the output of an intermediate hidden
layer still requires a costly matrix-vector multipli-
cation. To mitigate this, Devlin et al. (2014) made
the observation that for 1-layer NNLMs, the dot
product between each embedding+position pair
and the first hidden layer can be pre-computed af-
ter training is complete, which allows the matrix-
vector multiplication to be replaced by a hand-
ful of vector additions. Using these two tech-
niques in combination improves the runtime speed
of NNLMs by several orders of magnitude with no
degradation to accuracy.
To understand pre-computation, first assume
that we are training a NNLM that uses 250-
dimensional word embeddings, a four word con-
text window, and a 500-dimensional hidden layer.
The weight matrix for the first hidden layer is thus
1000 × 500. For each word in the vocabulary and
each of the four positions in the context vector, we
</bodyText>
<page confidence="0.977593">
256
</page>
<note confidence="0.934654">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 256–260,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.9909368">
Figure 1: The “pre-computation trick.” The dot
product between each word embedding and each
section of the hidden layer can be computed of-
fline.
Figure 2: Network architectures.
</figureCaption>
<bodyText confidence="0.999976833333333">
can pre-compute the dot product between the 250-
dimensional word embedding and the 250 × 500
section of the hidden layer. This results in four
500-dimensional vectors for each word that can be
stored in a lookup table. At test time, we can sim-
ply sum four vectors to obtain the output of the
first hidden layer. This is shown visually in Fig-
ure 1. Note that this is not an approximation, and
the resulting output vector is identical to the orig-
inal matrix-vector product. However, the major
limitation of the “pre-computation trick” is that it
only works with 1-hidden layer architectures, even
though more accurate models can nearly always be
obtained by training multi-layer networks.
In this paper, we explore a network architecture
where multiple hidden layers are placed “next to”
one another instead of “on top of” one another, as
is usually done. The output of these lateral lay-
ers are combined using an inexpensive element-
wise function and fed into the output layer. Cru-
cially, then, we can apply the pre-computation
trick to each hidden layer independently, allowing
for very powerful models that are orders of magni-
tude faster at runtime than a standard multi-layer
network.
Mathematically, this can be thought of as a gen-
eralization of maxout networks (Goodfellow et al.,
2013), where different element-wise combination
functions are explored rather than just the max
function.
</bodyText>
<sectionHeader confidence="0.976322" genericHeader="method">
2 Lateral Network
</sectionHeader>
<bodyText confidence="0.999889666666667">
In a standard feed-forward embedding-based neu-
ral network, the input tokens are mapped into a
continuous vector using an embedding table1, and
this embedding vector is fed into the first hidden
layer. The output of each hidden layer is then fed
into the next hidden layer. We refer to this as the
stacked architecture. For a two layer network, we
can represent the output of the final hidden layer
as:
</bodyText>
<equation confidence="0.993414">
H = O(W2O(W1E(x)))
</equation>
<bodyText confidence="0.999278916666667">
where x is the input vector, E(x) is the output
of the embedding layer, Wi is the weight matrix
for layer i, and O is the transfer function such as
tanh. Generally, H is then multiplied by an output
matrix and a softmax is performed to obtain the
output probabilities.
In the lateral network architecture, the embed-
ding layer is fed into two or more “side-by-side”
hidden layers, and the outputs of these hidden lay-
ers are combined using an element-wise function
such as maximum or multiplication. This is repre-
sented as:
</bodyText>
<equation confidence="0.997693">
H = C(O(W1E(x)), O(W2E(x)))
</equation>
<bodyText confidence="0.995132285714286">
Where C is a combination function that takes
two or more k-dimensional vectors as inputs and
produces as k-dimensional vector as output. If
C(h1, h2) = max(h1, h2) then this is equivalent
to a maxout network (Goodfellow et al., 2013). To
generalize this, we explore three different combi-
nation functions: 2
</bodyText>
<equation confidence="0.993434666666667">
Cmax(h1, h2) = max(h1, h2)
Cmul(h1, h2) = h1 ∗ (h2 + 1)
Cadd(h1, h2) = h1 + h2
</equation>
<bodyText confidence="0.999879166666667">
The three-or-more hidden layer versions are
constructed as expected.3
A visualization is given in Figure 2. Crucially,
for the lateral architecture, each hidden layer can
be pre-computed independently, allowing for very
fast n-gram probability lookups at runtime.
</bodyText>
<footnote confidence="0.997197571428571">
1The embeddings may or may not be trained jointly with
the rest of the model.
2Note that C is an element-wise function, so these repre-
sent the operation on a single dimension of the input vectors.
3The + 1 in Cm.., is used for all hidden layers after the
first. This is used to prevent the value from being very close
to 0.
</footnote>
<page confidence="0.992644">
257
</page>
<sectionHeader confidence="0.993778" genericHeader="method">
3 Language Modeling Results
</sectionHeader>
<bodyText confidence="0.9996235">
In this section we report results on a large scale
language modeling task.
</bodyText>
<subsectionHeader confidence="0.990058">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999950181818182">
Our LM training corpus consists of 120M words
from the New York Times portion of the English
GigaWord data set. This was chosen instead of the
commonly used 1M word Penn Tree Bank corpus
in order to better represent real world LM training
scenarios. We use all data from 2008 and 2009 as
training, the first 100k words from June 2010 as
validation, and the first 100k words from Decem-
ber 2010 as test. The data is segmented and to-
kenized using the Stanford Word Segmenter with
default settings.
</bodyText>
<subsectionHeader confidence="0.935749">
3.2 Neural Network Training
</subsectionHeader>
<bodyText confidence="0.999996222222222">
Training was performed with an in-house toolkit
using stochastic gradient descent. The vocab-
ulary is limited to 16k words so that the out-
put layer can be trained using a basic softmax
with self-normalization. All experiments use 250-
dimensional word embeddings and a tanh activa-
tion function. The weights were initialized in the
range [-0.05, 0.05], the batch size was 256, and the
initial learning rate was 0.25.
</bodyText>
<subsectionHeader confidence="0.996178">
3.3 5-gram LM Perplexity
</subsectionHeader>
<bodyText confidence="0.9999295">
5-gram results are shown in Table 1. The 1-layer
NNLM achieves a 13.2 perplexity improvement
over the Kneser-Ney smoothed baseline (Kneser
and Ney, 1995). Consistent with Schwenk et
al. (2014), using additional hidden layers to the
stacked (standard) network results in 2.0-3.0 per-
plexity improvements on top of the 1-layer model.
The lateral architecture significantly outper-
forms any of the stacked networks, achieving a
6.5 perplexity reduction over the 1-layer model.
The multiplicative combination function performs
better than the additive and max functions by a
small margin, which suggests that it better allows
for modeling complex relationships between input
words.
Perhaps most surprisingly, the additive function
performs as well as the max function, despite the
fact that it provides no additional modeling power
compared to a 1-layer network. However, it does
allow the model to generalize better than a 1-layer
network by explicitly tying together two or three
hidden nodes from each node in the output layer.
</bodyText>
<table confidence="0.999898538461539">
Condition PPL
5-gram KNLM 91.1
1-Layer (k=500) 77.9
1-Layer (k=1000) 77.7
2-Stacked (k=500) 76.3
2-Stacked (k=1000) 76.2
3-Stacked (k=1000) 74.8
2-Lateral Max (k=500) 73.8
2-Lateral Mul ... 72.7
2-Lateral Add 73.7
3-Lateral Max 73.1
3-Lateral Mul 71.1
3-Lateral Add 72.3
</table>
<tableCaption confidence="0.980523333333333">
Table 1: Perplexity of 5-gram models on the New
York Times test set. k is the size of the hidden
layer(s).
</tableCaption>
<subsectionHeader confidence="0.977423">
3.4 Runtime Speed
</subsectionHeader>
<bodyText confidence="0.999775172413793">
The runtime speed of the various models is
shown in Table 2. These are computed on a
single core of a E5-2650 2.6 GHz CPU. Con-
sistent with Devlin et al. (2014), we see that
the baseline model achieves only 230 n-gram
lookups per second (LPS) at test time, while the
pre-computed, self-normalized 1-layer network
achieves 600,000 LPS. Adding a second stacked
layer slows this down to 24,000 LPS due to the
500 × 500 matrix-vector multiplication that must
be performed. However, the lateral configura-
tion achieves 305,000 LPS while obtaining a bet-
ter perplexity than the stacked network. In com-
parison, the fastest backoff LM implementation,
KenLM (Heafield, 2011), achieves 1-2 million
lookups per second.
In terms of memory usage, it is difficult to fairly
compare backoff LMs and NNLMs because neural
networks scale linearly with the vocabulary size,
while backoff LMs scale linearly with the num-
ber of unique n-grams. In this case, the non-
precomputed neural network model is 25 MB, and
the pre-computed 2-lateral network is 136 MB.4
The KenLM models are 1.1 GB for the Probing
model and 317 MB for the Trie model. With a
vocabulary of 50k, the 2-lateral network would
be 425MB. In general, a pre-computed NNLM is
comparable to or smaller than an equivalent back-
off LM in terms of model size.
</bodyText>
<footnote confidence="0.9922865">
4The floats can be quantized to 2 bytes after training with-
out loss of accuracy.
</footnote>
<page confidence="0.984749">
258
</page>
<table confidence="0.9997367">
Condition Lookups
Per Sec.
KenLM Probing 1,923,000
KenLM Trie 950,000
1-Layer (No PC, No SN) 230
1-Layer (No PC) 13,000
1-Layer 600,000
2-Stacked 24,000
2-Stacked (Batch=128) 58,000
2-Lateral Mul 305,000
</table>
<tableCaption confidence="0.998433">
Table 2: Runtime speed of the 5-gram LM on a
</tableCaption>
<bodyText confidence="0.91367">
single CPU core. “PC” = pre-computation, “SN”
= self-normalization, which are used in all but the
first two experiments. The batch size is 1 except
when specified. 500-dimensional hidden layers
are used in all cases. “Float Ops.” is the ap-
proximate number of floating point operations per
lookup.
</bodyText>
<subsectionHeader confidence="0.948912">
3.5 High-Order LM Perplexity
</subsectionHeader>
<bodyText confidence="0.998602444444444">
We also report results on a 10-gram LM trained
on the same data, to explore whether the lateral
network can achieve an even higher relative gain
when a large input context window is available.
Results are shown in Table 3. Although there is a
large absolute improvement over the 5-gram LM,
the relative improvement between the 1-layer, 3-
stacked, and 3-lateral systems are similar to the
5-gram scenario.
</bodyText>
<table confidence="0.99928">
Condition PPL
1-Layer (k=500) 69.8
3-Stacked (k=1000) 65.8
3-Lateral Mul (k=500) 63.4
Gated Recurrent (k=1000) 55.4
</table>
<tableCaption confidence="0.966846666666667">
Table 3: Perplexity of 10-gram models on the New
York Times test set. The Gated Recurrent model
uses the full word history.
</tableCaption>
<bodyText confidence="0.9995598">
As another point of comparison we report re-
sults with an gated recurrent network (Cho et al.,
2014). As is consistent with the literature, the
recurrent network significantly outperforms any
of the feed-forward models (Sundermeyer et al.,
2013).
However, recurrent models have two major
downsides. First, they cannot easily be integrated
into existing MT/ASR engines without signifi-
cantly altering the search algorithm and search
</bodyText>
<table confidence="0.999798375">
Condition Test BLEU Test PPL
Baseline 37.95 -
+NNLM 1-Layer 38.89 138.3
+NNLM 2-Stacked 39.13 136.2
+NNLM 2-Lateral 39.15 132.3
+NNJM 1-Layer 40.71 6.33
+NNJM 2-Stacked 40.82 6.25
+NNJM 2-Lateral 40.89 6.13
</table>
<tableCaption confidence="0.93646">
Table 4: Results on English-German machine
translation test set.
</tableCaption>
<bodyText confidence="0.999286833333333">
space, since they require a fully expanded tar-
get context. Second, the matrix-vector product
between the previous hidden state and the hid-
den weight matrix cannot be pre-computed, which
makes the models significantly slower than pre-
computable feed-forward networks.
</bodyText>
<sectionHeader confidence="0.996634" genericHeader="method">
4 Machine Translation Results
</sectionHeader>
<bodyText confidence="0.999764516129032">
Although the lateral networks achieve a significant
reduction in LM perplexity over the 1-layer net-
work, it is not clear how much this will improved
performance in a downstream task. To evaluate
this, we trained two neural network models for
use as additional features in a machine translation
(MT) system.
The first feature is a 5-gram NNLM, which used
1000 dimensions for the stacked network and 500
for the lateral network. The second feature is a
neural network joint model (NNJM), which pre-
dicts each target word using 5-gram target context
and 7-gram source context. For evaluation, we
present both the model perplexity and the BLEU
score when using the model as an additional MT
feature.
Results are presented on a large scale English-
German speech translation task. The parallel train-
ing data consists of 600M words from a variety of
sources, including OPUS (Tiedemann, 2012) and
a large in-house web crawl. The baseline 4-gram
Kneser-Ney smoothed LM is trained on 7B words
of German data. The NNLM and NNTMs are
trained only on the parallel data. Our MT decoder
is a proprietary engine similar to Moses (Koehn et
al., 2007). The tuning set consists of 4000 utter-
ances from conversational and newswire data, and
the test set consists of 1500 sentences of collected
conversational data.
Results are show in Table 4. We can see that
perplexity improvements are similar to what is
</bodyText>
<page confidence="0.992756">
259
</page>
<bodyText confidence="0.999917">
seen in the English NYT data, and that improve-
ments in BLEU over a 1-layer model are small but
consistent. There is not a significant difference in
BLEU between the 2-Stacked and 2-Lateral con-
figuration.
</bodyText>
<sectionHeader confidence="0.9977" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999929818181818">
In this paper, we explored an alternate architec-
ture for embedding-based neural network models
which allows for a fully pre-computable network
with multiple hidden layers. The resulting mod-
els achieve better perplexity than a standard multi-
layer network and is at least an order of magnitude
faster at runtime.
In future work, we can assess the impact of
this model on a wider array of feed-forward
embedding-based neural network models, such as
the DSSM (Huang et al., 2013).
</bodyText>
<sectionHeader confidence="0.99699" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989795818181818">
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics.
Kyunghyun Cho, Bart van Merrienboer, C¸aglar
G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. 2014. Learning phrase representa-
tions using RNN encoder-decoder for statistical ma-
chine translation. CoRR.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In ACL.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron Courville, and Yoshua Bengio. 2013. Max-
out networks. arXiv preprint arXiv:1302.4389.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
international conference on Conference on informa-
tion &amp; knowledge management. ACM.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
Assoc. for Computational Linguistics (ACL).
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the Interna-
tional Conference on Machine Learning.
Holger Schwenk, Fethi Bougares, and Loıc Barrault.
2014. Efficient training strategies for deep neural
network language models. In NIPS Workshop on
Deep Learning and Representation Learning.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech &amp; Language.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In EMNLP.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schluter, and Hermann Ney.
2013. Comparison of feedforward and recurrent
neural network language models. In Proc. Conf.
Acoustics, Speech and Signal Process. (ICASSP).
J¨org Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In LREC.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
EMNLP.
</reference>
<page confidence="0.996795">
260
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930433">
<title confidence="0.999931">Pre-Computable Multi-Layer Neural Network Language Models</title>
<author confidence="0.998">Jacob Devlin Chris Quirk Arul Menezes</author>
<affiliation confidence="0.999741">Microsoft Research Microsoft Research Microsoft Research</affiliation>
<address confidence="0.999">Redmond, WA, USA Redmond, WA, USA Redmond, WA, USA</address>
<email confidence="0.998757">jdevlin@microsoft.comchrisq@microsoft.comarulm@microsoft.com</email>
<abstract confidence="0.997568666666667">In the last several years, neural network models have significantly improved accuracy in a number of NLP tasks. However, one serious drawback that has impeded their adoption in production systems is the slow runtime speed of neural network models compared to alternate models, such as maximum entropy classifiers. In Devlin et al. (2014), the authors presented a simple technique for speeding up feed-forward embedding-based neural network models, where the dot product between each word embedding and part of first hidden layer are offline. However, this technique cannot be used for hidden layers beyond the first. In this paper, we explore a neural network architecture where the embedding layer feeds into multiple hidden layers that are placed “next to” one another so that each can be pre-computed independently. On a large scale language modeling task, this architecture achieves a 10x speedup at runtime and a significant reduction in perplexity when compared to a standard multilayer network.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing. Computational linguistics.</title>
<date>1996</date>
<contexts>
<context position="1780" citStr="Berger et al., 1996" startWordPosition="272" endWordPosition="275">chitecture achieves a 10x speedup at runtime and a significant reduction in perplexity when compared to a standard multilayer network. 1 Introduction Neural network models have become extremely popular in the last several years for a wide variety of NLP tasks, including language modeling (Schwenk, 2007), sentiment analysis (Socher et al., 2013), translation modeling (Devlin et al., 2014), and many others (Collobert et al., 2011). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy (Berger et al., 1996) or backoff models (Kneser and Ney, 1995). One popular application of neural network models in NLP is using neural network language models (NNLMs) as an additional feature in an existing machine translation (MT) or automatic speech recognition (ASR) engines. NNLMs are particularly costly in this scenario, since decoding a single sentence typically requires tens of thousands or more n-gram lookups. Although we will focus on this particular scenario in this paper, it is important to note that the techniques presented generalize to any feed-forward embedding-based neural network model. One popula</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>C¸aglar G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<publisher>CoRR.</publisher>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, C¸aglar G¨ulc¸ehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="1592" citStr="Collobert et al., 2011" startWordPosition="242" endWordPosition="245">re the embedding layer feeds into multiple hidden layers that are placed “next to” one another so that each can be pre-computed independently. On a large scale language modeling task, this architecture achieves a 10x speedup at runtime and a significant reduction in perplexity when compared to a standard multilayer network. 1 Introduction Neural network models have become extremely popular in the last several years for a wide variety of NLP tasks, including language modeling (Schwenk, 2007), sentiment analysis (Socher et al., 2013), translation modeling (Devlin et al., 2014), and many others (Collobert et al., 2011). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy (Berger et al., 1996) or backoff models (Kneser and Ney, 1995). One popular application of neural network models in NLP is using neural network language models (NNLMs) as an additional feature in an existing machine translation (MT) or automatic speech recognition (ASR) engines. NNLMs are particularly costly in this scenario, since decoding a single sentence typically requires tens of thousands or more n-gram lookups. Although we</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1550" citStr="Devlin et al., 2014" startWordPosition="235" endWordPosition="238">plore a neural network architecture where the embedding layer feeds into multiple hidden layers that are placed “next to” one another so that each can be pre-computed independently. On a large scale language modeling task, this architecture achieves a 10x speedup at runtime and a significant reduction in perplexity when compared to a standard multilayer network. 1 Introduction Neural network models have become extremely popular in the last several years for a wide variety of NLP tasks, including language modeling (Schwenk, 2007), sentiment analysis (Socher et al., 2013), translation modeling (Devlin et al., 2014), and many others (Collobert et al., 2011). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy (Berger et al., 1996) or backoff models (Kneser and Ney, 1995). One popular application of neural network models in NLP is using neural network language models (NNLMs) as an additional feature in an existing machine translation (MT) or automatic speech recognition (ASR) engines. NNLMs are particularly costly in this scenario, since decoding a single sentence typically requires tens of tho</context>
<context position="2984" citStr="Devlin et al. (2014)" startWordPosition="459" endWordPosition="462">odel. One popular technique for improving the runtime speed of NNLMs involves training the network to be “approximately normalized,” so that the softmax normalizer does not have to be computed after training. Two algorithms have been proposed to achieve this: (1) noise-contrastive estimation (NCE) (Mnih and Teh, 2012; Vaswani et al., 2013) and (2) explicit self-normalization (Devlin et al., 2014), which is used in this paper. However, even with self-normalized networks, computing the output of an intermediate hidden layer still requires a costly matrix-vector multiplication. To mitigate this, Devlin et al. (2014) made the observation that for 1-layer NNLMs, the dot product between each embedding+position pair and the first hidden layer can be pre-computed after training is complete, which allows the matrixvector multiplication to be replaced by a handful of vector additions. Using these two techniques in combination improves the runtime speed of NNLMs by several orders of magnitude with no degradation to accuracy. To understand pre-computation, first assume that we are training a NNLM that uses 250- dimensional word embeddings, a four word context window, and a 500-dimensional hidden layer. The weight</context>
<context position="10115" citStr="Devlin et al. (2014)" startWordPosition="1645" endWordPosition="1648">er two or three hidden nodes from each node in the output layer. Condition PPL 5-gram KNLM 91.1 1-Layer (k=500) 77.9 1-Layer (k=1000) 77.7 2-Stacked (k=500) 76.3 2-Stacked (k=1000) 76.2 3-Stacked (k=1000) 74.8 2-Lateral Max (k=500) 73.8 2-Lateral Mul ... 72.7 2-Lateral Add 73.7 3-Lateral Max 73.1 3-Lateral Mul 71.1 3-Lateral Add 72.3 Table 1: Perplexity of 5-gram models on the New York Times test set. k is the size of the hidden layer(s). 3.4 Runtime Speed The runtime speed of the various models is shown in Table 2. These are computed on a single core of a E5-2650 2.6 GHz CPU. Consistent with Devlin et al. (2014), we see that the baseline model achieves only 230 n-gram lookups per second (LPS) at test time, while the pre-computed, self-normalized 1-layer network achieves 600,000 LPS. Adding a second stacked layer slows this down to 24,000 LPS due to the 500 × 500 matrix-vector multiplication that must be performed. However, the lateral configuration achieves 305,000 LPS while obtaining a better perplexity than the stacked network. In comparison, the fastest backoff LM implementation, KenLM (Heafield, 2011), achieves 1-2 million lookups per second. In terms of memory usage, it is difficult to fairly co</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian J Goodfellow</author>
<author>David Warde-Farley</author>
<author>Mehdi Mirza</author>
<author>Aaron Courville</author>
<author>Yoshua Bengio</author>
</authors>
<title>Maxout networks. arXiv preprint arXiv:1302.4389.</title>
<date>2013</date>
<contexts>
<context position="5392" citStr="Goodfellow et al., 2013" startWordPosition="852" endWordPosition="855"> multi-layer networks. In this paper, we explore a network architecture where multiple hidden layers are placed “next to” one another instead of “on top of” one another, as is usually done. The output of these lateral layers are combined using an inexpensive elementwise function and fed into the output layer. Crucially, then, we can apply the pre-computation trick to each hidden layer independently, allowing for very powerful models that are orders of magnitude faster at runtime than a standard multi-layer network. Mathematically, this can be thought of as a generalization of maxout networks (Goodfellow et al., 2013), where different element-wise combination functions are explored rather than just the max function. 2 Lateral Network In a standard feed-forward embedding-based neural network, the input tokens are mapped into a continuous vector using an embedding table1, and this embedding vector is fed into the first hidden layer. The output of each hidden layer is then fed into the next hidden layer. We refer to this as the stacked architecture. For a two layer network, we can represent the output of the final hidden layer as: H = O(W2O(W1E(x))) where x is the input vector, E(x) is the output of the embed</context>
<context position="6715" citStr="Goodfellow et al., 2013" startWordPosition="1077" endWordPosition="1080">ally, H is then multiplied by an output matrix and a softmax is performed to obtain the output probabilities. In the lateral network architecture, the embedding layer is fed into two or more “side-by-side” hidden layers, and the outputs of these hidden layers are combined using an element-wise function such as maximum or multiplication. This is represented as: H = C(O(W1E(x)), O(W2E(x))) Where C is a combination function that takes two or more k-dimensional vectors as inputs and produces as k-dimensional vector as output. If C(h1, h2) = max(h1, h2) then this is equivalent to a maxout network (Goodfellow et al., 2013). To generalize this, we explore three different combination functions: 2 Cmax(h1, h2) = max(h1, h2) Cmul(h1, h2) = h1 ∗ (h2 + 1) Cadd(h1, h2) = h1 + h2 The three-or-more hidden layer versions are constructed as expected.3 A visualization is given in Figure 2. Crucially, for the lateral architecture, each hidden layer can be pre-computed independently, allowing for very fast n-gram probability lookups at runtime. 1The embeddings may or may not be trained jointly with the rest of the model. 2Note that C is an element-wise function, so these represent the operation on a single dimension of the i</context>
</contexts>
<marker>Goodfellow, Warde-Farley, Mirza, Courville, Bengio, 2013</marker>
<rawString>Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. 2013. Maxout networks. arXiv preprint arXiv:1302.4389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>Kenlm: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="10618" citStr="Heafield, 2011" startWordPosition="1725" endWordPosition="1726">n in Table 2. These are computed on a single core of a E5-2650 2.6 GHz CPU. Consistent with Devlin et al. (2014), we see that the baseline model achieves only 230 n-gram lookups per second (LPS) at test time, while the pre-computed, self-normalized 1-layer network achieves 600,000 LPS. Adding a second stacked layer slows this down to 24,000 LPS due to the 500 × 500 matrix-vector multiplication that must be performed. However, the lateral configuration achieves 305,000 LPS while obtaining a better perplexity than the stacked network. In comparison, the fastest backoff LM implementation, KenLM (Heafield, 2011), achieves 1-2 million lookups per second. In terms of memory usage, it is difficult to fairly compare backoff LMs and NNLMs because neural networks scale linearly with the vocabulary size, while backoff LMs scale linearly with the number of unique n-grams. In this case, the nonprecomputed neural network model is 25 MB, and the pre-computed 2-lateral network is 136 MB.4 The KenLM models are 1.1 GB for the Probing model and 317 MB for the Trie model. With a vocabulary of 50k, the 2-lateral network would be 425MB. In general, a pre-computed NNLM is comparable to or smaller than an equivalent bac</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. Kenlm: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Po-Sen Huang</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Alex Acero</author>
<author>Larry Heck</author>
</authors>
<title>Learning deep structured semantic models for web search using clickthrough data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management.</booktitle>
<publisher>ACM.</publisher>
<marker>Huang, He, Gao, Deng, Acero, Heck, 2013</marker>
<rawString>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<contexts>
<context position="1821" citStr="Kneser and Ney, 1995" startWordPosition="280" endWordPosition="283">time and a significant reduction in perplexity when compared to a standard multilayer network. 1 Introduction Neural network models have become extremely popular in the last several years for a wide variety of NLP tasks, including language modeling (Schwenk, 2007), sentiment analysis (Socher et al., 2013), translation modeling (Devlin et al., 2014), and many others (Collobert et al., 2011). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy (Berger et al., 1996) or backoff models (Kneser and Ney, 1995). One popular application of neural network models in NLP is using neural network language models (NNLMs) as an additional feature in an existing machine translation (MT) or automatic speech recognition (ASR) engines. NNLMs are particularly costly in this scenario, since decoding a single sentence typically requires tens of thousands or more n-gram lookups. Although we will focus on this particular scenario in this paper, it is important to note that the techniques presented generalize to any feed-forward embedding-based neural network model. One popular technique for improving the runtime spe</context>
<context position="8688" citStr="Kneser and Ney, 1995" startWordPosition="1416" endWordPosition="1419">3.2 Neural Network Training Training was performed with an in-house toolkit using stochastic gradient descent. The vocabulary is limited to 16k words so that the output layer can be trained using a basic softmax with self-normalization. All experiments use 250- dimensional word embeddings and a tanh activation function. The weights were initialized in the range [-0.05, 0.05], the batch size was 256, and the initial learning rate was 0.25. 3.3 5-gram LM Perplexity 5-gram results are shown in Table 1. The 1-layer NNLM achieves a 13.2 perplexity improvement over the Kneser-Ney smoothed baseline (Kneser and Ney, 1995). Consistent with Schwenk et al. (2014), using additional hidden layers to the stacked (standard) network results in 2.0-3.0 perplexity improvements on top of the 1-layer model. The lateral architecture significantly outperforms any of the stacked networks, achieving a 6.5 perplexity reduction over the 1-layer model. The multiplicative combination function performs better than the additive and max functions by a small margin, which suggests that it better allows for modeling complex relationships between input words. Perhaps most surprisingly, the additive function performs as well as the max </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. Assoc. for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14685" citStr="Koehn et al., 2007" startWordPosition="2387" endWordPosition="2390"> word using 5-gram target context and 7-gram source context. For evaluation, we present both the model perplexity and the BLEU score when using the model as an additional MT feature. Results are presented on a large scale EnglishGerman speech translation task. The parallel training data consists of 600M words from a variety of sources, including OPUS (Tiedemann, 2012) and a large in-house web crawl. The baseline 4-gram Kneser-Ney smoothed LM is trained on 7B words of German data. The NNLM and NNTMs are trained only on the parallel data. Our MT decoder is a proprietary engine similar to Moses (Koehn et al., 2007). The tuning set consists of 4000 utterances from conversational and newswire data, and the test set consists of 1500 sentences of collected conversational data. Results are show in Table 4. We can see that perplexity improvements are similar to what is 259 seen in the English NYT data, and that improvements in BLEU over a 1-layer model are small but consistent. There is not a significant difference in BLEU between the 2-Stacked and 2-Lateral configuration. 5 Conclusion In this paper, we explored an alternate architecture for embedding-based neural network models which allows for a fully pre-c</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. Assoc. for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2682" citStr="Mnih and Teh, 2012" startWordPosition="413" endWordPosition="416">stly in this scenario, since decoding a single sentence typically requires tens of thousands or more n-gram lookups. Although we will focus on this particular scenario in this paper, it is important to note that the techniques presented generalize to any feed-forward embedding-based neural network model. One popular technique for improving the runtime speed of NNLMs involves training the network to be “approximately normalized,” so that the softmax normalizer does not have to be computed after training. Two algorithms have been proposed to achieve this: (1) noise-contrastive estimation (NCE) (Mnih and Teh, 2012; Vaswani et al., 2013) and (2) explicit self-normalization (Devlin et al., 2014), which is used in this paper. However, even with self-normalized networks, computing the output of an intermediate hidden layer still requires a costly matrix-vector multiplication. To mitigate this, Devlin et al. (2014) made the observation that for 1-layer NNLMs, the dot product between each embedding+position pair and the first hidden layer can be pre-computed after training is complete, which allows the matrixvector multiplication to be replaced by a handful of vector additions. Using these two techniques in </context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Fethi Bougares</author>
<author>Loıc Barrault</author>
</authors>
<title>Efficient training strategies for deep neural network language models.</title>
<date>2014</date>
<booktitle>In NIPS Workshop on Deep Learning and Representation Learning.</booktitle>
<contexts>
<context position="8727" citStr="Schwenk et al. (2014)" startWordPosition="1422" endWordPosition="1425">s performed with an in-house toolkit using stochastic gradient descent. The vocabulary is limited to 16k words so that the output layer can be trained using a basic softmax with self-normalization. All experiments use 250- dimensional word embeddings and a tanh activation function. The weights were initialized in the range [-0.05, 0.05], the batch size was 256, and the initial learning rate was 0.25. 3.3 5-gram LM Perplexity 5-gram results are shown in Table 1. The 1-layer NNLM achieves a 13.2 perplexity improvement over the Kneser-Ney smoothed baseline (Kneser and Ney, 1995). Consistent with Schwenk et al. (2014), using additional hidden layers to the stacked (standard) network results in 2.0-3.0 perplexity improvements on top of the 1-layer model. The lateral architecture significantly outperforms any of the stacked networks, achieving a 6.5 perplexity reduction over the 1-layer model. The multiplicative combination function performs better than the additive and max functions by a small margin, which suggests that it better allows for modeling complex relationships between input words. Perhaps most surprisingly, the additive function performs as well as the max function, despite the fact that it prov</context>
</contexts>
<marker>Schwenk, Bougares, Barrault, 2014</marker>
<rawString>Holger Schwenk, Fethi Bougares, and Loıc Barrault. 2014. Efficient training strategies for deep neural network language models. In NIPS Workshop on Deep Learning and Representation Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language.</journal>
<contexts>
<context position="1464" citStr="Schwenk, 2007" startWordPosition="225" endWordPosition="226">echnique cannot be used for hidden layers beyond the first. In this paper, we explore a neural network architecture where the embedding layer feeds into multiple hidden layers that are placed “next to” one another so that each can be pre-computed independently. On a large scale language modeling task, this architecture achieves a 10x speedup at runtime and a significant reduction in perplexity when compared to a standard multilayer network. 1 Introduction Neural network models have become extremely popular in the last several years for a wide variety of NLP tasks, including language modeling (Schwenk, 2007), sentiment analysis (Socher et al., 2013), translation modeling (Devlin et al., 2014), and many others (Collobert et al., 2011). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy (Berger et al., 1996) or backoff models (Kneser and Ney, 1995). One popular application of neural network models in NLP is using neural network language models (NNLMs) as an additional feature in an existing machine translation (MT) or automatic speech recognition (ASR) engines. NNLMs are particularly co</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech &amp; Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1506" citStr="Socher et al., 2013" startWordPosition="229" endWordPosition="232">ayers beyond the first. In this paper, we explore a neural network architecture where the embedding layer feeds into multiple hidden layers that are placed “next to” one another so that each can be pre-computed independently. On a large scale language modeling task, this architecture achieves a 10x speedup at runtime and a significant reduction in perplexity when compared to a standard multilayer network. 1 Introduction Neural network models have become extremely popular in the last several years for a wide variety of NLP tasks, including language modeling (Schwenk, 2007), sentiment analysis (Socher et al., 2013), translation modeling (Devlin et al., 2014), and many others (Collobert et al., 2011). However, a serious drawback of neural network models is their slow speeds in training and test time (runtime) relative to alternative models such as maximum entropy (Berger et al., 1996) or backoff models (Kneser and Ney, 1995). One popular application of neural network models in NLP is using neural network language models (NNLMs) as an additional feature in an existing machine translation (MT) or automatic speech recognition (ASR) engines. NNLMs are particularly costly in this scenario, since decoding a si</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ilya Oparin</author>
<author>Jean-Luc Gauvain</author>
<author>Ben Freiberg</author>
<author>Ralf Schluter</author>
<author>Hermann Ney</author>
</authors>
<title>Comparison of feedforward and recurrent neural network language models.</title>
<date>2013</date>
<booktitle>In Proc. Conf. Acoustics, Speech and Signal Process. (ICASSP).</booktitle>
<contexts>
<context position="12800" citStr="Sundermeyer et al., 2013" startWordPosition="2086" endWordPosition="2089">ment over the 5-gram LM, the relative improvement between the 1-layer, 3- stacked, and 3-lateral systems are similar to the 5-gram scenario. Condition PPL 1-Layer (k=500) 69.8 3-Stacked (k=1000) 65.8 3-Lateral Mul (k=500) 63.4 Gated Recurrent (k=1000) 55.4 Table 3: Perplexity of 10-gram models on the New York Times test set. The Gated Recurrent model uses the full word history. As another point of comparison we report results with an gated recurrent network (Cho et al., 2014). As is consistent with the literature, the recurrent network significantly outperforms any of the feed-forward models (Sundermeyer et al., 2013). However, recurrent models have two major downsides. First, they cannot easily be integrated into existing MT/ASR engines without significantly altering the search algorithm and search Condition Test BLEU Test PPL Baseline 37.95 - +NNLM 1-Layer 38.89 138.3 +NNLM 2-Stacked 39.13 136.2 +NNLM 2-Lateral 39.15 132.3 +NNJM 1-Layer 40.71 6.33 +NNJM 2-Stacked 40.82 6.25 +NNJM 2-Lateral 40.89 6.13 Table 4: Results on English-German machine translation test set. space, since they require a fully expanded target context. Second, the matrix-vector product between the previous hidden state and the hidden </context>
</contexts>
<marker>Sundermeyer, Oparin, Gauvain, Freiberg, Schluter, Ney, 2013</marker>
<rawString>Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain, Ben Freiberg, Ralf Schluter, and Hermann Ney. 2013. Comparison of feedforward and recurrent neural network language models. In Proc. Conf. Acoustics, Speech and Signal Process. (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Parallel data, tools and interfaces in opus.</title>
<date>2012</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="14436" citStr="Tiedemann, 2012" startWordPosition="2344" endWordPosition="2345">es in a machine translation (MT) system. The first feature is a 5-gram NNLM, which used 1000 dimensions for the stacked network and 500 for the lateral network. The second feature is a neural network joint model (NNJM), which predicts each target word using 5-gram target context and 7-gram source context. For evaluation, we present both the model perplexity and the BLEU score when using the model as an additional MT feature. Results are presented on a large scale EnglishGerman speech translation task. The parallel training data consists of 600M words from a variety of sources, including OPUS (Tiedemann, 2012) and a large in-house web crawl. The baseline 4-gram Kneser-Ney smoothed LM is trained on 7B words of German data. The NNLM and NNTMs are trained only on the parallel data. Our MT decoder is a proprietary engine similar to Moses (Koehn et al., 2007). The tuning set consists of 4000 utterances from conversational and newswire data, and the test set consists of 1500 sentences of collected conversational data. Results are show in Table 4. We can see that perplexity improvements are similar to what is 259 seen in the English NYT data, and that improvements in BLEU over a 1-layer model are small bu</context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>J¨org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with large-scale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2705" citStr="Vaswani et al., 2013" startWordPosition="417" endWordPosition="420">o, since decoding a single sentence typically requires tens of thousands or more n-gram lookups. Although we will focus on this particular scenario in this paper, it is important to note that the techniques presented generalize to any feed-forward embedding-based neural network model. One popular technique for improving the runtime speed of NNLMs involves training the network to be “approximately normalized,” so that the softmax normalizer does not have to be computed after training. Two algorithms have been proposed to achieve this: (1) noise-contrastive estimation (NCE) (Mnih and Teh, 2012; Vaswani et al., 2013) and (2) explicit self-normalization (Devlin et al., 2014), which is used in this paper. However, even with self-normalized networks, computing the output of an intermediate hidden layer still requires a costly matrix-vector multiplication. To mitigate this, Devlin et al. (2014) made the observation that for 1-layer NNLMs, the dot product between each embedding+position pair and the first hidden layer can be pre-computed after training is complete, which allows the matrixvector multiplication to be replaced by a handful of vector additions. Using these two techniques in combination improves th</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
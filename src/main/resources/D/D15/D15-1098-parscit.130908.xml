<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008652">
<title confidence="0.97466">
Component-Enhanced Chinese Character Embeddings
</title>
<author confidence="0.99963">
Yanran Li1, Wenjie Li1, Fei Sun2, and Sujian Li3
</author>
<affiliation confidence="0.997111">
1Department of Computing, The Hong Kong Polytechnic University, Hong Kong
2Institute of Computing Technology, Chinese Academy of Sciences, China
3Key Laboratory of Computational Linguistics, Peking University, MOE, China
</affiliation>
<email confidence="0.933587">
{csyli, cswjli}@comp.polyu.edu.hk, ofey.sunfei@gmail.com,
lisujian@pku.edu.cn
</email>
<sectionHeader confidence="0.99731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999664733333333">
Distributed word representations are very
useful for capturing semantic information
and have been successfully applied in a
variety of NLP tasks, especially on En-
glish. In this work, we innovatively de-
velop two component-enhanced Chinese
character embedding models and their bi-
gram extensions. Distinguished from En-
glish word embeddings, our models ex-
plore the compositions of Chinese char-
acters, which often serve as semantic in-
dictors inherently. The evaluations on
both word similarity and text classification
demonstrate the effectiveness of our mod-
els.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991293442623">
Due to its advantage over traditional one-hot rep-
resentation, distributed word representation has
demonstrated its benefit for semantic representa-
tion in various NLP tasks. Among the existing ap-
proaches (Huang et al, 2012; Levy and Goldberg,
2014; Yang and Eisenstein, 2015), the continu-
ous bag-of-words model (CBOW) and the continu-
ous skip-gram model (SkipGram) remain the most
popular ones that one can use to build word embed-
dings efficiently (Mikolov et al, 2013a; Mikolov
et al, 2013b). These two models learn the dis-
tributed representation of a word based on its con-
text. The context defined by the window of sur-
rounding words may unavoidably include certain
less semantically-relevant words and/or miss the
words with important and relevant meanings (Levy
and Goldberg, 2014).
To overcome this shortcoming, a line of research
deploys the order information of the words in the
contexts by either deriving the contexts using de-
pendency relations where the target word partici-
pates (Levy and Goldberg, 2014; Yu and Dredze,
2014; Bansal et al, 2014) or directly keeping the
order features (Ling et al, 2015). As to another
line, Luong et al (2013) captures morphological
composition by using neural networks and Qiu
et al (2014) introduces the morphological knowl-
edge as both additional input representation and
auxiliary supervision to the neural network frame-
work. While most previous work focuses on En-
glish, there is a little work on Chinese. Zhang et
al (2013) extracts the syntactical morphemes and
Cheng et al (2014) incorporates the POS tags and
dependency relations. Basically, the work in Chi-
nese follows the same ideas as in English.
Distinguished from English, Chinese characters
are logograms, of which over 80% are phono-
semantic compounds, with a semantic component
giving a broad category of meaning and a phonetic
component suggesting the sound1. For example,
the semantic component 亻 (human) of the Chi-
nese character 他 (he) provides the meaning con-
nected with human. In fact, the components of
most Chinese characters inherently bring with cer-
tain levels of semantics regardless of the contexts.
Being aware that the components of Chinese char-
acters are finer grained semantic units, then an im-
portant question arises before slipping to the appli-
cations of word embeddings—would it be better to
learn the semantic representations from the charac-
ter components in Chinese?
We approach this question from both the prac-
tical and the cognitive points of view. In prac-
tice, we expect the representations to be optimized
for good generalization. As analyzed before, the
components are more generic unit inside Chinese
characters that provides semantics. Such inher-
ent information somehow alleviates the shortcom-
ing of the external contexts. From the cognitive
point of view, it has been found that the knowl-
edge of semantic components significantly corre-
</bodyText>
<footnote confidence="0.935992">
1http://en.wikipedia.org/wiki/Radical_
(Chinese_characters)
</footnote>
<page confidence="0.953113">
829
</page>
<note confidence="0.6751415">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 829–834,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.996251730769231">
late to Chinese word reading and sentence compre-
hension (Ho et al, 2003).
These evidences inspire us to explore novel
Chinese character embedding models. Different
from word embeddings, character embeddings re-
late Chinese characters that occur in similar con-
texts with their component information. Chinese
characters convey the meanings from their compo-
nents, and beyond that, the meanings of most Chi-
nese words also take roots in their composite char-
acters. For example, the meaning of the Chinese
word I- (cradle) can be interpreted in terms of
its composite characters (sway) and I- (basket).
Considering this, we further extend character em-
beddings from uni-gram models to bi-gram mod-
els.
At the core of our work is the exploration of
Chinese semantic representations from a novel
character-based perspective. Our proposed Chi-
nese character embeddings incorporate the finer-
grained semantics from the components of char-
acters and in turn enrich the representations inher-
ently in addition to utilizing the external contexts.
The evaluations on both intrinsic word similarity
and extrinsic text classification demonstrate the ef-
fectiveness and potentials of the new models.
</bodyText>
<sectionHeader confidence="0.9727935" genericHeader="method">
2 Component-Enhanced Character
Embeddings
</sectionHeader>
<bodyText confidence="0.99988505">
Chinese characters are often composed of smaller
and primitive components called radicals or
radical-like components, which serve as the most
basic units for building character meanings. Dat-
ing back to the 2nd century AD, the Han dynasty
scholar Shen XU organizes his etymological dic-
tionary shuō wén jiě zì (word and expression) by
selecting 540 recurring graphic components that
he called bù (means “categories”). Bù is nearly
the same as what we call radicals today2. Most
radicals are common semantic components. Over
time, some original radicals evolve into radical-
like components. Nowadays, a Chinese character
often contains exactly one radical (rarely has two)
and several other radical-like components. In what
follows, we refer to as components both radicals
and radical-like components.
Distinguished from English, these composite
components are unique and inherent features in-
side Chinese characters. A lot of times, they allow
</bodyText>
<footnote confidence="0.510904">
2http://en.wikipedia.org/wiki/Radical_
(Chinese_characters)
</footnote>
<bodyText confidence="0.999897877551021">
us to assumingly understand or infer the meanings
of characters without any context. In other words,
the component-level features inherently bring with
additional information that benefits semantic rep-
resentations of characters. For example, we know
that the characters fj&apos;. (you), 4-A (he), tk (compan-
ion), Ia (companion), and C (people) all have the
meanings related to human because of their shared
component 4 (human), a variant of the Chinese
character A (human). This kind of component in-
formation is intrinsically different from the con-
texts deriving by dependency relations and POS
tags. It motivates us to investigate the component-
enhanced Chinese character embedding models.
While Sun et al (2014) utilizes radical information
in a supervised fashion, we build our models in a
holistic unsupervised and bottom-up way.
It is important to note the variation of a radical
inside a character. There are two types of varia-
tions. The main type is position-related. For ex-
ample, the radical of the Chinese character 71&lt;. (wa-
ter) is itself, but it becomes % as the radical of
� (pool). The original radicals are stretched or
squeezed so that they can fit into the general Chi-
nese character shape of a square. The second vari-
ation type emerges along with the history of char-
acter simplification when traditional characters are
converted into simplified characters. For instance,
* (eat) is written as P when it forms as a part
of some traditional characters, but is written as &apos;i
in simplified characters. To cope with these vari-
ations and recover the semantics, we match all the
radical variants back into their original forms. We
extract all the components to build a component
list for each Chinese character. With the assump-
tion that a character’s radical often bring more im-
portant semantics than the rest3, we regard the rad-
ical of a character as the first component in its com-
ponent list.
Let a sequence of characters D = {zi, ... , zNI
denotes a corpus of N characters over the char-
acter vocabulary V . And z, c, e, K, T, M, IV I de-
note the Chinese character, the context charac-
ter, the component list, the corresponding em-
bedding dimension, the context window size, the
number of components taken into account for
each character, and the vocabulary size, respec-
tively. We develop two component-enhanced
character embedding models, namely charCBOW
</bodyText>
<footnote confidence="0.737605666666667">
3Inside a character, its radical often serves as the semantic-
component while its other radical-like components may be
phonetics.
</footnote>
<page confidence="0.99025">
830
</page>
<bodyText confidence="0.999411">
and charSkipGram.
charCBOW follows the original continuous
bag-of-words model (CBOW) proposed by
(Mikolov et al, 2013a). We predict the central
character zi conditioned on a 2(M+1)TK-
dimensional vector that is the concatena-
tion of the remaining character-level contexts
(ci−T, ... , ci−1, ci+1, . . . , ci+T) and the compo-
nents in their component lists. More formally,
we wish to maximize the log likelihood of all the
characters as follows,
</bodyText>
<figure confidence="0.987293333333333">
INPUT PROJECTION OUTPUT
ci−T
ei−T
ci−T+1
ei−T+1
O
ci−T
ci−T+1
... zi
ci+T−1
ei+T−1
ci+T
ei+T
ci+T−1
ci+T
</figure>
<equation confidence="0.972344333333333">
∑L = logp(zi|hi), (a) charCBOW
zni ∈D INPUT PROJECTION OUTPUT
hi = cat(ci−T, ei−T, ... , ci+T, ei+T )
</equation>
<bodyText confidence="0.999835461538461">
where hi denotes the concatenation of the
component-enhanced contexts. We make predic-
tion using a 2KT (M+1)|V |-dimensional matrix
O. Different from the original CBOW model,
the extra parameter introduced in the matrix O
allows us to maintain the relative order of the
components and treat the radical differently from
the rest components.
The development of charSkipGram is straight-
forward. We derive the component-enhanced con-
texts as ((ci−T, ei−T), . . . , (ci+T, ei+T)) based on
the central character zi. The sum of log probabili-
ties given zi is maximized:
</bodyText>
<equation confidence="0.932556">
( )
log p(cj+i|zi) + log p(ej+i|zi)
</equation>
<bodyText confidence="0.961825125">
Figure 1 illustrates the two component-
enhanced character embedding models. It is easy
to extend charCBOW and charSkipGram to their
corresponding bi-character extensions. Denote
the zi, ci and ei in charCBOW and charSkipGram
as uni-character zui, cui and eui, the bi-character
extensions are the models fed by bi-character
formed zbi, cbi and ebi.
</bodyText>
<sectionHeader confidence="0.99922" genericHeader="method">
3 Evaluations
</sectionHeader>
<bodyText confidence="0.999012444444445">
We examine the quality of the proposed two Chi-
nese character embedding models as well as their
corresponding extensions on both intrinsic word
similarity evaluation and extrinsic text classifica-
tion evaluation.
Word Similarity. As the widely used public
word similarity datasets like WS-353 (Finkelstein
et al, 2001), RG-65 (Rubsenstein and Goode-
nough, 1965) are built for English embeddings,
</bodyText>
<table confidence="0.548377">
zi ci−T
ei−T
ci−T+1
ei−T+1
ci+T−1
ei+T−1
ci+T
ei+T
</table>
<figure confidence="0.982666">
(b) charSkipGram
</figure>
<figureCaption confidence="0.993723">
Figure 1: Illustrations of two component-
enhanced character embedding models.
</figureCaption>
<bodyText confidence="0.999654823529412">
we start from developing appropriate Chinese syn-
onym sets. Two candidate choices are Chinese
dictionaries HowNet (Dong and Dong, 2006) and
HIT-CIR’s Extended Tongyici Cilin (denoted as
E-TC)4. As HowNet contains less modern words,
such as 谷 歌 (Google), we select E-TC as our
benchmark for word similarity evaluation.
Text Classification. We use Tencent news ti-
tles as our text classification dataset5. A total of
8,826 titles of four categories (society, entertain-
ment, healthcare, and military) are extracted. The
lengths of titles range from 10 to 20 words. We
train ℓ2-regularized logistic regression classifiers
using the LIBLINEAR package (Fan et al, 2008)
with the learned embeddings.
To build the component-enhanced character em-
beddings, we employ the GB2312 character set
</bodyText>
<footnote confidence="0.969271666666667">
4http://ir.hit.edu.cn/demo/ltp/Sharing_
Plan.htm
5http://www.datatang.com/data/44341
</footnote>
<equation confidence="0.956533">
∑L = ∑T
zi∈D j=−T
j̸=�
</equation>
<page confidence="0.997413">
831
</page>
<tableCaption confidence="0.998881">
Table 1: Word Similarity Results of Embedding Models
</tableCaption>
<table confidence="0.9999707">
Model Spearman’s rank correlation (%)
A B C D E F G H I J K L
CBOW 33.2 25.2 32.2 27.8 36.5 37.6 43.2 40.2 37.3 39.5 44.2 40.4
SkipGram 35.9 26.7 33.8 29.9 36.6 40.2 45.3 44.3 39.0 41.2 46.9 43.0
charCBOW 34.0 23.2 34.1 26.7 37.8 49.2 48.1 44.5 40.2 42.0 48.0 43.2
charSkipGram 33.8 22.6 33.1 25.2 37.2 47.5 48.0 43.0 38.8 40.9 46.5 41.8
CBOW-bi 37.0 27.8 34.2 29.2 38.1 43.2 50.3 48.2 43.5 46.3 50.9 45.2
SkipGram-bi 38.2 29.0 34.0 29.4 38.9 44.9 50.2 49.3 45.6 48.4 51.3 47.4
charCBOW-bi 36.0 25.3 36.8 31.2 40.2 54.3 55.7 49.7 45.3 48.9 53.2 47.7
charSkipGram-bi 35.7 24.6 33.4 30.5 39.7 53.3 53.9 48.2 33.2 47.1 52.0 45.7
</table>
<tableCaption confidence="0.923122">
Table 2: Text Classification Results of Embedding Models
</tableCaption>
<table confidence="0.9922681">
Society Entertainment Healthcare Military
P R F P R F P R F P R F
43.0 28.0 33.9 48.2 32.7 39.0 47.6 29.5 36.4 57.6 40.8 47.8
47.2 31.1 37.5 49.8 34.0 40.4 48.4 32.7 39.0 58.8 42.3 49.2
57.4 37.4 45.2 62.2 42.0 50.1 59.2 45.3 51.3 70.3 51.0 59.1
50.3 34.6 41.0 57.6 34.5 43.2 57.3 42.5 48.8 67.8 48.3 56.4
46.2 29.0 35.6 50.3 35.0 41.3 51.0 33.6 40.5 62.2 45.7 52.7
50.9 34.6 41.2 51.4 37.2 43.2 52.1 35.6 42.3 62.1 49.0 54.8
62.2 39.8 48.5 66.7 46.6 54.9 62.2 50.2 55.6 74.4 53.8 62.4
54.4 38.2 44.9 59.2 36.5 45.2 62.0 47.9 54.0 73.4 53.5 61.9
</table>
<figure confidence="0.793428888888889">
Model
CBOW-bi
SkipGram-bi
charCBOW-bi
charSkipGram-bi
CBOW-combine
SkipGram-combine
charCBOW-combine
charSkipGram-combine
</figure>
<bodyText confidence="0.996603523809524">
and extract all their component lists. It is easy
to obtain the first components (i.e., the radicals),
as they are readily available in the online Xinhua
Dictionary6. For the rest radical-like components,
we extract them by matching the patterns like “�k
(from)+X” in the Xinhua dictionary. Such a pat-
tern indicates that a character has a component of
X. We also enrich the component lists by matching
the pattern “X is only seen” in Hong Kong Com-
puter Chinese Basic Component Reference7 .
It is observed that nearly 65% Chinese charac-
ters have only one component (their radicals), and
95% Chinese characters have two components (in-
cluding their radicals). Thus, we decide to main-
tain up to two extracted components to build the
character embeddings according to the frequency
of their occurrences. To cope with the radical vari-
ation problem, we transform 24 radical variants to
their origins, such as 4 to A (human), -f to T-
(hand), % to 71&lt;. (water) and 3- to t (foot). The
complete list of the transformations is provided in
</bodyText>
<footnote confidence="0.976271333333333">
6http://xh.5156edu.com/
7http://www.ogcio.gov.hk/tc/business/tech_
promotion/ccli/cliac/glyphs_guidelines.htm
</footnote>
<bodyText confidence="0.998493045454546">
Appendix for easy reference.
We adopt Chinese Wikipedia Dump8 to train
our models as well as the original CBOW and
SkipGram, implemented in the Word2Vec tool9 for
comparison. The corpus in total contains 232,894
articles. In preprocessing, we remove pure digit
words and non-Chinese characters, and ignore the
words less than 10 occurrences during training.
We set the context window size T as 2 and use 5
negative samples experimentally. All the embed-
ding dimensions K are set to 50.
In the word similarity evaluation, we compute
the Spearman’s rank correlation (Myers and Well,
1995) between the similarity scores based on the
learned embedding models and the E-TC similar-
ity scores computed by following Tian and Zhao
(2010). The bi-character embeddings are concate-
nation of the composite character embeddings. For
the text classification evaluation, we average the
composite single character embeddings for each
bi-gram. And each bi-gram overlaps with the pre-
vious one. The titles are represented by averaging
</bodyText>
<footnote confidence="0.9999375">
8http://download.wikipedia.com/zhwiki/
9https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.994814">
832
</page>
<bodyText confidence="0.982303218181818">
the embeddings of their composite grams10.
Table 1 presents the word similarity evaluation
results of the eight embedding models mentioned
above, where A–L denote the twelve categories in
E-TC. The first four rows are the results with the
uni-character inputs, and the last four rows corre-
spond to the bi-character embeddings results.
We can see that both CBOW and CBOW-bi per-
form worse than the corresponding SkipGram and
SkipGram-bi. This result is consistent with the
finding in the previous work (Pennington et al,
2014; Levy and Goldberg, 2014; Levy et al, 2015).
To some extent, CBOW and its extension CBOW-
bi are the most different among the eight (the first
four models in Table 1 and the first four models in
Table 2). They tie together the characters in each
context window by representing the context vector
as the sum of their characters’ vectors. Although
they have a potential of deriving better representa-
tions (Levy et al, 2015), they lose some particular
information from each unit of input in the average
operations.
Although the performance on twelve differ-
ent categories varies, in overall charCBOW,
charSkipGram and their extensions consistently
better correlate to E-TC. It provides the evidence
that the component information in Chinese char-
acters is of significance. Clearly, the bi-character
models achieve higher rank correlations. These re-
sults are not surprised. As a matter of fact, a major-
ity of Chinese words are compounds of two charac-
ters. Thus, in many cases two characters together
is equivalent to a Chinese word. Considering the
superiority of the bi-character models, we only ap-
ply them in the text classification evaluations.
The results shown in the first four rows of Ta-
ble 2 are similar to those in the word similarity
evaluation. Please notice the significant improve-
ment of charCBOW and charCBOW-bi. We con-
jecture this as a hint of the importance of the or-
der information, which is introduced by the extra
parameter in the output matrixes. Their better per-
formances verify our assumption that the radicals
are more important than non-radicals. This is also
attributed to the benefit from the order of the char-
acters in the contexts.
Actually, we also conduct an additional experi-
ment to combine the uni-gram and the bi-gram em-
beddings for text classification and notice in aver-
10We do not compare the uni-formed characters with bi-
formed compound characters. The word pairs that cannot be
found in the vocabulary are removed.
age about 8.4% of gain over the bi-gram embed-
dings alone. The detailed results are presented in
the last four rows of Table 2.
</bodyText>
<sectionHeader confidence="0.99054" genericHeader="method">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999975">
In this paper, we propose two component-
enhanced Chinese character embedding models
and their extensions to explore both the internal
compositions and the external contexts of Chinese
characters. Experimental results demonstrate their
benefits in learning rich semantic representations.
For the future work, we plan to devise embed-
ding models based together on the composition of
component-character and of character-word. The
two types of compositions will serve in a coordi-
nate fashion for the distributional representations.
</bodyText>
<sectionHeader confidence="0.998379" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999528666666667">
The work described in this paper was supported
by the grants from the Research Grants Coun-
cil of Hong Kong (PolyU 5202/12E and PolyU
152094/14E), the grants from the National Natu-
ral Science Foundation of China (61272291 and
61273278) and a PolyU internal grant (4-BCB5).
</bodyText>
<sectionHeader confidence="0.992996" genericHeader="conclusions">
Appendix
</sectionHeader>
<bodyText confidence="0.999924">
As mentioned in Section 3, we present the com-
plete list of transformations of the variant and orig-
inal forms of 24 radicals. The meaning columns
provide the corresponding meanings of the com-
ponents in the left.
</bodyText>
<table confidence="0.987783769230769">
transform meaning transform meaning
4J PT grass -f T- hand
4 A human � ,* water
11 A knife * 4-�&apos; vehicle
4 A dog YC -1 i V_ hit
,,,, k fire t . silk
1� � gold � , Z old
� ,2 t- wheat I + cattle
&apos;i * eat f * eat
4, T memory t JLN heart
&apos;EV M nest T_ _TI jade
� , speak ,� A cloth
A �] body 3— t walk
</table>
<page confidence="0.997372">
833
</page>
<sectionHeader confidence="0.996159" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999450692307692">
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.
Tailoring continuous word representations for de-
pendency parsing. In Proc. ofACL.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent,
et al. 2003. A neural probabilistic language
model. The Journal ofMachine Learning Research,
3: 1137-1155.
Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. of EMNLP, pages 740–750.
Fei Cheng, Kevin Duh, Yuji Matsumoto. 2014. Pars-
ing Chinese Synthetic Words with a Character-based
Dependency Model. LREC.
Ronan Collobert, Jason Weston, L´eon Bottou, et al.
2011. Natural language processing (almost) from
scratch. JMLR, 12.
Zhendong Dong and Qiang Dong. 2006. HowNet and
the Computation of Meaning. World Scientific Pub-
lishing Co. Pte. Ltd., Singapore.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, et al.
2008. LIBLINEAR: A library for large linear classi-
fication. The Journal ofMachine Learning Research,
9: 1871-1874.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, et
al. 2001. Placing search in context: the concept re-
visited. In Proc. of WWW.
Connie Suk-Han Ho, Ting-Ting Ng, and Wing-Kin
Ng. 2003. A “radical” approach to reading develop-
ment in Chinese: The role of semantic radicals and
phonetic radicals. In Journal of Literacy Research,
35(3), 849-878.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proc. ofACL.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, et al. 2014. A dependency parser
for tweets. In Proc. of EMNLP, pages 1001–1012,
Doha, Qatar, October.
Remi Lebret, Jo ´el Legrand, and Ronan Collobert.
2013. Is deep learning really necessary for word em-
beddings? In Proc. ofNIPS.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proc. ofACL.
Omer Levy, Yoav Goldberg, And Ido Dagan. 2015.
Improving Distributional Similarity with Lessons
Learned from Word Embeddings. In Proc. of TACL.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proc. of ACL, pages 1491–
1500.
Minh-Thang Luong, Richard Socher, and Christopher
D. Manning. 2013. Better Word Representations
with Recursive Neural Networks for Morphology. In
Proc. of CoNLL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
S. Corrado, and Jeffrey Dean. 2013b. Distributed
representations of words and phrases and their
composition-ality. In Advances in Neural Informa-
tion Processing Systems. pages 3111-3119.
Jerome L. Myers and Arnold D. Well. 1995. Research
Design &amp; Statistical Analysis. Routledge.
Jeffrey Pennington, Richard Socher, and Christopher
D. Manning. Glove: Global Vectors for Word Rep-
resentation. In Proc. ofEMNLP.
Siyu Qiu, Qing Cui, Jiang Bian, and et al. 2014. Co-
learning of Word Representations and Morpheme
Representations. In Proc. of COLING.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.
Yaming Sun, Lei Lin, Duyu Tang, et al. 2014. Radical-
Enhanced Chinese Character Embedding. CoRR abs/
1404.4714.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 3104–3112.
Duyu Tang, Furu Wei, Nan Yang, et al. 2014. Learning
sentiment-specific word embedding for twitter sen-
timent classification. In Proc. ofACL.
Jiu-le Tian and Wei Zhao. 2010. Words Similarity Al-
gorithm Based on Tongyici Cilin in Semantic Web
Adaptive Learning System.
Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015. Two/too simple adaptations of word2vec
for syntax problems. In Proc. of NAACL, Denver,
CO.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proc. of Computation and Lan-
guage.
Yi Yang and Jacob Eisenstein. 2015. Unsupervised
multi-domain adaptation with feature embeddings.
In Proc. ofNAACL-HIT.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proc. ofACL.
Meishan Zhang, Yue Zhang, Wan Xiang Che, and et al.
2013. Chinese parsing exploiting characters. In Proc.
ofACL.
</reference>
<page confidence="0.998706">
834
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.420139">
<title confidence="0.995023">Component-Enhanced Chinese Character Embeddings</title>
<author confidence="0.994199">Wenjie Fei</author>
<affiliation confidence="0.954810333333333">of Computing, The Hong Kong Polytechnic University, Hong of Computing Technology, Chinese Academy of Sciences, Laboratory of Computational Linguistics, Peking University, MOE,</affiliation>
<email confidence="0.8006075">csyli@comp.polyu.edu.hk,lisujian@pku.edu.cn</email>
<email confidence="0.8006075">cswjli@comp.polyu.edu.hk,lisujian@pku.edu.cn</email>
<abstract confidence="0.98418575">Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing. In</title>
<date>2014</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="2030" citStr="Bansal et al, 2014" startWordPosition="295" endWordPosition="298"> efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly keeping the order features (Ling et al, 2015). As to another line, Luong et al (2013) captures morphological composition by using neural networks and Qiu et al (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al (2013) extracts the syntactical morphemes and Cheng et al (2014) incorporates the POS tags and dependency relations. Basically, the work in Chinese follows the same ideas as in English. Dist</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Réjean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal ofMachine Learning Research,</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<marker>Bengio, Ducharme, Vincent, 2003</marker>
<rawString>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, et al. 2003. A neural probabilistic language model. The Journal ofMachine Learning Research, 3: 1137-1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>740--750</pages>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In Proc. of EMNLP, pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Cheng</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Parsing Chinese Synthetic Words with a Character-based Dependency Model.</title>
<date>2014</date>
<publisher>LREC.</publisher>
<contexts>
<context position="2504" citStr="Cheng et al (2014)" startWordPosition="372" endWordPosition="375">ng the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly keeping the order features (Ling et al, 2015). As to another line, Luong et al (2013) captures morphological composition by using neural networks and Qiu et al (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al (2013) extracts the syntactical morphemes and Cheng et al (2014) incorporates the POS tags and dependency relations. Basically, the work in Chinese follows the same ideas as in English. Distinguished from English, Chinese characters are logograms, of which over 80% are phonosemantic compounds, with a semantic component giving a broad category of meaning and a phonetic component suggesting the sound1. For example, the semantic component 亻 (human) of the Chinese character 他 (he) provides the meaning connected with human. In fact, the components of most Chinese characters inherently bring with certain levels of semantics regardless of the contexts. Being awar</context>
</contexts>
<marker>Cheng, Duh, Matsumoto, 2014</marker>
<rawString>Fei Cheng, Kevin Duh, Yuji Matsumoto. 2014. Parsing Chinese Synthetic Words with a Character-based Dependency Model. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>JMLR,</journal>
<volume>12</volume>
<marker>Collobert, Weston, Bottou, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, et al. 2011. Natural language processing (almost) from scratch. JMLR, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
</authors>
<title>HowNet and the Computation of Meaning. World Scientific Publishing Co.</title>
<date>2006</date>
<publisher>Pte. Ltd.,</publisher>
<contexts>
<context position="11088" citStr="Dong and Dong, 2006" startWordPosition="1713" endWordPosition="1716">haracter embedding models as well as their corresponding extensions on both intrinsic word similarity evaluation and extrinsic text classification evaluation. Word Similarity. As the widely used public word similarity datasets like WS-353 (Finkelstein et al, 2001), RG-65 (Rubsenstein and Goodenough, 1965) are built for English embeddings, zi ci−T ei−T ci−T+1 ei−T+1 ci+T−1 ei+T−1 ci+T ei+T (b) charSkipGram Figure 1: Illustrations of two componentenhanced character embedding models. we start from developing appropriate Chinese synonym sets. Two candidate choices are Chinese dictionaries HowNet (Dong and Dong, 2006) and HIT-CIR’s Extended Tongyici Cilin (denoted as E-TC)4. As HowNet contains less modern words, such as 谷 歌 (Google), we select E-TC as our benchmark for word similarity evaluation. Text Classification. We use Tencent news titles as our text classification dataset5. A total of 8,826 titles of four categories (society, entertainment, healthcare, and military) are extracted. The lengths of titles range from 10 to 20 words. We train ℓ2-regularized logistic regression classifiers using the LIBLINEAR package (Fan et al, 2008) with the learned embeddings. To build the component-enhanced character e</context>
</contexts>
<marker>Dong, Dong, 2006</marker>
<rawString>Zhendong Dong and Qiang Dong. 2006. HowNet and the Computation of Meaning. World Scientific Publishing Co. Pte. Ltd., Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal ofMachine Learning Research,</journal>
<volume>9</volume>
<pages>1871--1874</pages>
<contexts>
<context position="11615" citStr="Fan et al, 2008" startWordPosition="1795" endWordPosition="1798">synonym sets. Two candidate choices are Chinese dictionaries HowNet (Dong and Dong, 2006) and HIT-CIR’s Extended Tongyici Cilin (denoted as E-TC)4. As HowNet contains less modern words, such as 谷 歌 (Google), we select E-TC as our benchmark for word similarity evaluation. Text Classification. We use Tencent news titles as our text classification dataset5. A total of 8,826 titles of four categories (society, entertainment, healthcare, and military) are extracted. The lengths of titles range from 10 to 20 words. We train ℓ2-regularized logistic regression classifiers using the LIBLINEAR package (Fan et al, 2008) with the learned embeddings. To build the component-enhanced character embeddings, we employ the GB2312 character set 4http://ir.hit.edu.cn/demo/ltp/Sharing_ Plan.htm 5http://www.datatang.com/data/44341 ∑L = ∑T zi∈D j=−T j̸=� 831 Table 1: Word Similarity Results of Embedding Models Model Spearman’s rank correlation (%) A B C D E F G H I J K L CBOW 33.2 25.2 32.2 27.8 36.5 37.6 43.2 40.2 37.3 39.5 44.2 40.4 SkipGram 35.9 26.7 33.8 29.9 36.6 40.2 45.3 44.3 39.0 41.2 46.9 43.0 charCBOW 34.0 23.2 34.1 26.7 37.8 49.2 48.1 44.5 40.2 42.0 48.0 43.2 charSkipGram 33.8 22.6 33.1 25.2 37.2 47.5 48.0 43.</context>
</contexts>
<marker>Fan, Chang, Hsieh, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, et al. 2008. LIBLINEAR: A library for large linear classification. The Journal ofMachine Learning Research, 9: 1871-1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2001</date>
<booktitle>In Proc. of WWW.</booktitle>
<contexts>
<context position="10732" citStr="Finkelstein et al, 2001" startWordPosition="1661" endWordPosition="1664">ter embedding models. It is easy to extend charCBOW and charSkipGram to their corresponding bi-character extensions. Denote the zi, ci and ei in charCBOW and charSkipGram as uni-character zui, cui and eui, the bi-character extensions are the models fed by bi-character formed zbi, cbi and ebi. 3 Evaluations We examine the quality of the proposed two Chinese character embedding models as well as their corresponding extensions on both intrinsic word similarity evaluation and extrinsic text classification evaluation. Word Similarity. As the widely used public word similarity datasets like WS-353 (Finkelstein et al, 2001), RG-65 (Rubsenstein and Goodenough, 1965) are built for English embeddings, zi ci−T ei−T ci−T+1 ei−T+1 ci+T−1 ei+T−1 ci+T ei+T (b) charSkipGram Figure 1: Illustrations of two componentenhanced character embedding models. we start from developing appropriate Chinese synonym sets. Two candidate choices are Chinese dictionaries HowNet (Dong and Dong, 2006) and HIT-CIR’s Extended Tongyici Cilin (denoted as E-TC)4. As HowNet contains less modern words, such as 谷 歌 (Google), we select E-TC as our benchmark for word similarity evaluation. Text Classification. We use Tencent news titles as our text c</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, et al. 2001. Placing search in context: the concept revisited. In Proc. of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Connie Suk-Han Ho</author>
<author>Ting-Ting Ng</author>
<author>Wing-Kin Ng</author>
</authors>
<title>A “radical” approach to reading development in Chinese: The role of semantic radicals and phonetic radicals.</title>
<date>2003</date>
<journal>In Journal of Literacy Research,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>849--878</pages>
<contexts>
<context position="4175" citStr="Ho et al, 2003" startWordPosition="624" endWordPosition="627">e components are more generic unit inside Chinese characters that provides semantics. Such inherent information somehow alleviates the shortcoming of the external contexts. From the cognitive point of view, it has been found that the knowledge of semantic components significantly corre1http://en.wikipedia.org/wiki/Radical_ (Chinese_characters) 829 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 829–834, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. late to Chinese word reading and sentence comprehension (Ho et al, 2003). These evidences inspire us to explore novel Chinese character embedding models. Different from word embeddings, character embeddings relate Chinese characters that occur in similar contexts with their component information. Chinese characters convey the meanings from their components, and beyond that, the meanings of most Chinese words also take roots in their composite characters. For example, the meaning of the Chinese word I- (cradle) can be interpreted in terms of its composite characters (sway) and I- (basket). Considering this, we further extend character embeddings from uni-gram model</context>
</contexts>
<marker>Ho, Ng, Ng, 2003</marker>
<rawString>Connie Suk-Han Ho, Ting-Ting Ng, and Wing-Kin Ng. 2003. A “radical” approach to reading development in Chinese: The role of semantic radicals and phonetic radicals. In Journal of Literacy Research, 35(3), 849-878.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes. In</title>
<date>2012</date>
<booktitle>Proc. ofACL.</booktitle>
<contexts>
<context position="1199" citStr="Huang et al, 2012" startWordPosition="160" endWordPosition="163">work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line </context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1001--1012</pages>
<location>Doha, Qatar,</location>
<marker>Kong, Schneider, Swayamdipta, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, et al. 2014. A dependency parser for tweets. In Proc. of EMNLP, pages 1001–1012, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remi Lebret</author>
<author>Jo ´el Legrand</author>
<author>Ronan Collobert</author>
</authors>
<title>Is deep learning really necessary for word embeddings? In</title>
<date>2013</date>
<booktitle>Proc. ofNIPS.</booktitle>
<marker>Lebret, ´el Legrand, Collobert, 2013</marker>
<rawString>Remi Lebret, Jo ´el Legrand, and Ronan Collobert. 2013. Is deep learning really necessary for word embeddings? In Proc. ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="1224" citStr="Levy and Goldberg, 2014" startWordPosition="164" endWordPosition="167">ly develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the o</context>
<context position="16029" citStr="Levy and Goldberg, 2014" startWordPosition="2509" endWordPosition="2512">://download.wikipedia.com/zhwiki/ 9https://code.google.com/p/word2vec/ 832 the embeddings of their composite grams10. Table 1 presents the word similarity evaluation results of the eight embedding models mentioned above, where A–L denote the twelve categories in E-TC. The first four rows are the results with the uni-character inputs, and the last four rows correspond to the bi-character embeddings results. We can see that both CBOW and CBOW-bi perform worse than the corresponding SkipGram and SkipGram-bi. This result is consistent with the finding in the previous work (Pennington et al, 2014; Levy and Goldberg, 2014; Levy et al, 2015). To some extent, CBOW and its extension CBOWbi are the most different among the eight (the first four models in Table 1 and the first four models in Table 2). They tie together the characters in each context window by representing the context vector as the sum of their characters’ vectors. Although they have a potential of deriving better representations (Levy et al, 2015), they lose some particular information from each unit of input in the average operations. Although the performance on twelve different categories varies, in overall charCBOW, charSkipGram and their extens</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving Distributional Similarity with Lessons Learned from Word Embeddings.</title>
<date>2015</date>
<booktitle>In Proc. of TACL.</booktitle>
<contexts>
<context position="16048" citStr="Levy et al, 2015" startWordPosition="2513" endWordPosition="2516">/zhwiki/ 9https://code.google.com/p/word2vec/ 832 the embeddings of their composite grams10. Table 1 presents the word similarity evaluation results of the eight embedding models mentioned above, where A–L denote the twelve categories in E-TC. The first four rows are the results with the uni-character inputs, and the last four rows correspond to the bi-character embeddings results. We can see that both CBOW and CBOW-bi perform worse than the corresponding SkipGram and SkipGram-bi. This result is consistent with the finding in the previous work (Pennington et al, 2014; Levy and Goldberg, 2014; Levy et al, 2015). To some extent, CBOW and its extension CBOWbi are the most different among the eight (the first four models in Table 1 and the first four models in Table 2). They tie together the characters in each context window by representing the context vector as the sum of their characters’ vectors. Although they have a potential of deriving better representations (Levy et al, 2015), they lose some particular information from each unit of input in the average operations. Although the performance on twelve different categories varies, in overall charCBOW, charSkipGram and their extensions consistently b</context>
</contexts>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, And Ido Dagan. 2015. Improving Distributional Similarity with Lessons Learned from Word Embeddings. In Proc. of TACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1491--1500</pages>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proc. of ACL, pages 1491– 1500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better Word Representations with Recursive Neural Networks for Morphology.</title>
<date>2013</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="2128" citStr="Luong et al (2013)" startWordPosition="313" endWordPosition="316">epresentation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly keeping the order features (Ling et al, 2015). As to another line, Luong et al (2013) captures morphological composition by using neural networks and Qiu et al (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al (2013) extracts the syntactical morphemes and Cheng et al (2014) incorporates the POS tags and dependency relations. Basically, the work in Chinese follows the same ideas as in English. Distinguished from English, Chinese characters are logograms, of which over 80% are phonosemantic comp</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better Word Representations with Recursive Neural Networks for Morphology. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="1444" citStr="Mikolov et al, 2013" startWordPosition="200" endWordPosition="203">mantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly k</context>
<context position="8927" citStr="Mikolov et al, 2013" startWordPosition="1369" endWordPosition="1372">haracter vocabulary V . And z, c, e, K, T, M, IV I denote the Chinese character, the context character, the component list, the corresponding embedding dimension, the context window size, the number of components taken into account for each character, and the vocabulary size, respectively. We develop two component-enhanced character embedding models, namely charCBOW 3Inside a character, its radical often serves as the semanticcomponent while its other radical-like components may be phonetics. 830 and charSkipGram. charCBOW follows the original continuous bag-of-words model (CBOW) proposed by (Mikolov et al, 2013a). We predict the central character zi conditioned on a 2(M+1)TKdimensional vector that is the concatenation of the remaining character-level contexts (ci−T, ... , ci−1, ci+1, . . . , ci+T) and the components in their component lists. More formally, we wish to maximize the log likelihood of all the characters as follows, INPUT PROJECTION OUTPUT ci−T ei−T ci−T+1 ei−T+1 O ci−T ci−T+1 ... zi ci+T−1 ei+T−1 ci+T ei+T ci+T−1 ci+T ∑L = logp(zi|hi), (a) charCBOW zni ∈D INPUT PROJECTION OUTPUT hi = cat(ci−T, ei−T, ... , ci+T, ei+T ) where hi denotes the concatenation of the component-enhanced contexts</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their composition-ality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1444" citStr="Mikolov et al, 2013" startWordPosition="200" endWordPosition="203">mantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly k</context>
<context position="8927" citStr="Mikolov et al, 2013" startWordPosition="1369" endWordPosition="1372">haracter vocabulary V . And z, c, e, K, T, M, IV I denote the Chinese character, the context character, the component list, the corresponding embedding dimension, the context window size, the number of components taken into account for each character, and the vocabulary size, respectively. We develop two component-enhanced character embedding models, namely charCBOW 3Inside a character, its radical often serves as the semanticcomponent while its other radical-like components may be phonetics. 830 and charSkipGram. charCBOW follows the original continuous bag-of-words model (CBOW) proposed by (Mikolov et al, 2013a). We predict the central character zi conditioned on a 2(M+1)TKdimensional vector that is the concatenation of the remaining character-level contexts (ci−T, ... , ci−1, ci+1, . . . , ci+T) and the components in their component lists. More formally, we wish to maximize the log likelihood of all the characters as follows, INPUT PROJECTION OUTPUT ci−T ei−T ci−T+1 ei−T+1 O ci−T ci−T+1 ... zi ci+T−1 ei+T−1 ci+T ei+T ci+T−1 ci+T ∑L = logp(zi|hi), (a) charCBOW zni ∈D INPUT PROJECTION OUTPUT hi = cat(ci−T, ei−T, ... , ci+T, ei+T ) where hi denotes the concatenation of the component-enhanced contexts</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their composition-ality. In Advances in Neural Information Processing Systems. pages 3111-3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome L Myers</author>
<author>Arnold D Well</author>
</authors>
<date>1995</date>
<journal>Research Design &amp; Statistical Analysis. Routledge.</journal>
<contexts>
<context position="14972" citStr="Myers and Well, 1995" startWordPosition="2354" endWordPosition="2357">on/ccli/cliac/glyphs_guidelines.htm Appendix for easy reference. We adopt Chinese Wikipedia Dump8 to train our models as well as the original CBOW and SkipGram, implemented in the Word2Vec tool9 for comparison. The corpus in total contains 232,894 articles. In preprocessing, we remove pure digit words and non-Chinese characters, and ignore the words less than 10 occurrences during training. We set the context window size T as 2 and use 5 negative samples experimentally. All the embedding dimensions K are set to 50. In the word similarity evaluation, we compute the Spearman’s rank correlation (Myers and Well, 1995) between the similarity scores based on the learned embedding models and the E-TC similarity scores computed by following Tian and Zhao (2010). The bi-character embeddings are concatenation of the composite character embeddings. For the text classification evaluation, we average the composite single character embeddings for each bi-gram. And each bi-gram overlaps with the previous one. The titles are represented by averaging 8http://download.wikipedia.com/zhwiki/ 9https://code.google.com/p/word2vec/ 832 the embeddings of their composite grams10. Table 1 presents the word similarity evaluation </context>
</contexts>
<marker>Myers, Well, 1995</marker>
<rawString>Jerome L. Myers and Arnold D. Well. 1995. Research Design &amp; Statistical Analysis. Routledge.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global Vectors for Word Representation. In</title>
<booktitle>Proc. ofEMNLP.</booktitle>
<marker>Pennington, Socher, Manning, </marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global Vectors for Word Representation. In Proc. ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siyu Qiu</author>
<author>Qing Cui</author>
<author>Jiang Bian</author>
</authors>
<title>Colearning of Word Representations and Morpheme Representations.</title>
<date>2014</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="2209" citStr="Qiu et al (2014)" startWordPosition="325" endWordPosition="328">surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly keeping the order features (Ling et al, 2015). As to another line, Luong et al (2013) captures morphological composition by using neural networks and Qiu et al (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al (2013) extracts the syntactical morphemes and Cheng et al (2014) incorporates the POS tags and dependency relations. Basically, the work in Chinese follows the same ideas as in English. Distinguished from English, Chinese characters are logograms, of which over 80% are phonosemantic compounds, with a semantic component giving a broad category of meaning and a phoneti</context>
</contexts>
<marker>Qiu, Cui, Bian, 2014</marker>
<rawString>Siyu Qiu, Qing Cui, Jiang Bian, and et al. 2014. Colearning of Word Representations and Morpheme Representations. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Commun. ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM, 8(10):627–633, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaming Sun</author>
<author>Lei Lin</author>
<author>Duyu Tang</author>
</authors>
<date>2014</date>
<journal>RadicalEnhanced Chinese Character Embedding. CoRR</journal>
<volume>abs/</volume>
<pages>1404--4714</pages>
<contexts>
<context position="7027" citStr="Sun et al (2014)" startWordPosition="1048" endWordPosition="1051">er words, the component-level features inherently bring with additional information that benefits semantic representations of characters. For example, we know that the characters fj&apos;. (you), 4-A (he), tk (companion), Ia (companion), and C (people) all have the meanings related to human because of their shared component 4 (human), a variant of the Chinese character A (human). This kind of component information is intrinsically different from the contexts deriving by dependency relations and POS tags. It motivates us to investigate the componentenhanced Chinese character embedding models. While Sun et al (2014) utilizes radical information in a supervised fashion, we build our models in a holistic unsupervised and bottom-up way. It is important to note the variation of a radical inside a character. There are two types of variations. The main type is position-related. For example, the radical of the Chinese character 71&lt;. (water) is itself, but it becomes % as the radical of � (pool). The original radicals are stretched or squeezed so that they can fit into the general Chinese character shape of a square. The second variation type emerges along with the history of character simplification when tradit</context>
</contexts>
<marker>Sun, Lin, Tang, 2014</marker>
<rawString>Yaming Sun, Lei Lin, Duyu Tang, et al. 2014. RadicalEnhanced Chinese Character Embedding. CoRR abs/ 1404.4714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
</authors>
<title>Learning sentiment-specific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proc. ofACL.</booktitle>
<marker>Tang, Wei, Yang, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, et al. 2014. Learning sentiment-specific word embedding for twitter sentiment classification. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiu-le Tian</author>
<author>Wei Zhao</author>
</authors>
<title>Words Similarity Algorithm Based on Tongyici Cilin in Semantic Web Adaptive Learning System.</title>
<date>2010</date>
<marker>Jiu-le Tian, Zhao, 2010</marker>
<rawString>Jiu-le Tian and Wei Zhao. 2010. Words Similarity Algorithm Based on Tongyici Cilin in Semantic Web Adaptive Learning System.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Two/too simple adaptations of word2vec for syntax problems.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL,</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="2088" citStr="Ling et al, 2015" startWordPosition="305" endWordPosition="308">hese two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly keeping the order features (Ling et al, 2015). As to another line, Luong et al (2013) captures morphological composition by using neural networks and Qiu et al (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al (2013) extracts the syntactical morphemes and Cheng et al (2014) incorporates the POS tags and dependency relations. Basically, the work in Chinese follows the same ideas as in English. Distinguished from English, Chinese characters are logograms, </context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2015</marker>
<rawString>Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proc. of NAACL, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Oksana Yakhnenko</author>
<author>Nicolas Usunier</author>
</authors>
<title>Connecting language and knowledge bases with embedding models for relation extraction.</title>
<date>2013</date>
<booktitle>In Proc. of Computation and Language.</booktitle>
<marker>Weston, Bordes, Yakhnenko, Usunier, 2013</marker>
<rawString>Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proc. of Computation and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Unsupervised multi-domain adaptation with feature embeddings.</title>
<date>2015</date>
<booktitle>In Proc. ofNAACL-HIT.</booktitle>
<contexts>
<context position="1252" citStr="Yang and Eisenstein, 2015" startWordPosition="168" endWordPosition="171">enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. 1 Introduction Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al, 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-words model (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the word</context>
</contexts>
<marker>Yang, Eisenstein, 2015</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2015. Unsupervised multi-domain adaptation with feature embeddings. In Proc. ofNAACL-HIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="2009" citStr="Yu and Dredze, 2014" startWordPosition="291" endWordPosition="294">build word embeddings efficiently (Mikolov et al, 2013a; Mikolov et al, 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the words with important and relevant meanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly keeping the order features (Ling et al, 2015). As to another line, Luong et al (2013) captures morphological composition by using neural networks and Qiu et al (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al (2013) extracts the syntactical morphemes and Cheng et al (2014) incorporates the POS tags and dependency relations. Basically, the work in Chinese follows the same idea</context>
</contexts>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wan Xiang Che</author>
</authors>
<title>Chinese parsing exploiting characters.</title>
<date>2013</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="2446" citStr="Zhang et al (2013)" startWordPosition="363" endWordPosition="366"> information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al, 2014) or directly keeping the order features (Ling et al, 2015). As to another line, Luong et al (2013) captures morphological composition by using neural networks and Qiu et al (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al (2013) extracts the syntactical morphemes and Cheng et al (2014) incorporates the POS tags and dependency relations. Basically, the work in Chinese follows the same ideas as in English. Distinguished from English, Chinese characters are logograms, of which over 80% are phonosemantic compounds, with a semantic component giving a broad category of meaning and a phonetic component suggesting the sound1. For example, the semantic component 亻 (human) of the Chinese character 他 (he) provides the meaning connected with human. In fact, the components of most Chinese characters inherently bring with certain </context>
</contexts>
<marker>Zhang, Zhang, Che, 2013</marker>
<rawString>Meishan Zhang, Yue Zhang, Wan Xiang Che, and et al. 2013. Chinese parsing exploiting characters. In Proc. ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002790">
<title confidence="0.998954">
Image-Mediated Learning for
Zero-Shot Cross-Lingual Document Retrieval
</title>
<author confidence="0.993611">
Ruka Funaki, Hideki Nakayama
</author>
<affiliation confidence="0.990820333333333">
Machine Perception Group
Graduate School of Information Science and Technology
The University of Tokyo
</affiliation>
<email confidence="0.999127">
{funaki,nakayama}@nlab.ci.i.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999747375">
We propose an image-mediated learning
approach for cross-lingual document re-
trieval where no or only a few parallel
corpora are available. Using the images
in image-text documents of each language
as the hub, we derive a common seman-
tic subspace bridging two languages by
means of generalized canonical correla-
tion analysis. For the purpose of evalu-
ation, we create and release a new doc-
ument dataset consisting of three types
of data (English text, Japanese text, and
images). Our approach substantially en-
hances retrieval accuracy in zero-shot and
few-shot scenarios where text-to-text ex-
amples are scarce.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999811142857143">
Cross-lingual document retrieval (CLDR) is the
task of finding relevant documents in one lan-
guage given a query document in another lan-
guage. While sufficiently large-scale corpora are
critical for parallel corpus-based learning meth-
ods, manually creating corpora requires huge hu-
man effort and is unrealistic in many cases.
A straightforward approach is to crawl bilin-
gual documents from the Web for use as train-
ing data. However, because most documents on
the Web are written in one language, it is not al-
ways easy to collect a sufficient number of multi-
lingual documents, especially those involving mi-
nor languages. Let us consider the multimedia
information in documents. We can, for exam-
ple, find abundant pairings of text and images,
e.g., text with the ALT property of &lt;IMG&gt; tags in
HTML, text with photos posted to social network-
ing sites, and articles on Web news posted with
images. Unlike text, an image is a universal rep-
resentation; we can easily understand the semantic
</bodyText>
<figure confidence="0.5545165">
Image-Mediated Learning
Language 1 Documents Language 2 Documents
</figure>
<figureCaption confidence="0.992411">
Figure 1: Concept of image-mediated learning.
</figureCaption>
<bodyText confidence="0.999337259259259">
Our idea is to learn the relation between two lan-
guages indirectly by using images attached to text.
If two documents written in different languages
include images with similar image features, it is
likely that the texts contained in the two docu-
ments are similar. Based on this idea, we seek the
relation of texts written in different languages me-
diated by the similarity between images.
content of images regardless of our mother tongue.
Motivated by this observation, we expect that we
can learn the relation of two languages indirectly
through images, even if we do not have sufficient
bilingual text pairs (Figure 1).
Generally, traditional image recognition tech-
niques (or image features) are very poor com-
pared with those in the natural language process-
ing field. In recent years, however, deep learning
has resulted in a breakthrough in visual recogni-
tion and dramatically improved image recognition
accuracy in generic domains, which is rapidly ap-
proaching human recognition levels (Fang et al.,
2015). We expect that these state-of-the-art im-
age recognition technologies can effectively assist
CLDR tasks.
We show that hub images enable zero-shot
training of CLDR systems and improve retrieval
accuracy given only a few parallel text samples.
</bodyText>
<figure confidence="0.997383777777778">
Language1
Texts
pairs
images images
Learn relation indirectly
similarity
Language2
Texts
pairs
</figure>
<page confidence="0.976204">
585
</page>
<note confidence="0.857571">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 585–590,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.9995">
2.1 Multimodal Learning for CLDR
</subsectionHeader>
<bodyText confidence="0.999985363636363">
Multimodal learning, defined as a framework for
machine learning using inputs from multiple me-
dia or sensors, has played a key role in various
cross-modal applications. The most widely used
standard method for multimodal learning is canon-
ical correlation analysis (CCA) (Hotelling, 1936),
which projects multimodal data into a shared rep-
resentation. For example, CCA has been success-
fully used in image retrieval (tag to images) and
image annotation (image to tags) (Hardoon et al.,
2004; Rasiwasia et al., 2010; Gong et al., 2014). In
the context of CLDR, each language’s texts con-
stitute one modality. CCA has also commonly
been used for cross-lingual information retrieval
(Vinokourov et al., 2002; Li and Shawe-Taylor,
2004; Udupa and Khapra, 2010). Whereas CCA
can handle only two modalities, we need to con-
sider relations between three modalities because
we use images in addition to the two languages.
Therefore, we focus on an extension of CCA, gen-
eralized canonical correlation analysis (GCCA), to
handle more than two inputs (Kettenring, 1971).
</bodyText>
<subsectionHeader confidence="0.999333">
2.2 Zero-Shot Learning for CLDR
</subsectionHeader>
<bodyText confidence="0.9998841">
Our core idea is to use another modality (image)
as a hub to indirectly learn the relevance between
two different languages. The work by Rupnik et
al. is probably the closest to ours (Rupnik et al.,
2012). In their study, they used a popular language
(e.g., English) with enough bilingual documents
shared with other languages as a hub to enhance
CLDR for minor languages with few direct bilin-
gual texts available. Nevertheless, this method
assumes that parallel corpora of the hub and tar-
get languages exist and therefore, its application is
limited to specific domains where manual transla-
tions are readily available, such as Wikipedia and
news sites. Contrarily, because we use images as
the hub, we can use documents closed with respect
to each language for training. Considering that
current generic Web documents are mostly closed
with respect to one language, yet equipped with
rich multimedia data, our setup is assumed to be
more reasonable.
</bodyText>
<table confidence="0.9884168">
Division English Images Japanese
1 [train-E/I] E1 I1 -
2 [train-I/J] - I2 J2
3 [train-E/J] E3 - J3
4 [test-E/J] E4 - J4
</table>
<tableCaption confidence="0.986539666666667">
Table 1: Division of training and test data. Each
division of training dataset is missing one of the
three modalities.
</tableCaption>
<figureCaption confidence="0.996751">
Figure 2: System overview
</figureCaption>
<sectionHeader confidence="0.996923" genericHeader="method">
3 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.999972">
3.1 Overview of Image-Mediated Learning
</subsectionHeader>
<bodyText confidence="0.9977075">
We use the following notations for specifying each
non-overlapping data division.
</bodyText>
<listItem confidence="0.99970925">
1. [train-E/I]: Training documents consisting of
English text and images.
2. [train-I/J]: Training documents consisting of
images and Japanese text.
3. [train-E/J]: Training documents consisting of
English and Japanese text.
4. [test-E/J]: Test documents consisting of En-
glish and Japanese text.
</listItem>
<bodyText confidence="0.999936235294118">
We define IDs for each modality in each division
as given in Table 1. For example, E1 represents
features of English text in the [train-E/I] division.
Typical CLDR based on parallel corpora uses only
[train-E/J] for training and [test-E/J] for evalua-
tion. In the zero-shot learning scenario without
any [train-E/J] data, we use only [train-E/I] and
[train-I/J] for training. In the few-shot learning
scenario, we also use a small number of [train-E/J]
samples. We call this approach image-mediated
learning.
An overview of our system is depicted in Fig-
ure 2. We compress features by principal compo-
nent analysis (PCA) and train them by GCCA. For
testing, we compress features by PCA, project fea-
tures by GCCA, then, search the nearest neighbors
from Japanese to English in the joint space.
</bodyText>
<figure confidence="0.99868888">
Training
Test
Japanese Features
Japanese Features
English Features
English Features
Image Features
PCA
projection
PCA
PCA
PCA
PCA
projection
GCCA
projection
GCCA
projection
GCCA
Japanese Features
English Features
Joint Space
Nearest
Neighbors
Search
</figure>
<page confidence="0.981072">
586
</page>
<subsectionHeader confidence="0.998392">
3.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999407310344828">
A convolutional neural network (CNN) is one of
the most successful deep learning methods for vi-
sual recognition. It is known that we can ob-
tain very good image features by taking activation
of hidden neurons in a network pre-trained by a
sufficiently large dataset (Donahue et al., 2013).
We apply the CNN model pre-trained using the
ILSVRC2012 dataset (Russakovsky et al., 2015)
provided by Caffe (Jia et al., 2014), a standard
deep learning software package in the field of vi-
sual recognition.
As the text feature for both English and
Japanese, we use the bag-of-words (BoW) rep-
resentation and term frequency-inverse document
frequency (TF-IDF) weighting. The MeCab
(Kudo et al., 2004) library is used to divide
Japanese text into words by morphological analy-
sis. No preprocessing approaches like eliminating
stop words and stemming, are used.
the singularity issue.
Despite our training datasets having only two
of the three modalities as given in Table 1, we
can handle this situation naturally by computing
covariance matrices from the available data only.
For example, in the few-shot learning scenario,
we compute EEI using E1 and I1, and EEE us-
ing E1 and E3. In the zero-shot learning scenario,
because [train-E/J] is not available, we compute
EEE using E1 only and use a zero matrix for EEJ.
</bodyText>
<subsectionHeader confidence="0.99685">
3.4 Nearest Neighbor Search in Joint Space
</subsectionHeader>
<bodyText confidence="0.999724166666667">
We can find relevant documents in another lan-
guage by computing the distances from the query
documents using the coupled canonical subspaces.
Having set Japanese as the query language, we
retrieve documents written in English. Nearest
neighbors are obtained as follows:
</bodyText>
<equation confidence="0.465126">
3.3 GCCA j� := arg min d(ziJ, zjE), (2)
j
</equation>
<bodyText confidence="0.9995433125">
GCCA is a generalization of CCA for any m
modalities (m = 3 in our case). Although sev-
eral slightly different versions of GCCA have been
proposed (Carroll, 1968; Rupnik et al., 2012;
Velden and Takane, 2012), we implement the sim-
plest one (Kettenring, 1971) because GCCA itself
is not the main focus of this study.
Let E, I, and J denote English, images,
and Japanese, respectively. For feature vector
xk, Vk E {E, I, J}, let zk = (xk − xk)hk de-
note its canonical variables. Eij denotes a covari-
ance matrix of modalities i and j where i, j E
{E, I, J}. Projection vectors hk are computed
such that they maximize the sum of correlations
between each pair of modalities obtained by solv-
ing the following generalized eigenvalue problem:
</bodyText>
<equation confidence="0.9939225">
1
2
�=P ��(1)
EEE 0 0
0 EII 0 �h
0 0 EJJ
</equation>
<bodyText confidence="0.977139666666667">
where h = (hTE, hTI ,hTJ )T . The canon-
ical axises h are normalized such that
1 ∑3 k�{E,I,J} hT k Ekkhk = 1. Additionally,
</bodyText>
<sectionHeader confidence="0.544225" genericHeader="method">
3
</sectionHeader>
<bodyText confidence="0.96630675">
we add a regularization term to the self covariance
matrices to prevent over-fitting; that is, we set
Ekk —* Ekk + αI, where α is a parameter to avoid
where ziJ, zj
E are projected feature vectors of the
query and target documents, respectively, and d(·)
is a distance function, which in our case, is the
Euclidean distance.
</bodyText>
<sectionHeader confidence="0.999906" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.9461865">
4.1 Pascal Sentence Dataset with Japanese
Translation
</subsectionHeader>
<bodyText confidence="0.9999205625">
The UIUC Pascal Sentence Dataset (Rashtchian et
al., 2010) contains 1000 images, each of which
is annotated with five English sentences describ-
ing its content. This dataset was originally created
for the study of sentence generation from images,
which is one of the current hot topics in computer
vision. To establish a new benchmark dataset for
image-mediated CLDR, we included a Japanese
translation for each English sentence provided by
professional translators1, as shown in Figure 3. In
this experiment, we bundled the five sentences at-
tached to each image for use as one text document.
Therefore, in our setup, each of the 1000 docu-
ments in the dataset consists of three items: an im-
age, and the corresponding English and Japanese
text.
</bodyText>
<footnote confidence="0.829501333333333">
1Dataset is available at:
http://www.nlab.ci.i.u-tokyo.ac.jp/
dataset/pascal_sentence_jp/
</footnote>
<figure confidence="0.896729875">
0 EEI EEJ
EIE 0 EIJ
EJE EJI 0
�
�
) h
587
English Texts Images Japanese Texts
</figure>
<figureCaption confidence="0.8938605">
Figure 3: Examples from the Pascal Sentence Dataset with Japanese translations: each image has about
five sentences describing it from different perspectives.
</figureCaption>
<subsectionHeader confidence="0.91833">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.966555098039216">
We randomly sampled data from the dataset for
each division in Table 1 without any overlap; we
ignored the modality of each document that was
not available in each data division (e.g., Japanese
text in [train-E/I]). We ran experiments with vary-
ing sample sizes for [train-E/I] and [train-I/J], that
is, 100, 200, 300, and 400. Furthermore, we grad-
ually increased the number of [train-E/J] samples
from 0 to 100 to emulate the few-shot learning sce-
nario. The size of the test data [test-E/J] was fixed
at 100. Following this setup, we performed image-
mediated CLDR based on GCCA, and compared
the results with those obtained by standard CLDR
using only [train-E/J] data with CCA. We eval-
uated the performance with respect to the top-1
Japanese to English retrieval accuracy in the test
data. Given that we used 100 test samples, the
chance rate was 1%. For each run, we conducted
50 trials randomly replacing data and used the av-
erage score. All features were compressed into
100 dimensions via PCA and α was set to 0.01.
The experimental results, illustrated in Figure 4,
clearly show that better accuracy is obtained with
a greater number of text-image data in both zero-
shot and few-shot scenarios. We can expect even
better zero-shot accuracy with more text-image
data, although, we cannot increase [train-E/I] and
[train-I/J] more than 400 each in the current setup
because of the restricted dataset size. We sum-
marized results in zero-shot scenario in Table 2
in several cases. Although both GCCA and CCA
show improved performance as the sample size
- A family on a boat with a cross on a river
- A happy couple with a young child wearing a
life preserver sitting on a boat.
- A man, a woman, and a child sit on boat
with a large cross on it.
- A man, women and small child sitting on top
of a boat moving along the river.
- Family of three sitting on deck, child wearing
red vest, brush and shoes are seen in the
foreground.
- A black and white cow in a grassy field
stares at the camera.
- A black and white cow standing in a grassy
field.
- A black and white cow stands on grass
against a partly cloudy blue sky.
- a cow is gazing over the grass he is about to
graze
- Black and white cow standing in grassy field.
</bodyText>
<figure confidence="0.998802162162162">
︙
- 川で十字架のあるボートに乗っている家族。
- ボートに座っている、救命具を着た幼い子ど
もと幸せなカップル。
- 男性、女性と子どもが大きな十字架のある
ボートに座っています。
- 川に沿って動いているボートの上部に座って
いる男性、女性と小さな子ども。
- ブラシと靴が前景に写されている、子どもが
赤いベストを着て、デッキに座っている三人
の家族。
- 草地の黒と白の雌牛がカメラをじっと見てい
ます。
- 草地に立っている黒と白の雌牛。
- 一部曇った青空を背に黒と白の雌牛が草地に
立っています。
- 雌牛が食べようとしている草をじっと眺めて
います。
- 草地に立っている黒と白の雌牛。
︙
︙
Top 1 Retrieval Accuracy(%)
70
60
50
40
30
20
10
0
0 20 40 60 80 100
Sample Size of [train-E/J]
CCA
GCCA-400
GCCA-300
GCCA-200
GCCA-100
</figure>
<figureCaption confidence="0.73755225">
Figure 4: Retrieval accuracy varying the num-
ber of [train-E/J] data. Each colored line shows
the performance of our method with a differ-
ent sample size of [train-E/I] and [train-I/J] data
</figureCaption>
<bodyText confidence="0.996247384615385">
(e.g., GCCA-400 denotes respective 400 samples
of [train-E/I] and [train-I/J] for GCCA). We used
image features extracted from GoogLeNet and text
features represented as bags-of-words.
of [train-E/J] increases, not surprisingly, GCCA
is gradually overtaken by CCA when we have
enough samples to learn the relevance between
English and Japanese texts directly. However, ac-
curacies of image-mediated learning in the cases
when [train-E/J] is scarce are higher than CCA
baseline. Hence, we confirmed that the image-
mediated model is also effective in the few-shot
learning scenario.
</bodyText>
<page confidence="0.995393">
588
</page>
<table confidence="0.999728888888889">
Model Accuracy(%)
GCCA-400(BoW) 37.4 ± 3.8
GCCA-300(BoW) 27.6 ± 3.4
GCCA-200(BoW) 14.7 ± 3.2
GCCA-100(BoW) 9.8 ± 2.3
GCCA-400(TF-IDF) 42.0 ± 4.6
GCCA-300(TF-IDF) 31.6 ± 4.3
GCCA-200(TF-IDF) 17.2 ± 2.6
GCCA-100(TF-IDF) 11.8 ± 2.7
</table>
<tableCaption confidence="0.98847325">
Table 2: Accuracy of zero-shot learning. Image
features are extracted from GoogLeNet and both
the bag-of-words (BoW) and the TF-IDF model
are used as text features.
</tableCaption>
<figure confidence="0.9822185">
70
60
50
40
30
20
10
0
0 20 40 60 80 100
Sample Size of [train-E/J]
</figure>
<figureCaption confidence="0.9634355">
Figure 5: Retrieval accuracy using different image
features in image-mediated CLDR. The sample
size of both [train-E/I] and [train-I/J] is 400. Text
features are based on the bag-of-words model.
</figureCaption>
<subsectionHeader confidence="0.999407">
4.3 Effect of Image Features
</subsectionHeader>
<bodyText confidence="0.99988225">
We also verified the effect of the performance of
image features in our framework (see Figure 5
and Table 3). CNN has improved dramatically
over the last few years, and many new powerful
pre-trained networks are currently available. We
compared three different features extracted from
GoogLeNet (Szegedy et al., 2014), VGG 16 lay-
ers (Chatfield et al., 2014), and CaffeNet (Jia et al.,
2014; Krizhevsky et al., 2012). Additionally, we
tested the Fisher Vector (Perronnin et al., 2010),
which was the standard hand-crafted image fea-
ture before deep learning. We extracted features
from the pool5/7x7 s1 layer in GoogLeNet, fc6
layer in VGG, and fc6 layer in CaffeNet. For the
Fisher Vector, following the standard implemen-
tation, we compressed SIFT descriptors (Lowe,
</bodyText>
<table confidence="0.999676">
Feature Accuracy(%)
CNN(GoogLeNet), BoW 37.4 ± 3.8
CNN(VGG), BoW 31.3 ± 3.5
CNN(CaffeNet), BoW 25.1 ± 3.4
FisherVector, BoW 10.8 ± 2.7
CNN(GoogLeNet), TF-IDF 42.0 ± 4.6
CNN(VGG), TF-IDF 37.8 ± 2.9
CNN(CaffeNet), TF-IDF 29.7 ± 4.4
FisherVector, TF-IDF 12.6 ± 2.7
</table>
<tableCaption confidence="0.999236">
Table 3: Accuracy of zero-shot learning in multi-
</tableCaption>
<bodyText confidence="0.987446307692308">
ple image features. The sample size of both [train-
E/I] and [train-I/J] is 400. Both the bag-of-words
(BoW) and the TF-IDF model are used as text fea-
tures.
1999) into 64 dimensions by PCA, and used a
Gaussian mixture model with 64 components. We
used four spatial grids for the final feature extrac-
tion. Overall, the order of performance of features
corresponds to that known in the image classifica-
tion domain (Russakovsky et al., 2015). This re-
sult indicates that when more powerful image fea-
tures are used, better performance can be achieved
in image-mediated CLDR.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999963153846154">
We proposed an image-mediated learning ap-
proach to realize zero-shot or few-shot CLDR.
For evaluation, we created and released a new
dataset consisting of Japanese, English, and image
triplets, based on the widely used Pascal Sentence
Dataset. We showed that state-of-the-art CNN-
based image features can substantially improve
zero-shot CLDR performance. Considering that
image features have continued to improve rapidly
since the deep learning breakthrough and the uni-
versality of images in Web documents, this ap-
proach could become even more important in the
future.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999503">
This work was supported by JST CREST, JSPS
KAKENHI Grant Number 26730085. We thank
the three anonymous reviewers for their helpful
comments.
</bodyText>
<sectionHeader confidence="0.996794" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.708106">
Douglas J Carroll. 1968. Generalization of canoni-
cal correlation analysis to three or more sets of vari-
</reference>
<figure confidence="0.992702666666667">
Top 1 Retrieval Accuracy (%)
CCA
CNN(GoogLeNet)
CNN(VGG)
CNN(CaffeNet)
FisherVector
</figure>
<page confidence="0.984883">
589
</page>
<reference confidence="0.999557490740741">
ables. In Proceedings of the 76th Annual Conven-
tion of the American Psychological Association, vol-
ume 3, pages 227–228.
Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and
Andrew Zisserman. 2014. Return of the Devil in
the Details: Delving Deep into Convolutional Nets.
In Proceedings of the British Machine Vision Con-
ference, pages 1–11.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2013. DeCAF: A Deep Convolutional Activation
Feature for Generic Visual Recognition. In Proceed-
ings of the International Conference on Machine
Learning, pages 647–655.
Hao Fang, Saurabh Gupta, Forrest Iandola, K. Ru-
pesh Srivastava, Li Deng, Piotr Doll´ar, Jianfeng
Gao, Xiaodong He, Margaret Mitchell, John C. Platt,
C. Lawrence Zitnick, and Geoffrey Zweig. 2015.
From Captions to Visual Concepts and Back. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition.
Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana
Lazebnik. 2014. A Multiview Embedding Space
for Modeling Internet Images, Tags, and their Se-
mantics. International Journal of Computer Vision,
106(2):210–233.
David R Hardoon, Sandor Szedmak, and John Shawe-
Taylor. 2004. Canonical Correlation Analysis: an
Overview with Application to Learning Methods.
Neural Computation, 16(12):2639–2664.
Harold Hotelling. 1936. Relations between Two Sets
of Variants. Biometrika, 28:321–377.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. 2014. Caffe :
Convolutional Architecture for Fast Feature Embed-
ding. In Proceedings of the ACM International Con-
ference on Multimedia, pages 675–678.
Jon Robers Kettenring. 1971. Canonical Analysis of
Several Sets of Variables. Biometrika, 58(3):433–
451.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. ImageNet Classification with Deep Con-
volutional Neural Networks. In Advances in Neural
Information Processing Systems, pages 1097–1105.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing, pages 230–237.
Yaoyong Li and John Shawe-Taylor. 2004. Using
KCCA for Japanese-English Cross-language Infor-
mation Retrieval and Classification. In Learning
Methods for Text Understanding and Mining Work-
shop, pages 117–133.
David G Lowe. 1999. Object recognition from local
scale-invariant features. In Proceedings of the Sev-
enth IEEE International Conference on Computer
Vision, volume 2, pages 1150–1157.
Florent Perronnin, Jorge S´anchez, and Thomas
Mensink. 2010. Improving the Fisher kernel for
large-scale image classification. In Proceedings
of the European Conference on Computer Vision,
pages 143–156.
Cyrus Rashtchian, Peter Young, Micah Hodosh, Ju-
lia Hockenmaier, and North Goodwin Ave. 2010.
Collecting Image Annotations Using Amazon ’s
Mechanical Turk. Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics Workshop on Creating Speech and Lan-
guage Data with Amazon’s Mechanical Turk, pages
139–147.
Nikhil Rasiwasia, Jose Costa Pereira, Emanuele
Coviello, Gabriel Doyle, Gert R.G. Lanckriet, Roger
Levy, and Nuno Vasconcelos. 2010. A New Ap-
proach to Cross-modal Multimedia Retrieval. Pro-
ceedings of the International Conference on Multi-
media, pages 251–260.
Jan Rupnik, Andrej Muhiˇc, and Primo ˇSkraba. 2012.
Cross-Lingual Document Retrieval through Hub
Languages. In Neural Information Processing Sys-
tems Workshop.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet
Large Scale Visual Recognition Challenge. Interna-
tional Journal of Computer Vision.
Christian Szegedy, Scott Reed, Pierre Sermanet,
Vincent Vanhoucke, and Andrew Rabinovich.
2014. Going deeper with convolutions. CoRR
abs/1409.4842.
Raghavendra Udupa and Mitesh M Khapra. 2010.
Improving the Multilingual User Experience of
Wikipedia Using Cross-Language Name Search.
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
492–500.
Michel Velden and Yoshio Takane. 2012. General-
ized Canonical Correlation Analysis with Missing
Values. Computational Statistics, 27(3):551–571.
Alexei Vinokourov, John Shawe-Taylor, and Nello
Cristianini. 2002. Inferring a Semantic Representa-
tion of Text via Cross-Language Correlation Anal-
ysis. Advances in Neural Information Processing
Systems, pages 1473–1480.
</reference>
<page confidence="0.997346">
590
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.644524">
<title confidence="0.969085">Image-Mediated Learning Zero-Shot Cross-Lingual Document Retrieval</title>
<author confidence="0.91322">Ruka Funaki</author>
<author confidence="0.91322">Hideki</author>
<affiliation confidence="0.875518666666667">Machine Perception Graduate School of Information Science and The University of</affiliation>
<abstract confidence="0.999720705882353">We propose an image-mediated learning approach for cross-lingual document retrieval where no or only a few parallel corpora are available. Using the images in image-text documents of each language as the hub, we derive a common semantic subspace bridging two languages by means of generalized canonical correlation analysis. For the purpose of evaluation, we create and release a new document dataset consisting of three types of data (English text, Japanese text, and images). Our approach substantially enhances retrieval accuracy in zero-shot and few-shot scenarios where text-to-text examples are scarce.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas J Carroll</author>
</authors>
<title>Generalization of canonical correlation analysis to three or more sets of variables.</title>
<date>1968</date>
<booktitle>In Proceedings of the 76th Annual Convention of the American Psychological Association,</booktitle>
<volume>3</volume>
<pages>227--228</pages>
<contexts>
<context position="9186" citStr="Carroll, 1968" startWordPosition="1446" endWordPosition="1447">because [train-E/J] is not available, we compute EEE using E1 only and use a zero matrix for EEJ. 3.4 Nearest Neighbor Search in Joint Space We can find relevant documents in another language by computing the distances from the query documents using the coupled canonical subspaces. Having set Japanese as the query language, we retrieve documents written in English. Nearest neighbors are obtained as follows: 3.3 GCCA j� := arg min d(ziJ, zjE), (2) j GCCA is a generalization of CCA for any m modalities (m = 3 in our case). Although several slightly different versions of GCCA have been proposed (Carroll, 1968; Rupnik et al., 2012; Velden and Takane, 2012), we implement the simplest one (Kettenring, 1971) because GCCA itself is not the main focus of this study. Let E, I, and J denote English, images, and Japanese, respectively. For feature vector xk, Vk E {E, I, J}, let zk = (xk − xk)hk denote its canonical variables. Eij denotes a covariance matrix of modalities i and j where i, j E {E, I, J}. Projection vectors hk are computed such that they maximize the sum of correlations between each pair of modalities obtained by solving the following generalized eigenvalue problem: 1 2 �=P ��(1) EEE 0 0 0 EI</context>
</contexts>
<marker>Carroll, 1968</marker>
<rawString>Douglas J Carroll. 1968. Generalization of canonical correlation analysis to three or more sets of variables. In Proceedings of the 76th Annual Convention of the American Psychological Association, volume 3, pages 227–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chatfield</author>
<author>Karen Simonyan</author>
<author>Andrea Vedaldi</author>
<author>Andrew Zisserman</author>
</authors>
<title>Return of the Devil in the Details: Delving Deep into Convolutional Nets.</title>
<date>2014</date>
<booktitle>In Proceedings of the British Machine Vision Conference,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="15881" citStr="Chatfield et al., 2014" startWordPosition="2575" endWordPosition="2578">0 40 60 80 100 Sample Size of [train-E/J] Figure 5: Retrieval accuracy using different image features in image-mediated CLDR. The sample size of both [train-E/I] and [train-I/J] is 400. Text features are based on the bag-of-words model. 4.3 Effect of Image Features We also verified the effect of the performance of image features in our framework (see Figure 5 and Table 3). CNN has improved dramatically over the last few years, and many new powerful pre-trained networks are currently available. We compared three different features extracted from GoogLeNet (Szegedy et al., 2014), VGG 16 layers (Chatfield et al., 2014), and CaffeNet (Jia et al., 2014; Krizhevsky et al., 2012). Additionally, we tested the Fisher Vector (Perronnin et al., 2010), which was the standard hand-crafted image feature before deep learning. We extracted features from the pool5/7x7 s1 layer in GoogLeNet, fc6 layer in VGG, and fc6 layer in CaffeNet. For the Fisher Vector, following the standard implementation, we compressed SIFT descriptors (Lowe, Feature Accuracy(%) CNN(GoogLeNet), BoW 37.4 ± 3.8 CNN(VGG), BoW 31.3 ± 3.5 CNN(CaffeNet), BoW 25.1 ± 3.4 FisherVector, BoW 10.8 ± 2.7 CNN(GoogLeNet), TF-IDF 42.0 ± 4.6 CNN(VGG), TF-IDF 37.8 </context>
</contexts>
<marker>Chatfield, Simonyan, Vedaldi, Zisserman, 2014</marker>
<rawString>Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Return of the Devil in the Details: Delving Deep into Convolutional Nets. In Proceedings of the British Machine Vision Conference, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Donahue</author>
<author>Yangqing Jia</author>
<author>Oriol Vinyals</author>
<author>Judy Hoffman</author>
<author>Ning Zhang</author>
<author>Eric Tzeng</author>
<author>Trevor Darrell</author>
</authors>
<title>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>647--655</pages>
<contexts>
<context position="7657" citStr="Donahue et al., 2013" startWordPosition="1192" endWordPosition="1195">t neighbors from Japanese to English in the joint space. Training Test Japanese Features Japanese Features English Features English Features Image Features PCA projection PCA PCA PCA PCA projection GCCA projection GCCA projection GCCA Japanese Features English Features Joint Space Nearest Neighbors Search 586 3.2 Feature Extraction A convolutional neural network (CNN) is one of the most successful deep learning methods for visual recognition. It is known that we can obtain very good image features by taking activation of hidden neurons in a network pre-trained by a sufficiently large dataset (Donahue et al., 2013). We apply the CNN model pre-trained using the ILSVRC2012 dataset (Russakovsky et al., 2015) provided by Caffe (Jia et al., 2014), a standard deep learning software package in the field of visual recognition. As the text feature for both English and Japanese, we use the bag-of-words (BoW) representation and term frequency-inverse document frequency (TF-IDF) weighting. The MeCab (Kudo et al., 2004) library is used to divide Japanese text into words by morphological analysis. No preprocessing approaches like eliminating stop words and stemming, are used. the singularity issue. Despite our traini</context>
</contexts>
<marker>Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, Darrell, 2013</marker>
<rawString>Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2013. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. In Proceedings of the International Conference on Machine Learning, pages 647–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Fang</author>
<author>Saurabh Gupta</author>
<author>Forrest Iandola</author>
<author>K Rupesh Srivastava</author>
<author>Li Deng</author>
<author>Piotr Doll´ar</author>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Margaret Mitchell</author>
<author>John C Platt</author>
<author>C Lawrence Zitnick</author>
<author>Geoffrey Zweig</author>
</authors>
<title>From Captions to Visual Concepts and Back.</title>
<date>2015</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</booktitle>
<marker>Fang, Gupta, Iandola, Srivastava, Deng, Doll´ar, Gao, He, Mitchell, Platt, Zitnick, Zweig, 2015</marker>
<rawString>Hao Fang, Saurabh Gupta, Forrest Iandola, K. Rupesh Srivastava, Li Deng, Piotr Doll´ar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, and Geoffrey Zweig. 2015. From Captions to Visual Concepts and Back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunchao Gong</author>
<author>Qifa Ke</author>
<author>Michael Isard</author>
<author>Svetlana Lazebnik</author>
</authors>
<title>A Multiview Embedding Space for Modeling Internet Images, Tags, and their Semantics.</title>
<date>2014</date>
<journal>International Journal of Computer Vision,</journal>
<volume>106</volume>
<issue>2</issue>
<contexts>
<context position="4098" citStr="Gong et al., 2014" startWordPosition="626" endWordPosition="629">ciation for Computational Linguistics. 2 Related Work 2.1 Multimodal Learning for CLDR Multimodal learning, defined as a framework for machine learning using inputs from multiple media or sensors, has played a key role in various cross-modal applications. The most widely used standard method for multimodal learning is canonical correlation analysis (CCA) (Hotelling, 1936), which projects multimodal data into a shared representation. For example, CCA has been successfully used in image retrieval (tag to images) and image annotation (image to tags) (Hardoon et al., 2004; Rasiwasia et al., 2010; Gong et al., 2014). In the context of CLDR, each language’s texts constitute one modality. CCA has also commonly been used for cross-lingual information retrieval (Vinokourov et al., 2002; Li and Shawe-Taylor, 2004; Udupa and Khapra, 2010). Whereas CCA can handle only two modalities, we need to consider relations between three modalities because we use images in addition to the two languages. Therefore, we focus on an extension of CCA, generalized canonical correlation analysis (GCCA), to handle more than two inputs (Kettenring, 1971). 2.2 Zero-Shot Learning for CLDR Our core idea is to use another modality (im</context>
</contexts>
<marker>Gong, Ke, Isard, Lazebnik, 2014</marker>
<rawString>Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik. 2014. A Multiview Embedding Space for Modeling Internet Images, Tags, and their Semantics. International Journal of Computer Vision, 106(2):210–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Hardoon</author>
<author>Sandor Szedmak</author>
<author>John ShaweTaylor</author>
</authors>
<title>Canonical Correlation Analysis: an Overview with Application to Learning Methods.</title>
<date>2004</date>
<journal>Neural Computation,</journal>
<volume>16</volume>
<issue>12</issue>
<contexts>
<context position="4054" citStr="Hardoon et al., 2004" startWordPosition="618" endWordPosition="621">n, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work 2.1 Multimodal Learning for CLDR Multimodal learning, defined as a framework for machine learning using inputs from multiple media or sensors, has played a key role in various cross-modal applications. The most widely used standard method for multimodal learning is canonical correlation analysis (CCA) (Hotelling, 1936), which projects multimodal data into a shared representation. For example, CCA has been successfully used in image retrieval (tag to images) and image annotation (image to tags) (Hardoon et al., 2004; Rasiwasia et al., 2010; Gong et al., 2014). In the context of CLDR, each language’s texts constitute one modality. CCA has also commonly been used for cross-lingual information retrieval (Vinokourov et al., 2002; Li and Shawe-Taylor, 2004; Udupa and Khapra, 2010). Whereas CCA can handle only two modalities, we need to consider relations between three modalities because we use images in addition to the two languages. Therefore, we focus on an extension of CCA, generalized canonical correlation analysis (GCCA), to handle more than two inputs (Kettenring, 1971). 2.2 Zero-Shot Learning for CLDR </context>
</contexts>
<marker>Hardoon, Szedmak, ShaweTaylor, 2004</marker>
<rawString>David R Hardoon, Sandor Szedmak, and John ShaweTaylor. 2004. Canonical Correlation Analysis: an Overview with Application to Learning Methods. Neural Computation, 16(12):2639–2664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relations between Two Sets of Variants.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--321</pages>
<contexts>
<context position="3854" citStr="Hotelling, 1936" startWordPosition="587" endWordPosition="588">exts pairs images images Learn relation indirectly similarity Language2 Texts pairs 585 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 585–590, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work 2.1 Multimodal Learning for CLDR Multimodal learning, defined as a framework for machine learning using inputs from multiple media or sensors, has played a key role in various cross-modal applications. The most widely used standard method for multimodal learning is canonical correlation analysis (CCA) (Hotelling, 1936), which projects multimodal data into a shared representation. For example, CCA has been successfully used in image retrieval (tag to images) and image annotation (image to tags) (Hardoon et al., 2004; Rasiwasia et al., 2010; Gong et al., 2014). In the context of CLDR, each language’s texts constitute one modality. CCA has also commonly been used for cross-lingual information retrieval (Vinokourov et al., 2002; Li and Shawe-Taylor, 2004; Udupa and Khapra, 2010). Whereas CCA can handle only two modalities, we need to consider relations between three modalities because we use images in addition </context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relations between Two Sets of Variants. Biometrika, 28:321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangqing Jia</author>
<author>Evan Shelhamer</author>
<author>Jeff Donahue</author>
<author>Sergey Karayev</author>
<author>Jonathan Long</author>
<author>Ross Girshick</author>
<author>Sergio Guadarrama</author>
<author>Trevor Darrell</author>
</authors>
<title>Caffe : Convolutional Architecture for Fast Feature Embedding.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACM International Conference on Multimedia,</booktitle>
<pages>675--678</pages>
<contexts>
<context position="7786" citStr="Jia et al., 2014" startWordPosition="1213" endWordPosition="1216">eatures Image Features PCA projection PCA PCA PCA PCA projection GCCA projection GCCA projection GCCA Japanese Features English Features Joint Space Nearest Neighbors Search 586 3.2 Feature Extraction A convolutional neural network (CNN) is one of the most successful deep learning methods for visual recognition. It is known that we can obtain very good image features by taking activation of hidden neurons in a network pre-trained by a sufficiently large dataset (Donahue et al., 2013). We apply the CNN model pre-trained using the ILSVRC2012 dataset (Russakovsky et al., 2015) provided by Caffe (Jia et al., 2014), a standard deep learning software package in the field of visual recognition. As the text feature for both English and Japanese, we use the bag-of-words (BoW) representation and term frequency-inverse document frequency (TF-IDF) weighting. The MeCab (Kudo et al., 2004) library is used to divide Japanese text into words by morphological analysis. No preprocessing approaches like eliminating stop words and stemming, are used. the singularity issue. Despite our training datasets having only two of the three modalities as given in Table 1, we can handle this situation naturally by computing cova</context>
<context position="15913" citStr="Jia et al., 2014" startWordPosition="2581" endWordPosition="2584">J] Figure 5: Retrieval accuracy using different image features in image-mediated CLDR. The sample size of both [train-E/I] and [train-I/J] is 400. Text features are based on the bag-of-words model. 4.3 Effect of Image Features We also verified the effect of the performance of image features in our framework (see Figure 5 and Table 3). CNN has improved dramatically over the last few years, and many new powerful pre-trained networks are currently available. We compared three different features extracted from GoogLeNet (Szegedy et al., 2014), VGG 16 layers (Chatfield et al., 2014), and CaffeNet (Jia et al., 2014; Krizhevsky et al., 2012). Additionally, we tested the Fisher Vector (Perronnin et al., 2010), which was the standard hand-crafted image feature before deep learning. We extracted features from the pool5/7x7 s1 layer in GoogLeNet, fc6 layer in VGG, and fc6 layer in CaffeNet. For the Fisher Vector, following the standard implementation, we compressed SIFT descriptors (Lowe, Feature Accuracy(%) CNN(GoogLeNet), BoW 37.4 ± 3.8 CNN(VGG), BoW 31.3 ± 3.5 CNN(CaffeNet), BoW 25.1 ± 3.4 FisherVector, BoW 10.8 ± 2.7 CNN(GoogLeNet), TF-IDF 42.0 ± 4.6 CNN(VGG), TF-IDF 37.8 ± 2.9 CNN(CaffeNet), TF-IDF 29.7</context>
</contexts>
<marker>Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama, Darrell, 2014</marker>
<rawString>Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe : Convolutional Architecture for Fast Feature Embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675–678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Robers Kettenring</author>
</authors>
<title>Canonical Analysis of Several Sets of Variables.</title>
<date>1971</date>
<journal>Biometrika,</journal>
<volume>58</volume>
<issue>3</issue>
<pages>451</pages>
<contexts>
<context position="4620" citStr="Kettenring, 1971" startWordPosition="710" endWordPosition="711">ge annotation (image to tags) (Hardoon et al., 2004; Rasiwasia et al., 2010; Gong et al., 2014). In the context of CLDR, each language’s texts constitute one modality. CCA has also commonly been used for cross-lingual information retrieval (Vinokourov et al., 2002; Li and Shawe-Taylor, 2004; Udupa and Khapra, 2010). Whereas CCA can handle only two modalities, we need to consider relations between three modalities because we use images in addition to the two languages. Therefore, we focus on an extension of CCA, generalized canonical correlation analysis (GCCA), to handle more than two inputs (Kettenring, 1971). 2.2 Zero-Shot Learning for CLDR Our core idea is to use another modality (image) as a hub to indirectly learn the relevance between two different languages. The work by Rupnik et al. is probably the closest to ours (Rupnik et al., 2012). In their study, they used a popular language (e.g., English) with enough bilingual documents shared with other languages as a hub to enhance CLDR for minor languages with few direct bilingual texts available. Nevertheless, this method assumes that parallel corpora of the hub and target languages exist and therefore, its application is limited to specific dom</context>
<context position="9283" citStr="Kettenring, 1971" startWordPosition="1462" endWordPosition="1463">EEJ. 3.4 Nearest Neighbor Search in Joint Space We can find relevant documents in another language by computing the distances from the query documents using the coupled canonical subspaces. Having set Japanese as the query language, we retrieve documents written in English. Nearest neighbors are obtained as follows: 3.3 GCCA j� := arg min d(ziJ, zjE), (2) j GCCA is a generalization of CCA for any m modalities (m = 3 in our case). Although several slightly different versions of GCCA have been proposed (Carroll, 1968; Rupnik et al., 2012; Velden and Takane, 2012), we implement the simplest one (Kettenring, 1971) because GCCA itself is not the main focus of this study. Let E, I, and J denote English, images, and Japanese, respectively. For feature vector xk, Vk E {E, I, J}, let zk = (xk − xk)hk denote its canonical variables. Eij denotes a covariance matrix of modalities i and j where i, j E {E, I, J}. Projection vectors hk are computed such that they maximize the sum of correlations between each pair of modalities obtained by solving the following generalized eigenvalue problem: 1 2 �=P ��(1) EEE 0 0 0 EII 0 �h 0 0 EJJ where h = (hTE, hTI ,hTJ )T . The canonical axises h are normalized such that 1 ∑3</context>
</contexts>
<marker>Kettenring, 1971</marker>
<rawString>Jon Robers Kettenring. 1971. Canonical Analysis of Several Sets of Variables. Biometrika, 58(3):433– 451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>ImageNet Classification with Deep Convolutional Neural Networks.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1097--1105</pages>
<contexts>
<context position="15939" citStr="Krizhevsky et al., 2012" startWordPosition="2585" endWordPosition="2588">eval accuracy using different image features in image-mediated CLDR. The sample size of both [train-E/I] and [train-I/J] is 400. Text features are based on the bag-of-words model. 4.3 Effect of Image Features We also verified the effect of the performance of image features in our framework (see Figure 5 and Table 3). CNN has improved dramatically over the last few years, and many new powerful pre-trained networks are currently available. We compared three different features extracted from GoogLeNet (Szegedy et al., 2014), VGG 16 layers (Chatfield et al., 2014), and CaffeNet (Jia et al., 2014; Krizhevsky et al., 2012). Additionally, we tested the Fisher Vector (Perronnin et al., 2010), which was the standard hand-crafted image feature before deep learning. We extracted features from the pool5/7x7 s1 layer in GoogLeNet, fc6 layer in VGG, and fc6 layer in CaffeNet. For the Fisher Vector, following the standard implementation, we compressed SIFT descriptors (Lowe, Feature Accuracy(%) CNN(GoogLeNet), BoW 37.4 ± 3.8 CNN(VGG), BoW 31.3 ± 3.5 CNN(CaffeNet), BoW 25.1 ± 3.4 FisherVector, BoW 10.8 ± 2.7 CNN(GoogLeNet), TF-IDF 42.0 ± 4.6 CNN(VGG), TF-IDF 37.8 ± 2.9 CNN(CaffeNet), TF-IDF 29.7 ± 4.4 FisherVector, TF-ID</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information Processing Systems, pages 1097–1105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying Conditional Random Fields to Japanese Morphological Analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="8057" citStr="Kudo et al., 2004" startWordPosition="1255" endWordPosition="1258">ep learning methods for visual recognition. It is known that we can obtain very good image features by taking activation of hidden neurons in a network pre-trained by a sufficiently large dataset (Donahue et al., 2013). We apply the CNN model pre-trained using the ILSVRC2012 dataset (Russakovsky et al., 2015) provided by Caffe (Jia et al., 2014), a standard deep learning software package in the field of visual recognition. As the text feature for both English and Japanese, we use the bag-of-words (BoW) representation and term frequency-inverse document frequency (TF-IDF) weighting. The MeCab (Kudo et al., 2004) library is used to divide Japanese text into words by morphological analysis. No preprocessing approaches like eliminating stop words and stemming, are used. the singularity issue. Despite our training datasets having only two of the three modalities as given in Table 1, we can handle this situation naturally by computing covariance matrices from the available data only. For example, in the few-shot learning scenario, we compute EEI using E1 and I1, and EEE using E1 and E3. In the zero-shot learning scenario, because [train-E/J] is not available, we compute EEE using E1 only and use a zero ma</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying Conditional Random Fields to Japanese Morphological Analysis. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaoyong Li</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Using KCCA for Japanese-English Cross-language Information Retrieval and Classification.</title>
<date>2004</date>
<booktitle>In Learning Methods for Text Understanding and Mining Workshop,</booktitle>
<pages>117--133</pages>
<contexts>
<context position="4294" citStr="Li and Shawe-Taylor, 2004" startWordPosition="656" endWordPosition="659">ensors, has played a key role in various cross-modal applications. The most widely used standard method for multimodal learning is canonical correlation analysis (CCA) (Hotelling, 1936), which projects multimodal data into a shared representation. For example, CCA has been successfully used in image retrieval (tag to images) and image annotation (image to tags) (Hardoon et al., 2004; Rasiwasia et al., 2010; Gong et al., 2014). In the context of CLDR, each language’s texts constitute one modality. CCA has also commonly been used for cross-lingual information retrieval (Vinokourov et al., 2002; Li and Shawe-Taylor, 2004; Udupa and Khapra, 2010). Whereas CCA can handle only two modalities, we need to consider relations between three modalities because we use images in addition to the two languages. Therefore, we focus on an extension of CCA, generalized canonical correlation analysis (GCCA), to handle more than two inputs (Kettenring, 1971). 2.2 Zero-Shot Learning for CLDR Our core idea is to use another modality (image) as a hub to indirectly learn the relevance between two different languages. The work by Rupnik et al. is probably the closest to ours (Rupnik et al., 2012). In their study, they used a popula</context>
</contexts>
<marker>Li, Shawe-Taylor, 2004</marker>
<rawString>Yaoyong Li and John Shawe-Taylor. 2004. Using KCCA for Japanese-English Cross-language Information Retrieval and Classification. In Learning Methods for Text Understanding and Mining Workshop, pages 117–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Object recognition from local scale-invariant features.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh IEEE International Conference on Computer Vision,</booktitle>
<volume>2</volume>
<pages>1150--1157</pages>
<marker>Lowe, 1999</marker>
<rawString>David G Lowe. 1999. Object recognition from local scale-invariant features. In Proceedings of the Seventh IEEE International Conference on Computer Vision, volume 2, pages 1150–1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florent Perronnin</author>
<author>Jorge S´anchez</author>
<author>Thomas Mensink</author>
</authors>
<title>Improving the Fisher kernel for large-scale image classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Computer Vision,</booktitle>
<pages>143--156</pages>
<marker>Perronnin, S´anchez, Mensink, 2010</marker>
<rawString>Florent Perronnin, Jorge S´anchez, and Thomas Mensink. 2010. Improving the Fisher kernel for large-scale image classification. In Proceedings of the European Conference on Computer Vision, pages 143–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
<author>North Goodwin Ave</author>
</authors>
<title>Collecting Image Annotations Using Amazon ’s Mechanical Turk. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>139--147</pages>
<contexts>
<context position="10376" citStr="Rashtchian et al., 2010" startWordPosition="1664" endWordPosition="1667">roblem: 1 2 �=P ��(1) EEE 0 0 0 EII 0 �h 0 0 EJJ where h = (hTE, hTI ,hTJ )T . The canonical axises h are normalized such that 1 ∑3 k�{E,I,J} hT k Ekkhk = 1. Additionally, 3 we add a regularization term to the self covariance matrices to prevent over-fitting; that is, we set Ekk —* Ekk + αI, where α is a parameter to avoid where ziJ, zj E are projected feature vectors of the query and target documents, respectively, and d(·) is a distance function, which in our case, is the Euclidean distance. 4 Experiment 4.1 Pascal Sentence Dataset with Japanese Translation The UIUC Pascal Sentence Dataset (Rashtchian et al., 2010) contains 1000 images, each of which is annotated with five English sentences describing its content. This dataset was originally created for the study of sentence generation from images, which is one of the current hot topics in computer vision. To establish a new benchmark dataset for image-mediated CLDR, we included a Japanese translation for each English sentence provided by professional translators1, as shown in Figure 3. In this experiment, we bundled the five sentences attached to each image for use as one text document. Therefore, in our setup, each of the 1000 documents in the dataset</context>
</contexts>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, Ave, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, Julia Hockenmaier, and North Goodwin Ave. 2010. Collecting Image Annotations Using Amazon ’s Mechanical Turk. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 139–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikhil Rasiwasia</author>
<author>Jose Costa Pereira</author>
<author>Emanuele Coviello</author>
<author>Gabriel Doyle</author>
<author>Gert R G Lanckriet</author>
<author>Roger Levy</author>
<author>Nuno Vasconcelos</author>
</authors>
<title>A New Approach to Cross-modal Multimedia Retrieval.</title>
<date>2010</date>
<booktitle>Proceedings of the International Conference on Multimedia,</booktitle>
<pages>251--260</pages>
<contexts>
<context position="4078" citStr="Rasiwasia et al., 2010" startWordPosition="622" endWordPosition="625">tember 2015. c�2015 Association for Computational Linguistics. 2 Related Work 2.1 Multimodal Learning for CLDR Multimodal learning, defined as a framework for machine learning using inputs from multiple media or sensors, has played a key role in various cross-modal applications. The most widely used standard method for multimodal learning is canonical correlation analysis (CCA) (Hotelling, 1936), which projects multimodal data into a shared representation. For example, CCA has been successfully used in image retrieval (tag to images) and image annotation (image to tags) (Hardoon et al., 2004; Rasiwasia et al., 2010; Gong et al., 2014). In the context of CLDR, each language’s texts constitute one modality. CCA has also commonly been used for cross-lingual information retrieval (Vinokourov et al., 2002; Li and Shawe-Taylor, 2004; Udupa and Khapra, 2010). Whereas CCA can handle only two modalities, we need to consider relations between three modalities because we use images in addition to the two languages. Therefore, we focus on an extension of CCA, generalized canonical correlation analysis (GCCA), to handle more than two inputs (Kettenring, 1971). 2.2 Zero-Shot Learning for CLDR Our core idea is to use </context>
</contexts>
<marker>Rasiwasia, Pereira, Coviello, Doyle, Lanckriet, Levy, Vasconcelos, 2010</marker>
<rawString>Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert R.G. Lanckriet, Roger Levy, and Nuno Vasconcelos. 2010. A New Approach to Cross-modal Multimedia Retrieval. Proceedings of the International Conference on Multimedia, pages 251–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Rupnik</author>
<author>Andrej Muhiˇc</author>
<author>Primo ˇSkraba</author>
</authors>
<title>Cross-Lingual Document Retrieval through Hub Languages.</title>
<date>2012</date>
<booktitle>In Neural Information Processing Systems Workshop.</booktitle>
<marker>Rupnik, Muhiˇc, ˇSkraba, 2012</marker>
<rawString>Jan Rupnik, Andrej Muhiˇc, and Primo ˇSkraba. 2012. Cross-Lingual Document Retrieval through Hub Languages. In Neural Information Processing Systems Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Russakovsky</author>
<author>Jia Deng</author>
<author>Hao Su</author>
<author>Jonathan Krause</author>
<author>Sanjeev Satheesh</author>
<author>Sean Ma</author>
<author>Zhiheng Huang</author>
<author>Andrej Karpathy</author>
<author>Aditya Khosla</author>
<author>Michael Bernstein</author>
<author>Alexander C Berg</author>
<author>Li Fei-Fei</author>
</authors>
<title>ImageNet Large Scale Visual Recognition Challenge.</title>
<date>2015</date>
<journal>International Journal of Computer Vision.</journal>
<contexts>
<context position="7749" citStr="Russakovsky et al., 2015" startWordPosition="1206" endWordPosition="1209"> Japanese Features English Features English Features Image Features PCA projection PCA PCA PCA PCA projection GCCA projection GCCA projection GCCA Japanese Features English Features Joint Space Nearest Neighbors Search 586 3.2 Feature Extraction A convolutional neural network (CNN) is one of the most successful deep learning methods for visual recognition. It is known that we can obtain very good image features by taking activation of hidden neurons in a network pre-trained by a sufficiently large dataset (Donahue et al., 2013). We apply the CNN model pre-trained using the ILSVRC2012 dataset (Russakovsky et al., 2015) provided by Caffe (Jia et al., 2014), a standard deep learning software package in the field of visual recognition. As the text feature for both English and Japanese, we use the bag-of-words (BoW) representation and term frequency-inverse document frequency (TF-IDF) weighting. The MeCab (Kudo et al., 2004) library is used to divide Japanese text into words by morphological analysis. No preprocessing approaches like eliminating stop words and stemming, are used. the singularity issue. Despite our training datasets having only two of the three modalities as given in Table 1, we can handle this </context>
<context position="17036" citStr="Russakovsky et al., 2015" startWordPosition="2764" endWordPosition="2767">oW 10.8 ± 2.7 CNN(GoogLeNet), TF-IDF 42.0 ± 4.6 CNN(VGG), TF-IDF 37.8 ± 2.9 CNN(CaffeNet), TF-IDF 29.7 ± 4.4 FisherVector, TF-IDF 12.6 ± 2.7 Table 3: Accuracy of zero-shot learning in multiple image features. The sample size of both [trainE/I] and [train-I/J] is 400. Both the bag-of-words (BoW) and the TF-IDF model are used as text features. 1999) into 64 dimensions by PCA, and used a Gaussian mixture model with 64 components. We used four spatial grids for the final feature extraction. Overall, the order of performance of features corresponds to that known in the image classification domain (Russakovsky et al., 2015). This result indicates that when more powerful image features are used, better performance can be achieved in image-mediated CLDR. 5 Conclusion We proposed an image-mediated learning approach to realize zero-shot or few-shot CLDR. For evaluation, we created and released a new dataset consisting of Japanese, English, and image triplets, based on the widely used Pascal Sentence Dataset. We showed that state-of-the-art CNNbased image features can substantially improve zero-shot CLDR performance. Considering that image features have continued to improve rapidly since the deep learning breakthroug</context>
</contexts>
<marker>Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, Fei-Fei, 2015</marker>
<rawString>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Szegedy</author>
<author>Scott Reed</author>
<author>Pierre Sermanet</author>
<author>Vincent Vanhoucke</author>
<author>Andrew Rabinovich</author>
</authors>
<title>Going deeper with convolutions.</title>
<date>2014</date>
<tech>CoRR abs/1409.4842.</tech>
<contexts>
<context position="15841" citStr="Szegedy et al., 2014" startWordPosition="2567" endWordPosition="2570">t features. 70 60 50 40 30 20 10 0 0 20 40 60 80 100 Sample Size of [train-E/J] Figure 5: Retrieval accuracy using different image features in image-mediated CLDR. The sample size of both [train-E/I] and [train-I/J] is 400. Text features are based on the bag-of-words model. 4.3 Effect of Image Features We also verified the effect of the performance of image features in our framework (see Figure 5 and Table 3). CNN has improved dramatically over the last few years, and many new powerful pre-trained networks are currently available. We compared three different features extracted from GoogLeNet (Szegedy et al., 2014), VGG 16 layers (Chatfield et al., 2014), and CaffeNet (Jia et al., 2014; Krizhevsky et al., 2012). Additionally, we tested the Fisher Vector (Perronnin et al., 2010), which was the standard hand-crafted image feature before deep learning. We extracted features from the pool5/7x7 s1 layer in GoogLeNet, fc6 layer in VGG, and fc6 layer in CaffeNet. For the Fisher Vector, following the standard implementation, we compressed SIFT descriptors (Lowe, Feature Accuracy(%) CNN(GoogLeNet), BoW 37.4 ± 3.8 CNN(VGG), BoW 31.3 ± 3.5 CNN(CaffeNet), BoW 25.1 ± 3.4 FisherVector, BoW 10.8 ± 2.7 CNN(GoogLeNet), </context>
</contexts>
<marker>Szegedy, Reed, Sermanet, Vanhoucke, Rabinovich, 2014</marker>
<rawString>Christian Szegedy, Scott Reed, Pierre Sermanet, Vincent Vanhoucke, and Andrew Rabinovich. 2014. Going deeper with convolutions. CoRR abs/1409.4842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>Mitesh M Khapra</author>
</authors>
<title>Improving the Multilingual User Experience of Wikipedia Using Cross-Language Name Search. Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>492--500</pages>
<contexts>
<context position="4319" citStr="Udupa and Khapra, 2010" startWordPosition="660" endWordPosition="663">le in various cross-modal applications. The most widely used standard method for multimodal learning is canonical correlation analysis (CCA) (Hotelling, 1936), which projects multimodal data into a shared representation. For example, CCA has been successfully used in image retrieval (tag to images) and image annotation (image to tags) (Hardoon et al., 2004; Rasiwasia et al., 2010; Gong et al., 2014). In the context of CLDR, each language’s texts constitute one modality. CCA has also commonly been used for cross-lingual information retrieval (Vinokourov et al., 2002; Li and Shawe-Taylor, 2004; Udupa and Khapra, 2010). Whereas CCA can handle only two modalities, we need to consider relations between three modalities because we use images in addition to the two languages. Therefore, we focus on an extension of CCA, generalized canonical correlation analysis (GCCA), to handle more than two inputs (Kettenring, 1971). 2.2 Zero-Shot Learning for CLDR Our core idea is to use another modality (image) as a hub to indirectly learn the relevance between two different languages. The work by Rupnik et al. is probably the closest to ours (Rupnik et al., 2012). In their study, they used a popular language (e.g., English</context>
</contexts>
<marker>Udupa, Khapra, 2010</marker>
<rawString>Raghavendra Udupa and Mitesh M Khapra. 2010. Improving the Multilingual User Experience of Wikipedia Using Cross-Language Name Search. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 492–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Velden</author>
<author>Yoshio Takane</author>
</authors>
<title>Generalized Canonical Correlation Analysis with Missing Values.</title>
<date>2012</date>
<journal>Computational Statistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="9233" citStr="Velden and Takane, 2012" startWordPosition="1452" endWordPosition="1455">, we compute EEE using E1 only and use a zero matrix for EEJ. 3.4 Nearest Neighbor Search in Joint Space We can find relevant documents in another language by computing the distances from the query documents using the coupled canonical subspaces. Having set Japanese as the query language, we retrieve documents written in English. Nearest neighbors are obtained as follows: 3.3 GCCA j� := arg min d(ziJ, zjE), (2) j GCCA is a generalization of CCA for any m modalities (m = 3 in our case). Although several slightly different versions of GCCA have been proposed (Carroll, 1968; Rupnik et al., 2012; Velden and Takane, 2012), we implement the simplest one (Kettenring, 1971) because GCCA itself is not the main focus of this study. Let E, I, and J denote English, images, and Japanese, respectively. For feature vector xk, Vk E {E, I, J}, let zk = (xk − xk)hk denote its canonical variables. Eij denotes a covariance matrix of modalities i and j where i, j E {E, I, J}. Projection vectors hk are computed such that they maximize the sum of correlations between each pair of modalities obtained by solving the following generalized eigenvalue problem: 1 2 �=P ��(1) EEE 0 0 0 EII 0 �h 0 0 EJJ where h = (hTE, hTI ,hTJ )T . Th</context>
</contexts>
<marker>Velden, Takane, 2012</marker>
<rawString>Michel Velden and Yoshio Takane. 2012. Generalized Canonical Correlation Analysis with Missing Values. Computational Statistics, 27(3):551–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexei Vinokourov</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>1473--1480</pages>
<contexts>
<context position="4267" citStr="Vinokourov et al., 2002" startWordPosition="652" endWordPosition="655"> from multiple media or sensors, has played a key role in various cross-modal applications. The most widely used standard method for multimodal learning is canonical correlation analysis (CCA) (Hotelling, 1936), which projects multimodal data into a shared representation. For example, CCA has been successfully used in image retrieval (tag to images) and image annotation (image to tags) (Hardoon et al., 2004; Rasiwasia et al., 2010; Gong et al., 2014). In the context of CLDR, each language’s texts constitute one modality. CCA has also commonly been used for cross-lingual information retrieval (Vinokourov et al., 2002; Li and Shawe-Taylor, 2004; Udupa and Khapra, 2010). Whereas CCA can handle only two modalities, we need to consider relations between three modalities because we use images in addition to the two languages. Therefore, we focus on an extension of CCA, generalized canonical correlation analysis (GCCA), to handle more than two inputs (Kettenring, 1971). 2.2 Zero-Shot Learning for CLDR Our core idea is to use another modality (image) as a hub to indirectly learn the relevance between two different languages. The work by Rupnik et al. is probably the closest to ours (Rupnik et al., 2012). In thei</context>
</contexts>
<marker>Vinokourov, Shawe-Taylor, Cristianini, 2002</marker>
<rawString>Alexei Vinokourov, John Shawe-Taylor, and Nello Cristianini. 2002. Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis. Advances in Neural Information Processing Systems, pages 1473–1480.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
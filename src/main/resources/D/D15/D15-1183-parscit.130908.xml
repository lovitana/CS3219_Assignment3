<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9799365">
A Generative Word Embedding Model and its Low Rank Positive
Semidefinite Solution
</title>
<author confidence="0.991378">
Shaohua Li&apos;, Jun Zhu2, Chunyan Miao&apos;
</author>
<affiliation confidence="0.90417">
&apos;Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY),
Nanyang Technological University, Singapore
2Tsinghua University, P.R. China
</affiliation>
<email confidence="0.99091">
lish0018@ntu.edu.sg, dcszj@tsinghua.edu.cn, ascymiao@ntu.edu.sg
</email>
<sectionHeader confidence="0.99463" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974740740741">
Most existing word embedding methods
can be categorized into Neural Embedding
Models and Matrix Factorization (MF)-
based methods. However some mod-
els are opaque to probabilistic interpre-
tation, and MF-based methods, typically
solved using Singular Value Decomposi-
tion (SVD), may incur loss of corpus in-
formation. In addition, it is desirable to
incorporate global latent factors, such as
topics, sentiments or writing styles, into
the word embedding model. Since gen-
erative models provide a principled way
to incorporate latent factors, we propose a
generative word embedding model, which
is easy to interpret, and can serve as a
basis of more sophisticated latent factor
models. The model inference reduces to
a low rank weighted positive semidefinite
approximation problem. Its optimization
is approached by eigendecomposition on a
submatrix, followed by online blockwise
regression, which is scalable and avoids
the information loss in SVD. In experi-
ments on 7 common benchmark datasets,
our vectors are competitive to word2vec,
and better than other MF-based methods.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99973175">
The task of word embedding is to model the distri-
bution of a word and its context words using their
corresponding vectors in a Euclidean space. Then
by doing regression on the relevant statistics de-
rived from a corpus, a set of vectors are recovered
which best fit these statistics. These vectors, com-
monly referred to as the embeddings, capture se-
mantic/syntactic regularities between the words.
The core of a word embedding method is the
link function that connects the input — the embed-
dings, with the output — certain corpus statistics.
Based on the link function, the objective function
is developed. The reasonableness of the link func-
tion impacts the quality of the obtained embed-
dings, and different link functions are amenable
to different optimization algorithms, with different
scalability. Based on the forms of the link func-
tion and the optimization techniques, most meth-
ods can be divided into two classes: the traditional
neural embedding models, and more recent low
rank matrix factorization methods.
The neural embedding models use the softmax
link function to model the conditional distribution
of a word given its context (or vice versa) as a
function of the embeddings. The normalizer in the
softmax function brings intricacy to the optimiza-
tion, which is usually tackled by gradient-based
methods. The pioneering work was (Bengio et
al., 2003). Later Mnih and Hinton (2007) propose
three different link functions. However there are
interaction matrices between the embeddings in all
these models, which complicate and slow down
the training, hindering them from being trained
on huge corpora. Mikolov et al. (2013a) and
Mikolov et al. (2013b) greatly simplify the condi-
tional distribution, where the two embeddings in-
teract directly. They implemented the well-known
“word2vec”, which can be trained efficiently on
huge corpora. The obtained embeddings show ex-
cellent performance on various tasks.
Low-Rank Matrix Factorization (MF in short)
methods include various link functions and opti-
mization methods. The link functions are usu-
ally not softmax functions. MF methods aim to
reconstruct certain corpus statistics matrix by the
product of two low rank factor matrices. The ob-
jective is usually to minimize the reconstruction
error, optionally with other constraints. In this
line of research, Levy and Goldberg (2014b) find
that “word2vec” is essentially doing stochastic
weighted factorization of the word-context point-
wise mutual information (PMI) matrix. They then
</bodyText>
<page confidence="0.98">
1599
</page>
<note confidence="0.9850205">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1599–1609,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999899505747127">
factorize this matrix directly as a new method.
Pennington et al. (2014) propose a bilinear regres-
sion function of the conditional distribution, from
which a weighted MF problem on the bigram log-
frequency matrix is formulated. Gradient Descent
is used to find the embeddings. Recently, based
on the intuition that words can be organized in se-
mantic hierarchies, Yogatama et al. (2015) add hi-
erarchical sparse regularizers to the matrix recon-
struction error. With similar techniques, Faruqui
et al. (2015) reconstruct a set of pretrained embed-
dings using sparse vectors of greater dimensional-
ity. Dhillon et al. (2015) apply Canonical Corre-
lation Analysis (CCA) to the word matrix and the
context matrix, and use the canonical correlation
vectors between the two matrices as word embed-
dings. Stratos et al. (2014) and Stratos et al. (2015)
assume a Brown language model, and prove that
doing CCA on the bigram occurrences is equiva-
lent to finding a transformed solution of the lan-
guage model. Arora et al. (2015) assume there is a
hidden discourse vector on a random walk, which
determines the distribution of the current word.
The slowly evolving discourse vector puts a con-
straint on the embeddings in a small text window.
The maximum likelihood estimate of the embed-
dings within this text window approximately re-
duces to a squared norm objective.
There are two limitations in current word em-
bedding methods. The first limitation is, all MF-
based methods map words and their context words
to two different sets of embeddings, and then em-
ploy Singular Value Decomposition (SVD) to ob-
tain a low rank approximation of the word-context
matrix M. As SVD factorizes MTM, some in-
formation in M is lost, and the learned embed-
dings may not capture the most significant regu-
larities in M. Appendix A gives a toy example on
which SVD does not work properly.
The second limitation is, a generative model for
documents parametered by embeddings is absent
in recent development. Although (Stratos et al.,
2014; Stratos et al., 2015; Arora et al., 2015) are
based on generative processes, the generative pro-
cesses are only for deriving the local relationship
between embeddings within a small text window,
leaving the likelihood of a document undefined.
In addition, the learning objectives of some mod-
els, e.g. (Mikolov et al., 2013b, Eq.1), even have
no clear probabilistic interpretation. A genera-
tive word embedding model for documents is not
only easier to interpret and analyze, but more im-
portantly, provides a basis upon which document-
level global latent factors, such as document topics
(Wallach, 2006), sentiments (Lin and He, 2009),
writing styles (Zhao et al., 2011b), can be incor-
porated in a principled manner, to better model the
text distribution and extract relevant information.
Based on the above considerations, we pro-
pose to unify the embeddings of words and con-
text words. Our link function factorizes into three
parts: the interaction of two embeddings capturing
linear correlations of two words, a residual captur-
ing nonlinear or noisy correlations, and the uni-
gram priors. To reduce overfitting, we put Gaus-
sian priors on embeddings and residuals, and ap-
ply Jelinek-Mercer Smoothing to bigrams. Fur-
thermore, to model the probability of a sequence
of words, we assume that the contributions of
more than one context word approximately add up.
Thereby a generative model of documents is con-
structed, parameterized by embeddings and resid-
uals. The learning objective is to maximize the
corpus likelihood, which reduces to a weighted
low-rank positive semidefinite (PSD) approxima-
tion problem of the PMI matrix. A Block Co-
ordinate Descent algorithm is adopted to find an
approximate solution. This algorithm is based
on Eigendecomposition, which avoids information
loss in SVD, but brings challenges to scalability.
We then exploit the sparsity of the weight matrix
and implement an efficient online blockwise re-
gression algorithm. On seven benchmark datasets
covering similarity and analogy tasks, our method
achieves competitive and stable performance.
The source code of this method is provided at
https://github.com/askerlee/topicvec.
</bodyText>
<sectionHeader confidence="0.914614" genericHeader="introduction">
2 Notations and Definitions
</sectionHeader>
<bodyText confidence="0.970494230769231">
Throughout the paper, we always use a uppercase
bold letter as S, V to denote a matrix or set, a low-
ercase bold letter as vwi to denote a vector, a nor-
mal uppercase letter as N, W to denote a scalar
constant, and a normal lowercase letter as si, wi to
denote a scalar variable.
Suppose a vocabulary S = {si, · · · , sW} con-
sists of all the words, where W is the vocab-
ulary size. We further suppose si, · · · , sW are
sorted in decending order of the frequency, i.e.
sl is most frequent, and sW is least frequent.
A document di is a sequence of words di =
(wi1, · · · , wiLi), wij E S. A corpus is a collec-
</bodyText>
<page confidence="0.930913">
1600
</page>
<figure confidence="0.986096333333333">
Name Description
S Vocabulary {sl, · · · , sW}
V Embedding matrix (vs1, · · · , vsW )
D Corpus {dl, · · · , dm}
vsi Embedding of word si
as,s, Bigram residual for si, sj
P˜(si,sj) Empirical probability of si, sj in the corpus
u Unigram probability vector (P(s1),· · ·, P(sW))
A Residual matrix (as,s,)
( )
B Conditional probability matrix P(sj|si)
( )
</figure>
<tableCaption confidence="0.979923">
Table 1: Notation Table
</tableCaption>
<bodyText confidence="0.997859916666667">
tion of M documents D = {d1, · · · , dM}. In the
vocabulary, each word si is mapped to a vector vsi
in N-dimensional Euclidean space.
In a document, a sequence of words is referred
to as a text window, denoted by wi, · · · , wi+l, or
wi:wi+l in shorthand. A text window of chosen
size c before a word wi defines the context of wi
as wi−c, · · · , wi−1. Here wi is referred to as the
focus word. Each context word wi−j and the focus
word wi comprise a bigram wi−j, wi.
The Pointwise Mutual Information between two
words si, sj is defined as
</bodyText>
<equation confidence="0.964495">
P(si, sj)
PMI(si, sj) = log
P(si)P(sj).
</equation>
<sectionHeader confidence="0.966144" genericHeader="method">
3 Link Function of Text
</sectionHeader>
<bodyText confidence="0.999968">
In this section, we formulate the probability of a
sequence of words as a function of their embed-
dings. We start from the link function of bigrams,
which is the building blocks of a long sequence.
Then this link function is extended to a text win-
dow with c context words, as a first-order approx-
imation of the actual probability.
</bodyText>
<subsectionHeader confidence="0.999871">
3.1 Link Function of Bigrams
</subsectionHeader>
<bodyText confidence="0.999509">
We generalize the link function of “word2vec” and
“GloVe” to the following:
</bodyText>
<equation confidence="0.9980275">
{ }
P(si, sj) = exp v&gt; sjvsi + asisj P(si)P(sj) (1)
</equation>
<bodyText confidence="0.999956652173913">
The rationale for (1) originates from the idea of
the Product of Experts in (Hinton, 2002). Sup-
pose different types of semantic/syntactic regu-
larities between si and sj are encoded in differ-
ent dimensions of vsi, vsj. As exp{v&gt; sjvsi} =
Hl exp{vsi,l · vsj,l}, this means the effects of dif-
ferent regularities on the probability are combined
by multiplying together. If si and sj are indepen-
dent, their joint probability should be P(si)P(sj).
In the presence of correlations, the actual joint
probability P(si, sj) would be a scaling of it. The
scale factor reflects how much si and sj are pos-
itively or negatively correlated. Within the scale
factor, v&gt; sjvsi captures linear interactions between
si and sj, the residual asisj captures nonlinear or
noisy interactions. In applications, only v&gt; sjvsi is
of interest. Hence the bigger magnitude v&gt; sjvsi is
of relative to asisj, the better.
Note that we do not assume asisj = asjsi.
This provides the flexibility P(si, sj) =6 P(sj, si),
agreeing with the asymmetry of bigrams in natu-
ral languages. At the same time, v&gt; sjvsi imposes a
symmetric part between P(si, sj) and P(sj, si).
</bodyText>
<equation confidence="0.993173428571429">
(1) is equivalent to
{ }
P(sj|si)=exp v&gt; sjvsi + asisj+ log P(sj) , (2)
P(sj )
vsi + asisj. (3)
(3) of all bigrams is represented in matrix form:
V &gt;V + A = G, (4)
</equation>
<bodyText confidence="0.856207">
where G is the PMI matrix.
</bodyText>
<subsectionHeader confidence="0.855396">
3.1.1 Gaussian Priors on Embeddings
</subsectionHeader>
<bodyText confidence="0.990909428571429">
When (1) is employed on the regression of empir-
ical bigram probabilities, a practical issue arises:
more and more bigrams have zero frequency as
the constituting words become less frequent. A
zero-frequency bigram does not necessarily imply
negative correlation between the two constituting
words; it could simply result from missing data.
But in this case, even after smoothing, (1) will
force v&gt; sjvsi + asisj to be a big negative number,
making vsi overly long. The increased magnitude
of embeddings is a sign of overfitting.
To reduce overfitting of embeddings of infre-
quent words, we assign a Spherical Gaussian prior
N(0, 1
</bodyText>
<equation confidence="0.808481">
2µi I) to vsi:
P(vsi) ∼ exp{−µikvsik2},
</equation>
<bodyText confidence="0.999798">
where the hyperparameter µi increases as the fre-
quency of si decreases.
</bodyText>
<subsectionHeader confidence="0.574516">
3.1.2 Gaussian Priors on Residuals
</subsectionHeader>
<bodyText confidence="0.999978333333333">
We wish v&gt;sjvsi in (1) captures as much corre-
lations between si and sj as possible. Thus the
smaller asisj is, the better. In addition, the more
frequent si, sj is in the corpus, the less noise
there is in their empirical distribution, and thus the
residual asisj should be more heavily penalized.
</bodyText>
<figure confidence="0.58398575">
G PMI matrix PMI(si, sj)
( )
H Bigram empirical probability matrix P˜ (si, sj)
P(sj|si) &gt;
log =
s
j
v
</figure>
<page confidence="0.903006">
1601
</page>
<bodyText confidence="0.999652666666667">
To this end, we penalize the residual asisj
by f(˜P(si, sj))a2sisj, where f(·) is a nonnega-
tive monotonic transformation, referred to as the
weighting function. Let hij denote P˜(si, sj), then
the total penalty of all residuals are the square of
the weighted Frobenius norm of A:
</bodyText>
<equation confidence="0.9793695">
X f(hij)a2sisj = kAk2f(H). (5)
si,sjES
</equation>
<bodyText confidence="0.9987185">
By referring to “GloVe”, we use the following
weighting function, and find it performs well:
</bodyText>
<equation confidence="0.9186252">
√hij
phij &lt; Ccut, i =6 j
1 phij ≥ Ccut, i =6 j
Ccut
0 i = j
</equation>
<bodyText confidence="0.999955888888889">
where Ccut is chosen to cut the most frequent
0.02% of the bigrams off at 1. When si = sj, two
identical words usually have much smaller proba-
bility to collocate. Hence P˜(si, si) does not reflect
the true correlation of a word to itself, and should
not put constraints to the embeddings. We elimi-
nate their effects by setting f(hii) to 0.
If the domain of A is the whole space RW&apos;W,
then this penalty is equivalent to a Gaussian prior
</bodyText>
<equation confidence="0.944803">
� �
N 0, 1 on each asisj. The variances of the
2f(hij)
</equation>
<bodyText confidence="0.9853915">
Gaussians are determined by the bigram empirical
probability matrix H.
</bodyText>
<subsectionHeader confidence="0.771984">
3.1.3 Jelinek-Mercer Smoothing of Bigrams
</subsectionHeader>
<bodyText confidence="0.999955">
As another measure to reduce the impact of miss-
ing data, we apply the commonly used Jelinek-
Mercer Smoothing (Zhai and Lafferty, 2004)
to smooth the empirical conditional probability
</bodyText>
<equation confidence="0.991962">
˜ ˜
P(sj|si) by the unigram probability P(sj) as:
˜Psmoothed(sj|si) = (1−κ) P˜(sj|si)+κP(sj). (6)
</equation>
<bodyText confidence="0.8352545">
Accordingly, the smoothed bigram empirical
joint probability is defined as
</bodyText>
<equation confidence="0.992963">
P˜(si, sj) = (1−κ) P˜(si, sj)+κP(si)P(sj). (7)
</equation>
<bodyText confidence="0.99997525">
In practice, we find κ = 0.02 yields good re-
sults. When κ ≥ 0.04, the obtained embeddings
begin to degrade with κ, indicating that smoothing
distorts the true bigram distributions.
</bodyText>
<subsectionHeader confidence="0.999781">
3.2 Link Function of a Text Window
</subsectionHeader>
<bodyText confidence="0.999051368421053">
In the previous subsection, a regression link func-
tion of bigram probabilities is established. In
this section, we adopt a first-order approximation
based on Information Theory, and extend the link
function to a longer sequence w0, · · · , wc−1, wc.
Decomposing a distribution conditioned on n
random variables as the conditional distributions
on its subsets roots deeply in Information The-
ory. This is an intricate problem because there
could be both (pointwise) redundant information
and (pointwise) synergistic information among the
conditioning variables (Williams and Beer, 2010).
They are both functions of the PMI. Based on an
analysis of the complementing roles of these two
types of pointwise information, we assume they
are approximately equal and cancel each other
when computing the pointwise interaction infor-
mation. See Appendix B for a detailed discussion.
Following the above assumption, we have
</bodyText>
<equation confidence="0.989952">
PMI(w2; w0, w1) ≈ PMI(w2; w0)+PMI(w2; w1):
log P(w0,w1|w2) log P (w0 |w2)+logP(w1|w2)
P(w0, w 1 ) P (w 0 ) P (w 1 ) .
Plugging (1) and (3) into the above, we obtain
P(w0, w1, w2)
~ X2 (T 2 �log P (wi) .
≈ exp vwivwj + awiwj) + X
i,j=0 i=0
i7�j
</equation>
<bodyText confidence="0.999993">
We extend the above assumption to that the
pointwise interaction information is still close to
0 within a longer text window. Accordingly the
above equation extends to a context of size c &gt; 2:
</bodyText>
<equation confidence="0.9032339">
P(w0, · · · , wc)
�log P (wi) .
From it derives the conditional distribution of
wc, given its context w0, · · · , wc−1:
P (w0, · · · , wc)
P(wc  |w0 : wc−1)= P(w0, ·
·· , wc−1)
c− 1 c− 1
≈P(wc) exp vwcT X vwi + X awiwc. ( 8)
i=0 i=0
</equation>
<sectionHeader confidence="0.998739" genericHeader="method">
4 Generative Process and Likelihood
</sectionHeader>
<bodyText confidence="0.9990114">
We proceed to assume the text is generated from a
Markov chain of order c, i.e., a word only depends
on words within its context of size c. Given the
hyperparameter µ = (µ1, · · ·, µW), the generative
process of the whole corpus is:
</bodyText>
<listItem confidence="0.9172135">
1. For each word si, draw the embedding vsi
from N(0, 1
2µi I);
2. For each bigram si, sj, draw the residual
asisj from N(0, 2 f(1ij )) ;
3. For each document di, for the j-th word,
draw word wij from S with probability
P(wij  |wi,j−c : wi,j−1) defined by (8).
</listItem>
<figure confidence="0.764526625">
⎧
⎨⎪⎪
⎪⎪⎩
f(hij) =
,
exp (T c
l i,j=0 vwivwj + awiwj) + X
i=,4j i=0
</figure>
<page confidence="0.861534">
1602
</page>
<figureCaption confidence="0.999933">
Figure 1: The Graphical Model of PSDVec
</figureCaption>
<bodyText confidence="0.9999474">
The above generative process for a document d is
presented as a graphical model in Figure 1.
Based on this generative process, the probabil-
ity of a document di can be derived as follows,
given the embeddings and residuals V , A:
</bodyText>
<equation confidence="0.9715608">
P(di|V , A)
j−1 j−1
&gt;
P(wij) exp vwij X vwik+ X awikwij .
k=j−c k=j−c
</equation>
<sectionHeader confidence="0.965123" genericHeader="method">
5 Learning Algorithm
</sectionHeader>
<subsectionHeader confidence="0.995543">
5.1 Learning Objective
</subsectionHeader>
<bodyText confidence="0.99968275">
The learning objective is to find the embeddings
V that maximize the corpus log-likelihood (9).
Let xij denote the (smoothed) frequency of bi-
gram si, sj in the corpus. Then (9) is sorted as:
</bodyText>
<equation confidence="0.994271625">
log p(D, V , A)
=C0 − log Z(H,µ) − kAk2f(H) − XW µikvsik2
i=1
+
W,W X xij(v&gt;
sivsj + asisj). (10)
i,j=1
PW,W
</equation>
<bodyText confidence="0.98280925">
As the corpus size increases,
i,j=1 xij(v&gt;sivsj+asisj) will dominate the
parameter prior terms. Then we can ignore the
prior terms when maximizing (10).
</bodyText>
<equation confidence="0.769506236842106">
maxXxij (si
v&gt;
vsj+asisj)
= (X
xij) · max X Psmoothed(si, sj) log P(si, sj).
As both {˜Psmoothed(si, sj)} and {P(si, sj)}
sum to 1, the above sum is maximized when
P(si, sj) = ˜Psmoothed(si, sj).
The maximum likelihood estimator is then:
vw0 vw1 ···
vsi
µi
V A
vwc
aij
hij
d
Li
Y
j=1
P(sj|si) = ˜Psmoothed(sj|si),
&gt;
vsivsj + asisj =log
˜Psmoothed(sj|si)
P(sj) .
(11)
The complete-data likelihood of the corpus is:
p(D, V , A)
W,W � � YM
I Y
N(0, 0, 1
) N p(di|V, A)
2µi 2f(hij)
i,j=1 i=1
j−1 j−1
&gt;
P(wij) exp vwij X vwik+ X awikwij ,
k=j−c k=j−c
</equation>
<bodyText confidence="0.9207865">
where Z(H, µ) is the normalizing constant.
Taking the logarithm of both sides of
</bodyText>
<equation confidence="0.959449717948718">
p(D, A, V ) yields
si,sjES
G* = log B* − logu ⊗ (1 · · · 1), (12)
where “⊗” is the outer product.
Now we fix the values of v&gt;
sivsj + asisj at the
above optimal. The corpus likelihood becomes
log p(D, V , A) =C1 − kAk2 f(H) − XW µikvsik2,
i=1
subject to V &gt;V + A = G*, (13)
n
1
Z(H, µ) exp −
2
f (hi,j)asisj −
µikvsik2o
W,W X
i,j=1
XW
i=1
YW
i=1
M,Li
Y·
i,j=1
Writing (11) in matrix form: (˜Psmoothed
B* = (sj|si))
log p(D, V , A) where C1 = C0 + P xij log ˜Psmoothed(si, sj) −
log Z(H, µ) is constant.
=C0 − log Z(H,µ) − kAk2f(H)− XW µikvsik2
i=1
� �
X X X
+ v&gt; vwik+ awikwij , (9)
wij
where C0 = PM,Li i,j=1 log P (wij) is constant.
M,Li j−1 j−1
i k=j k=j
,j=1 −c −c
</equation>
<subsectionHeader confidence="0.787647">
5.2 Learning V as Low Rank PSD
Approximation
</subsectionHeader>
<bodyText confidence="0.9998014">
Once G* has been estimated from the corpus using
(12), we seek V that maximizes (13). This is to
find the maximum a posteriori (MAP) estimates
of V , A that satisfy V &gt;V + A = G*. Applying
this constraint to (13), we obtain
</bodyText>
<page confidence="0.932873">
1603
</page>
<bodyText confidence="0.91189">
Algorithm 1 BCD algorithm for finding a unreg-
ularized rank-N weighted PSD approximant.
Input: matrix G*, weight matrix W = f(H),
iteration number E, rank N
</bodyText>
<equation confidence="0.989316375">
Randomly initialize X(0)
fort = 1,··· ,Edo
Gt = W o G* + (1 − W) o X(t−1)
X(t) = PSD Approximate(Gt, N)
end for
λ, Q = Eigen Decomposition(X(T))
V * = diag(λ12 [1:N]) · QT[1:N]
Output: V *
</equation>
<bodyText confidence="0.991951137931035">
Let X = V TV . Then X is positive semidef-
inite of rank N. Finding V that minimizes (14)
is equivalent to finding a rank-N weighted posi-
tive semidefinite approximant X of G*, subject to
Tikhonov regularization. This problem does not
admit an analytic solution, and can only be solved
using local optimization methods.
First we consider a simpler case where all the
words in the vocabulary are enough frequent, and
thus Tikhonov regularization is unnecessary. In
this case, we set Vµi = 0, and (14) becomes an
unregularized optimization problem. We adopt the
Block Coordinate Descent (BCD) algorithm1 in
(Srebro et al., 2003) to approach this problem. The
original algorithm is to find a generic rank-N ma-
trix for a weighted approximation problem, and
we tailor it by constraining the matrix within the
positive semidefinite manifold.
We summarize our learning algorithm in Al-
gorithm 1. Here “o” is the entry-wise prod-
uct. We suppose the eigenvalues λ returned by
Eigen Decomposition(X) are in descending or-
der. QT[1:N] extracts the 1 to N rows from QT.
One key issue is how to initialize X. Srebro et
al. (2003) suggest to set X(0)=G*, and point out
that X(0) = 0 is far from a local optimum, thus
requires more iterations. However we find G* is
also far from a local optimum, and this setting con-
verges slowly too. Setting X(0) = G*/2 usually
</bodyText>
<footnote confidence="0.9938365">
1It is referred to as an Expectation-Maximization algo-
rithm by the original authors, but we think this is a misnomer.
</footnote>
<bodyText confidence="0.99779275">
yields a satisfactory solution in a few iterations.
The subroutine PSD Approximate() computes
the unweighted nearest rank-N PSD approxima-
tion, measured in F-norm (Higham, 1988).
</bodyText>
<subsectionHeader confidence="0.989067">
5.3 Online Blockwise Regression of V
</subsectionHeader>
<bodyText confidence="0.9968545">
In Algorithm 1, the essential subroutine
PSD Approximate() does eigendecomposi-
tion on Gt, which is dense due to the logarithm
transformation. Eigendecomposition on a W x W
dense matrix requires O(W2) space and O(W3)
time, difficult to scale up to a large vocabulary. In
addition, the majority of words in the vocabulary
are infrequent, and Tikhonov regularization is
necessary for them.
It is observed that, as words become less fre-
quent, fewer and fewer words appear around them
to form bigrams. Remind that the vocabulary
S = {s1, · · · , sW} are sorted in decending or-
der of the frequency, hence the lower-right blocks
of H and f(H) are very sparse, and cause these
blocks in (14) to contribute much less penalty rela-
tive to other regions. Therefore these blocks could
be ignored when doing regression, without sacri-
ficing too much accuracy. This intuition leads to
the following online blockwise regression.
The basic idea is to select a small set (e.g.
30,000) of the most frequent words as the core
words, and partition the remaining noncore words
into sets of moderate sizes. Bigrams consist-
ing of two core words are referred to as core bi-
grams, which correspond to the top-left blocks of
G and f(H). The embeddings of core words
are learned approximately using Algorithm 1, on
the top-left blocks of G and f(H). Then we fix
the embeddings of core words, and find the em-
beddings of each set of noncore words in turn.
After ignoring the lower-right regions of G and
f(H) which correspond to bigrams of two non-
core words, the quadratic terms of noncore em-
beddings are ignored. Consequently, finding these
embeddings becomes a weighted ridge regression
problem, which can be solved efficiently in closed-
form. Finally we combine all embeddings to get
the embeddings of the whole vocabulary. The de-
tails are as follows:
</bodyText>
<listItem confidence="0.99439875">
1. Partition S into K consecutive groups
S1, · · · , Sk. Take K = 3 as an example.
The first group is core words;
2. Accordingly partition G into K x K blocks,
</listItem>
<equation confidence="0.7727985">
arg max log p(D, V , A)
V
= arg min iiG*−V TV iif(H) + W µiiivsiii2. (14)
V i=1
</equation>
<page confidence="0.977227">
1604
</page>
<listItem confidence="0.833217">
3. Solve V &gt;1 V 1 + A11 = G11 using Algorithm
1, and obtain core embeddings V *1;
4. Set V1 = V*1, and find V2* that minimizes
the total penalty of the 12-th and 21-th blocks
of residuals (the 22-th block is ignored due to
its high sparsity):
</listItem>
<equation confidence="0.964170583333333">
arg min
V 2 kG12 − V &gt;1 V 2k2
f(H)12
+ kG21 − V2 &gt; V1k2f(H)21 + � µikvsik2
siES2
= arg min
V 2 �kG12−V &gt; 1 V 2k2¯f(H)12+ µikvsik2,
siES2
where f(H)12 = f(H)12 + f(H)&gt;21;
� �
G12 = G12 ◦ f(H)12 + G&gt; 21 ◦ f(H)&gt; 21
/ (f (H)12 + f (H)21) is the weighted aver-
</equation>
<bodyText confidence="0.9999506">
age of G12 and G&gt;21, “◦” and “/” are element-
wise product and division, respectively. The
columns in V 2 are independent, thus for each
vsi, it is a separate weighted ridge regression
problem, whose solution is (Holland, 1973):
</bodyText>
<equation confidence="0.705905">
v*
si =(V &gt; 1 diag( ¯fi)V 1+µiI)−1V &gt; 1 diag( ¯fi)¯gi,
</equation>
<bodyText confidence="0.999461">
where ¯fi and ¯gi are columns corresponding
to si in ¯f(H)12 and G12, respectively;
</bodyText>
<listItem confidence="0.996068166666667">
5. For any other set of noncore words Sk, find
V*k that minimizes the total penalty of the 1k-
th and k1-th blocks, ignoring all other kj-th
and jk-th blocks;
6. Combine all subsets of embeddings to form
V*. Here V* = (V*1, V*2, V*3).
</listItem>
<sectionHeader confidence="0.992848" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999990333333333">
We trained our model along with a few state-of-
the-art competitors on Wikipedia, and evaluated
the embeddings on 7 common benchmark sets.
</bodyText>
<subsectionHeader confidence="0.981515">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.972519">
Our own method is referred to as PSD. The com-
petitors include:
</bodyText>
<listItem confidence="0.98732">
• (Mikolov et al., 2013b): word2vec2, or
SGNS in some literature;
</listItem>
<footnote confidence="0.628576">
2https://code.google.com/p/word2vec/
</footnote>
<listItem confidence="0.9091607">
• (Levy and Goldberg, 2014b): the PPMI ma-
trix without dimension reduction, and SVD
of PPMI matrix, both yielded by hyperwords;
• (Pennington et al., 2014): GloVe3;
• (Stratos et al., 2015): Singular4, which does
SVD-based CCA on the weighted bigram fre-
quency matrix;
• (Faruqui et al., 2015): Sparse5, which learns
new sparse embeddings in a higher dimen-
sional space from pretrained embeddings.
</listItem>
<bodyText confidence="0.999607285714286">
All models were trained on the English Wikipedia
snapshot in March 2015. After removing non-
textual elements and non-English words, 2.04 bil-
lion words were left. We used the default hyperpa-
rameters in Hyperwords when training PPMI and
SVD. Word2vec, GloVe and Singular were trained
with their own default hyperparameters.
The embedding sets PSD-Reg-180K and PSD-
Unreg-180K were trained using our online block-
wise regression. Both sets contain the embed-
dings of the most frequent 180,000 words, based
on 25,000 core words. PSD-Unreg-180K was
traind with all µi = 0, i.e. disabling Tikhonov
regularization. PSD-Reg-180K was trained with
</bodyText>
<equation confidence="0.997239333333333">
µi = { 2 i ∈ [25001, 80000]
4 i ∈ [80001,130000]
8 i ∈ [130001,180000] , i.e. increased
</equation>
<bodyText confidence="0.9739344">
regularization as the sparsity increases. To con-
trast with the batch learning performance, the per-
formance of PSD-25K is listed, which contains the
core embeddings only. PSD-25K took advantages
that it contains much less false candidate words,
and some test tuples (generally harder ones) were
not evaluated due to missing words, thus its scores
are not comparable to others.
Sparse was trained with PSD-180K-reg as the
input embeddings, with default hyperparameters.
The benchmark sets are almost identical to
those in (Levy et al., 2015), except that (Luong et
al., 2013)’s Rare Words is not included, as many
rare words are cut off at the frequency 100, mak-
ing more than 1/3 of test pairs invalid.
Word Similarity There are 5 datasets: Word-
Sim Similarity (WS Sim) and WordSim Related-
ness (WS Rel) (Zesch et al., 2008; Agirre et al.,
2009), partitioned from WordSim353 (Finkelstein
et al., 2002); Bruni et al. (2012)’s MEN dataset;
</bodyText>
<footnote confidence="0.999869">
3http://nlp.stanford.edu/projects/glove/
4https://github.com/karlstratos/singular
5https://github.com/mfaruqui/sparse-coding
</footnote>
<table confidence="0.588076">
Partition f(H),A in the same way.
G11, f(H)11, A11 correspond to core bi-
</table>
<figure confidence="0.969716">
(V1
grams. Partition V into ����
S1
V 2
�
V 3
;
����
S2
����
S3
�
in this example as �G21
G31
G12 G13
G22 G23
G32 G33
�
� .
G11
</figure>
<page confidence="0.956951">
1605
</page>
<table confidence="0.998860909090909">
Similarity Tasks Analogy Tasks
Method WS Sim WS Rel MEN Turk SimLex Google MSR
word2vec 0.742 0.543 0.731 0.663 0.395 0.734 / 0.742 0.650 / 0.674
PPMI 0.735 0.678 0.717 0.659 0.308 0.476 / 0.524 0.183 / 0.217
SVD 0.687 0.608 0.711 0.524 0.270 0.230 / 0.240 0.123 / 0.113
GloVe 0.759 0.630 0.756 0.641 0.362 0.535 / 0.544 0.408 / 0.435
Singular 0.763 0.684 0.747 0.581 0.345 0.440 / 0.508 0.364 / 0.399
Sparse 0.739 0.585 0.725 0.625 0.355 0.240 / 0.282 0.253 / 0.274
PSD-Reg-180K 0.792 0.679 0.764 0.676 0.398 0.602 / 0.623 0.465 / 0.507
PSD-Unreg-180K 0.786 0.663 0.753 0.675 0.372 0.566 / 0.598 0.424 / 0.468
PSD-25K 0.801 0.676 0.765 0.678 0.393 0.671 / 0.695 0.533 / 0.586
</table>
<tableCaption confidence="0.999656">
Table 2: Performance of each method across different tasks.
</tableCaption>
<bodyText confidence="0.9995835">
Radinsky et al. (2011)’s Mechanical Turk dataset;
and (Hill et al., 2014)’s SimLex-999 dataset. The
embeddings were evaluated by the Spearman’s
rank correlation with the human ratings.
Word Analogy The two datasets are MSR’s
analogy dataset (Mikolov et al., 2013c), with 8000
questions, and Google’s analogy dataset (Mikolov
et al., 2013a), with 19544 questions. After filtering
questions involving out-of-vocabulary words, i.e.
words that appear less than 100 times in the cor-
pus, 7054 instances in MSR and 19364 instances
in Google were left. The analogy questions were
answered using 3CosAdd as well as 3CosMul pro-
posed by Levy and Goldberg (2014a).
</bodyText>
<subsectionHeader confidence="0.965367">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.996640181818182">
Table 2 shows the results on all tasks. Word2vec
significantly outperformed other methods on anal-
ogy tasks. PPMI and SVD performed much worse
on analogy tasks than reported in (Levy et al.,
2015), probably due to sub-optimal hyperparam-
eters. This suggests their performance is unstable.
The new embeddings yielded by Sparse systemat-
ically degraded compared to the old embeddings,
contradicting the claim in (Faruqui et al., 2015).
Our method PSD-Reg-180K performed well
consistently, and is best in 4 similarity tasks.
It performed worse than word2vec on analogy
tasks, but still better than other MF-based meth-
ods. By comparing to PSD-Unreg-180K, we see
Tikhonov regularization brings 1-4% performance
boost across tasks. In addition, on similarity tasks,
online blockwise regression only degrades slightly
compared to batch factorization. Their perfor-
mance gaps on analogy tasks were wider, but this
might be explained by the fact that some hard
cases were not counted in PSD-25K’s evaluation,
due to its limited vocabulary.
</bodyText>
<sectionHeader confidence="0.985756" genericHeader="method">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999988739130435">
In this paper, inspired by the link functions in
previous works, with the support from Informa-
tion Theory, we propose a new link function of a
text window, parameterized by the embeddings of
words and the residuals of bigrams. Based on the
link function, we establish a generative model of
documents. The learning objective is to find a set
of embeddings maximizing their posterior likeli-
hood given the corpus. This objective is reduced to
weighted low-rank positive-semidefinite approxi-
mation, subject to Tikhonov regularization. Then
we adopt a Block Coordinate Descent algorithm,
jointly with an online blockwise regression algo-
rithm to find an approximate solution. On seven
benchmark sets, the learned embeddings show
competitive and stable performance.
In the future work, we will incorporate global
latent factors into this generative model, such as
topics, sentiments, or writing styles, and develop
more elaborate models of documents. Through
learning such latent factors, important summary
information of documents would be acquired,
which are useful in various applications.
</bodyText>
<sectionHeader confidence="0.98706" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.96757225">
We thank Omer Levy, Thomas Mach, Peilin Zhao,
Mingkui Tan, Zhiqiang Xu and Chunlin Wu for
their helpful discussions and insights. This re-
search is supported by the National Research
Foundation, Prime Minister’s Office, Singapore
under its IDM Futures Funding Initiative and ad-
ministered by the Interactive and Digital Media
Programme Office.
</bodyText>
<page confidence="0.988581">
1606
</page>
<sectionHeader confidence="0.973123" genericHeader="method">
Appendix A Possible Trap in SVD
</sectionHeader>
<bodyText confidence="0.998994777777778">
Suppose M is the bigram matrix of interest. SVD
embeddings are derived from the low rank approx-
imation of MTM, by keeping the largest singular
values/vectors. When some of these singular val-
ues correspond to negative eigenvalues, undesir-
able correlations might be captured. The follow-
ing is an example of approximating a PMI matrix.
A vocabulary consists of 3 words s1, s2, s3.
Two corpora derive two PMI 0. 6 matrices:
</bodyText>
<equation confidence="0.879402333333333">
1.4
\
MM = ( 0 2 / , M(2) = I −1.6 −0.2 2 I .
</equation>
<bodyText confidence="0.974887">
They have identical left singular matrix and sin-
gular values (3, 2, 1), but their eigenvalues are
(3, 2,1) and (−3, 2, 1), respectively.
In a rank-2 approximation, the largest two
singular values/vectors are kept, and M(1) and
M(2) yield identical SVD embeddings V =
( 0.45 0.89 0
0 0 1 )
the algorithm, without affecting the validity of the
following conclusion). The embeddings of s1 and
s2 (columns 1 and 2 of V ) point at the same di-
rection, suggesting they are positively correlated.
However as M(2)
</bodyText>
<equation confidence="0.87979">
1,2 = M(2)
2,1 = −1.6 &lt; 0, they are
</equation>
<bodyText confidence="0.97014915">
actually negatively correlated in the second cor-
pus. This inconsistency is because the principal
eigenvalue of M(2) is negative, and yet the corre-
sponding singular value/vector is kept.
When using eigendecomposition, the largest
two positive eigenvalues/eigenvectors are kept.
M(1) yields the same embeddings V . M(2)
yields V (2) = (−0.89 0.45 0 ) , which correctly
0 0 1.41
preserves the negative correlation between s1, s2.
Appendix B Information Theory
Redundant information refers to the reduced un-
certainty by knowing the value of any one of the
conditioning variables (hence redundant). Syner-
gistic information is the reduced uncertainty as-
cribed to knowing all the values of conditioning
variables, that cannot be reduced by knowing the
value of any variable alone (hence synergistic).
The mutual information I(y; xi) and the redun-
dant information Rdn(y; x1, x2) are defined as:
</bodyText>
<equation confidence="0.880430666666667">
P(y|xi)
I(y; xi) = EP(xi,y)[log
]
</equation>
<bodyText confidence="0.963768333333333">
The synergistic information Syn(y; x1, x2) is
defined as the PI-function in (Williams and Beer,
2010), skipped here.
</bodyText>
<figureCaption confidence="0.7424145">
Figure 2: Different types of information among
3 random variables y, x1, x2. I(y; x1, x2) is
the mutual information between y and (x1, x2).
Rdn(y; x1, x2) and Syn(y; x1, x2) are the redun-
dant information and synergistic information be-
tween x1, x2, conditioning y, respectively.
</figureCaption>
<bodyText confidence="0.955674">
The interaction information Int(x1, x2, y) mea-
sures the relative strength of Rdn(y; x1, x2) and
Syn(y; x1, x2) (Timme et al., 2014):
</bodyText>
<equation confidence="0.9979354">
Int(x1, x2, y)
=Syn(y; x1, x2) − Rdn(y; x1, x2)
=I(y; x1, x2) − I(y; x1) − I(y; x2)
P (x1)P (x2)P (y)P (x1, x2, y)
=EP (x1,x2,y)[log P (x1, x2)P (x1, y)P (x2, y) ]
</equation>
<bodyText confidence="0.91275575">
Figure 2 shows the relationship of different
information among 3 random variables y, x1, x2
(based on Fig.1 in (Williams and Beer, 2010)).
PMI is the pointwise counterpart of mutual
information I. Similarly, all the above concepts
have their pointwise counterparts, obtained by
dropping the expectation operator. Specifically,
the pointwise interaction information is defined as
</bodyText>
<equation confidence="0.989101333333333">
PInt(x1, x2, y) = PMI(y; x1, x2) − PMI(y; x1) −
PMI(y; x2) = log P(x1)P (x2)P (y)P (x1,x2,y)
P (x1,x2)P(x1,y)P(x2,y) .
</equation>
<bodyText confidence="0.964179230769231">
If we know PInt(x1, x2, y), we can recover
PMI(y; x1, x2) from the mutual information over
the variable subsets, and then recover the joint
distribution P(x1, x2, y).
As the pointwise redundant information
PRdn(y; x1, x2) and the pointwise synergistic
information PSyn(y; x1, x2) are both higher-
order interaction terms, their magnitudes are
usually much smaller than the PMI terms. We
assume they are approximately equal, and thus
cancel each other when computing PInt. Given
this, PInt is always 0. In the case of three
words w0, w1, w2, PInt(w0, w1, w2) = 0 leads to
</bodyText>
<equation confidence="0.91887025">
PMI(w2; w0, w1) = PMI(w2; w0)+PMI(w2; w1).
(the rows may be scaled depending on
P(y)
Rdn(y; x1, x2) = EP(y) Imin EP(xi|y) [logPp(y)i) ]J
</equation>
<page confidence="0.992774">
1607
</page>
<sectionHeader confidence="0.981948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995687066666667">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19–27. Association for Computational Lin-
guistics.
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu
Ma, and Andrej Risteski. 2015. Random walks
on discourse spaces: a new generative language
model with applications to semantic word embed-
dings. ArXiv e-prints, arXiv:1502.03520 [cs.LG].
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, pages 1137–1155.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. The Journal of
Machine Learning Research, 3:993–1022.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145. Asso-
ciation for Computational Linguistics.
Scott C. Deerwester, Susan T Dumais, and Richard A.
Harshman. 1990. Indexing by latent semantic anal-
ysis. J. Am. Soc. Inf. Sci.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Proceedings ofAdvances in Neural Informa-
tion Processing Systems, pages 199–207.
Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar.
2015. Eigenwords: Spectral word embeddings. The
Journal of Machine Learning Research.
Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris
Dyer, and Noah A. Smith. 2015. Sparse overcom-
plete word vector representations. In Proceedings of
ACL 2015.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Trans. Inf. Syst., 20(1):116–
131, January.
Amir Globerson, Gal Chechik, Fernando Pereira, and
Naftali Tishby. 2007. Euclidean embedding of co-
occurrence data. Journal of Machine Learning Re-
search, vol. 8 (2007):2265–2295, Oct.
Nicholas J. Higham. 1988. Computing a nearest sym-
metric positive semidefinite matrix. Linear Algebra
and its Applications, 103(0):103 – 118.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. CoRR, abs/1408.3456.
Geoffrey Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural Com-
putation, 14(8):1771–1800.
Paul W. Holland. 1973. Weighted Ridge Regression:
Combining Ridge and Robust Regression Methods.
NBER Working Papers 0011, National Bureau of
Economic Research, Inc, September.
Daniel Hsu, Sham M Kakade, and Tong Zhang. 2012.
A spectral algorithm for learning hidden markov
models. Journal of Computer and System Sciences,
78(5):1460–1480.
Omer Levy and Yoav Goldberg. 2014a. Linguistic reg-
ularities in sparse and explicit word representations.
In Proceedings of CoNLL-2014, page 171.
Omer Levy and Yoav Goldberg. 2014b. Neural word
embeddings as implicit matrix factorization. In Pro-
ceedings of NIPS 2014.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM conference on Informa-
tion and Knowledge Management, pages 375–384.
ACM.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. CoNLL-2013, 104.
Thomas Mach. 2012. Eigenvalue Algorithms for Sym-
metric Hierarchical Matrices. Dissertation, Chem-
nitz University of Technology.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR 2013.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS 2013, pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of HLT-
NAACL 2013, pages 746–751.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th International Conference
on Machine learning, pages 641–648. ACM.
</reference>
<page confidence="0.833931">
1608
</page>
<reference confidence="0.999764393939394">
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: Computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th International Conference on World Wide Web,
WWW ’11, pages 337–346, New York, NY, USA.
ACM.
Nathan Srebro, Tommi Jaakkola, et al. 2003. Weighted
low-rank approximations. In Proceedings of ICML
2003, volume 3, pages 720–727.
Karl Stratos, Do-kyum Kim, Michael Collins, and
Daniel Hsu. 2014. A spectral algorithm for learn-
ing class-based n-gram models of natural language.
In Proceedings of the Association for Uncertainty in
Artificial Intelligence.
Karl Stratos, Michael Collins, and Daniel Hsu. 2015.
Model-based word embeddings from decomposi-
tions of count matrices. In Proceedings of ACL
2015.
Mingkui Tan, Ivor W. Tsang, Li Wang, Bart Vander-
eycken, and Sinno Jialin Pan. 2014. Riemannian
pursuit for big matrix recovery. In Proceedings of
ICML 2014, pages 1539–1547.
Nicholas Timme, Wesley Alford, Benjamin Flecker,
and John M Beggs. 2014. Synergy, redundancy,
and multivariate information measures: an experi-
mentalist’s perspective. Journal of Computational
Neuroscience, 36(2):119–140.
Hanna M Wallach. 2006. Topic modeling: beyond
bag-of-words. In Proceedings of the 23rd interna-
tional conference on Machine learning, pages 977–
984. ACM.
Paul L Williams and Randall D Beer. 2010. Non-
negative decomposition of multivariate information.
arXiv preprint arXiv:1004.2515.
Yan Yan, Mingkui Tan, Ivor Tsang, Yi Yang, Chengqi
Zhang, and Qinfeng Shi. 2015. Scalable maximum
margin matrix factorization by active riemannian
subspace search. In Proceedings of IJCAI 2015.
Dani Yogatama, Manaal Faruqui, Chris Dyer, and
Noah A Smith. 2015. Learning word representa-
tions with hierarchical sparse coding. In Proceed-
ings of ICML 2015.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych.
2008. Using wiktionary for computing semantic re-
latedness. In Proceedings of AAAI 2008, volume 8,
pages 861–866.
Chengxiang Zhai and John Lafferty. 2004. A study of
smoothing methods for language models applied to
information retrieval. ACM Transactions on Infor-
mation Systems (TOIS), 22(2):179–214.
Peilin Zhao, Steven CH Hoi, and Rong Jin. 2011a.
Double updating online learning. The Journal of
Machine Learning Research, 12:1587–1615.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing
He, Ee-Peng Lim, Hongfei Yan, and Xiaoming
Li. 2011b. Comparing twitter and traditional me-
dia using topic models. In Advances in Informa-
tion Retrieval (Proceedings of the 33rd Annual Eu-
ropean Conference on Information Retrieval Re-
search), pages 338–349. Springer.
</reference>
<page confidence="0.996054">
1609
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.690598">
<title confidence="0.998447">A Generative Word Embedding Model and its Low Rank Semidefinite Solution</title>
<author confidence="0.996124">Jun Chunyan</author>
<affiliation confidence="0.947571">NTU-UBC Research Centre of Excellence in Active Living for the Elderly Nanyang Technological University, University, P.R.</affiliation>
<email confidence="0.857053">lish0018@ntu.edu.sg,dcszj@tsinghua.edu.cn,ascymiao@ntu.edu.sg</email>
<abstract confidence="0.998511714285714">Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)based methods. However some models are opaque to probabilistic interpretation, and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information. In addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since generative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semidefinite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Arora</author>
<author>Yuanzhi Li</author>
<author>Yingyu Liang</author>
<author>Tengyu Ma</author>
<author>Andrej Risteski</author>
</authors>
<title>Random walks on discourse spaces: a new generative language model with applications to semantic word embeddings. ArXiv e-prints, arXiv:1502.03520 [cs.LG].</title>
<date>2015</date>
<contexts>
<context position="5145" citStr="Arora et al. (2015)" startWordPosition="781" endWordPosition="784">rarchical sparse regularizers to the matrix reconstruction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality. Dhillon et al. (2015) apply Canonical Correlation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embeddings. Stratos et al. (2014) and Stratos et al. (2015) assume a Brown language model, and prove that doing CCA on the bigram occurrences is equivalent to finding a transformed solution of the language model. Arora et al. (2015) assume there is a hidden discourse vector on a random walk, which determines the distribution of the current word. The slowly evolving discourse vector puts a constraint on the embeddings in a small text window. The maximum likelihood estimate of the embeddings within this text window approximately reduces to a squared norm objective. There are two limitations in current word embedding methods. The first limitation is, all MFbased methods map words and their context words to two different sets of embeddings, and then employ Singular Value Decomposition (SVD) to obtain a low rank approximation</context>
</contexts>
<marker>Arora, Li, Liang, Ma, Risteski, 2015</marker>
<rawString>Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2015. Random walks on discourse spaces: a new generative language model with applications to semantic word embeddings. ArXiv e-prints, arXiv:1502.03520 [cs.LG].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>1137--1155</pages>
<contexts>
<context position="2807" citStr="Bengio et al., 2003" startWordPosition="423" endWordPosition="426">rent optimization algorithms, with different scalability. Based on the forms of the link function and the optimization techniques, most methods can be divided into two classes: the traditional neural embedding models, and more recent low rank matrix factorization methods. The neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings. The normalizer in the softmax function brings intricacy to the optimization, which is usually tackled by gradient-based methods. The pioneering work was (Bengio et al., 2003). Later Mnih and Hinton (2007) propose three different link functions. However there are interaction matrices between the embeddings in all these models, which complicate and slow down the training, hindering them from being trained on huge corpora. Mikolov et al. (2013a) and Mikolov et al. (2013b) greatly simplify the conditional distribution, where the two embeddings interact directly. They implemented the well-known “word2vec”, which can be trained efficiently on huge corpora. The obtained embeddings show excellent performance on various tasks. Low-Rank Matrix Factorization (MF in short) me</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, pages 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>136--145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26774" citStr="Bruni et al. (2012)" startWordPosition="4660" endWordPosition="4663">valuated due to missing words, thus its scores are not comparable to others. Sparse was trained with PSD-180K-reg as the input embeddings, with default hyperparameters. The benchmark sets are almost identical to those in (Levy et al., 2015), except that (Luong et al., 2013)’s Rare Words is not included, as many rare words are cut off at the frequency 100, making more than 1/3 of test pairs invalid. Word Similarity There are 5 datasets: WordSim Similarity (WS Sim) and WordSim Relatedness (WS Rel) (Zesch et al., 2008; Agirre et al., 2009), partitioned from WordSim353 (Finkelstein et al., 2002); Bruni et al. (2012)’s MEN dataset; 3http://nlp.stanford.edu/projects/glove/ 4https://github.com/karlstratos/singular 5https://github.com/mfaruqui/sparse-coding Partition f(H),A in the same way. G11, f(H)11, A11 correspond to core bi(V1 grams. Partition V into ���� S1 V 2 � V 3 ; ���� S2 ���� S3 � in this example as �G21 G31 G12 G13 G22 G23 G32 G33 � � . G11 1605 Similarity Tasks Analogy Tasks Method WS Sim WS Rel MEN Turk SimLex Google MSR word2vec 0.742 0.543 0.731 0.663 0.395 0.734 / 0.742 0.650 / 0.674 PPMI 0.735 0.678 0.717 0.659 0.308 0.476 / 0.524 0.183 / 0.217 SVD 0.687 0.608 0.711 0.524 0.270 0.230 / 0.2</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136–145. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>J. Am. Soc. Inf. Sci.</journal>
<marker>Deerwester, Dumais, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. J. Am. Soc. Inf. Sci.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca.</title>
<date>2011</date>
<booktitle>In Proceedings ofAdvances in Neural Information Processing Systems,</booktitle>
<pages>199--207</pages>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer Dhillon, Dean P Foster, and Lyle H Ungar. 2011. Multi-view learning of word embeddings via cca. In Proceedings ofAdvances in Neural Information Processing Systems, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Eigenwords: Spectral word embeddings.</title>
<date>2015</date>
<journal>The Journal of Machine Learning Research.</journal>
<contexts>
<context position="4751" citStr="Dhillon et al. (2015)" startWordPosition="713" endWordPosition="716">cs. factorize this matrix directly as a new method. Pennington et al. (2014) propose a bilinear regression function of the conditional distribution, from which a weighted MF problem on the bigram logfrequency matrix is formulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in semantic hierarchies, Yogatama et al. (2015) add hierarchical sparse regularizers to the matrix reconstruction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality. Dhillon et al. (2015) apply Canonical Correlation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embeddings. Stratos et al. (2014) and Stratos et al. (2015) assume a Brown language model, and prove that doing CCA on the bigram occurrences is equivalent to finding a transformed solution of the language model. Arora et al. (2015) assume there is a hidden discourse vector on a random walk, which determines the distribution of the current word. The slowly evolving discourse vector puts a constraint on the embeddings in a small text w</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2015</marker>
<rawString>Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar. 2015. Eigenwords: Spectral word embeddings. The Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Yulia Tsvetkov</author>
<author>Dani Yogatama</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Sparse overcomplete word vector representations.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="4638" citStr="Faruqui et al. (2015)" startWordPosition="695" endWordPosition="698">ocessing, pages 1599–1609, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. factorize this matrix directly as a new method. Pennington et al. (2014) propose a bilinear regression function of the conditional distribution, from which a weighted MF problem on the bigram logfrequency matrix is formulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in semantic hierarchies, Yogatama et al. (2015) add hierarchical sparse regularizers to the matrix reconstruction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality. Dhillon et al. (2015) apply Canonical Correlation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embeddings. Stratos et al. (2014) and Stratos et al. (2015) assume a Brown language model, and prove that doing CCA on the bigram occurrences is equivalent to finding a transformed solution of the language model. Arora et al. (2015) assume there is a hidden discourse vector on a random walk, which determines the distributio</context>
<context position="25031" citStr="Faruqui et al., 2015" startWordPosition="4377" endWordPosition="4380">lts We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The competitors include: • (Mikolov et al., 2013b): word2vec2, or SGNS in some literature; 2https://code.google.com/p/word2vec/ • (Levy and Goldberg, 2014b): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords; • (Pennington et al., 2014): GloVe3; • (Stratos et al., 2015): Singular4, which does SVD-based CCA on the weighted bigram frequency matrix; • (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings. All models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English words, 2.04 billion words were left. We used the default hyperparameters in Hyperwords when training PPMI and SVD. Word2vec, GloVe and Singular were trained with their own default hyperparameters. The embedding sets PSD-Reg-180K and PSDUnreg-180K were trained using our online blockwise regression. Both sets contain the embeddings of the most frequent 180,000 words, base</context>
<context position="28951" citStr="Faruqui et al., 2015" startWordPosition="5014" endWordPosition="5017"> corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy and Goldberg (2014a). 6.2 Results Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on analogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), probably due to sub-optimal hyperparameters. This suggests their performance is unstable. The new embeddings yielded by Sparse systematically degraded compared to the old embeddings, contradicting the claim in (Faruqui et al., 2015). Our method PSD-Reg-180K performed well consistently, and is best in 4 similarity tasks. It performed worse than word2vec on analogy tasks, but still better than other MF-based methods. By comparing to PSD-Unreg-180K, we see Tikhonov regularization brings 1-4% performance boost across tasks. In addition, on similarity tasks, online blockwise regression only degrades slightly compared to batch factorization. Their performance gaps on analogy tasks were wider, but this might be explained by the fact that some hard cases were not counted in PSD-25K’s evaluation, due to its limited vocabulary. 7 </context>
</contexts>
<marker>Faruqui, Tsvetkov, Yogatama, Dyer, Smith, 2015</marker>
<rawString>Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah A. Smith. 2015. Sparse overcomplete word vector representations. In Proceedings of ACL 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>131</pages>
<contexts>
<context position="26753" citStr="Finkelstein et al., 2002" startWordPosition="4656" endWordPosition="4659">lly harder ones) were not evaluated due to missing words, thus its scores are not comparable to others. Sparse was trained with PSD-180K-reg as the input embeddings, with default hyperparameters. The benchmark sets are almost identical to those in (Levy et al., 2015), except that (Luong et al., 2013)’s Rare Words is not included, as many rare words are cut off at the frequency 100, making more than 1/3 of test pairs invalid. Word Similarity There are 5 datasets: WordSim Similarity (WS Sim) and WordSim Relatedness (WS Rel) (Zesch et al., 2008; Agirre et al., 2009), partitioned from WordSim353 (Finkelstein et al., 2002); Bruni et al. (2012)’s MEN dataset; 3http://nlp.stanford.edu/projects/glove/ 4https://github.com/karlstratos/singular 5https://github.com/mfaruqui/sparse-coding Partition f(H),A in the same way. G11, f(H)11, A11 correspond to core bi(V1 grams. Partition V into ���� S1 V 2 � V 3 ; ���� S2 ���� S3 � in this example as �G21 G31 G12 G13 G22 G23 G32 G33 � � . G11 1605 Similarity Tasks Analogy Tasks Method WS Sim WS Rel MEN Turk SimLex Google MSR word2vec 0.742 0.543 0.731 0.663 0.395 0.734 / 0.742 0.650 / 0.674 PPMI 0.735 0.678 0.717 0.659 0.308 0.476 / 0.524 0.183 / 0.217 SVD 0.687 0.608 0.711 0.</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Trans. Inf. Syst., 20(1):116– 131, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Globerson</author>
<author>Gal Chechik</author>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
</authors>
<title>Euclidean embedding of cooccurrence data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>8</volume>
<pages>2007--2265</pages>
<marker>Globerson, Chechik, Pereira, Tishby, 2007</marker>
<rawString>Amir Globerson, Gal Chechik, Fernando Pereira, and Naftali Tishby. 2007. Euclidean embedding of cooccurrence data. Journal of Machine Learning Research, vol. 8 (2007):2265–2295, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas J Higham</author>
</authors>
<title>Computing a nearest symmetric positive semidefinite matrix. Linear Algebra and its Applications,</title>
<date>1988</date>
<volume>103</volume>
<issue>0</issue>
<pages>118</pages>
<contexts>
<context position="21189" citStr="Higham, 1988" startWordPosition="3682" endWordPosition="3683">ts the 1 to N rows from QT. One key issue is how to initialize X. Srebro et al. (2003) suggest to set X(0)=G*, and point out that X(0) = 0 is far from a local optimum, thus requires more iterations. However we find G* is also far from a local optimum, and this setting converges slowly too. Setting X(0) = G*/2 usually 1It is referred to as an Expectation-Maximization algorithm by the original authors, but we think this is a misnomer. yields a satisfactory solution in a few iterations. The subroutine PSD Approximate() computes the unweighted nearest rank-N PSD approximation, measured in F-norm (Higham, 1988). 5.3 Online Blockwise Regression of V In Algorithm 1, the essential subroutine PSD Approximate() does eigendecomposition on Gt, which is dense due to the logarithm transformation. Eigendecomposition on a W x W dense matrix requires O(W2) space and O(W3) time, difficult to scale up to a large vocabulary. In addition, the majority of words in the vocabulary are infrequent, and Tikhonov regularization is necessary for them. It is observed that, as words become less frequent, fewer and fewer words appear around them to form bigrams. Remind that the vocabulary S = {s1, · · · , sW} are sorted in de</context>
</contexts>
<marker>Higham, 1988</marker>
<rawString>Nicholas J. Higham. 1988. Computing a nearest symmetric positive semidefinite matrix. Linear Algebra and its Applications, 103(0):103 – 118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="27930" citStr="Hill et al., 2014" startWordPosition="4856" endWordPosition="4859">24 0.183 / 0.217 SVD 0.687 0.608 0.711 0.524 0.270 0.230 / 0.240 0.123 / 0.113 GloVe 0.759 0.630 0.756 0.641 0.362 0.535 / 0.544 0.408 / 0.435 Singular 0.763 0.684 0.747 0.581 0.345 0.440 / 0.508 0.364 / 0.399 Sparse 0.739 0.585 0.725 0.625 0.355 0.240 / 0.282 0.253 / 0.274 PSD-Reg-180K 0.792 0.679 0.764 0.676 0.398 0.602 / 0.623 0.465 / 0.507 PSD-Unreg-180K 0.786 0.663 0.753 0.675 0.372 0.566 / 0.598 0.424 / 0.468 PSD-25K 0.801 0.676 0.765 0.678 0.393 0.671 / 0.695 0.533 / 0.586 Table 2: Performance of each method across different tasks. Radinsky et al. (2011)’s Mechanical Turk dataset; and (Hill et al., 2014)’s SimLex-999 dataset. The embeddings were evaluated by the Spearman’s rank correlation with the human ratings. Word Analogy The two datasets are MSR’s analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google’s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy and Goldberg (2014a). 6.2 Results Table 2 </context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>8</issue>
<contexts>
<context position="10474" citStr="Hinton, 2002" startWordPosition="1719" endWordPosition="1720">P(si)P(sj). 3 Link Function of Text In this section, we formulate the probability of a sequence of words as a function of their embeddings. We start from the link function of bigrams, which is the building blocks of a long sequence. Then this link function is extended to a text window with c context words, as a first-order approximation of the actual probability. 3.1 Link Function of Bigrams We generalize the link function of “word2vec” and “GloVe” to the following: { } P(si, sj) = exp v&gt; sjvsi + asisj P(si)P(sj) (1) The rationale for (1) originates from the idea of the Product of Experts in (Hinton, 2002). Suppose different types of semantic/syntactic regularities between si and sj are encoded in different dimensions of vsi, vsj. As exp{v&gt; sjvsi} = Hl exp{vsi,l · vsj,l}, this means the effects of different regularities on the probability are combined by multiplying together. If si and sj are independent, their joint probability should be P(si)P(sj). In the presence of correlations, the actual joint probability P(si, sj) would be a scaling of it. The scale factor reflects how much si and sj are positively or negatively correlated. Within the scale factor, v&gt; sjvsi captures linear interactions b</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul W Holland</author>
</authors>
<title>Weighted Ridge Regression: Combining Ridge and Robust Regression Methods.</title>
<date>1973</date>
<tech>NBER Working Papers 0011,</tech>
<institution>National Bureau of Economic Research, Inc,</institution>
<contexts>
<context position="24017" citStr="Holland, 1973" startWordPosition="4207" endWordPosition="4208">V2* that minimizes the total penalty of the 12-th and 21-th blocks of residuals (the 22-th block is ignored due to its high sparsity): arg min V 2 kG12 − V &gt;1 V 2k2 f(H)12 + kG21 − V2 &gt; V1k2f(H)21 + � µikvsik2 siES2 = arg min V 2 �kG12−V &gt; 1 V 2k2¯f(H)12+ µikvsik2, siES2 where f(H)12 = f(H)12 + f(H)&gt;21; � � G12 = G12 ◦ f(H)12 + G&gt; 21 ◦ f(H)&gt; 21 / (f (H)12 + f (H)21) is the weighted average of G12 and G&gt;21, “◦” and “/” are elementwise product and division, respectively. The columns in V 2 are independent, thus for each vsi, it is a separate weighted ridge regression problem, whose solution is (Holland, 1973): v* si =(V &gt; 1 diag( ¯fi)V 1+µiI)−1V &gt; 1 diag( ¯fi)¯gi, where ¯fi and ¯gi are columns corresponding to si in ¯f(H)12 and G12, respectively; 5. For any other set of noncore words Sk, find V*k that minimizes the total penalty of the 1kth and k1-th blocks, ignoring all other kj-th and jk-th blocks; 6. Combine all subsets of embeddings to form V*. Here V* = (V*1, V*2, V*3). 6 Experimental Results We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The c</context>
</contexts>
<marker>Holland, 1973</marker>
<rawString>Paul W. Holland. 1973. Weighted Ridge Regression: Combining Ridge and Robust Regression Methods. NBER Working Papers 0011, National Bureau of Economic Research, Inc, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
<author>Tong Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden markov models.</title>
<date>2012</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>78</volume>
<issue>5</issue>
<marker>Hsu, Kakade, Zhang, 2012</marker>
<rawString>Daniel Hsu, Sham M Kakade, and Tong Zhang. 2012. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):1460–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Linguistic regularities in sparse and explicit word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of CoNLL-2014,</booktitle>
<pages>171</pages>
<contexts>
<context position="3782" citStr="Levy and Goldberg (2014" startWordPosition="570" endWordPosition="573"> the two embeddings interact directly. They implemented the well-known “word2vec”, which can be trained efficiently on huge corpora. The obtained embeddings show excellent performance on various tasks. Low-Rank Matrix Factorization (MF in short) methods include various link functions and optimization methods. The link functions are usually not softmax functions. MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices. The objective is usually to minimize the reconstruction error, optionally with other constraints. In this line of research, Levy and Goldberg (2014b) find that “word2vec” is essentially doing stochastic weighted factorization of the word-context pointwise mutual information (PMI) matrix. They then 1599 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1599–1609, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. factorize this matrix directly as a new method. Pennington et al. (2014) propose a bilinear regression function of the conditional distribution, from which a weighted MF problem on the bigram logfrequency matrix is formulated. Gradient Descent is u</context>
<context position="24766" citStr="Levy and Goldberg, 2014" startWordPosition="4333" endWordPosition="4336"> respectively; 5. For any other set of noncore words Sk, find V*k that minimizes the total penalty of the 1kth and k1-th blocks, ignoring all other kj-th and jk-th blocks; 6. Combine all subsets of embeddings to form V*. Here V* = (V*1, V*2, V*3). 6 Experimental Results We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The competitors include: • (Mikolov et al., 2013b): word2vec2, or SGNS in some literature; 2https://code.google.com/p/word2vec/ • (Levy and Goldberg, 2014b): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords; • (Pennington et al., 2014): GloVe3; • (Stratos et al., 2015): Singular4, which does SVD-based CCA on the weighted bigram frequency matrix; • (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings. All models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English words, 2.04 billion words were left. We used the default hyperparameters in Hyperwords when training PPMI a</context>
<context position="28506" citStr="Levy and Goldberg (2014" startWordPosition="4945" endWordPosition="4948">echanical Turk dataset; and (Hill et al., 2014)’s SimLex-999 dataset. The embeddings were evaluated by the Spearman’s rank correlation with the human ratings. Word Analogy The two datasets are MSR’s analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google’s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy and Goldberg (2014a). 6.2 Results Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on analogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), probably due to sub-optimal hyperparameters. This suggests their performance is unstable. The new embeddings yielded by Sparse systematically degraded compared to the old embeddings, contradicting the claim in (Faruqui et al., 2015). Our method PSD-Reg-180K performed well consistently, and is best in 4 similarity tasks. It performed worse than word2vec on analogy tasks, but still bett</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014a. Linguistic regularities in sparse and explicit word representations. In Proceedings of CoNLL-2014, page 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embeddings as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS</booktitle>
<contexts>
<context position="3782" citStr="Levy and Goldberg (2014" startWordPosition="570" endWordPosition="573"> the two embeddings interact directly. They implemented the well-known “word2vec”, which can be trained efficiently on huge corpora. The obtained embeddings show excellent performance on various tasks. Low-Rank Matrix Factorization (MF in short) methods include various link functions and optimization methods. The link functions are usually not softmax functions. MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices. The objective is usually to minimize the reconstruction error, optionally with other constraints. In this line of research, Levy and Goldberg (2014b) find that “word2vec” is essentially doing stochastic weighted factorization of the word-context pointwise mutual information (PMI) matrix. They then 1599 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1599–1609, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. factorize this matrix directly as a new method. Pennington et al. (2014) propose a bilinear regression function of the conditional distribution, from which a weighted MF problem on the bigram logfrequency matrix is formulated. Gradient Descent is u</context>
<context position="24766" citStr="Levy and Goldberg, 2014" startWordPosition="4333" endWordPosition="4336"> respectively; 5. For any other set of noncore words Sk, find V*k that minimizes the total penalty of the 1kth and k1-th blocks, ignoring all other kj-th and jk-th blocks; 6. Combine all subsets of embeddings to form V*. Here V* = (V*1, V*2, V*3). 6 Experimental Results We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The competitors include: • (Mikolov et al., 2013b): word2vec2, or SGNS in some literature; 2https://code.google.com/p/word2vec/ • (Levy and Goldberg, 2014b): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords; • (Pennington et al., 2014): GloVe3; • (Stratos et al., 2015): Singular4, which does SVD-based CCA on the weighted bigram frequency matrix; • (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings. All models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English words, 2.04 billion words were left. We used the default hyperparameters in Hyperwords when training PPMI a</context>
<context position="28506" citStr="Levy and Goldberg (2014" startWordPosition="4945" endWordPosition="4948">echanical Turk dataset; and (Hill et al., 2014)’s SimLex-999 dataset. The embeddings were evaluated by the Spearman’s rank correlation with the human ratings. Word Analogy The two datasets are MSR’s analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google’s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy and Goldberg (2014a). 6.2 Results Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on analogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), probably due to sub-optimal hyperparameters. This suggests their performance is unstable. The new embeddings yielded by Sparse systematically degraded compared to the old embeddings, contradicting the claim in (Faruqui et al., 2015). Our method PSD-Reg-180K performed well consistently, and is best in 4 similarity tasks. It performed worse than word2vec on analogy tasks, but still bett</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014b. Neural word embeddings as implicit matrix factorization. In Proceedings of NIPS 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving distributional similarity with lessons learned from word embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--211</pages>
<contexts>
<context position="26395" citStr="Levy et al., 2015" startWordPosition="4593" endWordPosition="4596">i ∈ [25001, 80000] 4 i ∈ [80001,130000] 8 i ∈ [130001,180000] , i.e. increased regularization as the sparsity increases. To contrast with the batch learning performance, the performance of PSD-25K is listed, which contains the core embeddings only. PSD-25K took advantages that it contains much less false candidate words, and some test tuples (generally harder ones) were not evaluated due to missing words, thus its scores are not comparable to others. Sparse was trained with PSD-180K-reg as the input embeddings, with default hyperparameters. The benchmark sets are almost identical to those in (Levy et al., 2015), except that (Luong et al., 2013)’s Rare Words is not included, as many rare words are cut off at the frequency 100, making more than 1/3 of test pairs invalid. Word Similarity There are 5 datasets: WordSim Similarity (WS Sim) and WordSim Relatedness (WS Rel) (Zesch et al., 2008; Agirre et al., 2009), partitioned from WordSim353 (Finkelstein et al., 2002); Bruni et al. (2012)’s MEN dataset; 3http://nlp.stanford.edu/projects/glove/ 4https://github.com/karlstratos/singular 5https://github.com/mfaruqui/sparse-coding Partition f(H),A in the same way. G11, f(H)11, A11 correspond to core bi(V1 gram</context>
<context position="28717" citStr="Levy et al., 2015" startWordPosition="4980" endWordPosition="4983">Mikolov et al., 2013c), with 8000 questions, and Google’s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy and Goldberg (2014a). 6.2 Results Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on analogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), probably due to sub-optimal hyperparameters. This suggests their performance is unstable. The new embeddings yielded by Sparse systematically degraded compared to the old embeddings, contradicting the claim in (Faruqui et al., 2015). Our method PSD-Reg-180K performed well consistently, and is best in 4 similarity tasks. It performed worse than word2vec on analogy tasks, but still better than other MF-based methods. By comparing to PSD-Unreg-180K, we see Tikhonov regularization brings 1-4% performance boost across tasks. In addition, on similarity tasks, online blockwise regression only degra</context>
</contexts>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and Knowledge Management,</booktitle>
<pages>375--384</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6761" citStr="Lin and He, 2009" startWordPosition="1048" endWordPosition="1051">., 2015; Arora et al., 2015) are based on generative processes, the generative processes are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some models, e.g. (Mikolov et al., 2013b, Eq.1), even have no clear probabilistic interpretation. A generative word embedding model for documents is not only easier to interpret and analyze, but more importantly, provides a basis upon which documentlevel global latent factors, such as document topics (Wallach, 2006), sentiments (Lin and He, 2009), writing styles (Zhao et al., 2011b), can be incorporated in a principled manner, to better model the text distribution and extract relevant information. Based on the above considerations, we propose to unify the embeddings of words and context words. Our link function factorizes into three parts: the interaction of two embeddings capturing linear correlations of two words, a residual capturing nonlinear or noisy correlations, and the unigram priors. To reduce overfitting, we put Gaussian priors on embeddings and residuals, and apply Jelinek-Mercer Smoothing to bigrams. Furthermore, to model </context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of the 18th ACM conference on Information and Knowledge Management, pages 375–384. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>CoNLL-2013,</booktitle>
<pages>104</pages>
<contexts>
<context position="26429" citStr="Luong et al., 2013" startWordPosition="4599" endWordPosition="4602">30000] 8 i ∈ [130001,180000] , i.e. increased regularization as the sparsity increases. To contrast with the batch learning performance, the performance of PSD-25K is listed, which contains the core embeddings only. PSD-25K took advantages that it contains much less false candidate words, and some test tuples (generally harder ones) were not evaluated due to missing words, thus its scores are not comparable to others. Sparse was trained with PSD-180K-reg as the input embeddings, with default hyperparameters. The benchmark sets are almost identical to those in (Levy et al., 2015), except that (Luong et al., 2013)’s Rare Words is not included, as many rare words are cut off at the frequency 100, making more than 1/3 of test pairs invalid. Word Similarity There are 5 datasets: WordSim Similarity (WS Sim) and WordSim Relatedness (WS Rel) (Zesch et al., 2008; Agirre et al., 2009), partitioned from WordSim353 (Finkelstein et al., 2002); Bruni et al. (2012)’s MEN dataset; 3http://nlp.stanford.edu/projects/glove/ 4https://github.com/karlstratos/singular 5https://github.com/mfaruqui/sparse-coding Partition f(H),A in the same way. G11, f(H)11, A11 correspond to core bi(V1 grams. Partition V into ���� S1 V 2 � </context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recursive neural networks for morphology. CoNLL-2013, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mach</author>
</authors>
<title>Eigenvalue Algorithms for Symmetric Hierarchical Matrices. Dissertation,</title>
<date>2012</date>
<institution>Chemnitz University of Technology.</institution>
<marker>Mach, 2012</marker>
<rawString>Thomas Mach. 2012. Eigenvalue Algorithms for Symmetric Hierarchical Matrices. Dissertation, Chemnitz University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR</booktitle>
<contexts>
<context position="3077" citStr="Mikolov et al. (2013" startWordPosition="464" endWordPosition="467">s. The neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings. The normalizer in the softmax function brings intricacy to the optimization, which is usually tackled by gradient-based methods. The pioneering work was (Bengio et al., 2003). Later Mnih and Hinton (2007) propose three different link functions. However there are interaction matrices between the embeddings in all these models, which complicate and slow down the training, hindering them from being trained on huge corpora. Mikolov et al. (2013a) and Mikolov et al. (2013b) greatly simplify the conditional distribution, where the two embeddings interact directly. They implemented the well-known “word2vec”, which can be trained efficiently on huge corpora. The obtained embeddings show excellent performance on various tasks. Low-Rank Matrix Factorization (MF in short) methods include various link functions and optimization methods. The link functions are usually not softmax functions. MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices. The objective is usually to minimize the r</context>
<context position="6452" citStr="Mikolov et al., 2013" startWordPosition="1000" endWordPosition="1003">d the learned embeddings may not capture the most significant regularities in M. Appendix A gives a toy example on which SVD does not work properly. The second limitation is, a generative model for documents parametered by embeddings is absent in recent development. Although (Stratos et al., 2014; Stratos et al., 2015; Arora et al., 2015) are based on generative processes, the generative processes are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some models, e.g. (Mikolov et al., 2013b, Eq.1), even have no clear probabilistic interpretation. A generative word embedding model for documents is not only easier to interpret and analyze, but more importantly, provides a basis upon which documentlevel global latent factors, such as document topics (Wallach, 2006), sentiments (Lin and He, 2009), writing styles (Zhao et al., 2011b), can be incorporated in a principled manner, to better model the text distribution and extract relevant information. Based on the above considerations, we propose to unify the embeddings of words and context words. Our link function factorizes into thre</context>
<context position="24660" citStr="Mikolov et al., 2013" startWordPosition="4321" endWordPosition="4324">fi)V 1+µiI)−1V &gt; 1 diag( ¯fi)¯gi, where ¯fi and ¯gi are columns corresponding to si in ¯f(H)12 and G12, respectively; 5. For any other set of noncore words Sk, find V*k that minimizes the total penalty of the 1kth and k1-th blocks, ignoring all other kj-th and jk-th blocks; 6. Combine all subsets of embeddings to form V*. Here V* = (V*1, V*2, V*3). 6 Experimental Results We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The competitors include: • (Mikolov et al., 2013b): word2vec2, or SGNS in some literature; 2https://code.google.com/p/word2vec/ • (Levy and Goldberg, 2014b): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords; • (Pennington et al., 2014): GloVe3; • (Stratos et al., 2015): Singular4, which does SVD-based CCA on the weighted bigram frequency matrix; • (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings. All models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English w</context>
<context position="28119" citStr="Mikolov et al., 2013" startWordPosition="4884" endWordPosition="4887"> / 0.508 0.364 / 0.399 Sparse 0.739 0.585 0.725 0.625 0.355 0.240 / 0.282 0.253 / 0.274 PSD-Reg-180K 0.792 0.679 0.764 0.676 0.398 0.602 / 0.623 0.465 / 0.507 PSD-Unreg-180K 0.786 0.663 0.753 0.675 0.372 0.566 / 0.598 0.424 / 0.468 PSD-25K 0.801 0.676 0.765 0.678 0.393 0.671 / 0.695 0.533 / 0.586 Table 2: Performance of each method across different tasks. Radinsky et al. (2011)’s Mechanical Turk dataset; and (Hill et al., 2014)’s SimLex-999 dataset. The embeddings were evaluated by the Spearman’s rank correlation with the human ratings. Word Analogy The two datasets are MSR’s analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google’s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy and Goldberg (2014a). 6.2 Results Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on analogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at ICLR 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS 2013,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="3077" citStr="Mikolov et al. (2013" startWordPosition="464" endWordPosition="467">s. The neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings. The normalizer in the softmax function brings intricacy to the optimization, which is usually tackled by gradient-based methods. The pioneering work was (Bengio et al., 2003). Later Mnih and Hinton (2007) propose three different link functions. However there are interaction matrices between the embeddings in all these models, which complicate and slow down the training, hindering them from being trained on huge corpora. Mikolov et al. (2013a) and Mikolov et al. (2013b) greatly simplify the conditional distribution, where the two embeddings interact directly. They implemented the well-known “word2vec”, which can be trained efficiently on huge corpora. The obtained embeddings show excellent performance on various tasks. Low-Rank Matrix Factorization (MF in short) methods include various link functions and optimization methods. The link functions are usually not softmax functions. MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices. The objective is usually to minimize the r</context>
<context position="6452" citStr="Mikolov et al., 2013" startWordPosition="1000" endWordPosition="1003">d the learned embeddings may not capture the most significant regularities in M. Appendix A gives a toy example on which SVD does not work properly. The second limitation is, a generative model for documents parametered by embeddings is absent in recent development. Although (Stratos et al., 2014; Stratos et al., 2015; Arora et al., 2015) are based on generative processes, the generative processes are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some models, e.g. (Mikolov et al., 2013b, Eq.1), even have no clear probabilistic interpretation. A generative word embedding model for documents is not only easier to interpret and analyze, but more importantly, provides a basis upon which documentlevel global latent factors, such as document topics (Wallach, 2006), sentiments (Lin and He, 2009), writing styles (Zhao et al., 2011b), can be incorporated in a principled manner, to better model the text distribution and extract relevant information. Based on the above considerations, we propose to unify the embeddings of words and context words. Our link function factorizes into thre</context>
<context position="24660" citStr="Mikolov et al., 2013" startWordPosition="4321" endWordPosition="4324">fi)V 1+µiI)−1V &gt; 1 diag( ¯fi)¯gi, where ¯fi and ¯gi are columns corresponding to si in ¯f(H)12 and G12, respectively; 5. For any other set of noncore words Sk, find V*k that minimizes the total penalty of the 1kth and k1-th blocks, ignoring all other kj-th and jk-th blocks; 6. Combine all subsets of embeddings to form V*. Here V* = (V*1, V*2, V*3). 6 Experimental Results We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The competitors include: • (Mikolov et al., 2013b): word2vec2, or SGNS in some literature; 2https://code.google.com/p/word2vec/ • (Levy and Goldberg, 2014b): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords; • (Pennington et al., 2014): GloVe3; • (Stratos et al., 2015): Singular4, which does SVD-based CCA on the weighted bigram frequency matrix; • (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings. All models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English w</context>
<context position="28119" citStr="Mikolov et al., 2013" startWordPosition="4884" endWordPosition="4887"> / 0.508 0.364 / 0.399 Sparse 0.739 0.585 0.725 0.625 0.355 0.240 / 0.282 0.253 / 0.274 PSD-Reg-180K 0.792 0.679 0.764 0.676 0.398 0.602 / 0.623 0.465 / 0.507 PSD-Unreg-180K 0.786 0.663 0.753 0.675 0.372 0.566 / 0.598 0.424 / 0.468 PSD-25K 0.801 0.676 0.765 0.678 0.393 0.671 / 0.695 0.533 / 0.586 Table 2: Performance of each method across different tasks. Radinsky et al. (2011)’s Mechanical Turk dataset; and (Hill et al., 2014)’s SimLex-999 dataset. The embeddings were evaluated by the Spearman’s rank correlation with the human ratings. Word Analogy The two datasets are MSR’s analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google’s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy and Goldberg (2014a). 6.2 Results Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on analogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS 2013, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of HLTNAACL 2013,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="3077" citStr="Mikolov et al. (2013" startWordPosition="464" endWordPosition="467">s. The neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings. The normalizer in the softmax function brings intricacy to the optimization, which is usually tackled by gradient-based methods. The pioneering work was (Bengio et al., 2003). Later Mnih and Hinton (2007) propose three different link functions. However there are interaction matrices between the embeddings in all these models, which complicate and slow down the training, hindering them from being trained on huge corpora. Mikolov et al. (2013a) and Mikolov et al. (2013b) greatly simplify the conditional distribution, where the two embeddings interact directly. They implemented the well-known “word2vec”, which can be trained efficiently on huge corpora. The obtained embeddings show excellent performance on various tasks. Low-Rank Matrix Factorization (MF in short) methods include various link functions and optimization methods. The link functions are usually not softmax functions. MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices. The objective is usually to minimize the r</context>
<context position="6452" citStr="Mikolov et al., 2013" startWordPosition="1000" endWordPosition="1003">d the learned embeddings may not capture the most significant regularities in M. Appendix A gives a toy example on which SVD does not work properly. The second limitation is, a generative model for documents parametered by embeddings is absent in recent development. Although (Stratos et al., 2014; Stratos et al., 2015; Arora et al., 2015) are based on generative processes, the generative processes are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some models, e.g. (Mikolov et al., 2013b, Eq.1), even have no clear probabilistic interpretation. A generative word embedding model for documents is not only easier to interpret and analyze, but more importantly, provides a basis upon which documentlevel global latent factors, such as document topics (Wallach, 2006), sentiments (Lin and He, 2009), writing styles (Zhao et al., 2011b), can be incorporated in a principled manner, to better model the text distribution and extract relevant information. Based on the above considerations, we propose to unify the embeddings of words and context words. Our link function factorizes into thre</context>
<context position="24660" citStr="Mikolov et al., 2013" startWordPosition="4321" endWordPosition="4324">fi)V 1+µiI)−1V &gt; 1 diag( ¯fi)¯gi, where ¯fi and ¯gi are columns corresponding to si in ¯f(H)12 and G12, respectively; 5. For any other set of noncore words Sk, find V*k that minimizes the total penalty of the 1kth and k1-th blocks, ignoring all other kj-th and jk-th blocks; 6. Combine all subsets of embeddings to form V*. Here V* = (V*1, V*2, V*3). 6 Experimental Results We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The competitors include: • (Mikolov et al., 2013b): word2vec2, or SGNS in some literature; 2https://code.google.com/p/word2vec/ • (Levy and Goldberg, 2014b): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords; • (Pennington et al., 2014): GloVe3; • (Stratos et al., 2015): Singular4, which does SVD-based CCA on the weighted bigram frequency matrix; • (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings. All models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English w</context>
<context position="28119" citStr="Mikolov et al., 2013" startWordPosition="4884" endWordPosition="4887"> / 0.508 0.364 / 0.399 Sparse 0.739 0.585 0.725 0.625 0.355 0.240 / 0.282 0.253 / 0.274 PSD-Reg-180K 0.792 0.679 0.764 0.676 0.398 0.602 / 0.623 0.465 / 0.507 PSD-Unreg-180K 0.786 0.663 0.753 0.675 0.372 0.566 / 0.598 0.424 / 0.468 PSD-25K 0.801 0.676 0.765 0.678 0.393 0.671 / 0.695 0.533 / 0.586 Table 2: Performance of each method across different tasks. Radinsky et al. (2011)’s Mechanical Turk dataset; and (Hill et al., 2014)’s SimLex-999 dataset. The embeddings were evaluated by the Spearman’s rank correlation with the human ratings. Word Analogy The two datasets are MSR’s analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google’s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed by Levy and Goldberg (2014a). 6.2 Results Table 2 shows the results on all tasks. Word2vec significantly outperformed other methods on analogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in (Levy et al., 2015), </context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of HLTNAACL 2013, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th International Conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2837" citStr="Mnih and Hinton (2007)" startWordPosition="428" endWordPosition="431">, with different scalability. Based on the forms of the link function and the optimization techniques, most methods can be divided into two classes: the traditional neural embedding models, and more recent low rank matrix factorization methods. The neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings. The normalizer in the softmax function brings intricacy to the optimization, which is usually tackled by gradient-based methods. The pioneering work was (Bengio et al., 2003). Later Mnih and Hinton (2007) propose three different link functions. However there are interaction matrices between the embeddings in all these models, which complicate and slow down the training, hindering them from being trained on huge corpora. Mikolov et al. (2013a) and Mikolov et al. (2013b) greatly simplify the conditional distribution, where the two embeddings interact directly. They implemented the well-known “word2vec”, which can be trained efficiently on huge corpora. The obtained embeddings show excellent performance on various tasks. Low-Rank Matrix Factorization (MF in short) methods include various link fun</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th International Conference on Machine learning, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="4206" citStr="Pennington et al. (2014)" startWordPosition="627" endWordPosition="630">trix by the product of two low rank factor matrices. The objective is usually to minimize the reconstruction error, optionally with other constraints. In this line of research, Levy and Goldberg (2014b) find that “word2vec” is essentially doing stochastic weighted factorization of the word-context pointwise mutual information (PMI) matrix. They then 1599 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1599–1609, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. factorize this matrix directly as a new method. Pennington et al. (2014) propose a bilinear regression function of the conditional distribution, from which a weighted MF problem on the bigram logfrequency matrix is formulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in semantic hierarchies, Yogatama et al. (2015) add hierarchical sparse regularizers to the matrix reconstruction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality. Dhillon et al. (2015) apply Canonical Correlation Analysis (CCA) to the word</context>
<context position="24894" citStr="Pennington et al., 2014" startWordPosition="4354" endWordPosition="4357">, ignoring all other kj-th and jk-th blocks; 6. Combine all subsets of embeddings to form V*. Here V* = (V*1, V*2, V*3). 6 Experimental Results We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The competitors include: • (Mikolov et al., 2013b): word2vec2, or SGNS in some literature; 2https://code.google.com/p/word2vec/ • (Levy and Goldberg, 2014b): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords; • (Pennington et al., 2014): GloVe3; • (Stratos et al., 2015): Singular4, which does SVD-based CCA on the weighted bigram frequency matrix; • (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings. All models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English words, 2.04 billion words were left. We used the default hyperparameters in Hyperwords when training PPMI and SVD. Word2vec, GloVe and Singular were trained with their own default hyperparameters. The embedding sets PSD-Reg-180K and PS</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eugene Agichtein</author>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>A word at a time: Computing word relatedness using temporal semantic analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International Conference on World Wide Web, WWW ’11,</booktitle>
<pages>337--346</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="27879" citStr="Radinsky et al. (2011)" startWordPosition="4848" endWordPosition="4851"> / 0.674 PPMI 0.735 0.678 0.717 0.659 0.308 0.476 / 0.524 0.183 / 0.217 SVD 0.687 0.608 0.711 0.524 0.270 0.230 / 0.240 0.123 / 0.113 GloVe 0.759 0.630 0.756 0.641 0.362 0.535 / 0.544 0.408 / 0.435 Singular 0.763 0.684 0.747 0.581 0.345 0.440 / 0.508 0.364 / 0.399 Sparse 0.739 0.585 0.725 0.625 0.355 0.240 / 0.282 0.253 / 0.274 PSD-Reg-180K 0.792 0.679 0.764 0.676 0.398 0.602 / 0.623 0.465 / 0.507 PSD-Unreg-180K 0.786 0.663 0.753 0.675 0.372 0.566 / 0.598 0.424 / 0.468 PSD-25K 0.801 0.676 0.765 0.678 0.393 0.671 / 0.695 0.533 / 0.586 Table 2: Performance of each method across different tasks. Radinsky et al. (2011)’s Mechanical Turk dataset; and (Hill et al., 2014)’s SimLex-999 dataset. The embeddings were evaluated by the Spearman’s rank correlation with the human ratings. Word Analogy The two datasets are MSR’s analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google’s analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e. words that appear less than 100 times in the corpus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul proposed</context>
</contexts>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. 2011. A word at a time: Computing word relatedness using temporal semantic analysis. In Proceedings of the 20th International Conference on World Wide Web, WWW ’11, pages 337–346, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Weighted low-rank approximations.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML 2003,</booktitle>
<volume>3</volume>
<pages>720--727</pages>
<marker>Srebro, Jaakkola, 2003</marker>
<rawString>Nathan Srebro, Tommi Jaakkola, et al. 2003. Weighted low-rank approximations. In Proceedings of ICML 2003, volume 3, pages 720–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Do-kyum Kim</author>
<author>Michael Collins</author>
<author>Daniel Hsu</author>
</authors>
<title>A spectral algorithm for learning class-based n-gram models of natural language.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="4946" citStr="Stratos et al. (2014)" startWordPosition="745" endWordPosition="748"> logfrequency matrix is formulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in semantic hierarchies, Yogatama et al. (2015) add hierarchical sparse regularizers to the matrix reconstruction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality. Dhillon et al. (2015) apply Canonical Correlation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embeddings. Stratos et al. (2014) and Stratos et al. (2015) assume a Brown language model, and prove that doing CCA on the bigram occurrences is equivalent to finding a transformed solution of the language model. Arora et al. (2015) assume there is a hidden discourse vector on a random walk, which determines the distribution of the current word. The slowly evolving discourse vector puts a constraint on the embeddings in a small text window. The maximum likelihood estimate of the embeddings within this text window approximately reduces to a squared norm objective. There are two limitations in current word embedding methods. Th</context>
</contexts>
<marker>Stratos, Kim, Collins, Hsu, 2014</marker>
<rawString>Karl Stratos, Do-kyum Kim, Michael Collins, and Daniel Hsu. 2014. A spectral algorithm for learning class-based n-gram models of natural language. In Proceedings of the Association for Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Daniel Hsu</author>
</authors>
<title>Model-based word embeddings from decompositions of count matrices.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="4972" citStr="Stratos et al. (2015)" startWordPosition="750" endWordPosition="753">rmulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in semantic hierarchies, Yogatama et al. (2015) add hierarchical sparse regularizers to the matrix reconstruction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality. Dhillon et al. (2015) apply Canonical Correlation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embeddings. Stratos et al. (2014) and Stratos et al. (2015) assume a Brown language model, and prove that doing CCA on the bigram occurrences is equivalent to finding a transformed solution of the language model. Arora et al. (2015) assume there is a hidden discourse vector on a random walk, which determines the distribution of the current word. The slowly evolving discourse vector puts a constraint on the embeddings in a small text window. The maximum likelihood estimate of the embeddings within this text window approximately reduces to a squared norm objective. There are two limitations in current word embedding methods. The first limitation is, all</context>
<context position="24928" citStr="Stratos et al., 2015" startWordPosition="4360" endWordPosition="4363">blocks; 6. Combine all subsets of embeddings to form V*. Here V* = (V*1, V*2, V*3). 6 Experimental Results We trained our model along with a few state-ofthe-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets. 6.1 Experimental Setup Our own method is referred to as PSD. The competitors include: • (Mikolov et al., 2013b): word2vec2, or SGNS in some literature; 2https://code.google.com/p/word2vec/ • (Levy and Goldberg, 2014b): the PPMI matrix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords; • (Pennington et al., 2014): GloVe3; • (Stratos et al., 2015): Singular4, which does SVD-based CCA on the weighted bigram frequency matrix; • (Faruqui et al., 2015): Sparse5, which learns new sparse embeddings in a higher dimensional space from pretrained embeddings. All models were trained on the English Wikipedia snapshot in March 2015. After removing nontextual elements and non-English words, 2.04 billion words were left. We used the default hyperparameters in Hyperwords when training PPMI and SVD. Word2vec, GloVe and Singular were trained with their own default hyperparameters. The embedding sets PSD-Reg-180K and PSDUnreg-180K were trained using our</context>
</contexts>
<marker>Stratos, Collins, Hsu, 2015</marker>
<rawString>Karl Stratos, Michael Collins, and Daniel Hsu. 2015. Model-based word embeddings from decompositions of count matrices. In Proceedings of ACL 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingkui Tan</author>
<author>Ivor W Tsang</author>
<author>Li Wang</author>
<author>Bart Vandereycken</author>
<author>Sinno Jialin Pan</author>
</authors>
<title>Riemannian pursuit for big matrix recovery.</title>
<date>2014</date>
<booktitle>In Proceedings of ICML 2014,</booktitle>
<pages>1539--1547</pages>
<marker>Tan, Tsang, Wang, Vandereycken, Pan, 2014</marker>
<rawString>Mingkui Tan, Ivor W. Tsang, Li Wang, Bart Vandereycken, and Sinno Jialin Pan. 2014. Riemannian pursuit for big matrix recovery. In Proceedings of ICML 2014, pages 1539–1547.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Timme</author>
<author>Wesley Alford</author>
<author>Benjamin Flecker</author>
<author>John M Beggs</author>
</authors>
<title>Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective.</title>
<date>2014</date>
<journal>Journal of Computational Neuroscience,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="33514" citStr="Timme et al., 2014" startWordPosition="5753" endWordPosition="5756">d the redundant information Rdn(y; x1, x2) are defined as: P(y|xi) I(y; xi) = EP(xi,y)[log ] The synergistic information Syn(y; x1, x2) is defined as the PI-function in (Williams and Beer, 2010), skipped here. Figure 2: Different types of information among 3 random variables y, x1, x2. I(y; x1, x2) is the mutual information between y and (x1, x2). Rdn(y; x1, x2) and Syn(y; x1, x2) are the redundant information and synergistic information between x1, x2, conditioning y, respectively. The interaction information Int(x1, x2, y) measures the relative strength of Rdn(y; x1, x2) and Syn(y; x1, x2) (Timme et al., 2014): Int(x1, x2, y) =Syn(y; x1, x2) − Rdn(y; x1, x2) =I(y; x1, x2) − I(y; x1) − I(y; x2) P (x1)P (x2)P (y)P (x1, x2, y) =EP (x1,x2,y)[log P (x1, x2)P (x1, y)P (x2, y) ] Figure 2 shows the relationship of different information among 3 random variables y, x1, x2 (based on Fig.1 in (Williams and Beer, 2010)). PMI is the pointwise counterpart of mutual information I. Similarly, all the above concepts have their pointwise counterparts, obtained by dropping the expectation operator. Specifically, the pointwise interaction information is defined as PInt(x1, x2, y) = PMI(y; x1, x2) − PMI(y; x1) − PMI(y; </context>
</contexts>
<marker>Timme, Alford, Flecker, Beggs, 2014</marker>
<rawString>Nicholas Timme, Wesley Alford, Benjamin Flecker, and John M Beggs. 2014. Synergy, redundancy, and multivariate information measures: an experimentalist’s perspective. Journal of Computational Neuroscience, 36(2):119–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: beyond bag-of-words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning,</booktitle>
<pages>977--984</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6730" citStr="Wallach, 2006" startWordPosition="1045" endWordPosition="1046"> et al., 2014; Stratos et al., 2015; Arora et al., 2015) are based on generative processes, the generative processes are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some models, e.g. (Mikolov et al., 2013b, Eq.1), even have no clear probabilistic interpretation. A generative word embedding model for documents is not only easier to interpret and analyze, but more importantly, provides a basis upon which documentlevel global latent factors, such as document topics (Wallach, 2006), sentiments (Lin and He, 2009), writing styles (Zhao et al., 2011b), can be incorporated in a principled manner, to better model the text distribution and extract relevant information. Based on the above considerations, we propose to unify the embeddings of words and context words. Our link function factorizes into three parts: the interaction of two embeddings capturing linear correlations of two words, a residual capturing nonlinear or noisy correlations, and the unigram priors. To reduce overfitting, we put Gaussian priors on embeddings and residuals, and apply Jelinek-Mercer Smoothing to </context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M Wallach. 2006. Topic modeling: beyond bag-of-words. In Proceedings of the 23rd international conference on Machine learning, pages 977– 984. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul L Williams</author>
<author>Randall D Beer</author>
</authors>
<title>Nonnegative decomposition of multivariate information. arXiv preprint arXiv:1004.2515.</title>
<date>2010</date>
<contexts>
<context position="15202" citStr="Williams and Beer, 2010" startWordPosition="2526" endWordPosition="2529">ons. 3.2 Link Function of a Text Window In the previous subsection, a regression link function of bigram probabilities is established. In this section, we adopt a first-order approximation based on Information Theory, and extend the link function to a longer sequence w0, · · · , wc−1, wc. Decomposing a distribution conditioned on n random variables as the conditional distributions on its subsets roots deeply in Information Theory. This is an intricate problem because there could be both (pointwise) redundant information and (pointwise) synergistic information among the conditioning variables (Williams and Beer, 2010). They are both functions of the PMI. Based on an analysis of the complementing roles of these two types of pointwise information, we assume they are approximately equal and cancel each other when computing the pointwise interaction information. See Appendix B for a detailed discussion. Following the above assumption, we have PMI(w2; w0, w1) ≈ PMI(w2; w0)+PMI(w2; w1): log P(w0,w1|w2) log P (w0 |w2)+logP(w1|w2) P(w0, w 1 ) P (w 0 ) P (w 1 ) . Plugging (1) and (3) into the above, we obtain P(w0, w1, w2) ~ X2 (T 2 �log P (wi) . ≈ exp vwivwj + awiwj) + X i,j=0 i=0 i7�j We extend the above assumpti</context>
<context position="33089" citStr="Williams and Beer, 2010" startWordPosition="5681" endWordPosition="5684"> correlation between s1, s2. Appendix B Information Theory Redundant information refers to the reduced uncertainty by knowing the value of any one of the conditioning variables (hence redundant). Synergistic information is the reduced uncertainty ascribed to knowing all the values of conditioning variables, that cannot be reduced by knowing the value of any variable alone (hence synergistic). The mutual information I(y; xi) and the redundant information Rdn(y; x1, x2) are defined as: P(y|xi) I(y; xi) = EP(xi,y)[log ] The synergistic information Syn(y; x1, x2) is defined as the PI-function in (Williams and Beer, 2010), skipped here. Figure 2: Different types of information among 3 random variables y, x1, x2. I(y; x1, x2) is the mutual information between y and (x1, x2). Rdn(y; x1, x2) and Syn(y; x1, x2) are the redundant information and synergistic information between x1, x2, conditioning y, respectively. The interaction information Int(x1, x2, y) measures the relative strength of Rdn(y; x1, x2) and Syn(y; x1, x2) (Timme et al., 2014): Int(x1, x2, y) =Syn(y; x1, x2) − Rdn(y; x1, x2) =I(y; x1, x2) − I(y; x1) − I(y; x2) P (x1)P (x2)P (y)P (x1, x2, y) =EP (x1,x2,y)[log P (x1, x2)P (x1, y)P (x2, y) ] Figure 2 </context>
</contexts>
<marker>Williams, Beer, 2010</marker>
<rawString>Paul L Williams and Randall D Beer. 2010. Nonnegative decomposition of multivariate information. arXiv preprint arXiv:1004.2515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Yan</author>
<author>Mingkui Tan</author>
<author>Ivor Tsang</author>
<author>Yi Yang</author>
<author>Chengqi Zhang</author>
<author>Qinfeng Shi</author>
</authors>
<title>Scalable maximum margin matrix factorization by active riemannian subspace search.</title>
<date>2015</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<marker>Yan, Tan, Tsang, Yang, Zhang, Shi, 2015</marker>
<rawString>Yan Yan, Mingkui Tan, Ivor Tsang, Yi Yang, Chengqi Zhang, and Qinfeng Shi. 2015. Scalable maximum margin matrix factorization by active riemannian subspace search. In Proceedings of IJCAI 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Learning word representations with hierarchical sparse coding.</title>
<date>2015</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="4518" citStr="Yogatama et al. (2015)" startWordPosition="677" endWordPosition="680">l information (PMI) matrix. They then 1599 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1599–1609, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. factorize this matrix directly as a new method. Pennington et al. (2014) propose a bilinear regression function of the conditional distribution, from which a weighted MF problem on the bigram logfrequency matrix is formulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in semantic hierarchies, Yogatama et al. (2015) add hierarchical sparse regularizers to the matrix reconstruction error. With similar techniques, Faruqui et al. (2015) reconstruct a set of pretrained embeddings using sparse vectors of greater dimensionality. Dhillon et al. (2015) apply Canonical Correlation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embeddings. Stratos et al. (2014) and Stratos et al. (2015) assume a Brown language model, and prove that doing CCA on the bigram occurrences is equivalent to finding a transformed solution of the language</context>
</contexts>
<marker>Yogatama, Faruqui, Dyer, Smith, 2015</marker>
<rawString>Dani Yogatama, Manaal Faruqui, Chris Dyer, and Noah A Smith. 2015. Learning word representations with hierarchical sparse coding. In Proceedings of ICML 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using wiktionary for computing semantic relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI</booktitle>
<volume>8</volume>
<pages>861--866</pages>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Using wiktionary for computing semantic relatedness. In Proceedings of AAAI 2008, volume 8, pages 861–866.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to information retrieval.</title>
<date>2004</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="14133" citStr="Zhai and Lafferty, 2004" startWordPosition="2363" endWordPosition="2366">ually have much smaller probability to collocate. Hence P˜(si, si) does not reflect the true correlation of a word to itself, and should not put constraints to the embeddings. We eliminate their effects by setting f(hii) to 0. If the domain of A is the whole space RW&apos;W, then this penalty is equivalent to a Gaussian prior � � N 0, 1 on each asisj. The variances of the 2f(hij) Gaussians are determined by the bigram empirical probability matrix H. 3.1.3 Jelinek-Mercer Smoothing of Bigrams As another measure to reduce the impact of missing data, we apply the commonly used JelinekMercer Smoothing (Zhai and Lafferty, 2004) to smooth the empirical conditional probability ˜ ˜ P(sj|si) by the unigram probability P(sj) as: ˜Psmoothed(sj|si) = (1−κ) P˜(sj|si)+κP(sj). (6) Accordingly, the smoothed bigram empirical joint probability is defined as P˜(si, sj) = (1−κ) P˜(si, sj)+κP(si)P(sj). (7) In practice, we find κ = 0.02 yields good results. When κ ≥ 0.04, the obtained embeddings begin to degrade with κ, indicating that smoothing distorts the true bigram distributions. 3.2 Link Function of a Text Window In the previous subsection, a regression link function of bigram probabilities is established. In this section, we </context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2004. A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems (TOIS), 22(2):179–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peilin Zhao</author>
<author>Steven CH Hoi</author>
<author>Rong Jin</author>
</authors>
<title>Double updating online learning.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--1587</pages>
<contexts>
<context position="6796" citStr="Zhao et al., 2011" startWordPosition="1054" endWordPosition="1057">sed on generative processes, the generative processes are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some models, e.g. (Mikolov et al., 2013b, Eq.1), even have no clear probabilistic interpretation. A generative word embedding model for documents is not only easier to interpret and analyze, but more importantly, provides a basis upon which documentlevel global latent factors, such as document topics (Wallach, 2006), sentiments (Lin and He, 2009), writing styles (Zhao et al., 2011b), can be incorporated in a principled manner, to better model the text distribution and extract relevant information. Based on the above considerations, we propose to unify the embeddings of words and context words. Our link function factorizes into three parts: the interaction of two embeddings capturing linear correlations of two words, a residual capturing nonlinear or noisy correlations, and the unigram priors. To reduce overfitting, we put Gaussian priors on embeddings and residuals, and apply Jelinek-Mercer Smoothing to bigrams. Furthermore, to model the probability of a sequence of wo</context>
</contexts>
<marker>Zhao, Hoi, Jin, 2011</marker>
<rawString>Peilin Zhao, Steven CH Hoi, and Rong Jin. 2011a. Double updating online learning. The Journal of Machine Learning Research, 12:1587–1615.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Jianshu Weng</author>
</authors>
<title>Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011b. Comparing twitter and traditional media using topic models.</title>
<booktitle>In Advances in Information Retrieval (Proceedings of the 33rd Annual European Conference on Information Retrieval Research),</booktitle>
<pages>338--349</pages>
<publisher>Springer.</publisher>
<marker>Zhao, Jiang, Weng, </marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011b. Comparing twitter and traditional media using topic models. In Advances in Information Retrieval (Proceedings of the 33rd Annual European Conference on Information Retrieval Research), pages 338–349. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
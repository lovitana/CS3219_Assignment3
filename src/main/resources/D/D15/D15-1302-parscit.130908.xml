<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005028">
<title confidence="0.999393">
A Multi-lingual Annotated Dataset
for Aspect-Oriented Opinion Mining
</title>
<author confidence="0.898828">
Salud Maria Jim´enez Zafra&apos;, Giacomo Berardi&apos;, Andrea Esuli&apos;,
Diego Marcheggiani&apos;, Maria Teresa Martin-Valdivia&apos;, Alejandro Moreo Fern´andez&apos;
</author>
<affiliation confidence="0.978399">
&apos;Departamento de Inform´atica, Escuela Polit´ecnica Superior de Ja´en
</affiliation>
<address confidence="0.525517">
Universidad de Ja´en, E-23071 - Ja´en, Spain
</address>
<email confidence="0.951603">
{sjzafra, maite}@ujaen.es
</email>
<address confidence="0.4559445">
&apos;Istituto di Scienza e Tecnologie dell’Informazione “A. Faedo”
Consiglio Nazionale delle Ricerche, I-56124 - Pisa, Italy
</address>
<email confidence="0.992264">
{firstname.lastname}@isti.cnr.it
</email>
<sectionHeader confidence="0.99727" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986105263158">
We present the Trip-MAML dataset, a
Multi-Lingual dataset of hotel reviews
that have been manually annotated at the
sentence-level with Multi-Aspect senti-
ment labels. This dataset has been built
as an extension of an existent English-only
dataset, adding documents written in Ital-
ian and Spanish. We detail the dataset
construction process, covering the data
gathering, selection, and annotation. We
present inter-annotator agreement figures
and baseline experimental results, compar-
ing the three languages. Trip-MAML is
a multi-lingual dataset for aspect-oriented
opinion mining that enables researchers (i)
to face the problem on languages other
than English and (ii) to the experiment the
application of cross-lingual learning meth-
ods to the task.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997883745454546">
Reviews of products and services that are sponta-
neously produced by customers represent a source
of unquestionable value not only for marketing
strategies of private companies and organizations,
but also for other users since their purchasing de-
cisions are likely influenced by other customers’
opinions (Chevalier and Mayzlin, 2006).
Overall ratings (e.g., in terms of a five stars rat-
ing scale), and also aspect-specific ratings (e.g.,
the Cleanliness or Location of a hotel), are the typi-
cal additional information expressed by customers
in their reviews. Those ratings help to derive a
number of global scores to facilitate a first screen-
ing of the product or service at hand. Notwith-
standing, users who pay more attention to a par-
ticular aspect (e.g., the Rooms of a hotel) remain
constrained to manually inspect the entire text
of reviews in order to find out the reasons other
users argued in that respect. Methods for au-
tomatic analysis of the aspect-oriented sentiment
expressed in reviews would enable highlighting
aspect-relevant parts of the document, so as to al-
low users to perform a faster and focused inspec-
tion of them.
Previous work on opinion mining (Pang and
Lee, 2008) has already faced the overall sentiment
prediction (Pang et al., 2002), multiple aspect-
oriented analysis (Hu and Liu, 2004), and fine-
grained phrase-level analysis (Wilson et al., 2009).
Most of the available opinion mining datasets con-
tain only documents written in English, as this lan-
guage is the most used on the Internet and the one
for which more NLP tools and resources are avail-
able. (Hu and Liu, 2004) worked on the summa-
rization of reviews by means of weakly supervised
feature mining. (T¨ackstr¨om and McDonald, 2011)
used a finer-grained dataset in which global po-
larity annotation is applied also to each sentence
composing the document. Similarly did (Socher
et al., 2013) with the Stanford Sentiment Tree-
bank, which annotates each syntactically plausi-
ble phrase in thousands of sentences using anno-
tators from Amazon’s Mechanical Turk, annotat-
ing the polarity of phrases on a five-level scale.
(Lazaridou et al., 2013) performed a single-label
polarity annotation of elementary discourse units
of TripAdvisor reviews, adopting ten aspect labels.
(Marcheggiani et al., 2014) did a similar annota-
tion work, using sentences as the annotation ele-
ments and adopting a multi-label polarity annota-
tion, i.e., each sentence can be assigned to zero,
one, or more than one aspect.
Cross-lingual sentiment classification (Wan,
2009; Prettenhofer and Stein, 2011) explores the
scenario in which training data are available for
</bodyText>
<page confidence="0.76111">
2533
</page>
<note confidence="0.6317315">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2533–2538,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999535904761905">
a language that is different from the language of
the test documents. Cross-lingual learning meth-
ods have important practical applications, since
they allow to build classifiers for many languages
reusing the training data produced for a single lan-
guage (typically English), probably giving up a bit
of accuracy, but compensating it with a large save
in terms of human annotation costs.
Multi-lingual datasets are beneficial to the re-
search community both as a benchmark to ex-
plore cross-lingual learning and also as resources
on which to develop and test new NLP tools for
languages other than English. Prettenhofer and
Stein (2010) used a multi-lingual dataset focused
on full-document classification at the global po-
larity level. Denecke (2008) used a dataset of 200
Amazon reviews in German to test cross-lingual
document polarity classification using an English
training set. Klinger and Cimiano (2014) pro-
duced a bi-lingual dataset (English and German),
named USAGE, in which aspect expressions and
subjective expressions are annotated in Amazon
product reviews. In (Klinger and Cimiano, 2014)
aspect expressions can be any piece of text that
mentions a relevant property of the reviewed en-
tity (e.g., washer, hose, looks) and are not categor-
ical label, as in our dataset. The USAGE dataset is
thus more oriented at information extraction rather
than at text classification applications. Banea et al.
(2010) used machine translation to create a multi-
lingual version of the information-extraction ori-
ented MPQA dataset (Wiebe et al., 2005) on six
languages (English, Arabic, French, German, Ro-
manian and Spanish).
In this paper we present Trip-MAML, which
extends the Trip-MA1 dataset of Marcheggiani et
al. (2014) with Italian and Spanish annotated re-
views. We describe Trip-MAML and report ex-
periments aimed at defining a first baseline. Both
the dataset and the software used in experiments
are publicly available at http://hlt.isti.
cnr.it/trip-maml/.
</bodyText>
<sectionHeader confidence="0.995799" genericHeader="method">
2 Annotation Process
</sectionHeader>
<bodyText confidence="0.99991275">
We recall the annotation process adopted by
Marcheggiani et al. (2014) for Trip-MA and the
procedure we employed to extend it into Trip-
MAML. We will use the national codes EN, ES,
</bodyText>
<footnote confidence="0.644309333333333">
1Marcheggiani et al. (2014) gave no name to their dataset,
here we name it Trip-MA to identify its source and its multi-
aspect nature.
</footnote>
<bodyText confidence="0.998549666666667">
and IT, to denote the English, Spanish, and Ital-
ian parts of the Trip-MAML dataset, respectively.
Note that EN coincides with Trip-MA.
</bodyText>
<subsectionHeader confidence="0.9917">
2.1 English Reviews
</subsectionHeader>
<bodyText confidence="0.99985237037037">
The Trip-MA dataset was created by Marcheg-
giani et al. (2014) by annotating a set of 442
reviews, written in English, randomly sampled
from the publicly available TripAdvisor dataset of
Wang et al. (2010), composed by 235,793 reviews.
Each review comes with an overall rating on a dis-
crete ordinal scale from 1 to 5 “stars”. The dataset
was annotated according to 9 recurrent aspects fre-
quently involved in hotel reviews: Rooms, Clean-
liness, Value, Service, Location, Check-in, Business,
Food, and Building. The last two are not officially
rated by TripAdvisor but were added because they
are frequently commented in reviews. Two “catch-
all” aspects, Other and NotRelated, were also added,
for a total of 11 aspect. Aspect Other denotes
opinions that are pertinent to the hotel being re-
viewed, but not relevant to any of the former nine
aspects (e.g., generic evaluations like Pulitzer ex-
ceeded our expectations). Aspect NotRelated de-
notes opinions that are not related to the hotel
(e.g., Tour Eiffel is amazing).
If a sentence is relevant to an aspect, the possi-
ble sentiment label values are three: Positive, Neg-
ative, and Neutral/Mixed2. Neutral/Mixed annotates
subjective evaluations that are not clearly polar-
ized (e.g., The hotel was fine with some excep-
tions).
</bodyText>
<subsectionHeader confidence="0.850641">
2.1.1 Annotation protocol
</subsectionHeader>
<bodyText confidence="0.9999875">
Marcheggiani et al. (2014) relied on three human
annotators to annotate each sentence of the 442 re-
views with respect to polarities of opinions that are
relevant to any of the 11 aspects. 73 reviews, out
of 442, were independently annotated by all the
annotators in order to measure the inter-annotator
agreement, while the remaining 369 reviews were
partitioned into 3 equally-sized sets, one for each
annotator. Bias in the estimation of inter-annotator
agreement was minimized by sorting the list of re-
views of each annotator so that every eighth re-
view was common to all annotators; this ensured
that each annotator had the same amount of coding
experience when labeling the same shared review.
</bodyText>
<footnote confidence="0.818788666666667">
2Marcheggiani et al. (2014) initially distinguished be-
tween implicit and explicit opinions but the human agreement
was so low they removed this distinction from the schema.
</footnote>
<page confidence="0.950567">
2534
</page>
<table confidence="0.999898">
# Reviews # Sentences # Opinion-laden sentences
EN 442 5799 4810
ES 500 2620 2400
IT 500 2593 2392
</table>
<tableCaption confidence="0.993045">
Table 1: Number of reviews, sentences, and sen-
tences with at least one opinion annotation.
</tableCaption>
<subsectionHeader confidence="0.994882">
2.2 Spanish and Italian Reviews
</subsectionHeader>
<bodyText confidence="0.999974175">
For the creation of ES and IT parts of the Trip-
MAML dataset we followed the same annotation
protocol of Marcheggiani et al. (2014), employ-
ing teams of three native speakers as annotators
for each language. We crawled the Spanish and
Italian reviews from TripAdvisor by accessing its
websites with the ‘.es’ and ‘.it’ domains, which
mostly contains reviews in the national language.
From that domains we downloaded the reviews
for the 10 most visited cities in Spain and Italy,
respectively. We downloaded 10 reviews for ev-
ery hotel of each city, obtaining a total of 17,020
reviews for Spanish and 33,325 for Italian. For
each dataset, 500 reviews were selected by ran-
domly sampling 50 reviews for each city. We thus
obtained 139 unique reviews for each annotator,
plus 83 reviews which all three annotators inde-
pendently annotated.
We decided to annotate the aspects that were
ratable on TripAdvisor at the time of our crawl
(April 2015: Rooms, Cleanliness, Value, Service, Lo-
cation, and Sleep Quality). Differently from the as-
pect schema in EN, we included the new aspect
Sleep Quality, and we did not consider the miss-
ing aspects Check-in and Businnes, which are, in
any case, the least frequent aspects in the Trip-MA
dataset (see Table 2). We kept the additional as-
pects Food, Building, Other, and NotRelated, as they
still appear frequently in the reviews. We adopted
the same 3-values sentiment label schema of EN,
i.e., Positive, Negative, or Neutral/Mixed.
Following the same procedure adopted by
Marcheggiani et al. (2014), the Spanish and Ital-
ian annotator teams performed a preliminary an-
notation session on reviews not included in the fi-
nal dataset. This preliminary activity was aimed at
aligning the annotators’ understanding about the
labeling process for the different aspects, by shar-
ing and solving any doubt that might arise during
the annotation of some examples.
</bodyText>
<subsectionHeader confidence="0.993652">
2.3 Statistics
</subsectionHeader>
<bodyText confidence="0.999954517241379">
Table 1 shows that English reviews have, on av-
erage, about double the number of sentences of
Spanish and Italian reviews. This can be in part
motivated by observing that the sentences in EN
are, on average, 25% shorter than in ES and IT.
Also, after a manual inspection of the data, we
found that the EN part contains some reviews re-
lated to long vacations in resorts, thus describ-
ing in longer details the experience, while IT and
ES reviews are mainly related to relatively short
visits to classic hotels. However, the portion of
opinionated sentences is similar across the three
parts, indicating homogeneity in content, which is
confirmed by the detailed aspect-level statistics re-
ported in Table 2.
Both aspect and sentiment labels show imbal-
anced distributions that follow similar distribu-
tions across the three parts. The most frequent
aspect in all collections is Other, followed by
Rooms, Service, and Location. Building and Value
are among the least frequent ones. The average
value of the Pearson correlation between the lists
of the shared aspects ranked by their relative fre-
quency, measured pairwise among the three parts,
is 0.795, which indicates a good uniformity of
content among the parts. In all the three parts, Pos-
itive is the most frequent sentiment label, followed
by Negative. Location is always the aspect with the
highest frequency of positive labels.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="method">
3 Inter-annotator Agreement
</sectionHeader>
<bodyText confidence="0.999534947368421">
We measured the inter-annotator agreement in two
steps. The F1 score measures the agreement on
aspect identification, regardless of the sentiment
label assigned. Then symmetric Macro-averaged
Mean Absolute Error (sMAEM) (Baccianella et
al., 2009) measures the agreement on sentiment
labels on the annotations for which the annotators
agreed at the aspect level. Aspect NotRelated is not
included in agreement evaluation, nor in the ex-
periments of Section 4. sMAEM is computed be-
tween each of the three possible pairs of annota-
tors and then averaged to determine the agreement
values reported in Table 3.
Agreement on aspect detection is higher for ES
and IT than for EN. This difference is in part moti-
vated by the fact that the two aspects that are miss-
ing in ES and IT have low agreement on EN, and
the novel Sleep Quality aspect has instead a high
agreement. However, also on the other aspects
</bodyText>
<page confidence="0.985682">
2535
</page>
<table confidence="0.9973131875">
Other Service Rooms Clean. Food Loc. Check-in Sleep-q. Value Build. Busin. NotRelated Total
Pos 893 513 484 180 287 435 93 - 188 185 23 63 3344
Neg 353 248 287 66 127 51 56 - 87 62 3 40 1377
EN 167 40 111 5 82 38 12 - 35 22 4 350 866
Neu 1413 801 882 251 496 524 161 - 310 269 30 453 5587
Total
Pos 634 382 275 181 128 452 - 126 114 71 - 39 2402
Neg 244 85 159 40 37 38 - 75 48 28 - 38 792
ES 46 19 62 6 32 22 - 6 18 7 - 4 222
Neu 924 486 496 227 197 512 - 207 180 106 - 81 3416
Total
Pos 582 415 267 259 207 389 - 103 135 77 - 50 2484
Neg 189 74 110 65 56 22 - 50 27 43 - 15 651
IT 102 30 59 10 52 49 - 1 32 32 - 100 467
Neu 873 519 436 334 315 460 - 154 194 152 - 165 3602
Total
</table>
<tableCaption confidence="0.967274">
Table 2: Number of opinion expressions at the sentence level of the datasets.
</tableCaption>
<table confidence="0.999909857142857">
Other Service Rooms Clean. Food Loc. Check-in Sleep-q. Value Build. Busin. Avg
EN Fl .607 .719 .793 .733 .794 .795 .464 - .575 .553 .631 .675
sMAE`x .308 .219 .191 .114 .234 .259 .003 - .202 .150 .029 .171
ES Fl .789 .911 .854 .933 .882 .896 - .895 .829 .538 - .836
sMAE`x .174 .093 .133 .303 .120 .293 - .000 .150 .184 - .161
IT Fl .676 .812 .788 .913 .884 .856 - .789 .858 .532 - .790
sMAE`x .292 .166 .242 .114 .204 .204 - .067 .292 .114 - .188
</table>
<tableCaption confidence="0.893555">
Table 3: Inter-annotator agreement. F1 on sentence-level aspect identification (higher is better). sMAEM
on sentence-level sentiment agreement (only on matching aspects, lower is better).
</tableCaption>
<bodyText confidence="0.9987838">
there is, in general, a higher or equal agreement in
ES and IT with respect to EN, indicating that the
formers two were annotated in a more consistent
way. The agreement on assignment of sentiment
label is rather similar across the whole dataset.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999975642857143">
The experiments we present here are aimed at
defining a shared baseline for future experiments.
For this reason we chose a relatively simple setup
that uses a simple learning model and minimal lin-
guistic resources. We used a sentence-level Linear
Chain (LC) Conditional Random Field (Lafferty
et al., 2001) as described by Marcheggiani et al.
(2014). With respect to the features extracted from
text, we used three simple features types: word
unigrams, bigrams, and SentiWordNet-based fea-
tures, which consist of a Positive and a Negative
feature extracted every time the review contains
a word that is marked as such in SentiWordNet
(Baccianella et al., 2010). To use SentiWord-
Net on ES and IT, we used Multilingual Cen-
tral Repository (Gonzalez-Agirre et al., 2012) and
MultiWordNet (Pianta et al., 2002) to map senti-
ment labels to Spanish and to Italian, respectively.
Experiments were run separately on the EN, ES,
and IT parts, leaving cross-lingual experiments to
future work. On each part we built five 70%/30%
train/test splits, randomly generated by sampling
the reviews annotated by single reviewers (we left
out reviews annotated by all the reviewers, as we
consider that part of the dataset more useful as
a validation set for the optimization of methods
tested in future experiments). We then run the five
experiments and averaged their results.
</bodyText>
<subsectionHeader confidence="0.997717">
4.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999900555555556">
As for the agreement evaluation (Section 3), we
split the evaluation of experiments into two parts,
aspect detection and sentiment labeling. For the
sentiment labeling part we used simple Macro-
averaged Mean Absolute Error (MAEM, not the
symmetric version) as the true dataset labels are
the reference ones in this case, while in the anno-
tator agreement case the two sets of labels have
equal importance.
</bodyText>
<subsectionHeader confidence="0.746252">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.9993005">
Experiments on ES and IT obtain better F1 values
than on EN, indicating that the observed higher
human agreement can be also explained by a lower
hardness of the task when working with Spanish
</bodyText>
<page confidence="0.981607">
2536
</page>
<table confidence="0.993597571428571">
Other Service Rooms Clean. Food Loc. Check-in Sleep-q. Value Build. Busin. Avg
EN Fl .482 .595 .626 .729 .541 .616 .230 - .331 .281 .222 .465
MAEM .549 .822 .641 .968 .585 .959 .264 - .598 .471 .000 .586
ES Fl .520 .668 .766 .782 .567 .730 - .416 .593 .215 - .584
MAEM .839 .737 .515 .377 .516 1.002 - .395 .564 .000 - .549
IT Fl .576 .747 .646 .770 .697 .757 - .254 .630 .087 - .574
MAEM .707 .781 .809 .887 .829 .746 - .053 .403 .000 - .579
</table>
<tableCaption confidence="0.997285">
Table 4: Linear Chain CRFs experiments. F1 on sentence-level aspect identification (higher is better).
MAEM on sentence-level sentiment assignment (only on correctly identified aspects, lower is better).
</tableCaption>
<bodyText confidence="0.961399375">
and Italian.
MAEM values are all similar across languages,
again confirming what has been observed on
agreement. However, MAEM values on experi-
ments are sensibly worse than those measured on
agreement, possibly due to the fact that we used
very basic features, with limited use of sentiment-
related information.
</bodyText>
<sectionHeader confidence="0.995732" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999458125">
We have presented Trip-MAML a multi-lingual
extension of Trip-MA, originally presented in
(Marcheggiani et al., 2014). The extension pro-
cess involved crawling and selecting the reviews
for the two new languages, Spanish and Italian,
and their annotation by a total of six native lan-
guage speakers. We measured dataset statistics
and inter-annotator agreement, which show that
the new ES and IT parts we produced are consis-
tent with the original EN part. We also presented
experiments on the dataset, based on a linear chain
CRFs model for the automatic detection of aspects
and their sentiment labels, establishing a baseline
for future research. Trip-MAML enables the ex-
ploration of cross-lingual approaches to the prob-
lem of multi-aspect sentiment classification.
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99782">
This work has been partially supported by ATTOS
project (TIN2012-38536-C03-0) from the Spanish
Government.
</bodyText>
<sectionHeader confidence="0.999309" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999032787234043">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Evaluation measures for ordinal re-
gression. In Proocedings of the 9th Conference on
Intelligent Systems Design and Applications (ISDA
2009), pages 283–287.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200–2204.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2010. Multilingual subjectivity: Are more lan-
guages better? In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ’10, pages 28–36, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Judith A Chevalier and Dina Mayzlin. 2006. The ef-
fect of word of mouth on sales: Online book re-
views. Journal of marketing research, 43(3):345–
354.
Kerstin Denecke. 2008. Using sentiwordnet for mul-
tilingual sentiment analysis. In Data Engineering
Workshop, 2008. ICDEW 2008. IEEE 24th Interna-
tional Conference on, pages 507–512. IEEE.
Aitor Gonzalez-Agirre, Egoitz Laparra, and German
Rigau. 2012. Multilingual central repository ver-
sion 3.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC 2012), pages 2525–2529.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.
Roman Klinger and Philipp Cimiano. 2014. The usage
review corpus for fine-grained, multi-lingual opin-
ion analysis. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC), Reyk-
javik, Iceland, May. ELRA.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 18th Inter-
national Conference on Machine Learning (ICML
2001), pages 282–289.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
</reference>
<page confidence="0.892204">
2537
</page>
<reference confidence="0.998015876923077">
discourse representations. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics (ACL 2013), pages
1630–1639.
Diego Marcheggiani, Oscar T¨ackstr¨om, Andrea Esuli,
and Fabrizio Sebastiani. 2014. Hierarchical multi-
label conditional random fields for aspect-oriented
opinion mining. In Proceedings of the 36th Eu-
ropean Conference on IR Research (ECIR 2014),
pages 273–285.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. Multiwordnet: developing an aligned
multilingual database. In Proceedings of the first in-
ternational conference on global WordNet, volume
152, pages 55–63.
Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural corre-
spondence learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 1118–1127.
Peter Prettenhofer and Benno Stein. 2011. Cross-
lingual adaptation using structural correspondence
learning. ACM Transactions on Intelligent Systems
and Technology (TIST), 3(1):13.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP 2013), volume 1631, page 1642.
Oscar T¨ackstr¨om and Ryan McDonald. 2011. Dis-
covering fine-grained sentiment with latent variable
structured prediction models. In Proceedings of the
33rd European Conference on Advances in Informa-
tion Retrieval (ECIR 2011), pages 368–374.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, pages 235–243.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data:
A rating regression approach. In Proceedings of
the 16th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD
2010), pages 783–792.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational linguistics, 35(3):399–433.
</reference>
<page confidence="0.990063">
2538
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.682198">
<title confidence="0.9990855">A Multi-lingual Annotated for Aspect-Oriented Opinion Mining</title>
<author confidence="0.999223">Maria Jim´enez Giacomo Andrea Maria Teresa Alejandro Moreo</author>
<affiliation confidence="0.993599666666667">de Inform´atica, Escuela Polit´ecnica Superior de Universidad de Ja´en, E-23071 - Ja´en, di Scienza e Tecnologie dell’Informazione “A.</affiliation>
<address confidence="0.739825">Consiglio Nazionale delle Ricerche, I-56124 - Pisa,</address>
<abstract confidence="0.9970211">We present the Trip-MAML dataset, a Multi-Lingual dataset of hotel reviews that have been manually annotated at the sentence-level with Multi-Aspect sentiment labels. This dataset has been built as an extension of an existent English-only dataset, adding documents written in Italian and Spanish. We detail the dataset construction process, covering the data gathering, selection, and annotation. We present inter-annotator agreement figures and baseline experimental results, comparing the three languages. Trip-MAML is a multi-lingual dataset for aspect-oriented opinion mining that enables researchers (i) to face the problem on languages other than English and (ii) to the experiment the application of cross-lingual learning methods to the task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Evaluation measures for ordinal regression.</title>
<date>2009</date>
<booktitle>In Proocedings of the 9th Conference on Intelligent Systems Design and Applications (ISDA</booktitle>
<pages>283--287</pages>
<contexts>
<context position="12497" citStr="Baccianella et al., 2009" startWordPosition="1963" endWordPosition="1966">the lists of the shared aspects ranked by their relative frequency, measured pairwise among the three parts, is 0.795, which indicates a good uniformity of content among the parts. In all the three parts, Positive is the most frequent sentiment label, followed by Negative. Location is always the aspect with the highest frequency of positive labels. 3 Inter-annotator Agreement We measured the inter-annotator agreement in two steps. The F1 score measures the agreement on aspect identification, regardless of the sentiment label assigned. Then symmetric Macro-averaged Mean Absolute Error (sMAEM) (Baccianella et al., 2009) measures the agreement on sentiment labels on the annotations for which the annotators agreed at the aspect level. Aspect NotRelated is not included in agreement evaluation, nor in the experiments of Section 4. sMAEM is computed between each of the three possible pairs of annotators and then averaged to determine the agreement values reported in Table 3. Agreement on aspect detection is higher for ES and IT than for EN. This difference is in part motivated by the fact that the two aspects that are missing in ES and IT have low agreement on EN, and the novel Sleep Quality aspect has instead a </context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2009</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2009. Evaluation measures for ordinal regression. In Proocedings of the 9th Conference on Intelligent Systems Design and Applications (ISDA 2009), pages 283–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<volume>10</volume>
<pages>2200--2204</pages>
<contexts>
<context position="15468" citStr="Baccianella et al., 2010" startWordPosition="2545" endWordPosition="2548">e aimed at defining a shared baseline for future experiments. For this reason we chose a relatively simple setup that uses a simple learning model and minimal linguistic resources. We used a sentence-level Linear Chain (LC) Conditional Random Field (Lafferty et al., 2001) as described by Marcheggiani et al. (2014). With respect to the features extracted from text, we used three simple features types: word unigrams, bigrams, and SentiWordNet-based features, which consist of a Positive and a Negative feature extracted every time the review contains a word that is marked as such in SentiWordNet (Baccianella et al., 2010). To use SentiWordNet on ES and IT, we used Multilingual Central Repository (Gonzalez-Agirre et al., 2012) and MultiWordNet (Pianta et al., 2002) to map sentiment labels to Spanish and to Italian, respectively. Experiments were run separately on the EN, ES, and IT parts, leaving cross-lingual experiments to future work. On each part we built five 70%/30% train/test splits, randomly generated by sampling the reviews annotated by single reviewers (we left out reviews annotated by all the reviewers, as we consider that part of the dataset more useful as a validation set for the optimization of me</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multilingual subjectivity: Are more languages better?</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5513" citStr="Banea et al. (2010)" startWordPosition="830" endWordPosition="833"> to test cross-lingual document polarity classification using an English training set. Klinger and Cimiano (2014) produced a bi-lingual dataset (English and German), named USAGE, in which aspect expressions and subjective expressions are annotated in Amazon product reviews. In (Klinger and Cimiano, 2014) aspect expressions can be any piece of text that mentions a relevant property of the reviewed entity (e.g., washer, hose, looks) and are not categorical label, as in our dataset. The USAGE dataset is thus more oriented at information extraction rather than at text classification applications. Banea et al. (2010) used machine translation to create a multilingual version of the information-extraction oriented MPQA dataset (Wiebe et al., 2005) on six languages (English, Arabic, French, German, Romanian and Spanish). In this paper we present Trip-MAML, which extends the Trip-MA1 dataset of Marcheggiani et al. (2014) with Italian and Spanish annotated reviews. We describe Trip-MAML and report experiments aimed at defining a first baseline. Both the dataset and the software used in experiments are publicly available at http://hlt.isti. cnr.it/trip-maml/. 2 Annotation Process We recall the annotation proces</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, 2010</marker>
<rawString>Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 2010. Multilingual subjectivity: Are more languages better? In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 28–36, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith A Chevalier</author>
<author>Dina Mayzlin</author>
</authors>
<title>The effect of word of mouth on sales: Online book reviews.</title>
<date>2006</date>
<journal>Journal of marketing research,</journal>
<volume>43</volume>
<issue>3</issue>
<pages>354</pages>
<contexts>
<context position="1614" citStr="Chevalier and Mayzlin, 2006" startWordPosition="215" endWordPosition="218">ults, comparing the three languages. Trip-MAML is a multi-lingual dataset for aspect-oriented opinion mining that enables researchers (i) to face the problem on languages other than English and (ii) to the experiment the application of cross-lingual learning methods to the task. 1 Introduction Reviews of products and services that are spontaneously produced by customers represent a source of unquestionable value not only for marketing strategies of private companies and organizations, but also for other users since their purchasing decisions are likely influenced by other customers’ opinions (Chevalier and Mayzlin, 2006). Overall ratings (e.g., in terms of a five stars rating scale), and also aspect-specific ratings (e.g., the Cleanliness or Location of a hotel), are the typical additional information expressed by customers in their reviews. Those ratings help to derive a number of global scores to facilitate a first screening of the product or service at hand. Notwithstanding, users who pay more attention to a particular aspect (e.g., the Rooms of a hotel) remain constrained to manually inspect the entire text of reviews in order to find out the reasons other users argued in that respect. Methods for automat</context>
</contexts>
<marker>Chevalier, Mayzlin, 2006</marker>
<rawString>Judith A Chevalier and Dina Mayzlin. 2006. The effect of word of mouth on sales: Online book reviews. Journal of marketing research, 43(3):345– 354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Denecke</author>
</authors>
<title>Using sentiwordnet for multilingual sentiment analysis.</title>
<date>2008</date>
<booktitle>In Data Engineering Workshop,</booktitle>
<pages>507--512</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4847" citStr="Denecke (2008)" startWordPosition="728" endWordPosition="729">tions, since they allow to build classifiers for many languages reusing the training data produced for a single language (typically English), probably giving up a bit of accuracy, but compensating it with a large save in terms of human annotation costs. Multi-lingual datasets are beneficial to the research community both as a benchmark to explore cross-lingual learning and also as resources on which to develop and test new NLP tools for languages other than English. Prettenhofer and Stein (2010) used a multi-lingual dataset focused on full-document classification at the global polarity level. Denecke (2008) used a dataset of 200 Amazon reviews in German to test cross-lingual document polarity classification using an English training set. Klinger and Cimiano (2014) produced a bi-lingual dataset (English and German), named USAGE, in which aspect expressions and subjective expressions are annotated in Amazon product reviews. In (Klinger and Cimiano, 2014) aspect expressions can be any piece of text that mentions a relevant property of the reviewed entity (e.g., washer, hose, looks) and are not categorical label, as in our dataset. The USAGE dataset is thus more oriented at information extraction ra</context>
</contexts>
<marker>Denecke, 2008</marker>
<rawString>Kerstin Denecke. 2008. Using sentiwordnet for multilingual sentiment analysis. In Data Engineering Workshop, 2008. ICDEW 2008. IEEE 24th International Conference on, pages 507–512. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitor Gonzalez-Agirre</author>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Multilingual central repository version 3.0.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>2525--2529</pages>
<contexts>
<context position="15574" citStr="Gonzalez-Agirre et al., 2012" startWordPosition="2563" endWordPosition="2566">mple setup that uses a simple learning model and minimal linguistic resources. We used a sentence-level Linear Chain (LC) Conditional Random Field (Lafferty et al., 2001) as described by Marcheggiani et al. (2014). With respect to the features extracted from text, we used three simple features types: word unigrams, bigrams, and SentiWordNet-based features, which consist of a Positive and a Negative feature extracted every time the review contains a word that is marked as such in SentiWordNet (Baccianella et al., 2010). To use SentiWordNet on ES and IT, we used Multilingual Central Repository (Gonzalez-Agirre et al., 2012) and MultiWordNet (Pianta et al., 2002) to map sentiment labels to Spanish and to Italian, respectively. Experiments were run separately on the EN, ES, and IT parts, leaving cross-lingual experiments to future work. On each part we built five 70%/30% train/test splits, randomly generated by sampling the reviews annotated by single reviewers (we left out reviews annotated by all the reviewers, as we consider that part of the dataset more useful as a validation set for the optimization of methods tested in future experiments). We then run the five experiments and averaged their results. 4.1 Eval</context>
</contexts>
<marker>Gonzalez-Agirre, Laparra, Rigau, 2012</marker>
<rawString>Aitor Gonzalez-Agirre, Egoitz Laparra, and German Rigau. 2012. Multilingual central repository version 3.0. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012), pages 2525–2529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2594" citStr="Hu and Liu, 2004" startWordPosition="379" endWordPosition="382">, users who pay more attention to a particular aspect (e.g., the Rooms of a hotel) remain constrained to manually inspect the entire text of reviews in order to find out the reasons other users argued in that respect. Methods for automatic analysis of the aspect-oriented sentiment expressed in reviews would enable highlighting aspect-relevant parts of the document, so as to allow users to perform a faster and focused inspection of them. Previous work on opinion mining (Pang and Lee, 2008) has already faced the overall sentiment prediction (Pang et al., 2002), multiple aspectoriented analysis (Hu and Liu, 2004), and finegrained phrase-level analysis (Wilson et al., 2009). Most of the available opinion mining datasets contain only documents written in English, as this language is the most used on the Internet and the one for which more NLP tools and resources are available. (Hu and Liu, 2004) worked on the summarization of reviews by means of weakly supervised feature mining. (T¨ackstr¨om and McDonald, 2011) used a finer-grained dataset in which global polarity annotation is applied also to each sentence composing the document. Similarly did (Socher et al., 2013) with the Stanford Sentiment Treebank,</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Klinger</author>
<author>Philipp Cimiano</author>
</authors>
<title>The usage review corpus for fine-grained, multi-lingual opinion analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC),</booktitle>
<publisher>ELRA.</publisher>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="5007" citStr="Klinger and Cimiano (2014)" startWordPosition="750" endWordPosition="753">ly giving up a bit of accuracy, but compensating it with a large save in terms of human annotation costs. Multi-lingual datasets are beneficial to the research community both as a benchmark to explore cross-lingual learning and also as resources on which to develop and test new NLP tools for languages other than English. Prettenhofer and Stein (2010) used a multi-lingual dataset focused on full-document classification at the global polarity level. Denecke (2008) used a dataset of 200 Amazon reviews in German to test cross-lingual document polarity classification using an English training set. Klinger and Cimiano (2014) produced a bi-lingual dataset (English and German), named USAGE, in which aspect expressions and subjective expressions are annotated in Amazon product reviews. In (Klinger and Cimiano, 2014) aspect expressions can be any piece of text that mentions a relevant property of the reviewed entity (e.g., washer, hose, looks) and are not categorical label, as in our dataset. The USAGE dataset is thus more oriented at information extraction rather than at text classification applications. Banea et al. (2010) used machine translation to create a multilingual version of the information-extraction orien</context>
</contexts>
<marker>Klinger, Cimiano, 2014</marker>
<rawString>Roman Klinger and Philipp Cimiano. 2014. The usage review corpus for fine-grained, multi-lingual opinion analysis. In Proceedings of the Language Resources and Evaluation Conference (LREC), Reykjavik, Iceland, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning (ICML</booktitle>
<pages>282--289</pages>
<contexts>
<context position="15115" citStr="Lafferty et al., 2001" startWordPosition="2489" endWordPosition="2492">agreement (only on matching aspects, lower is better). there is, in general, a higher or equal agreement in ES and IT with respect to EN, indicating that the formers two were annotated in a more consistent way. The agreement on assignment of sentiment label is rather similar across the whole dataset. 4 Experiments The experiments we present here are aimed at defining a shared baseline for future experiments. For this reason we chose a relatively simple setup that uses a simple learning model and minimal linguistic resources. We used a sentence-level Linear Chain (LC) Conditional Random Field (Lafferty et al., 2001) as described by Marcheggiani et al. (2014). With respect to the features extracted from text, we used three simple features types: word unigrams, bigrams, and SentiWordNet-based features, which consist of a Positive and a Negative feature extracted every time the review contains a word that is marked as such in SentiWordNet (Baccianella et al., 2010). To use SentiWordNet on ES and IT, we used Multilingual Central Repository (Gonzalez-Agirre et al., 2012) and MultiWordNet (Pianta et al., 2002) to map sentiment labels to Spanish and to Italian, respectively. Experiments were run separately on t</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (ICML 2001), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Ivan Titov</author>
<author>Caroline Sporleder</author>
</authors>
<title>A bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1630--1639</pages>
<contexts>
<context position="3403" citStr="Lazaridou et al., 2013" startWordPosition="511" endWordPosition="514">on the Internet and the one for which more NLP tools and resources are available. (Hu and Liu, 2004) worked on the summarization of reviews by means of weakly supervised feature mining. (T¨ackstr¨om and McDonald, 2011) used a finer-grained dataset in which global polarity annotation is applied also to each sentence composing the document. Similarly did (Socher et al., 2013) with the Stanford Sentiment Treebank, which annotates each syntactically plausible phrase in thousands of sentences using annotators from Amazon’s Mechanical Turk, annotating the polarity of phrases on a five-level scale. (Lazaridou et al., 2013) performed a single-label polarity annotation of elementary discourse units of TripAdvisor reviews, adopting ten aspect labels. (Marcheggiani et al., 2014) did a similar annotation work, using sentences as the annotation elements and adopting a multi-label polarity annotation, i.e., each sentence can be assigned to zero, one, or more than one aspect. Cross-lingual sentiment classification (Wan, 2009; Prettenhofer and Stein, 2011) explores the scenario in which training data are available for 2533 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2533</context>
</contexts>
<marker>Lazaridou, Titov, Sporleder, 2013</marker>
<rawString>Angeliki Lazaridou, Ivan Titov, and Caroline Sporleder. 2013. A bayesian model for joint unsupervised induction of sentiment, aspect and discourse representations. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1630–1639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego Marcheggiani</author>
<author>Oscar T¨ackstr¨om</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Hierarchical multilabel conditional random fields for aspect-oriented opinion mining.</title>
<date>2014</date>
<booktitle>In Proceedings of the 36th European Conference on IR Research (ECIR 2014),</booktitle>
<pages>273--285</pages>
<marker>Marcheggiani, T¨ackstr¨om, Esuli, Sebastiani, 2014</marker>
<rawString>Diego Marcheggiani, Oscar T¨ackstr¨om, Andrea Esuli, and Fabrizio Sebastiani. 2014. Hierarchical multilabel conditional random fields for aspect-oriented opinion mining. In Proceedings of the 36th European Conference on IR Research (ECIR 2014), pages 273–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="2470" citStr="Pang and Lee, 2008" startWordPosition="360" endWordPosition="363">gs help to derive a number of global scores to facilitate a first screening of the product or service at hand. Notwithstanding, users who pay more attention to a particular aspect (e.g., the Rooms of a hotel) remain constrained to manually inspect the entire text of reviews in order to find out the reasons other users argued in that respect. Methods for automatic analysis of the aspect-oriented sentiment expressed in reviews would enable highlighting aspect-relevant parts of the document, so as to allow users to perform a faster and focused inspection of them. Previous work on opinion mining (Pang and Lee, 2008) has already faced the overall sentiment prediction (Pang et al., 2002), multiple aspectoriented analysis (Hu and Liu, 2004), and finegrained phrase-level analysis (Wilson et al., 2009). Most of the available opinion mining datasets contain only documents written in English, as this language is the most used on the Internet and the one for which more NLP tools and resources are available. (Hu and Liu, 2004) worked on the summarization of reviews by means of weakly supervised feature mining. (T¨ackstr¨om and McDonald, 2011) used a finer-grained dataset in which global polarity annotation is app</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2541" citStr="Pang et al., 2002" startWordPosition="371" endWordPosition="374">ing of the product or service at hand. Notwithstanding, users who pay more attention to a particular aspect (e.g., the Rooms of a hotel) remain constrained to manually inspect the entire text of reviews in order to find out the reasons other users argued in that respect. Methods for automatic analysis of the aspect-oriented sentiment expressed in reviews would enable highlighting aspect-relevant parts of the document, so as to allow users to perform a faster and focused inspection of them. Previous work on opinion mining (Pang and Lee, 2008) has already faced the overall sentiment prediction (Pang et al., 2002), multiple aspectoriented analysis (Hu and Liu, 2004), and finegrained phrase-level analysis (Wilson et al., 2009). Most of the available opinion mining datasets contain only documents written in English, as this language is the most used on the Internet and the one for which more NLP tools and resources are available. (Hu and Liu, 2004) worked on the summarization of reviews by means of weakly supervised feature mining. (T¨ackstr¨om and McDonald, 2011) used a finer-grained dataset in which global polarity annotation is applied also to each sentence composing the document. Similarly did (Soche</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuele Pianta</author>
<author>Luisa Bentivogli</author>
<author>Christian Girardi</author>
</authors>
<title>Multiwordnet: developing an aligned multilingual database.</title>
<date>2002</date>
<booktitle>In Proceedings of the first international conference on global WordNet,</booktitle>
<volume>152</volume>
<pages>55--63</pages>
<contexts>
<context position="15613" citStr="Pianta et al., 2002" startWordPosition="2569" endWordPosition="2572"> minimal linguistic resources. We used a sentence-level Linear Chain (LC) Conditional Random Field (Lafferty et al., 2001) as described by Marcheggiani et al. (2014). With respect to the features extracted from text, we used three simple features types: word unigrams, bigrams, and SentiWordNet-based features, which consist of a Positive and a Negative feature extracted every time the review contains a word that is marked as such in SentiWordNet (Baccianella et al., 2010). To use SentiWordNet on ES and IT, we used Multilingual Central Repository (Gonzalez-Agirre et al., 2012) and MultiWordNet (Pianta et al., 2002) to map sentiment labels to Spanish and to Italian, respectively. Experiments were run separately on the EN, ES, and IT parts, leaving cross-lingual experiments to future work. On each part we built five 70%/30% train/test splits, randomly generated by sampling the reviews annotated by single reviewers (we left out reviews annotated by all the reviewers, as we consider that part of the dataset more useful as a validation set for the optimization of methods tested in future experiments). We then run the five experiments and averaged their results. 4.1 Evaluation Measures As for the agreement ev</context>
</contexts>
<marker>Pianta, Bentivogli, Girardi, 2002</marker>
<rawString>Emanuele Pianta, Luisa Bentivogli, and Christian Girardi. 2002. Multiwordnet: developing an aligned multilingual database. In Proceedings of the first international conference on global WordNet, volume 152, pages 55–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Prettenhofer</author>
<author>Benno Stein</author>
</authors>
<title>Crosslanguage text classification using structural correspondence learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1118--1127</pages>
<contexts>
<context position="4733" citStr="Prettenhofer and Stein (2010)" startWordPosition="710" endWordPosition="713">nguage that is different from the language of the test documents. Cross-lingual learning methods have important practical applications, since they allow to build classifiers for many languages reusing the training data produced for a single language (typically English), probably giving up a bit of accuracy, but compensating it with a large save in terms of human annotation costs. Multi-lingual datasets are beneficial to the research community both as a benchmark to explore cross-lingual learning and also as resources on which to develop and test new NLP tools for languages other than English. Prettenhofer and Stein (2010) used a multi-lingual dataset focused on full-document classification at the global polarity level. Denecke (2008) used a dataset of 200 Amazon reviews in German to test cross-lingual document polarity classification using an English training set. Klinger and Cimiano (2014) produced a bi-lingual dataset (English and German), named USAGE, in which aspect expressions and subjective expressions are annotated in Amazon product reviews. In (Klinger and Cimiano, 2014) aspect expressions can be any piece of text that mentions a relevant property of the reviewed entity (e.g., washer, hose, looks) and </context>
</contexts>
<marker>Prettenhofer, Stein, 2010</marker>
<rawString>Peter Prettenhofer and Benno Stein. 2010. Crosslanguage text classification using structural correspondence learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 1118–1127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Prettenhofer</author>
<author>Benno Stein</author>
</authors>
<title>Crosslingual adaptation using structural correspondence learning.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>3--1</pages>
<contexts>
<context position="3836" citStr="Prettenhofer and Stein, 2011" startWordPosition="574" endWordPosition="577">tes each syntactically plausible phrase in thousands of sentences using annotators from Amazon’s Mechanical Turk, annotating the polarity of phrases on a five-level scale. (Lazaridou et al., 2013) performed a single-label polarity annotation of elementary discourse units of TripAdvisor reviews, adopting ten aspect labels. (Marcheggiani et al., 2014) did a similar annotation work, using sentences as the annotation elements and adopting a multi-label polarity annotation, i.e., each sentence can be assigned to zero, one, or more than one aspect. Cross-lingual sentiment classification (Wan, 2009; Prettenhofer and Stein, 2011) explores the scenario in which training data are available for 2533 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2533–2538, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. a language that is different from the language of the test documents. Cross-lingual learning methods have important practical applications, since they allow to build classifiers for many languages reusing the training data produced for a single language (typically English), probably giving up a bit of accuracy, but compensating it wit</context>
</contexts>
<marker>Prettenhofer, Stein, 2011</marker>
<rawString>Peter Prettenhofer and Benno Stein. 2011. Crosslingual adaptation using structural correspondence learning. ACM Transactions on Intelligent Systems and Technology (TIST), 3(1):13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing (EMNLP 2013),</booktitle>
<volume>volume</volume>
<pages>1631--1642</pages>
<contexts>
<context position="3156" citStr="Socher et al., 2013" startWordPosition="473" endWordPosition="476">2002), multiple aspectoriented analysis (Hu and Liu, 2004), and finegrained phrase-level analysis (Wilson et al., 2009). Most of the available opinion mining datasets contain only documents written in English, as this language is the most used on the Internet and the one for which more NLP tools and resources are available. (Hu and Liu, 2004) worked on the summarization of reviews by means of weakly supervised feature mining. (T¨ackstr¨om and McDonald, 2011) used a finer-grained dataset in which global polarity annotation is applied also to each sentence composing the document. Similarly did (Socher et al., 2013) with the Stanford Sentiment Treebank, which annotates each syntactically plausible phrase in thousands of sentences using annotators from Amazon’s Mechanical Turk, annotating the polarity of phrases on a five-level scale. (Lazaridou et al., 2013) performed a single-label polarity annotation of elementary discourse units of TripAdvisor reviews, adopting ten aspect labels. (Marcheggiani et al., 2014) did a similar annotation work, using sentences as the annotation elements and adopting a multi-label polarity annotation, i.e., each sentence can be assigned to zero, one, or more than one aspect. </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP 2013), volume 1631, page 1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
</authors>
<title>Discovering fine-grained sentiment with latent variable structured prediction models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33rd European Conference on Advances in Information Retrieval (ECIR</booktitle>
<pages>368--374</pages>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>Oscar T¨ackstr¨om and Ryan McDonald. 2011. Discovering fine-grained sentiment with latent variable structured prediction models. In Proceedings of the 33rd European Conference on Advances in Information Retrieval (ECIR 2011), pages 368–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>235--243</pages>
<contexts>
<context position="3805" citStr="Wan, 2009" startWordPosition="572" endWordPosition="573">hich annotates each syntactically plausible phrase in thousands of sentences using annotators from Amazon’s Mechanical Turk, annotating the polarity of phrases on a five-level scale. (Lazaridou et al., 2013) performed a single-label polarity annotation of elementary discourse units of TripAdvisor reviews, adopting ten aspect labels. (Marcheggiani et al., 2014) did a similar annotation work, using sentences as the annotation elements and adopting a multi-label polarity annotation, i.e., each sentence can be assigned to zero, one, or more than one aspect. Cross-lingual sentiment classification (Wan, 2009; Prettenhofer and Stein, 2011) explores the scenario in which training data are available for 2533 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2533–2538, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. a language that is different from the language of the test documents. Cross-lingual learning methods have important practical applications, since they allow to build classifiers for many languages reusing the training data produced for a single language (typically English), probably giving up a bit of ac</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 235–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongning Wang</author>
<author>Yue Lu</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Latent aspect rating analysis on review text data: A rating regression approach.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD</booktitle>
<pages>783--792</pages>
<contexts>
<context position="6755" citStr="Wang et al. (2010)" startWordPosition="1030" endWordPosition="1033">i et al. (2014) for Trip-MA and the procedure we employed to extend it into TripMAML. We will use the national codes EN, ES, 1Marcheggiani et al. (2014) gave no name to their dataset, here we name it Trip-MA to identify its source and its multiaspect nature. and IT, to denote the English, Spanish, and Italian parts of the Trip-MAML dataset, respectively. Note that EN coincides with Trip-MA. 2.1 English Reviews The Trip-MA dataset was created by Marcheggiani et al. (2014) by annotating a set of 442 reviews, written in English, randomly sampled from the publicly available TripAdvisor dataset of Wang et al. (2010), composed by 235,793 reviews. Each review comes with an overall rating on a discrete ordinal scale from 1 to 5 “stars”. The dataset was annotated according to 9 recurrent aspects frequently involved in hotel reviews: Rooms, Cleanliness, Value, Service, Location, Check-in, Business, Food, and Building. The last two are not officially rated by TripAdvisor but were added because they are frequently commented in reviews. Two “catchall” aspects, Other and NotRelated, were also added, for a total of 11 aspect. Aspect Other denotes opinions that are pertinent to the hotel being reviewed, but not rel</context>
</contexts>
<marker>Wang, Lu, Zhai, 2010</marker>
<rawString>Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010. Latent aspect rating analysis on review text data: A rating regression approach. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2010), pages 783–792.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="5644" citStr="Wiebe et al., 2005" startWordPosition="850" endWordPosition="853">gual dataset (English and German), named USAGE, in which aspect expressions and subjective expressions are annotated in Amazon product reviews. In (Klinger and Cimiano, 2014) aspect expressions can be any piece of text that mentions a relevant property of the reviewed entity (e.g., washer, hose, looks) and are not categorical label, as in our dataset. The USAGE dataset is thus more oriented at information extraction rather than at text classification applications. Banea et al. (2010) used machine translation to create a multilingual version of the information-extraction oriented MPQA dataset (Wiebe et al., 2005) on six languages (English, Arabic, French, German, Romanian and Spanish). In this paper we present Trip-MAML, which extends the Trip-MA1 dataset of Marcheggiani et al. (2014) with Italian and Spanish annotated reviews. We describe Trip-MAML and report experiments aimed at defining a first baseline. Both the dataset and the software used in experiments are publicly available at http://hlt.isti. cnr.it/trip-maml/. 2 Annotation Process We recall the annotation process adopted by Marcheggiani et al. (2014) for Trip-MA and the procedure we employed to extend it into TripMAML. We will use the natio</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<booktitle>Computational linguistics,</booktitle>
<pages>35--3</pages>
<contexts>
<context position="2655" citStr="Wilson et al., 2009" startWordPosition="388" endWordPosition="391">., the Rooms of a hotel) remain constrained to manually inspect the entire text of reviews in order to find out the reasons other users argued in that respect. Methods for automatic analysis of the aspect-oriented sentiment expressed in reviews would enable highlighting aspect-relevant parts of the document, so as to allow users to perform a faster and focused inspection of them. Previous work on opinion mining (Pang and Lee, 2008) has already faced the overall sentiment prediction (Pang et al., 2002), multiple aspectoriented analysis (Hu and Liu, 2004), and finegrained phrase-level analysis (Wilson et al., 2009). Most of the available opinion mining datasets contain only documents written in English, as this language is the most used on the Internet and the one for which more NLP tools and resources are available. (Hu and Liu, 2004) worked on the summarization of reviews by means of weakly supervised feature mining. (T¨ackstr¨om and McDonald, 2011) used a finer-grained dataset in which global polarity annotation is applied also to each sentence composing the document. Similarly did (Socher et al., 2013) with the Stanford Sentiment Treebank, which annotates each syntactically plausible phrase in thous</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational linguistics, 35(3):399–433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
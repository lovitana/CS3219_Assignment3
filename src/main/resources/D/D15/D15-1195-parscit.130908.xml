<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018553">
<title confidence="0.986009">
Script Induction as Language Modeling
</title>
<author confidence="0.999269">
Rachel Rudinger1, Pushpendre Rastogi1, Francis Ferraro1, and Benjamin Van Durme1,2
</author>
<affiliation confidence="0.985103333333333">
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.977467" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992625">
The narrative cloze is an evaluation met-
ric commonly used for work on automatic
script induction. While prior work in this
area has focused on count-based meth-
ods from distributional semantics, such as
pointwise mutual information, we argue
that the narrative cloze can be productively
reframed as a language modeling task. By
training a discriminative language model
for this task, we attain improvements of up
to 27 percent over prior methods on stan-
dard narrative cloze metrics.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999520714285714">
Although the concept of scripts in artificial intelli-
gence dates back to the 1970s (Schank and Abel-
son, 1977), interest in this topic has renewed with
recent efforts to automatically induce scripts from
text on a large scale. One particularly influential
work in this area, Chambers and Jurafsky (2008),
treats the problem of script induction as one of
learning narrative chains, which they accomplish
using simple textual co-occurrence statistics. For
the novel task of learning narrative chains, they
introduce a new evaluation metric, the narrative
cloze test, which involves predicting a missing
event from a chain of events drawn from text.
Several follow-up works (Chambers and Jurafsky,
2009; Jans et al., 2012; Pichotta and Mooney,
2014; Rudinger et al., 2015) employ and ex-
tend Chambers and Jurafsky (2008)’s methods for
learning narrative chains, each using the narrative
cloze to evaluate their work. 1
In this paper, we take the position that the nar-
rative cloze test, which has been treated predom-
</bodyText>
<note confidence="0.9928886">
1A number of related works on script induction use alter-
native task formulations and evaluations. (Chambers, 2013;
Cheung et al., 2013; Cheung and Penn, 2013; Frermann et
al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Reg-
neri et al., 2010)
</note>
<bodyText confidence="0.99991875">
inantly as a method for evaluating script knowl-
edge, is more productively thought of simply as a
language modeling task.2 To support this claim,
we demonstrate a marked improvement over pre-
vious methods on this task using a powerful dis-
criminative language model – the Log-Bilinear
model (LBL). Based on this finding, we believe
one of the following conclusions must follow: ei-
ther discriminative language models are a more
effective technique for script induction than pre-
vious methods, or the narrative cloze test is not a
suitable evaluation for this task.3
</bodyText>
<sectionHeader confidence="0.991307" genericHeader="method">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.999607157894737">
Following the definitions of Chambers and Juraf-
sky (2008), a narrative chain is “a partially or-
dered set of narrative events that share a common
actor,” where a narrative event is “a tuple of an
event (most simply a verb) and its participants,
represented as typed dependencies.” (De Marneffe
et al., 2006) Formally, e := (v, d), where e is a
narrative event, v is a verb lemma, and d is the
syntactic dependency (nsubj or dobj) between v
and the protagonist. As an example, consider the
following narrative:
John studied for the exam and aced it.
His teacher congratulated him.
With John as protagonist, we have a se-
quence of three narrative events: (study, nsubj),
(ace, nsubj), and (congratulate, dobj).
In the narrative cloze test, a sequence of nar-
rative events (like the example provided here) is
extracted automatically from a document, and one
</bodyText>
<footnote confidence="0.99931925">
2Manshadi et al. (2008) also take a language modeling
approach to event prediction, although their experiments are
not directly comparable.
3We note that, whether the narrative cloze was originally
intended as a rigorous evaluation of script induction tech-
niques or merely a preliminary metric, we are motivated by
the observation that this evaluation has nonetheless become a
standard metric for this task.
</footnote>
<page confidence="0.858993">
1681
</page>
<note confidence="0.683069">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1681–1686,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999673214285714">
narrative event is removed; the task is to predict
the missing event.
didates are ranked by their observed frequency in
training, without regard to context.
Data Each of the models discussed in the fol-
lowing section are trained and tested on chains of
narrative events extracted from stories in the New
York Times portion of the Gigaword corpus (Graff
et al., 2003) with Concrete annotations (Ferraro et
al., 2014). Training is on the entirety of the 1994–
2006 portion (16,688,422 chains with 58,515,838
narrative events); development is a subset of the
2007–2008 portion (10,000 chains with 35,109
events); and test is a subset of the 2009–2010 por-
tion (5,000 chains with 17,836 events). All ex-
tracted chains are of length two or greater.
Chain Extraction To extract chains of narra-
tive events for training and testing, we rely on
the (automatically-generated) coreference chains
present in Concretely Annotated Gigaword. Each
narrative event in an extracted chain is derived
from a single mention in the corresponding coref-
erence chain, i.e., it consists of the verb and syn-
tactic dependency (nsubj or dobj) that governs
the head of the mention, if such a dependency ex-
ists. Overlapping mentions within a coreference
chain are collapsed to a single mention to avoid
redundant extractions.
</bodyText>
<sectionHeader confidence="0.994417" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.999984058823529">
In this section we present each of the models we
train for the narrative cloze evaluation. In a sin-
gle narrative cloze test, a sequence of narrative
events, (e1, · · · , eL), with an insertion point, k,
for the missing event is provided. Given a fixed
vocabulary of narrative events, V, a candidate se-
quence is generated for each vocabulary item by
inserting that item into the sequence at index k.
Each model generates a score for the candidate se-
quences, yielding a ranking over the vocabulary
items. The rank assigned to the actual missing vo-
cabulary item is the score the model receives on
that cloze test. In this case, we set V to include
all narrative events, e, that occur at least ten times
in training, yielding a vocabulary size of 12,452.
All out-of-vocabulary events are converted to (and
scored as) the symbol UNK.
</bodyText>
<subsectionHeader confidence="0.732527">
3.1 Count-based Methods
</subsectionHeader>
<bodyText confidence="0.999318428571428">
Unigram Baseline (UNI) A simple but strong
baseline introduced by Pichotta and Mooney
(2014) for this task is the unigram model: can-
Unordered PMI (UOP) The original model for
this task, proposed by Chambers and Jurafsky
(2008), is based on the pointwise mutual informa-
tion (PMI) between events.
</bodyText>
<equation confidence="0.9933855">
C(e1, e2)
pmi(e1, e2) a log C(e1, *)C(*, e2) (1)
</equation>
<bodyText confidence="0.999492428571429">
Here, C(e1, e2) is the number of times e1 and e2
occur in the same narrative event sequence, i.e.,
the number of times they “had a coreferring entity
filling the values of [their] dependencies,” and the
ordering of e1 and e2 is not considered. In our
implementation, individual counts are defined as
follows:
</bodyText>
<equation confidence="0.9603665">
EC(e, *) := C(e, e&apos;) (2)
e&apos;EV
</equation>
<bodyText confidence="0.9937785">
This model selects the best candidate event in a
given cloze test according to the following score:
</bodyText>
<equation confidence="0.989980333333333">
L
eˆ = arg max pmi(e, ei) (3)
eEV i=1
</equation>
<bodyText confidence="0.999909076923077">
We tune this model with an option to apply a mod-
ified version of discounting for PMI from Pantel
and Ravichandran (2004).
Ordered PMI (OP) This model is a slight vari-
ation on Unordered PMI introduced by Jans et al.
(2012). The only distinction is that C(e1, e2) is
treated as an asymmetric count, sensitive to the or-
der in which e1 and e2 occur within a chain.
Bigram Probability (BG) Another variant intro-
duced by Jans et al. (2012), the “bigram proba-
bility” model uses conditional probabilities rather
than PMI to compute scores. In a cloze test, this
model selects the following event:
</bodyText>
<equation confidence="0.844843">
eˆ = arg max
eEV
</equation>
<bodyText confidence="0.9997758">
where p(e2|e1) = C(e1,e2) C(e1,∗)and C(e1, e2) is asym-
metric. We tune this model with an option to per-
form absolute discounting. Note that this model is
not a bigram model in the typical language mod-
eling sense.
</bodyText>
<equation confidence="0.996528333333333">
k L
p(e|ei) H p(ei|e) (4)
i=1 i=k+1
</equation>
<page confidence="0.971836">
1682
</page>
<table confidence="0.996961166666667">
Len UNI UOP OP BG LBL2 LBL4 Tests
2 490 1887 2363 1613 369 371 5668
3 452 1271 1752 1009 330 334 2793
4 323 806 1027 502 229 232 1616
5 364 735 937 442 254 243 1330
6 347 666 891 483 257 249 942
7 330 629 838 468 241 237 630
8 259 466 510 278 208 201 512
9 299 610 639 348 198 195 396
10+ 331 472 397 277 240 229 3949
ALL 400 1115 1382 868 294 292 17836
(a) Average Rank
Len UNI UOP OP BG LBL2 LBL4 Tests
2 23.9 09.4 11.9 23.8 34.0 34.1 5668
3 28.8 08.2 11.1 28.0 36.3 35.6 2793
4 33.9 07.7 14.4 32.2 38.7 38.7 1616
5 33.4 10.1 18.7 34.0 39.6 40.3 1330
6 34.8 10.9 22.2 36.8 40.5 41.9 942
7 32.5 12.2 24.0 34.9 39.4 39.2 630
8 36.7 13.7 21.7 38.7 41.6 43.2 512
9 37.9 15.2 28.5 39.1 41.7 43.2 396
10+ 31.4 18.5 24.0 32.7 35.7 35.7 3949
ALL 29.5 11.6 16.8 29.8 36.5 36.6 17836
(c) Percent Recall at 10
Len UNI UOP OP BG LBL2 LBL4 Tests
2 .148 .053 .077 .149 .205 .204 5668
3 .179 .043 .065 .164 .217 .215 2793
4 .226 .042 .064 .195 .253 .253 1616
5 .225 .049 .076 .213 .261 .266 1330
6 .213 .054 .079 .214 .254 .263 942
7 .213 .061 .092 .215 .243 .247 630
8 .235 .063 .091 .244 .268 .278 512
9 .259 .058 .107 .252 .280 .278 396
10+ .191 .082 .113 .193 .198 .205 3949
ALL .186 .057 .083 .181 .221 .223 17836
(b) Mean Reciprocal Rank (MRR)
Len UNI UOP OP BG LBL2 LBL4 Tests
2 41.7 16.9 25.5 38.6 51.2 51.0 5668
3 46.8 20.2 30.2 45.0 54.8 54.0 2793
4 53.8 25.3 37.8 54.0 59.0 60.0 1616
5 52.5 29.9 40.5 54.3 59.1 61.1 1330
6 53.9 33.2 40.7 55.2 60.6 61.7 942
7 51.8 34.3 42.7 56.5 61.6 63.8 630
8 58.2 42.2 47.7 61.3 67.2 67.0 512
9 58.1 42.2 47.7 60.1 66.2 67.0 396
10+ 49.9 47.4 50.1 54.2 58.4 59.8 3949
ALL 48.0 28.6 36.4 48.3 56.3 56.8 17836
(d) Percent Recall at 50
</table>
<tableCaption confidence="0.975770666666667">
Table 1: Narrative cloze results bucketed by chain length for each model and scoring metric with best results in bold. The
models are Unigram Model (UNI), Unordered PMI (UOP), Ordered PMI (OP), Bigram Probability Model (BG), Log-Bilinear
Model N=2 (LBL2), Log-Bilinear Model N=4 (LBL4)
</tableCaption>
<bodyText confidence="0.999617555555556">
Skip N-gram We tune the previous three
models (UOP, OP, and BG) with the skip n-gram
counting methods introduced by Jans et al. (2012)
for this task, varying the ways in which the
counts, C(e1, e2), are collected. Using skip-n
counting, C(e1, e2) is incremented every time e1
and e2 co-occur within a window of size n. We
experiment with skip-0 (consecutive events only),
skip-3 (window size 3), and skip-all (entire chain
length) settings.
For each of the four narrative cloze scoring
metrics we report on (average rank, mean re-
ciprocal rank, recall at 10, and recall at 50),
we tune the Unordered PMI, Ordered PMI, and
Bigram Probability models over the following
parameter space: {skip-0, skip-3, skip-all} x
{discount, no-discount} x {T=4, T=10, T=20},
where T is a pairwise count threshold.
</bodyText>
<subsectionHeader confidence="0.99722">
3.2 A Discriminative Method
</subsectionHeader>
<bodyText confidence="0.969298571428571">
Log-Bilinear Language Model (LBL) The
Log-Bilinear language model is a language model
that was introduced by Mnih and Hinton (2007).
Like other language models, the LBL produces
a probability distribution over the next possible
word given a sequence of N previously observed
words. N is a hyper-parameter that determines the
size of the context used for computing the prob-
abilities. While many variants of the LBL have
been proposed since its introduction, we use the
simple variant described below.
Formally, we associate one context vector ce E
Rd, one bias parameter be E R, and one tar-
get vector te E Rd to each narrative event
e E V U { UNK, BOS, EOS }. V is the vocab-
ulary of events and BOS, EOS, and UNK are the
beginning-of-sequence, end-of-sequence, and out-
of-vocabulary symbols, respectively. The proba-
bility of an event e that appears after a sequence
s = [s1, s2, ... , sN] of context words is defined
as:
</bodyText>
<equation confidence="0.997836">
exp(t� eˆts + be)
p(e|s) =
exp(te&apos;ˆts + be/)
e&apos;∈V∪{ UNK, EOS }
</equation>
<bodyText confidence="0.997378166666667">
The O operator performs element-wise multiplica-
tion of two vectors. The parameters that are opti-
mized during training are mj Hj E [1, ... , N] and
ce, te He E V U { UNK, BOS, EOS }. To calcu-
late the log-probability of a sequence of narrative
events E = (e1, ... , eL) we compute:
</bodyText>
<equation confidence="0.978205">
�log(p(ei|fE(ei))) (5)
+ log(p(EOS|fE(EOS)))
</equation>
<bodyText confidence="0.6576175">
Here fE is a function that returns the sequence
of N words that precede the event ei in the se-
</bodyText>
<equation confidence="0.841289">
N
j=1
</equation>
<bodyText confidence="0.595273">
where
</bodyText>
<equation confidence="0.997694">
ˆts =
mj O csj
n
l(S) =
i=1
</equation>
<page confidence="0.979535">
1683
</page>
<figure confidence="0.997395384615385">
avgrnk 1000 mrr 0.20
500 0.15
0 0.10
0.05
0.00
UNI UOP OP BG LBL2LBL4 UNI UOP OP BG LBL2LBL4
model model
rec10 30 rec50 40
20 20
10 0
0
UNI UOP OP BG LBL2LBL4 UNI UOP OP BG LBL2LBL4
model model
</figure>
<figureCaption confidence="0.997652">
Figure 1: Narrative cloze results over all chain lengths. Unigram Model (UNI), Unordered PMI Model (UOP), Ordered PMI
Model (OP), Bigram Probability Model (BG), Log-Bilinear Model with context size 2 or 4 (LBL2, LBL4). Average Rank
(avgrnk), Mean Reciprocal Rank (mrr), % Recall at 10 (rec10), % Recall at 50 (rec50).
</figureCaption>
<bodyText confidence="0.998307538461538">
quence E&apos; made by prepending N BOS tokens and
appending a single EOS token to E.
The LBL models are trained by minimizing
the objective described in Equation 5 for all the
sequences in the training corpus. We used the
OxLM toolkit (Paul et al., 2014) which internally
uses Noise-Contrastive Estimation (NCE) (Gut-
mann and Hyv¨arinen, 2010) and processor paral-
lelization for speeding up the training. For this
task, we train LBL models with N = 2 (LBL2)
and N = 4 (LBL4). In our experiments, increas-
ing context size to N = 6 did not significantly
improve (or degrade) performance.
</bodyText>
<sectionHeader confidence="0.99062" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999962">
Table 1 shows the results of 17,836 narrative cloze
tests (derived from 5,000 held-out test chains),
with results bucketed by chain length. Perfor-
mance is reported on four metrics: average rank,
mean reciprocal rank, recall at 10, and recall at 50.
For each of the four metrics, the best overall
performance is achieved by one of the two LBL
models (context size 2 or 4); the LBL models
also achieve the best performance on every chain
length. Not only are the gains achieved by the
discriminative LBL consistent across metrics and
chain length, they are large. For average rank, the
LBL achieves a 27.0% relative improvement over
the best non-discriminative model; for mean re-
ciprocal rank, a 19.9% improvement; for recall at
10, a 22.8% improvement; and for recall at 50,
a 17.6% improvement. (See Figure 1.) Further-
more, note that both PMI models and the Bigram
model have been individually tuned for each met-
ric, while the LBL models have not. (The two LBL
models are tuned only for overall perplexity on the
development set.)
All models trend toward improved performance
on longer chains. Because the unigram model also
improves with chain length, it appears that longer
chains contain more frequent events and are thus
easier to predict. However, LBL performance is
also likely improving on longer chains because
of additional contextual information, as is evident
from LBL4’s slight relative gains over LBL2 on
longer chains.
</bodyText>
<sectionHeader confidence="0.995786" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99221575">
Pointwise mutual information and other related
count-based techniques have been used widely
to identify semantically similar words (Church
and Hanks, 1990; Lin and Pantel, 2001; Tur-
</bodyText>
<page confidence="0.987629">
1684
</page>
<bodyText confidence="0.99996664">
ney and Pantel, 2010), so it is natural that these
techniques have also been applied to the task
of script induction. Qualitatively, PMI often
identifies intuitively compelling matches; among
the top 15 events to share a high PMI with
(eat, nsubj) under the Unordered PMI model, for
example, we find events such as (overeat, nsubj),
(taste, nsubj), (smell, nsubj), (cook, nsubj),
and (serve, dobj). When evaluated by the narra-
tive cloze test, however, these count-based meth-
ods are overshadowed by the performance of a
general-purpose discriminative language model.
Our decision to attempt this task with the Log-
Bilinear model was motivated by the simple ob-
servation that the narrative cloze test is, in reality,
a language modeling task. Does the LBL’s suc-
cess on this task mean that work in script induc-
tion should abandon traditional count-based meth-
ods for discriminative language modeling tech-
niques? Or does it mean that an alternative eval-
uation metric is required to measure script knowl-
edge? While we believe our results are sufficient
to conclude that one of these alternatives is the
case, we leave the task of determining which to
future research.
</bodyText>
<sectionHeader confidence="0.997312" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999558181818182">
This work was supported by the Paul Allen In-
stitute for Artificial Intelligence (Acquisition and
Use of Paraphrases in a Knowledge-Rich Setting),
a National Science Foundation Graduate Research
Fellowship (Grant No. DGE-1232825), the Johns
Hopkins HLTCOE, and DARPA DEFT (FA8750-
13-2-001, Large Scale Paraphrasing for Natural
Language Understanding). We would also like to
thank three anonymous reviewers for their feed-
back. Any opinions expressed in this work are
those of the authors.
</bodyText>
<sectionHeader confidence="0.998545" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996775815384615">
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, pages 789–797, Colum-
bus, Ohio. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 602–610, Suntec,
Singapore. Association for Computational Linguis-
tics.
Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In EMNLP,
volume 13, pages 1797–1807.
Jackie Chi Kit Cheung and Gerald Penn. 2013. Prob-
abilistic domain modelling with contextualized dis-
tributional semantic vectors. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 392–401. Association for
Computational Linguistics.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 837–846, Atlanta, Georgia, June. Association
for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449–454.
Francis Ferraro, Max Thomas, Matthew R. Gormley,
Travis Wolfe, Craig Harman, and Benjamin Van
Durme. 2014. Concretely Annotated Corpora. In
4th Workshop on Automated Knowledge Base Con-
struction (AKBC).
Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014.
A hierarchical bayesian model for unsupervised in-
duction of script knowledge. EACL 2014, page 49.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2003. English gigaword. Linguistic Data
Consortium, Philadelphia.
Michael Gutmann and Aapo Hyv¨arinen. 2010. Noise-
contrastive estimation: A new estimation princi-
ple for unnormalized statistical models. In Inter-
national Conference on Artificial Intelligence and
Statistics, pages 297–304.
Bram Jans, Steven Bethard, Ivan Vuli´c, and Marie-
Francine Moens. 2012. Skip n-grams and ranking
functions for predicting script events. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics, pages 336–344, Avignon, France. Association
for Computational Linguistics.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proceedings of the
seventh ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 323–
328. ACM.
</reference>
<page confidence="0.816569">
1685
</page>
<reference confidence="0.999560111111112">
Mehdi Manshadi, Reid Swanson, and Andrew S Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In FLAIRS
Conference, pages 159–164.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641–648. ACM.
Ashutosh Modi and Ivan Titov. 2014. Inducing neural
models of script knowledge. CoNLL-2014, page 49.
Patrick Pantel and Deepak Ravichandran. 2004.
Automatically labeling semantic classes. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
321–328, Boston, Massachusetts, USA. Association
for Computational Linguistics.
Baltescu Paul, Blunsom Phil, and Hoang Hieu. 2014.
Oxlm: A neural language modelling framework for
machine translation. The Prague Bulletin of Mathe-
matical Linguistics, 102(1):81–92.
Karl Pichotta and Raymond Mooney. 2014. Statisti-
cal script learning with multi-argument events. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 220–229, Gothenburg, Sweden. As-
sociation for Computational Linguistics.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 979–988. Association for Computa-
tional Linguistics.
Rachel Rudinger, Vera Demberg, Ashutosh Modi, Ben-
jamin Van Durme, and Manfred Pinkal. 2015.
Learning to predict script events from domain-
specific text. Lexical and Computational Semantics
(* SEM 2015), page 205.
Roger Schank and Robert Abelson. 1977. Scripts,
plans, goals and understanding: An inquiry into hu-
man knowledge structures. Lawrence Erlbaum As-
sociates, Hillsdale, NJ.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188, January.
</reference>
<page confidence="0.992829">
1686
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.348260">
<title confidence="0.999891">Script Induction as Language Modeling</title>
<author confidence="0.984391">Pushpendre Francis</author>
<author confidence="0.984391">Benjamin Van</author>
<affiliation confidence="0.76426">for Language and Speech Language Technology Center of Johns Hopkins University</affiliation>
<abstract confidence="0.995703307692308">cloze an evaluation metric commonly used for work on automatic script induction. While prior work in this area has focused on count-based methods from distributional semantics, such as pointwise mutual information, we argue that the narrative cloze can be productively reframed as a language modeling task. By training a discriminative language model for this task, we attain improvements of up to 27 percent over prior methods on standard narrative cloze metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>789--797</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1045" citStr="Chambers and Jurafsky (2008)" startWordPosition="155" endWordPosition="158">tributional semantics, such as pointwise mutual information, we argue that the narrative cloze can be productively reframed as a language modeling task. By training a discriminative language model for this task, we attain improvements of up to 27 percent over prior methods on standard narrative cloze metrics. 1 Introduction Although the concept of scripts in artificial intelligence dates back to the 1970s (Schank and Abelson, 1977), interest in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale. One particularly influential work in this area, Chambers and Jurafsky (2008), treats the problem of script induction as one of learning narrative chains, which they accomplish using simple textual co-occurrence statistics. For the novel task of learning narrative chains, they introduce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate th</context>
<context position="2638" citStr="Chambers and Jurafsky (2008)" startWordPosition="411" endWordPosition="415">as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.2 To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model – the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: either discriminative language models are a more effective technique for script induction than previous methods, or the narrative cloze test is not a suitable evaluation for this task.3 2 Task Definition Following the definitions of Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.” (De Marneffe et al., 2006) Formally, e := (v, d), where e is a narrative event, v is a verb lemma, and d is the syntactic dependency (nsubj or dobj) between v and the protagonist. As an example, consider the following narrative: John studied for the exam and aced it. His teacher congratulated him. With John as protagonist, we have a sequence of three narrative events: (study</context>
<context position="6421" citStr="Chambers and Jurafsky (2008)" startWordPosition="1034" endWordPosition="1037">nces, yielding a ranking over the vocabulary items. The rank assigned to the actual missing vocabulary item is the score the model receives on that cloze test. In this case, we set V to include all narrative events, e, that occur at least ten times in training, yielding a vocabulary size of 12,452. All out-of-vocabulary events are converted to (and scored as) the symbol UNK. 3.1 Count-based Methods Unigram Baseline (UNI) A simple but strong baseline introduced by Pichotta and Mooney (2014) for this task is the unigram model: canUnordered PMI (UOP) The original model for this task, proposed by Chambers and Jurafsky (2008), is based on the pointwise mutual information (PMI) between events. C(e1, e2) pmi(e1, e2) a log C(e1, *)C(*, e2) (1) Here, C(e1, e2) is the number of times e1 and e2 occur in the same narrative event sequence, i.e., the number of times they “had a coreferring entity filling the values of [their] dependencies,” and the ordering of e1 and e2 is not considered. In our implementation, individual counts are defined as follows: EC(e, *) := C(e, e&apos;) (2) e&apos;EV This model selects the best candidate event in a given cloze test according to the following score: L eˆ = arg max pmi(e, ei) (3) eEV i=1 We tu</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of ACL-08: HLT, pages 789–797, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>602--610</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="1441" citStr="Chambers and Jurafsky, 2009" startWordPosition="214" endWordPosition="217">o the 1970s (Schank and Abelson, 1977), interest in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale. One particularly influential work in this area, Chambers and Jurafsky (2008), treats the problem of script induction as one of learning narrative chains, which they accomplish using simple textual co-occurrence statistics. For the novel task of learning narrative chains, they introduce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating scri</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 602–610, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
</authors>
<title>Event schema induction with a probabilistic entity-driven model.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<volume>13</volume>
<pages>1797--1807</pages>
<contexts>
<context position="1866" citStr="Chambers, 2013" startWordPosition="286" endWordPosition="287">uce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.2 To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model – the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: either discriminative language models are a more effective technique for script induction</context>
</contexts>
<marker>Chambers, 2013</marker>
<rawString>Nathanael Chambers. 2013. Event schema induction with a probabilistic entity-driven model. In EMNLP, volume 13, pages 1797–1807.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Gerald Penn</author>
</authors>
<title>Probabilistic domain modelling with contextualized distributional semantic vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>392--401</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1910" citStr="Cheung and Penn, 2013" startWordPosition="292" endWordPosition="295">ative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.2 To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model – the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: either discriminative language models are a more effective technique for script induction than previous methods, or the narrative clo</context>
</contexts>
<marker>Cheung, Penn, 2013</marker>
<rawString>Jackie Chi Kit Cheung and Gerald Penn. 2013. Probabilistic domain modelling with contextualized distributional semantic vectors. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392–401. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Probabilistic frame induction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>837--846</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1887" citStr="Cheung et al., 2013" startWordPosition="288" endWordPosition="291">tion metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.2 To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model – the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: either discriminative language models are a more effective technique for script induction than previous method</context>
</contexts>
<marker>Cheung, Poon, Vanderwende, 2013</marker>
<rawString>Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Vanderwende. 2013. Probabilistic frame induction. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 837–846, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="14811" citStr="Church and Hanks, 1990" startWordPosition="2574" endWordPosition="2577">only for overall perplexity on the development set.) All models trend toward improved performance on longer chains. Because the unigram model also improves with chain length, it appears that longer chains contain more frequent events and are thus easier to predict. However, LBL performance is also likely improving on longer chains because of additional contextual information, as is evident from LBL4’s slight relative gains over LBL2 on longer chains. 5 Conclusion Pointwise mutual information and other related count-based techniques have been used widely to identify semantically similar words (Church and Hanks, 1990; Lin and Pantel, 2001; Tur1684 ney and Pantel, 2010), so it is natural that these techniques have also been applied to the task of script induction. Qualitatively, PMI often identifies intuitively compelling matches; among the top 15 events to share a high PMI with (eat, nsubj) under the Unordered PMI model, for example, we find events such as (overeat, nsubj), (taste, nsubj), (smell, nsubj), (cook, nsubj), and (serve, dobj). When evaluated by the narrative cloze test, however, these count-based methods are overshadowed by the performance of a general-purpose discriminative language model. Ou</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Ferraro</author>
<author>Max Thomas</author>
<author>Matthew R Gormley</author>
<author>Travis Wolfe</author>
<author>Craig Harman</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Concretely Annotated Corpora.</title>
<date>2014</date>
<booktitle>In 4th Workshop on Automated Knowledge Base Construction (AKBC).</booktitle>
<marker>Ferraro, Thomas, Gormley, Wolfe, Harman, Van Durme, 2014</marker>
<rawString>Francis Ferraro, Max Thomas, Matthew R. Gormley, Travis Wolfe, Craig Harman, and Benjamin Van Durme. 2014. Concretely Annotated Corpora. In 4th Workshop on Automated Knowledge Base Construction (AKBC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lea Frermann</author>
<author>Ivan Titov</author>
<author>Manfred Pinkal</author>
</authors>
<title>A hierarchical bayesian model for unsupervised induction of script knowledge.</title>
<date>2014</date>
<booktitle>EACL 2014,</booktitle>
<pages>49</pages>
<contexts>
<context position="1933" citStr="Frermann et al., 2014" startWordPosition="296" endWordPosition="299"> involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.2 To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model – the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: either discriminative language models are a more effective technique for script induction than previous methods, or the narrative cloze test is not a suitab</context>
</contexts>
<marker>Frermann, Titov, Pinkal, 2014</marker>
<rawString>Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014. A hierarchical bayesian model for unsupervised induction of script knowledge. EACL 2014, page 49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2003</date>
<booktitle>English gigaword. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="4406" citStr="Graff et al., 2003" startWordPosition="697" endWordPosition="700">s nonetheless become a standard metric for this task. 1681 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1681–1686, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. narrative event is removed; the task is to predict the missing event. didates are ranked by their observed frequency in training, without regard to context. Data Each of the models discussed in the following section are trained and tested on chains of narrative events extracted from stories in the New York Times portion of the Gigaword corpus (Graff et al., 2003) with Concrete annotations (Ferraro et al., 2014). Training is on the entirety of the 1994– 2006 portion (16,688,422 chains with 58,515,838 narrative events); development is a subset of the 2007–2008 portion (10,000 chains with 35,109 events); and test is a subset of the 2009–2010 portion (5,000 chains with 17,836 events). All extracted chains are of length two or greater. Chain Extraction To extract chains of narrative events for training and testing, we rely on the (automatically-generated) coreference chains present in Concretely Annotated Gigaword. Each narrative event in an extracted chai</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2003</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gutmann</author>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>Noisecontrastive estimation: A new estimation principle for unnormalized statistical models.</title>
<date>2010</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>297--304</pages>
<marker>Gutmann, Hyv¨arinen, 2010</marker>
<rawString>Michael Gutmann and Aapo Hyv¨arinen. 2010. Noisecontrastive estimation: A new estimation principle for unnormalized statistical models. In International Conference on Artificial Intelligence and Statistics, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bram Jans</author>
<author>Steven Bethard</author>
<author>Ivan Vuli´c</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Skip n-grams and ranking functions for predicting script events.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>336--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France.</location>
<marker>Jans, Bethard, Vuli´c, Moens, 2012</marker>
<rawString>Bram Jans, Steven Bethard, Ivan Vuli´c, and MarieFrancine Moens. 2012. Skip n-grams and ranking functions for predicting script events. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 336–344, Avignon, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>323--328</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="14833" citStr="Lin and Pantel, 2001" startWordPosition="2578" endWordPosition="2581">ity on the development set.) All models trend toward improved performance on longer chains. Because the unigram model also improves with chain length, it appears that longer chains contain more frequent events and are thus easier to predict. However, LBL performance is also likely improving on longer chains because of additional contextual information, as is evident from LBL4’s slight relative gains over LBL2 on longer chains. 5 Conclusion Pointwise mutual information and other related count-based techniques have been used widely to identify semantically similar words (Church and Hanks, 1990; Lin and Pantel, 2001; Tur1684 ney and Pantel, 2010), so it is natural that these techniques have also been applied to the task of script induction. Qualitatively, PMI often identifies intuitively compelling matches; among the top 15 events to share a high PMI with (eat, nsubj) under the Unordered PMI model, for example, we find events such as (overeat, nsubj), (taste, nsubj), (smell, nsubj), (cook, nsubj), and (serve, dobj). When evaluated by the narrative cloze test, however, these count-based methods are overshadowed by the performance of a general-purpose discriminative language model. Our decision to attempt </context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 323– 328. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Manshadi</author>
<author>Reid Swanson</author>
<author>Andrew S Gordon</author>
</authors>
<title>Learning a probabilistic model of event sequences from internet weblog stories.</title>
<date>2008</date>
<booktitle>In FLAIRS Conference,</booktitle>
<pages>159--164</pages>
<contexts>
<context position="1956" citStr="Manshadi et al., 2008" startWordPosition="300" endWordPosition="303">missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.2 To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model – the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: either discriminative language models are a more effective technique for script induction than previous methods, or the narrative cloze test is not a suitable evaluation for this </context>
<context position="3456" citStr="Manshadi et al. (2008)" startWordPosition="552" endWordPosition="555">ed as typed dependencies.” (De Marneffe et al., 2006) Formally, e := (v, d), where e is a narrative event, v is a verb lemma, and d is the syntactic dependency (nsubj or dobj) between v and the protagonist. As an example, consider the following narrative: John studied for the exam and aced it. His teacher congratulated him. With John as protagonist, we have a sequence of three narrative events: (study, nsubj), (ace, nsubj), and (congratulate, dobj). In the narrative cloze test, a sequence of narrative events (like the example provided here) is extracted automatically from a document, and one 2Manshadi et al. (2008) also take a language modeling approach to event prediction, although their experiments are not directly comparable. 3We note that, whether the narrative cloze was originally intended as a rigorous evaluation of script induction techniques or merely a preliminary metric, we are motivated by the observation that this evaluation has nonetheless become a standard metric for this task. 1681 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1681–1686, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. narrative event</context>
</contexts>
<marker>Manshadi, Swanson, Gordon, 2008</marker>
<rawString>Mehdi Manshadi, Reid Swanson, and Andrew S Gordon. 2008. Learning a probabilistic model of event sequences from internet weblog stories. In FLAIRS Conference, pages 159–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10783" citStr="Mnih and Hinton (2007)" startWordPosition="1868" endWordPosition="1871">with skip-0 (consecutive events only), skip-3 (window size 3), and skip-all (entire chain length) settings. For each of the four narrative cloze scoring metrics we report on (average rank, mean reciprocal rank, recall at 10, and recall at 50), we tune the Unordered PMI, Ordered PMI, and Bigram Probability models over the following parameter space: {skip-0, skip-3, skip-all} x {discount, no-discount} x {T=4, T=10, T=20}, where T is a pairwise count threshold. 3.2 A Discriminative Method Log-Bilinear Language Model (LBL) The Log-Bilinear language model is a language model that was introduced by Mnih and Hinton (2007). Like other language models, the LBL produces a probability distribution over the next possible word given a sequence of N previously observed words. N is a hyper-parameter that determines the size of the context used for computing the probabilities. While many variants of the LBL have been proposed since its introduction, we use the simple variant described below. Formally, we associate one context vector ce E Rd, one bias parameter be E R, and one target vector te E Rd to each narrative event e E V U { UNK, BOS, EOS }. V is the vocabulary of events and BOS, EOS, and UNK are the beginning-of</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashutosh Modi</author>
<author>Ivan Titov</author>
</authors>
<title>Inducing neural models of script knowledge.</title>
<date>2014</date>
<booktitle>CoNLL-2014,</booktitle>
<pages>page</pages>
<contexts>
<context position="1978" citStr="Modi and Titov, 2014" startWordPosition="304" endWordPosition="307">ain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.2 To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model – the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: either discriminative language models are a more effective technique for script induction than previous methods, or the narrative cloze test is not a suitable evaluation for this task.3 2 Task Definiti</context>
</contexts>
<marker>Modi, Titov, 2014</marker>
<rawString>Ashutosh Modi and Ivan Titov. 2014. Inducing neural models of script knowledge. CoNLL-2014, page 49. Patrick Pantel and Deepak Ravichandran. 2004.</rawString>
</citation>
<citation valid="false">
<title>Automatically labeling semantic classes.</title>
<booktitle>HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>321--328</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boston, Massachusetts, USA.</location>
<marker></marker>
<rawString>Automatically labeling semantic classes. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 321–328, Boston, Massachusetts, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baltescu Paul</author>
<author>Blunsom Phil</author>
<author>Hoang Hieu</author>
</authors>
<title>Oxlm: A neural language modelling framework for machine translation.</title>
<date>2014</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<volume>102</volume>
<issue>1</issue>
<contexts>
<context position="12859" citStr="Paul et al., 2014" startWordPosition="2256" endWordPosition="2259">OP BG LBL2LBL4 UNI UOP OP BG LBL2LBL4 model model Figure 1: Narrative cloze results over all chain lengths. Unigram Model (UNI), Unordered PMI Model (UOP), Ordered PMI Model (OP), Bigram Probability Model (BG), Log-Bilinear Model with context size 2 or 4 (LBL2, LBL4). Average Rank (avgrnk), Mean Reciprocal Rank (mrr), % Recall at 10 (rec10), % Recall at 50 (rec50). quence E&apos; made by prepending N BOS tokens and appending a single EOS token to E. The LBL models are trained by minimizing the objective described in Equation 5 for all the sequences in the training corpus. We used the OxLM toolkit (Paul et al., 2014) which internally uses Noise-Contrastive Estimation (NCE) (Gutmann and Hyv¨arinen, 2010) and processor parallelization for speeding up the training. For this task, we train LBL models with N = 2 (LBL2) and N = 4 (LBL4). In our experiments, increasing context size to N = 6 did not significantly improve (or degrade) performance. 4 Experimental Results Table 1 shows the results of 17,836 narrative cloze tests (derived from 5,000 held-out test chains), with results bucketed by chain length. Performance is reported on four metrics: average rank, mean reciprocal rank, recall at 10, and recall at 50.</context>
</contexts>
<marker>Paul, Phil, Hieu, 2014</marker>
<rawString>Baltescu Paul, Blunsom Phil, and Hoang Hieu. 2014. Oxlm: A neural language modelling framework for machine translation. The Prague Bulletin of Mathematical Linguistics, 102(1):81–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Pichotta</author>
<author>Raymond Mooney</author>
</authors>
<title>Statistical script learning with multi-argument events.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>220--229</pages>
<institution>Gothenburg, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="1487" citStr="Pichotta and Mooney, 2014" startWordPosition="222" endWordPosition="225"> in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale. One particularly influential work in this area, Chambers and Jurafsky (2008), treats the problem of script induction as one of learning narrative chains, which they accomplish using simple textual co-occurrence statistics. For the novel task of learning narrative chains, they introduce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating script knowledge, is more productively thought of </context>
<context position="6287" citStr="Pichotta and Mooney (2014)" startWordPosition="1011" endWordPosition="1014">d for each vocabulary item by inserting that item into the sequence at index k. Each model generates a score for the candidate sequences, yielding a ranking over the vocabulary items. The rank assigned to the actual missing vocabulary item is the score the model receives on that cloze test. In this case, we set V to include all narrative events, e, that occur at least ten times in training, yielding a vocabulary size of 12,452. All out-of-vocabulary events are converted to (and scored as) the symbol UNK. 3.1 Count-based Methods Unigram Baseline (UNI) A simple but strong baseline introduced by Pichotta and Mooney (2014) for this task is the unigram model: canUnordered PMI (UOP) The original model for this task, proposed by Chambers and Jurafsky (2008), is based on the pointwise mutual information (PMI) between events. C(e1, e2) pmi(e1, e2) a log C(e1, *)C(*, e2) (1) Here, C(e1, e2) is the number of times e1 and e2 occur in the same narrative event sequence, i.e., the number of times they “had a coreferring entity filling the values of [their] dependencies,” and the ordering of e1 and e2 is not considered. In our implementation, individual counts are defined as follows: EC(e, *) := C(e, e&apos;) (2) e&apos;EV This mode</context>
</contexts>
<marker>Pichotta, Mooney, 2014</marker>
<rawString>Karl Pichotta and Raymond Mooney. 2014. Statistical script learning with multi-argument events. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–229, Gothenburg, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Alexander Koller</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning script knowledge with web experiments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>979--988</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2001" citStr="Regneri et al., 2010" startWordPosition="308" endWordPosition="312">om text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015) employ and extend Chambers and Jurafsky (2008)’s methods for learning narrative chains, each using the narrative cloze to evaluate their work. 1 In this paper, we take the position that the narrative cloze test, which has been treated predom1A number of related works on script induction use alternative task formulations and evaluations. (Chambers, 2013; Cheung et al., 2013; Cheung and Penn, 2013; Frermann et al., 2014; Manshadi et al., 2008; Modi and Titov, 2014; Regneri et al., 2010) inantly as a method for evaluating script knowledge, is more productively thought of simply as a language modeling task.2 To support this claim, we demonstrate a marked improvement over previous methods on this task using a powerful discriminative language model – the Log-Bilinear model (LBL). Based on this finding, we believe one of the following conclusions must follow: either discriminative language models are a more effective technique for script induction than previous methods, or the narrative cloze test is not a suitable evaluation for this task.3 2 Task Definition Following the defini</context>
</contexts>
<marker>Regneri, Koller, Pinkal, 2010</marker>
<rawString>Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning script knowledge with web experiments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 979–988. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Rudinger</author>
<author>Vera Demberg</author>
<author>Ashutosh Modi</author>
<author>Benjamin Van Durme</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning to predict script events from domainspecific text.</title>
<date>2015</date>
<booktitle>Lexical and Computational Semantics (* SEM 2015),</booktitle>
<pages>205</pages>
<marker>Rudinger, Demberg, Modi, Van Durme, Pinkal, 2015</marker>
<rawString>Rachel Rudinger, Vera Demberg, Ashutosh Modi, Benjamin Van Durme, and Manfred Pinkal. 2015. Learning to predict script events from domainspecific text. Lexical and Computational Semantics (* SEM 2015), page 205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Schank</author>
<author>Robert Abelson</author>
</authors>
<title>Scripts, plans, goals and understanding: An inquiry into human knowledge structures. Lawrence Erlbaum Associates,</title>
<date>1977</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="852" citStr="Schank and Abelson, 1977" startWordPosition="124" endWordPosition="128">niversity Abstract The narrative cloze is an evaluation metric commonly used for work on automatic script induction. While prior work in this area has focused on count-based methods from distributional semantics, such as pointwise mutual information, we argue that the narrative cloze can be productively reframed as a language modeling task. By training a discriminative language model for this task, we attain improvements of up to 27 percent over prior methods on standard narrative cloze metrics. 1 Introduction Although the concept of scripts in artificial intelligence dates back to the 1970s (Schank and Abelson, 1977), interest in this topic has renewed with recent efforts to automatically induce scripts from text on a large scale. One particularly influential work in this area, Chambers and Jurafsky (2008), treats the problem of script induction as one of learning narrative chains, which they accomplish using simple textual co-occurrence statistics. For the novel task of learning narrative chains, they introduce a new evaluation metric, the narrative cloze test, which involves predicting a missing event from a chain of events drawn from text. Several follow-up works (Chambers and Jurafsky, 2009; Jans et a</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger Schank and Robert Abelson. 1977. Scripts, plans, goals and understanding: An inquiry into human knowledge structures. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188, January.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.970849">
Solving General Arithmetic Word Problems
</title>
<author confidence="0.996028">
Subhro Roy Dan Roth
</author>
<affiliation confidence="0.9897915">
University of Illinois, University of Illinois,
Urbana Champaign Urbana Champaign
</affiliation>
<email confidence="0.997068">
sroy9@illinois.edu danr@illinois.edu
</email>
<sectionHeader confidence="0.993836" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937">
This paper presents a novel approach to au-
tomatically solving arithmetic word problems.
This is the first algorithmic approach that
can handle arithmetic problems with multi-
ple steps and operations, without depending
on additional annotations or predefined tem-
plates. We develop a theory for expression
trees that can be used to represent and evalu-
ate the target arithmetic expressions; we use
it to uniquely decompose the target arithmetic
problem to multiple classification problems;
we then compose an expression tree, combin-
ing these with world knowledge through a con-
strained inference framework. Our classifiers
gain from the use of quantity schemas that sup-
ports better extraction of features. Experimen-
tal results show that our method outperforms
existing systems, achieving state of the art per-
formance on benchmark datasets of arithmetic
word problems.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997846787878788">
In recent years there is growing interest in understand-
ing natural language text for the purpose of answering
science related questions from text as well as quanti-
tative problems of various kinds. In this context, un-
derstanding and solving arithmetic word problems is of
specific interest. Word problems arise naturally when
reading the financial section of a newspaper, following
election coverage, or when studying elementary school
arithmetic word problems. These problems pose an in-
teresting challenge to the NLP community, due to its
concise and relatively straightforward text, and seem-
ingly simple semantics. Arithmetic word problems are
usually directed towards elementary school students,
and can be solved by combining the numbers men-
tioned in text with basic operations (addition, subtrac-
tion, multiplication, division). They are simpler than
algebra word problems which require students to iden-
tify variables, and form equations with these variables
to solve the problem.
Initial methods to address arithmetic word problems
have mostly focussed on subsets of problems, restrict-
ing the number or the type of operations used (Roy et
al., 2015; Hosseini et al., 2014) but could not deal with
multi-step arithmetic problems involving all four basic
operations. The template based method of (Kushman
et al., 2014), on the other hand, can deal with all types
of problems, but implicitly assumes that the solution is
generated from a set of predefined equation templates.
In this paper, we present a novel approach which
can solve a general class of arithmetic problems with-
out predefined equation templates. In particular, it can
handle multiple step arithmetic problems as shown in
Example 1.
</bodyText>
<construct confidence="0.570003833333333">
Example 1
Gwen was organizing her book case making sure each
of the shelves had exactly 9 books on it. She has 2 types
of books - mystery books and picture books. If she had 3
shelves of mystery books and 5 shelves ofpicture books,
how many books did she have in total?
</construct>
<bodyText confidence="0.999529586206896">
The solution involves understanding that the number
of shelves needs to be summed up, and that the total
number of shelves needs to be multiplied by the num-
ber of books each shelf can hold. In addition, one has
to understand that the number “2” is not a direct part of
the solution of the problem.
While a solution to these problems eventually re-
quires composing multi-step numeric expressions from
text, we believe that directly predicting this complex
expression from text is not feasible.
At the heart of our technical approach is the novel
notion of an Expression Tree. We show that the arith-
metic expressions we are interested in can always be
represented using an Expression Tree that has some
unique decomposition properties. This allows us to de-
compose the problem of mapping the text to the arith-
metic expression to a collection of simple prediction
problems, each determining the lowest common ances-
tor operation between a pair of quantities mentioned in
the problem. We then formulate the decision problem
of composing the final expression tree as a joint infer-
ence problem, via an objective function that consists of
all these decomposed prediction problems, along with
legitimacy and background knowledge constraints.
Learning to generate the simpler decomposed ex-
pressions allows us to support generalization across
problems types. In particular, our system could solve
Example 1 even though it has never seen a problem that
requires both addition and multiplication operations.
</bodyText>
<page confidence="0.950193">
1743
</page>
<note confidence="0.9850045">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743–1752,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.973143444444444">
We also introduce a second concept, that of quantity
schema, that allows us to focus on the information rel-
evant to each quantity mentioned in the text. We show
that features extracted from quantity schemas help rea-
soning effectively about the solution. Moreover, quan-
tity schemas help identify unnecessary text snippets in
the problem text. For instance, in Example 2, the in-
formation that “Tom washed cars over the weekend” is
irrelevant; he could have performed any activity to earn
money. In order to solve the problem, we only need to
know that he had $76 last week, and now he has $86.
Example 2
Last week Tom had $74. He washed cars over the week-
end and now has $86. How much money did he make
from the job?
We combine the classifiers’ decisions using a con-
strained inference framework that allows for incorpo-
rating world knowledge as constraints. For example,
we deliberatively incorporate the information that, if
the problems asks about an “amount”, the answer must
be positive, and if the question starts with “how many”,
the answer will most likely be an integer.
Our system is evaluated on two existing datasets of
arithmetic word problems, achieving state of the art
performance on both. We also create a new dataset of
multistep arithmetic problems, and show that our sys-
tem achieves competitive performance in this challeng-
ing evaluation setting.
The next section describes the related work in the
area of automated math word problem solving. We then
present the theory of expression trees and our decom-
position strategy that is based on it. Sec. 4 presents the
overall computational approach, including the way we
use quantity schemas to learn the mapping from text
to expression tree components. Finally, we discuss our
experimental study and conclude.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999891041666667">
Previous work in automated arithmetic problem solvers
has focussed on a restricted subset of problems. The
system described in (Hosseini et al., 2014) handles
only addition and subtraction problems, and requires
additional annotated data for verb categories. In con-
trast, our system does not require any additional an-
notations and can handle a more general category of
problems. The approach in (Roy et al., 2015) sup-
ports all four basic operations, and uses a pipeline of
classifiers to predict different properties of the prob-
lem. However, it makes assumptions on the number of
quantities mentioned in the problem text, as well as the
number of arithmetic steps required to solve the prob-
lem. In contrast, our system does not have any such
restrictions, effectively handling problems mentioning
multiple quantities and requiring multiple steps. Kush-
man’s approach to automatically solving algebra word
problems (Kushman et al., 2014) might be the most re-
lated to ours. It tries to map numbers from the prob-
lem text to predefined equation templates. However,
they implicitly assume that similar equation forms have
been seen in the training data. In contrast, our system
can perform competitively, even when it has never seen
similar expressions in training.
There is a recent interest in understanding text for
the purpose of solving scientific and quantitative prob-
lems of various kinds. Our approach is related to work
in understanding and solving elementary school stan-
dardized tests (Clark, 2015). The system described in
(Berant et al., 2014) attempts to automatically answer
biology questions, by extracting the structure of bio-
logical processes from text. There has also been efforts
to solve geometry questions by jointly understanding
diagrams and associated text (Seo et al., 2014). A re-
cent work (Sadeghi et al., 2015) tries to answer science
questions by visually verifying relations from images.
Our constrained inference module falls under the
general framework of Constrained Conditional Mod-
els (CCM) (Chang et al., 2012). In particular, we use
the L + I scheme of CCMs, which predicts structured
output by independently learning several simple com-
ponents, combining them at inference time. This has
been successfully used to incorporate world knowledge
at inference time, as well as getting around the need
for large amounts of jointly annotated data for struc-
tured prediction (Roth and Yih, 2005; Punyakanok et
al., 2005; Punyakanok et al., 2008; Clarke and Lapata,
2006; Barzilay and Lapata, 2006; Roy et al., 2015).
</bodyText>
<sectionHeader confidence="0.974544" genericHeader="method">
3 Expression Tree and Problem
Decomposition
</sectionHeader>
<bodyText confidence="0.999966761904762">
We address the problem of automatically solving arith-
metic word problems. The input to our system is
the problem text P, which mentions n quantities
q1, q2, . . . , qn. Our goal is to map this problem to a
read-once arithmetic expression E that, when evalu-
ated, provides the problem’s solution. We define a
read-once arithmetic expression as one that makes use
of each quantity at most once. We say that E is a valid
expression, if it is such a Read-Once arithmetic expres-
sion, and we only consider in this work problems that
can be solved using valid expressions (it’s possible that
they can be solved also with invalid expressions).
An expression tree T for a valid expression E is a
binary tree whose leaves represent quantities, and each
internal node represents one of the four basic opera-
tions. For a non-leaf node n, we represent the operation
associated with it as O(n), and its left and right child
as lc(n) and rc(n) respectively. The numeric value of
the quantity associated with a leaf node n is denoted
as Q(n). Each node n also has a value associated with
it, represented as VAL(n), which can be computed in a
</bodyText>
<page confidence="0.793867">
1744
</page>
<equation confidence="0.95614925">
recursive way as follows:
VAL(n) =
r Q(n) if n is a leaf (1)
Sl VAL(lc(n)) O(n) VAL(rc(n)) otherwise
</equation>
<bodyText confidence="0.923569">
For any expression tree T for expression E with root
node nroot, the value of VAL(nroot) is exactly equal
to the numeric value of the expression E. Therefore,
this gives a natural representation of numeric expres-
sions, providing a natural parenthesization of the nu-
meric expression. Fig 1 shows an example of an arith-
metic problem with solution expression and an expres-
sion tree for the solution expression.
Problem
Gwen was organizing her book case making sure each
of the shelves had exactly 9 books on it. She has 2 types
of books - mystery books and picture books. If she had 3
shelves of mystery books and 5 shelves ofpicture books,
how many books did she have total?
Solution Expression Tree of Solution
</bodyText>
<figure confidence="0.595187666666667">
(3 + 5) x 9 = 72 ×
+ 9
3 5
</figure>
<figureCaption confidence="0.991963">
Figure 1: An arithmetic word problem, solution expression
and the corresponding expression tree
</figureCaption>
<listItem confidence="0.814787857142857">
Definition An expression tree T for a valid expression
E is called monotonic if it satisfies the following con-
ditions:
1. If an addition node is connected to a subtraction
node, then the subtraction node is the parent.
2. If a multiplication node is connected to a division
node, then the division node is the parent.
</listItem>
<bodyText confidence="0.982371">
Fig 2 shows two different expression trees for the
same expression. Fig 2b is monotonic whereas fig 2a is
not.
</bodyText>
<figureCaption confidence="0.964621333333333">
Figure 2: Two different expression trees for the numeric ex-
pression (3 x 5) + 7 − 8 − 9. The right one is monotonic,
whereas the left one is not.
</figureCaption>
<bodyText confidence="0.99980459375">
Our decomposition relies on the idea of monotonic
expression trees. We try to predict for each pair of
quantities qi, qj, the operation at the lowest common
ancestor (LCA) node of the monotonic expression tree
for the solution expression. We also predict for each
quantity, whether it is relevant to the solution. Finally,
an inference module combines all these predictions.
In the rest of the section, we show that for any pair of
quantities qi, qj in the solution expression, any mono-
tonic tree for the solution expression has the same LCA
operation. Therefore, predicting the LCA operation be-
comes a multiclass classification problem.
The reason that we consider the monotonic represen-
tation of the expression tree is that different trees could
otherwise give different LCA operation for a given pair
of quantities. For example, in Fig 2, the LCA opera-
tion for quantities 5 and 8 can be + or −, depending on
which tree is considered.
Definition We define an addition-subtraction chain
of an expression tree to be the maximal connected set
of nodes labeled with addition or subtraction.
The nodes of an addition-subtraction (AS) chain C
represent a set of terms being added or subtracted.
These terms are sub-expressions created by subtrees
rooted at neighboring nodes of the chain. We call these
terms the chain terms of C, and the whole expression,
after node operations have been applied to the chain
terms, the chain expression of C. For example, in fig
2, the shaded nodes form an addition-subtraction chain.
The chain expression is (3x5)+7−8−9, and the chain
terms are 3 x 5, 7, 8 and 9. We define a multiplication-
division (MD) chain in a similar way.
</bodyText>
<construct confidence="0.792928">
Theorem 3.1. Every valid expression can be repre-
sented by a monotonic expression tree.
</construct>
<bodyText confidence="0.99996828">
Proof. The proof is procedural, that is, we provide a
method to convert any expression tree to a monotonic
expression tree for the same expression. Consider a
non-monotonic expression tree E, and without loss of
generality, assume that the first condition for mono-
tonicity is not valid. Therefore, there exists an addi-
tion node ni and a subtraction node nj, and ni is the
parent of nj. Consider an addition-subtraction chain C
which includes ni, nj. We now replace the nodes of C
and its subtrees in the following way. We add a sin-
gle subtraction node n_. The left subtree of n_ has all
the addition chain terms connected by addition nodes,
and the right subtree of n_ has all the subtraction chain
terms connected by addition nodes. Both subtrees of
n_ only require addition nodes, hence monotonicity
condition is satisfied. We can construct the monotonic
tree in Fig 2b from the non-monotonic tree of Fig 2a us-
ing this procedure. The addition chain terms are 3 x 5
and 7, and the subtraction chain terms are 8 and 9. As
as was described above, we introduce the root subtrac-
tion node in Fig 2b and attach the addition chain terms
to the left and the subtraction chain terms to the right.
The same line of reasoning can be used to handle the
second condition with multiplication and division re-
placing addition and subtraction, respectively.
</bodyText>
<figure confidence="0.994481875">
(a) (b)
3
×
+
5
7
−
8
+
9
3
× 7 8 9
+
5
−
+
</figure>
<page confidence="0.964215">
1745
</page>
<bodyText confidence="0.999845833333334">
Theorem 3.2. Consider two valid expression trees T 1
and T2 for the same expression E. Let C1, C2 be
the chain containing the root nodes of T 1 and T2 re-
spectively. The chain type (addition-subtraction or
multiplication-division) as well as the the set of chain
terms of C1 and C2 are identical.
Proof. We first prove that the chains containing the
roots are both AS or both MD, and then show that the
chain terms are also identical.
We prove by contradiction that the chain type is
same. Let C1’s type be “addition-subtraction” and C2’s
type be “multiplication-division” (without loss of gen-
erality). Since both C1 and C2 generate the same ex-
pression E, we have that E can be represented as sum
(or difference) of two expressions as well as product(or
division) of two expressions. Transforming a sum (or
difference) of expressions to a product (or division)
requires taking common terms from the expressions,
which imply that the sum (or difference) had dupli-
cate quantities. The opposite transformation adds same
term to various expressions leading to multiple uses of
the same quantity. Therefore, this will force at least one
of C1 and C2 to use the same quantity more than once,
violating validity.
We now need to show that individual chain terms are
also identical. Without loss of generality, let us assume
that both C1 and C2 are “addition-subtraction” chains.
Suppose the chain terms of C1 and C2 are not identi-
cal. The chain expression for both the chains will be the
same (since they are root chains, the chain expressions
has to be the same as E). Let the chain expression for
C1 be Pi ti − Pi ti, where ti’s are the addition chain
terms and ti are the subtraction chain terms. Similarly,
let the chain expression for C2 be Pi si − Pi si. We
know that Pi ti − Pi ti = P i si − Pi si, but the set
of ti’s and ti’s is not the same as the set of si and si’s.
However it should be possible to transform one form
to the other using mathematical manipulations. This
transformation will involve taking common terms, or
multiplying two terms, or both. Following previous
explanation, this will force one of the expressions to
have duplicate quantities, violating validity. Hence, the
chain terms of C1 and C2 are identical.
Consider an expression tree T for a valid expres-
sion E. For a distinct pair of quantities qi, qj par-
ticipating in expression E, we denote by ni, nj the
leaves of the expression tree T representing qi, qj, re-
spectively. Let nLCA(qi, qj; T) to be the lowest com-
mon ancestor node of ni and nj. We also define
order(qi,qj;T) to be true if ni appears in the left
subtree of nLCA(qi, qj; T) and nj appears in the right
subtree of nLCA(qi, qj; T) and set order(qi, qj; T) to
false otherwise. Finally we define OLCA(qi, qj; T) for
a pair of quantities qi, qj as follows :
</bodyText>
<equation confidence="0.999720909090909">
OLCA(qi, qj, T) =
+ if O(nLCA(qi, qj; T )) = +
× if O(nLCA(qi, qj; T )) = ×
− if O(nLCA(qi, qj; T )) = − and
order(qi, qj; T) = true
−reverse if O(nLCA(qi, qj; T)) = − and
order(qi, qj; T) = false
÷ if O(nLCA(qi, qj; T)) = ÷ and
order(qi, qj; T) = true
÷reverse if O(nLCA(qi, qj; T)) = ÷ and
order(qi, qj; T) = false
</equation>
<bodyText confidence="0.9979611">
Definition Given two expression trees T 1 and T2 for
the same expression E, T 1 is LCA-equivalent to T2 if
for every pair quantities qi, qj in the expression E, we
have OLCA(qi, qj, T 1) = OLCA(qi, qj, T2).
Theorem 3.3. All monotonic expression trees for an
expression are LCA-equivalent to each other.
Proof. We prove by induction on the number of quanti-
ties used in an expression. For all expressions E with 2
quantities, there exists only one monotonic expression
tree, and hence, the statement is trivially true. This sat-
isfies our base case.
For the inductive case, we assume that for all expres-
sions with k &lt; n quantities, the theorem is true. Now,
we need to prove that any expression with n nodes will
also satisfy the property.
Consider a valid (as in all cases) expression E, with
monotonic expression trees T 1 and T2. From theorem
3.2, we know that the chains containing the roots of
T 1 and T2 have identical type and terms. Given two
quantities qi, qj of E, the lowest common ancestor of
both T 1 and T2 will either both belong to the chain
containing the root, or both belong to one of the chain
terms. If the LCA node is part of the chain for both
T 1 and T2, monotonic property ensures that the LCA
operation will be identical. If the LCA node is part of a
chain term (which is an expression tree of size less than
n), the property is satisfied by induction hypothesis.
The theory just presented suggests that it is possible
to uniquely decompose the overall problem to simpler
steps and this will be exploited in the next section.
</bodyText>
<sectionHeader confidence="0.987768" genericHeader="method">
4 Mapping Problems to Expression Trees
</sectionHeader>
<bodyText confidence="0.999623875">
Given the uniqueness properties proved in Sec. 3, it is
sufficient to identify the operation between any two rel-
evant quantities in the text, in order to determine the
unique valid expression. In fact, identifying the op-
eration between any pair of quantities provides much
needed redundancy given the uncertainty in identifying
the operation from text, and we exploit it in our final
joint inference.
</bodyText>
<figure confidence="0.97715425">
⎧
⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
(2)
</figure>
<page confidence="0.975493">
1746
</page>
<bodyText confidence="0.999928692307692">
Consequently, our overall method proceeds as fol-
lows: given the problem text P, we detect quantities
q1, q2, . . . , qn. We then use two classifiers, one for rel-
evance and other to predict the LCA operations for a
monotonic expression tree of the solution. Our training
makes use of the notion of quantity schemas, which we
describe in Section 4.2. The distributional output of
these classifiers is then used in a joint inference proce-
dure that determines the final expression tree.
Our training data consists of problem text paired
with a monotonic expression tree for the solution ex-
pression. Both the relevance and LCA operation clas-
sifiers are trained on gold annotations.
</bodyText>
<subsectionHeader confidence="0.986646">
4.1 Global Inference for Expression Trees
</subsectionHeader>
<bodyText confidence="0.9996894">
In this subsection, we define the scoring functions cor-
responding to the decomposed problems, and show
how we combine these scores to perform global infer-
ence. For a problem P with quantities q1, q2, . . . , qn,
we define the following scoring functions:
</bodyText>
<listItem confidence="0.997749666666667">
1. PAIR(qi, qj, op) : Scores the likelihood of
OLCA(qi, qj, T ) = op, where T is a monotone
expression tree of the solution expression of P. A
multiclass classifier trained to predict LCA opera-
tions (Section 4.4) can provide these scores.
2. IRR(q) : Scores the likelihood of quantity q being
</listItem>
<bodyText confidence="0.926050777777778">
an irrelevant quantity in P, that is, q is not used in
creating the solution. A binary classifier trained
to predict whether a quantity q is relevant or not
(Section 4.3), can provide these scores.
For an expression E, letZ(E) be the set of all quanti-
ties in P which are not used in expression E. Let T be a
monotonic expression tree for E. We define Score(E)
of an expression E in terms of the above scoring func-
tions and a scaling parameter wIRR as follows:
</bodyText>
<equation confidence="0.9973575">
11 Score(E) =wIRR IRR(q)+ (3)
qEZ(E)
11 PAIR(qi, qj, OLCA(qi, qj, T))
qi,qj /EZ(E)
</equation>
<bodyText confidence="0.996589">
Our final expression tree is an outcome of a con-
strained optimization process, following (Roth and
Yih, 2004; Chang et al., 2012). Our objective function
makes use of the scores returned by IRR(·) and PAIR(·)
to determine the expression tree and is constrained by
legitimacy and background knowledge constraints, de-
tailed below.
</bodyText>
<listItem confidence="0.997392285714286">
1. Positive Answer: Most arithmetic problems ask-
ing for amounts or number of objects usually have
a positive number as an answer. Therefore, while
searching for the best scoring expression, we re-
ject expressions generating negative answer.
2. Integral Answer: Problems with questions such
as ‘how many” usually expect integral solutions.
</listItem>
<bodyText confidence="0.939684733333333">
We only consider integral solutions as legitimate
outputs for such problems.
Let C be the set of valid expressions that can be
formed using the quantities in a problem P, and which
satisfy the above constraints. The inference algorithm
now becomes the following:
arg max Score(E) (4)
EEC
The space of possible expressions is large, and we
employ a beam search strategy to find the highest
scoring constraint satisfying expression (Chang et al.,
2012). We construct an expression tree using a bottom
up approach, first enumerating all possible sets of irrel-
evant quantities, and next over all possible expressions,
keeping the top k at each step. We give details below.
</bodyText>
<listItem confidence="0.899338222222222">
1. Enumerating Irrelevant Quantities: We gener-
ate a state for all possible sets of irrelevant quan-
tities, ensuring that there is at least two relevant
quantities in each state. We refer to each of the rel-
evant quantities in each state as a term. Therefore,
each state can be represented as a set of terms.
2. Enumerating Expressions: For generating a next
state S&apos; from S, we choose a pair of terms ti and tj
in S and one of the four basic operations, and form
</listItem>
<bodyText confidence="0.918307833333333">
a new term by combining terms ti and tj with the
operation. Since we do not know which of the
possible next states will lead to the optimal goal
state, we enumerate all possible next states (that
is, enumerate all possible pairs of terms and all
possible operations); we prune the beam to keep
only the top k candidates. We terminate when all
the states in the beam have exactly one term.
Once we have a top k list of candidate expression
trees, we choose the highest scoring tree which satisfies
the constraints. However, there might not be any tree
in the beam which satisfies the constraints, in which
case, we choose the top candidate in the beam. We use
k = 200 in our experiments.
In order to choose the value for the wIRR, we search
over the set {10−6,10−4,10−2,1,102,104,106}, and
choose the parameter setting which gives the highest
accuracy on the training data.
</bodyText>
<subsectionHeader confidence="0.996138">
4.2 Quantity Schema
</subsectionHeader>
<bodyText confidence="0.9999885">
In order to generalize across problem types as well
as over simple manipulations of the text, it is neces-
sary to train our system only with relevant information
from the problem text. E.g., for the problem in exam-
ple 2, we do not want to take decisions based on how
Tom earned money. Therefore, there is a need to ex-
tract the relevant information from the problem text.
To this end, we introduce the concept of a quantity
schema which we extract for each quantity in the prob-
lem’s text. Along with the question asked, the quantity
</bodyText>
<page confidence="0.982583">
1747
</page>
<bodyText confidence="0.99898225">
schemas provides all the information needed to solve
most arithmetic problems.
A quantity schema for a quantity q in problem P
consists of the following components.
</bodyText>
<listItem confidence="0.9956763">
1. Associated Verb For each quantity q, we detect
the verb associated with it. We traverse up the
dependency tree starting from the quantity men-
tion, and choose the first verb we reach. We used
the easy first dependency parser (Goldberg and El-
hadad, 2010).
2. Subject of Associated Verb We detect the noun
phrase, which acts as subject of the associated
verb (if one exists).
3. Unit We use a shallow parser to detect the phrase
</listItem>
<bodyText confidence="0.7416062">
p in which the quantity q is mentioned. All to-
kens of the phrase (other than the number itself)
are considered as unit tokens. Also, if p is fol-
lowed by the prepositional phrase “of” and a noun
phrase (according to the shallow parser annota-
tions), we also consider tokens from this second
noun phrase as unit tokens. Finally, if no unit
token can be extracted, we assign the unit of the
neighboring quantities as the unit of q (following
previous work (Hosseini et al., 2014)).
</bodyText>
<listItem confidence="0.990763666666667">
4. Related Noun Phrases We consider all noun
phrases which are connected to the phrase p con-
taining quantity q, with NP-PP-NP attachment. If
only one quantity is mentioned in a sentence, we
consider all noun phrases in it as related.
5. Rate We determine whether quantity q refers to a
</listItem>
<bodyText confidence="0.968581833333333">
rate in the text, as well as extract two unit compo-
nents defining the rate. For example, “7 kilome-
ters per hour” has two components “kilometers”
and “hour”. Similarly, for sentences describing
unit cost like “Each egg costs 2 dollars”, “2” is
a rate, with units “dollars” and “egg”.
In addition to extracting the quantity schemas for
each quantity, we extract the surface form text which
poses the question. For example, in the question sen-
tence, “How much will John have to pay if he wants to
buy 7 oranges?”, our extractor outputs “How much will
John have to pay” as the question.
</bodyText>
<subsectionHeader confidence="0.998455">
4.3 Relevance Classifier
</subsectionHeader>
<bodyText confidence="0.9999908">
We train a binary SVM classifier to determine, given
problem text P and a quantity q in it, whether q is
needed in the numeric expression generating the solu-
tion. We train on gold annotations and use the score of
the classifier as the scoring function IRR(·).
</bodyText>
<sectionHeader confidence="0.670203" genericHeader="method">
4.3.1 Features
</sectionHeader>
<bodyText confidence="0.77730725">
The features are extracted from the quantity schemas
and can be broadly categorized into three groups:
1. Unit features: Most questions specifically men-
tion the object whose amount needs to be com-
puted, and hence questions provide valuable clue
as to which quantities can be irrelevant. We add a
feature for whether the unit of quantity q is present
in the question tokens. Also, we add a feature
based on whether the units of other quantities have
better matches with question tokens (based on the
number of tokens matched), and one based on the
number of quantities which have the maximum
number of matches with the question tokens.
2. Related NP features: Often units are not enough
to differentiate between relevant and irrelevant
quantities. Consider the following:
</bodyText>
<figure confidence="0.666342">
Example 3
</figure>
<figureCaption confidence="0.75580475">
Problem : There are 8 apples in a pile on the
desk. Each apple comes in a package of 11. 5
apples are added to the pile. How many apples
are there in the pile?
</figureCaption>
<equation confidence="0.818309">
Solution: (8 + 5) = 13
</equation>
<bodyText confidence="0.999915375">
The relevance decision depends on the noun
phrase “the pile”, which is absent in the second
sentence. We add a feature indicating whether
a related noun phrase is present in the question.
Also, we add a feature based on whether the re-
lated noun phrases of other quantities have bet-
ter match with the question. Extraction of related
noun phrases is described in Section 4.2.
</bodyText>
<listItem confidence="0.831838">
3. Miscellaneous Features: When a problem men-
</listItem>
<bodyText confidence="0.9862056">
tions only two quantities, both of them are usually
relevant. Hence, we also add a feature based on
the number of quantities mentioned in text.
We include pairwise conjunction of the above fea-
tures.
</bodyText>
<subsectionHeader confidence="0.995354">
4.4 LCA Operation Classifier
</subsectionHeader>
<bodyText confidence="0.999983666666667">
In order to predict LCA operations, we train a multi-
class SVM classifier. Given problem text P and a pair
of quantities pi and pj, the classifier predicts one of the
six labels described in Eq. 2. We consider the confi-
dence scores for each label supplied by the classifier as
the scoring function PAIR(·).
</bodyText>
<subsectionHeader confidence="0.507576">
4.4.1 Features
</subsectionHeader>
<bodyText confidence="0.99734">
We use the following categories of features:
1. Individual Quantity features: Dependent verbs
have been shown to play significant role in solv-
ing addition and subtraction problems (Hosseini
et al., 2014). Hence, we add the dependent verb
of the quantity as a feature. Multiplication and
division problems are largely dependent on rates
described in text. To capture that, we add a fea-
ture based on whether the quantity is a rate, and
whether any component of rate unit is present in
</bodyText>
<page confidence="0.97561">
1748
</page>
<bodyText confidence="0.999038333333333">
the question. In addition to these quantity schema
features, we add selected tokens from the neigh-
borhood of the quantity mention. Neighborhood
of quantities are often highly informative of LCA
operations, for example, “He got 80 more mar-
bles”, the term “more” usually indicates addition.
We add as features adverbs and comparative ad-
jectives mentioned in a window of size 5 around
the quantity mention.
</bodyText>
<listItem confidence="0.978712888888889">
2. Quantity Pair features: For a pair (qi, qj) we add
features to indicate whether they have the same
dependent verbs, to indicate whether both depen-
dent verbs refer to the same verb mention, whether
the units of qi and qj are the same and, if one
of them is a rate, which component of the unit
matches with the other quantity’s unit. Finally, we
add a feature indicating whether the value of qi is
greater than the value of qj.
3. Question Features: Finally, we add a few fea-
tures based on the question asked. In particular,
for arithmetic problems where only one operation
is needed, the question contains signals for the re-
quired operation. Specifically, we add indicator
features based on whether the question mentions
comparison-related tokens (e.g., “more”, “less” or
“than”), or whether the question asks for a rate
(indicated by tokens such as “each” or “one”).
</listItem>
<bodyText confidence="0.999138333333333">
We include pairwise conjunction of the above fea-
tures. For both classifiers, we use the Illinois-SL pack-
age 1 under default settings.
</bodyText>
<sectionHeader confidence="0.991241" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999885571428571">
In this section, we evaluate the proposed method on
publicly available datasets of arithmetic word prob-
lems. We evaluate separately the relevance and LCA
operation classifiers, and show the contribution of var-
ious features. Lastly, we evaluate the performance of
the full system, and quantify the gains achieved by the
constraints.
</bodyText>
<subsectionHeader confidence="0.975718">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9481618">
We evaluate our system on three datasets, each of
which comprise a different category of arithmetic word
problems.
1. AI2 Dataset: This is a collection of 395 addition
and subtraction problems, released by (Hosseini
et al., 2014). They performed a 3-fold cross vali-
dation, with every fold containing problems from
different sources. This helped them evaluate ro-
bustness to domain diversity. We follow the same
evaluation setting.
</bodyText>
<footnote confidence="0.988418">
1http://cogcomp.cs.illinois.edu/page/software view/Illinois-
SL
</footnote>
<listItem confidence="0.548345">
2. IL Dataset: This is a collection of arithmetic
</listItem>
<bodyText confidence="0.981972764705882">
problems released by (Roy et al., 2015). Each of
these problems can be solved by performing one
operation. However, there are multiple problems
having the same template. To counter this, we per-
form a few modifications to the dataset. First, for
each problem, we replace the numbers and nouns
with the part of speech tags, and then we cluster
the problems based on unigrams and bigrams from
this modified problem text. In particular, we clus-
ter problems together whose unigram-bigram sim-
ilarity is over 90%. We next prune each cluster to
keep at most 5 problems in each cluster. Finally
we create the folds ensuring all problems in a clus-
ter are assigned to the same fold, and each fold has
similar distribution of all operations. We have a fi-
nal set of 562 problems, and we use a 5-fold cross
validation to evaluate on this dataset.
</bodyText>
<listItem confidence="0.998525166666667">
3. Commoncore Dataset: In order to test our sys-
tem’s ability to handle multi-step problems, we
create a new dataset of multi-step arithmetic
problems. The problems were extracted from
www.commoncoresheets.com. In total, there were
600 problems, 100 for each of the following types:
(a) Addition followed by Subtraction
(b) Subtraction followed by Addition
(c) Addition and Multiplication
(d) Addition and Division
(e) Subtraction and Multiplication
(f) Subtraction and Division
</listItem>
<bodyText confidence="0.999778333333333">
This dataset had no irrelevant quantities. There-
fore, we did not use the relevance classifier in our
evaluations.
In order to test our system’s ability to generalize
across problem types, we perform a 6-fold cross
validation, with each fold containing all the prob-
lems from one of the aforementioned categories.
This is a more challenging setting relative to the
individual data sets mentioned above, since we are
evaluating on multi-step problems, without ever
looking at problems which require the same set of
operations.
</bodyText>
<subsectionHeader confidence="0.995461">
5.2 Relevance Classifier
</subsectionHeader>
<bodyText confidence="0.999805428571429">
Table 2 evaluates the performance of the relevance clas-
sifier on the AI2 and IL datasets. We report two accu-
racy values: Relax - fraction of quantities which the
classifier got correct, and Strict - fraction of math prob-
lems, for which all quantities were correctly classified.
We report accuracy using all features and then remov-
ing each feature group, one at a time.
</bodyText>
<page confidence="0.973171">
1749
</page>
<table confidence="0.9981925">
AI2 IL CC
Relax Strict Relax Strict Relax Strict
All features 88.7 85.1 75.7 75.7 60.0 25.8
No Individual Quantity features 73.6 67.6 52.0 52.0 29.2 0.0
No Quantity Pair features 83.2 79.8 63.6 63.6 49.3 16.5
No Question features 86.8 83.9 73.3 73.3 60.5 28.3
</table>
<tableCaption confidence="0.996285">
Table 1: Performance of LCA Operation classifier on the datasets AI2, IL and CC.
</tableCaption>
<table confidence="0.9992415">
AI2 IL
Relax Strict Relax Strict
All features 94.7 89.1 95.4 93.2
No Unit features 88.9 71.5 92.8 91.0
No NP features 94.9 89.6 95.0 91.2
No Misc. features 92.0 85.9 93.7 89.8
</table>
<tableCaption confidence="0.974826">
Table 2: Performance of Relevance classifier on the datasets
AI2 and IL.
</tableCaption>
<bodyText confidence="0.9999865">
We see that features related to units of quantities play
the most significant role in determining relevance of
quantities. Also, the related NP features are not helpful
for the AI2 dataset.
</bodyText>
<subsectionHeader confidence="0.974878">
5.3 LCA Operation Classifier
</subsectionHeader>
<bodyText confidence="0.999977421052632">
Table 1 evaluates the performance of the LCA Oper-
ation classifier on the AI2, IL and CC datasets. As
before, we report two accuracies - Relax - fraction of
quantity pairs for which the classifier correctly pre-
dicted the LCA operation, and Strict - fraction of math
problems, for which all quantity pairs were correctly
classified. We report accuracy using all features and
then removing each feature group, one at a time.
The strict and relaxed accuracies for IL dataset are
identical, since each problem in IL dataset only re-
quires one operation. The features related to individual
quantities are most significant; in particular, the accu-
racy goes to 0.0 in the CC dataset, without using indi-
vidual quantity features. The question features are not
helpful for classification in the CC dataset. This can be
attributed to the fact that all problems in CC dataset re-
quire multiple operations, and questions in multi-step
problems usually do not contain information for each
of the required operations.
</bodyText>
<subsectionHeader confidence="0.863518">
5.4 Global Inference Module
</subsectionHeader>
<bodyText confidence="0.999304333333333">
Table 3 shows the performance of our system in cor-
rectly solving arithmetic word problems. We show
the impact of various contraints, and also compare
against previously best known results on the AI2 and
IL datasets. We also show results using each of the two
constraints separately, and using no constraints at all.
</bodyText>
<table confidence="0.998030375">
AI2 IL CC
All constraints 72.0 73.9 45.2
Positive constraint 78.0 72.5 36.5
Integral constraint 71.8 73.4 39.0
No constraint 77.7 71.9 29.6
(Hosseini et al., 2014) 77.7 - -
(Roy et al., 2015) - 52.7 -
(Kushman et al., 2014) 64.0 73.7 2.3
</table>
<tableCaption confidence="0.7845055">
Table 3: Accuracy in correctly solving arithmetic problems.
First four rows represent various configurations of our sys-
tem. We achieve state of the art results in both AI2 and IL
datasets.
</tableCaption>
<bodyText confidence="0.999976228571429">
The previously known best result in the AI2 dataset
is reported in (Hosseini et al., 2014). Since we follow
the exact same evaluation settings, our results are di-
rectly comparable. We achieve state of the art results,
without having access to any additional annotated data,
unlike (Hosseini et al., 2014), who use labeled data for
verb categorization. For the IL dataset, we acquired the
system of (Roy et al., 2015) from the authors, and ran
it with the same fold information. We outperform their
system by an absolute gain of over 20%. We believe
that the improvement was mainly due to the depen-
dence of the system of (Roy et al., 2015) on lexical and
neighborhood of quantity features. In contrast, features
from quantity schemas help us generalize across prob-
lem types. Finally, we also compare against the tem-
plate based system of (Kushman et al., 2014). (Hos-
seini et al., 2014) mentions the result of running the
system of (Kushman et al., 2014) on AI2 dataset, and
we report their result here. For IL and CC datasets, we
used the system released by (Kushman et al., 2014).
The integrality constraint is particularly helpful
when division is involved, since it can lead to fractional
answers. It does not help in case of the AI2 dataset,
which involves only addition and subtraction problems.
The role of the constraints becomes more significant in
case of multi-step problems and, in particular, they con-
tribute an absolute improvement of over 15% over the
system without constraints on the CC dataset. The tem-
plate based system of (Kushman et al., 2014) performs
on par with our system on the IL dataset. We believe
that it is due to the small number of equation templates
in the IL dataset. It performs poorly on the CC dataset,
since we evaluate on unseen problem types, which do
not ensure that equation templates in the test data will
be seen in the training data.
</bodyText>
<page confidence="0.970321">
1750
</page>
<subsectionHeader confidence="0.544994">
5.5 Discussion
</subsectionHeader>
<bodyText confidence="0.99999195">
The leading source of errors for the classifiers are er-
roneous quantity schema extraction and lack of under-
standing of unknown or rare verbs. For the relevance
classifier on the AI2 dataset, 25% of the errors were
due to mistakes in extracting the quantity schemas and
20% could be attributed to rare verbs. For the LCA
operation classifier on the same dataset, 16% of the er-
rors were due to unknown verbs and 15% were due to
mistakes in extracting the schemas. The erroneous ex-
traction of accurate quantity schemas is very significant
for the IL dataset, contributing 57% of the errors for the
relevance classifier and 39% of the errors for the LCA
operation classifier. For the operation classifier on the
CC dataset, 8% of the errors were due to verbs and 16%
were due to faulty quantity schema extraction. Quan-
tity Schema extraction is challenging due to parsing is-
sues as well as some non-standard rate patterns, and it
will be one of the future work targets. For example, in
the sentence, “How many 4-dollar toys can he buy?”,
we fail to extract the rate component of the quantity 4.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999314878787879">
This paper presents a novel method for understanding
and solving a general class of arithmetic word prob-
lems. Our approach can solve all problems whose so-
lution can be expressed by a read-once arithmetic ex-
pression, where each quantity from the problem text
appears at most once in the expression. We develop a
novel theoretical framework, centered around the no-
tion of monotone expression trees, and showed how
this representation can be used to get a unique decom-
position of the problem. This theory naturally leads to a
computational solution that we have shown to uniquely
determine the solution - determine the arithmetic oper-
ation between any two quantities identified in the text.
This theory underlies our algorithmic solution - we de-
velop classifiers and a constrained inference approach
that exploits redundancy in the information, and show
that this yields strong performance on several bench-
mark collections. In particular, our approach achieves
state of the art performance on two publicly available
arithmetic problem datasets and can support natural
generalizations. Specifically, our approach performs
competitively on multistep problems, even when it has
never observed the particular problem type before.
Although we develop and use the notion of expres-
sion trees in the context of numerical expressions, the
concept is more general. In particular, if we allow
leaves of expression trees to represent variables, we
can express algebraic expressions and equations in this
framework. Hence a similar approach can be targeted
towards algebra word problems, a direction we wish to
investigate in the future.
The datasets used in the paper are available for
download at http://cogcomp.cs.illinois.edu/page/resource view/98.
</bodyText>
<sectionHeader confidence="0.996559" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999564">
This research was sponsored by DARPA (under agree-
ment number FA8750-13-2-0008), and a grant from
AI2. Any opinions, findings, conclusions or recom-
mendations are those of the authors and do not nec-
essarily reflect the view of the agencies.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999835630434783">
R. Barzilay and M. Lapata. 2006. Aggregation via
Set Partitioning for Natural Language Generation.
In Human Language Technologies - North American
Chapter of the Association for Computational Lin-
guistics, June.
J. Berant, V. Srikumar, P. Chen, A. V. Linden, B. Hard-
ing, B. Huang, P. Clark, and C. D. Manning. 2014.
Modeling biological processes for reading compre-
hension. In Proceedings of EMNLP.
M. Chang, L. Ratinov, and D. Roth. 2012. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399–431, 6.
P. Clark. 2015. Elementary School Science and Math
Tests as a Driver for AI: Take the Aristo Challenge!
In Proceedings of IAAI.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 144–151, Sydney, Australia, July. ACL.
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 742–750, Los Angeles, California, June.
M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kush-
man. 2014. Learning to solve arithmetic word prob-
lems with verb categorization. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2014, October 25-
29, 2014, Doha, Qatar, pages 523–533.
N. Kushman, L. Zettlemoyer, R. Barzilay, and Y. Artzi.
2014. Learning to automatically solve algebra word
problems. In ACL, pages 271–281.
V. Punyakanok, D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role label-
ing. In Proc. of the International Joint Conference
on Artificial Intelligence (IJCAI), pages 1117–1123.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2).
D. Roth and W. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Hwee Tou Ng and Ellen Riloff, editors,
</reference>
<page confidence="0.810785">
1751
</page>
<reference confidence="0.999752428571429">
Proc. of the Conference on Computational Natural
Language Learning (CoNLL), pages 1–8. Associa-
tion for Computational Linguistics.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In Proc. of
the International Conference on Machine Learning
(ICML), pages 737–744.
S. Roy, T. Vieira, and D. Roth. 2015. Reasoning about
quantities in natural language. Transactions of the
Association for Computational Linguistics, 3.
F. Sadeghi, S. K. Divvala, and A. Farhadi. 2015.
Viske: Visual knowledge extraction and question an-
swering by visual verification of relation phrases. In
The IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), June.
M. J. Seo, H. Hajishirzi, A. Farhadi, and O. Etzioni.
2014. Diagram understanding in geometry ques-
tions. In Proceedings of the Twenty-Eighth AAAI
Conference on Artificial Intelligence, July 27 -31,
2014, Qu´ebec City, Qu´ebec, Canada., pages 2831–
2838.
</reference>
<page confidence="0.994099">
1752
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.863675">
<title confidence="0.999968">Solving General Arithmetic Word Problems</title>
<author confidence="0.999263">Subhro Roy Dan Roth</author>
<affiliation confidence="0.9398945">University of Illinois, University of Illinois, Urbana Champaign Urbana Champaign</affiliation>
<email confidence="0.998606">sroy9@illinois.edudanr@illinois.edu</email>
<abstract confidence="0.999223">This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers from the use of schemas supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Aggregation via Set Partitioning for Natural Language Generation.</title>
<date>2006</date>
<booktitle>In Human Language Technologies - North American Chapter of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="9032" citStr="Barzilay and Lapata, 2006" startWordPosition="1430" endWordPosition="1433">ations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P, which mentions n quantities q1, q2, . . . , qn. Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once arithmetic expression, and we only consider in this work problems that can be so</context>
</contexts>
<marker>Barzilay, Lapata, 2006</marker>
<rawString>R. Barzilay and M. Lapata. 2006. Aggregation via Set Partitioning for Natural Language Generation. In Human Language Technologies - North American Chapter of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>V Srikumar</author>
<author>P Chen</author>
<author>A V Linden</author>
<author>B Harding</author>
<author>B Huang</author>
<author>P Clark</author>
<author>C D Manning</author>
</authors>
<title>Modeling biological processes for reading comprehension.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8065" citStr="Berant et al., 2014" startWordPosition="1279" endWordPosition="1282"> 2014) might be the most related to ours. It tries to map numbers from the problem text to predefined equation templates. However, they implicitly assume that similar equation forms have been seen in the training data. In contrast, our system can perform competitively, even when it has never seen similar expressions in training. There is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds. Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text. There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014). A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning se</context>
</contexts>
<marker>Berant, Srikumar, Chen, Linden, Harding, Huang, Clark, Manning, 2014</marker>
<rawString>J. Berant, V. Srikumar, P. Chen, A. V. Linden, B. Harding, B. Huang, P. Clark, and C. D. Manning. 2014. Modeling biological processes for reading comprehension. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Structured learning with constrained conditional models.</title>
<date>2012</date>
<booktitle>Machine Learning,</booktitle>
<volume>88</volume>
<issue>3</issue>
<pages>6</pages>
<contexts>
<context position="8554" citStr="Chang et al., 2012" startWordPosition="1353" endWordPosition="1356">ork in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text. There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014). A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmeti</context>
<context position="21963" citStr="Chang et al., 2012" startWordPosition="3754" endWordPosition="3757">n creating the solution. A binary classifier trained to predict whether a quantity q is relevant or not (Section 4.3), can provide these scores. For an expression E, letZ(E) be the set of all quantities in P which are not used in expression E. Let T be a monotonic expression tree for E. We define Score(E) of an expression E in terms of the above scoring functions and a scaling parameter wIRR as follows: 11 Score(E) =wIRR IRR(q)+ (3) qEZ(E) 11 PAIR(qi, qj, OLCA(qi, qj, T)) qi,qj /EZ(E) Our final expression tree is an outcome of a constrained optimization process, following (Roth and Yih, 2004; Chang et al., 2012). Our objective function makes use of the scores returned by IRR(·) and PAIR(·) to determine the expression tree and is constrained by legitimacy and background knowledge constraints, detailed below. 1. Positive Answer: Most arithmetic problems asking for amounts or number of objects usually have a positive number as an answer. Therefore, while searching for the best scoring expression, we reject expressions generating negative answer. 2. Integral Answer: Problems with questions such as ‘how many” usually expect integral solutions. We only consider integral solutions as legitimate outputs for </context>
</contexts>
<marker>Chang, Ratinov, Roth, 2012</marker>
<rawString>M. Chang, L. Ratinov, and D. Roth. 2012. Structured learning with constrained conditional models. Machine Learning, 88(3):399–431, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clark</author>
</authors>
<title>Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!</title>
<date>2015</date>
<booktitle>In Proceedings of IAAI.</booktitle>
<contexts>
<context position="8018" citStr="Clark, 2015" startWordPosition="1273" endWordPosition="1274"> algebra word problems (Kushman et al., 2014) might be the most related to ours. It tries to map numbers from the problem text to predefined equation templates. However, they implicitly assume that similar equation forms have been seen in the training data. In contrast, our system can perform competitively, even when it has never seen similar expressions in training. There is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds. Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text. There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014). A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts</context>
</contexts>
<marker>Clark, 2015</marker>
<rawString>P. Clark. 2015. Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge! In Proceedings of IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Constraint-based sentence compression: An integer programming approach.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>144--151</pages>
<publisher>ACL.</publisher>
<location>Sydney, Australia,</location>
<contexts>
<context position="9005" citStr="Clarke and Lapata, 2006" startWordPosition="1426" endWordPosition="1429">by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P, which mentions n quantities q1, q2, . . . , qn. Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once arithmetic expression, and we only consider in this w</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>J. Clarke and M. Lapata. 2006. Constraint-based sentence compression: An integer programming approach. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 144–151, Sydney, Australia, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--750</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="25480" citStr="Goldberg and Elhadad, 2010" startWordPosition="4357" endWordPosition="4361">evant information from the problem text. To this end, we introduce the concept of a quantity schema which we extract for each quantity in the problem’s text. Along with the question asked, the quantity 1747 schemas provides all the information needed to solve most arithmetic problems. A quantity schema for a quantity q in problem P consists of the following components. 1. Associated Verb For each quantity q, we detect the verb associated with it. We traverse up the dependency tree starting from the quantity mention, and choose the first verb we reach. We used the easy first dependency parser (Goldberg and Elhadad, 2010). 2. Subject of Associated Verb We detect the noun phrase, which acts as subject of the associated verb (if one exists). 3. Unit We use a shallow parser to detect the phrase p in which the quantity q is mentioned. All tokens of the phrase (other than the number itself) are considered as unit tokens. Also, if p is followed by the prepositional phrase “of” and a noun phrase (according to the shallow parser annotations), we also consider tokens from this second noun phrase as unit tokens. Finally, if no unit token can be extracted, we assign the unit of the neighboring quantities as the unit of q</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Y. Goldberg and M. Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742–750, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Hosseini</author>
<author>H Hajishirzi</author>
<author>O Etzioni</author>
<author>N Kushman</author>
</authors>
<title>Learning to solve arithmetic word problems with verb categorization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>523--533</pages>
<location>Doha, Qatar,</location>
<contexts>
<context position="2243" citStr="Hosseini et al., 2014" startWordPosition="330" endWordPosition="333">atively straightforward text, and seemingly simple semantics. Arithmetic word problems are usually directed towards elementary school students, and can be solved by combining the numbers mentioned in text with basic operations (addition, subtraction, multiplication, division). They are simpler than algebra word problems which require students to identify variables, and form equations with these variables to solve the problem. Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with multi-step arithmetic problems involving all four basic operations. The template based method of (Kushman et al., 2014), on the other hand, can deal with all types of problems, but implicitly assumes that the solution is generated from a set of predefined equation templates. In this paper, we present a novel approach which can solve a general class of arithmetic problems without predefined equation templates. In particular, it can handle multiple step arithmetic problems as shown in Example 1. Example 1 Gwen was organizing her book case making sure each of the shelves </context>
<context position="6666" citStr="Hosseini et al., 2014" startWordPosition="1056" endWordPosition="1059">formance in this challenging evaluation setting. The next section describes the related work in the area of automated math word problem solving. We then present the theory of expression trees and our decomposition strategy that is based on it. Sec. 4 presents the overall computational approach, including the way we use quantity schemas to learn the mapping from text to expression tree components. Finally, we discuss our experimental study and conclude. 2 Related Work Previous work in automated arithmetic problem solvers has focussed on a restricted subset of problems. The system described in (Hosseini et al., 2014) handles only addition and subtraction problems, and requires additional annotated data for verb categories. In contrast, our system does not require any additional annotations and can handle a more general category of problems. The approach in (Roy et al., 2015) supports all four basic operations, and uses a pipeline of classifiers to predict different properties of the problem. However, it makes assumptions on the number of quantities mentioned in the problem text, as well as the number of arithmetic steps required to solve the problem. In contrast, our system does not have any such restrict</context>
<context position="26129" citStr="Hosseini et al., 2014" startWordPosition="4476" endWordPosition="4479">Verb We detect the noun phrase, which acts as subject of the associated verb (if one exists). 3. Unit We use a shallow parser to detect the phrase p in which the quantity q is mentioned. All tokens of the phrase (other than the number itself) are considered as unit tokens. Also, if p is followed by the prepositional phrase “of” and a noun phrase (according to the shallow parser annotations), we also consider tokens from this second noun phrase as unit tokens. Finally, if no unit token can be extracted, we assign the unit of the neighboring quantities as the unit of q (following previous work (Hosseini et al., 2014)). 4. Related Noun Phrases We consider all noun phrases which are connected to the phrase p containing quantity q, with NP-PP-NP attachment. If only one quantity is mentioned in a sentence, we consider all noun phrases in it as related. 5. Rate We determine whether quantity q refers to a rate in the text, as well as extract two unit components defining the rate. For example, “7 kilometers per hour” has two components “kilometers” and “hour”. Similarly, for sentences describing unit cost like “Each egg costs 2 dollars”, “2” is a rate, with units “dollars” and “egg”. In addition to extracting th</context>
<context position="29436" citStr="Hosseini et al., 2014" startWordPosition="5050" endWordPosition="5053">oned in text. We include pairwise conjunction of the above features. 4.4 LCA Operation Classifier In order to predict LCA operations, we train a multiclass SVM classifier. Given problem text P and a pair of quantities pi and pj, the classifier predicts one of the six labels described in Eq. 2. We consider the confidence scores for each label supplied by the classifier as the scoring function PAIR(·). 4.4.1 Features We use the following categories of features: 1. Individual Quantity features: Dependent verbs have been shown to play significant role in solving addition and subtraction problems (Hosseini et al., 2014). Hence, we add the dependent verb of the quantity as a feature. Multiplication and division problems are largely dependent on rates described in text. To capture that, we add a feature based on whether the quantity is a rate, and whether any component of rate unit is present in 1748 the question. In addition to these quantity schema features, we add selected tokens from the neighborhood of the quantity mention. Neighborhood of quantities are often highly informative of LCA operations, for example, “He got 80 more marbles”, the term “more” usually indicates addition. We add as features adverbs</context>
<context position="31726" citStr="Hosseini et al., 2014" startWordPosition="5431" endWordPosition="5434"> under default settings. 5 Experimental Results In this section, we evaluate the proposed method on publicly available datasets of arithmetic word problems. We evaluate separately the relevance and LCA operation classifiers, and show the contribution of various features. Lastly, we evaluate the performance of the full system, and quantify the gains achieved by the constraints. 5.1 Datasets We evaluate our system on three datasets, each of which comprise a different category of arithmetic word problems. 1. AI2 Dataset: This is a collection of 395 addition and subtraction problems, released by (Hosseini et al., 2014). They performed a 3-fold cross validation, with every fold containing problems from different sources. This helped them evaluate robustness to domain diversity. We follow the same evaluation setting. 1http://cogcomp.cs.illinois.edu/page/software view/IllinoisSL 2. IL Dataset: This is a collection of arithmetic problems released by (Roy et al., 2015). Each of these problems can be solved by performing one operation. However, there are multiple problems having the same template. To counter this, we perform a few modifications to the dataset. First, for each problem, we replace the numbers and n</context>
<context position="36589" citStr="Hosseini et al., 2014" startWordPosition="6229" endWordPosition="6232">ions, and questions in multi-step problems usually do not contain information for each of the required operations. 5.4 Global Inference Module Table 3 shows the performance of our system in correctly solving arithmetic word problems. We show the impact of various contraints, and also compare against previously best known results on the AI2 and IL datasets. We also show results using each of the two constraints separately, and using no constraints at all. AI2 IL CC All constraints 72.0 73.9 45.2 Positive constraint 78.0 72.5 36.5 Integral constraint 71.8 73.4 39.0 No constraint 77.7 71.9 29.6 (Hosseini et al., 2014) 77.7 - - (Roy et al., 2015) - 52.7 - (Kushman et al., 2014) 64.0 73.7 2.3 Table 3: Accuracy in correctly solving arithmetic problems. First four rows represent various configurations of our system. We achieve state of the art results in both AI2 and IL datasets. The previously known best result in the AI2 dataset is reported in (Hosseini et al., 2014). Since we follow the exact same evaluation settings, our results are directly comparable. We achieve state of the art results, without having access to any additional annotated data, unlike (Hosseini et al., 2014), who use labeled data for verb </context>
</contexts>
<marker>Hosseini, Hajishirzi, Etzioni, Kushman, 2014</marker>
<rawString>M. J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, pages 523–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushman</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
<author>Y Artzi</author>
</authors>
<title>Learning to automatically solve algebra word problems.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>271--281</pages>
<contexts>
<context position="2387" citStr="Kushman et al., 2014" startWordPosition="352" endWordPosition="355">nd can be solved by combining the numbers mentioned in text with basic operations (addition, subtraction, multiplication, division). They are simpler than algebra word problems which require students to identify variables, and form equations with these variables to solve the problem. Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with multi-step arithmetic problems involving all four basic operations. The template based method of (Kushman et al., 2014), on the other hand, can deal with all types of problems, but implicitly assumes that the solution is generated from a set of predefined equation templates. In this paper, we present a novel approach which can solve a general class of arithmetic problems without predefined equation templates. In particular, it can handle multiple step arithmetic problems as shown in Example 1. Example 1 Gwen was organizing her book case making sure each of the shelves had exactly 9 books on it. She has 2 types of books - mystery books and picture books. If she had 3 shelves of mystery books and 5 shelves ofpic</context>
<context position="7451" citStr="Kushman et al., 2014" startWordPosition="1179" endWordPosition="1182"> annotations and can handle a more general category of problems. The approach in (Roy et al., 2015) supports all four basic operations, and uses a pipeline of classifiers to predict different properties of the problem. However, it makes assumptions on the number of quantities mentioned in the problem text, as well as the number of arithmetic steps required to solve the problem. In contrast, our system does not have any such restrictions, effectively handling problems mentioning multiple quantities and requiring multiple steps. Kushman’s approach to automatically solving algebra word problems (Kushman et al., 2014) might be the most related to ours. It tries to map numbers from the problem text to predefined equation templates. However, they implicitly assume that similar equation forms have been seen in the training data. In contrast, our system can perform competitively, even when it has never seen similar expressions in training. There is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds. Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant</context>
<context position="36649" citStr="Kushman et al., 2014" startWordPosition="6243" endWordPosition="6246">tain information for each of the required operations. 5.4 Global Inference Module Table 3 shows the performance of our system in correctly solving arithmetic word problems. We show the impact of various contraints, and also compare against previously best known results on the AI2 and IL datasets. We also show results using each of the two constraints separately, and using no constraints at all. AI2 IL CC All constraints 72.0 73.9 45.2 Positive constraint 78.0 72.5 36.5 Integral constraint 71.8 73.4 39.0 No constraint 77.7 71.9 29.6 (Hosseini et al., 2014) 77.7 - - (Roy et al., 2015) - 52.7 - (Kushman et al., 2014) 64.0 73.7 2.3 Table 3: Accuracy in correctly solving arithmetic problems. First four rows represent various configurations of our system. We achieve state of the art results in both AI2 and IL datasets. The previously known best result in the AI2 dataset is reported in (Hosseini et al., 2014). Since we follow the exact same evaluation settings, our results are directly comparable. We achieve state of the art results, without having access to any additional annotated data, unlike (Hosseini et al., 2014), who use labeled data for verb categorization. For the IL dataset, we acquired the system o</context>
<context position="37931" citStr="Kushman et al., 2014" startWordPosition="6465" endWordPosition="6468">me fold information. We outperform their system by an absolute gain of over 20%. We believe that the improvement was mainly due to the dependence of the system of (Roy et al., 2015) on lexical and neighborhood of quantity features. In contrast, features from quantity schemas help us generalize across problem types. Finally, we also compare against the template based system of (Kushman et al., 2014). (Hosseini et al., 2014) mentions the result of running the system of (Kushman et al., 2014) on AI2 dataset, and we report their result here. For IL and CC datasets, we used the system released by (Kushman et al., 2014). The integrality constraint is particularly helpful when division is involved, since it can lead to fractional answers. It does not help in case of the AI2 dataset, which involves only addition and subtraction problems. The role of the constraints becomes more significant in case of multi-step problems and, in particular, they contribute an absolute improvement of over 15% over the system without constraints on the CC dataset. The template based system of (Kushman et al., 2014) performs on par with our system on the IL dataset. We believe that it is due to the small number of equation templat</context>
</contexts>
<marker>Kushman, Zettlemoyer, Barzilay, Artzi, 2014</marker>
<rawString>N. Kushman, L. Zettlemoyer, R. Barzilay, and Y. Artzi. 2014. Learning to automatically solve algebra word problems. In ACL, pages 271–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The necessity of syntactic parsing for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1117--1123</pages>
<contexts>
<context position="8955" citStr="Punyakanok et al., 2005" startWordPosition="1418" endWordPosition="1421">i et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P, which mentions n quantities q1, q2, . . . , qn. Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once ari</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2005. The necessity of syntactic parsing for semantic role labeling. In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI), pages 1117–1123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="8980" citStr="Punyakanok et al., 2008" startWordPosition="1422" endWordPosition="1425">answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P, which mentions n quantities q1, q2, . . . , qn. Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once arithmetic expression, and w</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>1--8</pages>
<publisher>Association for Computational Linguistics.</publisher>
<contexts>
<context position="21942" citStr="Roth and Yih, 2004" startWordPosition="3750" endWordPosition="3753"> is, q is not used in creating the solution. A binary classifier trained to predict whether a quantity q is relevant or not (Section 4.3), can provide these scores. For an expression E, letZ(E) be the set of all quantities in P which are not used in expression E. Let T be a monotonic expression tree for E. We define Score(E) of an expression E in terms of the above scoring functions and a scaling parameter wIRR as follows: 11 Score(E) =wIRR IRR(q)+ (3) qEZ(E) 11 PAIR(qi, qj, OLCA(qi, qj, T)) qi,qj /EZ(E) Our final expression tree is an outcome of a constrained optimization process, following (Roth and Yih, 2004; Chang et al., 2012). Our objective function makes use of the scores returned by IRR(·) and PAIR(·) to determine the expression tree and is constrained by legitimacy and background knowledge constraints, detailed below. 1. Positive Answer: Most arithmetic problems asking for amounts or number of objects usually have a positive number as an answer. Therefore, while searching for the best scoring expression, we reject expressions generating negative answer. 2. Integral Answer: Problems with questions such as ‘how many” usually expect integral solutions. We only consider integral solutions as le</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Hwee Tou Ng and Ellen Riloff, editors, Proc. of the Conference on Computational Natural Language Learning (CoNLL), pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML),</booktitle>
<pages>737--744</pages>
<contexts>
<context position="8930" citStr="Roth and Yih, 2005" startWordPosition="1414" endWordPosition="1417"> recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P, which mentions n quantities q1, q2, . . . , qn. Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if i</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>D. Roth and W. Yih. 2005. Integer linear programming inference for conditional random fields. In Proc. of the International Conference on Machine Learning (ICML), pages 737–744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Roy</author>
<author>T Vieira</author>
<author>D Roth</author>
</authors>
<title>Reasoning about quantities in natural language.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>3</volume>
<contexts>
<context position="2219" citStr="Roy et al., 2015" startWordPosition="326" endWordPosition="329">ts concise and relatively straightforward text, and seemingly simple semantics. Arithmetic word problems are usually directed towards elementary school students, and can be solved by combining the numbers mentioned in text with basic operations (addition, subtraction, multiplication, division). They are simpler than algebra word problems which require students to identify variables, and form equations with these variables to solve the problem. Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with multi-step arithmetic problems involving all four basic operations. The template based method of (Kushman et al., 2014), on the other hand, can deal with all types of problems, but implicitly assumes that the solution is generated from a set of predefined equation templates. In this paper, we present a novel approach which can solve a general class of arithmetic problems without predefined equation templates. In particular, it can handle multiple step arithmetic problems as shown in Example 1. Example 1 Gwen was organizing her book case making s</context>
<context position="6929" citStr="Roy et al., 2015" startWordPosition="1098" endWordPosition="1101">all computational approach, including the way we use quantity schemas to learn the mapping from text to expression tree components. Finally, we discuss our experimental study and conclude. 2 Related Work Previous work in automated arithmetic problem solvers has focussed on a restricted subset of problems. The system described in (Hosseini et al., 2014) handles only addition and subtraction problems, and requires additional annotated data for verb categories. In contrast, our system does not require any additional annotations and can handle a more general category of problems. The approach in (Roy et al., 2015) supports all four basic operations, and uses a pipeline of classifiers to predict different properties of the problem. However, it makes assumptions on the number of quantities mentioned in the problem text, as well as the number of arithmetic steps required to solve the problem. In contrast, our system does not have any such restrictions, effectively handling problems mentioning multiple quantities and requiring multiple steps. Kushman’s approach to automatically solving algebra word problems (Kushman et al., 2014) might be the most related to ours. It tries to map numbers from the problem t</context>
<context position="9051" citStr="Roy et al., 2015" startWordPosition="1434" endWordPosition="1437">strained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015). 3 Expression Tree and Problem Decomposition We address the problem of automatically solving arithmetic word problems. The input to our system is the problem text P, which mentions n quantities q1, q2, . . . , qn. Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution. We define a read-once arithmetic expression as one that makes use of each quantity at most once. We say that E is a valid expression, if it is such a Read-Once arithmetic expression, and we only consider in this work problems that can be solved using valid ex</context>
<context position="32078" citStr="Roy et al., 2015" startWordPosition="5480" endWordPosition="5483">ed by the constraints. 5.1 Datasets We evaluate our system on three datasets, each of which comprise a different category of arithmetic word problems. 1. AI2 Dataset: This is a collection of 395 addition and subtraction problems, released by (Hosseini et al., 2014). They performed a 3-fold cross validation, with every fold containing problems from different sources. This helped them evaluate robustness to domain diversity. We follow the same evaluation setting. 1http://cogcomp.cs.illinois.edu/page/software view/IllinoisSL 2. IL Dataset: This is a collection of arithmetic problems released by (Roy et al., 2015). Each of these problems can be solved by performing one operation. However, there are multiple problems having the same template. To counter this, we perform a few modifications to the dataset. First, for each problem, we replace the numbers and nouns with the part of speech tags, and then we cluster the problems based on unigrams and bigrams from this modified problem text. In particular, we cluster problems together whose unigram-bigram similarity is over 90%. We next prune each cluster to keep at most 5 problems in each cluster. Finally we create the folds ensuring all problems in a cluste</context>
<context position="36617" citStr="Roy et al., 2015" startWordPosition="6236" endWordPosition="6239"> problems usually do not contain information for each of the required operations. 5.4 Global Inference Module Table 3 shows the performance of our system in correctly solving arithmetic word problems. We show the impact of various contraints, and also compare against previously best known results on the AI2 and IL datasets. We also show results using each of the two constraints separately, and using no constraints at all. AI2 IL CC All constraints 72.0 73.9 45.2 Positive constraint 78.0 72.5 36.5 Integral constraint 71.8 73.4 39.0 No constraint 77.7 71.9 29.6 (Hosseini et al., 2014) 77.7 - - (Roy et al., 2015) - 52.7 - (Kushman et al., 2014) 64.0 73.7 2.3 Table 3: Accuracy in correctly solving arithmetic problems. First four rows represent various configurations of our system. We achieve state of the art results in both AI2 and IL datasets. The previously known best result in the AI2 dataset is reported in (Hosseini et al., 2014). Since we follow the exact same evaluation settings, our results are directly comparable. We achieve state of the art results, without having access to any additional annotated data, unlike (Hosseini et al., 2014), who use labeled data for verb categorization. For the IL d</context>
</contexts>
<marker>Roy, Vieira, Roth, 2015</marker>
<rawString>S. Roy, T. Vieira, and D. Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sadeghi</author>
<author>S K Divvala</author>
<author>A Farhadi</author>
</authors>
<title>Viske: Visual knowledge extraction and question answering by visual verification of relation phrases.</title>
<date>2015</date>
<booktitle>In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),</booktitle>
<contexts>
<context position="8347" citStr="Sadeghi et al., 2015" startWordPosition="1323" endWordPosition="1326">t has never seen similar expressions in training. There is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds. Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text. There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014). A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et a</context>
</contexts>
<marker>Sadeghi, Divvala, Farhadi, 2015</marker>
<rawString>F. Sadeghi, S. K. Divvala, and A. Farhadi. 2015. Viske: Visual knowledge extraction and question answering by visual verification of relation phrases. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Seo</author>
<author>H Hajishirzi</author>
<author>A Farhadi</author>
<author>O Etzioni</author>
</authors>
<title>Diagram understanding in geometry questions.</title>
<date>2014</date>
<booktitle>In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,</booktitle>
<volume>27</volume>
<pages>2831--2838</pages>
<location>Qu´ebec City, Qu´ebec, Canada.,</location>
<contexts>
<context position="8309" citStr="Seo et al., 2014" startWordPosition="1315" endWordPosition="1318">perform competitively, even when it has never seen similar expressions in training. There is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds. Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015). The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text. There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014). A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images. Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012). In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time. This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured predictio</context>
</contexts>
<marker>Seo, Hajishirzi, Farhadi, Etzioni, 2014</marker>
<rawString>M. J. Seo, H. Hajishirzi, A. Farhadi, and O. Etzioni. 2014. Diagram understanding in geometry questions. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu´ebec City, Qu´ebec, Canada., pages 2831– 2838.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001476">
<note confidence="0.762479">
Learn to Solve Algebra Word Problems Using Quadratic Programming
2 2
</note>
<author confidence="0.453414">
Lipu Zhou, Shuaixiang Dai, and Liwei Chen
</author>
<affiliation confidence="0.408416">
Baidu Inc., Beijing, China
</affiliation>
<email confidence="0.905226">
zhoulipu@outlook.com, {daishuaixiang, chenliwei}@baidu.com
</email>
<sectionHeader confidence="0.993511" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999821777777778">
This paper presents a new algorithm to
automatically solve algebra word prob-
lems. Our algorithm solves a word prob-
lem via analyzing a hypothesis space con-
taining all possible equation systems gen-
erated by assigning the numbers in the
word problem into a set of equation sys-
tem templates extracted from the training
data. To obtain a robust decision surface,
we train a log-linear model to make the
margin between the correct assignments
and the false ones as large as possible.
This results in a quadratic programming
(QP) problem which can be efficiently
solved. Experimental results show that our
algorithm achieves 79.7% accuracy, about
10% higher than the state-of-the-art base-
line (Kushman et al., 2014).
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999263555555555">
An algebra word problem describes a mathemat-
ical problem which can be typically modeled by
an equation system, as demonstrated in Figure 1.
Seeking to automatically solve word problems is
a classical AI problem (Bobrow, 1964). The word
problem solver is traditionally created by the rule-
based approach (Lev et al., 2004; Mukherjee and
Garain, 2008; Matsuzaki et al., 2013). Recently,
using machine learning techniques to construct the
solver has become a new trend (Kushman et al.,
2014; Hosseini et al., 2014; Amnueypornsakul
and Bhat, 2014; Roy et al., 2015). This is based
on the fact that word problems derived from the
same mathematical problem share some common
semantic and syntactic features due to the same
underlying logic. Our method follows this trend.1
To solve a word problem, our algorithm ana-
lyzes all the possible ways to assign the numbers
</bodyText>
<footnote confidence="0.9944405">
1Our code is available at http://pan.baidu.com/
s/1dD336Sx
</footnote>
<table confidence="0.88522925">
Our method Kushman’s method
Word An amusement park sells 2 kinds of tickets. Tickets for
problem tickets cost $ 4. On a certain day,
On that same day the admission
How many children were admitted
were admitted?
children cost $ 1.50. Adult
278 people entered the park.
fees collected totaled $ 792.
on that day? How many adults
Template U1 u2 nl 0 u1 u2 n1 0
n2 ul n3 u2 N 0 n2 U n3 u2 n4 0
Assignment 2,1.5, 4, 278, 792 2,1.5, 4, 278, 792, nouns
n1, n2, n3, n4 n1,nVn�,n4,u1,u2 1 ,uz
Possible 5432120 174 543210022520
Assignment
</table>
<figureCaption confidence="0.999032">
Figure 1: Comparison between our algorithm and
(Kushman et al., 2014). Nouns are boldfaced.
</figureCaption>
<bodyText confidence="0.999789653846154">
in the word problem to a set of equation system
templates. Kushman et al. (2014) also consider
filling the equation system templates to generate
the candidate equations. But Kushman’s template
contains number slots (e.g. n1, n2, n3, n4 in Fig-
ure 1) and unknown slots (e.g. ui, u21, u12, u22
in Figure 1). They separately consider assigning
nouns into the unknown slots and numbers into
the number slots, as demonstrated in Figure 1. As
filling the unknown slots is closely related to the
number slots assignment, we only consider assign-
ing the number slots, and design effective features
to describe the relationship between numbers and
unknowns. This scheme significantly reduces the
hypothesis space, as illustrated in Figure 1, which
benefits the learning and inference processes.
We use a log-linear model to describe the tem-
plate selection and number assignment. To learn
the model parameters of such problem, maxi-
mizing the log-likelihood objective is generally
adopted (Kwiatkowski et al., 2010; Kushman et
al., 2014). The key difficulty of this method is
that calculating the gradient of the objective func-
tion needs to sum over exponentially many sam-
ples. Thus, it is essential to approximate the gra-
dient. For instance, Kushman et al. (2014) use
</bodyText>
<page confidence="0.962069">
817
</page>
<note confidence="0.6562725">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 817–822,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999732307692308">
beam search to approximately calculate the gra-
dient. This method can not exploit all the training
samples. Thus the resulting model may be sub-
optimal. Motivated by the work (Taskar et al.,
2005; Li, 2014), we adopt the max-margin objec-
tive. This results in a QP problem and opens the
way toward an efficient learning algorithm (Koller
and Friedman, 2009).
We evaluate our algorithm on the benchmark
dataset provided by (Kushman et al., 2014). The
experimental results show that our algorithm sig-
nificantly outperforms the state-of-the-art base-
line (Kushman et al., 2014).
</bodyText>
<sectionHeader confidence="0.962929" genericHeader="method">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.980876">
Our word problem solver is constructed by train-
ing a log-linear model to find the correct mapping
from a word problem to an equation.
Notations: Let X denote the set of training word
problems, and T denote the set of equation sys-
tem templates abstracted from X as (Kushman et
al., 2014). xi is the i-th word problem in X.
Assume Tj is the j-th equation system template
in T, and NTj = { n1Tj 7 7 , n2T. ,···, nT I is the
set of number slots of Tj, where m represents
the size of NTj. Denote the numbers in xi by
</bodyText>
<equation confidence="0.93247">
Nxi = {n1xi, n2xi, · · · , nl }, where l represents
xi
</equation>
<bodyText confidence="0.999878454545455">
the size of Nxi. Assuming l ≥ m, we further de-
fine πijk a sequence of m numbers chosen from
Nxi without repetition. Given πijk, we can map
Tj to an equation system eijk by filling the num-
ber slots NTj of Tj sequently with the numbers in
πijk. Solving eijk, we can obtain the correspond-
ing solution sijk. To simplify the notation, we de-
fine yijk = (Tj, πijk, eijk, sijk) the k-th derivation
give xi and Tj, and let Yi denote the set of all pos-
sible yijk given xi and T . Therefore, to correctly
solve xi is to find the correct yijk ∈ Yi.
</bodyText>
<listItem confidence="0.696375333333333">
Probabilistic Model: As (Kushman et al., 2014),
we use the log-linear model to define the probabil-
ity of yijk ∈ Yi given xi:
</listItem>
<equation confidence="0.985443666666667">
eθ·φ(xi,yijk)
p(yijk|xi; θ) = E eθ·φ(xi,y&apos;ijk) (1)
y&apos;ijkEYi
</equation>
<bodyText confidence="0.999965">
where θ is the parameter vector of the model, and
φ (xi, yijk) denotes the feature function. We adopt
the max-margin objective (Vapnik, 2013) to di-
rectly learn the decision boundary for the correct
derivations and the false ones.
</bodyText>
<sectionHeader confidence="0.708644" genericHeader="method">
3 Learning and Inference
</sectionHeader>
<subsectionHeader confidence="0.998751">
3.1 Learning
</subsectionHeader>
<bodyText confidence="0.995758666666667">
Using (1), we obtain the difference between the
log-probability of a correct derivation ycijk ∈ Yi
and a false one yf ijl ∈ Yi as:
</bodyText>
<equation confidence="0.9154165">
)ln P �yc ijk|xi; θ� − ln P(yf ijl|xi; θ
=θ ·(φ (xi, yjk) − (xi, yijl) ) (2)
</equation>
<bodyText confidence="0.9994937">
Note that the subtraction in (2) cancels the denom-
inator of (1) which contains extensive computa-
tion. To decrease the generalization error of the
learned model, we would like the minimal gap be-
tween the correct derivations and the false ones as
large as possible. In practice, we may not find a
decision hyperplane to perfectly separate the cor-
rect and the false derivations. Generally, this can
be solved by introducing a slack variable ξijkl ≥
0 (Bishop, 2006) for each constraint derived from
</bodyText>
<equation confidence="0.9668845">
(2). Define ϕ
(xi,ycijk,yijl) = φ (xi, yic k) −
(xi, y mol) . For ∀ xi ∈ X, the resulting optimiza-
tion problem is:
1�
2kθk2 + C
i,j,k,l
s.t. θ · ϕ (xi, ycijk, yijl) ≥ 1 − ξijkl, ξijkl ≥ 0
</equation>
<bodyText confidence="0.999857">
The parameter C is used to balance the slack vari-
able penalty and the margin. This is a QP problem
and has been well studied (Platt, 1999; Fan et al.,
2008).
According to the Karush-Kuhn-Tucker (KKT)
condition, only a part of the constraints is active
for the solution of (3) (Bishop, 2006). This leads
to an efficient learning algorithm called constraint
generation (Koller and Friedman, 2009; Felzen-
szwalb et al., 2010). Specifically, an initial model
is trained by a randomly selected subset of the
constraints. Next this model is used to check the
constraints and at most N false deviations that are
erroneously classified by this model are collected
for each word problem. These constraints are then
added to train a new model. This process repeats
until converges. Our experimental results show
that this process converges fast.
</bodyText>
<subsectionHeader confidence="0.926852">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.9993725">
When we obtain the model parameter θ, the infer-
ence can be performed by finding the maximum
</bodyText>
<figure confidence="0.443122">
arg min
ξijkl (3)
</figure>
<page confidence="0.985763">
818
</page>
<table confidence="0.995636454545455">
Single slot features
Relation between numbers and the question
sentence.
Position of a number w.r.t a comparative word.
Context of a number.
Is one or two?
Is a multiplier?
Is between 0 and 1?
Slot pair features
Relation between two numbers.
Context similarity between two numbers.
Does there exist coreference relationship?
Are two numbers both multipliers?
Are two numbers in the same sentence or con-
tinuous sentences?
Information of raw path and dependency path
between two numbers
One number is larger than another.
Solution features
Is integer solution?
Is positive solution?
Is between 0 and 1?
</table>
<tableCaption confidence="0.8730825">
Table 1: Features used in our algorithm.
value of (1). This can be simplified by computing
</tableCaption>
<equation confidence="0.8590965">
arg max θ · φ(xi,yijk) (4)
yijk∈Yi
</equation>
<bodyText confidence="0.999943833333333">
As we only consider assigning the number slots of
the templates in T , generally, the size of the possi-
ble assignments per word problem is bearable, as
shown in the Table 2. Thus we simply evaluate all
the yijk E Yi. The one with the largest score is
considered as the solution of xi.
</bodyText>
<sectionHeader confidence="0.999715" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999977923076923">
A feature vector φ (xi, yijk) is calculated for each
word problem xi and derivation yijk pair. As
Kushman (2014), a feature is associated with a sig-
nature related to the template of yijk. We extract
three kinds of features, i.e., single slot features,
slot pair features and solution features. Unless
otherwise stated, single slot and slot pair features
are associated with the slot and slot pair signature
of the equation system template, respectively, and
solution features are generated for the signature of
the equation system template. Table 1 lists the fea-
tures used in our algorithm. The detailed descrip-
tion is as follows.
</bodyText>
<subsectionHeader confidence="0.995093">
4.1 Single Slot Features
</subsectionHeader>
<bodyText confidence="0.99997946">
To reduce the search space, we only consider the
assignment of the number slots of the template. It
seems that our algorithm will lose the information
about the unknown. But such information can be
recovered by the features that include the infor-
mation of the question sentence. Specifically, we
associate a number with all the nouns in the same
sentence sorted by the length of the dependence
path between them. For instance, [$, tickets, chil-
dren] is the sorted noun list for 1.5 in Figure 1.
Assume the n-th noun of the nouns associated to a
given number is the first noun that appears in the
question sentence. We quantify the relationship
between a number and a queried entity by the re-
ciprocal of n. For instance, in Figure 1, “children”
appears in the question sentence, and it is the third
noun associated to 1.5. So the value of this feature
is 1/3. A larger value of this feature means a num-
ber more likely relates to the queried entity. The
maximum value of this feature is 1. Thus we intro-
duce a feature to indicate whether this special case
occurs. We also use a feature to indicate whether
a number appears in the question sentence.
The comparative meaning is sensitive to both
the comparative words and the position of a num-
ber relative to them. For example, “one number
is 3 less than twice another” is different to “one
number is 3 more than twice another”, but equal to
“twice a number is 3 more than another”. To ac-
count for this, we use the comparative words cou-
pled with the position of a number relative to them
as features.
On the other hand, we use the lemma, part of
speech (POS) tag and the dependence type related
to the word within a widow [-5, +5] around a num-
ber as features. Besides, if the POS tag or the
named entity tag of a number is not labeled as a
general number, we also import these tags together
with the first noun and the dependence type related
to the number as features.
Additionally, the numbers 1 and 2 are usually
used to indicate the number of variables, such as
“the sum of two numbers”. To capture such usage,
we use a feature to denote whether a number is
one or two as (Kushman et al., 2014). Since such
usage appears in various kinds of word problems,
this feature does not contain the slot signature. We
also generate features to indicate whether a num-
ber belongs to (0, 1), and whether it is a multiplier,
such as twice, triple.
</bodyText>
<page confidence="0.99649">
819
</page>
<subsectionHeader confidence="0.97139">
4.2 Slot Pair Features
</subsectionHeader>
<bodyText confidence="0.9668892">
Assume n1 and n2 are two numbers in a word
problem. Suppose NP1 and NP2 are the lists of
nouns associated to n1 and n2 (described in sec-
tion 4.1), respectively. We evaluate the relation-
ship r (n1, n2) between n1 and n2 by:
</bodyText>
<equation confidence="0.658505">
2
ord (noun) + ord (noun2)
</equation>
<bodyText confidence="0.9996734">
where ord(·) denotes the index of a noun
in NPZ (i = 1, 2), starting from 1. A larger
r (n1, n2) means n1 and n2 are more related. The
maximum value of r (n1, n2) is 1, which occurs
when the first nouns of NP1 and NP2 are equal.
We use a feature to indicate whether r (n1, n2) is
1. This feature helps to import some basic rules
of the arithmetic operation, e.g., the units of sum-
mands should be the same.
If two slots are symmetric in a template (e.g.,
n2 and n3 in Figure 1), the contexts around both
numbers are generally similar. Assume CT1 and
CT2 are two sets of certain tags within a window
around n1 and n2, respectively. Then we calculate
the contextual similarity between n1 and n2 by:
</bodyText>
<equation confidence="0.9827275">
sim (ST1, ST2) = |ST1�ST2|
|ST1 U ST2|
</equation>
<bodyText confidence="0.999991928571429">
In this paper, the tags include the lemma, POS tag
and dependence type, and the window size is 5.
Besides, we exploit features to denote whether
there exists coreference relationship between any
elements of the sentences where n1 and n2 locate,
and whether two numbers are both multipliers. Fi-
nally, according to (Kushman et al., 2014), we
generate features related to the raw path and de-
pendence path between two numbers, and use the
numeric relation between them as a feature to im-
port some basic arithmetic rules, such as the posi-
tive summands are smaller than their sum. We also
include features to indicate whether two numbers
are in the same sentence or continuous sentences.
</bodyText>
<subsectionHeader confidence="0.999046">
4.3 Solution Features
</subsectionHeader>
<bodyText confidence="0.999924285714286">
Many word problems are math problems about the
real life. This background leads the solutions of
many word problems have some special numerical
properties, such as the positive and integer prop-
erties used by (Kushman et al., 2014). To capture
such fact, we introduce a set of features to describe
the solution properties.
</bodyText>
<sectionHeader confidence="0.998629" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9984768">
Dataset: The dataset used in our experiment is
provided by (Kushman et al., 2014). Equiva-
lent equation systme templates are automatically
merged. The word problems are parsed by (Man-
ning et al., 2014). The version of the parser is the
same as (Kushman et al., 2014). The performance
of our algorithm is evaluated by comparing each
number of the correct answer with the calculated
one, regardless of the ordering. We report the av-
erage accuracy of 5-fold cross-validation.
Learning: We use liblinear (Fan et al., 2008) to
solve the QP problem. The parameter C in (3)
is set to 0.01 in all the following experiments. We
randomly select 300 false derivations of each word
problem to form the initial training set. We add at
most 300 false derivations for each word problem
during the constraint generation step, and use 5-
fold cross-validation to avoid overfitting. We stop
iterating when the cross-validation error becomes
worse or the training error converges or none new
constraints are generated.
Supervision Level: We consider the learning with
two different levels of supervision. In the first
case, the learning is conducted by providing the
equation and the correct answer of every training
sample. In the second case, the correct answer is
available for every training sample but without the
equation. Instead, all the templates are given, but
the correspondence between the template and the
training sample is not available. During learning,
the algorithm should evaluate every derivation of
each template to find the true one.
Results: Table 2 lists the learning statistics for our
algorithm and (Kushman et al., 2014). We can ob-
serve that the number of possible alignments per
word problem of our algorithm is much smaller
than (Kushman et al., 2014). However, the num-
ber of all the false alignments is still 80K. Us-
ing the constraint generation algorithm (Koller and
Friedman, 2009), only 9K false alignments are
used in the quadratic programming. We trained
our model on a Intel i5-3210M CUP and 4G RAM
laptop. Kushman’s algorithm (2014) needs much
more memory than our algorithm and can not run
on a general laptop. Therefore, we tested their al-
gorithm on a workstation with Intel E5-2620 CPU
and 128G memory. As shown in Table 2, their al-
gorithm takes more time than our algorithm.
Table 3 lists the accuracy of our algorithm and
Kushman’s algorithm (2014). It is clear that our
</bodyText>
<figure confidence="0.992955625">
max
nouni1∈NP1,
nounj2∈NP2
s.t. nouni1=nounj2
⎛
⎝
⎞
⎠
</figure>
<page confidence="0.977873">
820
</page>
<table confidence="0.9995917">
Mean negative samples 80K
Mean negative samples used in learning 9K
Mean time for feature extraction 22m
Mean training time 7.3m
Mean feature extraction and training 83m
time of (Kushman et al., 2014)
# Alignments per problem of (Kushman 4M
et al., 2014)
# Alignments per problem of our algo- 1.9K
rithm
</table>
<tableCaption confidence="0.992195">
Table 2: Learning statistics.
</tableCaption>
<table confidence="0.9993706">
Algorithm Accuracy
Our algorithm fully supervised 79.7%
Our algorithm weakly supervised 72.3%
Kushman’s algorithm (2014) fully 68.7%
supervised
</table>
<tableCaption confidence="0.992421">
Table 3: Algorithm comparison.
</tableCaption>
<table confidence="0.999718">
Feature Ablation Accuracy
Without single slot features 70.4%
Without slot pair features 69.3%
Without solution features 71.8%
</table>
<tableCaption confidence="0.999916">
Table 4: Ablation study for fully supervised data.
</tableCaption>
<bodyText confidence="0.997763707317073">
algorithm obtains better result. The result of the
weakly supervised data is worse than the fully su-
pervised one. But this result is still higher than
Kushman’s fully supervised result.
Table 4 gives the results of our algorithm with
different feature ablations. We can find that all the
features are helpful to get the correct solution and
none of them dramatically surpasses the others.
Discuss: Although our algorithm gives a better re-
sult than (Kushman et al., 2014), there still exist
two main problems that need to be further investi-
gated, as demonstrated in Table 5. The first prob-
lem is caused by our feature for semantic repre-
sentation. Our current lexicalized feature can not
generalize well for the unseen words. For exam-
ple, it is hard for our algorithm to relate the word
“forfeits” to “minus”, if it does not appear in the
training corpus. The second problem is caused by
the fact that our algorithm only considers the sin-
gle noun as the entity of a word problem. Thus
when the entity is a complicated noun phrase, our
algorithm may fail.
Problem Example
Lexicalized A woman is paid 20 dollars for
features can each day she works and forfeits
not gener- a 5 dollars for each day she is
alize well idle. At the end of 25 days she
for unseen nets 450 dollars. How many
words. days did she work?
The probability that San
Francisco plays in the next
super bowl is nine times the
probability that they do not
Can not deal play in the next super bowl.
with compli- The probability that San
cated noun Francisco plays in the next
phrases. super bowl plus the probabil-
ity that they do not play is 1.
What is the probability that
San Francisco plays in the
next super bowl?
</bodyText>
<tableCaption confidence="0.929069">
Table 5: The problems of our algorithm.
</tableCaption>
<sectionHeader confidence="0.98981" genericHeader="conclusions">
6 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.9999818">
In this paper, we present a new algorithm to learn
to solve algebra word problems. To reduce the
possible derivations, we only consider filling the
number slots of the equation system templates,
and design effective features to describe the rela-
tionship between numbers and unknowns. Addi-
tionally, we use the max-margin objective to train
the log-linear model. This results in a QP prob-
lem that can be efficiently solved via the constraint
generation algorithm. Experimental results show
that our algorithm significantly outperforms the
state-of-the-art baseline (Kushman et al., 2014).
Our future work will focus on studying the per-
formance of applying nonlinear kernel function to
the QP problem (3), and using the word embed-
ding vector (Bengio et al., 2003; Mikolov et al.,
2013) to replace current lexicalized features. Be-
sides, we would like to compare our algorithm
with the algorithms designed for specific word
problems, such as (Hosseini et al., 2014).
</bodyText>
<sectionHeader confidence="0.998894" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999611">
This work is supported by the National Basic
Research Program of China (973 program No.
2014CB340505). We would like to thank Hua
Wu and the anonymous reviewers for their helpful
comments that improved the work considerably.
</bodyText>
<page confidence="0.997134">
821
</page>
<sectionHeader confidence="0.981146" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998403583333334">
Bussaba Amnueypornsakul and Suma Bhat. 2014.
Machine-guided solution to mathematical word
problems.
Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(6):1137–1155.
Christopher M Bishop. 2006. Pattern recognition and
machine learning. springer.
Daniel G Bobrow. 1964. Natural language input for a
computer problem solving system.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Pedro F Felzenszwalb, Ross B Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(9):1627–1645.
Mohammad Javad Hosseini, Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 523–533.
Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. MIT
press.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. ACL (1), pages 271–
281.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the 2010 con-
ference on empirical methods in natural language
processing, pages 1223–1233. Association for Com-
putational Linguistics.
Iddo Lev, Bill MacCartney, Christopher D Manning,
and Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In Proceed-
ings of the 2nd Workshop on Text Meaning and In-
terpretation, pages 9–16. Association for Computa-
tional Linguistics.
Hang Li. 2014. Learning to rank for information re-
trieval and natural language processing. Synthesis
Lectures on Human Language Technologies, 7(3):1–
121.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Takuya Matsuzaki, Hidenao Iwane, Hirokazu Anai,
and Noriko Arai. 2013. The complexity of math
problems–linguistic, or computational. In Proceed-
ings of the Sixth International Joint Conference on
Natural Language Processing, pages 73–81.
Tomas Mikolov, Wen Tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space-
word representations. In HLT-NAACL.
Anirban Mukherjee and Utpal Garain. 2008. A review
of methods for automatic understanding of natural
language mathematical problems. Artificial Intelli-
gence Review, 29(2):93–122.
John C. Platt. 1999. Fast training of support vector
machines using sequential minimal optimization. In
B. Scho04lkopf, C. Burges and A. Smola (Eds.), Ad-
vances in kernel methods - Support vector learning.
Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reason-
ing about quantities in natural language. Transac-
tions of the Association for Computational Linguis-
tics, 3:1–13.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceed-
ings of the 22nd international conference on Ma-
chine learning, pages 896–903. ACM.
Vladimir Vapnik. 2013. The nature of statistical learn-
ing theory. Springer Science &amp; Business Media.
</reference>
<page confidence="0.997999">
822
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.120164">
<title confidence="0.928054">Learn to Solve Algebra Word Problems Using Quadratic Programming</title>
<phone confidence="0.594486">2 2</phone>
<author confidence="0.78252">Lipu Zhou</author>
<author confidence="0.78252">Shuaixiang Dai</author>
<author confidence="0.78252">Liwei Chen</author>
<affiliation confidence="0.473912">Baidu Inc., Beijing, China</affiliation>
<abstract confidence="0.970522684210526">This paper presents a new algorithm to automatically solve algebra word problems. Our algorithm solves a word problem via analyzing a hypothesis space containing all possible equation systems generated by assigning the numbers in the word problem into a set of equation system templates extracted from the training data. To obtain a robust decision surface, we train a log-linear model to make the margin between the correct assignments and the false ones as large as possible. This results in a quadratic programming (QP) problem which can be efficiently solved. Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art baseline (Kushman et al., 2014).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bussaba Amnueypornsakul</author>
<author>Suma Bhat</author>
</authors>
<date>2014</date>
<note>Machine-guided solution to mathematical word problems.</note>
<contexts>
<context position="1472" citStr="Amnueypornsakul and Bhat, 2014" startWordPosition="225" endWordPosition="228">out 10% higher than the state-of-the-art baseline (Kushman et al., 2014). 1 Introduction An algebra word problem describes a mathematical problem which can be typically modeled by an equation system, as demonstrated in Figure 1. Seeking to automatically solve word problems is a classical AI problem (Bobrow, 1964). The word problem solver is traditionally created by the rulebased approach (Lev et al., 2004; Mukherjee and Garain, 2008; Matsuzaki et al., 2013). Recently, using machine learning techniques to construct the solver has become a new trend (Kushman et al., 2014; Hosseini et al., 2014; Amnueypornsakul and Bhat, 2014; Roy et al., 2015). This is based on the fact that word problems derived from the same mathematical problem share some common semantic and syntactic features due to the same underlying logic. Our method follows this trend.1 To solve a word problem, our algorithm analyzes all the possible ways to assign the numbers 1Our code is available at http://pan.baidu.com/ s/1dD336Sx Our method Kushman’s method Word An amusement park sells 2 kinds of tickets. Tickets for problem tickets cost $ 4. On a certain day, On that same day the admission How many children were admitted were admitted? children cost</context>
</contexts>
<marker>Amnueypornsakul, Bhat, 2014</marker>
<rawString>Bussaba Amnueypornsakul and Suma Bhat. 2014. Machine-guided solution to mathematical word problems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rjean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<issue>6</issue>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, Rjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3(6):1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern recognition and machine learning.</title>
<date>2006</date>
<publisher>springer.</publisher>
<contexts>
<context position="6743" citStr="Bishop, 2006" startWordPosition="1150" endWordPosition="1151">bility of a correct derivation ycijk ∈ Yi and a false one yf ijl ∈ Yi as: )ln P �yc ijk|xi; θ� − ln P(yf ijl|xi; θ =θ ·(φ (xi, yjk) − (xi, yijl) ) (2) Note that the subtraction in (2) cancels the denominator of (1) which contains extensive computation. To decrease the generalization error of the learned model, we would like the minimal gap between the correct derivations and the false ones as large as possible. In practice, we may not find a decision hyperplane to perfectly separate the correct and the false derivations. Generally, this can be solved by introducing a slack variable ξijkl ≥ 0 (Bishop, 2006) for each constraint derived from (2). Define ϕ (xi,ycijk,yijl) = φ (xi, yic k) − (xi, y mol) . For ∀ xi ∈ X, the resulting optimization problem is: 1� 2kθk2 + C i,j,k,l s.t. θ · ϕ (xi, ycijk, yijl) ≥ 1 − ξijkl, ξijkl ≥ 0 The parameter C is used to balance the slack variable penalty and the margin. This is a QP problem and has been well studied (Platt, 1999; Fan et al., 2008). According to the Karush-Kuhn-Tucker (KKT) condition, only a part of the constraints is active for the solution of (3) (Bishop, 2006). This leads to an efficient learning algorithm called constraint generation (Koller and</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M Bishop. 2006. Pattern recognition and machine learning. springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel G Bobrow</author>
</authors>
<title>Natural language input for a computer problem solving system.</title>
<date>1964</date>
<contexts>
<context position="1156" citStr="Bobrow, 1964" startWordPosition="177" endWordPosition="178">st decision surface, we train a log-linear model to make the margin between the correct assignments and the false ones as large as possible. This results in a quadratic programming (QP) problem which can be efficiently solved. Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art baseline (Kushman et al., 2014). 1 Introduction An algebra word problem describes a mathematical problem which can be typically modeled by an equation system, as demonstrated in Figure 1. Seeking to automatically solve word problems is a classical AI problem (Bobrow, 1964). The word problem solver is traditionally created by the rulebased approach (Lev et al., 2004; Mukherjee and Garain, 2008; Matsuzaki et al., 2013). Recently, using machine learning techniques to construct the solver has become a new trend (Kushman et al., 2014; Hosseini et al., 2014; Amnueypornsakul and Bhat, 2014; Roy et al., 2015). This is based on the fact that word problems derived from the same mathematical problem share some common semantic and syntactic features due to the same underlying logic. Our method follows this trend.1 To solve a word problem, our algorithm analyzes all the pos</context>
</contexts>
<marker>Bobrow, 1964</marker>
<rawString>Daniel G Bobrow. 1964. Natural language input for a computer problem solving system.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="7121" citStr="Fan et al., 2008" startWordPosition="1228" endWordPosition="1231">and the false ones as large as possible. In practice, we may not find a decision hyperplane to perfectly separate the correct and the false derivations. Generally, this can be solved by introducing a slack variable ξijkl ≥ 0 (Bishop, 2006) for each constraint derived from (2). Define ϕ (xi,ycijk,yijl) = φ (xi, yic k) − (xi, y mol) . For ∀ xi ∈ X, the resulting optimization problem is: 1� 2kθk2 + C i,j,k,l s.t. θ · ϕ (xi, ycijk, yijl) ≥ 1 − ξijkl, ξijkl ≥ 0 The parameter C is used to balance the slack variable penalty and the margin. This is a QP problem and has been well studied (Platt, 1999; Fan et al., 2008). According to the Karush-Kuhn-Tucker (KKT) condition, only a part of the constraints is active for the solution of (3) (Bishop, 2006). This leads to an efficient learning algorithm called constraint generation (Koller and Friedman, 2009; Felzenszwalb et al., 2010). Specifically, an initial model is trained by a randomly selected subset of the constraints. Next this model is used to check the constraints and at most N false deviations that are erroneously classified by this model are collected for each word problem. These constraints are then added to train a new model. This process repeats un</context>
<context position="14561" citStr="Fan et al., 2008" startWordPosition="2540" endWordPosition="2543">apture such fact, we introduce a set of features to describe the solution properties. 5 Experiments Dataset: The dataset used in our experiment is provided by (Kushman et al., 2014). Equivalent equation systme templates are automatically merged. The word problems are parsed by (Manning et al., 2014). The version of the parser is the same as (Kushman et al., 2014). The performance of our algorithm is evaluated by comparing each number of the correct answer with the calculated one, regardless of the ordering. We report the average accuracy of 5-fold cross-validation. Learning: We use liblinear (Fan et al., 2008) to solve the QP problem. The parameter C in (3) is set to 0.01 in all the following experiments. We randomly select 300 false derivations of each word problem to form the initial training set. We add at most 300 false derivations for each word problem during the constraint generation step, and use 5- fold cross-validation to avoid overfitting. We stop iterating when the cross-validation error becomes worse or the training error converges or none new constraints are generated. Supervision Level: We consider the learning with two different levels of supervision. In the first case, the learning </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>Ross B Girshick</author>
<author>David McAllester</author>
<author>Deva Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence,</title>
<date>2010</date>
<journal>IEEE Transactions on,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context position="7386" citStr="Felzenszwalb et al., 2010" startWordPosition="1267" endWordPosition="1271">int derived from (2). Define ϕ (xi,ycijk,yijl) = φ (xi, yic k) − (xi, y mol) . For ∀ xi ∈ X, the resulting optimization problem is: 1� 2kθk2 + C i,j,k,l s.t. θ · ϕ (xi, ycijk, yijl) ≥ 1 − ξijkl, ξijkl ≥ 0 The parameter C is used to balance the slack variable penalty and the margin. This is a QP problem and has been well studied (Platt, 1999; Fan et al., 2008). According to the Karush-Kuhn-Tucker (KKT) condition, only a part of the constraints is active for the solution of (3) (Bishop, 2006). This leads to an efficient learning algorithm called constraint generation (Koller and Friedman, 2009; Felzenszwalb et al., 2010). Specifically, an initial model is trained by a randomly selected subset of the constraints. Next this model is used to check the constraints and at most N false deviations that are erroneously classified by this model are collected for each word problem. These constraints are then added to train a new model. This process repeats until converges. Our experimental results show that this process converges fast. 3.2 Inference When we obtain the model parameter θ, the inference can be performed by finding the maximum arg min ξijkl (3) 818 Single slot features Relation between numbers and the ques</context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. 2010. Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627–1645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Javad Hosseini</author>
<author>Hannaneh Hajishirzi</author>
<author>Oren Etzioni</author>
<author>Nate Kushman</author>
</authors>
<title>Learning to solve arithmetic word problems with verb categorization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>523--533</pages>
<contexts>
<context position="1440" citStr="Hosseini et al., 2014" startWordPosition="221" endWordPosition="224">eves 79.7% accuracy, about 10% higher than the state-of-the-art baseline (Kushman et al., 2014). 1 Introduction An algebra word problem describes a mathematical problem which can be typically modeled by an equation system, as demonstrated in Figure 1. Seeking to automatically solve word problems is a classical AI problem (Bobrow, 1964). The word problem solver is traditionally created by the rulebased approach (Lev et al., 2004; Mukherjee and Garain, 2008; Matsuzaki et al., 2013). Recently, using machine learning techniques to construct the solver has become a new trend (Kushman et al., 2014; Hosseini et al., 2014; Amnueypornsakul and Bhat, 2014; Roy et al., 2015). This is based on the fact that word problems derived from the same mathematical problem share some common semantic and syntactic features due to the same underlying logic. Our method follows this trend.1 To solve a word problem, our algorithm analyzes all the possible ways to assign the numbers 1Our code is available at http://pan.baidu.com/ s/1dD336Sx Our method Kushman’s method Word An amusement park sells 2 kinds of tickets. Tickets for problem tickets cost $ 4. On a certain day, On that same day the admission How many children were admit</context>
</contexts>
<marker>Hosseini, Hajishirzi, Etzioni, Kushman, 2014</marker>
<rawString>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Nir Friedman</author>
</authors>
<title>Probabilistic graphical models: principles and techniques.</title>
<date>2009</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="4276" citStr="Koller and Friedman, 2009" startWordPosition="689" endWordPosition="692">ntial to approximate the gradient. For instance, Kushman et al. (2014) use 817 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 817–822, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. beam search to approximately calculate the gradient. This method can not exploit all the training samples. Thus the resulting model may be suboptimal. Motivated by the work (Taskar et al., 2005; Li, 2014), we adopt the max-margin objective. This results in a QP problem and opens the way toward an efficient learning algorithm (Koller and Friedman, 2009). We evaluate our algorithm on the benchmark dataset provided by (Kushman et al., 2014). The experimental results show that our algorithm significantly outperforms the state-of-the-art baseline (Kushman et al., 2014). 2 Problem Formulation Our word problem solver is constructed by training a log-linear model to find the correct mapping from a word problem to an equation. Notations: Let X denote the set of training word problems, and T denote the set of equation system templates abstracted from X as (Kushman et al., 2014). xi is the i-th word problem in X. Assume Tj is the j-th equation system </context>
<context position="7358" citStr="Koller and Friedman, 2009" startWordPosition="1263" endWordPosition="1266">hop, 2006) for each constraint derived from (2). Define ϕ (xi,ycijk,yijl) = φ (xi, yic k) − (xi, y mol) . For ∀ xi ∈ X, the resulting optimization problem is: 1� 2kθk2 + C i,j,k,l s.t. θ · ϕ (xi, ycijk, yijl) ≥ 1 − ξijkl, ξijkl ≥ 0 The parameter C is used to balance the slack variable penalty and the margin. This is a QP problem and has been well studied (Platt, 1999; Fan et al., 2008). According to the Karush-Kuhn-Tucker (KKT) condition, only a part of the constraints is active for the solution of (3) (Bishop, 2006). This leads to an efficient learning algorithm called constraint generation (Koller and Friedman, 2009; Felzenszwalb et al., 2010). Specifically, an initial model is trained by a randomly selected subset of the constraints. Next this model is used to check the constraints and at most N false deviations that are erroneously classified by this model are collected for each word problem. These constraints are then added to train a new model. This process repeats until converges. Our experimental results show that this process converges fast. 3.2 Inference When we obtain the model parameter θ, the inference can be performed by finding the maximum arg min ξijkl (3) 818 Single slot features Relation </context>
<context position="15937" citStr="Koller and Friedman, 2009" startWordPosition="2764" endWordPosition="2767">training sample but without the equation. Instead, all the templates are given, but the correspondence between the template and the training sample is not available. During learning, the algorithm should evaluate every derivation of each template to find the true one. Results: Table 2 lists the learning statistics for our algorithm and (Kushman et al., 2014). We can observe that the number of possible alignments per word problem of our algorithm is much smaller than (Kushman et al., 2014). However, the number of all the false alignments is still 80K. Using the constraint generation algorithm (Koller and Friedman, 2009), only 9K false alignments are used in the quadratic programming. We trained our model on a Intel i5-3210M CUP and 4G RAM laptop. Kushman’s algorithm (2014) needs much more memory than our algorithm and can not run on a general laptop. Therefore, we tested their algorithm on a workstation with Intel E5-2620 CPU and 128G memory. As shown in Table 2, their algorithm takes more time than our algorithm. Table 3 lists the accuracy of our algorithm and Kushman’s algorithm (2014). It is clear that our max nouni1∈NP1, nounj2∈NP2 s.t. nouni1=nounj2 ⎛ ⎝ ⎞ ⎠ 820 Mean negative samples 80K Mean negative sa</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>Daphne Koller and Nir Friedman. 2009. Probabilistic graphical models: principles and techniques. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Kushman</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning to automatically solve algebra word problems.</title>
<date>2014</date>
<journal>ACL</journal>
<volume>1</volume>
<pages>271--281</pages>
<contexts>
<context position="914" citStr="Kushman et al., 2014" startWordPosition="137" endWordPosition="140">lgorithm solves a word problem via analyzing a hypothesis space containing all possible equation systems generated by assigning the numbers in the word problem into a set of equation system templates extracted from the training data. To obtain a robust decision surface, we train a log-linear model to make the margin between the correct assignments and the false ones as large as possible. This results in a quadratic programming (QP) problem which can be efficiently solved. Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art baseline (Kushman et al., 2014). 1 Introduction An algebra word problem describes a mathematical problem which can be typically modeled by an equation system, as demonstrated in Figure 1. Seeking to automatically solve word problems is a classical AI problem (Bobrow, 1964). The word problem solver is traditionally created by the rulebased approach (Lev et al., 2004; Mukherjee and Garain, 2008; Matsuzaki et al., 2013). Recently, using machine learning techniques to construct the solver has become a new trend (Kushman et al., 2014; Hosseini et al., 2014; Amnueypornsakul and Bhat, 2014; Roy et al., 2015). This is based on the </context>
<context position="2446" citStr="Kushman et al., 2014" startWordPosition="398" endWordPosition="401">.baidu.com/ s/1dD336Sx Our method Kushman’s method Word An amusement park sells 2 kinds of tickets. Tickets for problem tickets cost $ 4. On a certain day, On that same day the admission How many children were admitted were admitted? children cost $ 1.50. Adult 278 people entered the park. fees collected totaled $ 792. on that day? How many adults Template U1 u2 nl 0 u1 u2 n1 0 n2 ul n3 u2 N 0 n2 U n3 u2 n4 0 Assignment 2,1.5, 4, 278, 792 2,1.5, 4, 278, 792, nouns n1, n2, n3, n4 n1,nVn�,n4,u1,u2 1 ,uz Possible 5432120 174 543210022520 Assignment Figure 1: Comparison between our algorithm and (Kushman et al., 2014). Nouns are boldfaced. in the word problem to a set of equation system templates. Kushman et al. (2014) also consider filling the equation system templates to generate the candidate equations. But Kushman’s template contains number slots (e.g. n1, n2, n3, n4 in Figure 1) and unknown slots (e.g. ui, u21, u12, u22 in Figure 1). They separately consider assigning nouns into the unknown slots and numbers into the number slots, as demonstrated in Figure 1. As filling the unknown slots is closely related to the number slots assignment, we only consider assigning the number slots, and design effectiv</context>
<context position="3720" citStr="Kushman et al. (2014)" startWordPosition="603" endWordPosition="606">rs and unknowns. This scheme significantly reduces the hypothesis space, as illustrated in Figure 1, which benefits the learning and inference processes. We use a log-linear model to describe the template selection and number assignment. To learn the model parameters of such problem, maximizing the log-likelihood objective is generally adopted (Kwiatkowski et al., 2010; Kushman et al., 2014). The key difficulty of this method is that calculating the gradient of the objective function needs to sum over exponentially many samples. Thus, it is essential to approximate the gradient. For instance, Kushman et al. (2014) use 817 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 817–822, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. beam search to approximately calculate the gradient. This method can not exploit all the training samples. Thus the resulting model may be suboptimal. Motivated by the work (Taskar et al., 2005; Li, 2014), we adopt the max-margin objective. This results in a QP problem and opens the way toward an efficient learning algorithm (Koller and Friedman, 2009). We evaluate our algorithm on the benchmark</context>
<context position="5666" citStr="Kushman et al., 2014" startWordPosition="956" endWordPosition="959">, · · · , nl }, where l represents xi the size of Nxi. Assuming l ≥ m, we further define πijk a sequence of m numbers chosen from Nxi without repetition. Given πijk, we can map Tj to an equation system eijk by filling the number slots NTj of Tj sequently with the numbers in πijk. Solving eijk, we can obtain the corresponding solution sijk. To simplify the notation, we define yijk = (Tj, πijk, eijk, sijk) the k-th derivation give xi and Tj, and let Yi denote the set of all possible yijk given xi and T . Therefore, to correctly solve xi is to find the correct yijk ∈ Yi. Probabilistic Model: As (Kushman et al., 2014), we use the log-linear model to define the probability of yijk ∈ Yi given xi: eθ·φ(xi,yijk) p(yijk|xi; θ) = E eθ·φ(xi,y&apos;ijk) (1) y&apos;ijkEYi where θ is the parameter vector of the model, and φ (xi, yijk) denotes the feature function. We adopt the max-margin objective (Vapnik, 2013) to directly learn the decision boundary for the correct derivations and the false ones. 3 Learning and Inference 3.1 Learning Using (1), we obtain the difference between the log-probability of a correct derivation ycijk ∈ Yi and a false one yf ijl ∈ Yi as: )ln P �yc ijk|xi; θ� − ln P(yf ijl|xi; θ =θ ·(φ (xi, yjk) − (x</context>
<context position="11751" citStr="Kushman et al., 2014" startWordPosition="2039" endWordPosition="2042">ve to them as features. On the other hand, we use the lemma, part of speech (POS) tag and the dependence type related to the word within a widow [-5, +5] around a number as features. Besides, if the POS tag or the named entity tag of a number is not labeled as a general number, we also import these tags together with the first noun and the dependence type related to the number as features. Additionally, the numbers 1 and 2 are usually used to indicate the number of variables, such as “the sum of two numbers”. To capture such usage, we use a feature to denote whether a number is one or two as (Kushman et al., 2014). Since such usage appears in various kinds of word problems, this feature does not contain the slot signature. We also generate features to indicate whether a number belongs to (0, 1), and whether it is a multiplier, such as twice, triple. 819 4.2 Slot Pair Features Assume n1 and n2 are two numbers in a word problem. Suppose NP1 and NP2 are the lists of nouns associated to n1 and n2 (described in section 4.1), respectively. We evaluate the relationship r (n1, n2) between n1 and n2 by: 2 ord (noun) + ord (noun2) where ord(·) denotes the index of a noun in NPZ (i = 1, 2), starting from 1. A lar</context>
<context position="13339" citStr="Kushman et al., 2014" startWordPosition="2336" endWordPosition="2339">.g., n2 and n3 in Figure 1), the contexts around both numbers are generally similar. Assume CT1 and CT2 are two sets of certain tags within a window around n1 and n2, respectively. Then we calculate the contextual similarity between n1 and n2 by: sim (ST1, ST2) = |ST1�ST2| |ST1 U ST2| In this paper, the tags include the lemma, POS tag and dependence type, and the window size is 5. Besides, we exploit features to denote whether there exists coreference relationship between any elements of the sentences where n1 and n2 locate, and whether two numbers are both multipliers. Finally, according to (Kushman et al., 2014), we generate features related to the raw path and dependence path between two numbers, and use the numeric relation between them as a feature to import some basic arithmetic rules, such as the positive summands are smaller than their sum. We also include features to indicate whether two numbers are in the same sentence or continuous sentences. 4.3 Solution Features Many word problems are math problems about the real life. This background leads the solutions of many word problems have some special numerical properties, such as the positive and integer properties used by (Kushman et al., 2014).</context>
<context position="15671" citStr="Kushman et al., 2014" startWordPosition="2718" endWordPosition="2721">sion Level: We consider the learning with two different levels of supervision. In the first case, the learning is conducted by providing the equation and the correct answer of every training sample. In the second case, the correct answer is available for every training sample but without the equation. Instead, all the templates are given, but the correspondence between the template and the training sample is not available. During learning, the algorithm should evaluate every derivation of each template to find the true one. Results: Table 2 lists the learning statistics for our algorithm and (Kushman et al., 2014). We can observe that the number of possible alignments per word problem of our algorithm is much smaller than (Kushman et al., 2014). However, the number of all the false alignments is still 80K. Using the constraint generation algorithm (Koller and Friedman, 2009), only 9K false alignments are used in the quadratic programming. We trained our model on a Intel i5-3210M CUP and 4G RAM laptop. Kushman’s algorithm (2014) needs much more memory than our algorithm and can not run on a general laptop. Therefore, we tested their algorithm on a workstation with Intel E5-2620 CPU and 128G memory. As s</context>
<context position="17651" citStr="Kushman et al., 2014" startWordPosition="3043" endWordPosition="3046">Without single slot features 70.4% Without slot pair features 69.3% Without solution features 71.8% Table 4: Ablation study for fully supervised data. algorithm obtains better result. The result of the weakly supervised data is worse than the fully supervised one. But this result is still higher than Kushman’s fully supervised result. Table 4 gives the results of our algorithm with different feature ablations. We can find that all the features are helpful to get the correct solution and none of them dramatically surpasses the others. Discuss: Although our algorithm gives a better result than (Kushman et al., 2014), there still exist two main problems that need to be further investigated, as demonstrated in Table 5. The first problem is caused by our feature for semantic representation. Our current lexicalized feature can not generalize well for the unseen words. For example, it is hard for our algorithm to relate the word “forfeits” to “minus”, if it does not appear in the training corpus. The second problem is caused by the fact that our algorithm only considers the single noun as the entity of a word problem. Thus when the entity is a complicated noun phrase, our algorithm may fail. Problem Example L</context>
<context position="19514" citStr="Kushman et al., 2014" startWordPosition="3368" endWordPosition="3371">rithm. 6 Conclusion and Future work In this paper, we present a new algorithm to learn to solve algebra word problems. To reduce the possible derivations, we only consider filling the number slots of the equation system templates, and design effective features to describe the relationship between numbers and unknowns. Additionally, we use the max-margin objective to train the log-linear model. This results in a QP problem that can be efficiently solved via the constraint generation algorithm. Experimental results show that our algorithm significantly outperforms the state-of-the-art baseline (Kushman et al., 2014). Our future work will focus on studying the performance of applying nonlinear kernel function to the QP problem (3), and using the word embedding vector (Bengio et al., 2003; Mikolov et al., 2013) to replace current lexicalized features. Besides, we would like to compare our algorithm with the algorithms designed for specific word problems, such as (Hosseini et al., 2014). 7 Acknowledgments This work is supported by the National Basic Research Program of China (973 program No. 2014CB340505). We would like to thank Hua Wu and the anonymous reviewers for their helpful comments that improved the</context>
</contexts>
<marker>Kushman, Artzi, Zettlemoyer, Barzilay, 2014</marker>
<rawString>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. ACL (1), pages 271– 281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic ccg grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 conference on empirical methods in natural language processing,</booktitle>
<pages>1223--1233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3470" citStr="Kwiatkowski et al., 2010" startWordPosition="560" endWordPosition="563">ers into the number slots, as demonstrated in Figure 1. As filling the unknown slots is closely related to the number slots assignment, we only consider assigning the number slots, and design effective features to describe the relationship between numbers and unknowns. This scheme significantly reduces the hypothesis space, as illustrated in Figure 1, which benefits the learning and inference processes. We use a log-linear model to describe the template selection and number assignment. To learn the model parameters of such problem, maximizing the log-likelihood objective is generally adopted (Kwiatkowski et al., 2010; Kushman et al., 2014). The key difficulty of this method is that calculating the gradient of the objective function needs to sum over exponentially many samples. Thus, it is essential to approximate the gradient. For instance, Kushman et al. (2014) use 817 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 817–822, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. beam search to approximately calculate the gradient. This method can not exploit all the training samples. Thus the resulting model may be suboptima</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higherorder unification. In Proceedings of the 2010 conference on empirical methods in natural language processing, pages 1223–1233. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iddo Lev</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
<author>Roger Levy</author>
</authors>
<title>Solving logic puzzles: From robust processing to precise semantics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2nd Workshop on Text Meaning and Interpretation,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1250" citStr="Lev et al., 2004" startWordPosition="191" endWordPosition="194">ignments and the false ones as large as possible. This results in a quadratic programming (QP) problem which can be efficiently solved. Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art baseline (Kushman et al., 2014). 1 Introduction An algebra word problem describes a mathematical problem which can be typically modeled by an equation system, as demonstrated in Figure 1. Seeking to automatically solve word problems is a classical AI problem (Bobrow, 1964). The word problem solver is traditionally created by the rulebased approach (Lev et al., 2004; Mukherjee and Garain, 2008; Matsuzaki et al., 2013). Recently, using machine learning techniques to construct the solver has become a new trend (Kushman et al., 2014; Hosseini et al., 2014; Amnueypornsakul and Bhat, 2014; Roy et al., 2015). This is based on the fact that word problems derived from the same mathematical problem share some common semantic and syntactic features due to the same underlying logic. Our method follows this trend.1 To solve a word problem, our algorithm analyzes all the possible ways to assign the numbers 1Our code is available at http://pan.baidu.com/ s/1dD336Sx Ou</context>
</contexts>
<marker>Lev, MacCartney, Manning, Levy, 2004</marker>
<rawString>Iddo Lev, Bill MacCartney, Christopher D Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. In Proceedings of the 2nd Workshop on Text Meaning and Interpretation, pages 9–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
</authors>
<title>Learning to rank for information retrieval and natural language processing. Synthesis Lectures on Human Language Technologies,</title>
<date>2014</date>
<volume>7</volume>
<issue>3</issue>
<pages>121</pages>
<contexts>
<context position="4126" citStr="Li, 2014" startWordPosition="666" endWordPosition="667">this method is that calculating the gradient of the objective function needs to sum over exponentially many samples. Thus, it is essential to approximate the gradient. For instance, Kushman et al. (2014) use 817 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 817–822, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. beam search to approximately calculate the gradient. This method can not exploit all the training samples. Thus the resulting model may be suboptimal. Motivated by the work (Taskar et al., 2005; Li, 2014), we adopt the max-margin objective. This results in a QP problem and opens the way toward an efficient learning algorithm (Koller and Friedman, 2009). We evaluate our algorithm on the benchmark dataset provided by (Kushman et al., 2014). The experimental results show that our algorithm significantly outperforms the state-of-the-art baseline (Kushman et al., 2014). 2 Problem Formulation Our word problem solver is constructed by training a log-linear model to find the correct mapping from a word problem to an equation. Notations: Let X denote the set of training word problems, and T denote the </context>
</contexts>
<marker>Li, 2014</marker>
<rawString>Hang Li. 2014. Learning to rank for information retrieval and natural language processing. Synthesis Lectures on Human Language Technologies, 7(3):1– 121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="14244" citStr="Manning et al., 2014" startWordPosition="2486" endWordPosition="2490"> two numbers are in the same sentence or continuous sentences. 4.3 Solution Features Many word problems are math problems about the real life. This background leads the solutions of many word problems have some special numerical properties, such as the positive and integer properties used by (Kushman et al., 2014). To capture such fact, we introduce a set of features to describe the solution properties. 5 Experiments Dataset: The dataset used in our experiment is provided by (Kushman et al., 2014). Equivalent equation systme templates are automatically merged. The word problems are parsed by (Manning et al., 2014). The version of the parser is the same as (Kushman et al., 2014). The performance of our algorithm is evaluated by comparing each number of the correct answer with the calculated one, regardless of the ordering. We report the average accuracy of 5-fold cross-validation. Learning: We use liblinear (Fan et al., 2008) to solve the QP problem. The parameter C in (3) is set to 0.01 in all the following experiments. We randomly select 300 false derivations of each word problem to form the initial training set. We add at most 300 false derivations for each word problem during the constraint generati</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
</authors>
<title>Hidenao Iwane, Hirokazu Anai, and Noriko Arai.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>73--81</pages>
<marker>Matsuzaki, 2013</marker>
<rawString>Takuya Matsuzaki, Hidenao Iwane, Hirokazu Anai, and Noriko Arai. 2013. The complexity of math problems–linguistic, or computational. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 73–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen Tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous spaceword representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL.</booktitle>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen Tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous spaceword representations. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anirban Mukherjee</author>
<author>Utpal Garain</author>
</authors>
<title>A review of methods for automatic understanding of natural language mathematical problems.</title>
<date>2008</date>
<journal>Artificial Intelligence Review,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="1278" citStr="Mukherjee and Garain, 2008" startWordPosition="195" endWordPosition="198">alse ones as large as possible. This results in a quadratic programming (QP) problem which can be efficiently solved. Experimental results show that our algorithm achieves 79.7% accuracy, about 10% higher than the state-of-the-art baseline (Kushman et al., 2014). 1 Introduction An algebra word problem describes a mathematical problem which can be typically modeled by an equation system, as demonstrated in Figure 1. Seeking to automatically solve word problems is a classical AI problem (Bobrow, 1964). The word problem solver is traditionally created by the rulebased approach (Lev et al., 2004; Mukherjee and Garain, 2008; Matsuzaki et al., 2013). Recently, using machine learning techniques to construct the solver has become a new trend (Kushman et al., 2014; Hosseini et al., 2014; Amnueypornsakul and Bhat, 2014; Roy et al., 2015). This is based on the fact that word problems derived from the same mathematical problem share some common semantic and syntactic features due to the same underlying logic. Our method follows this trend.1 To solve a word problem, our algorithm analyzes all the possible ways to assign the numbers 1Our code is available at http://pan.baidu.com/ s/1dD336Sx Our method Kushman’s method Wo</context>
</contexts>
<marker>Mukherjee, Garain, 2008</marker>
<rawString>Anirban Mukherjee and Utpal Garain. 2008. A review of methods for automatic understanding of natural language mathematical problems. Artificial Intelligence Review, 29(2):93–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization. In</title>
<date>1999</date>
<contexts>
<context position="7102" citStr="Platt, 1999" startWordPosition="1226" endWordPosition="1227"> derivations and the false ones as large as possible. In practice, we may not find a decision hyperplane to perfectly separate the correct and the false derivations. Generally, this can be solved by introducing a slack variable ξijkl ≥ 0 (Bishop, 2006) for each constraint derived from (2). Define ϕ (xi,ycijk,yijl) = φ (xi, yic k) − (xi, y mol) . For ∀ xi ∈ X, the resulting optimization problem is: 1� 2kθk2 + C i,j,k,l s.t. θ · ϕ (xi, ycijk, yijl) ≥ 1 − ξijkl, ξijkl ≥ 0 The parameter C is used to balance the slack variable penalty and the margin. This is a QP problem and has been well studied (Platt, 1999; Fan et al., 2008). According to the Karush-Kuhn-Tucker (KKT) condition, only a part of the constraints is active for the solution of (3) (Bishop, 2006). This leads to an efficient learning algorithm called constraint generation (Koller and Friedman, 2009; Felzenszwalb et al., 2010). Specifically, an initial model is trained by a randomly selected subset of the constraints. Next this model is used to check the constraints and at most N false deviations that are erroneously classified by this model are collected for each word problem. These constraints are then added to train a new model. This</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Fast training of support vector machines using sequential minimal optimization. In B. Scho04lkopf, C. Burges and A. Smola (Eds.), Advances in kernel methods - Support vector learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhro Roy</author>
<author>Tim Vieira</author>
<author>Dan Roth</author>
</authors>
<title>Reasoning about quantities in natural language.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--1</pages>
<contexts>
<context position="1491" citStr="Roy et al., 2015" startWordPosition="229" endWordPosition="232">-the-art baseline (Kushman et al., 2014). 1 Introduction An algebra word problem describes a mathematical problem which can be typically modeled by an equation system, as demonstrated in Figure 1. Seeking to automatically solve word problems is a classical AI problem (Bobrow, 1964). The word problem solver is traditionally created by the rulebased approach (Lev et al., 2004; Mukherjee and Garain, 2008; Matsuzaki et al., 2013). Recently, using machine learning techniques to construct the solver has become a new trend (Kushman et al., 2014; Hosseini et al., 2014; Amnueypornsakul and Bhat, 2014; Roy et al., 2015). This is based on the fact that word problems derived from the same mathematical problem share some common semantic and syntactic features due to the same underlying logic. Our method follows this trend.1 To solve a word problem, our algorithm analyzes all the possible ways to assign the numbers 1Our code is available at http://pan.baidu.com/ s/1dD336Sx Our method Kushman’s method Word An amusement park sells 2 kinds of tickets. Tickets for problem tickets cost $ 4. On a certain day, On that same day the admission How many children were admitted were admitted? children cost $ 1.50. Adult 278 </context>
</contexts>
<marker>Roy, Vieira, Roth, 2015</marker>
<rawString>Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics, 3:1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Vassil Chatalbashev</author>
<author>Daphne Koller</author>
<author>Carlos Guestrin</author>
</authors>
<title>Learning structured prediction models: A large margin approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>896--903</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4115" citStr="Taskar et al., 2005" startWordPosition="662" endWordPosition="665">he key difficulty of this method is that calculating the gradient of the objective function needs to sum over exponentially many samples. Thus, it is essential to approximate the gradient. For instance, Kushman et al. (2014) use 817 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 817–822, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. beam search to approximately calculate the gradient. This method can not exploit all the training samples. Thus the resulting model may be suboptimal. Motivated by the work (Taskar et al., 2005; Li, 2014), we adopt the max-margin objective. This results in a QP problem and opens the way toward an efficient learning algorithm (Koller and Friedman, 2009). We evaluate our algorithm on the benchmark dataset provided by (Kushman et al., 2014). The experimental results show that our algorithm significantly outperforms the state-of-the-art baseline (Kushman et al., 2014). 2 Problem Formulation Our word problem solver is constructed by training a log-linear model to find the correct mapping from a word problem to an equation. Notations: Let X denote the set of training word problems, and T </context>
</contexts>
<marker>Taskar, Chatalbashev, Koller, Guestrin, 2005</marker>
<rawString>Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. 2005. Learning structured prediction models: A large margin approach. In Proceedings of the 22nd international conference on Machine learning, pages 896–903. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>2013</date>
<publisher>Springer Science &amp; Business Media.</publisher>
<contexts>
<context position="5946" citStr="Vapnik, 2013" startWordPosition="1006" endWordPosition="1007"> eijk, we can obtain the corresponding solution sijk. To simplify the notation, we define yijk = (Tj, πijk, eijk, sijk) the k-th derivation give xi and Tj, and let Yi denote the set of all possible yijk given xi and T . Therefore, to correctly solve xi is to find the correct yijk ∈ Yi. Probabilistic Model: As (Kushman et al., 2014), we use the log-linear model to define the probability of yijk ∈ Yi given xi: eθ·φ(xi,yijk) p(yijk|xi; θ) = E eθ·φ(xi,y&apos;ijk) (1) y&apos;ijkEYi where θ is the parameter vector of the model, and φ (xi, yijk) denotes the feature function. We adopt the max-margin objective (Vapnik, 2013) to directly learn the decision boundary for the correct derivations and the false ones. 3 Learning and Inference 3.1 Learning Using (1), we obtain the difference between the log-probability of a correct derivation ycijk ∈ Yi and a false one yf ijl ∈ Yi as: )ln P �yc ijk|xi; θ� − ln P(yf ijl|xi; θ =θ ·(φ (xi, yjk) − (xi, yijl) ) (2) Note that the subtraction in (2) cancels the denominator of (1) which contains extensive computation. To decrease the generalization error of the learned model, we would like the minimal gap between the correct derivations and the false ones as large as possible. I</context>
</contexts>
<marker>Vapnik, 2013</marker>
<rawString>Vladimir Vapnik. 2013. The nature of statistical learning theory. Springer Science &amp; Business Media.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
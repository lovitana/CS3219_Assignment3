<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000287">
<title confidence="0.99218">
Sarcastic or Not: Word Embeddings to Predict the Literal or Sarcastic
Meaning of Words
</title>
<author confidence="0.983467">
Debanjan Ghosh§ and Weiwei Guot and Smaranda Muresan$
</author>
<affiliation confidence="0.964167666666667">
§School of Communication and Information, Rutgers University, NJ, USA
tDepartment of Computer Science, Columbia University, NY, USA
$Center for Computational Learning Systems, Columbia University, NY, USA
</affiliation>
<email confidence="0.984101">
debanjan.ghosh@rutgers.edu, weiwei@cs.columbia.edu, smara@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993832" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914913043478">
Sarcasm is generally characterized as a
figure of speech that involves the substi-
tution of a literal by a figurative mean-
ing, which is usually the opposite of the
original literal meaning. We re-frame the
sarcasm detection task as a type of word
sense disambiguation problem, where the
sense of a word is either literal or sar-
castic. We call this the Literal/Sarcastic
Sense Disambiguation (LSSD) task. We
address two issues: 1) how to collect a set
of target words that can have either literal
or sarcastic meanings depending on con-
text; and 2) given an utterance and a target
word, how to automatically detect whether
the target word is used in the literal or the
sarcastic sense. For the latter, we investi-
gate several distributional semantics meth-
ods and show that a Support Vector Ma-
chines (SVM) classifier with a modified
kernel using word embeddings achieves a
7-10% F1 improvement over a strong lex-
ical baseline.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999775363636364">
Recognizing sarcasm is important for understand-
ing people’s actual sentiments and beliefs. For
example, failing to recognize the following mes-
sage as being sarcastic “I love that I have to go
back to the emergency room”, will lead a senti-
ment and opinion analysis system to infer that the
author’s sentiment is positive towards the event
of “going to the emergency room”. Current ap-
proaches have framed the sarcasm detection task
as predicting whether a full utterance is sarcastic
or not (Davidov et al., 2010; Gonz´alez-Ib´a˜nez et
al., 2011; Riloff et al., 2013; Liebrecht et al., 2013;
Maynard and Greenwood, 2014).
We propose a re-framing of sarcasm detection
as a type of word sense disambiguation problem:
given an utterance and a target word, identify
whether the sense of the target word is literal or
sarcastic. We call this the Literal/Sarcastic Sense
Disambiguation (LSSD) task. In the above utter-
ance, the word “love” is used in a sarcastic, non-
literal sense (the author’s intended meaning be-
ing most likely the opposite of the original literal
meaning - a negative sentiment, such as “hate”).
Two key challenges need to be addressed: 1) how
to collect a set of target words that can have a lit-
eral or a sarcastic sense, depending on context;
and 2) given an utterance containing a target word,
how can we determine whether the target word is
used in its literal sense (e.g., “I love to take a nice
stroll in the park every morning”), or in a sarcastic
sense (e.g., “I love going to the dentist.”).
To address the first challenge, we need to iden-
tify a set of words from sarcastic utterances, which
have a figurative/sarcastic sense (e.g., “love” in the
utterance “I love going to the dentist”). We pro-
pose a crowdsourcing task where Turkers in Ama-
zon Mechanical Turk (MTurk) platform are given
sarcastic utterances (tweets labeled with #sarcasm
or #sarcastic hashtags) and are asked to re-phrase
those messages so that they convey the author’s in-
tended meaning (“I love going to the dentist” can
be rephrased as “I hate going to the dentist” or
“I don’t like going to the dentist”). 1 Given this
parallel dataset, we use unsupervised alignment
techniques to identify semantically opposite words
(e.g., “love” ↔ “hate”, “brilliant” ↔ “stupid”,
“never” ↔ “always”). The words from these pairs
that appear in the original sarcastic utterances are
then considered as our collection of target words
(e.g., “love”, “brilliant”, “never”) that can have
both a sarcastic and a literal sense depending on
the context (Section 2).
To address the second challenge, we compare
several distributional semantics methods generally
used in word sense disambiguation tasks (Sec-
</bodyText>
<footnote confidence="0.663186">
1utterances and messages are used interchangeably.
</footnote>
<page confidence="0.676132">
1003
</page>
<note confidence="0.996957733333333">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1003–1012,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
Target Sense Utterance
S ... starting off the new year great
!!!!! sick in bed ...
great L ... you don’t need a record label to
have great music ...
Lsent ... i’m in love with this song great
job justin ...
S yay something to be proud of 3rd
poorest in the NATION ...
proud L im filipino with dark brown eye
and forever true and proud ...
Lsent but i’m proud of all the beliebers
AROUND THE WORLD ...
</note>
<tableCaption confidence="0.998207">
Table 1: Examples of Targets and their Senses
</tableCaption>
<bodyText confidence="0.999951708333333">
tion 3). We show that using word embeddings in
a modified SVM kernel achieves the best results
(Section 4). To collect training and test datasets
for each of the target words, we use Twitter mes-
sages that contain those words. For the sarcas-
tic sense (S), we use tweets that contain the target
word and are labeled with the #sarcasm or #sar-
castic hashtags. For the literal sense (L), we col-
lect tweets that contain the target word and are
not labeled with the #sarcastic or #sarcasm hash-
tags. Table 1 shows examples of two targets words
(“great” and “proud”) and their sarcastic sense (S)
and literal sense (L). In addition, for the literal
sense, we also consider a special case, where the
tweets are labeled with either positive or nega-
tive hashtags (e.g., #happy, #sad) as proposed by
Gonzalez et al. (2011). We denote these senti-
ment tweets as Laent (Table 1). Gonzalez et al.
(2011) argue that it is harder to distinguish sar-
castic from non-sarcastic messages where the non-
sarcastic messages contain sentiment. Our results
support this argument (97% F1 measure for the
best result for S vs. L, compared to 84% F1 for
the best result for S vs. Laent; Section 4).2
</bodyText>
<subsectionHeader confidence="0.607929">
2 Collection of Target Words
</subsectionHeader>
<bodyText confidence="0.9993611">
To collect a set of target words that can have either
literal or sarcastic meaning depending on context,
we propose a two step approach: 1) a crowdsourc-
ing task to collect a parallel dataset of sarcastic
utterances and their re-phrasings that convey the
authors’ intended meaning; and 2) unsupervised
alignment techniques to detect semantically oppo-
site words/phrases.
Crowdsourcing Task. Given a sarcastic mes-
sage (SM), Turkers were asked to re-phrase the
</bodyText>
<footnote confidence="0.9988315">
2The datasets used in the experiments is available at
https://github.com/debanjanghosh/sarcasm wsd.
</footnote>
<bodyText confidence="0.9917224">
message so that the new message is likely to ex-
press the author’s intended meaning (IM). Exam-
ples of an original sarcastic message (1) and three
messages generated by the Turkers (2) is given be-
low:
</bodyText>
<listItem confidence="0.991168875">
(1) [SM] I am so happy that I am going
back to the emergency room.
(2) a. [IM1] I don’t like that I have to go to
the emergency room again.
b. [IM2] I am so upset I have to return to
the emergency room.
c. [IM3] I’m so unhappy that I am going
back to the emergency room.
</listItem>
<bodyText confidence="0.978191833333333">
From the above examples, we can see that align-
ing the sarcastic message (SM) to the re-phrasings
containing the author’s intended meaning gener-
ated by the Turkers (IM1, IM2, IM3) will al-
low us to detect that “happy” can be aligned to
“don’t like”, “upset”, and “unhappy”. Based on
this alignment, “happy” will be considered as a
target word for the LSSD task.
We used 1,000 sarcastic messages collected
from Twitter using the #sarcasm and #sarcastic
hashtags. The Turkers were provided with de-
tailed instructions of the task including a defini-
tion of sarcasm, the task description, and multi-
ple examples. In addition, for messages that con-
tain one or more sentences and where sarcasm is
related to only a part of the message, the Turk-
ers were instructed to consider the entire message
in their rephrasing. This emphasis was added to
avoid high asymmetry in the length between the
original sarcastic message and the rephrasing of
the intended meaning. For each original sarcas-
tic message (SM), we asked five Turkers to do the
rephrasing task. Each HIT contains 1 sarcastic
message, and Turkers were paid 5 cents for each
HIT. To ensure a high quality level, only quali-
fied workers were allowed to perform the task (i.e.,
more than 90% approval rate and at least 500 ap-
proved HITs). In this way, we obtained a dataset
of 5,000 SM-IM pairs.
Unsupervised Techniques to Detect Semanti-
cally Opposite Words/Phrases. We use two
methods for unsupervised alignment. First, we
use the co-training algorithm for paraphrase detec-
tion developed by Barzilay and McKeown (2001).
This algorithm is used for two specific reasons.
First, our dataset is similar in nature to the parallel
</bodyText>
<page confidence="0.99434">
1004
</page>
<bodyText confidence="0.999975375">
monolingual dataset used in Barzilay and McK-
eown (2001), and thus lexical and contextual in-
formation from tweets can be used to extract the
candidate targets words for LSSD. For instance,
we can align the [SM] and [IM3] (from the above
examples), where except for the words happy and
unhappy, the majority of the words in the two
messages are anchor words and thus happy and
unhappy can be extracted as paraphrases via co-
training. To model contextual information, such
as part of speech tagging for the co-training algo-
rithm, we used Tweet NLP (Gimpel et al., 2011).
Second, Bannard and Callison-Burch (2005) no-
ticed that the co-training method proposed by-
Barzilay and McKeown (2001) requires identical
bounding substrings and has bias towards single
words while extracting paraphrases. This apparent
limitation, however, is advantageous to us because
we are specifically interested in extracting target
words. Co-training resulted in 367 extracted pairs
of paraphrases.
We also considered a statistical machine transla-
tion (SMT) alignment method - IBM Model 4 with
HMM alignment implemented in Giza++ (Och
and Ney, 2000). We used Moses software(Koehn
et al., 2007) to extract lexical translations by align-
ing the dataset of 5,000 SM-IM pairs. From
the set of 367 extracted paraphrases using Barzi-
lay and McKeown (2001)’s approach, we selected
only those paraphrases where the lexical transla-
tion scores φ (resulted after running Moses) are
≥ 0.8. After filtering via translation scores and
manual inspection, we obtained a set of 80 seman-
tically opposite paraphrases. Given this set of se-
mantically opposite words, the words that appear
in the sarcastic messages were consider our target
words for LSSD (70 target words after lemmatiza-
tion). They range from verbs, such as “love” and
“like”, adjectives, such as “brilliant”, “genius”,
and adverbs, such as “really”.
</bodyText>
<sectionHeader confidence="0.952724" genericHeader="introduction">
3 Literal/Sarcastic Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.998221888888889">
Our Literal/Sarcastic Sense Disambiguation
(LSSD) task is formulated as follows: given a
candidate utterance (i.e., a tweet) that contains a
target word t, identify whether the sense of t is
sarcastic (5) or literal (L). In order to be able to
solve this problem we need training and test data
for each target word that consists of utterances
where the target word is used either in the literal
sense or the sarcastic sense.
</bodyText>
<equation confidence="0.9766567">
love(26802), like(14995), great(14495), good(11624),
really(9825), right(6771), fun(6603), best(6182),
better(5960), glad(5748), yeah(5504), nice(4443),
awesome(4196), excited(4027), always(3807),
happy(3098), cool(2705), amazing(1952), fa-
vorite(1883), perfect(1792), wonderful(1749), won-
der(1476), lovely(1424), super(1390), fantastic(1369),
joy(1176), cute(1007), beautiful(981), sweet(800),
hot(729), proud(703), shocked(645), interested(624),
brilliant(576), genius(481), attractive(449), mature(427)
</equation>
<tableCaption confidence="0.939527">
Table 2: Target words and # of training instances
per sense
</tableCaption>
<subsectionHeader confidence="0.998233">
3.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.999978096774194">
To collect training and test datasets for each of the
target words, we use Twitter messages that contain
those words. For the sarcastic sense (5), we use
tweets that contain the target word and are labeled
with the #sarcasm or #sarcastic hashtag. For the
literal sense (L), we collect tweets that contain the
target word and are not labeled with the #sarcastic
or #sarcasm hashtags. In addition, for the literal
sense we also consider a special case, where the
tweets are labeled with either positive or negative
sentiment hashtags (e.g., #happy, #sad). Thus, we
consider two LSSD tasks: 5 vs. L and 5 vs. Lsent,
and aim to collect a balanced dataset for each tar-
get word.
For the 70 target words (see Section 2), we col-
lected a total of 2,542,249 tweets via Twitter API .
We considered a setup where 80% of data is used
for training, 10% for development, and 10% for
test. We empirically set the number of minimum
training instances for each sense of the target word
to 400 without any upper restriction. This resulted
in 37 target words to be used in the LSSD exper-
iments. Table 2 shows all the target words and
their corresponding number of training instances
for each sense (5 and L/Lsent). The size of train-
ing data ranges from 26,802 for the target word
“love” to 427 for the word “mature”. As we will
see in the results sections, however, the size of
the training data is not always the key factor in
the LSSD task, especially for the methods that use
word embeddings.
</bodyText>
<subsectionHeader confidence="0.999852">
3.2 Learning Approaches
</subsectionHeader>
<bodyText confidence="0.9998506">
We consider two classical approaches used in
word sense disambiguation tasks: 1) distributional
approaches where each sense of a target word is
represented as a context vector derived from the
training data; and 2) classification approaches (5
</bodyText>
<page confidence="0.929546">
1005
</page>
<construct confidence="0.196429">
vs. L; S vs. Lsent) for each target word.
</construct>
<subsectionHeader confidence="0.567702">
3.2.1 Distributional Approaches
</subsectionHeader>
<bodyText confidence="0.999978574468085">
The Distributional Hypothesis in linguistics is de-
rived from the semantic theory of language usage,
i.e., words that are used and occur in the same
contexts tend to purport similar meanings (Harris,
1954). Distributional semantic models (DSMs)
use vectors that represent the contexts (e.g., co-
occurring words) in which target words appear in
a corpus, as proxies for meaning representations.
Geometric techniques such as cosine similarity are
then applied to these vectors to measure the simi-
larity in meaning of corresponding words.
The DSMs are a natural approach to model our
LSSD task. For each target word t we build two
context-vectors that will represent the two senses
of the target word t using the training data: one for
the sarcastic sense S using the sarcastic training
data for t (vs) and one for the literal sense L using
the literal sense training data for t (VI).3 Given
a test message u containing a target word t, we
first represent the target word as a vector eu using
all the context words inside u. To predict whether
t is used in a literal or sarcastic sense in the test
message u we simply apply geometric techniques
(e.g., cosine similarity) between vu and the two
sense vectors vs and el, choosing the one with the
maximum score.
To create the two sense vectors vs and vl for
each of the target words t, we use the posi-
tive pointwise mutual information model (PPMI)
(Church and Hanks, 1990). Based on t’s con-
text words ck in a window of 10 words, we sep-
arately computed PPMI for sarcastic and literal
senses using t’s training data. The size of the con-
text widow used in DSMs is generally between 5
and 10, and in our experiments we used a win-
dow of 10 words since tweets often include mean-
ingful words/tokens at the end of the tweets (e.g.,
interjections, such as “yay”, “ohh”; upper-case
words, such as, “GREAT”; novel hashtags, such
as “#notreally”, “#lolol”; emoticons, such as “:(”).
We sorted the context words based on the PPMI
scores and for each target word t we selected a
maximum of 1,000 context words per sense to ap-
proximate the two senses of the target word (i.e.,
the vectors vs and vl for each target word t consist
of a maximum of 1,000 words). Table 3 shows
some target words and their corresponding con-
</bodyText>
<footnote confidence="0.9721225">
3In the remaining of this section we will only mention L
and not Lsent for clarity and brevity.
</footnote>
<table confidence="0.999641117647059">
Targets Senses Context Vector
love S ignored, being, waking, work, sick,
L #not
Lsent please, follow, ♥, her, :)
happy, family, blessed, cute, birth-
day
fun S work, tomorrow, homework, friday,
L sleep
Lsent hope, join, girl, game, friend
#friends, #family, weekend, amaz-
ing, #christmas
joy S working, snow, waking, studying,
L sick
Lsent yesterday, sweet, special, prayer,
laughter
wishing, warmth, love, christmas,
peace
</table>
<tableCaption confidence="0.999859">
Table 3: Target words and their context words
</tableCaption>
<bodyText confidence="0.998406342857143">
text words that were selected based on high PPMI
scores.
To predict whether t is used in a literal or sar-
castic sense in the test message u we simply apply
the cosine similarity to the vu (vector representa-
tion of the target word t in the test message u) and
the two sense vectors vs and vl of t, choosing the
one with the maximum score. All vector elements
are given by the tf-idf values of the corresponding
words. This approach, denoted as the “PPMI base-
line”, is the baseline for our DSM experiments.
Context Vectors with Word Embedding: The
above method considers that the context vectors
vs and vl of each target word t contain the co-
occurring words selected by their PPMI values.
We enhance the representation of context vectors
to represent each word in the context vector by
its word embedding. We experiment with three
different methods of obtaining word embeddings:
Weighted Textual Matrix Factorization (WTMF)
(Guo and Diab, 2012b); word2vec that imple-
ments the skip-gram and continuous bag-of-words
models (CBOW) of Mikolov et al. (2013a), and
GloVe (Pennington et al., 2014), a log-bilinear re-
gression model based upon global word-word co-
occurrence count in the training corpora.
After removing the tweets that are used as test
sets, we build the three word embedding mod-
els in an unsupervised fashion with the remaining
2,482,763 tweets from our original data collection
(Section 3.1). In each of the three models, each
word w is represented by its d-dimensional vec-
tor w� of real numbers, where d=100 for all of the
embedding algorithms in our experiments. For the
size of the embedding vectors, it is common to use
</bodyText>
<page confidence="0.96413">
1006
</page>
<bodyText confidence="0.999788571428571">
100 or 300 dimensions, with larger dimensions for
larger datasets. Our current dataset is smaller than
the ones used in other applications of word embed-
dings (e.g., Pennington et al. (2014) have used bil-
lion tweets to create word embedding) so we opted
for 100 dimensional vectors. Below are the short
descriptions of the three word embedding models:
</bodyText>
<listItem confidence="0.935828529411765">
• Weighted Textual Matrix Factorization
(WTMF): Low-dimensional vectors have
been used in WSD tasks, since they are
computationally efficient and provide better
generalization than surface words. A dimen-
sion reduction method is Weighted Textual
Matrix Factorization (WTMF) (Guo and
Diab, 2012b), which is designed specifically
for short texts, and has been successfully
applied in WSD tasks (Guo and Diab,
2012a). WTMF models unobserved words,
thus providing more robust embeddings for
short texts such as tweets.
• word2vec Representation: We use both the
Skip-gram model and the Continuous Bag-
of-Words (CBOW) model (Mikolov et al.,
2013a; Mikolov et al., 2013c) as imple-
</listItem>
<bodyText confidence="0.9374725">
mented in the word2vec gensim python li-
brary. 4 Given a window size of n words
around a word w, the skip-gram model pre-
dicts the neighboring words given the current
word. In contrast, the CBOW model predicts
the current word w, given the neighboring
words in the window. We considered a con-
text window of 10 words.
</bodyText>
<listItem confidence="0.830914333333333">
• GloVe Representation: GloVe (Pennington et
al., 2014) is a word embedding model that
is based upon weighted least-square model
trained on global word-word co-occurrence
counts instead of the local context used by
word2vec.
</listItem>
<bodyText confidence="0.999660555555556">
Here, the LSSD task is similar to the baseline:
to predict whether the target word t in the test mes-
sage u is used in a literal or sarcastic sense, we
simply use a similarity measure between the ~vu
(vector representation of the target word t in the
test message u) and the two sense vectors ~vs and
~vl of t, choosing the one with the maximum score.
The difference from the baseline is twofold: First,
all vectors elements are word embeddings (i.e.,
</bodyText>
<footnote confidence="0.82019">
4https://radimrehurek.com/gensim/models/word2vec.html
</footnote>
<bodyText confidence="0.932754583333333">
100-d vectors). Second, we use the maximum-
valued matrix-element (MVME) algorithm intro-
duced by Islam and Inkpen (2008), which has been
shown to be particularly useful for computing the
similarity of short texts. We modify this algorithm
to use word embeddings (MV MEwe). The idea
behind the MVME algorithm is that it finds a one-
to-one “word alignment” between two utterances
(i.e., sentences) based on the pairwise word sim-
ilarity. Only the aligned words contribute to the
overall similarity score.
Algorithm 1 MV MEwe
</bodyText>
<listItem confidence="0.993330064516129">
1: procedure MV MEwe(vs,vu)
2: vswords ← vs.elements()
3: vuwords ←vu.elements()
4: M[vswords.size(),vuwords.size()] ← 0
5: for k ← 0, vswords.size() do
6: ck ← vswords[k]
7: ~ck ← getEmbedding(ck)
8: for j ← 0, vuwords.size() do
9: wj ← vuwords[j]
10: ~wj ← getEmbedding(wj)
11: M[k][j] ← cosine(~ck, ~wj)
12: end for
13: end for
14: while True do
15: repeat
16: max ← getMax(M)
17: Sim ← Sim + max
18: rm, cm ← getRowCol(M, max)
19: . Remove rm row and cm column from M
20: remove(M, rm, cm)
21: until max &gt; 0 Or M.size() &gt; 0
22: end while
23: Return Sim
24: end procedure
26: procedure GETEMBEDDING(word)
27: Return wemodel[word]
28: end procedure
29: procedure GETROWCOL(M,max)
30: row, col ← M.indexOf(max)
31: Return row, col
32: end procedure
</listItem>
<bodyText confidence="0.999617333333333">
Algorithm 1 presents the pseudocode of
our modified algorithm for word embeddings,
MV MEwe. Let the total similarity between ~vs
and ~vu be Sim. For each context word ck from
~vs and each word wj from ~vu, we compute a ma-
trix where the value of the matrix element Mjk
</bodyText>
<page confidence="0.988244">
1007
</page>
<bodyText confidence="0.999852454545454">
denotes the cosine similarity between the embed-
ded vectors ~ck and ~wj [lines 5 -13]. Next, we first
select the matrix cell that has the highest similarity
value in M (max) and add this to the Sim score
[lines 16-17]. Let the rm and cm be the row and
the column of the cell containing max (maximum-
valued matrix element), respectively. Next, we re-
move all the matrix elements of the rm-th row and
the cm-th column from M [line 20]. We repeat
this procedure until we have traversed through all
the rows and columns of M or max = 0 [line 21].
</bodyText>
<subsectionHeader confidence="0.636362">
3.2.2 Classification Approaches
</subsectionHeader>
<bodyText confidence="0.999870833333333">
The second approach for our LSSD task is to treat
it as a binary classification task to identify the sar-
castic or literal sense of a target word t. We have
two classification tasks: S vs. L and S vs. Lsent
for each of the 37 target words. We use the lib-
SVM toolkit (Chang and Lin, 2011). Development
data is used for tuning parameters.
SVM Baseline: The SVM baseline for LSSD
tasks uses n-grams and lexicon-based binary-
valued features that are commonly used in exist-
ing state-of-the-art sarcasm detection approaches
(Gonz´alez-Ib´a˜nez et al., 2011; Tchokni et al.,
2014). They are derived from i) bag-of-words
(BoW) representations of words, ii) LIWC dic-
tionary (Pennebaker et al., 2001), and iii) a list
of interjections (e.g., “ah”, “oh”, “yeah”), punc-
tuations (e.g., “!”, “?”), and emoticons collected
from Wikipedia. CMU Tweet Tokenizer is em-
ployed for tokenization. 5 We kept unigrams
unchanged when all the characters are upper-
case (e.g., “NEVER” in “A shooting in Oakland?
That NEVER happens! #sarcasm”) but otherwise
words are converted to lower case. We also change
all numbers to a generic number token “22”. To
avoid any bias during experiments, we removed
the target words from the tweets as well as any
hashtag used to determine the sense of the tweet
(e.g., #sarcasm, #sarcastic, #happy, #sad).
SVM with MV MEwe Kernel: We propose a
new kernel kernelwe to compute the semantic
similarity between two tweets ur and us using the
MV MEwe method introduced for the DSM ap-
proach, and the three types of word embeddings
(WTMF, word2vec, and GloVe). The similarity
measure in the kernel is similar to the algorithm
MV MEwe described in Algorithm 1, but instead
</bodyText>
<footnote confidence="0.836108">
5http://www.ark.cs.cmu.edu/TweetNLP/
</footnote>
<bodyText confidence="0.999979090909091">
of measuring the similarity between the sense vec-
tors of t (~vs, ~vl) and the vector representation of t
in test message (~vu), now we measure the similar-
ity between two tweets ur and us. For each k-th
index word wk in ur and l-th index word wl in
us we compute the cosine similarity between the
embedded vectors of the words and fill up a sim-
ilarity matrix M. We select the matrix cell that
has the highest similarity, add this similarity score
to the total similarity Sim, remove the row and
column from M that has highest similarity score,
and repeat the procedure (similar to Algorithm 1).
We noticed that MV MEwe algorithm carefully
chooses the best candidate word wl in us for the
wk word in ur since wl is the most similar word to
wk. The algorithm continues the same procedure
for all the remaining words in ur and us. The fi-
nal Sim is used as the kernel similarity between
ur and us. We augment this kernel kernelwe into
libSVM and during evaluation we run supervised
LSSD classification for each target word t sepa-
rately.
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="background">
4 Results and Discussions
</sectionHeader>
<bodyText confidence="0.998191884615385">
Tables 4 and 5 show the results for the LSSD
experiments using distributional approaches and
classification-based approaches, respectively. For
brevity, we only report the average Precision (P),
Recall (R), and F1 scores with their standard
deviation (SD) (given by ‘±’), and the targets
with maximum/minimum F1 scores. w2vsg and
w2vcbow represent the skip-gram and CBOW mod-
els implemented in word2vec, respectively.
Table 4 presents the results of distributional
approaches (Section 3.2.1). We observe that
the word embedding methods have better perfor-
mance than the PPMI baseline for both S vs. L
and S vs. Lsent disambiguation tasks. Also,
the average P/R/F1 scores for S vs. L are much
higher than for S vs. Lsent. Since all tweets with
Lsent sense were collected using sentiment hash-
tags (Gonz´alez-Ib´a˜nez et al., 2011), they might be
lexically more similar to the S tweets than the L
tweets are and thus identifying the sense of a tar-
get word t between S vs. Lsent is a harder task. In
Table 4 we also observe that the average F1 scores
between WTMF, w2vsg, w2vcbow, and GloVe are
comparable and between 84%-86%, with w2vsg
and w2vcbow achieving slightly higher F1.
Table 5 outlines the LSSD experiments us-
</bodyText>
<page confidence="0.943205">
1008
</page>
<table confidence="0.999804714285714">
Expr. Senses Avg. P Avg. R Avg. F1 Max. F1(Target) Min. F1(Target)
S 73.5 f 3.6 84.6 f 6.0 78.5 f 3.2 83.9(mature) 68.8(wonder)
L 83.1 f 5.0 70.6 f 5.5 76.1 f 3.4 82.7(love) 68.3(nice)
PPMIbl S 67.8 f 7.0 76.2 f 13.6 70.4 f 7.6 81.8(joy) 43.8(like)
Lsent 74.2 f 7.1 62.7 f 12.8 66.9 f 6.6 78.6(joy) 47.1(interested)
S 83.0 f 3.4 87.2 f 5.4 84.9 f 2.4 91.4(mature) 78.7(wonder)
L 87.5 f 4.4 82.7 f 4.5 84.9 f 2.2 90.5(mature) 80.6(nice)
WTMF S 67.4 f 5.5 86.5 f 5.1 75.6 f 3.9 84.4(joy) 65.8(interested)
Lsent 82.1 f 5.8 58.9 f 9.7 68.1 f 7.2 81.5(joy) 50.0(genius)
S 83.7 f 3.6 85.6 f 5.6 84.5 f 2.8 90.6(joy) 78.8(sweet)
L 86.3 f 4.6 84.0 f 4.3 85.0 f 2.5 89.6(joy) 79.2(like)
GloVe S 70.7 f 5.1 84.3 f 5.0 76.8 f 3.9 85.4(joy) 67.1(interested)
Lsent 80.7 f 5.4 64.7 f 8.5 71.5 f 6.1 84.0(joy) 54.7(hot)
S 84.9 f 3.3 87.0 f 4.8 85.8 f 2.6 90.9(mature) 80.7(like)
L 87.5 f 4.1 85.1 f 4.0 86.2 f 2.5 90.7(mature) 80.2(like)
w2vsg S 70.8 f 4.8 85.7 f 5.1 77.4 f 4.0 86.7(joy) 68.1(interested)
Lsent 82.2 f 5.7 64.3 f 7.8 71.9 f 5.9 85.4(joy) 57.4(interested)
S 84.9 f 3.2 86.7 f 4.7 85.6 f 2.5 90.9(mature) 80.7(like)
L 87.3 f 4.0 85.1 f 3.8 86.1 f 2.4 90.7(mature) 80.2(like)
w2vcbow S 70.7 f 4.8 85.8 f 5.0 77.4 f 4.0 86.4(joy) 68.6(attractive)
Lsent 82.0 f 5.6 64.0 f 7.7 71.7 f 5.8 85.0(joy) 58.7(interested)
</table>
<tableCaption confidence="0.99825">
Table 4: Evaluation of distributional approaches (PMI and word embedding) for LSSD experiments
</tableCaption>
<table confidence="0.999815666666667">
Expr. Senses Avg. P Avg. R Avg. F1 Max. F1(Target) Min. F1(Target)
S 87.0 f 3.3 85.6 f 3.1 86.3 f 2.7 91.7(yeah) 75.4(sweet)
L 85.9 f 2.8 87.1 f 3.6 86.5 f 2.8 91.8(yeah) 76.1(sweet)
SV Mbl S 77.3 f 4.6 78.2 f 4.2 77.7 f 3.8 85.5(love) 68.6(brilliant)
Lsent 77.8 f 3.7 76.7 f 6.4 77.1 f 4.7 85.8(love) 64.6(attractive)
S 94.1 f 2.2 94.6 f 1.8 94.3 f 1.8 97.3(brilliant) 88.3(joy)
L 94.6 f 1.8 94.0 f 2.3 94.3 f 1.9 97.2(mature) 87.9(joy)
kernelWTMF S 79.0 f 4.6 78.8 f 4.4 78.8 f 3.8 84.8(mature) 61.0(genius)
Lsent 78.8 f 3.7 78.9 f 4.9 78.8 f 3.6 85.4(mature) 63.5(genius)
S 95.7 f 1.6 97.4 f 1.7 96.5 f 1.1 99.1(mature) 92.9(glad)
L 97.4 f 1.6 95.6 f 1.7 96.5 f 1.2 99.1(mature) 92.7(interested)
kernelGloV e S 79.5 f 3.5 83.1 f 3.0 81.2 f 2.8 86.9(joy) 74.2(attractive)
Lsent 82.2 f 3.0 78.3 f 4.4 80.2 f 3.4 86.6(joy) 69.2(attractive)
S 96.6 f 1.1 98.5 f 0.6 97.5 f 0.4 99.2(cute) 93.8(interested)
L 98.5 f 0.7 96.5 f 1.2 97.5 f 0.5 99.2(cute) 93.5(interested)
kernelw2vgg S 81.9 f 3.8 88.1 f 3.2 84.8 f 3.0 88.8(love) 74.2(genius)
Lsent 87.0 f 3.2 80.2 f 4.7 83.4 f 3.5 88.8(love) 73.3(genius)
S 96.4 f 1.0 98.2 f 1.1 97.3 f 0.6 99.1(mature) 93.8(interested)
L 98.2 f 1.1 96.3 f 1.1 97.2 f 0.7 99.1(mature) 93.5(interested)
kernelw2vcbow S 81.7 f 3.8 88.6 f 2.9 84.9 f 2.8 89.5(love) 74.8(genius)
Lsent 87.4 f 2.9 79.9 f 4.8 83.4 f 3.4 89.2(love) 74.4(genius)
</table>
<tableCaption confidence="0.999839">
Table 5: Evaluation of classification approaches (SVMbl and kernelwe) for LSSD experiments
</tableCaption>
<bodyText confidence="0.999919466666667">
ing the classification approaches (Section 3.2.2):
SVM baseline (SV Mbl) and SVM using the
kernelwe with word embeddings (kernelWTMF,
kernelGloVe, kernelw2vsg, and kernelw2vcbow).
The classification approaches give better perfor-
mance compared to the distributional approaches.
The SVMbl is around 7-8 % higher than the
PPMIbl and comparable with the word embed-
dings used in distributional approaches (Table 4).
In addition, our new SVM kernel method using
word embeddings shows significantly better re-
sults when compared to the SV Mbl (and distri-
butional approaches). For instance, for the S vs.
L task, the average F1 is 96-97%, which is more
than 10% higher than SVMbl. Similarly, for S
vs. Lsent task, F1 scores reported by the kernel
using word2vec embeddings are in the range of
83%-84% compared to 77% given by the SVMbl,
showing an absolute increase of 7%. As stated ear-
lier, MVME algorithm aligns similar word pairs
found in its inputs and this performs well for short
texts (i.e., tweets). Thus, the MVME algorithm
combined with word embedding in kernelwe re-
sults in very high F1. Among the word embedding
models, word2vec models give marginally better
results compared to GloVe and WTMF, and GloVe
outperforms marginally WTMF. Similar to Table
4, here, the average F1 scores for S vs. L task are
higher than the S vs. Lsent results.
In terms of the best and worst performing tar-
</bodyText>
<page confidence="0.991977">
1009
</page>
<bodyText confidence="0.999971222222222">
gets, 5VMbl prefers targets with more training
data (e.g., “yeah”, “love” vs. “sweet”, “attractive”;
see Table 2). In contrast, word embedding mod-
els for “joy” and “mature”, two targets with com-
paratively low number of training instances have
achieved very high F1 using both distributional
and classification approaches (Table 4 and 5). This
can be explained by the fact that for words, such as
“joy”, “mature”, “cute”, and “brilliant”, the con-
texts of their literal and sarcastic sense are quite
different, and DSMs and word embeddings are
able to capture the difference. For example, ob-
serve in the Table 3, negative sentiment words,
i.e., “sick”, “working”, “snow” are the context
words for targets “joy” and “love”, where as posi-
tive sentiment words, such as, “blessed”, “family”,
“christmas”, and “peace” are the context words for
L or Lsent senses. Overall, out of 37 targets, only
5 targets (“mature”, “joy”, “cute”, “love”, and
“yeah”) achieved “maximum” F1 scores in vari-
ous experimental settings (Tables 4 and 5) whereas
targets such as “interested”, “genius”, and “attrac-
tive” achieved low F1 scores.
In terms of variance in results, SVM results
show low SD (0-4%). For distributional ap-
proaches, SD is slightly higher (5-8%) for several
cases.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99944164">
Two lines of research are directly relevant to our
work: sarcasm detection in Twitter and applica-
tion of distributional semantics, such as word em-
bedding techniques to various NLP tasks. In con-
trast to current research on sarcasm and irony de-
tection (Davidov et al., 2010; Riloff et al., 2013;
Liebrecht et al., 2013; Maynard and Greenwood,
2014), we have introduced a reframing of this task
as a type of word sense disambiguation problem,
where the sense of a word is sarcastic or literal.
Our SVM baseline uses the lexical features pro-
posed in previous research on sarcasm detection
(e.g., LIWC lexicon, interjections, pragmatic fea-
tures) (Liebrecht et al., 2013; Gonz´alez-Ib´a˜nez et
al., 2011; Reyes et al., 2013). Our analysis of tar-
get words where the sarcastic sense is the opposite
of the literal sense is related to the idea of “pos-
itive sentiment toward a negative situation” pro-
posed by Riloff et al. (2013) and recently studied
by Joshi et al. (2015). In our approach, we chose
distributional semantic approaches that learn con-
textual information of targets effectively from a
large corpus containing both literal and sarcastic
uses of words and show that word embedding are
highly accurate in predicting the sarcastic or lit-
eral sense of a word (Tables 4 and 5). This ap-
proach has the potential to capture more nuanced
cases of sarcasm, beyond “positive sentiment to-
wards a negative situation” (e.g., one of our target
words was “shocked” which is negative). How-
ever, our current framing is still inherently limited
to cases where sarcasm is characterized as a figure
of speech where the author means the opposite of
what she says, due to our approach of selecting the
target words.
Low-dimensional text representation, such as
WTMF, have been successful in WSD disam-
biguation research and in computing similarity be-
tween short texts (Guo and Diab, 2012a; Guo and
Diab, 2012b). word2vec and GloVe representa-
tions have provided state-of-the-art results on var-
ious word similarity and analogy detection task
(Mikolov et al., 2013c; Mikolov et al., 2013b;
Pennington et al., 2014). Word embedding based
models are also used for other NLP tasks such as
dependency parsing, semantic role labeling, POS
tagging, NER, question-answering (Bansal et al.,
2014; Collobert et al., 2011; Weston et al., 2015)
and our work on LSSD is a novel application of
word embeddings.
</bodyText>
<sectionHeader confidence="0.996417" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999981952380952">
We proposed a reframing of the sarcasm detec-
tion task as a type of word sense disambiguation
problem, where the sense of a word is its sarcas-
tic or literal sense. Using a crowdsourcing exper-
iment and unsupervised methods for detecting se-
mantically opposite phrases, we collected a set of
target words to be used in the LSSD task. We
compared several distributional semantics meth-
ods, and showed that using word embeddings in
a modified SVM kernel achieves the best results
(an increase of 10% F1 and 8% F1 for 5 vs. L
and 5 vs. Lsent disambiguation task, respectively,
against a SVM baseline). While the SVM base-
line preferred larger amounts of training data (best
performance achieved on the targets words with
higher number of training examples), the methods
using word embeddings seem to perform well on
target words where there might be an inherent dif-
ference in the contextual sarcastic and literal use of
a target word, even if the training data was smaller.
We want to investigate further the nature and
</bodyText>
<page confidence="0.974051">
1010
</page>
<bodyText confidence="0.999963127659575">
size of training data useful for the LSSD task.
For example, to test the effect of larger training
dataset, we utilized pre-trained word vectors from
GloVe (trained with 2 Billion tweets, using 100 di-
mensions).6 For S vs. L disambiguation, the av-
erage F1 was 88.9%, which is 7% lower than the
result using GloVe on our training set of tweets
(much smaller) designed for the LSSD task. This
shows the training data utilized to create word em-
bedding models in GloVe probably do not contain
enough sarcastic tweets.
Regarding the size of the training data, recall
that the unsupervised alignment approach had ex-
tracted 70 target words (Section 2), although we
have used 37 target words as we did not have
enough training data for the remaining targets.
Thus, we plan to collect more training data for
these targets as well as more target words (espe-
cially for the S vs. Lsent task). In addition, we
plan to improve our unsupervised methods for de-
tecting semantically opposite meaning (e.g., us-
ing the IM-IM dataset in addition to the SM-IM
dataset).
One common criticism of research based on use
of hashtags as gold labels is that the training ut-
terances could be noisy. In other words, tweets
might be sarcastic but not have #sarcasm or #sar-
castic hashtags. We did a small manual validation
on a dataset of 180 tweets from the Lsent class us-
ing 3 annotators (we asked them to say whether
the tweet is sarcastic or not). For cases where all
3 coders agree none of them were considered sar-
castic, while when only 2 coders agree 1 tweet out
of 180 was considered sarcastic. In future, we plan
to perform additional experiments to study the is-
sue of noisy data. We hope that the release of our
datasets will stimulate other studies related to the
sarcasm detection problem, including addressing
the issue of noisy data.
We also plan to study the effect of hyper-
parameters in designing the DSMs. Recently,
Levy et al. (2015) have argued that parameter set-
tings have a large impact on the success of word
embedding models. We want to follow their ex-
periments to study whether parameter tuning in
PMI based disambiguation can improve its perfor-
mance.
</bodyText>
<footnote confidence="0.983435">
6Downloaded from http://nlp.stanford.edu/projects/glove/
</footnote>
<sectionHeader confidence="0.987167" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999909">
This paper is based on work supported by the
DARPA-DEFT program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. The authors thank the
anonymous reviewers for helpful comments.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999388465116279">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 597–604. As-
sociation for Computational Linguistics.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Regina Barzilay and Kathleen R McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, pages 50–57. Asso-
ciation for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ’10.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42–47. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.798187">
1011
</page>
<reference confidence="0.999846168421053">
Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: A closer look. In ACL (Short Papers), pages
581–586. Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2012a. Learning the
latent semantics of a concept from its definition.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2, pages 140–144. Association for
Computational Linguistics.
Weiwei Guo and Mona Diab. 2012b. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
864–872. Association for Computational Linguis-
tics.
Zellig S Harris. 1954. Distributional structure. Word,
10:146–162.
Aminul Islam and Diana Inkpen. 2008. Semantic text
similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Aditya Joshi, Vinita Sharma, and Pushpak Bhat-
tacharyya. 2015. Harnessing context incongruity
for sarcasm detection. In Proceedings of the 53rd
Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), pages 757–762, Beijing,
China, July. Association for Computational Linguis-
tics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.
CC Liebrecht, FA Kunneman, and APJ van den Bosch.
2013. The perfect solution for detecting sarcasm in
tweets# not.
Diana Maynard and Mark A Greenwood. 2014. Who
cares about sarcastic tweets? investigating the im-
pact of sarcasm on sentiment analysis. In Proceed-
ings of LREC.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013c. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates, 71:2001.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.
Antonio Reyes, Paolo Rosso, and Tony Veale. 2013.
A multidimensional approach for detecting irony
in twitter. Language resources and evaluation,
47(1):239–268.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive senti-
ment and negative situation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 704–714. Association for
Computational Linguistics.
Simo Tchokni, Diarmuid O S´eaghdha, and Daniele
Quercia. 2014. Emoticons and phrases: Status sym-
bols in social media. In Eighth International AAAI
Conference on Weblogs and Social Media.
Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete ques-
tion answering: A set of prerequisite toy tasks.
arXiv preprint arXiv:1502.05698.
</reference>
<page confidence="0.994878">
1012
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.320387">
<title confidence="0.85485">Sarcastic or Not: Word Embeddings to Predict the Literal or Meaning of Words</title>
<affiliation confidence="0.734934">of Communication and Information, Rutgers University, NJ, of Computer Science, Columbia University, NY,</affiliation>
<address confidence="0.991735">for Computational Learning Systems, Columbia University, NY, USA</address>
<email confidence="0.99992">debanjan.ghosh@rutgers.edu,weiwei@cs.columbia.edu,smara@ccls.columbia.edu</email>
<abstract confidence="0.996266166666667">Sarcasm is generally characterized as a figure of speech that involves the substitution of a literal by a figurative meanwhich is usually the the original literal meaning. We re-frame the sarcasm detection task as a type of word sense disambiguation problem, where the a word is either sar- We call this the Literal/Sarcastic Sense Disambiguation (LSSD) task. We address two issues: 1) how to collect a set of target words that can have either literal or sarcastic meanings depending on context; and 2) given an utterance and a target word, how to automatically detect whether the target word is used in the literal or the sarcastic sense. For the latter, we investigate several distributional semantics methods and show that a Support Vector Machines (SVM) classifier with a modified kernel using word embeddings achieves a 7-10% F1 improvement over a strong lexical baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>597--604</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9242" citStr="Bannard and Callison-Burch (2005)" startWordPosition="1542" endWordPosition="1545">ure to the parallel 1004 monolingual dataset used in Barzilay and McKeown (2001), and thus lexical and contextual information from tweets can be used to extract the candidate targets words for LSSD. For instance, we can align the [SM] and [IM3] (from the above examples), where except for the words happy and unhappy, the majority of the words in the two messages are anchor words and thus happy and unhappy can be extracted as paraphrases via cotraining. To model contextual information, such as part of speech tagging for the co-training algorithm, we used Tweet NLP (Gimpel et al., 2011). Second, Bannard and Callison-Burch (2005) noticed that the co-training method proposed byBarzilay and McKeown (2001) requires identical bounding substrings and has bias towards single words while extracting paraphrases. This apparent limitation, however, is advantageous to us because we are specifically interested in extracting target words. Co-training resulted in 367 extracted pairs of paraphrases. We also considered a statistical machine translation (SMT) alignment method - IBM Model 4 with HMM alignment implemented in Giza++ (Och and Ney, 2000). We used Moses software(Koehn et al., 2007) to extract lexical translations by alignin</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 597–604. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33779" citStr="Bansal et al., 2014" startWordPosition="5689" endWordPosition="5692"> due to our approach of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is its sarcastic or literal sense. Using a crowdsourcing experiment and unsupervised methods for detecting semantically opposite phrases, we collected a set of target words to be used in the LSSD task. We compared several distributional semantics methods, and showed that using word embeddings in a modified SVM kernel achieves the </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>50--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8522" citStr="Barzilay and McKeown (2001)" startWordPosition="1420" endWordPosition="1423">ntended meaning. For each original sarcastic message (SM), we asked five Turkers to do the rephrasing task. Each HIT contains 1 sarcastic message, and Turkers were paid 5 cents for each HIT. To ensure a high quality level, only qualified workers were allowed to perform the task (i.e., more than 90% approval rate and at least 500 approved HITs). In this way, we obtained a dataset of 5,000 SM-IM pairs. Unsupervised Techniques to Detect Semantically Opposite Words/Phrases. We use two methods for unsupervised alignment. First, we use the co-training algorithm for paraphrase detection developed by Barzilay and McKeown (2001). This algorithm is used for two specific reasons. First, our dataset is similar in nature to the parallel 1004 monolingual dataset used in Barzilay and McKeown (2001), and thus lexical and contextual information from tweets can be used to extract the candidate targets words for LSSD. For instance, we can align the [SM] and [IM3] (from the above examples), where except for the words happy and unhappy, the majority of the words in the two messages are anchor words and thus happy and unhappy can be extracted as paraphrases via cotraining. To model contextual information, such as part of speech t</context>
<context position="9953" citStr="Barzilay and McKeown (2001)" startWordPosition="1649" endWordPosition="1653"> identical bounding substrings and has bias towards single words while extracting paraphrases. This apparent limitation, however, is advantageous to us because we are specifically interested in extracting target words. Co-training resulted in 367 extracted pairs of paraphrases. We also considered a statistical machine translation (SMT) alignment method - IBM Model 4 with HMM alignment implemented in Giza++ (Och and Ney, 2000). We used Moses software(Koehn et al., 2007) to extract lexical translations by aligning the dataset of 5,000 SM-IM pairs. From the set of 367 extracted paraphrases using Barzilay and McKeown (2001)’s approach, we selected only those paraphrases where the lexical translation scores φ (resulted after running Moses) are ≥ 0.8. After filtering via translation scores and manual inspection, we obtained a set of 80 semantically opposite paraphrases. Given this set of semantically opposite words, the words that appear in the sarcastic messages were consider our target words for LSSD (70 target words after lemmatization). They range from verbs, such as “love” and “like”, adjectives, such as “brilliant”, “genius”, and adverbs, such as “really”. 3 Literal/Sarcastic Sense Disambiguation Our Literal</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 50–57. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="22290" citStr="Chang and Lin, 2011" startWordPosition="3721" endWordPosition="3724"> the row and the column of the cell containing max (maximumvalued matrix element), respectively. Next, we remove all the matrix elements of the rm-th row and the cm-th column from M [line 20]. We repeat this procedure until we have traversed through all the rows and columns of M or max = 0 [line 21]. 3.2.2 Classification Approaches The second approach for our LSSD task is to treat it as a binary classification task to identify the sarcastic or literal sense of a target word t. We have two classification tasks: S vs. L and S vs. Lsent for each of the 37 target words. We use the libSVM toolkit (Chang and Lin, 2011). Development data is used for tuning parameters. SVM Baseline: The SVM baseline for LSSD tasks uses n-grams and lexicon-based binaryvalued features that are commonly used in existing state-of-the-art sarcasm detection approaches (Gonz´alez-Ib´a˜nez et al., 2011; Tchokni et al., 2014). They are derived from i) bag-of-words (BoW) representations of words, ii) LIWC dictionary (Pennebaker et al., 2001), and iii) a list of interjections (e.g., “ah”, “oh”, “yeah”), punctuations (e.g., “!”, “?”), and emoticons collected from Wikipedia. CMU Tweet Tokenizer is employed for tokenization. 5 We kept unig</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="14790" citStr="Church and Hanks, 1990" startWordPosition="2430" endWordPosition="2433"> (vs) and one for the literal sense L using the literal sense training data for t (VI).3 Given a test message u containing a target word t, we first represent the target word as a vector eu using all the context words inside u. To predict whether t is used in a literal or sarcastic sense in the test message u we simply apply geometric techniques (e.g., cosine similarity) between vu and the two sense vectors vs and el, choosing the one with the maximum score. To create the two sense vectors vs and vl for each of the target words t, we use the positive pointwise mutual information model (PPMI) (Church and Hanks, 1990). Based on t’s context words ck in a window of 10 words, we separately computed PPMI for sarcastic and literal senses using t’s training data. The size of the context widow used in DSMs is generally between 5 and 10, and in our experiments we used a window of 10 words since tweets often include meaningful words/tokens at the end of the tweets (e.g., interjections, such as “yay”, “ohh”; upper-case words, such as, “GREAT”; novel hashtags, such as “#notreally”, “#lolol”; emoticons, such as “:(”). We sorted the context words based on the PPMI scores and for each target word t we selected a maximum</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="33803" citStr="Collobert et al., 2011" startWordPosition="5693" endWordPosition="5696">of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is its sarcastic or literal sense. Using a crowdsourcing experiment and unsupervised methods for detecting semantically opposite phrases, we collected a set of target words to be used in the LSSD task. We compared several distributional semantics methods, and showed that using word embeddings in a modified SVM kernel achieves the best results (an increas</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10.</booktitle>
<contexts>
<context position="1877" citStr="Davidov et al., 2010" startWordPosition="292" endWordPosition="295">d kernel using word embeddings achieves a 7-10% F1 improvement over a strong lexical baseline. 1 Introduction Recognizing sarcasm is important for understanding people’s actual sentiments and beliefs. For example, failing to recognize the following message as being sarcastic “I love that I have to go back to the emergency room”, will lead a sentiment and opinion analysis system to infer that the author’s sentiment is positive towards the event of “going to the emergency room”. Current approaches have framed the sarcasm detection task as predicting whether a full utterance is sarcastic or not (Davidov et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014). We propose a re-framing of sarcasm detection as a type of word sense disambiguation problem: given an utterance and a target word, identify whether the sense of the target word is literal or sarcastic. We call this the Literal/Sarcastic Sense Disambiguation (LSSD) task. In the above utterance, the word “love” is used in a sarcastic, nonliteral sense (the author’s intended meaning being most likely the opposite of the original literal meaning - a negative sentiment, such as “hate”). Two</context>
<context position="31786" citStr="Davidov et al., 2010" startWordPosition="5364" endWordPosition="5367">, “love”, and “yeah”) achieved “maximum” F1 scores in various experimental settings (Tables 4 and 5) whereas targets such as “interested”, “genius”, and “attractive” achieved low F1 scores. In terms of variance in results, SVM results show low SD (0-4%). For distributional approaches, SD is slightly higher (5-8%) for several cases. 5 Related Work Two lines of research are directly relevant to our work: sarcasm detection in Twitter and application of distributional semantics, such as word embedding techniques to various NLP tasks. In contrast to current research on sarcasm and irony detection (Davidov et al., 2010; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014), we have introduced a reframing of this task as a type of word sense disambiguation problem, where the sense of a word is sarcastic or literal. Our SVM baseline uses the lexical features proposed in previous research on sarcasm detection (e.g., LIWC lexicon, interjections, pragmatic features) (Liebrecht et al., 2013; Gonz´alez-Ib´a˜nez et al., 2011; Reyes et al., 2013). Our analysis of target words where the sarcastic sense is the opposite of the literal sense is related to the idea of “positive sentiment toward a nega</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papersVolume 2, pages 42–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonz´alez-Ib´a˜nez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholder</author>
</authors>
<title>Identifying sarcasm in twitter: A closer look.</title>
<date>2011</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>581--586</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gonz´alez-Ib´a˜nez, Muresan, Wacholder, 2011</marker>
<rawString>Roberto Gonz´alez-Ib´a˜nez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in twitter: A closer look. In ACL (Short Papers), pages 581–586. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Learning the latent semantics of a concept from its definition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>140--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17126" citStr="Guo and Diab, 2012" startWordPosition="2837" endWordPosition="2840">maximum score. All vector elements are given by the tf-idf values of the corresponding words. This approach, denoted as the “PPMI baseline”, is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors vs and vl of each target word t contain the cooccurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that implements the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear regression model based upon global word-word cooccurrence count in the training corpora. After removing the tweets that are used as test sets, we build the three word embedding models in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d-dimensional vector w� of real numbers, where d=100 for all of the embedding</context>
<context position="18465" citStr="Guo and Diab, 2012" startWordPosition="3052" endWordPosition="3055">th larger dimensions for larger datasets. Our current dataset is smaller than the ones used in other applications of word embeddings (e.g., Pennington et al. (2014) have used billion tweets to create word embedding) so we opted for 100 dimensional vectors. Below are the short descriptions of the three word embedding models: • Weighted Textual Matrix Factorization (WTMF): Low-dimensional vectors have been used in WSD tasks, since they are computationally efficient and provide better generalization than surface words. A dimension reduction method is Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b), which is designed specifically for short texts, and has been successfully applied in WSD tasks (Guo and Diab, 2012a). WTMF models unobserved words, thus providing more robust embeddings for short texts such as tweets. • word2vec Representation: We use both the Skip-gram model and the Continuous Bagof-Words (CBOW) model (Mikolov et al., 2013a; Mikolov et al., 2013c) as implemented in the word2vec gensim python library. 4 Given a window size of n words around a word w, the skip-gram model predicts the neighboring words given the current word. In contrast, the CBOW model predicts the current </context>
<context position="33382" citStr="Guo and Diab, 2012" startWordPosition="5629" endWordPosition="5632">of a word (Tables 4 and 5). This approach has the potential to capture more nuanced cases of sarcasm, beyond “positive sentiment towards a negative situation” (e.g., one of our target words was “shocked” which is negative). However, our current framing is still inherently limited to cases where sarcasm is characterized as a figure of speech where the author means the opposite of what she says, due to our approach of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012a. Learning the latent semantics of a concept from its definition. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 140–144. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>864--872</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17126" citStr="Guo and Diab, 2012" startWordPosition="2837" endWordPosition="2840">maximum score. All vector elements are given by the tf-idf values of the corresponding words. This approach, denoted as the “PPMI baseline”, is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors vs and vl of each target word t contain the cooccurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that implements the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear regression model based upon global word-word cooccurrence count in the training corpora. After removing the tweets that are used as test sets, we build the three word embedding models in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d-dimensional vector w� of real numbers, where d=100 for all of the embedding</context>
<context position="18465" citStr="Guo and Diab, 2012" startWordPosition="3052" endWordPosition="3055">th larger dimensions for larger datasets. Our current dataset is smaller than the ones used in other applications of word embeddings (e.g., Pennington et al. (2014) have used billion tweets to create word embedding) so we opted for 100 dimensional vectors. Below are the short descriptions of the three word embedding models: • Weighted Textual Matrix Factorization (WTMF): Low-dimensional vectors have been used in WSD tasks, since they are computationally efficient and provide better generalization than surface words. A dimension reduction method is Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b), which is designed specifically for short texts, and has been successfully applied in WSD tasks (Guo and Diab, 2012a). WTMF models unobserved words, thus providing more robust embeddings for short texts such as tweets. • word2vec Representation: We use both the Skip-gram model and the Continuous Bagof-Words (CBOW) model (Mikolov et al., 2013a; Mikolov et al., 2013c) as implemented in the word2vec gensim python library. 4 Given a window size of n words around a word w, the skip-gram model predicts the neighboring words given the current word. In contrast, the CBOW model predicts the current </context>
<context position="33382" citStr="Guo and Diab, 2012" startWordPosition="5629" endWordPosition="5632">of a word (Tables 4 and 5). This approach has the potential to capture more nuanced cases of sarcasm, beyond “positive sentiment towards a negative situation” (e.g., one of our target words was “shocked” which is negative). However, our current framing is still inherently limited to cases where sarcasm is characterized as a figure of speech where the author means the opposite of what she says, due to our approach of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012b. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 864–872. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--146</pages>
<contexts>
<context position="13579" citStr="Harris, 1954" startWordPosition="2219" endWordPosition="2220">SD task, especially for the methods that use word embeddings. 3.2 Learning Approaches We consider two classical approaches used in word sense disambiguation tasks: 1) distributional approaches where each sense of a target word is represented as a context vector derived from the training data; and 2) classification approaches (5 1005 vs. L; S vs. Lsent) for each target word. 3.2.1 Distributional Approaches The Distributional Hypothesis in linguistics is derived from the semantic theory of language usage, i.e., words that are used and occur in the same contexts tend to purport similar meanings (Harris, 1954). Distributional semantic models (DSMs) use vectors that represent the contexts (e.g., cooccurring words) in which target words appear in a corpus, as proxies for meaning representations. Geometric techniques such as cosine similarity are then applied to these vectors to measure the similarity in meaning of corresponding words. The DSMs are a natural approach to model our LSSD task. For each target word t we build two context-vectors that will represent the two senses of the target word t using the training data: one for the sarcastic sense S using the sarcastic training data for t (vs) and on</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S Harris. 1954. Distributional structure. Word, 10:146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data (TKDD),</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="20008" citStr="Islam and Inkpen (2008)" startWordPosition="3306" endWordPosition="3309">Here, the LSSD task is similar to the baseline: to predict whether the target word t in the test message u is used in a literal or sarcastic sense, we simply use a similarity measure between the ~vu (vector representation of the target word t in the test message u) and the two sense vectors ~vs and ~vl of t, choosing the one with the maximum score. The difference from the baseline is twofold: First, all vectors elements are word embeddings (i.e., 4https://radimrehurek.com/gensim/models/word2vec.html 100-d vectors). Second, we use the maximumvalued matrix-element (MVME) algorithm introduced by Islam and Inkpen (2008), which has been shown to be particularly useful for computing the similarity of short texts. We modify this algorithm to use word embeddings (MV MEwe). The idea behind the MVME algorithm is that it finds a oneto-one “word alignment” between two utterances (i.e., sentences) based on the pairwise word similarity. Only the aligned words contribute to the overall similarity score. Algorithm 1 MV MEwe 1: procedure MV MEwe(vs,vu) 2: vswords ← vs.elements() 3: vuwords ←vu.elements() 4: M[vswords.size(),vuwords.size()] ← 0 5: for k ← 0, vswords.size() do 6: ck ← vswords[k] 7: ~ck ← getEmbedding(ck) 8</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data (TKDD), 2(2):10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Joshi</author>
<author>Vinita Sharma</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Harnessing context incongruity for sarcasm detection.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),</booktitle>
<pages>757--762</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="32478" citStr="Joshi et al. (2015)" startWordPosition="5481" endWordPosition="5484">4), we have introduced a reframing of this task as a type of word sense disambiguation problem, where the sense of a word is sarcastic or literal. Our SVM baseline uses the lexical features proposed in previous research on sarcasm detection (e.g., LIWC lexicon, interjections, pragmatic features) (Liebrecht et al., 2013; Gonz´alez-Ib´a˜nez et al., 2011; Reyes et al., 2013). Our analysis of target words where the sarcastic sense is the opposite of the literal sense is related to the idea of “positive sentiment toward a negative situation” proposed by Riloff et al. (2013) and recently studied by Joshi et al. (2015). In our approach, we chose distributional semantic approaches that learn contextual information of targets effectively from a large corpus containing both literal and sarcastic uses of words and show that word embedding are highly accurate in predicting the sarcastic or literal sense of a word (Tables 4 and 5). This approach has the potential to capture more nuanced cases of sarcasm, beyond “positive sentiment towards a negative situation” (e.g., one of our target words was “shocked” which is negative). However, our current framing is still inherently limited to cases where sarcasm is charact</context>
</contexts>
<marker>Joshi, Sharma, Bhattacharyya, 2015</marker>
<rawString>Aditya Joshi, Vinita Sharma, and Pushpak Bhattacharyya. 2015. Harnessing context incongruity for sarcasm detection. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 757–762, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9799" citStr="Koehn et al., 2007" startWordPosition="1624" endWordPosition="1627">impel et al., 2011). Second, Bannard and Callison-Burch (2005) noticed that the co-training method proposed byBarzilay and McKeown (2001) requires identical bounding substrings and has bias towards single words while extracting paraphrases. This apparent limitation, however, is advantageous to us because we are specifically interested in extracting target words. Co-training resulted in 367 extracted pairs of paraphrases. We also considered a statistical machine translation (SMT) alignment method - IBM Model 4 with HMM alignment implemented in Giza++ (Och and Ney, 2000). We used Moses software(Koehn et al., 2007) to extract lexical translations by aligning the dataset of 5,000 SM-IM pairs. From the set of 367 extracted paraphrases using Barzilay and McKeown (2001)’s approach, we selected only those paraphrases where the lexical translation scores φ (resulted after running Moses) are ≥ 0.8. After filtering via translation scores and manual inspection, we obtained a set of 80 semantically opposite paraphrases. Given this set of semantically opposite words, the words that appear in the sarcastic messages were consider our target words for LSSD (70 target words after lemmatization). They range from verbs,</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving distributional similarity with lessons learned from word embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--211</pages>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CC Liebrecht</author>
<author>FA Kunneman</author>
<author>APJ van den Bosch</author>
</authors>
<title>The perfect solution for detecting sarcasm in tweets# not.</title>
<date>2013</date>
<marker>Liebrecht, Kunneman, van den Bosch, 2013</marker>
<rawString>CC Liebrecht, FA Kunneman, and APJ van den Bosch. 2013. The perfect solution for detecting sarcasm in tweets# not.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Maynard</author>
<author>Mark A Greenwood</author>
</authors>
<title>Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="1985" citStr="Maynard and Greenwood, 2014" startWordPosition="308" endWordPosition="311">roduction Recognizing sarcasm is important for understanding people’s actual sentiments and beliefs. For example, failing to recognize the following message as being sarcastic “I love that I have to go back to the emergency room”, will lead a sentiment and opinion analysis system to infer that the author’s sentiment is positive towards the event of “going to the emergency room”. Current approaches have framed the sarcasm detection task as predicting whether a full utterance is sarcastic or not (Davidov et al., 2010; Gonz´alez-Ib´a˜nez et al., 2011; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014). We propose a re-framing of sarcasm detection as a type of word sense disambiguation problem: given an utterance and a target word, identify whether the sense of the target word is literal or sarcastic. We call this the Literal/Sarcastic Sense Disambiguation (LSSD) task. In the above utterance, the word “love” is used in a sarcastic, nonliteral sense (the author’s intended meaning being most likely the opposite of the original literal meaning - a negative sentiment, such as “hate”). Two key challenges need to be addressed: 1) how to collect a set of target words that can have a literal or a s</context>
<context position="31861" citStr="Maynard and Greenwood, 2014" startWordPosition="5376" endWordPosition="5379">imental settings (Tables 4 and 5) whereas targets such as “interested”, “genius”, and “attractive” achieved low F1 scores. In terms of variance in results, SVM results show low SD (0-4%). For distributional approaches, SD is slightly higher (5-8%) for several cases. 5 Related Work Two lines of research are directly relevant to our work: sarcasm detection in Twitter and application of distributional semantics, such as word embedding techniques to various NLP tasks. In contrast to current research on sarcasm and irony detection (Davidov et al., 2010; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014), we have introduced a reframing of this task as a type of word sense disambiguation problem, where the sense of a word is sarcastic or literal. Our SVM baseline uses the lexical features proposed in previous research on sarcasm detection (e.g., LIWC lexicon, interjections, pragmatic features) (Liebrecht et al., 2013; Gonz´alez-Ib´a˜nez et al., 2011; Reyes et al., 2013). Our analysis of target words where the sarcastic sense is the opposite of the literal sense is related to the idea of “positive sentiment toward a negative situation” proposed by Riloff et al. (2013) and recently studied by Jo</context>
</contexts>
<marker>Maynard, Greenwood, 2014</marker>
<rawString>Diana Maynard and Mark A Greenwood. 2014. Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="17234" citStr="Mikolov et al. (2013" startWordPosition="2853" endWordPosition="2856">h, denoted as the “PPMI baseline”, is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors vs and vl of each target word t contain the cooccurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that implements the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear regression model based upon global word-word cooccurrence count in the training corpora. After removing the tweets that are used as test sets, we build the three word embedding models in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d-dimensional vector w� of real numbers, where d=100 for all of the embedding algorithms in our experiments. For the size of the embedding vectors, it is common to use 1006 100 or 300 d</context>
<context position="18811" citStr="Mikolov et al., 2013" startWordPosition="3106" endWordPosition="3109"> Matrix Factorization (WTMF): Low-dimensional vectors have been used in WSD tasks, since they are computationally efficient and provide better generalization than surface words. A dimension reduction method is Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b), which is designed specifically for short texts, and has been successfully applied in WSD tasks (Guo and Diab, 2012a). WTMF models unobserved words, thus providing more robust embeddings for short texts such as tweets. • word2vec Representation: We use both the Skip-gram model and the Continuous Bagof-Words (CBOW) model (Mikolov et al., 2013a; Mikolov et al., 2013c) as implemented in the word2vec gensim python library. 4 Given a window size of n words around a word w, the skip-gram model predicts the neighboring words given the current word. In contrast, the CBOW model predicts the current word w, given the neighboring words in the window. We considered a context window of 10 words. • GloVe Representation: GloVe (Pennington et al., 2014) is a word embedding model that is based upon weighted least-square model trained on global word-word co-occurrence counts instead of the local context used by word2vec. Here, the LSSD task is sim</context>
<context position="33556" citStr="Mikolov et al., 2013" startWordPosition="5655" endWordPosition="5658"> our target words was “shocked” which is negative). However, our current framing is still inherently limited to cases where sarcasm is characterized as a figure of speech where the author means the opposite of what she says, due to our approach of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is its sarcastic or literal sense. Using a crowdsourcing experiment and unsupervised methods for detecting se</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</title>
<date>2013</date>
<contexts>
<context position="17234" citStr="Mikolov et al. (2013" startWordPosition="2853" endWordPosition="2856">h, denoted as the “PPMI baseline”, is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors vs and vl of each target word t contain the cooccurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that implements the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear regression model based upon global word-word cooccurrence count in the training corpora. After removing the tweets that are used as test sets, we build the three word embedding models in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d-dimensional vector w� of real numbers, where d=100 for all of the embedding algorithms in our experiments. For the size of the embedding vectors, it is common to use 1006 100 or 300 d</context>
<context position="18811" citStr="Mikolov et al., 2013" startWordPosition="3106" endWordPosition="3109"> Matrix Factorization (WTMF): Low-dimensional vectors have been used in WSD tasks, since they are computationally efficient and provide better generalization than surface words. A dimension reduction method is Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b), which is designed specifically for short texts, and has been successfully applied in WSD tasks (Guo and Diab, 2012a). WTMF models unobserved words, thus providing more robust embeddings for short texts such as tweets. • word2vec Representation: We use both the Skip-gram model and the Continuous Bagof-Words (CBOW) model (Mikolov et al., 2013a; Mikolov et al., 2013c) as implemented in the word2vec gensim python library. 4 Given a window size of n words around a word w, the skip-gram model predicts the neighboring words given the current word. In contrast, the CBOW model predicts the current word w, given the neighboring words in the window. We considered a context window of 10 words. • GloVe Representation: GloVe (Pennington et al., 2014) is a word embedding model that is based upon weighted least-square model trained on global word-word co-occurrence counts instead of the local context used by word2vec. Here, the LSSD task is sim</context>
<context position="33556" citStr="Mikolov et al., 2013" startWordPosition="5655" endWordPosition="5658"> our target words was “shocked” which is negative). However, our current framing is still inherently limited to cases where sarcasm is characterized as a figure of speech where the author means the opposite of what she says, due to our approach of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is its sarcastic or literal sense. Using a crowdsourcing experiment and unsupervised methods for detecting se</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="17234" citStr="Mikolov et al. (2013" startWordPosition="2853" endWordPosition="2856">h, denoted as the “PPMI baseline”, is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors vs and vl of each target word t contain the cooccurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that implements the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear regression model based upon global word-word cooccurrence count in the training corpora. After removing the tweets that are used as test sets, we build the three word embedding models in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d-dimensional vector w� of real numbers, where d=100 for all of the embedding algorithms in our experiments. For the size of the embedding vectors, it is common to use 1006 100 or 300 d</context>
<context position="18811" citStr="Mikolov et al., 2013" startWordPosition="3106" endWordPosition="3109"> Matrix Factorization (WTMF): Low-dimensional vectors have been used in WSD tasks, since they are computationally efficient and provide better generalization than surface words. A dimension reduction method is Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b), which is designed specifically for short texts, and has been successfully applied in WSD tasks (Guo and Diab, 2012a). WTMF models unobserved words, thus providing more robust embeddings for short texts such as tweets. • word2vec Representation: We use both the Skip-gram model and the Continuous Bagof-Words (CBOW) model (Mikolov et al., 2013a; Mikolov et al., 2013c) as implemented in the word2vec gensim python library. 4 Given a window size of n words around a word w, the skip-gram model predicts the neighboring words given the current word. In contrast, the CBOW model predicts the current word w, given the neighboring words in the window. We considered a context window of 10 words. • GloVe Representation: GloVe (Pennington et al., 2014) is a word embedding model that is based upon weighted least-square model trained on global word-word co-occurrence counts instead of the local context used by word2vec. Here, the LSSD task is sim</context>
<context position="33556" citStr="Mikolov et al., 2013" startWordPosition="5655" endWordPosition="5658"> our target words was “shocked” which is negative). However, our current framing is still inherently limited to cases where sarcasm is characterized as a figure of speech where the author means the opposite of what she says, due to our approach of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is its sarcastic or literal sense. Using a crowdsourcing experiment and unsupervised methods for detecting se</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013c. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Giza++: Training of statistical translation models.</title>
<date>2000</date>
<contexts>
<context position="9755" citStr="Och and Ney, 2000" startWordPosition="1617" endWordPosition="1620">co-training algorithm, we used Tweet NLP (Gimpel et al., 2011). Second, Bannard and Callison-Burch (2005) noticed that the co-training method proposed byBarzilay and McKeown (2001) requires identical bounding substrings and has bias towards single words while extracting paraphrases. This apparent limitation, however, is advantageous to us because we are specifically interested in extracting target words. Co-training resulted in 367 extracted pairs of paraphrases. We also considered a statistical machine translation (SMT) alignment method - IBM Model 4 with HMM alignment implemented in Giza++ (Och and Ney, 2000). We used Moses software(Koehn et al., 2007) to extract lexical translations by aligning the dataset of 5,000 SM-IM pairs. From the set of 367 extracted paraphrases using Barzilay and McKeown (2001)’s approach, we selected only those paraphrases where the lexical translation scores φ (resulted after running Moses) are ≥ 0.8. After filtering via translation scores and manual inspection, we obtained a set of 80 semantically opposite paraphrases. Given this set of semantically opposite words, the words that appear in the sarcastic messages were consider our target words for LSSD (70 target words </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Giza++: Training of statistical translation models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James W Pennebaker</author>
<author>Martha E Francis</author>
<author>Roger J Booth</author>
</authors>
<title>Linguistic inquiry and word count: Liwc</title>
<date>2001</date>
<pages>71--2001</pages>
<contexts>
<context position="22692" citStr="Pennebaker et al., 2001" startWordPosition="3780" endWordPosition="3783"> classification task to identify the sarcastic or literal sense of a target word t. We have two classification tasks: S vs. L and S vs. Lsent for each of the 37 target words. We use the libSVM toolkit (Chang and Lin, 2011). Development data is used for tuning parameters. SVM Baseline: The SVM baseline for LSSD tasks uses n-grams and lexicon-based binaryvalued features that are commonly used in existing state-of-the-art sarcasm detection approaches (Gonz´alez-Ib´a˜nez et al., 2011; Tchokni et al., 2014). They are derived from i) bag-of-words (BoW) representations of words, ii) LIWC dictionary (Pennebaker et al., 2001), and iii) a list of interjections (e.g., “ah”, “oh”, “yeah”), punctuations (e.g., “!”, “?”), and emoticons collected from Wikipedia. CMU Tweet Tokenizer is employed for tokenization. 5 We kept unigrams unchanged when all the characters are uppercase (e.g., “NEVER” in “A shooting in Oakland? That NEVER happens! #sarcasm”) but otherwise words are converted to lower case. We also change all numbers to a generic number token “22”. To avoid any bias during experiments, we removed the target words from the tweets as well as any hashtag used to determine the sense of the tweet (e.g., #sarcasm, #sarc</context>
</contexts>
<marker>Pennebaker, Francis, Booth, 2001</marker>
<rawString>James W Pennebaker, Martha E Francis, and Roger J Booth. 2001. Linguistic inquiry and word count: Liwc 2001. Mahway: Lawrence Erlbaum Associates, 71:2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="17273" citStr="Pennington et al., 2014" startWordPosition="2859" endWordPosition="2862">is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors vs and vl of each target word t contain the cooccurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that implements the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear regression model based upon global word-word cooccurrence count in the training corpora. After removing the tweets that are used as test sets, we build the three word embedding models in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d-dimensional vector w� of real numbers, where d=100 for all of the embedding algorithms in our experiments. For the size of the embedding vectors, it is common to use 1006 100 or 300 dimensions, with larger dimensions for l</context>
<context position="19215" citStr="Pennington et al., 2014" startWordPosition="3178" endWordPosition="3181"> models unobserved words, thus providing more robust embeddings for short texts such as tweets. • word2vec Representation: We use both the Skip-gram model and the Continuous Bagof-Words (CBOW) model (Mikolov et al., 2013a; Mikolov et al., 2013c) as implemented in the word2vec gensim python library. 4 Given a window size of n words around a word w, the skip-gram model predicts the neighboring words given the current word. In contrast, the CBOW model predicts the current word w, given the neighboring words in the window. We considered a context window of 10 words. • GloVe Representation: GloVe (Pennington et al., 2014) is a word embedding model that is based upon weighted least-square model trained on global word-word co-occurrence counts instead of the local context used by word2vec. Here, the LSSD task is similar to the baseline: to predict whether the target word t in the test message u is used in a literal or sarcastic sense, we simply use a similarity measure between the ~vu (vector representation of the target word t in the test message u) and the two sense vectors ~vs and ~vl of t, choosing the one with the maximum score. The difference from the baseline is twofold: First, all vectors elements are wo</context>
<context position="33606" citStr="Pennington et al., 2014" startWordPosition="5663" endWordPosition="5666">ive). However, our current framing is still inherently limited to cases where sarcasm is characterized as a figure of speech where the author means the opposite of what she says, due to our approach of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is its sarcastic or literal sense. Using a crowdsourcing experiment and unsupervised methods for detecting semantically opposite phrases, we collected a set of</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
<author>Tony Veale</author>
</authors>
<title>A multidimensional approach for detecting irony in twitter. Language resources and evaluation,</title>
<date>2013</date>
<pages>47--1</pages>
<contexts>
<context position="32233" citStr="Reyes et al., 2013" startWordPosition="5436" endWordPosition="5439">on of distributional semantics, such as word embedding techniques to various NLP tasks. In contrast to current research on sarcasm and irony detection (Davidov et al., 2010; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014), we have introduced a reframing of this task as a type of word sense disambiguation problem, where the sense of a word is sarcastic or literal. Our SVM baseline uses the lexical features proposed in previous research on sarcasm detection (e.g., LIWC lexicon, interjections, pragmatic features) (Liebrecht et al., 2013; Gonz´alez-Ib´a˜nez et al., 2011; Reyes et al., 2013). Our analysis of target words where the sarcastic sense is the opposite of the literal sense is related to the idea of “positive sentiment toward a negative situation” proposed by Riloff et al. (2013) and recently studied by Joshi et al. (2015). In our approach, we chose distributional semantic approaches that learn contextual information of targets effectively from a large corpus containing both literal and sarcastic uses of words and show that word embedding are highly accurate in predicting the sarcastic or literal sense of a word (Tables 4 and 5). This approach has the potential to captur</context>
</contexts>
<marker>Reyes, Rosso, Veale, 2013</marker>
<rawString>Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A multidimensional approach for detecting irony in twitter. Language resources and evaluation, 47(1):239–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Ashequl Qadir</author>
<author>Prafulla Surve</author>
<author>Lalindra De Silva</author>
<author>Nathan Gilbert</author>
<author>Ruihong Huang</author>
</authors>
<title>Sarcasm as contrast between a positive sentiment and negative situation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>704--714</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Riloff, Qadir, Surve, De Silva, Gilbert, Huang, 2013</marker>
<rawString>Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, and Ruihong Huang. 2013. Sarcasm as contrast between a positive sentiment and negative situation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 704–714. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simo Tchokni</author>
<author>Diarmuid O S´eaghdha</author>
<author>Daniele Quercia</author>
</authors>
<title>Emoticons and phrases: Status symbols in social media.</title>
<date>2014</date>
<booktitle>In Eighth International AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>Tchokni, S´eaghdha, Quercia, 2014</marker>
<rawString>Simo Tchokni, Diarmuid O S´eaghdha, and Daniele Quercia. 2014. Emoticons and phrases: Status symbols in social media. In Eighth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Sumit Chopra</author>
<author>Tomas Mikolov</author>
</authors>
<title>Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</title>
<date>2015</date>
<contexts>
<context position="33825" citStr="Weston et al., 2015" startWordPosition="5697" endWordPosition="5700">words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is its sarcastic or literal sense. Using a crowdsourcing experiment and unsupervised methods for detecting semantically opposite phrases, we collected a set of target words to be used in the LSSD task. We compared several distributional semantics methods, and showed that using word embeddings in a modified SVM kernel achieves the best results (an increase of 10% F1 and 8% F1 </context>
</contexts>
<marker>Weston, Bordes, Chopra, Mikolov, 2015</marker>
<rawString>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003225">
<title confidence="0.998098666666667">
That’s So Annoying!!!: A Lexical and Frame-Semantic
Embedding Based Data Augmentation Approach to Automatic
Categorization of Annoying Behaviors using #petpeeve Tweets ∗
</title>
<author confidence="0.995235">
William Yang Wang and Diyi Yang
</author>
<affiliation confidence="0.938100666666667">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.999434">
{yww,diyiy}@cs.cmu.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999903">
We propose a novel data augmentation ap-
proach to enhance computational behav-
ioral analysis using social media text. In
particular, we collect a Twitter corpus of
the descriptions of annoying behaviors us-
ing the #petpeeve hashtags. In the qual-
itative analysis, we study the language
use in these tweets, with a special focus
on the fine-grained categories and the ge-
ographic variation of the language. In
quantitative analysis, we show that lexi-
cal and syntactic features are useful for au-
tomatic categorization of annoying behav-
iors, and frame-semantic features further
boost the performance; that leveraging
large lexical embeddings to create addi-
tional training instances significantly im-
proves the lexical model; and incorporat-
ing frame-semantic embedding achieves
the best overall performance.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954363636364">
In the ever-expanding era of social media, many
scientific disciplines, such as health and health-
care, biology, and learning sciences, have adopted
computational approaches to exploit patterns and
behaviors in large datasets (Wang et al., 2015;
Chen and Lonardi, 2009; Baker and Yacef, 2009).
In contrast, the primary methods for behavioral
sciences still rely on lab experiments with limited
amount of subjects, which are time consuming and
financially expensive. In addition to this, it is also
difficult to obtain a set of samples with geograph-
</bodyText>
<footnote confidence="0.556551666666667">
∗We understand that many people find long titles annoy-
ing, so we intentionally use a very long one to help people
understand what “pet peeve” means.
</footnote>
<figureCaption confidence="0.99739">
Figure 1: An anonymized example of #petpeeve tweets.
</figureCaption>
<bodyText confidence="0.9997158125">
ical variations in traditional lab-based behavioral
experiments.
While the social media data are abundantly
available, computational approaches to behavioral
sciences using Twitter are not well-studied. Even
when statistical techniques are applied to these
tasks, their concentration has been on simple sta-
tistical significance tests and descriptive statis-
tics (De Charms, 2013; Zhang et al., 2013). There-
fore, we believe that statistical natural language
processing techniques are needed for insightful
analysis and interpretation in behavioral studies.
In this paper, we use Twitter as a corpus for
computational behavioral science. More specifi-
cally, we focus on a case study of analyzing an-
noying behaviors. To do this, we exploit a corpus
of 9 million tweets (Cheng et al., 2010), and ex-
tract the tweets that describe these behaviors us-
ing the #petpeeve hashtags. #petpeeve is a pop-
ular Twitter hashtag, which describes behaviors
that might be annoying to others. An example
of #petpeeve tweets is shown in Figure 1. To fa-
cilitate the analysis, we manually annotate 3,375
tweets with 60 fine-grained categories, which will
be described in Section 3. We use a sparse mixed-
effects topic model to analyze the salient words
in each category, as well as the geographic varia-
tions. We show that lexical, syntactic, and seman-
tic features enhance the automatic categorization
of annoying behaviors; and that the performance
is further improved with a novel lexical and frame-
semantic embedding based data augmentation ap-
</bodyText>
<page confidence="0.913611">
2557
</page>
<note confidence="0.9846675">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2557–2563,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<listItem confidence="0.942903">
proach. Our main contributions are three-fold:
• We provide a Twitter corpus with fine-
grained annotations for computational behav-
ior studies;
• We qualitatively analyze the Twitter language
concerning annoying behaviors, with a focus
on the topics and geographical variations;
• We propose various linguistic features and a
novel data augmentation approach for auto-
matic categorization of annoying behaviors.
</listItem>
<bodyText confidence="0.9996675">
We outline related work in the next section. The
dataset is described in Section 3. We introduce the
approach for analyzing #petpeeve Tweets in Sec-
tion 4. Experimental results are shown in Sec-
tion 5. We discuss possible applications in Sec-
tion 6, and conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999180625">
Psychologists, behavioral scientists, and computer
scientists have studied a wide-range of methods
for behavior extraction (Mast et al., 2015). For ex-
ample, in lab experiments, arm and body postures
(Marcos-Ramiro et al., 2013) are often used to ex-
tract self-touch and gestures, while eye gaze (Fu-
nes Mora and Odobez, 2012), head pose (Ba and
Odobez, 2011), face location and motion (Nguyen
et al., 2012), and full-body pose (Shotton et al.,
2013) can also be used as cues to extract gaz-
ing, nodding, and arm-related behaviors. There
are also significant amount of studies of extract-
ing facial and speech features to understand smil-
ing (Bartlett et al., 2008), eye contact (Marin-
Jimenez et al., 2014), and verbal behaviors (Basu,
2002).
With the surge of interest in computational
social science (Lazer et al., 2009), Twitter has
become a popular resource to study data-driven
methods in social science (Miller, 2011). For ex-
ample, O’Connor et al. (2010a) align the Twit-
ter messages with public opinion time series to
study computational political science. Ritter et
al. (2010) study Twitter dialogues using a clus-
tering approach. Bollen et al. (2011) use a sen-
timent analysis approach to predict the Ameri-
can stock market via Twitter. Li et al. (2014b)
have investigated the alignment of Twitter mood
with weather for sentiment analysis. In recent
years, language technology researchers have fo-
cused on developing genre-specific Twitter part-
of-speech tagging (Gimpel et al., 2011), named
</bodyText>
<table confidence="0.999452470588235">
Label % Label %
appearance .14 services .02
disrespect .06 traffic .02
language .06 advertisement .01
hygiene .05 bragging .01
relationship .05 children .01
dishonesty .03 complaining .01
hypocrisy .03 indolence .01
incompetence .03 physical .01
interruption .03 punctuality .01
monetary .03 racial .01
sexual .03 religious .01
arrogance .02 selfishness .01
celebrity .02 silence .01
ignorance .02 smoking .01
privacy .02 talkative .01
products .02 weather .01
</table>
<tableCaption confidence="0.99114475">
Table 1: The categories and percentages of annoying behav-
iors in #petpeeve tweets in our dataset. Note that 17% of the
#petpeeve tweets are identified as other unrelated behaviors
(not shown).
</tableCaption>
<bodyText confidence="0.999914666666667">
entity recognition (Ritter et al., 2011), summariza-
tion (O’Connor et al., 2010b), sentiment analy-
sis (Agarwal et al., 2011), event extraction (Ritter
et al., 2012; Li et al., 2014a), paraphrasing (Xu et
al., 2014), machine translation (Ling et al., 2013),
and dependency parsing (Kong et al., 2014) meth-
ods. To the best of our knowledge, even though
there have been studies on using Twitter hashtags
to study language-related behaviors (Gonz´alez-
Ib´anez et al., 2011; Bamman and Smith, 2015),
Twitter NLP approaches to non-linguistic behav-
iors are not well studied in general.
</bodyText>
<sectionHeader confidence="0.992818" genericHeader="method">
3 The Dataset
</sectionHeader>
<bodyText confidence="0.999974142857143">
We use the Twitter corpus with 9 million sam-
pled messages collected in prior work (Cheng et
al., 2010), which includes a total of 121K users.
The dataset includes latitude and longitude infor-
mation.
We extract 3,375 tweets1 with #petpeeve hash-
tags. We follow past work to annotate the
tweets (Ritter et al., 2012; Li et al., 2014a): we
apply the LDA clustering + human-identification
approach to label the categories of the described
annoying behaviors in these tweets. The human
annotation process includes two stages: first, the
annotators identify the 50 categories from the clus-
tering process, and use these topics as a candi-
</bodyText>
<footnote confidence="0.983882">
1http://www.cs.cmu.edu/˜yww/data/petpeeves.zip
</footnote>
<page confidence="0.989474">
2558
</page>
<bodyText confidence="0.999863714285714">
date label set to annotate the data; in the second
stage, the categories are refined (to 60 classes)
from the first pass, and the data is re-annotated
with the refined human-specified category labels.
Due to the complexity of this fine-grained anno-
tation task, the inter-annotator agreement rate be-
tween two annotators is moderate (0.445).
The annotated categories and label distribution2
of the dataset are shown in Table 1. In our random
samples, the states that post the most #petpeeve
tweets are NY, MD, CA, NJ, FL, GA, VA, TX,
NC, PA, and DC. In our predictive experiments,
we randomly select 60% of tweets for training, and
40% for testing.
</bodyText>
<sectionHeader confidence="0.989854" genericHeader="method">
4 Our Approach
</sectionHeader>
<bodyText confidence="0.999958333333333">
In this section, we describe our methods for the
qualitative and quantitative analyses. In particular,
we briefly review a supervised approach of using
sparse mixed-effects topic model to visualize the
topical words to analyze this behavior data. For
the quantitative task of automatic categorization of
tweets, we propose a novel approach to create ad-
ditional training data, using continuous lexical and
semantic representations.
</bodyText>
<subsectionHeader confidence="0.996141">
4.1 Supervised Topic Modeling
</subsectionHeader>
<bodyText confidence="0.999952944444445">
To analyze the salient words for each category of
annoying behaviors, we utilize SAGE (Eisenstein
et al., 2011), a state-of-the-art mixed-effect topic
model, which has been used in several NLP ap-
plications (Sim et al., 2012; Wang et al., 2012).
SAGE is ideal for our text analytic purposes, be-
cause it is supervised, and it builds relatively clean
topic models by considering the additive effects
and the background distribution of words. There-
fore, we can use SAGE to visualize the salient
words for each category of annoying behaviors
using the 3,375 #petpeeve tweets. Each tweet
is treated as a document, and we use Markov
Chain Monte Carlo for inference. To facilitate
the geographical analysis, we use Google’s reverse
geocoding service to extract the state information
from coordinates, and apply SAGE for visualiza-
tion.
</bodyText>
<footnote confidence="0.74668">
2The categories that are not shown in the table are back-
</footnote>
<construct confidence="0.980262">
stabbing, boring, copycat, drinking, drug, empty promise,
impoliteness, inconsiderate, indirect, insecurity, interference,
irresponsible, jealous, judge, loneliness, misunderstanding,
negativity, noisy, parents, politics, repetition, showoff, snob-
bish, stability, swearing, time-wasting, ungratefulness, and
others.
</construct>
<subsectionHeader confidence="0.9656655">
4.2 Embedding-Based Data Augmentation
for Automatic Categorization of Tweets
</subsectionHeader>
<bodyText confidence="0.998138">
In addition to the visualization task, we also ask
the question: can we use linguistic cues to predict
tweets that describe different annoying behaviors?
We formulate the problem as a multiclass classi-
fication task, and consider the following feature
sets:
</bodyText>
<listItem confidence="0.988399411764706">
• Lexical Features: we extract unigrams as
surface-level lexical features.
• Part-of-Speech Features: to model shallow
syntactic cues, we extract lexicalized part-of-
speech features using the Stanford part-of-
speech tagger (Toutanova et al., 2003).
• Dependency Triples: to better understand
the deeper syntactic dependencies of key-
words in tweets, we have also extracted typed
dependency triples (e.g., nsubj(hatej)) using
the MaltParser (Nivre et al., 2007).
• Frame-Semantics Features: SE-
MAFOR (Das et al., 2010) is a state-of-
the-art frame-semantics parser that produces
FrameNet-style semantic annotation. We use
SEMAFOR to extract frame-level semantic
features.
</listItem>
<bodyText confidence="0.997811782608695">
Embeddings for Data Augmentation Since the
Twitter messages are often short and noisy, and
the training data is relatively scarce for each class,
we consider the feasibility of leveraging external
resources, in particular, continuous word embed-
dings (Mikolov et al., 2013a) to enhance the mul-
ticlass text categorization model.
Two major challenges for leveraging word em-
beddings for tweet classification are: 1) because
word embeddings are continuous, it is difficult to
fuse them with other discrete syntactic and se-
mantic features; 2) it is not straightforward how
one should transform the word-level representa-
tion to the tweet-level representation. In our pre-
liminary experiments, we have evaluated the con-
tinuous word representation method (Turian et al.,
2010), as well as incorporating neighboring words
in the embeddings as additional features, but both
methods fail to outperform the lexical baseline that
uses only bag-of-word unigrams.
To solve this problem, we propose the use of
neighboring words in continuous representations
to create new instances to augment the training
</bodyText>
<page confidence="0.937456">
2559
</page>
<table confidence="0.99956475">
weather ungratefulness traffic timewasting talkative swearing stability snobbish
rains helped cop wastingmytime Tweeters curse mood smut
STORM ungrateful lane colleagues Xs teary sensitive intellectual
Blizzarad clearly pulled Wen wht qweet91 dudes moneycars
snowed r speed BruklynFinest sheesh swears nigga LoWQUI
SNOW them Slow hold TwitterJail 10 up lifestyle
smoking silence showoff sexual services selfishness repetition religious
JAYECANE guilty louis box fil ONLY dislike sinners
reggie R rims wonder requests Selfish repeat IAmKevinTerrell
smoking response seein Preach convos selfish myself spiritual
smoke conversation makin suck TIP stay same CHURCH
smokers sending bag pussy products hit over FOLK
</table>
<tableCaption confidence="0.975007">
Table 2: The salient words for categories of annoying behaviors learned by the sparse additive generative model of text.
</tableCaption>
<table confidence="0.99049475">
State Top Topical Words
NY stalkers niqqas der den part dats liek havin
MD fuckouttahere missing ima dmv fan situation tongue
CA pocket clown phones football fit acting lip
NJ nite blame p hips pum summer elses seein
FL daddy both chipped pum rims nappy foh children
GA oo affioncrockett season cigarettes year tatoos
VA lane language middle might check winter past duke
TX drama lmaoooo gtfoh nappy two jk stare unfollow
NC everyday ear chic during hello wayansjr tryn nicca
PA 10 huh killyaself lifestyle shades round texts fucc
DC dmv uncle nosey stare cares bish 1st lips
</table>
<tableCaption confidence="0.9829035">
Table 3: The geographical variation of the annoying behav-
iors.
</tableCaption>
<bodyText confidence="0.961274">
dataset. More specifically, in the embedding vo-
cabulary W, we search for the k-nearest-neighbor
(knn) word w for a query term using cosine sim-
ilarity between query Q� and target word vectors
W�:
</bodyText>
<equation confidence="0.951548">
arg max cosine((j, W�) (1)
w∈W
</equation>
<bodyText confidence="0.998730947368421">
For each word in a tweet, we query the exter-
nal embeddings, and replace them with their knn
words to create a new training instance. For ex-
ample, consider the tweet “Being late is terrible”
with the punctuality label, after searching for knn
words for each token, we create a new training in-
stance: “Be behind are bad” with the same label.
Frame-Semantic Embeddings Although lexi-
cal (Mikolov et al., 2013a) and dependency based
embeddings (Levy and Goldberg, 2014) have been
studied, semantic-based embedding is still less un-
derstood. We consider the continuous embedding
of semantic frames (Baker et al., 1998). To do this,
we semantically parsed 3.8 million tweets using
SEMAFOR (Das et al., 2010), and built a continu-
ous bag-of-frame model to represent each seman-
tic frame using Word2Vec3. We then use the same
data augmentation approach to create additional
instances with these semantic frame embeddings.
</bodyText>
<footnote confidence="0.935573">
3https://code.google.com/p/word2vec/
</footnote>
<table confidence="0.9999358">
Features Precision Recall F1
Lexical .341 .342 .341
+POS .345 .346 .346
+Dependency* .349 .350 .350
+Semantic Frames* .365 .367 .366
</table>
<tableCaption confidence="0.6917845">
Table 4: Comparing linguistic features for categorizing an-
noying behaviors. The best results are highlighted in bold.*
indicates that the result is significantly better than the lexical
baseline (p &lt; .0001).
</tableCaption>
<sectionHeader confidence="0.998266" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999303">
5.1 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999996875">
We show the results of the visualization of salient
words for each category of tweets in Table 2.
SAGE clearly does a good job identifying annoy-
ing specific behaviors in each category. For ex-
ample, in the traffic category, we see that the key-
words “cop” and “pulled” that associate with traf-
fic stop are identified. Also, “slow” and “speed”
are also recognized as annoying behaviors dur-
ing traffic. In the selfishness category, the word
“ONLY” and “Selfish” are corrected identified. In
the silence category, we see that the word “R” is
promising, because it indicates the behavior when
someone reads a blackberry message without re-
ply. We see that many slang expressions are asso-
ciated with various labels.
In Table 3, we show the geographical varia-
tion of tweets. The word “dmv” (DC-Maryland-
Virginia) is correctly associated with MD and DC,
and when we search the database, these #petpeeve
tweets mainly refer to the 2010 snowstorm in the
Winter affecting these areas. The “daddy” is
prominent in the state of Florida, while the word
“rims” is also identified, showing the unique car
culture of this southern state.
</bodyText>
<subsectionHeader confidence="0.995179">
5.2 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.9796975">
Experimental Setup We use the logistic regres-
sion model from LibShortText (Yu et al., 2013)
</bodyText>
<page confidence="0.964938">
2560
</page>
<table confidence="0.999311888888889">
Methods Prec. Rec. F1 Imp.
Lexical Baseline (No Data Augmentation) .341 .342 .341 —
+ UrbanDictionary Embeddings .343 .344 .344 0.9%
+ Twitter Embeddings* .357 .358 .358 4.7%
+ GoogleNews Embeddings* .364 .366 .365 6.1%
All Features Baseline (No Data Augmentation) .365 .367 .366 —
+ Lexical (GoogleNews) and Frame-Semantic Embeddings* .376 .377 .376 2.7%
+ Lexical (Twitter) and Frame-Semantic Embeddings* .379 .380 .379 3.6%
+ Lexical (UD) and Frame-Semantic Embeddings* .379 .381 .380 3.8%
</table>
<tableCaption confidence="0.934888">
Table 5: The effectiveness of leveraging continuous embeddings to create additional training instances. Imp.: relative improve-
ment to the baseline without data augmentation. The best results for each section are highlighted in bold.* indicates that the
result is significantly better than the baseline without data augmentation (p &lt; .0001).
</tableCaption>
<bodyText confidence="0.999865827586207">
as the classifier in our 60-way multi-class classifi-
cation experiments. Grid search is used to select
the best hyper-parameter using the training data
only. A final classifier is then trained using the
best hyper-parameters and test set results are re-
ported. We set k = 5 for knn in our data augmen-
tation experiments: the training data is expanded
to 5 times of the original size. We use a paired
two-tailed student’s t test to assess the statistical
significance.
Word2Vec is used to train various lexical and
semantic embedding models. We consider three
lexical embeddings and one frame-semantic em-
beddings for data augmentation: 1) Google-
News Lexical Embeddings trained with 100 bil-
lion words (Mikolov et al., 2013b); 2) Twitter Lex-
ical Embeddings trained with 51 million of words;
3) Urban Dictionary lexical embeddings trained
with 53 million of words from slang definitions
and examples; 4) Twitter Semantic Frame Embed-
dings trained with 27 million frames.
Varying Feature Sets We compare various fea-
tures in Table 4. We see that adding shallow part-
of-speech features does not have a strong effect
on the performance, but adding the dependency
triples significantly outperforms the lexical base-
line. We see that the semantic frames are partic-
ular useful, showing a 7% relative improvement
over the baseline.
</bodyText>
<subsectionHeader confidence="0.750233">
The Effectiveness of Data Augmentation Table 5
</subsectionHeader>
<bodyText confidence="0.996729">
shows the results of data augmentation. We see
that using the Google News lexical embeddings to
augment the training data brings a 6.1% relative
F1 improvement over the lexical baseline. When
considering the additional frame-semantic embed-
dings from Twitter, our system obtains the best F1
of 0.380, bringing a 3.8% improvement over the
no data augmentation baseline with all linguistic
features.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999995913043478">
We provide a case study of automatically cat-
egorizing annoying behaviors using #petpeeve
Tweets. We hope that this study can further solicit
relevant research on fine-grained analysis of an-
noying behaviors in different dimensions, and use
computational approaches to improve social good.
For example, by using coordinates and other APIs,
one might analyze the annoying behaviors in the
public working environments (e.g., office, meeting
rooms, etc.). By understanding what annoys their
employees, companies can renovate their working
setups, refine their policies, and improve the satis-
faction and productivity of their employees.
In addition to #petpeeve Tweets, there are many
other interesting hashtags that align well with tra-
ditional topics in behavior sciences. For exam-
ple, hashtags like #occupywallstreet can be used
to study crowd behaviors in terms of a political un-
rest. The #ALS hashtag can be used to study public
behaviors in reaction to philanthropic campaigns.
Overall, Tweets from carefully selected hashtags
can be inexpensive to obtain, and facilitate signif-
icant amount of behavioral studies.
</bodyText>
<sectionHeader confidence="0.992496" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999903666666667">
In this paper, we have presented a case study of the
annoying behaviors using Twitter as a corpus. Our
fine-grained visualization approach shows insights
of different categories of these behaviors, with the
geographical effects. We also show that linguis-
tic cues are useful to categorize these behaviors
automatically, and that using lexical and semantic
embeddings as a data augmentation method sig-
nificantly improves the performance.
</bodyText>
<page confidence="0.985495">
2561
</page>
<sectionHeader confidence="0.96168" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.934086527777778">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media, pages 30–38.
Association for Computational Linguistics.
Sileye O Ba and Jean-Marc Odobez. 2011. Mul-
tiperson visual focus of attention from head pose
and meeting contextual cues. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
33(1):101–116.
Ryan SJD Baker and Kalina Yacef. 2009. The state
of educational data mining in 2009: A review and
future visions. JEDM-Journal of Educational Data
Mining, 1(1):3–17.
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics-Volume 1, pages 86–90. Associ-
ation for Computational Linguistics.
David Bamman and Noah A Smith. 2015. Contextual-
ized sarcasm detection on twitter. In Ninth Interna-
tional AAAI Conference on Web and Social Media.
Marian Bartlett, Gwen Littlewort, Tingfan Wu, and
Javier Movellan. 2008. Computer expression recog-
nition toolbox. In Automatic Face &amp; Gesture Recog-
nition, 2008. FG’08. 8th IEEE International Confer-
ence on, pages 1–2. IEEE.
Sumit Basu. 2002. Conversational scene analysis.
Ph.D. thesis, MaSSachuSettS InStitute of Technol-
ogy.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1–8.
Jake Y Chen and Stefano Lonardi. 2009. Biological
data mining. CRC Press.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: a content-based
approach to geo-locating twitter users. In Proceed-
ings of the 19th ACM international conference on In-
formation and knowledge management, pages 759–
768. ACM.
D. Das, N. Schneider, D. Chen, and N.A. Smith.
2010. Probabilistic frame-semantic parsing. In
HLT-NAACL 2010, page 948956, Los Angeles, Cal-
ifornia, USA, June.
Richard De Charms. 2013. Personal causation: The
internal affective determinants of behavior. Rout-
ledge.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 1041–1048.
Kenneth Alberto Funes Mora and J Odobez. 2012.
Gaze estimation from multimodal kinect data. In
Computer Vision and Pattern Recognition Work-
shops (CVPRW), 2012 IEEE Computer Society Con-
ference on, pages 25–30. IEEE.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42–47. Association for Computa-
tional Linguistics.
Roberto Gonz´alez-Ib´anez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: a closer look. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers-Volume 2, pages 581–586. Association for
Computational Linguistics.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A Smith. 2014. A dependency parser for
tweets. In EMNLP.
David Lazer, Alex Sandy Pentland, Lada Adamic,
Sinan Aral, Albert Laszlo Barabasi, Devon Brewer,
Nicholas Christakis, Noshir Contractor, James
Fowler, Myron Gutmann, et al. 2009. Life in the
network: the coming age of computational social
science. Science (New York, NY), 323(5915):721.
Omer Levy and Yoav Goldberg. 2014. Dependen-
cybased word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics, volume 2, pages 302–308.
Jiwei Li, Alan Ritter, Claire Cardie, and Eduard Hovy.
2014a. Major life event extraction from twitter
based on congratulations/condolences speech acts.
In Proceedings of Empirical Methods in Natural
Language Processing.
Jiwei Li, Xun Wang, and Eduard Hovy. 2014b. What
a nasty day: Exploring mood-weather relationship
from twitter. In Proceedings of the 23rd ACM
International Conference on Conference on Infor-
mation and Knowledge Management, pages 1309–
1318. ACM.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 176–186, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Alvaro Marcos-Ramiro, Daniel Pizarro-Perez, Marta
Marron-Romera, Laurent Nguyen, and Daniel
</reference>
<page confidence="0.90599">
2562
</page>
<reference confidence="0.997030609090909">
Gatica-Perez. 2013. Body communicative cue ex-
traction for conversational analysis. In Automatic
Face and Gesture Recognition (FG), 2013 10th
IEEE International Conference and Workshops on,
pages 1–8. IEEE.
Manuel Jes´us Marin-Jimenez, Andrew Zisserman,
Marcin Eichner, and Vittorio Ferrari. 2014. Detect-
ing people looking at each other in videos. Interna-
tional Journal of Computer Vision, 106(3):282–296.
Marianne Schmid Mast, Daniel Gatica-Perez, Denise
Frauendorfer, Laurent Nguyen, and Tanzeem
Choudhury. 2015. Social sensing for psy-
chology automated interpersonal behavior assess-
ment. Current Directions in Psychological Science,
24(2):154–160.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Greg Miller. 2011. Social scientists wade into the
tweet stream. Science, 333(6051):1814–1815.
Laurent Nguyen, Jean-Marc Odobez, and Daniel
Gatica-Perez. 2012. Using self-context for multi-
modal detection of head nods in face-to-face inter-
actions. In Proceedings of the 14th ACM interna-
tional conference on Multimodal interaction, pages
289–292. ACM.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02):95–135.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010a.
From tweets to polls: Linking text sentiment to
public opinion time series. ICWSM, 11:122–129.
Brendan O’Connor, Michel Krieger, and David Ahn.
2010b. Tweetmotif: Exploratory search and topic
summarization for twitter. In ICWSM.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Proc
of NAACL.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524–1534. Association for Computational Linguis-
tics.
Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012.
Open domain event extraction from twitter. In Pro-
ceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 1104–1112. ACM.
Jamie Shotton, Toby Sharp, Alex Kipman, Andrew
Fitzgibbon, Mark Finocchio, Andrew Blake, Mat
Cook, and Richard Moore. 2013. Real-time human
pose recognition in parts from single depth images.
Communications of the ACM, 56(1):116–124.
Yanchuan Sim, Noah A. Smith, and David A. Smith.
2012. Discovering factions in the computational lin-
guistics community. In Proceedings of the ACL-
2012 Special Workshop on Rediscovering 50 Years
of Discoveries, ACL ’12 Special Workshop on Re-
discovering 50 Years of Discoveries.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
William Yang Wang, Elijah Mayfield, Suresh Naidu,
and Jeremiah Dittmar. 2012. Historical analysis
of legal opinions with a sparse mixed-effects latent
variable model. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pages 740–749.
Association for Computational Linguistics.
Shiliang Wang, Michael J Paul, and Mark Dredze.
2015. Social media as a sensor of air quality and
public response in china. Journal of medical Inter-
net research, 17(3).
Wei Xu, Alan Ritter, Chris Callison-Burch, William B.
Dolan, and Yangfeng Ji. 2014. Extracting lexi-
cally divergent paraphrases from Twitter. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 2(1).
H Yu, C Ho, Y Juan, and C Lin. 2013. Libshort-
text: A library for short-text classification and analy-
sis. Technical report, Technical Report. http://www.
csie. ntu. edu. tw/˜ cjlin/ papers/libshorttext. pdf.
Ni Zhang, Shelly Campo, Kathleen F Janz, Petya Eck-
ler, Jingzhen Yang, Linda G Snetselaar, and Alessio
Signorini. 2013. Electronic word of mouth on twit-
ter about physical activity in the united states: ex-
ploratory infodemiology study. Journal of medical
Internet research, 15(11).
</reference>
<page confidence="0.871384">
2563
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935935">
<title confidence="0.992450666666667">That’s So Annoying!!!: A Lexical and Embedding Based Data Augmentation Approach to of Annoying Behaviors using</title>
<author confidence="0.999259">Yang Wang</author>
<affiliation confidence="0.994186333333333">Language Technologies Institute School of Computer Science Carnegie Mellon</affiliation>
<abstract confidence="0.998612285714286">We propose a novel data augmentation approach to enhance computational behavioral analysis using social media text. In particular, we collect a Twitter corpus of the descriptions of annoying behaviors usthe In the qualitative analysis, we study the language use in these tweets, with a special focus on the fine-grained categories and the geographic variation of the language. In quantitative analysis, we show that lexical and syntactic features are useful for automatic categorization of annoying behaviors, and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create additional training instances significantly improves the lexical model; and incorporating frame-semantic embedding achieves the best overall performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>30--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6562" citStr="Agarwal et al., 2011" startWordPosition="1004" endWordPosition="1007"> dishonesty .03 complaining .01 hypocrisy .03 indolence .01 incompetence .03 physical .01 interruption .03 punctuality .01 monetary .03 racial .01 sexual .03 religious .01 arrogance .02 selfishness .01 celebrity .02 silence .01 ignorance .02 smoking .01 privacy .02 talkative .01 products .02 weather .01 Table 1: The categories and percentages of annoying behaviors in #petpeeve tweets in our dataset. Note that 17% of the #petpeeve tweets are identified as other unrelated behaviors (not shown). entity recognition (Ritter et al., 2011), summarization (O’Connor et al., 2010b), sentiment analysis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) methods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 12</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, pages 30–38. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sileye O Ba</author>
<author>Jean-Marc Odobez</author>
</authors>
<title>Multiperson visual focus of attention from head pose and meeting contextual cues. Pattern Analysis and Machine Intelligence,</title>
<date>2011</date>
<journal>IEEE Transactions on,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="4652" citStr="Ba and Odobez, 2011" startWordPosition="705" endWordPosition="708">n the next section. The dataset is described in Section 3. We introduce the approach for analyzing #petpeeve Tweets in Section 4. Experimental results are shown in Section 5. We discuss possible applications in Section 6, and conclude in Section 7. 2 Related Work Psychologists, behavioral scientists, and computer scientists have studied a wide-range of methods for behavior extraction (Mast et al., 2015). For example, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to extract self-touch and gestures, while eye gaze (Funes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) </context>
</contexts>
<marker>Ba, Odobez, 2011</marker>
<rawString>Sileye O Ba and Jean-Marc Odobez. 2011. Multiperson visual focus of attention from head pose and meeting contextual cues. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(1):101–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan SJD Baker</author>
<author>Kalina Yacef</author>
</authors>
<title>The state of educational data mining in 2009: A review and future visions.</title>
<date>2009</date>
<journal>JEDM-Journal of Educational Data Mining,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1424" citStr="Baker and Yacef, 2009" startWordPosition="202" endWordPosition="205">for automatic categorization of annoying behaviors, and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create additional training instances significantly improves the lexical model; and incorporating frame-semantic embedding achieves the best overall performance. 1 Introduction In the ever-expanding era of social media, many scientific disciplines, such as health and healthcare, biology, and learning sciences, have adopted computational approaches to exploit patterns and behaviors in large datasets (Wang et al., 2015; Chen and Lonardi, 2009; Baker and Yacef, 2009). In contrast, the primary methods for behavioral sciences still rely on lab experiments with limited amount of subjects, which are time consuming and financially expensive. In addition to this, it is also difficult to obtain a set of samples with geograph∗We understand that many people find long titles annoying, so we intentionally use a very long one to help people understand what “pet peeve” means. Figure 1: An anonymized example of #petpeeve tweets. ical variations in traditional lab-based behavioral experiments. While the social media data are abundantly available, computational approache</context>
</contexts>
<marker>Baker, Yacef, 2009</marker>
<rawString>Ryan SJD Baker and Kalina Yacef. 2009. The state of educational data mining in 2009: A review and future visions. JEDM-Journal of Educational Data Mining, 1(1):3–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14432" citStr="Baker et al., 1998" startWordPosition="2210" endWordPosition="2213">(1) w∈W For each word in a tweet, we query the external embeddings, and replace them with their knn words to create a new training instance. For example, consider the tweet “Being late is terrible” with the punctuality label, after searching for knn words for each token, we create a new training instance: “Be behind are bad” with the same label. Frame-Semantic Embeddings Although lexical (Mikolov et al., 2013a) and dependency based embeddings (Levy and Goldberg, 2014) have been studied, semantic-based embedding is still less understood. We consider the continuous embedding of semantic frames (Baker et al., 1998). To do this, we semantically parsed 3.8 million tweets using SEMAFOR (Das et al., 2010), and built a continuous bag-of-frame model to represent each semantic frame using Word2Vec3. We then use the same data augmentation approach to create additional instances with these semantic frame embeddings. 3https://code.google.com/p/word2vec/ Features Precision Recall F1 Lexical .341 .342 .341 +POS .345 .346 .346 +Dependency* .349 .350 .350 +Semantic Frames* .365 .367 .366 Table 4: Comparing linguistic features for categorizing annoying behaviors. The best results are highlighted in bold.* indicates th</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics-Volume 1, pages 86–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Noah A Smith</author>
</authors>
<title>Contextualized sarcasm detection on twitter.</title>
<date>2015</date>
<booktitle>In Ninth International AAAI Conference on Web and Social Media.</booktitle>
<contexts>
<context position="6930" citStr="Bamman and Smith, 2015" startWordPosition="1063" endWordPosition="1066"> in #petpeeve tweets in our dataset. Note that 17% of the #petpeeve tweets are identified as other unrelated behaviors (not shown). entity recognition (Ritter et al., 2011), summarization (O’Connor et al., 2010b), sentiment analysis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) methods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude information. We extract 3,375 tweets1 with #petpeeve hashtags. We follow past work to annotate the tweets (Ritter et al., 2012; Li et al., 2014a): we apply the LDA clustering + human-identification approach to label the categories of the described annoying behaviors in these tweets. The human annotation process i</context>
</contexts>
<marker>Bamman, Smith, 2015</marker>
<rawString>David Bamman and Noah A Smith. 2015. Contextualized sarcasm detection on twitter. In Ninth International AAAI Conference on Web and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marian Bartlett</author>
<author>Gwen Littlewort</author>
<author>Tingfan Wu</author>
<author>Javier Movellan</author>
</authors>
<title>Computer expression recognition toolbox.</title>
<date>2008</date>
<booktitle>In Automatic Face &amp; Gesture Recognition, 2008. FG’08. 8th IEEE International Conference on,</booktitle>
<pages>1--2</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4955" citStr="Bartlett et al., 2008" startWordPosition="756" endWordPosition="759"> and computer scientists have studied a wide-range of methods for behavior extraction (Mast et al., 2015). For example, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to extract self-touch and gestures, while eye gaze (Funes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion time series to study computational political science. Ritter et al. (2010) study Twitter dialogues using a clustering approach. Bollen et al. (2011) use a sentiment analysis approach to predict the American stock market via Twitter. Li et al. (2014b) have </context>
</contexts>
<marker>Bartlett, Littlewort, Wu, Movellan, 2008</marker>
<rawString>Marian Bartlett, Gwen Littlewort, Tingfan Wu, and Javier Movellan. 2008. Computer expression recognition toolbox. In Automatic Face &amp; Gesture Recognition, 2008. FG’08. 8th IEEE International Conference on, pages 1–2. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumit Basu</author>
</authors>
<title>Conversational scene analysis.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>MaSSachuSettS InStitute of Technology.</institution>
<contexts>
<context position="5031" citStr="Basu, 2002" startWordPosition="770" endWordPosition="771">Mast et al., 2015). For example, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to extract self-touch and gestures, while eye gaze (Funes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion time series to study computational political science. Ritter et al. (2010) study Twitter dialogues using a clustering approach. Bollen et al. (2011) use a sentiment analysis approach to predict the American stock market via Twitter. Li et al. (2014b) have investigated the alignment of Twitter mood with weather for sentiment analys</context>
</contexts>
<marker>Basu, 2002</marker>
<rawString>Sumit Basu. 2002. Conversational scene analysis. Ph.D. thesis, MaSSachuSettS InStitute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiaojun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5447" citStr="Bollen et al. (2011)" startWordPosition="834" endWordPosition="837">re are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion time series to study computational political science. Ritter et al. (2010) study Twitter dialogues using a clustering approach. Bollen et al. (2011) use a sentiment analysis approach to predict the American stock market via Twitter. Li et al. (2014b) have investigated the alignment of Twitter mood with weather for sentiment analysis. In recent years, language technology researchers have focused on developing genre-specific Twitter partof-speech tagging (Gimpel et al., 2011), named Label % Label % appearance .14 services .02 disrespect .06 traffic .02 language .06 advertisement .01 hygiene .05 bragging .01 relationship .05 children .01 dishonesty .03 complaining .01 hypocrisy .03 indolence .01 incompetence .03 physical .01 interruption .03</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jake Y Chen</author>
<author>Stefano Lonardi</author>
</authors>
<title>Biological data mining.</title>
<date>2009</date>
<publisher>CRC Press.</publisher>
<contexts>
<context position="1400" citStr="Chen and Lonardi, 2009" startWordPosition="198" endWordPosition="201">tic features are useful for automatic categorization of annoying behaviors, and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create additional training instances significantly improves the lexical model; and incorporating frame-semantic embedding achieves the best overall performance. 1 Introduction In the ever-expanding era of social media, many scientific disciplines, such as health and healthcare, biology, and learning sciences, have adopted computational approaches to exploit patterns and behaviors in large datasets (Wang et al., 2015; Chen and Lonardi, 2009; Baker and Yacef, 2009). In contrast, the primary methods for behavioral sciences still rely on lab experiments with limited amount of subjects, which are time consuming and financially expensive. In addition to this, it is also difficult to obtain a set of samples with geograph∗We understand that many people find long titles annoying, so we intentionally use a very long one to help people understand what “pet peeve” means. Figure 1: An anonymized example of #petpeeve tweets. ical variations in traditional lab-based behavioral experiments. While the social media data are abundantly available,</context>
</contexts>
<marker>Chen, Lonardi, 2009</marker>
<rawString>Jake Y Chen and Stefano Lonardi. 2009. Biological data mining. CRC Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Cheng</author>
<author>James Caverlee</author>
<author>Kyumin Lee</author>
</authors>
<title>You are where you tweet: a content-based approach to geo-locating twitter users.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>759--768</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2666" citStr="Cheng et al., 2010" startWordPosition="394" endWordPosition="397">s using Twitter are not well-studied. Even when statistical techniques are applied to these tasks, their concentration has been on simple statistical significance tests and descriptive statistics (De Charms, 2013; Zhang et al., 2013). Therefore, we believe that statistical natural language processing techniques are needed for insightful analysis and interpretation in behavioral studies. In this paper, we use Twitter as a corpus for computational behavioral science. More specifically, we focus on a case study of analyzing annoying behaviors. To do this, we exploit a corpus of 9 million tweets (Cheng et al., 2010), and extract the tweets that describe these behaviors using the #petpeeve hashtags. #petpeeve is a popular Twitter hashtag, which describes behaviors that might be annoying to others. An example of #petpeeve tweets is shown in Figure 1. To facilitate the analysis, we manually annotate 3,375 tweets with 60 fine-grained categories, which will be described in Section 3. We use a sparse mixedeffects topic model to analyze the salient words in each category, as well as the geographic variations. We show that lexical, syntactic, and semantic features enhance the automatic categorization of annoying</context>
<context position="7132" citStr="Cheng et al., 2010" startWordPosition="1098" endWordPosition="1101">2010b), sentiment analysis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) methods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude information. We extract 3,375 tweets1 with #petpeeve hashtags. We follow past work to annotate the tweets (Ritter et al., 2012; Li et al., 2014a): we apply the LDA clustering + human-identification approach to label the categories of the described annoying behaviors in these tweets. The human annotation process includes two stages: first, the annotators identify the 50 categories from the clustering process, and use these topics as a candi1http://www.cs.cmu.edu/˜yww/data/petpeeves.zip 2558 date label set to ann</context>
</contexts>
<marker>Cheng, Caverlee, Lee, 2010</marker>
<rawString>Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010. You are where you tweet: a content-based approach to geo-locating twitter users. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 759– 768. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N Schneider</author>
<author>D Chen</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing.</title>
<date>2010</date>
<booktitle>In HLT-NAACL 2010,</booktitle>
<pages>948956</pages>
<location>Los Angeles, California, USA,</location>
<contexts>
<context position="10880" citStr="Das et al., 2010" startWordPosition="1662" endWordPosition="1665"> We formulate the problem as a multiclass classification task, and consider the following feature sets: • Lexical Features: we extract unigrams as surface-level lexical features. • Part-of-Speech Features: to model shallow syntactic cues, we extract lexicalized part-ofspeech features using the Stanford part-ofspeech tagger (Toutanova et al., 2003). • Dependency Triples: to better understand the deeper syntactic dependencies of keywords in tweets, we have also extracted typed dependency triples (e.g., nsubj(hatej)) using the MaltParser (Nivre et al., 2007). • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-ofthe-art frame-semantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Embeddings for Data Augmentation Since the Twitter messages are often short and noisy, and the training data is relatively scarce for each class, we consider the feasibility of leveraging external resources, in particular, continuous word embeddings (Mikolov et al., 2013a) to enhance the multiclass text categorization model. Two major challenges for leveraging word embeddings for tweet classification are: 1) because word embeddings are continuo</context>
<context position="14520" citStr="Das et al., 2010" startWordPosition="2225" endWordPosition="2228">their knn words to create a new training instance. For example, consider the tweet “Being late is terrible” with the punctuality label, after searching for knn words for each token, we create a new training instance: “Be behind are bad” with the same label. Frame-Semantic Embeddings Although lexical (Mikolov et al., 2013a) and dependency based embeddings (Levy and Goldberg, 2014) have been studied, semantic-based embedding is still less understood. We consider the continuous embedding of semantic frames (Baker et al., 1998). To do this, we semantically parsed 3.8 million tweets using SEMAFOR (Das et al., 2010), and built a continuous bag-of-frame model to represent each semantic frame using Word2Vec3. We then use the same data augmentation approach to create additional instances with these semantic frame embeddings. 3https://code.google.com/p/word2vec/ Features Precision Recall F1 Lexical .341 .342 .341 +POS .345 .346 .346 +Dependency* .349 .350 .350 +Semantic Frames* .365 .367 .366 Table 4: Comparing linguistic features for categorizing annoying behaviors. The best results are highlighted in bold.* indicates that the result is significantly better than the lexical baseline (p &lt; .0001). 5 Experimen</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>D. Das, N. Schneider, D. Chen, and N.A. Smith. 2010. Probabilistic frame-semantic parsing. In HLT-NAACL 2010, page 948956, Los Angeles, California, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard De Charms</author>
</authors>
<title>Personal causation: The internal affective determinants of behavior.</title>
<date>2013</date>
<publisher>Routledge.</publisher>
<marker>De Charms, 2013</marker>
<rawString>Richard De Charms. 2013. Personal causation: The internal affective determinants of behavior. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>1041--1048</pages>
<contexts>
<context position="8945" citStr="Eisenstein et al., 2011" startWordPosition="1382" endWordPosition="1385">s for training, and 40% for testing. 4 Our Approach In this section, we describe our methods for the qualitative and quantitative analyses. In particular, we briefly review a supervised approach of using sparse mixed-effects topic model to visualize the topical words to analyze this behavior data. For the quantitative task of automatic categorization of tweets, we propose a novel approach to create additional training data, using continuous lexical and semantic representations. 4.1 Supervised Topic Modeling To analyze the salient words for each category of annoying behaviors, we utilize SAGE (Eisenstein et al., 2011), a state-of-the-art mixed-effect topic model, which has been used in several NLP applications (Sim et al., 2012; Wang et al., 2012). SAGE is ideal for our text analytic purposes, because it is supervised, and it builds relatively clean topic models by considering the additive effects and the background distribution of words. Therefore, we can use SAGE to visualize the salient words for each category of annoying behaviors using the 3,375 #petpeeve tweets. Each tweet is treated as a document, and we use Markov Chain Monte Carlo for inference. To facilitate the geographical analysis, we use Goog</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011. Sparse additive generative models of text. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1041–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Alberto Funes Mora</author>
<author>J Odobez</author>
</authors>
<title>Gaze estimation from multimodal kinect data.</title>
<date>2012</date>
<booktitle>In Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on,</booktitle>
<pages>25--30</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4619" citStr="Mora and Odobez, 2012" startWordPosition="699" endWordPosition="702">ehaviors. We outline related work in the next section. The dataset is described in Section 3. We introduce the approach for analyzing #petpeeve Tweets in Section 4. Experimental results are shown in Section 5. We discuss possible applications in Section 6, and conclude in Section 7. 2 Related Work Psychologists, behavioral scientists, and computer scientists have studied a wide-range of methods for behavior extraction (Mast et al., 2015). For example, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to extract self-touch and gestures, while eye gaze (Funes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For </context>
</contexts>
<marker>Mora, Odobez, 2012</marker>
<rawString>Kenneth Alberto Funes Mora and J Odobez. 2012. Gaze estimation from multimodal kinect data. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2012 IEEE Computer Society Conference on, pages 25–30. IEEE.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papersVolume 2, pages 42–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Gonz´alez-Ib´anez</author>
<author>Smaranda Muresan</author>
<author>Nina Wacholder</author>
</authors>
<title>Identifying sarcasm in twitter: a closer look.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2,</booktitle>
<pages>581--586</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gonz´alez-Ib´anez, Muresan, Wacholder, 2011</marker>
<rawString>Roberto Gonz´alez-Ib´anez, Smaranda Muresan, and Nina Wacholder. 2011. Identifying sarcasm in twitter: a closer look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 581–586. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
<author>Archna Bhatia</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6737" citStr="Kong et al., 2014" startWordPosition="1032" endWordPosition="1035">nce .02 selfishness .01 celebrity .02 silence .01 ignorance .02 smoking .01 privacy .02 talkative .01 products .02 weather .01 Table 1: The categories and percentages of annoying behaviors in #petpeeve tweets in our dataset. Note that 17% of the #petpeeve tweets are identified as other unrelated behaviors (not shown). entity recognition (Ritter et al., 2011), summarization (O’Connor et al., 2010b), sentiment analysis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) methods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude information. We extract 3,375 tweets1 with #petpeeve hashtags. We follow past work to annotate the tweets (Ritter et al.,</context>
</contexts>
<marker>Kong, Schneider, Swayamdipta, Bhatia, Dyer, Smith, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A Smith. 2014. A dependency parser for tweets. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lazer</author>
<author>Alex Sandy Pentland</author>
</authors>
<title>Lada Adamic, Sinan Aral, Albert Laszlo Barabasi,</title>
<date>2009</date>
<publisher>Science</publisher>
<location>Devon</location>
<marker>Lazer, Pentland, 2009</marker>
<rawString>David Lazer, Alex Sandy Pentland, Lada Adamic, Sinan Aral, Albert Laszlo Barabasi, Devon Brewer, Nicholas Christakis, Noshir Contractor, James Fowler, Myron Gutmann, et al. 2009. Life in the network: the coming age of computational social science. Science (New York, NY), 323(5915):721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>302--308</pages>
<contexts>
<context position="14285" citStr="Levy and Goldberg, 2014" startWordPosition="2188" endWordPosition="2191">ch for the k-nearest-neighbor (knn) word w for a query term using cosine similarity between query Q� and target word vectors W�: arg max cosine((j, W�) (1) w∈W For each word in a tweet, we query the external embeddings, and replace them with their knn words to create a new training instance. For example, consider the tweet “Being late is terrible” with the punctuality label, after searching for knn words for each token, we create a new training instance: “Be behind are bad” with the same label. Frame-Semantic Embeddings Although lexical (Mikolov et al., 2013a) and dependency based embeddings (Levy and Goldberg, 2014) have been studied, semantic-based embedding is still less understood. We consider the continuous embedding of semantic frames (Baker et al., 1998). To do this, we semantically parsed 3.8 million tweets using SEMAFOR (Das et al., 2010), and built a continuous bag-of-frame model to represent each semantic frame using Word2Vec3. We then use the same data augmentation approach to create additional instances with these semantic frame embeddings. 3https://code.google.com/p/word2vec/ Features Precision Recall F1 Lexical .341 .342 .341 +POS .345 .346 .346 +Dependency* .349 .350 .350 +Semantic Frames*</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 2, pages 302–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Alan Ritter</author>
<author>Claire Cardie</author>
<author>Eduard Hovy</author>
</authors>
<title>Major life event extraction from twitter based on congratulations/condolences speech acts.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5547" citStr="Li et al. (2014" startWordPosition="853" endWordPosition="856">(Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion time series to study computational political science. Ritter et al. (2010) study Twitter dialogues using a clustering approach. Bollen et al. (2011) use a sentiment analysis approach to predict the American stock market via Twitter. Li et al. (2014b) have investigated the alignment of Twitter mood with weather for sentiment analysis. In recent years, language technology researchers have focused on developing genre-specific Twitter partof-speech tagging (Gimpel et al., 2011), named Label % Label % appearance .14 services .02 disrespect .06 traffic .02 language .06 advertisement .01 hygiene .05 bragging .01 relationship .05 children .01 dishonesty .03 complaining .01 hypocrisy .03 indolence .01 incompetence .03 physical .01 interruption .03 punctuality .01 monetary .03 racial .01 sexual .03 religious .01 arrogance .02 selfishness .01 cele</context>
<context position="7359" citStr="Li et al., 2014" startWordPosition="1137" endWordPosition="1140">s. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude information. We extract 3,375 tweets1 with #petpeeve hashtags. We follow past work to annotate the tweets (Ritter et al., 2012; Li et al., 2014a): we apply the LDA clustering + human-identification approach to label the categories of the described annoying behaviors in these tweets. The human annotation process includes two stages: first, the annotators identify the 50 categories from the clustering process, and use these topics as a candi1http://www.cs.cmu.edu/˜yww/data/petpeeves.zip 2558 date label set to annotate the data; in the second stage, the categories are refined (to 60 classes) from the first pass, and the data is re-annotated with the refined human-specified category labels. Due to the complexity of this fine-grained anno</context>
</contexts>
<marker>Li, Ritter, Cardie, Hovy, 2014</marker>
<rawString>Jiwei Li, Alan Ritter, Claire Cardie, and Eduard Hovy. 2014a. Major life event extraction from twitter based on congratulations/condolences speech acts. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Xun Wang</author>
<author>Eduard Hovy</author>
</authors>
<title>What a nasty day: Exploring mood-weather relationship from twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,</booktitle>
<pages>1309--1318</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5547" citStr="Li et al. (2014" startWordPosition="853" endWordPosition="856">(Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion time series to study computational political science. Ritter et al. (2010) study Twitter dialogues using a clustering approach. Bollen et al. (2011) use a sentiment analysis approach to predict the American stock market via Twitter. Li et al. (2014b) have investigated the alignment of Twitter mood with weather for sentiment analysis. In recent years, language technology researchers have focused on developing genre-specific Twitter partof-speech tagging (Gimpel et al., 2011), named Label % Label % appearance .14 services .02 disrespect .06 traffic .02 language .06 advertisement .01 hygiene .05 bragging .01 relationship .05 children .01 dishonesty .03 complaining .01 hypocrisy .03 indolence .01 incompetence .03 physical .01 interruption .03 punctuality .01 monetary .03 racial .01 sexual .03 religious .01 arrogance .02 selfishness .01 cele</context>
<context position="7359" citStr="Li et al., 2014" startWordPosition="1137" endWordPosition="1140">s. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude information. We extract 3,375 tweets1 with #petpeeve hashtags. We follow past work to annotate the tweets (Ritter et al., 2012; Li et al., 2014a): we apply the LDA clustering + human-identification approach to label the categories of the described annoying behaviors in these tweets. The human annotation process includes two stages: first, the annotators identify the 50 categories from the clustering process, and use these topics as a candi1http://www.cs.cmu.edu/˜yww/data/petpeeves.zip 2558 date label set to annotate the data; in the second stage, the categories are refined (to 60 classes) from the first pass, and the data is re-annotated with the refined human-specified category labels. Due to the complexity of this fine-grained anno</context>
</contexts>
<marker>Li, Wang, Hovy, 2014</marker>
<rawString>Jiwei Li, Xun Wang, and Eduard Hovy. 2014b. What a nasty day: Exploring mood-weather relationship from twitter. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1309– 1318. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Guang Xiang</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Microblogs as parallel corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>176--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6693" citStr="Ling et al., 2013" startWordPosition="1025" endWordPosition="1028">3 racial .01 sexual .03 religious .01 arrogance .02 selfishness .01 celebrity .02 silence .01 ignorance .02 smoking .01 privacy .02 talkative .01 products .02 weather .01 Table 1: The categories and percentages of annoying behaviors in #petpeeve tweets in our dataset. Note that 17% of the #petpeeve tweets are identified as other unrelated behaviors (not shown). entity recognition (Ritter et al., 2011), summarization (O’Connor et al., 2010b), sentiment analysis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) methods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude information. We extract 3,375 tweets1 with #petpeeve hashtags. We follow past</context>
</contexts>
<marker>Ling, Xiang, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and Isabel Trancoso. 2013. Microblogs as parallel corpora. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 176–186, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro Marcos-Ramiro</author>
<author>Daniel Pizarro-Perez</author>
<author>Marta Marron-Romera</author>
<author>Laurent Nguyen</author>
<author>Daniel Gatica-Perez</author>
</authors>
<title>Body communicative cue extraction for conversational analysis.</title>
<date>2013</date>
<booktitle>In Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on,</booktitle>
<pages>1--8</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4523" citStr="Marcos-Ramiro et al., 2013" startWordPosition="681" endWordPosition="684">linguistic features and a novel data augmentation approach for automatic categorization of annoying behaviors. We outline related work in the next section. The dataset is described in Section 3. We introduce the approach for analyzing #petpeeve Tweets in Section 4. Experimental results are shown in Section 5. We discuss possible applications in Section 6, and conclude in Section 7. 2 Related Work Psychologists, behavioral scientists, and computer scientists have studied a wide-range of methods for behavior extraction (Mast et al., 2015). For example, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to extract self-touch and gestures, while eye gaze (Funes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter h</context>
</contexts>
<marker>Marcos-Ramiro, Pizarro-Perez, Marron-Romera, Nguyen, Gatica-Perez, 2013</marker>
<rawString>Alvaro Marcos-Ramiro, Daniel Pizarro-Perez, Marta Marron-Romera, Laurent Nguyen, and Daniel Gatica-Perez. 2013. Body communicative cue extraction for conversational analysis. In Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on, pages 1–8. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Jes´us Marin-Jimenez</author>
<author>Andrew Zisserman</author>
<author>Marcin Eichner</author>
<author>Vittorio Ferrari</author>
</authors>
<title>Detecting people looking at each other in videos.</title>
<date>2014</date>
<journal>International Journal of Computer Vision,</journal>
<volume>106</volume>
<issue>3</issue>
<marker>Marin-Jimenez, Zisserman, Eichner, Ferrari, 2014</marker>
<rawString>Manuel Jes´us Marin-Jimenez, Andrew Zisserman, Marcin Eichner, and Vittorio Ferrari. 2014. Detecting people looking at each other in videos. International Journal of Computer Vision, 106(3):282–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marianne Schmid Mast</author>
<author>Daniel Gatica-Perez</author>
<author>Denise Frauendorfer</author>
<author>Laurent Nguyen</author>
<author>Tanzeem Choudhury</author>
</authors>
<title>Social sensing for psychology automated interpersonal behavior assessment.</title>
<date>2015</date>
<booktitle>Current Directions in Psychological Science,</booktitle>
<pages>24--2</pages>
<contexts>
<context position="4438" citStr="Mast et al., 2015" startWordPosition="667" endWordPosition="670">ith a focus on the topics and geographical variations; • We propose various linguistic features and a novel data augmentation approach for automatic categorization of annoying behaviors. We outline related work in the next section. The dataset is described in Section 3. We introduce the approach for analyzing #petpeeve Tweets in Section 4. Experimental results are shown in Section 5. We discuss possible applications in Section 6, and conclude in Section 7. 2 Related Work Psychologists, behavioral scientists, and computer scientists have studied a wide-range of methods for behavior extraction (Mast et al., 2015). For example, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to extract self-touch and gestures, while eye gaze (Funes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With </context>
</contexts>
<marker>Mast, Gatica-Perez, Frauendorfer, Nguyen, Choudhury, 2015</marker>
<rawString>Marianne Schmid Mast, Daniel Gatica-Perez, Denise Frauendorfer, Laurent Nguyen, and Tanzeem Choudhury. 2015. Social sensing for psychology automated interpersonal behavior assessment. Current Directions in Psychological Science, 24(2):154–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="11303" citStr="Mikolov et al., 2013" startWordPosition="1722" endWordPosition="1725">dencies of keywords in tweets, we have also extracted typed dependency triples (e.g., nsubj(hatej)) using the MaltParser (Nivre et al., 2007). • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-ofthe-art frame-semantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Embeddings for Data Augmentation Since the Twitter messages are often short and noisy, and the training data is relatively scarce for each class, we consider the feasibility of leveraging external resources, in particular, continuous word embeddings (Mikolov et al., 2013a) to enhance the multiclass text categorization model. Two major challenges for leveraging word embeddings for tweet classification are: 1) because word embeddings are continuous, it is difficult to fuse them with other discrete syntactic and semantic features; 2) it is not straightforward how one should transform the word-level representation to the tweet-level representation. In our preliminary experiments, we have evaluated the continuous word representation method (Turian et al., 2010), as well as incorporating neighboring words in the embeddings as additional features, but both methods f</context>
<context position="14225" citStr="Mikolov et al., 2013" startWordPosition="2180" endWordPosition="2183">ore specifically, in the embedding vocabulary W, we search for the k-nearest-neighbor (knn) word w for a query term using cosine similarity between query Q� and target word vectors W�: arg max cosine((j, W�) (1) w∈W For each word in a tweet, we query the external embeddings, and replace them with their knn words to create a new training instance. For example, consider the tweet “Being late is terrible” with the punctuality label, after searching for knn words for each token, we create a new training instance: “Be behind are bad” with the same label. Frame-Semantic Embeddings Although lexical (Mikolov et al., 2013a) and dependency based embeddings (Levy and Goldberg, 2014) have been studied, semantic-based embedding is still less understood. We consider the continuous embedding of semantic frames (Baker et al., 1998). To do this, we semantically parsed 3.8 million tweets using SEMAFOR (Das et al., 2010), and built a continuous bag-of-frame model to represent each semantic frame using Word2Vec3. We then use the same data augmentation approach to create additional instances with these semantic frame embeddings. 3https://code.google.com/p/word2vec/ Features Precision Recall F1 Lexical .341 .342 .341 +POS </context>
<context position="17942" citStr="Mikolov et al., 2013" startWordPosition="2764" endWordPosition="2767">lect the best hyper-parameter using the training data only. A final classifier is then trained using the best hyper-parameters and test set results are reported. We set k = 5 for knn in our data augmentation experiments: the training data is expanded to 5 times of the original size. We use a paired two-tailed student’s t test to assess the statistical significance. Word2Vec is used to train various lexical and semantic embedding models. We consider three lexical embeddings and one frame-semantic embeddings for data augmentation: 1) GoogleNews Lexical Embeddings trained with 100 billion words (Mikolov et al., 2013b); 2) Twitter Lexical Embeddings trained with 51 million of words; 3) Urban Dictionary lexical embeddings trained with 53 million of words from slang definitions and examples; 4) Twitter Semantic Frame Embeddings trained with 27 million frames. Varying Feature Sets We compare various features in Table 4. We see that adding shallow partof-speech features does not have a strong effect on the performance, but adding the dependency triples significantly outperforms the lexical baseline. We see that the semantic frames are particular useful, showing a 7% relative improvement over the baseline. The</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="11303" citStr="Mikolov et al., 2013" startWordPosition="1722" endWordPosition="1725">dencies of keywords in tweets, we have also extracted typed dependency triples (e.g., nsubj(hatej)) using the MaltParser (Nivre et al., 2007). • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-ofthe-art frame-semantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Embeddings for Data Augmentation Since the Twitter messages are often short and noisy, and the training data is relatively scarce for each class, we consider the feasibility of leveraging external resources, in particular, continuous word embeddings (Mikolov et al., 2013a) to enhance the multiclass text categorization model. Two major challenges for leveraging word embeddings for tweet classification are: 1) because word embeddings are continuous, it is difficult to fuse them with other discrete syntactic and semantic features; 2) it is not straightforward how one should transform the word-level representation to the tweet-level representation. In our preliminary experiments, we have evaluated the continuous word representation method (Turian et al., 2010), as well as incorporating neighboring words in the embeddings as additional features, but both methods f</context>
<context position="14225" citStr="Mikolov et al., 2013" startWordPosition="2180" endWordPosition="2183">ore specifically, in the embedding vocabulary W, we search for the k-nearest-neighbor (knn) word w for a query term using cosine similarity between query Q� and target word vectors W�: arg max cosine((j, W�) (1) w∈W For each word in a tweet, we query the external embeddings, and replace them with their knn words to create a new training instance. For example, consider the tweet “Being late is terrible” with the punctuality label, after searching for knn words for each token, we create a new training instance: “Be behind are bad” with the same label. Frame-Semantic Embeddings Although lexical (Mikolov et al., 2013a) and dependency based embeddings (Levy and Goldberg, 2014) have been studied, semantic-based embedding is still less understood. We consider the continuous embedding of semantic frames (Baker et al., 1998). To do this, we semantically parsed 3.8 million tweets using SEMAFOR (Das et al., 2010), and built a continuous bag-of-frame model to represent each semantic frame using Word2Vec3. We then use the same data augmentation approach to create additional instances with these semantic frame embeddings. 3https://code.google.com/p/word2vec/ Features Precision Recall F1 Lexical .341 .342 .341 +POS </context>
<context position="17942" citStr="Mikolov et al., 2013" startWordPosition="2764" endWordPosition="2767">lect the best hyper-parameter using the training data only. A final classifier is then trained using the best hyper-parameters and test set results are reported. We set k = 5 for knn in our data augmentation experiments: the training data is expanded to 5 times of the original size. We use a paired two-tailed student’s t test to assess the statistical significance. Word2Vec is used to train various lexical and semantic embedding models. We consider three lexical embeddings and one frame-semantic embeddings for data augmentation: 1) GoogleNews Lexical Embeddings trained with 100 billion words (Mikolov et al., 2013b); 2) Twitter Lexical Embeddings trained with 51 million of words; 3) Urban Dictionary lexical embeddings trained with 53 million of words from slang definitions and examples; 4) Twitter Semantic Frame Embeddings trained with 27 million frames. Varying Feature Sets We compare various features in Table 4. We see that adding shallow partof-speech features does not have a strong effect on the performance, but adding the dependency triples significantly outperforms the lexical baseline. We see that the semantic frames are particular useful, showing a 7% relative improvement over the baseline. The</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Miller</author>
</authors>
<title>Social scientists wade into the tweet stream.</title>
<date>2011</date>
<journal>Science,</journal>
<volume>333</volume>
<issue>6051</issue>
<contexts>
<context position="5213" citStr="Miller, 2011" startWordPosition="798" endWordPosition="799">a and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion time series to study computational political science. Ritter et al. (2010) study Twitter dialogues using a clustering approach. Bollen et al. (2011) use a sentiment analysis approach to predict the American stock market via Twitter. Li et al. (2014b) have investigated the alignment of Twitter mood with weather for sentiment analysis. In recent years, language technology researchers have focused on developing genre-specific Twitter partof-speech tagging (Gimpel et al., 2011), named Label % Label % appearance .</context>
</contexts>
<marker>Miller, 2011</marker>
<rawString>Greg Miller. 2011. Social scientists wade into the tweet stream. Science, 333(6051):1814–1815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Nguyen</author>
<author>Jean-Marc Odobez</author>
<author>Daniel Gatica-Perez</author>
</authors>
<title>Using self-context for multimodal detection of head nods in face-to-face interactions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 14th ACM international conference on Multimodal interaction,</booktitle>
<pages>289--292</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4700" citStr="Nguyen et al., 2012" startWordPosition="713" endWordPosition="716">Section 3. We introduce the approach for analyzing #petpeeve Tweets in Section 4. Experimental results are shown in Section 5. We discuss possible applications in Section 6, and conclude in Section 7. 2 Related Work Psychologists, behavioral scientists, and computer scientists have studied a wide-range of methods for behavior extraction (Mast et al., 2015). For example, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to extract self-touch and gestures, while eye gaze (Funes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion t</context>
</contexts>
<marker>Nguyen, Odobez, Gatica-Perez, 2012</marker>
<rawString>Laurent Nguyen, Jean-Marc Odobez, and Daniel Gatica-Perez. 2012. Using self-context for multimodal detection of head nods in face-to-face interactions. In Proceedings of the 14th ACM international conference on Multimodal interaction, pages 289–292. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="10824" citStr="Nivre et al., 2007" startWordPosition="1653" endWordPosition="1656">predict tweets that describe different annoying behaviors? We formulate the problem as a multiclass classification task, and consider the following feature sets: • Lexical Features: we extract unigrams as surface-level lexical features. • Part-of-Speech Features: to model shallow syntactic cues, we extract lexicalized part-ofspeech features using the Stanford part-ofspeech tagger (Toutanova et al., 2003). • Dependency Triples: to better understand the deeper syntactic dependencies of keywords in tweets, we have also extracted typed dependency triples (e.g., nsubj(hatej)) using the MaltParser (Nivre et al., 2007). • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-ofthe-art frame-semantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Embeddings for Data Augmentation Since the Twitter messages are often short and noisy, and the training data is relatively scarce for each class, we consider the feasibility of leveraging external resources, in particular, continuous word embeddings (Mikolov et al., 2013a) to enhance the multiclass text categorization model. Two major challenges for leveraging word embeddings for tweet cla</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<journal>ICWSM,</journal>
<pages>11--122</pages>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R Routledge, and Noah A Smith. 2010a. From tweets to polls: Linking text sentiment to public opinion time series. ICWSM, 11:122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Michel Krieger</author>
<author>David Ahn</author>
</authors>
<title>Tweetmotif: Exploratory search and topic summarization for twitter.</title>
<date>2010</date>
<booktitle>In ICWSM.</booktitle>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>Brendan O’Connor, Michel Krieger, and David Ahn. 2010b. Tweetmotif: Exploratory search and topic summarization for twitter. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<booktitle>In Proc of NAACL.</booktitle>
<contexts>
<context position="5373" citStr="Ritter et al. (2010)" startWordPosition="822" endWordPosition="825">be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion time series to study computational political science. Ritter et al. (2010) study Twitter dialogues using a clustering approach. Bollen et al. (2011) use a sentiment analysis approach to predict the American stock market via Twitter. Li et al. (2014b) have investigated the alignment of Twitter mood with weather for sentiment analysis. In recent years, language technology researchers have focused on developing genre-specific Twitter partof-speech tagging (Gimpel et al., 2011), named Label % Label % appearance .14 services .02 disrespect .06 traffic .02 language .06 advertisement .01 hygiene .05 bragging .01 relationship .05 children .01 dishonesty .03 complaining .01 </context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations. In Proc of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1524--1534</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6479" citStr="Ritter et al., 2011" startWordPosition="991" endWordPosition="994">guage .06 advertisement .01 hygiene .05 bragging .01 relationship .05 children .01 dishonesty .03 complaining .01 hypocrisy .03 indolence .01 incompetence .03 physical .01 interruption .03 punctuality .01 monetary .03 racial .01 sexual .03 religious .01 arrogance .02 selfishness .01 celebrity .02 silence .01 ignorance .02 smoking .01 privacy .02 talkative .01 products .02 weather .01 Table 1: The categories and percentages of annoying behaviors in #petpeeve tweets in our dataset. Note that 17% of the #petpeeve tweets are identified as other unrelated behaviors (not shown). entity recognition (Ritter et al., 2011), summarization (O’Connor et al., 2010b), sentiment analysis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) methods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled </context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1524–1534. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni</author>
<author>Sam Clark</author>
</authors>
<title>Open domain event extraction from twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>1104--1112</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6601" citStr="Ritter et al., 2012" startWordPosition="1010" endWordPosition="1013"> .03 indolence .01 incompetence .03 physical .01 interruption .03 punctuality .01 monetary .03 racial .01 sexual .03 religious .01 arrogance .02 selfishness .01 celebrity .02 silence .01 ignorance .02 smoking .01 privacy .02 talkative .01 products .02 weather .01 Table 1: The categories and percentages of annoying behaviors in #petpeeve tweets in our dataset. Note that 17% of the #petpeeve tweets are identified as other unrelated behaviors (not shown). entity recognition (Ritter et al., 2011), summarization (O’Connor et al., 2010b), sentiment analysis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) methods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude</context>
</contexts>
<marker>Ritter, Etzioni, Clark, 2012</marker>
<rawString>Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012. Open domain event extraction from twitter. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1104–1112. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Shotton</author>
<author>Toby Sharp</author>
<author>Alex Kipman</author>
<author>Andrew Fitzgibbon</author>
<author>Mark Finocchio</author>
<author>Andrew Blake</author>
<author>Mat Cook</author>
<author>Richard Moore</author>
</authors>
<title>Real-time human pose recognition in parts from single depth images.</title>
<date>2013</date>
<journal>Communications of the ACM,</journal>
<volume>56</volume>
<issue>1</issue>
<contexts>
<context position="4743" citStr="Shotton et al., 2013" startWordPosition="720" endWordPosition="723">nalyzing #petpeeve Tweets in Section 4. Experimental results are shown in Section 5. We discuss possible applications in Section 6, and conclude in Section 7. 2 Related Work Psychologists, behavioral scientists, and computer scientists have studied a wide-range of methods for behavior extraction (Mast et al., 2015). For example, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to extract self-touch and gestures, while eye gaze (Funes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gazing, nodding, and arm-related behaviors. There are also significant amount of studies of extracting facial and speech features to understand smiling (Bartlett et al., 2008), eye contact (MarinJimenez et al., 2014), and verbal behaviors (Basu, 2002). With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For example, O’Connor et al. (2010a) align the Twitter messages with public opinion time series to study computational political</context>
</contexts>
<marker>Shotton, Sharp, Kipman, Fitzgibbon, Finocchio, Blake, Cook, Moore, 2013</marker>
<rawString>Jamie Shotton, Toby Sharp, Alex Kipman, Andrew Fitzgibbon, Mark Finocchio, Andrew Blake, Mat Cook, and Richard Moore. 2013. Real-time human pose recognition in parts from single depth images. Communications of the ACM, 56(1):116–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanchuan Sim</author>
<author>Noah A Smith</author>
<author>David A Smith</author>
</authors>
<title>Discovering factions in the computational linguistics community.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL2012 Special Workshop on Rediscovering 50 Years of Discoveries, ACL ’12 Special Workshop on Rediscovering 50 Years of Discoveries.</booktitle>
<contexts>
<context position="9057" citStr="Sim et al., 2012" startWordPosition="1400" endWordPosition="1403">ntitative analyses. In particular, we briefly review a supervised approach of using sparse mixed-effects topic model to visualize the topical words to analyze this behavior data. For the quantitative task of automatic categorization of tweets, we propose a novel approach to create additional training data, using continuous lexical and semantic representations. 4.1 Supervised Topic Modeling To analyze the salient words for each category of annoying behaviors, we utilize SAGE (Eisenstein et al., 2011), a state-of-the-art mixed-effect topic model, which has been used in several NLP applications (Sim et al., 2012; Wang et al., 2012). SAGE is ideal for our text analytic purposes, because it is supervised, and it builds relatively clean topic models by considering the additive effects and the background distribution of words. Therefore, we can use SAGE to visualize the salient words for each category of annoying behaviors using the 3,375 #petpeeve tweets. Each tweet is treated as a document, and we use Markov Chain Monte Carlo for inference. To facilitate the geographical analysis, we use Google’s reverse geocoding service to extract the state information from coordinates, and apply SAGE for visualizati</context>
</contexts>
<marker>Sim, Smith, Smith, 2012</marker>
<rawString>Yanchuan Sim, Noah A. Smith, and David A. Smith. 2012. Discovering factions in the computational linguistics community. In Proceedings of the ACL2012 Special Workshop on Rediscovering 50 Years of Discoveries, ACL ’12 Special Workshop on Rediscovering 50 Years of Discoveries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10612" citStr="Toutanova et al., 2003" startWordPosition="1622" endWordPosition="1625">, time-wasting, ungratefulness, and others. 4.2 Embedding-Based Data Augmentation for Automatic Categorization of Tweets In addition to the visualization task, we also ask the question: can we use linguistic cues to predict tweets that describe different annoying behaviors? We formulate the problem as a multiclass classification task, and consider the following feature sets: • Lexical Features: we extract unigrams as surface-level lexical features. • Part-of-Speech Features: to model shallow syntactic cues, we extract lexicalized part-ofspeech features using the Stanford part-ofspeech tagger (Toutanova et al., 2003). • Dependency Triples: to better understand the deeper syntactic dependencies of keywords in tweets, we have also extracted typed dependency triples (e.g., nsubj(hatej)) using the MaltParser (Nivre et al., 2007). • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-ofthe-art frame-semantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Embeddings for Data Augmentation Since the Twitter messages are often short and noisy, and the training data is relatively scarce for each class, we consider the feasibility of lev</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11798" citStr="Turian et al., 2010" startWordPosition="1797" endWordPosition="1800">we consider the feasibility of leveraging external resources, in particular, continuous word embeddings (Mikolov et al., 2013a) to enhance the multiclass text categorization model. Two major challenges for leveraging word embeddings for tweet classification are: 1) because word embeddings are continuous, it is difficult to fuse them with other discrete syntactic and semantic features; 2) it is not straightforward how one should transform the word-level representation to the tweet-level representation. In our preliminary experiments, we have evaluated the continuous word representation method (Turian et al., 2010), as well as incorporating neighboring words in the embeddings as additional features, but both methods fail to outperform the lexical baseline that uses only bag-of-word unigrams. To solve this problem, we propose the use of neighboring words in continuous representations to create new instances to augment the training 2559 weather ungratefulness traffic timewasting talkative swearing stability snobbish rains helped cop wastingmytime Tweeters curse mood smut STORM ungrateful lane colleagues Xs teary sensitive intellectual Blizzarad clearly pulled Wen wht qweet91 dudes moneycars snowed r speed</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Elijah Mayfield</author>
<author>Suresh Naidu</author>
<author>Jeremiah Dittmar</author>
</authors>
<title>Historical analysis of legal opinions with a sparse mixed-effects latent variable model.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>740--749</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9077" citStr="Wang et al., 2012" startWordPosition="1404" endWordPosition="1407">. In particular, we briefly review a supervised approach of using sparse mixed-effects topic model to visualize the topical words to analyze this behavior data. For the quantitative task of automatic categorization of tweets, we propose a novel approach to create additional training data, using continuous lexical and semantic representations. 4.1 Supervised Topic Modeling To analyze the salient words for each category of annoying behaviors, we utilize SAGE (Eisenstein et al., 2011), a state-of-the-art mixed-effect topic model, which has been used in several NLP applications (Sim et al., 2012; Wang et al., 2012). SAGE is ideal for our text analytic purposes, because it is supervised, and it builds relatively clean topic models by considering the additive effects and the background distribution of words. Therefore, we can use SAGE to visualize the salient words for each category of annoying behaviors using the 3,375 #petpeeve tweets. Each tweet is treated as a document, and we use Markov Chain Monte Carlo for inference. To facilitate the geographical analysis, we use Google’s reverse geocoding service to extract the state information from coordinates, and apply SAGE for visualization. 2The categories </context>
</contexts>
<marker>Wang, Mayfield, Naidu, Dittmar, 2012</marker>
<rawString>William Yang Wang, Elijah Mayfield, Suresh Naidu, and Jeremiah Dittmar. 2012. Historical analysis of legal opinions with a sparse mixed-effects latent variable model. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 740–749. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiliang Wang</author>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>Social media as a sensor of air quality and public response in china.</title>
<date>2015</date>
<journal>Journal of medical Internet research,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="1376" citStr="Wang et al., 2015" startWordPosition="194" endWordPosition="197"> lexical and syntactic features are useful for automatic categorization of annoying behaviors, and frame-semantic features further boost the performance; that leveraging large lexical embeddings to create additional training instances significantly improves the lexical model; and incorporating frame-semantic embedding achieves the best overall performance. 1 Introduction In the ever-expanding era of social media, many scientific disciplines, such as health and healthcare, biology, and learning sciences, have adopted computational approaches to exploit patterns and behaviors in large datasets (Wang et al., 2015; Chen and Lonardi, 2009; Baker and Yacef, 2009). In contrast, the primary methods for behavioral sciences still rely on lab experiments with limited amount of subjects, which are time consuming and financially expensive. In addition to this, it is also difficult to obtain a set of samples with geograph∗We understand that many people find long titles annoying, so we intentionally use a very long one to help people understand what “pet peeve” means. Figure 1: An anonymized example of #petpeeve tweets. ical variations in traditional lab-based behavioral experiments. While the social media data a</context>
</contexts>
<marker>Wang, Paul, Dredze, 2015</marker>
<rawString>Shiliang Wang, Michael J Paul, and Mark Dredze. 2015. Social media as a sensor of air quality and public response in china. Journal of medical Internet research, 17(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
<author>Yangfeng Ji</author>
</authors>
<title>Extracting lexically divergent paraphrases from Twitter.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="6652" citStr="Xu et al., 2014" startWordPosition="1019" endWordPosition="1022">ruption .03 punctuality .01 monetary .03 racial .01 sexual .03 religious .01 arrogance .02 selfishness .01 celebrity .02 silence .01 ignorance .02 smoking .01 privacy .02 talkative .01 products .02 weather .01 Table 1: The categories and percentages of annoying behaviors in #petpeeve tweets in our dataset. Note that 17% of the #petpeeve tweets are identified as other unrelated behaviors (not shown). entity recognition (Ritter et al., 2011), summarization (O’Connor et al., 2010b), sentiment analysis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) methods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude information. We extract 3,375 tweets</context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. 2014. Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics (TACL), 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>C Ho</author>
<author>Y Juan</author>
<author>C Lin</author>
</authors>
<title>Libshorttext: A library for short-text classification and analysis.</title>
<date>2013</date>
<tech>Technical report, Technical Report.</tech>
<note>http://www. csie. ntu. edu. tw/˜ cjlin/ papers/libshorttext. pdf.</note>
<contexts>
<context position="16385" citStr="Yu et al., 2013" startWordPosition="2521" endWordPosition="2524"> without reply. We see that many slang expressions are associated with various labels. In Table 3, we show the geographical variation of tweets. The word “dmv” (DC-MarylandVirginia) is correctly associated with MD and DC, and when we search the database, these #petpeeve tweets mainly refer to the 2010 snowstorm in the Winter affecting these areas. The “daddy” is prominent in the state of Florida, while the word “rims” is also identified, showing the unique car culture of this southern state. 5.2 Quantitative Evaluation Experimental Setup We use the logistic regression model from LibShortText (Yu et al., 2013) 2560 Methods Prec. Rec. F1 Imp. Lexical Baseline (No Data Augmentation) .341 .342 .341 — + UrbanDictionary Embeddings .343 .344 .344 0.9% + Twitter Embeddings* .357 .358 .358 4.7% + GoogleNews Embeddings* .364 .366 .365 6.1% All Features Baseline (No Data Augmentation) .365 .367 .366 — + Lexical (GoogleNews) and Frame-Semantic Embeddings* .376 .377 .376 2.7% + Lexical (Twitter) and Frame-Semantic Embeddings* .379 .380 .379 3.6% + Lexical (UD) and Frame-Semantic Embeddings* .379 .381 .380 3.8% Table 5: The effectiveness of leveraging continuous embeddings to create additional training instance</context>
</contexts>
<marker>Yu, Ho, Juan, Lin, 2013</marker>
<rawString>H Yu, C Ho, Y Juan, and C Lin. 2013. Libshorttext: A library for short-text classification and analysis. Technical report, Technical Report. http://www. csie. ntu. edu. tw/˜ cjlin/ papers/libshorttext. pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Zhang</author>
<author>Shelly Campo</author>
<author>Kathleen F Janz</author>
<author>Petya Eckler</author>
<author>Jingzhen Yang</author>
<author>Linda G Snetselaar</author>
<author>Alessio Signorini</author>
</authors>
<title>Electronic word of mouth on twitter about physical activity in the united states: exploratory infodemiology study.</title>
<date>2013</date>
<journal>Journal of medical Internet research,</journal>
<volume>15</volume>
<issue>11</issue>
<contexts>
<context position="2280" citStr="Zhang et al., 2013" startWordPosition="332" endWordPosition="335">mples with geograph∗We understand that many people find long titles annoying, so we intentionally use a very long one to help people understand what “pet peeve” means. Figure 1: An anonymized example of #petpeeve tweets. ical variations in traditional lab-based behavioral experiments. While the social media data are abundantly available, computational approaches to behavioral sciences using Twitter are not well-studied. Even when statistical techniques are applied to these tasks, their concentration has been on simple statistical significance tests and descriptive statistics (De Charms, 2013; Zhang et al., 2013). Therefore, we believe that statistical natural language processing techniques are needed for insightful analysis and interpretation in behavioral studies. In this paper, we use Twitter as a corpus for computational behavioral science. More specifically, we focus on a case study of analyzing annoying behaviors. To do this, we exploit a corpus of 9 million tweets (Cheng et al., 2010), and extract the tweets that describe these behaviors using the #petpeeve hashtags. #petpeeve is a popular Twitter hashtag, which describes behaviors that might be annoying to others. An example of #petpeeve tweet</context>
</contexts>
<marker>Zhang, Campo, Janz, Eckler, Yang, Snetselaar, Signorini, 2013</marker>
<rawString>Ni Zhang, Shelly Campo, Kathleen F Janz, Petya Eckler, Jingzhen Yang, Linda G Snetselaar, and Alessio Signorini. 2013. Electronic word of mouth on twitter about physical activity in the united states: exploratory infodemiology study. Journal of medical Internet research, 15(11).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
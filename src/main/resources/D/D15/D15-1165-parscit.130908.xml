<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9993215">
A Comparison between Count and Neural Network Models Based on
Joint Translation and Reordering Sequences
</title>
<author confidence="0.980166">
Andreas Guta, Tamer Alkhouli, Jan-Thorsten Peter, Joern Wuebker, Hermann Ney
</author>
<affiliation confidence="0.821640666666667">
Human Language Technology and Pattern Recognition Group
RWTH Aachen University
Aachen, Germany
</affiliation>
<email confidence="0.998388">
{surname}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.994785" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993125">
We propose a conversion of bilingual
sentence pairs and the corresponding
word alignments into novel linear se-
quences. These are joint translation
and reordering (JTR) uniquely defined
sequences, combining interdepending
lexical and alignment dependencies on
the word level into a single framework.
They are constructed in a simple manner
while capturing multiple alignments
and empty words. JTR sequences can
be used to train a variety of models.
We investigate the performances of n-
gram models with modified Kneser-Ney
smoothing, feed-forward and recur-
rent neural network architectures when
estimated on JTR sequences, and com-
pare them to the operation sequence
model (Durrani et al., 2013b). Evalua-
tions on the IWSLT German→English,
WMT German→English and BOLT
Chinese→English tasks show that JTR
models improve state-of-the-art phrase-
based systems by up to 2.2 BLEU.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999821240740741">
Standard phrase-based machine translation (Och
et al., 1999; Zens et al., 2002; Koehn et al., 2003)
uses relative frequencies of phrase pairs to esti-
mate a translation model. The phrase table is ex-
tracted from a bilingual text aligned on the word
level, using e.g. GIZA++ (Och and Ney, 2003). Al-
though the phrase pairs capture internal dependen-
cies between the source and target phrases aligned
to each other, they fail to model dependencies that
extend beyond phrase boundaries. Phrase-based
decoding involves concatenating target phrases.
The burden of ensuring that the result is linguisti-
cally consistent falls on the language model (LM).
This work proposes word-based translation
models that are potentially capable of capturing
long-range dependencies. We do this in two steps:
First, given bilingual sentence pairs and the asso-
ciated word alignments, we convert the informa-
tion into uniquely defined linear sequences. These
sequenecs encode both word reordering and trans-
lation information. Thus, they are referred to as
joint translation and reordering (JTR) sequences.
Second, we train an n-gram model with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) on the resulting JTR sequences. This yields
a model that fuses interdepending reordering and
translation dependencies into a single framework.
Although JTR n-gram models are closely re-
lated to the operation sequence model (OSM)
(Durrani et al., 2013b), there are three main dif-
ferences. To begin with, the OSM employs min-
imal translation units (MTUs), which are essen-
tially atomic phrases. As the MTUs are extracted
sentence-wise, a word can potentially appear in
multiple MTUs. In order to avoid overlapping
translation units, we define the JTR sequences
on the level of words. Consequently, JTR se-
quences have smaller vocabulary sizes than OSM
sequences and lead to models with less sparsity.
Moreover, we argue that JTR sequences offer a
simpler reordering approach than operation se-
quences, as they handle reorderings without the
need to predict gaps. Finally, when used as an
additional model in the log-linear framework of
phrase-based decoding, an n-gram model trained
on JTR sequences introduces only one single fea-
ture to be tuned, whereas the OSM additionally
uses 4 supportive features (Durrani et al., 2013b).
Experimental results confirm that this simplifica-
tion does not make JTR models less expressive, as
their performance is on par with the OSM.
Due to data sparsity, increasing the n-gram or-
der of count-based models beyond a certain point
becomes useless. To address this, we resort to neu-
</bodyText>
<page confidence="0.927845">
1401
</page>
<note confidence="0.984734">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1401–1411,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999645371428571">
ral networks (NNs), as they have been successfully
applied to machine translation recently (Sunder-
meyer et al., 2014; Devlin et al., 2014). They are
able to score any word combination without re-
quiring additional smoothing techniques. We ex-
periment with feed-forward and recurrent trans-
lation networks, benefiting from their smoothing
capabilities. To this end, we split the linear se-
quence into two sequences for the neural transla-
tion models to operate on. This is possible due to
the simplicity of the JTR sequence. We show that
the count and NN models perform well on their
own, and that combining them yields even better
results.
In this work, we apply n-gram models with
modified Kneser-Ney smoothing during phrase-
based decoding and neural JTR models in rescor-
ing. However, using a phrase-based system is not
required by the model, but only the initial step to
demonstrate the strength of JTR models, which
can be applied independently of the underlying de-
coding framework. While the focus of this work is
on the development and comparison of the models,
the long-term goal is to decode using JTR mod-
els without the limitations introduced by phrases,
in order to exploit the full potential of JTR mod-
els. The JTR models are estimated on word align-
ments, which we obtain using GIZA++ in this pa-
per. The future aim is to also generate improved
word alignments by a joint optimization of both
the alignments and the models, similar to the train-
ing of IBM models (Brown et al., 1990; Brown et
al., 1993). In the long run, we intend to achieve a
consistency between decoding and training using
the introduced JTR models.
</bodyText>
<sectionHeader confidence="0.996943" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999939671641791">
In order to address the downsides of the phrase
translation model, various approaches have been
taken. Mari˜no et al. (2006) proposed a bilingual
language model (BILM) that operates on bilin-
gual n-grams, with an own n-gram decoder re-
quiring monotone alignments. The lexical re-
ordering model introduced in (Tillmann, 2004)
was integrated into phrase-based decoding. Crego
and Yvon (2010) adapted the approach to BILMs.
The bilingual n-grams are further advanced in
(Niehues et al., 2011), where they operate on non-
monotone alignments within a phrase-based trans-
lation framework. Compared to our JTR models,
their BILMs treat jointly aligned source words as
minimal translation units, ignore unaligned source
words and do not include reordering information.
Durrani et al. (2011) developed the OSM which
combined dependencies on bilingual word pairs
and reordering information into a single frame-
work. It used an own decoder that was based on n-
grams of MTUs and predicted single translation or
reordering operations. This was further advanced
in (Durrani et al., 2013a) by a decoder that was
capable of predicting whole sequences of MTUs,
similar to a phrase-based decoder. In (Durrani et
al., 2013b), a slightly enhanced version of OSM
was integrated into the log-linear framework of
the Moses system (Koehn et al., 2007). Both the
BILM (Stewart et al., 2014) and the OSM (Durrani
et al., 2014) can be smoothed using word classes.
Guta et al. (2015) introduced the extended trans-
lation model (ETM), which operates on the word
level and augments the IBM models by an addi-
tional bilingual word pair and a reordering opera-
tion. It is implemented into the log-linear frame-
work of a phrase-based decoder and shown to be
competitive with a 7-gram OSM.
The JTR n-gram models proposed within this
work can be seen as an extension of the ETM.
Nevertheless, JTR models utilize linear sequences
of dependencies and combine the translation of
bilingual word pairs and reoderings into a sin-
gle model. The ETM, however, features separate
models for the translation of individual words and
reorderings and provides an explicit treatment of
multiple alignments. As they operate on linear se-
quences, JTR count models can be implemented
using existing toolkits for n-gram language mod-
els, e.g. the KenLM toolkit (Heafield et al., 2013).
An HMM approach for word-to-phrase align-
ments was presented in (Deng and Byrne, 2005),
showing performance similar to IBM Model 4 on
the task of bitext alignment. Feng et al. (2013)
propose several models which rely only on the in-
formation provided by the source side and pre-
dict reorderings. Contrastingly, JTR models in-
corporate target information as well and predict
both translations and reorderings jointly in a sin-
gle framework.
Zhang et al. (2013) explore different Markov
chain orderings for an n-gram model on MTUs
in rescoring. Feng and Cohn (2013) present an-
other generative word-based Markov chain trans-
lation model which exploits a hierarchical Pitman-
Yor process for smoothing, but it is only applied
to induce word alignments. Their follow-up work
(Feng et al., 2014) introduces a Markov-model on
</bodyText>
<page confidence="0.994123">
1402
</page>
<bodyText confidence="0.999848541666667">
MTUs, similar to the OSM described above.
Recently, neural machine translation has
emerged as an alternative to phrase-based decod-
ing, where NNs are used as standalone models to
decode source input. In (Sutskever et al., 2014),
a recurrent NN was used to encode a source
sequence, and output a target sentence once the
source sentence was fully encoded in the network.
The network did not have any explicit treatment
of alignments. Bahdanau et al. (2015) introduced
soft alignments as part of the network architecture.
In this work, we make use of hard alignments
instead, where we encode the alignments in the
source and target sequences, requiring no mod-
ifications of existing feed-forward and recurrent
NN architectures. Our feed-forward models are
based on the architectures proposed in (Devlin et
al., 2014), while the recurrent models are based
on (Sundermeyer et al., 2014). Further recent
research on applying NN models for extended
context was carried out in (Le et al., 2012; Auli
et al., 2013; Hu et al., 2014). All of these works
focus on lexical context and ignore the reordering
aspect covered in our work.
</bodyText>
<sectionHeader confidence="0.992958" genericHeader="method">
3 JTR Sequences
</sectionHeader>
<bodyText confidence="0.999906653846154">
The core idea of this work is the interpretation of
a bilingual sentence pair and its word alignment
as a linear sequence of K joint translation and re-
ordering (JTR) tokens gK1 . Formally, the sequence
gK1 (f1J ,eI1,bI1) is a uniquely defined interpretation
of a given source sentence f1J, its translation eI1 and
the inverted alignment bI1, where bi denotes the
ordered sequence of source positions j aligned to
target position i. We drop the explicit mention of
(f1J ,eI1,bI1) to allow for a better readability. Each
JTR token is either an aligned bilingual word pair
hf,ei or a reordering class 0j0 j.
Unaligned words on the source and target side
are processed as if they were aligned to the empty
word ε. Hence, an unaligned source word f gener-
ates the token hf,εi, and an unaligned target word
e the token hε,ei.
Each word of the source and target sentences is
to appear in the corresponding JTR sequence ex-
actly once. For multiply-aligned target words e,
the first source word f that is aligned to e gener-
ates the token hf,ei. All other source words f0,
that are also aligned to e, are processed as if they
were aligned to the artificial word σ. Thus, each
of these f0 generates a token hf0,σi. The same
approach is applied to multiply-aligned source
</bodyText>
<figure confidence="0.857540777777778">
Algorithm 1 JTR Conversion Algorithm
1: procedure JTRCONVERSION(fJ1 , eI1, bI1)
2: gK1 ← 0/
3: // last translated source position j0
4: j0← 0
5: for i ← 1 to I do
6: if ei is unaligned then
7: // align ei to the empty word ε
8: APPEND(gK1 , hε,eii)
9: continue
10: // ei is aligned to at least one source word
11: j ← first source position in bi
12: if j = j0 then
13: // ei is aligned to the same fj as ei−1
14: APPEND(gK1 , hσ,eii)
15: continue
16: if j =6 j0 +1 then
17: // alignment step is non-monotone
18: REORDERINGS(fJ1, bI1, gK 1 ,j0, j)
19: // 1-to-1 translation: fj is aligned to ei
20: APPEND(gK1 , hfj,eii)
21: j0 ← j
22: // generate all other fj that are also
23: // aligned to the current target word ei
24: for all remaining j in bi do
25: APPEND(gK1 , hfj,σi)
26: j0 ← j
27: // check last alignment step at sentence end
28: if j0 =6 J then
29: // last alignment step is non-monotone
30: REORDERINGS(f1J , bI1, gK1, j0, J+ 1)
31: return gK1
33: // called when a reordering class is appended
34: procedure REORDERINGS(f1 J , bI1, gK1 , j0, j)
35: // check if the predecessor is unaligned
36: if fj−1 is unaligned then
37: // get unaligned predecessors
38: fj−1
j0 ← unaligned predecessors of fj
39: // check if the alignment step to the first
40: // unaligned predecessor is monotone
41: if j0 =6 j0 + 1 then
42: // non-monotone: add reordering class
43: APPEND(gK1 , 0j0, j0)
44: // translate unaligned predecessors by ε
</figure>
<listItem confidence="0.9408008">
45: for f ← fj0 to fj−1 do
46: APPEND(gK1 , hf,εi)
47: else
48: // non-monotone: add reordering class
49: APPEND(gK1 , 0j0, j)
</listItem>
<bodyText confidence="0.723341166666667">
words. Similar to Feng and Cohn (2013), we clas-
sify the reordered source positions j0 and j by 0j0 j:
step backward (←), j = j0 − 1
jump forward (r--,), j &gt; j0 + 1
jump backward (�), j &lt; j0 − 1.
The reordering classes are illustrated in Figure 1.
</bodyText>
<figure confidence="0.949858090909091">
⎧
⎨⎪
⎪⎩
0j0 j =
1403
i i
i−1
i i−1
i−1
j j0 j0 j j j0
(a) step backward (←) (b) jump forward (n) (c) jump backward (�)
</figure>
<figureCaption confidence="0.999975">
Figure 1: Overview of the different reordering classes in JTR sequences.
</figureCaption>
<subsectionHeader confidence="0.999309">
3.1 Sequence Conversion
</subsectionHeader>
<bodyText confidence="0.999846">
Algorithm 1 presents the formal conversion of a
bilingual sentence pair and its alignment into the
corresponding JTR sequence gK1 . At first, gK1 is
initialized by an empty sequence (line 2). For each
target position i = 1,...,I it is extended by at least
one token. During the generation process, we store
the last visited source position j0 (line 4). If a tar-
get word ei is
</bodyText>
<listItem confidence="0.966416625">
• unaligned, we align it to the empty word ε
and append hε,eii to the current gK1 (line 8),
• if it is aligned to the same fj as ei−1, we only
add hσ,eii (line 14),
• otherwise we append hfj,eii (line 20) and
• in case there are more source words aligned
to ei, we additionally append hfj,σi for each
of these (line 24).
</listItem>
<bodyText confidence="0.9980805">
Before a token hfj,eii is generated, we have to
check whether the alignment step from j0 to j is
monotone (line 16). In case it is not, we have to
deal with reorderings (line 34). We define that
a token hfj−1,εi is to be generated right before
the generation of the token containing fj. Thus,
if f j−1 is not aligned, we first determine the con-
tiguous sequence of unaligned predecessors f j−1
</bodyText>
<equation confidence="0.461433">
j0
</equation>
<bodyText confidence="0.969132363636364">
(line 38). Next, if the step from j0 to j0 is not
monotone, we add the corresponding reordering
class (line 43). Afterwards we append all hf j0,εi
to hfj−1,εi. If fj−1 is aligned, we do not have to
process unaligned source words and only append
the corresponding reordering class (line 49).
Figure 2 illustrates the generation steps of a
JTR sequence, whose result is presented in Ta-
ble 1. The alignment steps are denoted by the ar-
rows connecting the alignment points. The first
dashed alignment point indicates the hε,,i token
that is generated right after the hFeld,fieldi to-
ken. The second dashed alignment point indicates
the hein,εi token, which corresponds to the un-
aligned source word ein. Note, that the hein,εi
Figure 2: This example illustrates the JTR se-
quence gK 1 for a German→English sentence pair
including the word-to-word alignment.
token has to be generated right before h.,.i is
generated. Therefore, there is no forward jump
from hCode,codei to h.,.i, but a monotone step
to hein,εi followed by h.,.i.
</bodyText>
<subsectionHeader confidence="0.999939">
3.2 Training of Count Models
</subsectionHeader>
<bodyText confidence="0.999633666666667">
As the JTR sequence gK 1 is a unique interpretation
of a bilingual sentence pair and its alignment, the
probability p(f1J ,eI1,bI1) can be computed as:
</bodyText>
<equation confidence="0.8711605">
p(f1J ,eI
1,bI1) = p(gK1). (1)
The probability of gK1 can be factorized and ap-
proximated by an n-gram model.
p(gk|gk−1
k−n+1) (2)
</equation>
<bodyText confidence="0.99989725">
Within this work, we first estimate the Viterbi
alignment for the bilingual training data using
GIZA++ (Och and Ney, 2003). Secondly, the con-
version presented in Algorithm 1 is applied to ob-
tain the JTR sequences, on which we estimate an
n-gram model with modified Kneser-Ney smooth-
ing as described in (Chen and Goodman, 1998) us-
ing the KenLM toolkit1 (Heafield et al., 2013).
</bodyText>
<footnote confidence="0.663049">
1https://kheafield.com/code/kenlm/
</footnote>
<figure confidence="0.956286128205128">
.
code
your
enter
,
field
Command
the
in
geben
Sie
im
Feld
Befehl
Ihren
Code
ein
.
K
p(gK1 ) = ∏
k=1
1404
k gk sk tk
1 y δ y
2 him,ini im in
3 hσ,thei σ the
4 y δ y
5 hBefehl,Commandi Befehl Command
6 ← δ ←
7 hFeld,fieldi Feld field
8 hε,,i ε ,
9 x δ x
10 hgeben,enteri geben enter
11 hSie,σi Sie σ
12 y δ y
13 hIhren,youri Ihren your
14 hCode,codei Code code
15 hein,εi ein ε
16 h.,.i . .
</figure>
<tableCaption confidence="0.973161">
Table 1: The left side of this table presents the JTR
</tableCaption>
<bodyText confidence="0.89250175">
tokens gk corresponding to Figure 2. The right
side shows the source and target tokens sk and tk
obtained from the JTR tokens gk. They are used
for the training of NNs (cf. Section 4).
</bodyText>
<subsectionHeader confidence="0.986542">
3.3 Integration into Phrase-based Decoding
</subsectionHeader>
<bodyText confidence="0.999975842105263">
Basically, each phrase table entry is annotated
with both the word alignment information, which
also allows to identify unaligned source words,
and the corresponding JTR sequence. The JTR
model is added to the log-linear framework as an
additional n-gram model. Within the phrase-based
decoder, we extend each search state such that it
additionally stores the JTR model history.
In comparison to the OSM, the JTR model does
not predict gaps. Local reorderings within phrases
are handled implicitly. On the other hand, we rep-
resent long-range reorderings between phrases by
the coverage vector and limit them by reordering
constraints.
Phrase-pairs ending with unaligned source
words at their right boundary prove to be a prob-
lem during decoding. As shown in Subsection 3.1,
the conversion from word alignments to JTR se-
quences assumes that each token corresponding to
an unaligned source word is generated immedi-
ately before the token corresponding to the closest
aligned source position to its right. However, if a
phrase ends with an unaligned fj as its rightmost
source word, the generation of the hfj,εi token has
to be postponed until the next word fj+1 is to be
translated or, even worse, fj+1 has already been
translated before.
To address this issue, we constrained the phrase
table extraction to discard entries with unaligned
source tokens at the right boundary. For IWSLT
De→En, this led to a baseline weaker by 0.2 BLEU
than the one described in Section 5. In order to
have an unconstrained and fair baseline, we there-
after removed this constraint and forced such dele-
tion tokens to be generated at the end of the se-
quence. Hence, we accept that the JTR model
might compute the wrong score in these special
cases.
</bodyText>
<sectionHeader confidence="0.997789" genericHeader="method">
4 Neural Networks
</sectionHeader>
<bodyText confidence="0.999941785714286">
Usually, smoothing techniques are applied to
count-based models to handle unseen events. A
neural network does not suffer from this, as it
is able to score unseen events without additional
smoothing techniques. In the following, we will
describe how to adapt JTR sequences to be used
with feed-forward and recurrent NNs.
The first thing to notice is the vocabulary size,
mainly determined by the number of bilingual
word pairs, which constituted atomic units in the
count-based models. NNs that compute probabil-
ity values at the output layer evaluate a softmax
function that produces normalized scores that sum
up to unity. The softmax function is given by:
</bodyText>
<equation confidence="0.997029166666667">
1 ) = eoei(ei−1
1 )
p(ei|ei−1 (3)
∑|V|
w=1 eow(ei−1
1 )
</equation>
<bodyText confidence="0.9998856">
where oei and ow are the raw unnormalized output
layer values for the words ei and w, respectively,
and |V |is the vocabulary size. The output layer
is a function of the context ei−1
1 . Computing the
denominator is expensive for large vocabularies,
as it requires computing the output for all words.
Therefore, we split JTR tokens gk and use indi-
vidual words as input and output units, such that
the NN receives jumps, source and target words as
input and outputs target words and jumps. Hence,
the resulting neural model is not a LM, but a trans-
lation model with different input and output vo-
cabularies. A JTR sequence gK1 is split into its
source and target parts sK1 and tK1 . The construc-
tion of the JTR source sequence sK1 proceeds as
follows: Whenever a bilingual pair is encountered,
the source word is kept and the target word is dis-
carded. In addition, all jump classes are replaced
by a special token δ. The JTR target sequence tK1 is
constructed similarly by keeping the target words
and dropping source words, and the jump classes
are also kept. Table 1 shows the JTR source and
target sequences corresponding to JTR sequence
of Figure 2.
</bodyText>
<page confidence="0.959813">
1405
</page>
<bodyText confidence="0.999932857142857">
Due to the design of the JTR sequence, pro-
ducing the source and target JTR sequences is
straightforward. The resulting sequences can then
be used with existing NN architectures, without
further modifications to the design of the net-
works. This results in powerful models that re-
quire little effort to implement.
</bodyText>
<subsectionHeader confidence="0.996075">
4.1 Feed-forward Neural JTR
</subsectionHeader>
<bodyText confidence="0.999775">
First, we will apply a feed-forward NN (FFNN) to
the JTR sequence. FFNN models resemble count-
based models in using a predefined limited context
size, but they do not encounter the same smooth-
ing problems. In this work, we use a FFNN similar
to that proposed in (Devlin et al., 2014), defined
as:
</bodyText>
<equation confidence="0.991984">
p(tk|tk−1
k−n,skk−n). (4)
</equation>
<bodyText confidence="0.9999959">
It scores the JTR target word tk at position k us-
ing the current source word sk, and the history of
n JTR source words. In addition, the n JTR target
words preceding tk are used as context. The FFNN
computes the score by looking up the vector em-
beddings of the source and target context words,
concatenating them, then evaluating the rest of the
network. We reduce the output layer to a short-
list of the most frequent words, and compute word
class probabilities for the remaining words.
</bodyText>
<subsectionHeader confidence="0.989125">
4.2 Recurrent Neural JTR
</subsectionHeader>
<bodyText confidence="0.999884142857143">
Unlike feed-forward NNs, recurrent NNs (RNNs)
enable the use of unbounded context. Following
(Sundermeyer et al., 2014), we use bidirectional
recurrent NNs (BRNNs) to capture the full JTR
source side. The BRNN uses the JTR target side
as well as the full JTR source side as context, and
it is given by:
</bodyText>
<equation confidence="0.992373">
p(tk|tk−1
1 ,sK 1 ) (5)
</equation>
<bodyText confidence="0.99905275">
This equation is realized by a network that uses
forward and backward recurrent layers to capture
the complete source sentence. By a forward layer
we imply a recurrent hidden layer that processes
a given sequence from left to right, while a back-
ward layer does the processing backwards, from
right to left. The source sentence is basically split
at a given position k, then past and future represen-
tations of the sentence are recursively computed
by the forward and backward layers, respectively.
To include the target side, we provide the forward
layer with the target input tk−1 as well, that is,
we aggregate the embeddings of the input source
word sk and the input target word tk−1 before they
are fed into the forward layer. Due to recurrency,
the forward layer encodes the parts (tk−1
</bodyText>
<equation confidence="0.463053333333333">
1 ,sk 1), and
the backward layer encodes sKk , and together they
encode (tk−1
</equation>
<bodyText confidence="0.995922823529412">
1 ,sK 1 ), which is used to score the out-
put target word tk. For the sake of comparison
to FFNN and count models, we also experiment
with a recurrent model that does not include future
source information, this is obtained by replacing
the term sK 1 with sk1 in Eq. 5. It will be referred
to as the unidirectional recurrent neural network
(URNN) model in the experiments.
Note that the JTR source and target sides
include jump information, therefore, the RNN
model described above explicitly models reorder-
ing. In contrast, the models proposed in (Sunder-
meyer et al., 2014) do not include any jumps, and
hence do not provide an explicit way of includ-
ing word reordering. In addition, the JTR RNN
models do not require the use of IBM-1 lexica to
resolve multiply-aligned words. As discussed in
Section 3, these cases are resolved by aligning the
multiply-aligned word to the first word on the op-
posite side.
The integration of the NNs into the decoder is
not trivial, due to the dependence on the target
context. In the case of RNNs, the context is un-
bounded, which would affect state recombination,
and lead to less variety in the beam used to prune
the search space. Therefore, the RNN scores are
computed using approximations instead (Auli et
al., 2013; Alkhouli et al., 2015). In (Alkhouli et
al., 2015), it is shown that approximate RNN inte-
gration into the phrase-based decoder has a slight
advantage over n-best rescoring. Therefore, we
apply RNNs in rescoring in this work, and to al-
low for a direct comparison between FFNNs and
RNNs, we apply FFNNs in rescoring as well.
</bodyText>
<sectionHeader confidence="0.997958" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999970714285714">
We perform experiments on the large-
scale IWSLT 20132 (Cettolo et al.,
2014) German→English, WMT 20153
German→English and the DARPA BOLT
Chinese→English tasks. The statistics for the
bilingual corpora are shown in Table 2. Word
alignments are generated with the GIZA++ toolkit
</bodyText>
<footnote confidence="0.999001">
2http://www.iwslt2013.org
3http://www.statmt.org/wmt15/
</footnote>
<equation confidence="0.998810333333333">
K
p(tK 1 |sK 1 ) ≈ ∏
k=1
K
p(tK1 |sK1 ) = ∏
k=1
</equation>
<page confidence="0.946681">
1406
</page>
<table confidence="0.8718286">
IWSLT WMT BOLT
German English German English Chinese English
Sentences 4.32M 4.22M 4.08M
Run. Words 108M 109M 106M 108M 78M 86M
Vocabulary 836K 792K 814K 773K 384K 817K
</table>
<tableCaption confidence="0.8042525">
Table 2: Statistics for the bilingual training data of the IWSLT 2013 German—*English, WMT 2015
German—*English, and the DARPA BOLT Chinese—*English translation tasks.
</tableCaption>
<bodyText confidence="0.994285725">
(Och and Ney, 2003). We use a standard phrase-
based translation system (Koehn et al., 2003).
The decoding process is implemented as a beam
search. All baselines contain phrasal and lexical
smoothing models for both directions, word and
phrase penalties, a distance-based reordering
model, enhanced low frequency features (Chen
et al., 2011), a hierarchical reordering model
(HRM) (Galley and Manning, 2008), a word
class LM (Wuebker et al., 2013) and an n-gram
LM. The lexical and phrase translation models of
all baseline systems are trained on all provided
bilingual data. The log-linear feature weights are
tuned with minimum error rate training (MERT)
(Och, 2003) on BLEU (Papineni et al., 2001). All
systems are evaluated with MultEval (Clark et al.,
2011). The reported BLEU scores are averaged
over three MERT optimization runs.
All LMs, OSMs and count-based JTR models
are estimated with the KenLM toolkit (Heafield et
al., 2013). The OSM and the count-based JTR
model are implemented in the phrasal decoder.
NNs are used only in rescoring. The 9-gram
FFNNs are trained with two hidden layers. The
short lists contain the 10k most frequent words,
and all remaining words are clusterd into 1000
word classes. The projecton layer has 17 x 100
nodes, the first hidden layer 1000 and the sec-
ond 500. The RNNs have LSTM architectures.
The URNN has 2 hidden layers while the BRNN
has one forward, one backward and one addi-
tional hidden layer. All layers have 200 nodes,
while the output layer is class-factored using 2000
classes. For the count-based JTR model and OSM
we tuned the n-gram size on the tuning set of each
task. For the full data, 7-grams were used for the
IWSLT and WMT tasks, and 8-grams for BOLT.
When using in-domain data, smaller n-gram sizes
were used. All rescoring experiments used 1000-
best lists without duplicates.
</bodyText>
<subsectionHeader confidence="0.993654">
5.1 Tasks description
</subsectionHeader>
<bodyText confidence="0.999991541666667">
The domain of IWSLT consists of lecture-type
talks presented at TED conferences which are also
available online4. All systems are optimized on
the dev2010 corpus, named dev here. Some
of the OSM and JTR systems are trained on the
TED portions of the data containing 138K sen-
tences. To estimate the 4-gram LM, we addi-
tionally make use of parts of the Shuffled News,
LDC English Gigaword and 109-French-English
corpora, selected by a cross-entropy difference cri-
terion (Moore and Lewis, 2010). In total, 1.7 bil-
lion running words are taken for LM training. The
BOLT Chinese—*English task is evaluated on the
“discussion forum” domain. The 5-gram LM is
trained on 2.9 billion running words in total. The
in-domain data consists of a subset of 67.8K sen-
tences and we used a set of 1845 sentences for tun-
ing. The evaluation set test1 contains 1844 and
test2 1124 sentences. For the WMT task, we
used the target side of the bilingual data and all
monolingual data to train a pruned 5-gram LM on
a total of 4.4 billion running words. We concate-
nated the newstest2011 and newstest2012
corpora for tuning the systems.
</bodyText>
<subsectionHeader confidence="0.779913">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999930461538461">
We start with the IWSLT 2013 German—* English
task, where we compare between the different JTR
and OSM models. The results are shown in Ta-
ble 3. When comparing the in-domain n-gram
JTR model trained using Kneser-Ney smoothing
(KN) to OSM, we observe that the n-gram KN
JTR model improves the baseline by 1.4 BLEU
on both test and eval11. The OSM model
performs similarly, with a slight disadvantage on
eval11. In comparison, the FFNN of Eq. (4) im-
proves the baseline by 0.7–0.9 BLEU, compared to
the slightly better 0.8–1.1 BLEU achieved by the
URNN. The difference between the FFNN and the
</bodyText>
<footnote confidence="0.969989">
4http://www.ted.com/
</footnote>
<page confidence="0.970207">
1407
</page>
<table confidence="0.9998985625">
data dev test eval11
baseline full 33.3 30.8 35.7
+OSM TED 34.5 32.2 36.8
+FFNN TED 34.0 31.7 36.4
+URNN TED 34.2 31.9 36.5
+BRNN TED 34.4 32.1 36.8
+KN TED 34.6 32.2 37.1
+BRNN TED 35.0 32.8 37.7
+OSM full 34.1 31.6 36.5
+FFNN full 33.9 31.5 36.0
+KN full 34.2 31.6 36.6
+KN TED 34.9 32.4 37.1
+FFNN TED 35.2 32.7 37.2
+FFNN full 35.1 32.7 37.2
+BRNN TED 35.5 33.0 37.4
+BRNN TED 35.4 33.0 37.3
</table>
<tableCaption confidence="0.965962">
Table 3: Results measured in BLEU for the IWSLT
German→English task.
</tableCaption>
<table confidence="0.609809416666667">
traindata
baseline
+OSM indomain
+FFNN indomain
+BRNN indomain
+KN indomain
+OSM full
+FFNN full
+KN full
+KN indomain
+FFNN full
+RNN indomain
</table>
<tableCaption confidence="0.9813215">
Table 4: Results measured in BLEU for the BOLT
Chinese→English task.
</tableCaption>
<bodyText confidence="0.999839619047619">
URNN is that the latter captures the unbounded
source and target history that extends until the be-
ginning of the sentences, giving it an advantage
over the FFNN. The performance of the URNN
can be improved by including the future part of the
source sentence, as described in Eq. (5), resulting
in the BRNN model. Next, we explore whether the
models are additive. When rescoring the n-gram
KN JTR output with the BRNN, an additional im-
provement of 0.6 BLEU is obtained. There are two
reasons for this: The BRNN includes the future
part of the source input when scoring target words.
This information is not used by the KN model.
Moreover, the BRNN is able to score word com-
binations unseen in training, while the KN model
uses backing off to score unseen events.
When training the KN, FFNN, and OSM mod-
els on the full data, we observe less gains in com-
parison to in-domain data training. However, com-
bining the KN models trained on in-domain and
full data gives additional gains, which suggests
that although the in-domain model is more adapted
to the task, it still can gain from out-of-domain
data. Adding the FFNN on top improves the com-
bination. Note here that the FFNN sees the same
information as the KN model, but the difference is
that the NN operates on the word level rather than
the word-pair level. Second, the FFNN is able to
handle unseen sequences by design, without the
need for the backing off workaround. The BRNN
improves the combination more than the FFNN,
as the model captures an unbounded source and
target history in addition to an unbounded future
source context. Combining the KN, FFNN and
BRNN JTR models leads to an overall gain of 2.2
BLEU on both dev and test.
Next, we present the BOLT Chinese→English
results, shown in Table 4. Comparing n-gram
KN JTR and OSM trained on the in-domain data
shows they perform equally well on test1, im-
proving the baseline by 0.7 BLEU, with a slight ad-
vantage for the JTR model on test2. The feed-
forward and the recurrent in-domain networks
yield the same results in comparison to each other.
Training the OSM and JTR models on the full data
yields slightly worse results than in-domain train-
ing. However, combining the two types of training
improves the results. This is shown when adding
the in-domain KN JTR model on top of the model
trained on full data, improving it by up to 0.4
BLEU. Rescoring with the feed-forward and the
recurrent network improves this even further, sup-
porting the previous observation that the n-gram
KN JTR and NNs complement each other. The
combination of the 4 models yields an overall im-
provement of 1.2–1.4 BLEU.
Finally, we compare KN JTR and OSM models
on the WMT German→English task in Table 5.
The two models perform almost similar to each
other. The JTR model improves the baseline by
up to 0.7 BLEU. Rescoring the KN JTR with the
FFNN improves it by up to 0.3 BLEU leading to an
overall improvement between 0.5 and 1.0 BLEU.
</bodyText>
<figure confidence="0.9918535">
test1 test2
18.1 17.0
18.8 17.2
18.6 17.6
18.6 17.6
18.8 17.5
18.5 17.2
18.4 17.4
18.8 17.3
19.0 17.7
19.2 18.3
19.3 18.4
</figure>
<page confidence="0.943416">
1408
</page>
<table confidence="0.999412285714286">
2013 newstest 2015
2014
baseline 28.1 28.6 29.4
+OSM 28.6 28.9 30.0
+FFNN 28.7 28.9 29.7
+KN 28.8 28.9 29.9
+FFNN 29.1 29.1 30.0
</table>
<tableCaption confidence="0.9890045">
Table 5: Results measured in BLEU for the WMT
German→English task.
</tableCaption>
<subsectionHeader confidence="0.999424">
5.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999803">
To investigate the effect of including jump infor-
mation in the JTR sequence, we trained a BRNN
using jump classes and another excluding them.
The BRNNs were used in rescoring. Below, we
demonstrate the difference between the systems:
</bodyText>
<tableCaption confidence="0.729062666666667">
source: wir kommen sp¨ater noch auf diese Leute zur¨uck .
reference: We’ll come back to these people later.
Hypothesis 1:
</tableCaption>
<table confidence="0.765525375">
JTR source: wir kommen 3 zur¨uck 3 sp¨ater noch auf
diese Leute 3 .
JTR target: we come n back &lt;--, later 6 to these people
r )&apos;.
Hypothesis 2:
JTR source: wir kommen sp¨ater noch auf diese Leute
zur¨uck .
JTR target: we come later 6 on these guys back.
</table>
<bodyText confidence="0.999903722222222">
Note the German verb “zur¨uckkommen”, which
is split into “kommen” and “zur¨uck”. German
places “kommen” at the second position and
“zur¨uck” towards the end of the sentence. Unlike
German, the corresponding English phrase “come
back” has the words adjacent to each other. We
found that the system including jumps prefers the
correct translation of the verb, as shown in Hy-
pothesis 1 above. The system translates “kom-
men” to “come”, jumps forward to “zur¨uck”,
translates it to “back”, then jumps back to continue
translating the word “sp¨ater”. In contrast, the sys-
tem that excludes jump classes is blind to this sep-
aration of words. It favors Hypothesis 2 which is
a strictly monotone translation of the German sen-
tence. This is also reflected by the BLEU scores,
where we found the system including jump classes
outperforming the one without by up to 0.8 BLEU.
</bodyText>
<sectionHeader confidence="0.998212" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989894736842">
We introduced a method that converts bilingual
sentence pairs and their word alignments into joint
translation and reordering (JTR) sequences. They
combine interdepending lexical and alignment de-
pendencies into a single framework. A main ad-
vantage of JTR sequences is that a variety of mod-
els can be trained on them. Here, we have esti-
mated n-gram models with modified Kneser-Ney
smoothing, FFNN and RNN architectures on JTR
sequences.
We compared our count-based JTR model to the
OSM, both used in phrase-based decoding, and
showed that the JTR model performed at least as
good as OSM, with a slight advantage for JTR. In
comparison to the OSM, the JTR model operates
on words, leading to a smaller vocabulary size.
Moreover, it utilizes simpler reordering structures
without gaps and only requires one log-linear fea-
ture to be tuned, whereas the OSM needs 5. Due
to the flexibility of JTR sequences, we can ap-
ply them also to FFNNs and RNNs. Utilizing
two count models and applying both networks in
rescoring gains the overall highest improvement
over the phrase-based system by up to 2.2 BLEU,
on the German→English IWSLT task. The com-
bination outperforms OSM by up to 1.2 BLEU on
the BOLT Chinese→English tasks.
The JTR models are not dependent on the
phrase-based framework, and one of the long-
term goals is to perform standalone decoding with
the JTR models independently of phrase-based
systems. Without the limitations introduced by
phrases, we believe that JTR models could per-
form even better. In addition, we aim to use JTR
models to obtain the alignment, which would then
be used to train the JTR models in an iterative
manner, achieving consistency and hoping for im-
proved models.
</bodyText>
<sectionHeader confidence="0.994207" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995689">
This work has received funding from the Euro-
pean Union’s Horizon 2020 research and innova-
tion programme under grant agreement no 645452
(QT21). This material is partially based upon
work supported by the DARPA BOLT project un-
der Contract No. HR0011- 12-C-0015. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views of
DARPA.
</bodyText>
<page confidence="0.995344">
1409
</page>
<sectionHeader confidence="0.982426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999656445454546">
Tamer Alkhouli, Felix Rietig, and Hermann Ney. 2015.
Investigations on phrase-based decoding with recur-
rent neural network language and translation mod-
els. In Proceedings of the EMNLP 2015 Tenth
Workshop on Statistical Machine Translation, Lis-
bon, Portugal, September. to appear.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 1044–1054, Seattle, USA, Octo-
ber.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations, San Diego,
Calefornia, USA, May.
Peter F. Brown, John Cocke, Stephan A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Rossin.
1990. A Statistical Approach to Machine Transla-
tion. Computational Linguistics, 16(2):79–85, June.
Peter F. Brown, Stephan A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263–311, June.
Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa
Bentivogli, and Marcello Federico. 2014. Report on
the 11th iwslt evaluation campaign, iwslt 2014. In
International Workshop on Spoken Language Trans-
lation, pages 2–11, Lake Tahoe, CA, USA, Decem-
ber.
Stanley F. Chen and Joshuo Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In MT Summit XIII, pages 269–275, Xiamen,
China, September.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176–181, Portland, Oregon,
June.
Josep Maria Crego and Franc¸ois Yvon. 2010. Improv-
ing reordering with linguistically informed bilingual
n-grams. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010: Posters), pages 197–205, Beijing, China.
Yonggang Deng and William Byrne. 2005. Hmm word
and phrase alignment for statistical machine transla-
tion. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 169–
176, Vancouver, British Columbia, Canada, October.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1370–1380, Baltimore, MD, USA,
June.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1045–1054, Portland, Oregon, USA, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a. Model with minimal translation units, but
decode with phrases. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1–11, Atlanta, Geor-
gia, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013b. Can
markov models over minimal translation units help
phrase-based smt? In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 399–
405, Sofia, Bulgaria, August.
Nadir Durrani, Philipp Koehn, Helmut Schmid, and
Alexander Fraser. 2014. Investigating the useful-
ness of generalized word representations in smt. In
COLING, Dublin, Ireland, August.
Yang Feng and Trevor Cohn. 2013. A markov
model of machine translation using non-parametric
bayesian inference. In 51st Annual Meeting of the
Association for Computational Linguistics, pages
333–342, Sofia, Bulgaria, August.
Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.
2013. Advancements in reordering models for sta-
tistical machine translation. In Annual Meeting
of the Assoc. for Computational Linguistics, pages
322–332, Sofia, Bulgaria, August.
Yang Feng, Trevor Cohn, and Xinkai Du. 2014. Fac-
tored markov translation with robust modeling. In
Proceedings of the Eighteenth Conference on Com-
putational Natural Language Learning, pages 151–
159, Ann Arbor, Michigan, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
</reference>
<page confidence="0.768475">
1410
</page>
<reference confidence="0.999904587155964">
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 848–856, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Andreas Guta, Joern Wuebker, Miguel Grac¸a, Yunsu
Kim, and Hermann Ney. 2015. Extended translation
models in phrase-based decoding. In Proceedings
of the EMNLP 2015 Tenth Workshop on Statistical
Machine Translation, Lisbon, Portugal, September.
to appear.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria, August.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20–29,
Gothenburg, Sweden, April.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal Phrase-Based Translation. In Proceedings of the
2003 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
03), pages 127–133, Edmonton, Alberta.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantine, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
pages 177–180, Prague, Czech Republic, June.
Hai Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
39–48, Montreal, Canada, June.
Jos´e B Mari˜no, Rafael E Banchs, Josep M Crego, Adri`a
de Gispert, Patrik Lambert, Jos´e A R Fonollosa, and
Marta R Costa-juss`a. 2006. N-gram-based Machine
Translation. Comput. Linguist., 32(4):527–549, De-
cember.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short
Papers), pages 220–224, Uppsala, Sweden, July.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel, 2011. Proceedings of the Sixth Work-
shop on Statistical Machine Translation, chapter
Wider Context by Using Bilingual Language Mod-
els in Machine Translation, pages 198–206.
Franz J. Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51,
March.
Franz J. Och, Christoph Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. Joint SIGDAT Conf.
on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora, pages 20–28, Uni-
versity of Maryland, College Park, MD, June.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160–167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.
Darelene Stewart, Roland Kuhn, Eric Joanis, and
George Foster. 2014. Coarse split and lump bilin-
gual languagemodels for richer source information
in smt. In AMTA, Vancouver, BC, Canada, October.
Martin Sundermeyer, Tamer Alkhouli, Wuebker Wue-
bker, and Hermann Ney. 2014. Translation Model-
ing with Bidirectional Recurrent Neural Networks.
In Conference on Empirical Methods on Natural
Language Processing, pages 14–25, Doha, Qatar,
October.
Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.
2014. Sequence to sequence learning with neural
networks. In Advances in Neural Information Pro-
cessing Systems 27, pages 3104–3112.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ’04, pages 101–104, Stroudsburg,
PA, USA.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377–1381, Seattle, USA, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-Based Statistical Machine Transla-
tion. In 25th German Conf. on Artificial Intelligence
(KI2002), pages 18–32, Aachen, Germany, Septem-
ber.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 12–21,
Atlanta, Georgia, June.
</reference>
<page confidence="0.993071">
1411
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.480503">
<title confidence="0.999845">A Comparison between Count and Neural Network Models Based Joint Translation and Reordering Sequences</title>
<author confidence="0.996791">Andreas Guta</author>
<author confidence="0.996791">Tamer Alkhouli</author>
<author confidence="0.996791">Jan-Thorsten Peter</author>
<author confidence="0.996791">Joern Wuebker</author>
<author confidence="0.996791">Hermann</author>
<affiliation confidence="0.7744845">Human Language Technology and Pattern Recognition RWTH Aachen</affiliation>
<address confidence="0.897553">Aachen,</address>
<abstract confidence="0.995323">We propose a conversion of bilingual sentence pairs and the corresponding alignments into novel linear sequences. These are joint and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. investigate the performances of gram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluaon the IWSLT and BOLT tasks show that JTR models improve state-of-the-art phrasesystems by up to BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tamer Alkhouli</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>Investigations on phrase-based decoding with recurrent neural network language and translation models.</title>
<date>2015</date>
<booktitle>In Proceedings of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation,</booktitle>
<location>Lisbon, Portugal,</location>
<note>to appear.</note>
<contexts>
<context position="23945" citStr="Alkhouli et al., 2015" startWordPosition="4094" endWordPosition="4097">rd reordering. In addition, the JTR RNN models do not require the use of IBM-1 lexica to resolve multiply-aligned words. As discussed in Section 3, these cases are resolved by aligning the multiply-aligned word to the first word on the opposite side. The integration of the NNs into the decoder is not trivial, due to the dependence on the target context. In the case of RNNs, the context is unbounded, which would affect state recombination, and lead to less variety in the beam used to prune the search space. Therefore, the RNN scores are computed using approximations instead (Auli et al., 2013; Alkhouli et al., 2015). In (Alkhouli et al., 2015), it is shown that approximate RNN integration into the phrase-based decoder has a slight advantage over n-best rescoring. Therefore, we apply RNNs in rescoring in this work, and to allow for a direct comparison between FFNNs and RNNs, we apply FFNNs in rescoring as well. 5 Evaluation We perform experiments on the largescale IWSLT 20132 (Cettolo et al., 2014) German→English, WMT 20153 German→English and the DARPA BOLT Chinese→English tasks. The statistics for the bilingual corpora are shown in Table 2. Word alignments are generated with the GIZA++ toolkit 2http://ww</context>
</contexts>
<marker>Alkhouli, Rietig, Ney, 2015</marker>
<rawString>Tamer Alkhouli, Felix Rietig, and Hermann Ney. 2015. Investigations on phrase-based decoding with recurrent neural network language and translation models. In Proceedings of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation, Lisbon, Portugal, September. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint Language and Translation Modeling with Recurrent Neural Networks.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="9754" citStr="Auli et al., 2013" startWordPosition="1544" endWordPosition="1547">ot have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lexical context and ignore the reordering aspect covered in our work. 3 JTR Sequences The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 . Formally, the sequence gK1 (f1J ,eI1,bI1) is a uniquely defined interpretation of a given source sentence f1J, its translation eI1 and the inverted alignment bI1, where bi denotes the ordered sequence of source positions j aligned to target position i. We drop the explicit mention of (f</context>
<context position="23921" citStr="Auli et al., 2013" startWordPosition="4090" endWordPosition="4093">way of including word reordering. In addition, the JTR RNN models do not require the use of IBM-1 lexica to resolve multiply-aligned words. As discussed in Section 3, these cases are resolved by aligning the multiply-aligned word to the first word on the opposite side. The integration of the NNs into the decoder is not trivial, due to the dependence on the target context. In the case of RNNs, the context is unbounded, which would affect state recombination, and lead to less variety in the beam used to prune the search space. Therefore, the RNN scores are computed using approximations instead (Auli et al., 2013; Alkhouli et al., 2015). In (Alkhouli et al., 2015), it is shown that approximate RNN integration into the phrase-based decoder has a slight advantage over n-best rescoring. Therefore, we apply RNNs in rescoring in this work, and to allow for a direct comparison between FFNNs and RNNs, we apply FFNNs in rescoring as well. 5 Evaluation We perform experiments on the largescale IWSLT 20132 (Cettolo et al., 2014) German→English, WMT 20153 German→English and the DARPA BOLT Chinese→English tasks. The statistics for the bilingual corpora are shown in Table 2. Word alignments are generated with the G</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint Language and Translation Modeling with Recurrent Neural Networks. In Conference on Empirical Methods in Natural Language Processing, pages 1044–1054, Seattle, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2015</date>
<booktitle>In International Conference on Learning Representations,</booktitle>
<location>San Diego, Calefornia, USA,</location>
<contexts>
<context position="9205" citStr="Bahdanau et al. (2015)" startWordPosition="1456" endWordPosition="1459">rarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on 1402 MTUs, similar to the OSM described above. Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input. In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network. The network did not have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lex</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2015</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, San Diego, Calefornia, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephan A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Rossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="5463" citStr="Brown et al., 1990" startWordPosition="850" endWordPosition="853"> demonstrate the strength of JTR models, which can be applied independently of the underlying decoding framework. While the focus of this work is on the development and comparison of the models, the long-term goal is to decode using JTR models without the limitations introduced by phrases, in order to exploit the full potential of JTR models. The JTR models are estimated on word alignments, which we obtain using GIZA++ in this paper. The future aim is to also generate improved word alignments by a joint optimization of both the alignments and the models, similar to the training of IBM models (Brown et al., 1990; Brown et al., 1993). In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models. 2 Previous Work In order to address the downsides of the phrase translation model, various approaches have been taken. Mari˜no et al. (2006) proposed a bilingual language model (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are fu</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Rossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephan A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Rossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):79–85, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephan A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="5484" citStr="Brown et al., 1993" startWordPosition="854" endWordPosition="857">ength of JTR models, which can be applied independently of the underlying decoding framework. While the focus of this work is on the development and comparison of the models, the long-term goal is to decode using JTR models without the limitations introduced by phrases, in order to exploit the full potential of JTR models. The JTR models are estimated on word alignments, which we obtain using GIZA++ in this paper. The future aim is to also generate improved word alignments by a joint optimization of both the alignments and the models, similar to the training of IBM models (Brown et al., 1990; Brown et al., 1993). In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models. 2 Previous Work In order to address the downsides of the phrase translation model, various approaches have been taken. Mari˜no et al. (2006) proposed a bilingual language model (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are further advanced in (Ni</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephan A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Jan Niehues</author>
<author>Sebastian St¨uker</author>
<author>Luisa Bentivogli</author>
<author>Marcello Federico</author>
</authors>
<title>Report on the 11th iwslt evaluation campaign, iwslt</title>
<date>2014</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<pages>2--11</pages>
<location>Lake Tahoe, CA, USA,</location>
<marker>Cettolo, Niehues, St¨uker, Bentivogli, Federico, 2014</marker>
<rawString>Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th iwslt evaluation campaign, iwslt 2014. In International Workshop on Spoken Language Translation, pages 2–11, Lake Tahoe, CA, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshuo Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2370" citStr="Chen and Goodman, 1998" startWordPosition="347" endWordPosition="350">rden of ensuring that the result is linguistically consistent falls on the language model (LM). This work proposes word-based translation models that are potentially capable of capturing long-range dependencies. We do this in two steps: First, given bilingual sentence pairs and the associated word alignments, we convert the information into uniquely defined linear sequences. These sequenecs encode both word reordering and translation information. Thus, they are referred to as joint translation and reordering (JTR) sequences. Second, we train an n-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the resulting JTR sequences. This yields a model that fuses interdepending reordering and translation dependencies into a single framework. Although JTR n-gram models are closely related to the operation sequence model (OSM) (Durrani et al., 2013b), there are three main differences. To begin with, the OSM employs minimal translation units (MTUs), which are essentially atomic phrases. As the MTUs are extracted sentence-wise, a word can potentially appear in multiple MTUs. In order to avoid overlapping translation units, we define the JTR sequences on the level of words. Consequently, JTR se</context>
<context position="15925" citStr="Chen and Goodman, 1998" startWordPosition="2688" endWordPosition="2691">f Count Models As the JTR sequence gK 1 is a unique interpretation of a bilingual sentence pair and its alignment, the probability p(f1J ,eI1,bI1) can be computed as: p(f1J ,eI 1,bI1) = p(gK1). (1) The probability of gK1 can be factorized and approximated by an n-gram model. p(gk|gk−1 k−n+1) (2) Within this work, we first estimate the Viterbi alignment for the bilingual training data using GIZA++ (Och and Ney, 2003). Secondly, the conversion presented in Algorithm 1 is applied to obtain the JTR sequences, on which we estimate an n-gram model with modified Kneser-Ney smoothing as described in (Chen and Goodman, 1998) using the KenLM toolkit1 (Heafield et al., 2013). 1https://kheafield.com/code/kenlm/ . code your enter , field Command the in geben Sie im Feld Befehl Ihren Code ein . K p(gK1 ) = ∏ k=1 1404 k gk sk tk 1 y δ y 2 him,ini im in 3 hσ,thei σ the 4 y δ y 5 hBefehl,Commandi Befehl Command 6 ← δ ← 7 hFeld,fieldi Feld field 8 hε,,i ε , 9 x δ x 10 hgeben,enteri geben enter 11 hSie,σi Sie σ 12 y δ y 13 hIhren,youri Ihren your 14 hCode,codei Code code 15 hein,εi ein ε 16 h.,.i . . Table 1: The left side of this table presents the JTR tokens gk corresponding to Figure 2. The right side shows the source a</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshuo Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University, Cambridge, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
<author>Howard Johnson</author>
</authors>
<title>Unpacking and transforming feature functions: New ways to smooth phrase tables.</title>
<date>2011</date>
<booktitle>In MT Summit XIII,</booktitle>
<pages>269--275</pages>
<location>Xiamen, China,</location>
<contexts>
<context position="25320" citStr="Chen et al., 2011" startWordPosition="4312" endWordPosition="4315">sh Sentences 4.32M 4.22M 4.08M Run. Words 108M 109M 106M 108M 78M 86M Vocabulary 836K 792K 814K 773K 384K 817K Table 2: Statistics for the bilingual training data of the IWSLT 2013 German—*English, WMT 2015 German—*English, and the DARPA BOLT Chinese—*English translation tasks. (Och and Ney, 2003). We use a standard phrasebased translation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported BLEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). T</context>
</contexts>
<marker>Chen, Kuhn, Foster, Johnson, 2011</marker>
<rawString>Boxing Chen, Roland Kuhn, George Foster, and Howard Johnson. 2011. Unpacking and transforming feature functions: New ways to smooth phrase tables. In MT Summit XIII, pages 269–275, Xiamen, China, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics:shortpapers,</booktitle>
<pages>176--181</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="25741" citStr="Clark et al., 2011" startWordPosition="4380" endWordPosition="4383">rch. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported BLEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescoring. The 9-gram FFNNs are trained with two hidden layers. The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes. The projecton layer has 17 x 100 nodes, the first hidden layer 1000 and the second 500. The RNNs have LSTM architectures. The URNN has 2 hidden l</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Maria Crego</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Improving reordering with linguistically informed bilingual n-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010: Posters),</booktitle>
<pages>197--205</pages>
<location>Beijing, China.</location>
<contexts>
<context position="6003" citStr="Crego and Yvon (2010)" startWordPosition="935" endWordPosition="938">gnments and the models, similar to the training of IBM models (Brown et al., 1990; Brown et al., 1993). In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models. 2 Previous Work In order to address the downsides of the phrase translation model, various approaches have been taken. Mari˜no et al. (2006) proposed a bilingual language model (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are further advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translati</context>
</contexts>
<marker>Crego, Yvon, 2010</marker>
<rawString>Josep Maria Crego and Franc¸ois Yvon. 2010. Improving reordering with linguistically informed bilingual n-grams. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010: Posters), pages 197–205, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>Hmm word and phrase alignment for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>169--176</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="8020" citStr="Deng and Byrne, 2005" startWordPosition="1263" endWordPosition="1266"> this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reoderings into a single model. The ETM, however, features separate models for the translation of individual words and reorderings and provides an explicit treatment of multiple alignments. As they operate on linear sequences, JTR count models can be implemented using existing toolkits for n-gram language models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoot</context>
</contexts>
<marker>Deng, Byrne, 2005</marker>
<rawString>Yonggang Deng and William Byrne. 2005. Hmm word and phrase alignment for statistical machine translation. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 169– 176, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and Robust Neural Network Joint Models for Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1370--1380</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="4117" citStr="Devlin et al., 2014" startWordPosition="619" endWordPosition="622">rimental results confirm that this simplification does not make JTR models less expressive, as their performance is on par with the OSM. Due to data sparsity, increasing the n-gram order of count-based models beyond a certain point becomes useless. To address this, we resort to neu1401 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1401–1411, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ral networks (NNs), as they have been successfully applied to machine translation recently (Sundermeyer et al., 2014; Devlin et al., 2014). They are able to score any word combination without requiring additional smoothing techniques. We experiment with feed-forward and recurrent translation networks, benefiting from their smoothing capabilities. To this end, we split the linear sequence into two sequences for the neural translation models to operate on. This is possible due to the simplicity of the JTR sequence. We show that the count and NN models perform well on their own, and that combining them yields even better results. In this work, we apply n-gram models with modified Kneser-Ney smoothing during phrasebased decoding and</context>
<context position="9563" citStr="Devlin et al., 2014" startWordPosition="1511" endWordPosition="1514">input. In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network. The network did not have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lexical context and ignore the reordering aspect covered in our work. 3 JTR Sequences The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 . Formally, the sequence gK1 (f1J ,eI1,bI1) is a uniquely defined interpretation of a given source</context>
<context position="20920" citStr="Devlin et al., 2014" startWordPosition="3563" endWordPosition="3566">e to the design of the JTR sequence, producing the source and target JTR sequences is straightforward. The resulting sequences can then be used with existing NN architectures, without further modifications to the design of the networks. This results in powerful models that require little effort to implement. 4.1 Feed-forward Neural JTR First, we will apply a feed-forward NN (FFNN) to the JTR sequence. FFNN models resemble countbased models in using a predefined limited context size, but they do not encounter the same smoothing problems. In this work, we use a FFNN similar to that proposed in (Devlin et al., 2014), defined as: p(tk|tk−1 k−n,skk−n). (4) It scores the JTR target word tk at position k using the current source word sk, and the history of n JTR source words. In addition, the n JTR target words preceding tk are used as context. The FFNN computes the score by looking up the vector embeddings of the source and target context words, concatenating them, then evaluating the rest of the network. We reduce the output layer to a shortlist of the most frequent words, and compute word class probabilities for the remaining words. 4.2 Recurrent Neural JTR Unlike feed-forward NNs, recurrent NNs (RNNs) en</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and Robust Neural Network Joint Models for Statistical Machine Translation. In 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370–1380, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1045--1054</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6394" citStr="Durrani et al. (2011)" startWordPosition="994" endWordPosition="997">odel (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are further advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durra</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1045–1054, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
</authors>
<title>Model with minimal translation units, but decode with phrases.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1--11</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1005" citStr="Durrani et al., 2013" startWordPosition="139" endWordPosition="142">ng word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each ot</context>
<context position="2620" citStr="Durrani et al., 2013" startWordPosition="385" endWordPosition="388">ingual sentence pairs and the associated word alignments, we convert the information into uniquely defined linear sequences. These sequenecs encode both word reordering and translation information. Thus, they are referred to as joint translation and reordering (JTR) sequences. Second, we train an n-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the resulting JTR sequences. This yields a model that fuses interdepending reordering and translation dependencies into a single framework. Although JTR n-gram models are closely related to the operation sequence model (OSM) (Durrani et al., 2013b), there are three main differences. To begin with, the OSM employs minimal translation units (MTUs), which are essentially atomic phrases. As the MTUs are extracted sentence-wise, a word can potentially appear in multiple MTUs. In order to avoid overlapping translation units, we define the JTR sequences on the level of words. Consequently, JTR sequences have smaller vocabulary sizes than OSM sequences and lead to models with less sparsity. Moreover, we argue that JTR sequences offer a simpler reordering approach than operation sequences, as they handle reorderings without the need to predict</context>
<context position="6682" citStr="Durrani et al., 2013" startWordPosition="1041" endWordPosition="1044">urther advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes. Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation. It is implemented into the log-linear fra</context>
</contexts>
<marker>Durrani, Fraser, Schmid, 2013</marker>
<rawString>Nadir Durrani, Alexander Fraser, and Helmut Schmid. 2013a. Model with minimal translation units, but decode with phrases. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1–11, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Can markov models over minimal translation units help phrase-based smt?</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>399--405</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1005" citStr="Durrani et al., 2013" startWordPosition="139" endWordPosition="142">ng word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each ot</context>
<context position="2620" citStr="Durrani et al., 2013" startWordPosition="385" endWordPosition="388">ingual sentence pairs and the associated word alignments, we convert the information into uniquely defined linear sequences. These sequenecs encode both word reordering and translation information. Thus, they are referred to as joint translation and reordering (JTR) sequences. Second, we train an n-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the resulting JTR sequences. This yields a model that fuses interdepending reordering and translation dependencies into a single framework. Although JTR n-gram models are closely related to the operation sequence model (OSM) (Durrani et al., 2013b), there are three main differences. To begin with, the OSM employs minimal translation units (MTUs), which are essentially atomic phrases. As the MTUs are extracted sentence-wise, a word can potentially appear in multiple MTUs. In order to avoid overlapping translation units, we define the JTR sequences on the level of words. Consequently, JTR sequences have smaller vocabulary sizes than OSM sequences and lead to models with less sparsity. Moreover, we argue that JTR sequences offer a simpler reordering approach than operation sequences, as they handle reorderings without the need to predict</context>
<context position="6682" citStr="Durrani et al., 2013" startWordPosition="1041" endWordPosition="1044">urther advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes. Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation. It is implemented into the log-linear fra</context>
</contexts>
<marker>Durrani, Fraser, Schmid, Hoang, Koehn, 2013</marker>
<rawString>Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013b. Can markov models over minimal translation units help phrase-based smt? In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 399– 405, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Philipp Koehn</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>Investigating the usefulness of generalized word representations in smt.</title>
<date>2014</date>
<booktitle>In COLING,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="7010" citStr="Durrani et al., 2014" startWordPosition="1097" endWordPosition="1100">2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes. Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation. It is implemented into the log-linear framework of a phrase-based decoder and shown to be competitive with a 7-gram OSM. The JTR n-gram models proposed within this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reoderings into a single model. The ET</context>
</contexts>
<marker>Durrani, Koehn, Schmid, Fraser, 2014</marker>
<rawString>Nadir Durrani, Philipp Koehn, Helmut Schmid, and Alexander Fraser. 2014. Investigating the usefulness of generalized word representations in smt. In COLING, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Trevor Cohn</author>
</authors>
<title>A markov model of machine translation using non-parametric bayesian inference.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>333--342</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8493" citStr="Feng and Cohn (2013)" startWordPosition="1341" endWordPosition="1344">guage models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on 1402 MTUs, similar to the OSM described above. Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input. In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was ful</context>
<context position="12748" citStr="Feng and Cohn (2013)" startWordPosition="2111" endWordPosition="2114">dering class is appended 34: procedure REORDERINGS(f1 J , bI1, gK1 , j0, j) 35: // check if the predecessor is unaligned 36: if fj−1 is unaligned then 37: // get unaligned predecessors 38: fj−1 j0 ← unaligned predecessors of fj 39: // check if the alignment step to the first 40: // unaligned predecessor is monotone 41: if j0 =6 j0 + 1 then 42: // non-monotone: add reordering class 43: APPEND(gK1 , 0j0, j0) 44: // translate unaligned predecessors by ε 45: for f ← fj0 to fj−1 do 46: APPEND(gK1 , hf,εi) 47: else 48: // non-monotone: add reordering class 49: APPEND(gK1 , 0j0, j) words. Similar to Feng and Cohn (2013), we classify the reordered source positions j0 and j by 0j0 j: step backward (←), j = j0 − 1 jump forward (r--,), j &gt; j0 + 1 jump backward (�), j &lt; j0 − 1. The reordering classes are illustrated in Figure 1. ⎧ ⎨⎪ ⎪⎩ 0j0 j = 1403 i i i−1 i i−1 i−1 j j0 j0 j j j0 (a) step backward (←) (b) jump forward (n) (c) jump backward (�) Figure 1: Overview of the different reordering classes in JTR sequences. 3.1 Sequence Conversion Algorithm 1 presents the formal conversion of a bilingual sentence pair and its alignment into the corresponding JTR sequence gK1 . At first, gK1 is initialized by an empty se</context>
</contexts>
<marker>Feng, Cohn, 2013</marker>
<rawString>Yang Feng and Trevor Cohn. 2013. A markov model of machine translation using non-parametric bayesian inference. In 51st Annual Meeting of the Association for Computational Linguistics, pages 333–342, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Jan-Thorsten Peter</author>
<author>Hermann Ney</author>
</authors>
<title>Advancements in reordering models for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>322--332</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8116" citStr="Feng et al. (2013)" startWordPosition="1280" endWordPosition="1283">s of dependencies and combine the translation of bilingual word pairs and reoderings into a single model. The ETM, however, features separate models for the translation of individual words and reorderings and provides an explicit treatment of multiple alignments. As they operate on linear sequences, JTR count models can be implemented using existing toolkits for n-gram language models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014)</context>
</contexts>
<marker>Feng, Peter, Ney, 2013</marker>
<rawString>Minwei Feng, Jan-Thorsten Peter, and Hermann Ney. 2013. Advancements in reordering models for statistical machine translation. In Annual Meeting of the Assoc. for Computational Linguistics, pages 322–332, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Trevor Cohn</author>
<author>Xinkai Du</author>
</authors>
<title>Factored markov translation with robust modeling.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>151--159</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="8716" citStr="Feng et al., 2014" startWordPosition="1376" endWordPosition="1379">Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on 1402 MTUs, similar to the OSM described above. Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input. In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network. The network did not have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments i</context>
</contexts>
<marker>Feng, Cohn, Du, 2014</marker>
<rawString>Yang Feng, Trevor Cohn, and Xinkai Du. 2014. Factored markov translation with robust modeling. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 151– 159, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>848--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="25386" citStr="Galley and Manning, 2008" startWordPosition="4321" endWordPosition="4324">8M 78M 86M Vocabulary 836K 792K 814K 773K 384K 817K Table 2: Statistics for the bilingual training data of the IWSLT 2013 German—*English, WMT 2015 German—*English, and the DARPA BOLT Chinese—*English translation tasks. (Och and Ney, 2003). We use a standard phrasebased translation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported BLEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasa</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 848–856, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Guta</author>
<author>Joern Wuebker</author>
<author>Miguel Grac¸a</author>
<author>Yunsu Kim</author>
<author>Hermann Ney</author>
</authors>
<title>Extended translation models in phrase-based decoding.</title>
<date>2015</date>
<booktitle>In Proceedings of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation,</booktitle>
<location>Lisbon, Portugal,</location>
<note>to appear.</note>
<marker>Guta, Wuebker, Grac¸a, Kim, Ney, 2015</marker>
<rawString>Andreas Guta, Joern Wuebker, Miguel Grac¸a, Yunsu Kim, and Hermann Ney. 2015. Extended translation models in phrase-based decoding. In Proceedings of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation, Lisbon, Portugal, September. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>690--696</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7933" citStr="Heafield et al., 2013" startWordPosition="1249" endWordPosition="1252">der and shown to be competitive with a 7-gram OSM. The JTR n-gram models proposed within this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reoderings into a single model. The ETM, however, features separate models for the translation of individual words and reorderings and provides an explicit treatment of multiple alignments. As they operate on linear sequences, JTR count models can be implemented using existing toolkits for n-gram language models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based M</context>
<context position="15974" citStr="Heafield et al., 2013" startWordPosition="2697" endWordPosition="2700">e interpretation of a bilingual sentence pair and its alignment, the probability p(f1J ,eI1,bI1) can be computed as: p(f1J ,eI 1,bI1) = p(gK1). (1) The probability of gK1 can be factorized and approximated by an n-gram model. p(gk|gk−1 k−n+1) (2) Within this work, we first estimate the Viterbi alignment for the bilingual training data using GIZA++ (Och and Ney, 2003). Secondly, the conversion presented in Algorithm 1 is applied to obtain the JTR sequences, on which we estimate an n-gram model with modified Kneser-Ney smoothing as described in (Chen and Goodman, 1998) using the KenLM toolkit1 (Heafield et al., 2013). 1https://kheafield.com/code/kenlm/ . code your enter , field Command the in geben Sie im Feld Befehl Ihren Code ein . K p(gK1 ) = ∏ k=1 1404 k gk sk tk 1 y δ y 2 him,ini im in 3 hσ,thei σ the 4 y δ y 5 hBefehl,Commandi Befehl Command 6 ← δ ← 7 hFeld,fieldi Feld field 8 hε,,i ε , 9 x δ x 10 hgeben,enteri geben enter 11 hSie,σi Sie σ 12 y δ y 13 hIhren,youri Ihren your 14 hCode,codei Code code 15 hein,εi ein ε 16 h.,.i . . Table 1: The left side of this table presents the JTR tokens gk corresponding to Figure 2. The right side shows the source and target tokens sk and tk obtained from the JTR </context>
<context position="25917" citStr="Heafield et al., 2013" startWordPosition="4408" endWordPosition="4411">tures (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported BLEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescoring. The 9-gram FFNNs are trained with two hidden layers. The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes. The projecton layer has 17 x 100 nodes, the first hidden layer 1000 and the second 500. The RNNs have LSTM architectures. The URNN has 2 hidden layers while the BRNN has one forward, one backward and one additional hidden layer. All layers have 200 nodes, while the output layer is class-factored using 2000 classes. For </context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690–696, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Michael Auli</author>
<author>Qin Gao</author>
<author>Jianfeng Gao</author>
</authors>
<title>Minimum translation modeling with recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="9772" citStr="Hu et al., 2014" startWordPosition="1548" endWordPosition="1551">t treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lexical context and ignore the reordering aspect covered in our work. 3 JTR Sequences The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 . Formally, the sequence gK1 (f1J ,eI1,bI1) is a uniquely defined interpretation of a given source sentence f1J, its translation eI1 and the inverted alignment bI1, where bi denotes the ordered sequence of source positions j aligned to target position i. We drop the explicit mention of (f1J ,eI1,bI1) to al</context>
</contexts>
<marker>Hu, Auli, Gao, Gao, 2014</marker>
<rawString>Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum translation modeling with recurrent neural networks. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29, Gothenburg, Sweden, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL03),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="1299" citStr="Koehn et al., 2003" startWordPosition="183" endWordPosition="186">ignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguistically consistent falls on the language model (LM). This work proposes word-based translation models that ar</context>
<context position="25071" citStr="Koehn et al., 2003" startWordPosition="4276" endWordPosition="4279">orpora are shown in Table 2. Word alignments are generated with the GIZA++ toolkit 2http://www.iwslt2013.org 3http://www.statmt.org/wmt15/ K p(tK 1 |sK 1 ) ≈ ∏ k=1 K p(tK1 |sK1 ) = ∏ k=1 1406 IWSLT WMT BOLT German English German English Chinese English Sentences 4.32M 4.22M 4.08M Run. Words 108M 109M 106M 108M 78M 86M Vocabulary 836K 792K 814K 773K 384K 817K Table 2: Statistics for the bilingual training data of the IWSLT 2013 German—*English, WMT 2015 German—*English, and the DARPA BOLT Chinese—*English translation tasks. (Och and Ney, 2003). We use a standard phrasebased translation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL03), pages 127–133, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantine, and</location>
<contexts>
<context position="6937" citStr="Koehn et al., 2007" startWordPosition="1083" endWordPosition="1086">ource words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes. Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation. It is implemented into the log-linear framework of a phrase-based decoder and shown to be competitive with a 7-gram OSM. The JTR n-gram models proposed within this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the trans</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantine, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous Space Translation Models with Neural Networks.</title>
<date>2012</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>39--48</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="9735" citStr="Le et al., 2012" startWordPosition="1540" endWordPosition="1543">The network did not have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lexical context and ignore the reordering aspect covered in our work. 3 JTR Sequences The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 . Formally, the sequence gK1 (f1J ,eI1,bI1) is a uniquely defined interpretation of a given source sentence f1J, its translation eI1 and the inverted alignment bI1, where bi denotes the ordered sequence of source positions j aligned to target position i. We drop the exp</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous Space Translation Models with Neural Networks. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari˜no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>N-gram-based Machine Translation.</title>
<date>2006</date>
<journal>Comput. Linguist.,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>Jos´e B Mari˜no, Rafael E Banchs, Josep M Crego, Adri`a de Gispert, Patrik Lambert, Jos´e A R Fonollosa, and Marta R Costa-juss`a. 2006. N-gram-based Machine Translation. Comput. Linguist., 32(4):527–549, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent Selection of Language Model Training Data.</title>
<date>2010</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>220--224</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="27335" citStr="Moore and Lewis, 2010" startWordPosition="4650" endWordPosition="4653">ain data, smaller n-gram sizes were used. All rescoring experiments used 1000- best lists without duplicates. 5.1 Tasks description The domain of IWSLT consists of lecture-type talks presented at TED conferences which are also available online4. All systems are optimized on the dev2010 corpus, named dev here. Some of the OSM and JTR systems are trained on the TED portions of the data containing 138K sentences. To estimate the 4-gram LM, we additionally make use of parts of the Shuffled News, LDC English Gigaword and 109-French-English corpora, selected by a cross-entropy difference criterion (Moore and Lewis, 2010). In total, 1.7 billion running words are taken for LM training. The BOLT Chinese—*English task is evaluated on the “discussion forum” domain. The 5-gram LM is trained on 2.9 billion running words in total. The in-domain data consists of a subset of 67.8K sentences and we used a set of 1845 sentences for tuning. The evaluation set test1 contains 1844 and test2 1124 sentences. For the WMT task, we used the target side of the bilingual data and all monolingual data to train a pruned 5-gram LM on a total of 4.4 billion running words. We concatenated the newstest2011 and newstest2012 corpora for t</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>R.C. Moore and W. Lewis. 2010. Intelligent Selection of Language Model Training Data. In ACL (Short Papers), pages 220–224, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<date>2011</date>
<booktitle>Proceedings of the Sixth Workshop on Statistical Machine Translation, chapter Wider Context by Using Bilingual Language Models in Machine Translation,</booktitle>
<pages>198--206</pages>
<contexts>
<context position="6103" citStr="Niehues et al., 2011" startWordPosition="951" endWordPosition="954">3). In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models. 2 Previous Work In order to address the downsides of the phrase translation model, various approaches have been taken. Mari˜no et al. (2006) proposed a bilingual language model (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are further advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that </context>
</contexts>
<marker>Niehues, Herrmann, Vogel, Waibel, 2011</marker>
<rawString>Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel, 2011. Proceedings of the Sixth Workshop on Statistical Machine Translation, chapter Wider Context by Using Bilingual Language Models in Machine Translation, pages 198–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1491" citStr="Och and Ney, 2003" startWordPosition="217" endWordPosition="220">neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguistically consistent falls on the language model (LM). This work proposes word-based translation models that are potentially capable of capturing long-range dependencies. We do this in two steps: First, given bilingual sentence pairs and the associated word alignments, we convert the information into u</context>
<context position="15721" citStr="Och and Ney, 2003" startWordPosition="2653" endWordPosition="2656">d alignment. token has to be generated right before h.,.i is generated. Therefore, there is no forward jump from hCode,codei to h.,.i, but a monotone step to hein,εi followed by h.,.i. 3.2 Training of Count Models As the JTR sequence gK 1 is a unique interpretation of a bilingual sentence pair and its alignment, the probability p(f1J ,eI1,bI1) can be computed as: p(f1J ,eI 1,bI1) = p(gK1). (1) The probability of gK1 can be factorized and approximated by an n-gram model. p(gk|gk−1 k−n+1) (2) Within this work, we first estimate the Viterbi alignment for the bilingual training data using GIZA++ (Och and Ney, 2003). Secondly, the conversion presented in Algorithm 1 is applied to obtain the JTR sequences, on which we estimate an n-gram model with modified Kneser-Ney smoothing as described in (Chen and Goodman, 1998) using the KenLM toolkit1 (Heafield et al., 2013). 1https://kheafield.com/code/kenlm/ . code your enter , field Command the in geben Sie im Feld Befehl Ihren Code ein . K p(gK1 ) = ∏ k=1 1404 k gk sk tk 1 y δ y 2 him,ini im in 3 hσ,thei σ the 4 y δ y 5 hBefehl,Commandi Befehl Command 6 ← δ ← 7 hFeld,fieldi Feld field 8 hε,,i ε , 9 x δ x 10 hgeben,enteri geben enter 11 hSie,σi Sie σ 12 y δ y 13</context>
<context position="25000" citStr="Och and Ney, 2003" startWordPosition="4264" endWordPosition="4267">e DARPA BOLT Chinese→English tasks. The statistics for the bilingual corpora are shown in Table 2. Word alignments are generated with the GIZA++ toolkit 2http://www.iwslt2013.org 3http://www.statmt.org/wmt15/ K p(tK 1 |sK 1 ) ≈ ∏ k=1 K p(tK1 |sK1 ) = ∏ k=1 1406 IWSLT WMT BOLT German English German English Chinese English Sentences 4.32M 4.22M 4.08M Run. Words 108M 109M 106M 108M 78M 86M Vocabulary 836K 792K 814K 773K 384K 817K Table 2: Statistics for the bilingual training data of the IWSLT 2013 German—*English, WMT 2015 German—*English, and the DARPA BOLT Chinese—*English translation tasks. (Och and Ney, 2003). We use a standard phrasebased translation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Alignment Models for Statistical Machine Translation.</title>
<date>1999</date>
<booktitle>In Proc. Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>College Park, MD,</location>
<contexts>
<context position="1259" citStr="Och et al., 1999" startWordPosition="175" endWordPosition="178">le manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguistically consistent falls on the language model (LM). This work propos</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz J. Och, Christoph Tillmann, and Hermann Ney. 1999. Improved Alignment Models for Statistical Machine Translation. In Proc. Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28, University of Maryland, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="25647" citStr="Och, 2003" startWordPosition="4366" endWordPosition="4367">lation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported BLEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescoring. The 9-gram FFNNs are trained with two hidden layers. The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes. The projecton layer has 17 x 100 nodes, the first h</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, P.O. Box</journal>
<volume>218</volume>
<pages>10598</pages>
<location>Yorktown Heights, NY</location>
<contexts>
<context position="25679" citStr="Papineni et al., 2001" startWordPosition="4370" endWordPosition="4373"> et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported BLEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescoring. The 9-gram FFNNs are trained with two hidden layers. The short lists contain the 10k most frequent words, and all remaining words are clusterd into 1000 word classes. The projecton layer has 17 x 100 nodes, the first hidden layer 1000 and the second </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. IBM Research Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darelene Stewart</author>
<author>Roland Kuhn</author>
<author>Eric Joanis</author>
<author>George Foster</author>
</authors>
<title>Coarse split and lump bilingual languagemodels for richer source information in smt.</title>
<date>2014</date>
<booktitle>In AMTA,</booktitle>
<location>Vancouver, BC, Canada,</location>
<contexts>
<context position="6975" citStr="Stewart et al., 2014" startWordPosition="1090" endWordPosition="1093">ering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decoder that was based on ngrams of MTUs and predicted single translation or reordering operations. This was further advanced in (Durrani et al., 2013a) by a decoder that was capable of predicting whole sequences of MTUs, similar to a phrase-based decoder. In (Durrani et al., 2013b), a slightly enhanced version of OSM was integrated into the log-linear framework of the Moses system (Koehn et al., 2007). Both the BILM (Stewart et al., 2014) and the OSM (Durrani et al., 2014) can be smoothed using word classes. Guta et al. (2015) introduced the extended translation model (ETM), which operates on the word level and augments the IBM models by an additional bilingual word pair and a reordering operation. It is implemented into the log-linear framework of a phrase-based decoder and shown to be competitive with a 7-gram OSM. The JTR n-gram models proposed within this work can be seen as an extension of the ETM. Nevertheless, JTR models utilize linear sequences of dependencies and combine the translation of bilingual word pairs and reo</context>
</contexts>
<marker>Stewart, Kuhn, Joanis, Foster, 2014</marker>
<rawString>Darelene Stewart, Roland Kuhn, Eric Joanis, and George Foster. 2014. Coarse split and lump bilingual languagemodels for richer source information in smt. In AMTA, Vancouver, BC, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Wuebker Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation Modeling with Bidirectional Recurrent Neural Networks.</title>
<date>2014</date>
<booktitle>In Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>14--25</pages>
<location>Doha, Qatar,</location>
<contexts>
<context position="4095" citStr="Sundermeyer et al., 2014" startWordPosition="614" endWordPosition="618">rrani et al., 2013b). Experimental results confirm that this simplification does not make JTR models less expressive, as their performance is on par with the OSM. Due to data sparsity, increasing the n-gram order of count-based models beyond a certain point becomes useless. To address this, we resort to neu1401 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1401–1411, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ral networks (NNs), as they have been successfully applied to machine translation recently (Sundermeyer et al., 2014; Devlin et al., 2014). They are able to score any word combination without requiring additional smoothing techniques. We experiment with feed-forward and recurrent translation networks, benefiting from their smoothing capabilities. To this end, we split the linear sequence into two sequences for the neural translation models to operate on. This is possible due to the simplicity of the JTR sequence. We show that the count and NN models perform well on their own, and that combining them yields even better results. In this work, we apply n-gram models with modified Kneser-Ney smoothing during ph</context>
<context position="9631" citStr="Sundermeyer et al., 2014" startWordPosition="1522" endWordPosition="1525">encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network. The network did not have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the recurrent models are based on (Sundermeyer et al., 2014). Further recent research on applying NN models for extended context was carried out in (Le et al., 2012; Auli et al., 2013; Hu et al., 2014). All of these works focus on lexical context and ignore the reordering aspect covered in our work. 3 JTR Sequences The core idea of this work is the interpretation of a bilingual sentence pair and its word alignment as a linear sequence of K joint translation and reordering (JTR) tokens gK1 . Formally, the sequence gK1 (f1J ,eI1,bI1) is a uniquely defined interpretation of a given source sentence f1J, its translation eI1 and the inverted alignment bI1, w</context>
<context position="21591" citStr="Sundermeyer et al., 2014" startWordPosition="3678" endWordPosition="3681">res the JTR target word tk at position k using the current source word sk, and the history of n JTR source words. In addition, the n JTR target words preceding tk are used as context. The FFNN computes the score by looking up the vector embeddings of the source and target context words, concatenating them, then evaluating the rest of the network. We reduce the output layer to a shortlist of the most frequent words, and compute word class probabilities for the remaining words. 4.2 Recurrent Neural JTR Unlike feed-forward NNs, recurrent NNs (RNNs) enable the use of unbounded context. Following (Sundermeyer et al., 2014), we use bidirectional recurrent NNs (BRNNs) to capture the full JTR source side. The BRNN uses the JTR target side as well as the full JTR source side as context, and it is given by: p(tk|tk−1 1 ,sK 1 ) (5) This equation is realized by a network that uses forward and backward recurrent layers to capture the complete source sentence. By a forward layer we imply a recurrent hidden layer that processes a given sequence from left to right, while a backward layer does the processing backwards, from right to left. The source sentence is basically split at a given position k, then past and future re</context>
<context position="23240" citStr="Sundermeyer et al., 2014" startWordPosition="3969" endWordPosition="3973">d layer encodes sKk , and together they encode (tk−1 1 ,sK 1 ), which is used to score the output target word tk. For the sake of comparison to FFNN and count models, we also experiment with a recurrent model that does not include future source information, this is obtained by replacing the term sK 1 with sk1 in Eq. 5. It will be referred to as the unidirectional recurrent neural network (URNN) model in the experiments. Note that the JTR source and target sides include jump information, therefore, the RNN model described above explicitly models reordering. In contrast, the models proposed in (Sundermeyer et al., 2014) do not include any jumps, and hence do not provide an explicit way of including word reordering. In addition, the JTR RNN models do not require the use of IBM-1 lexica to resolve multiply-aligned words. As discussed in Section 3, these cases are resolved by aligning the multiply-aligned word to the first word on the opposite side. The integration of the NNs into the decoder is not trivial, due to the dependence on the target context. In the case of RNNs, the context is unbounded, which would affect state recombination, and lead to less variety in the beam used to prune the search space. There</context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Wuebker Wuebker, and Hermann Ney. 2014. Translation Modeling with Bidirectional Recurrent Neural Networks. In Conference on Empirical Methods on Natural Language Processing, pages 14–25, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="8977" citStr="Sutskever et al., 2014" startWordPosition="1417" endWordPosition="1420">ngle framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on 1402 MTUs, similar to the OSM described above. Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input. In (Sutskever et al., 2014), a recurrent NN was used to encode a source sequence, and output a target sentence once the source sentence was fully encoded in the network. The network did not have any explicit treatment of alignments. Bahdanau et al. (2015) introduced soft alignments as part of the network architecture. In this work, we make use of hard alignments instead, where we encode the alignments in the source and target sequences, requiring no modifications of existing feed-forward and recurrent NN architectures. Our feed-forward models are based on the architectures proposed in (Devlin et al., 2014), while the re</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004: Short Papers, HLTNAACL-Short ’04,</booktitle>
<pages>101--104</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5938" citStr="Tillmann, 2004" startWordPosition="928" endWordPosition="929">ved word alignments by a joint optimization of both the alignments and the models, similar to the training of IBM models (Brown et al., 1990; Brown et al., 1993). In the long run, we intend to achieve a consistency between decoding and training using the introduced JTR models. 2 Previous Work In order to address the downsides of the phrase translation model, various approaches have been taken. Mari˜no et al. (2006) proposed a bilingual language model (BILM) that operates on bilingual n-grams, with an own n-gram decoder requiring monotone alignments. The lexical reordering model introduced in (Tillmann, 2004) was integrated into phrase-based decoding. Crego and Yvon (2010) adapted the approach to BILMs. The bilingual n-grams are further advanced in (Niehues et al., 2011), where they operate on nonmonotone alignments within a phrase-based translation framework. Compared to our JTR models, their BILMs treat jointly aligned source words as minimal translation units, ignore unaligned source words and do not include reordering information. Durrani et al. (2011) developed the OSM which combined dependencies on bilingual word pairs and reordering information into a single framework. It used an own decode</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proceedings of HLT-NAACL 2004: Short Papers, HLTNAACL-Short ’04, pages 101–104, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Stephan Peitz</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>Improving statistical machine translation with word class models.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1377--1381</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="25426" citStr="Wuebker et al., 2013" startWordPosition="4329" endWordPosition="4332">4K 817K Table 2: Statistics for the bilingual training data of the IWSLT 2013 German—*English, WMT 2015 German—*English, and the DARPA BOLT Chinese—*English translation tasks. (Och and Ney, 2003). We use a standard phrasebased translation system (Koehn et al., 2003). The decoding process is implemented as a beam search. All baselines contain phrasal and lexical smoothing models for both directions, word and phrase penalties, a distance-based reordering model, enhanced low frequency features (Chen et al., 2011), a hierarchical reordering model (HRM) (Galley and Manning, 2008), a word class LM (Wuebker et al., 2013) and an n-gram LM. The lexical and phrase translation models of all baseline systems are trained on all provided bilingual data. The log-linear feature weights are tuned with minimum error rate training (MERT) (Och, 2003) on BLEU (Papineni et al., 2001). All systems are evaluated with MultEval (Clark et al., 2011). The reported BLEU scores are averaged over three MERT optimization runs. All LMs, OSMs and count-based JTR models are estimated with the KenLM toolkit (Heafield et al., 2013). The OSM and the count-based JTR model are implemented in the phrasal decoder. NNs are used only in rescorin</context>
</contexts>
<marker>Wuebker, Peitz, Rietig, Ney, 2013</marker>
<rawString>Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving statistical machine translation with word class models. In Conference on Empirical Methods in Natural Language Processing, pages 1377–1381, Seattle, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Phrase-Based Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In 25th German Conf. on Artificial Intelligence (KI2002),</booktitle>
<pages>18--32</pages>
<location>Aachen, Germany,</location>
<contexts>
<context position="1278" citStr="Zens et al., 2002" startWordPosition="179" endWordPosition="182">pturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German→English, WMT German→English and BOLT Chinese→English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU. 1 Introduction Standard phrase-based machine translation (Och et al., 1999; Zens et al., 2002; Koehn et al., 2003) uses relative frequencies of phrase pairs to estimate a translation model. The phrase table is extracted from a bilingual text aligned on the word level, using e.g. GIZA++ (Och and Ney, 2003). Although the phrase pairs capture internal dependencies between the source and target phrases aligned to each other, they fail to model dependencies that extend beyond phrase boundaries. Phrase-based decoding involves concatenating target phrases. The burden of ensuring that the result is linguistically consistent falls on the language model (LM). This work proposes word-based trans</context>
</contexts>
<marker>Zens, Och, Ney, 2002</marker>
<rawString>Richard Zens, Franz Josef Och, and Hermann Ney. 2002. Phrase-Based Statistical Machine Translation. In 25th German Conf. on Artificial Intelligence (KI2002), pages 18–32, Aachen, Germany, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Kristina Toutanova</author>
<author>Chris Quirk</author>
<author>Jianfeng Gao</author>
</authors>
<title>Beyond left-to-right: Multiple decomposition structures for smt.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>12--21</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="8389" citStr="Zhang et al. (2013)" startWordPosition="1324" endWordPosition="1327">operate on linear sequences, JTR count models can be implemented using existing toolkits for n-gram language models, e.g. the KenLM toolkit (Heafield et al., 2013). An HMM approach for word-to-phrase alignments was presented in (Deng and Byrne, 2005), showing performance similar to IBM Model 4 on the task of bitext alignment. Feng et al. (2013) propose several models which rely only on the information provided by the source side and predict reorderings. Contrastingly, JTR models incorporate target information as well and predict both translations and reorderings jointly in a single framework. Zhang et al. (2013) explore different Markov chain orderings for an n-gram model on MTUs in rescoring. Feng and Cohn (2013) present another generative word-based Markov chain translation model which exploits a hierarchical PitmanYor process for smoothing, but it is only applied to induce word alignments. Their follow-up work (Feng et al., 2014) introduces a Markov-model on 1402 MTUs, similar to the OSM described above. Recently, neural machine translation has emerged as an alternative to phrase-based decoding, where NNs are used as standalone models to decode source input. In (Sutskever et al., 2014), a recurren</context>
</contexts>
<marker>Zhang, Toutanova, Quirk, Gao, 2013</marker>
<rawString>Hui Zhang, Kristina Toutanova, Chris Quirk, and Jianfeng Gao. 2013. Beyond left-to-right: Multiple decomposition structures for smt. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 12–21, Atlanta, Georgia, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
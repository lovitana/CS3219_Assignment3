<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009614">
<title confidence="0.9859">
Recognizing Textual Entailment Using Probabilistic Inference
</title>
<author confidence="0.990227">
Lei Sha, Sujian Li, Tingsong Jiang, Baobao Chang, Zhifang Sui
</author>
<affiliation confidence="0.944021333333333">
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
Collaborative Innovation Center for Language Ability, Xuzhou 221009 China
</affiliation>
<email confidence="0.985656">
{shalei, lisujian, tingsong, chbb, szf}@pku.edu.cn
</email>
<sectionHeader confidence="0.993636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998944869565217">
Recognizing Text Entailment (RTE) plays
an important role in NLP applications in-
cluding question answering, information
retrieval, etc. In recent work, some re-
search explore “deep” expressions such as
discourse commitments or strict logic for
representing the text. However, these ex-
pressions suffer from the limitation of in-
ference inconvenience or translation loss.
To overcome the limitations, in this paper,
we propose to use the predicate-argument
structures to represent the discourse com-
mitments extracted from text. At the same
time, with the help of the YAGO knowl-
edge, we borrow the distant supervision
technique to mine the implicit facts from
the text. We also construct a probabilistic
network for all the facts and conduct infer-
ence to judge the confidence of each fact
for RTE. The experimental results show
that our proposed method achieves a com-
petitive result compared to the previous
work.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.988252309090909">
For the natural language, a common phenomenon
is that there exist a lot of ways to express the
same or similar meaning. To discover such dif-
ferent expressions, the Recognising Textual En-
tailment (RTE) task is proposed to judge whether
the meaning of one text (denoted as H) can be in-
ferred (entailed) from the other one (T)(Dagan et
al., 2006). For many natural language processing
applications like question answering, information
retrieval which involve the diversity of natural lan-
guage, recognising textual entailments is a critical
step.
PASCAL Recognizing Textual Entailment
(RTE) Challenges (Dagan et al., 2006) have
witnessed a variety of excellent systems which
intend to recognize the textual entailment in-
stances. These systems mainly employ “shallow”
techniques, including heuristics, term overlap,
syntactic dependencies(Vanderwende et al., 2006;
Jijkoun and de Rijke, 2005; Malakasiotis and
Androutsopoulos, 2007; Haghighi et al., 2005).
As Hickl (2008) stated, the shallow approaches do
not work well for long sentences for the missing
of underlying information which needs to be
mined from the surface level expression.
Recently, some deep techniques are developed
to mine the facts latent in the text. Hickl (2008)
proposed the concept of discourse commitments
which can be seen as the set of propositions in-
ferred from the text, and used a series of syntax-
level and semantic-level rules to extract the com-
mitments from the T-H pairs. Then the RTE task
is reduced to the identification of the commitments
from T which are most likely to support the infer-
ence of the commitments from H. From the work
of Hickl (2008), we can see that a deep under-
standing of text is critical to the RTE performance
and discourse commitments can serve a good me-
dia to understanding text. However, the limitation
of Hickl (2008)’s work is, the extracted discourse
commitments are still from the original text and do
not explore the implicit meaning latent behind the
text.
Another kind of deep methods involves first
transferring natural language to logic represen-
tation and then conducting strict logic inference
based on the logic representations (de Salvo Braz
et al., 2006; Tatu and Moldovan, 2006; Wotzlaw
and Coote, 2013). Through logic inference, some
implicit knowledge behind the text can be mined.
However, it is not easy to translate the natural lan-
guage text into formal logic expressions and the
translation process inevitably suffer from great in-
formation loss.
Through analysis above, in our work, we pro-
</bodyText>
<page confidence="0.908749">
1620
</page>
<note confidence="0.932633">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1620–1625,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.99995">
Figure 1: The Framework of our RTE system
</figureCaption>
<figure confidence="0.98780941025641">
Align the predicates to
the YAGO database
YAGO
T: Ayrton Senna was married to a doctor who
lives in Austin, the capital of Texas, in 1998.
H: Ayrton Senna lives in Texas.
T: R1(e11,e12)
R2(e21,e22)
R3(e31,e32)
H: RH(eH1,eH2)
YAGO
Inference rules
R
R
R
T
...
R
H
R1(e11,e12)^R2(e21,e22)=&gt;R3(e31,e32)
R4(e41,e42)^R5(e51,e52)=&gt;R6(e61,e62)
R7(e71,e72)^R8(e81,e82)=&gt;R9(e91,e92)
. ..
Rp(ep1,ep2)^Rq(eq1,eq2)=&gt;Rr(er1,er2)
Construct Markov
Logic Network
facts
P(RH)
H is True or false?
R
R
R
AIME
MLN
...
R
train
Discourse commitment
extract
</figure>
<bodyText confidence="0.99970025">
pose to use the predicate-argument structure to
represent the extracted discourse commitments.
Inspired by the work of (Mintz et al., 2009), we
make use of the external knowledge YAGO and
borrow the distant supervision technique to mine
implicit facts for the extracted predicates. For ex-
ample,
Ayrton Senna was married to a doctor who lives
in Austin, the capital of Texas, in 1998.
We translate this example into the predicate-
argument structures such as bemarried(Senna,
doctor), livein(doctor, Austin), captial(Austin,
Texas). Then through distant supervision, we
can get some new facts livein(Senna, Austin),
livein(Senna, Texas).
To judge the confidence of the new facts, we
construct a probabilistic network with all the facts
and adopt the Markov Logic Network (MLN) to
calculate the probability of each new fact, which
can be further used to recognize text entailments.
</bodyText>
<sectionHeader confidence="0.950053" genericHeader="method">
2 Our RTE System
</sectionHeader>
<bodyText confidence="0.999981260869565">
To make full use of the underlying information in
sentences and lessen the effect brought by natu-
ral language’s vagueness, we design a RTE sys-
tem which is composed of three stages, as shown
in Figure 1.
First, we decompose the sentences in T-H pair
to a series of discourse commitments as Hickl
(2008) did. Since the syntax of these commit-
ments are very simple, we can directly transform
them to predicates (or 3-arg tuples). Then we use
YAGO, a large semantic database including sev-
eral thousands relations, to provide distant super-
vision. The predicates in T are matched to YAGO
facts due to some metrics. At last, we use the
Markov Logic Network(MLN) (Richardson and
Domingos, 2006) to infer the correctness of the
predicates in H. The MLN is constructed us-
ing the inference rules (“soft” rules) generated by
AMIE system (Gal´arraga et al., 2013) on YAGO.
Each rule has a weight which should be trained by
real world facts. Using this framework, we can ap-
ply the “soft” logic to the textual entailment recog-
nition task.
</bodyText>
<subsectionHeader confidence="0.998278">
2.1 Extracting Predicates from Sentences
</subsectionHeader>
<bodyText confidence="0.999658307692308">
Discourse commitment is our baseline system pro-
posed by Hickl (2008) which can decompose one
sentence to a series of shorter and simpler sen-
tences which completely contain the origin sen-
tence’s information. One of the advantages of
discourse commitments is that it can use a lot of
syntax-level and semantic level rules to extract the
underlying information of one sentence. For ex-
ample, the following T-H pair can be decomposed
as Figure 2. The discourse commitments of T con-
tains the information: Ayrton Senna married in
1998, which is not easy to be captured by “shal-
low” methods.
</bodyText>
<listItem confidence="0.556989333333333">
T : Ayrton Senna was married to a doctor who
lives in Austin, the capital of Texas, in 1998.
H : Ayrton Senna lives in Texas.
</listItem>
<bodyText confidence="0.9881995">
Since we need to infer new facts using the ex-
tracted commitments, we transfer all the commit-
</bodyText>
<page confidence="0.959981">
1621
</page>
<bodyText confidence="0.718747">
Text: Ayrton Senna was married to a doctor who lives in
Austin, the capital of Texas.
</bodyText>
<figureCaption confidence="0.983714">
Figure 2: Text Commitments Example
</figureCaption>
<bodyText confidence="0.650739333333333">
Text: Ayrton Senna was married to a doctor who lives in
Austin, the capital of Texas.
H1. livein(Ayrton Senna, Texas)
</bodyText>
<note confidence="0.415833">
Hypothesis: Ayrton Senna lives in Texas.
</note>
<figureCaption confidence="0.974787">
Figure 3: Text Predicates Example
</figureCaption>
<figure confidence="0.943742571428571">
T1. Ayrton Senna was married to a doctor
T2. [The] doctor lives in Austin
T3. Austin [is] the capital of Texas
Hypothesis: Ayrton Senna lives in Texas.
T1. bemarriedTo(Ayrton Senna,Doctor
T2. livein(Doctor, Austin)
T3. beCapitalof(Austin, Texas)
</figure>
<bodyText confidence="0.999701909090909">
ments to predicates. For example, the commit-
ments in Figure 2 can be transformed to the pred-
icates (or triples) shown in Figure 3. We use RE-
VERB (Fader et al., 2011) to extract the triples
(predicate + 2 Arguments). To make the infer-
ence process in the next section more convenient,
we order that all of the arguments should be NPs.
Therefore, we check if the arguments in the triples
contain or have overlap with any of the NPs, re-
place it with that NP, and the predicates are suc-
cessfully extracted.
</bodyText>
<subsectionHeader confidence="0.959205">
2.2 Distant supervision with YAGO
</subsectionHeader>
<bodyText confidence="0.999829476190476">
The goal of distant supervision is to use the knowl-
edge of YAGO (Mahdisoltani et al., 2014) to help
the textual entailment recognition task. The facts
in YAGO have various type of connections with
each other. We think this connection is very useful
for RTE. Therefore, the predicates in the T-H pair
should be matched to the YAGO facts for making
advantage of the connection information.
Since YAGO is very large, the common predi-
cates can easily matched to YAGO facts in most
cases. However, YAGO cannot contain every
predicate in T. We run DIRT (Lin and Pan-
tel, 2001) system on 1GB text random sampled
from Gigaword corpus and for each predicate we
choose the top-10 similar predicates as its synony-
mous predicate. If the origin predicate cannot be
found in YAGO, we instead check for the top-10
similar predicates. If we still cannot find a match,
that means this predicate has very little connection
with other predicates and cannot be supervised by
YAGO.
</bodyText>
<subsectionHeader confidence="0.99395">
2.3 Probabilistic Inference
</subsectionHeader>
<bodyText confidence="0.999894">
The goal of MLN (Richardson and Domingos,
2006) is to implement the probabilistic inference,
or “soft” logic inference. MLN is constructed by
an inference rule base. Each rule has a weight
which needs to be well trained by real word
facts. We use AMIE to mine inference rules from
YAGO.
AMIE1 (Gal´arraga et al., 2013) is a state-of-
the-art inference rule mining system. The motiva-
tion of AMIE is that KBs themselves often already
contain enough information to derive and add new
facts. If, for example, a KB contains the fact that
a child has a mother, then the mother’s husband is
most likely the father.
</bodyText>
<equation confidence="0.430343">
motherOf(m, c)∧marriedTo(m, f) ⇒ fatherOf(f, c)
</equation>
<bodyText confidence="0.992843851851852">
AMIE can mine such inference rules from large
KBs. The inference rules can be directly used for
constructing Markov logic network in the next sec-
tion. In addition, the process of mining inference
rules is quite efficient so that it is very helpful for
our RTE task.
We use AMIE only to extract the inference
rules. After the inference rules are prepared, we
can construct a MLN. We give the related facts in
YAGO to the MLN, then the weights of each infer-
ence rule can tune to a best fit for these facts. After
the weights of each inference rule are well trained,
the MLN is well prepared to use. Given the pred-
icates in T, we first select all the related rules to
construct a simple MLN, and then give the MLN
some facts. After that, the MLN will calculate the
probabilities of the unknown new facts. The ar-
guments of the new facts are the permutations of
all the ground atoms (or entities). For example, if
we give the facts “hasChild (Cliton, Chelsea)” and
“IsMarriedto (Cliton, Hillary)”, the MLN will out-
put the probability of “hasChild (Cliton, Hillary)”,
“hasChild (Hillary, Chelsea)”, “IsMarriedto (Cli-
ton, Chelsea)”, etc. Obviously, the probability of
“hasChild (Hillary, Chelsea)” may be the highest,
so that it is most likely to be true. The MLN
constructing and inferring can be implemented by
</bodyText>
<footnote confidence="0.948349">
1http://www.mpi-inf.mpg.de/departments/databases-and-
information-systems/research/yago-naga/amie/
</footnote>
<page confidence="0.951909">
1622
</page>
<table confidence="0.998970428571429">
Approach Accuracy
Term overlap (Zanzotto and Moschitti, 2006) 67.50 %
Graph Matching (MacCartney et al., 2006) 65.33 %
Classification-Based (Hickl et al., 2006) 77.00 %
Discourse Commitment (Hickl, 2008) 84.93 %
Strict logic (Tatu and Moldovan, 2006) 71.59 %
Our Framework 85.16 %
</table>
<tableCaption confidence="0.999927">
Table 1: Performance of Inference based RTE
</tableCaption>
<bodyText confidence="0.999887">
Alchemy2, which provides a series of algorithms
for statistical relational learning and probabilistic
logic inference, based on the Markov logic repre-
sentation.
After the new facts are inferred, we check if
these facts can cover the predicates in H. If so,
we decide T entails H.
</bodyText>
<sectionHeader confidence="0.999406" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.999952387096774">
We evaluate the performance of our framework
for RTE on the PASCAL RTE-23 and RTE-34
datasets, which has 1600 examples. We use the
YAGO2 for aligning predicates and mining infer-
ence rules. YAGO2 contains more than 940K facts
and about 470K entities. We run the AMIE sys-
tem on YAGO2 for only one time to get all infer-
ence rules (about more than 1.8K in total). For
each T-H pair, we only choose a portion of re-
lated inference rules to construct MLN. The cho-
sen rules must contain at least one predicate which
occurred in the predicates of T-H pair. We only
use the MLN to infer when the discourse commit-
ment paraphrasing cannot identify a T-H pair as
”Entailment”, which is a back-off method.
We compare our result with 5 baseline systems:
(1) Zanzotto and Moschitti (2006)’s simple term-
overlap measure, (2) MacCartney et al. (2006)’s
semantic graph-mapping approach, (3) Hickl et al.
(2006)’s classification-based term alignment ap-
proach. (4) Hickl (2008)’s discourse commitment
based Alignment, (5) Tatu and Moldovan (2006)’s
strict logic based method. The comparison of the 5
baselines and our framework is shown in Table 1.
Since we only need to judge “Yes” or “No” for the
1600 examples, the precision is equal to the recall,
so that we only report the precision.
According to the Table 1, the performance of
our framework is higher than Hickl (2008)’s base-
line, which is significant (Wilcoxon signed-rank
test, p &lt; 0.05). The reason is that we have
</bodyText>
<footnote confidence="0.998877666666667">
2http://alchemy.cs.washington.edu/
3http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/
4http://pascallin.ecs.soton.ac.uk/Challenges/RTE3/
</footnote>
<bodyText confidence="0.999791818181818">
added the inference portion to Hickl (2008)’s
method. Therefore, some T-H pairs which had to
be judged by semantic reasoning can be corrected
by our framework. For instance, T is “Hughes
loved his wife, Gracia, and was absolutely ob-
sessed with his little daughter Elicia.” and H is
“Gracia’s daughter is Elicia.” It is not easy for the
former baselines to recognize this entailment, but
our framework can easily recognize it to be “true”.
In this way, our framework has achieved a higher
result.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="method">
4 Related work
</sectionHeader>
<bodyText confidence="0.999647617647059">
Textual Entailment Recognizing (RTE) task has
been widely studied by many previous works.
Firstly, the method based on similarity and over-
lap (Malakasiotis and Androutsopoulos, 2007; Ji-
jkoun and de Rijke, 2005; Wan et al., 2006). This
kind of methods can help solve the paraphrase
recognition problem, which is a subset of RTE.
Another important similarity-based method is tree
kernel (Zanzotto and Moschitti, 2006), which rely
on the cross-pair similarity between two pairs
(T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;).
Secondly, some approaches extract the knowl-
edge in T-H pair and check if the knowledge in T
contains the knowledge in H. Hickl (2008) trans-
formed the T-H pair into discourse commitments,
reducing the RTE task to the identification of the
commitments from a T which support the infer-
ence of the H. Other works map the text to logical
meaning representations, and then strict logic en-
tailment methods, possibly by invoking theorem
provers.
Thirdly, some works make use of statistical
classifiers which leverages a wide variety of fea-
tures. The language expression of each T-H pair
are represented by a feature vector (f1, f2 · · · fm).
The feature vector contains the scores of different
similarity measures applied to the pair, and possi-
bly other features.
There are also other works based on predicate-
argument representations with Markov Logic for
RTE, such as Rios et al. (2014) and Beltagy et al.
(2013). However, they did not use discourse com-
mitments to extract predicate-argument triples,
which may lead to severe information loss.
</bodyText>
<sectionHeader confidence="0.995819" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.963335">
This paper introduced a new framework to solve
the Textual Entailment Recognizing task. This
</bodyText>
<page confidence="0.953386">
1623
</page>
<bodyText confidence="0.999971722222222">
framework makes full use of Markov logic net-
work for probabilistic inference. We hold that
probabilistic inference is better than strict logic
method since transforming from language form
to strict logic form could lose a lot of informa-
tion. Therefore it is extremely hard for the theo-
rem provers to perform well.
In addition, we use YAGO database for distant
supervision. The predicates extracted from T-H
pair are first aligned with YAGO. If it succeeds, the
inference procedure of MLN will become much
more accurate. In addition, the inference rules for
constructing MLN are also extracted from YAGO
database using AIME system.
This framework can correctly recognize the en-
tailment T-H pairs which must be judged using
inference. This is our improvement over the pre-
vious work.
</bodyText>
<sectionHeader confidence="0.978722" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9985434">
This research is supported by National Key Basic
Research Program of China (No.2014CB340504)
and National Natural Science Foundation of China
(No.61375074,61273318). The contact authors of
this paper are Sujian Li and Baobao Chang.
</bodyText>
<sectionHeader confidence="0.998453" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998552949367088">
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Deep semantics with probabilistic logical form. In
Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (* SEM-13).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine learning challenges. evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising tectual entailment, pages 177–
190. Springer.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2006. An
inference model for semantic entailment in natural
language. In Machine Learning Challenges. Evalu-
ating Predictive Uncertainty, Visual Object Classi-
fication, and Recognising Tectual Entailment, pages
261–286. Springer.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535–1545. Association for Computational
Linguistics.
Luis Antonio Gal´arraga, Christina Teflioudi, Katja
Hose, and Fabian Suchanek. 2013. Amie: associ-
ation rule mining under incomplete evidence in on-
tological knowledge bases. In Proceedings of the
22nd international conference on World Wide Web,
pages 413–422. International World Wide Web Con-
ferences Steering Committee.
Aria D Haghighi, Andrew Y Ng, and Christopher D
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 387–394.
Association for Computational Linguistics.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with lccs groundhog sys-
tem. In Proceedings of the Second PASCAL Chal-
lenges Workshop.
Andrew Hickl. 2008. Using discourse commitments to
recognize textual entailment. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 337–344. Association
for Computational Linguistics.
Valentin Jijkoun and Maarten de Rijke. 2005. Recog-
nizing textual entailment using lexical similarity. In
Proceedings of the PASCAL Challenges Workshop
on Recognising Textual Entailment, pages 73–76.
Dekang Lin and Patrick Pantel. 2001. Dirt@ sbt@
discovery of inference rules from text. In Proceed-
ings of the seventh ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 323–328. ACM.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation of Computational Linguistics, pages 41–48.
Association for Computational Linguistics.
Farzaneh Mahdisoltani, Joanna Biega, and Fabian
Suchanek. 2014. Yago3: A knowledge base from
multilingual wikipedias. In 7th Biennial Conference
on Innovative Data Systems Research. CIDR 2015.
Prodromos Malakasiotis and Ion Androutsopoulos.
2007. Learning textual entailment using svms and
string similarity measures. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pages 42–47. Association for Com-
putational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
</reference>
<page confidence="0.875174">
1624
</page>
<reference confidence="0.999167">
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine learning, 62(1-
2):107–136.
Miguel Rios, Lucia Specia, Alexander Gelbukh, and
Ruslan Mitkov. 2014. Statistical relational learning
to recognise textual entailment. In Computational
Linguistics and Intelligent Text Processing, pages
330–339. Springer.
Marta Tatu and Dan Moldovan. 2006. A logic-
based semantic approach to recognizing textual en-
tailment. In Proceedings of the COLING/ACL on
Main conference poster sessions, pages 819–826.
Association for Computational Linguistics.
Lucy Vanderwende, Arul Menezes, and Rion Snow.
2006. Microsoft research at rte-2: Syntactic con-
tributions in the entailment task: an implementation.
In Proceedings of the Second PASCAL Challenges
Workshop.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. 2006. Using dependency-based features
to take the para-farce out of paraphrase. In Pro-
ceedings of the Australasian Language Technology
Workshop, volume 2006.
Andreas Wotzlaw and Ravi Coote. 2013. A logic-
based approach for recognizing textual entailment
supported by ontological background knowledge.
arXiv preprint arXiv:1310.4938.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments
with cross-pair similarities. In International confer-
ence on computational linguistics and the 44th An-
nual meeting of the Association for computational
linguistics, volume 1, pages 401–408. Association
for Computational Linguistics.
</reference>
<page confidence="0.992348">
1625
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.440853">
<title confidence="0.999238">Recognizing Textual Entailment Using Probabilistic Inference</title>
<author confidence="0.7353485">Lei Sha</author>
<author confidence="0.7353485">Sujian Li</author>
<author confidence="0.7353485">Tingsong Jiang</author>
<author confidence="0.7353485">Baobao Chang</author>
<author confidence="0.7353485">Zhifang Key Laboratory of Computational Linguistics</author>
<author confidence="0.7353485">Ministry of</author>
<affiliation confidence="0.996116">School of Electronics Engineering and Computer Science, Peking</affiliation>
<address confidence="0.991846">Collaborative Innovation Center for Language Ability, Xuzhou 221009</address>
<email confidence="0.978068">lisujian,tingsong,chbb,</email>
<abstract confidence="0.997579666666667">Recognizing Text Entailment (RTE) plays an important role in NLP applications including question answering, information retrieval, etc. In recent work, some research explore “deep” expressions such as discourse commitments or strict logic for representing the text. However, these expressions suffer from the limitation of inference inconvenience or translation loss. To overcome the limitations, in this paper, we propose to use the predicate-argument structures to represent the discourse commitments extracted from text. At the same time, with the help of the YAGO knowledge, we borrow the distant supervision technique to mine the implicit facts from the text. We also construct a probabilistic network for all the facts and conduct inference to judge the confidence of each fact for RTE. The experimental results show that our proposed method achieves a competitive result compared to the previous work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Cuong Chau</author>
<author>Gemma Boleda</author>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Deep semantics with probabilistic logical form.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (* SEM-13).</booktitle>
<contexts>
<context position="15651" citStr="Beltagy et al. (2013)" startWordPosition="2533" endWordPosition="2536">ce of the H. Other works map the text to logical meaning representations, and then strict logic entailment methods, possibly by invoking theorem provers. Thirdly, some works make use of statistical classifiers which leverages a wide variety of features. The language expression of each T-H pair are represented by a feature vector (f1, f2 · · · fm). The feature vector contains the scores of different similarity measures applied to the pair, and possibly other features. There are also other works based on predicateargument representations with Markov Logic for RTE, such as Rios et al. (2014) and Beltagy et al. (2013). However, they did not use discourse commitments to extract predicate-argument triples, which may lead to severe information loss. 5 Conclusion This paper introduced a new framework to solve the Textual Entailment Recognizing task. This 1623 framework makes full use of Markov logic network for probabilistic inference. We hold that probabilistic inference is better than strict logic method since transforming from language form to strict logic form could lose a lot of information. Therefore it is extremely hard for the theorem provers to perform well. In addition, we use YAGO database for dista</context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. Deep semantics with probabilistic logical form. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (* SEM-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1665" citStr="Dagan et al., 2006" startWordPosition="250" endWordPosition="253">s from the text. We also construct a probabilistic network for all the facts and conduct inference to judge the confidence of each fact for RTE. The experimental results show that our proposed method achieves a competitive result compared to the previous work. 1 Introduction For the natural language, a common phenomenon is that there exist a lot of ways to express the same or similar meaning. To discover such different expressions, the Recognising Textual Entailment (RTE) task is proposed to judge whether the meaning of one text (denoted as H) can be inferred (entailed) from the other one (T)(Dagan et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al.</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177– 190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodrigo de Salvo Braz</author>
<author>Roxana Girju</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
</authors>
<title>An inference model for semantic entailment in natural language. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment,</title>
<date>2006</date>
<pages>261--286</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3495" citStr="Braz et al., 2006" startWordPosition="537" endWordPosition="540">pport the inference of the commitments from H. From the work of Hickl (2008), we can see that a deep understanding of text is critical to the RTE performance and discourse commitments can serve a good media to understanding text. However, the limitation of Hickl (2008)’s work is, the extracted discourse commitments are still from the original text and do not explore the implicit meaning latent behind the text. Another kind of deep methods involves first transferring natural language to logic representation and then conducting strict logic inference based on the logic representations (de Salvo Braz et al., 2006; Tatu and Moldovan, 2006; Wotzlaw and Coote, 2013). Through logic inference, some implicit knowledge behind the text can be mined. However, it is not easy to translate the natural language text into formal logic expressions and the translation process inevitably suffer from great information loss. Through analysis above, in our work, we pro1620 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1620–1625, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: The Framework of our RTE system Align the predi</context>
</contexts>
<marker>Braz, Girju, Punyakanok, Roth, Sammons, 2006</marker>
<rawString>Rodrigo de Salvo Braz, Roxana Girju, Vasin Punyakanok, Dan Roth, and Mark Sammons. 2006. An inference model for semantic entailment in natural language. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 261–286. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8105" citStr="Fader et al., 2011" startWordPosition="1286" endWordPosition="1289">tments Example Text: Ayrton Senna was married to a doctor who lives in Austin, the capital of Texas. H1. livein(Ayrton Senna, Texas) Hypothesis: Ayrton Senna lives in Texas. Figure 3: Text Predicates Example T1. Ayrton Senna was married to a doctor T2. [The] doctor lives in Austin T3. Austin [is] the capital of Texas Hypothesis: Ayrton Senna lives in Texas. T1. bemarriedTo(Ayrton Senna,Doctor T2. livein(Doctor, Austin) T3. beCapitalof(Austin, Texas) ments to predicates. For example, the commitments in Figure 2 can be transformed to the predicates (or triples) shown in Figure 3. We use REVERB (Fader et al., 2011) to extract the triples (predicate + 2 Arguments). To make the inference process in the next section more convenient, we order that all of the arguments should be NPs. Therefore, we check if the arguments in the triples contain or have overlap with any of the NPs, replace it with that NP, and the predicates are successfully extracted. 2.2 Distant supervision with YAGO The goal of distant supervision is to use the knowledge of YAGO (Mahdisoltani et al., 2014) to help the textual entailment recognition task. The facts in YAGO have various type of connections with each other. We think this connec</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535–1545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Antonio Gal´arraga</author>
<author>Christina Teflioudi</author>
<author>Katja Hose</author>
<author>Fabian Suchanek</author>
</authors>
<title>Amie: association rule mining under incomplete evidence in ontological knowledge bases.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web,</booktitle>
<pages>413--422</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<marker>Gal´arraga, Teflioudi, Hose, Suchanek, 2013</marker>
<rawString>Luis Antonio Gal´arraga, Christina Teflioudi, Katja Hose, and Fabian Suchanek. 2013. Amie: association rule mining under incomplete evidence in ontological knowledge bases. In Proceedings of the 22nd international conference on World Wide Web, pages 413–422. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria D Haghighi</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>387--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2272" citStr="Haghighi et al., 2005" startWordPosition="330" endWordPosition="333">n et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T-H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likely</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria D Haghighi, Andrew Y Ng, and Christopher D Manning. 2005. Robust textual inference via graph matching. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 387–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with lccs groundhog system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="11686" citStr="Hickl et al., 2006" startWordPosition="1886" endWordPosition="1889">lsea)” and “IsMarriedto (Cliton, Hillary)”, the MLN will output the probability of “hasChild (Cliton, Hillary)”, “hasChild (Hillary, Chelsea)”, “IsMarriedto (Cliton, Chelsea)”, etc. Obviously, the probability of “hasChild (Hillary, Chelsea)” may be the highest, so that it is most likely to be true. The MLN constructing and inferring can be implemented by 1http://www.mpi-inf.mpg.de/departments/databases-andinformation-systems/research/yago-naga/amie/ 1622 Approach Accuracy Term overlap (Zanzotto and Moschitti, 2006) 67.50 % Graph Matching (MacCartney et al., 2006) 65.33 % Classification-Based (Hickl et al., 2006) 77.00 % Discourse Commitment (Hickl, 2008) 84.93 % Strict logic (Tatu and Moldovan, 2006) 71.59 % Our Framework 85.16 % Table 1: Performance of Inference based RTE Alchemy2, which provides a series of algorithms for statistical relational learning and probabilistic logic inference, based on the Markov logic representation. After the new facts are inferred, we check if these facts can cover the predicates in H. If so, we decide T entails H. 3 Experiment We evaluate the performance of our framework for RTE on the PASCAL RTE-23 and RTE-34 datasets, which has 1600 examples. We use the YAGO2 for a</context>
<context position="13034" citStr="Hickl et al. (2006)" startWordPosition="2117" endWordPosition="2120">m on YAGO2 for only one time to get all inference rules (about more than 1.8K in total). For each T-H pair, we only choose a portion of related inference rules to construct MLN. The chosen rules must contain at least one predicate which occurred in the predicates of T-H pair. We only use the MLN to infer when the discourse commitment paraphrasing cannot identify a T-H pair as ”Entailment”, which is a back-off method. We compare our result with 5 baseline systems: (1) Zanzotto and Moschitti (2006)’s simple termoverlap measure, (2) MacCartney et al. (2006)’s semantic graph-mapping approach, (3) Hickl et al. (2006)’s classification-based term alignment approach. (4) Hickl (2008)’s discourse commitment based Alignment, (5) Tatu and Moldovan (2006)’s strict logic based method. The comparison of the 5 baselines and our framework is shown in Table 1. Since we only need to judge “Yes” or “No” for the 1600 examples, the precision is equal to the recall, so that we only report the precision. According to the Table 1, the performance of our framework is higher than Hickl (2008)’s baseline, which is significant (Wilcoxon signed-rank test, p &lt; 0.05). The reason is that we have 2http://alchemy.cs.washington.edu/ 3</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing textual entailment with lccs groundhog system. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
</authors>
<title>Using discourse commitments to recognize textual entailment.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>337--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2289" citStr="Hickl (2008)" startWordPosition="335" endWordPosition="336">atural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T-H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likely to support the i</context>
<context position="5810" citStr="Hickl (2008)" startWordPosition="896" endWordPosition="897">facts livein(Senna, Austin), livein(Senna, Texas). To judge the confidence of the new facts, we construct a probabilistic network with all the facts and adopt the Markov Logic Network (MLN) to calculate the probability of each new fact, which can be further used to recognize text entailments. 2 Our RTE System To make full use of the underlying information in sentences and lessen the effect brought by natural language’s vagueness, we design a RTE system which is composed of three stages, as shown in Figure 1. First, we decompose the sentences in T-H pair to a series of discourse commitments as Hickl (2008) did. Since the syntax of these commitments are very simple, we can directly transform them to predicates (or 3-arg tuples). Then we use YAGO, a large semantic database including several thousands relations, to provide distant supervision. The predicates in T are matched to YAGO facts due to some metrics. At last, we use the Markov Logic Network(MLN) (Richardson and Domingos, 2006) to infer the correctness of the predicates in H. The MLN is constructed using the inference rules (“soft” rules) generated by AMIE system (Gal´arraga et al., 2013) on YAGO. Each rule has a weight which should be tra</context>
<context position="11729" citStr="Hickl, 2008" startWordPosition="1894" endWordPosition="1895">N will output the probability of “hasChild (Cliton, Hillary)”, “hasChild (Hillary, Chelsea)”, “IsMarriedto (Cliton, Chelsea)”, etc. Obviously, the probability of “hasChild (Hillary, Chelsea)” may be the highest, so that it is most likely to be true. The MLN constructing and inferring can be implemented by 1http://www.mpi-inf.mpg.de/departments/databases-andinformation-systems/research/yago-naga/amie/ 1622 Approach Accuracy Term overlap (Zanzotto and Moschitti, 2006) 67.50 % Graph Matching (MacCartney et al., 2006) 65.33 % Classification-Based (Hickl et al., 2006) 77.00 % Discourse Commitment (Hickl, 2008) 84.93 % Strict logic (Tatu and Moldovan, 2006) 71.59 % Our Framework 85.16 % Table 1: Performance of Inference based RTE Alchemy2, which provides a series of algorithms for statistical relational learning and probabilistic logic inference, based on the Markov logic representation. After the new facts are inferred, we check if these facts can cover the predicates in H. If so, we decide T entails H. 3 Experiment We evaluate the performance of our framework for RTE on the PASCAL RTE-23 and RTE-34 datasets, which has 1600 examples. We use the YAGO2 for aligning predicates and mining inference rul</context>
<context position="13099" citStr="Hickl (2008)" startWordPosition="2127" endWordPosition="2128"> 1.8K in total). For each T-H pair, we only choose a portion of related inference rules to construct MLN. The chosen rules must contain at least one predicate which occurred in the predicates of T-H pair. We only use the MLN to infer when the discourse commitment paraphrasing cannot identify a T-H pair as ”Entailment”, which is a back-off method. We compare our result with 5 baseline systems: (1) Zanzotto and Moschitti (2006)’s simple termoverlap measure, (2) MacCartney et al. (2006)’s semantic graph-mapping approach, (3) Hickl et al. (2006)’s classification-based term alignment approach. (4) Hickl (2008)’s discourse commitment based Alignment, (5) Tatu and Moldovan (2006)’s strict logic based method. The comparison of the 5 baselines and our framework is shown in Table 1. Since we only need to judge “Yes” or “No” for the 1600 examples, the precision is equal to the recall, so that we only report the precision. According to the Table 1, the performance of our framework is higher than Hickl (2008)’s baseline, which is significant (Wilcoxon signed-rank test, p &lt; 0.05). The reason is that we have 2http://alchemy.cs.washington.edu/ 3http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/ 4http://pascall</context>
<context position="14879" citStr="Hickl (2008)" startWordPosition="2405" endWordPosition="2406">cognizing (RTE) task has been widely studied by many previous works. Firstly, the method based on similarity and overlap (Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006). This kind of methods can help solve the paraphrase recognition problem, which is a subset of RTE. Another important similarity-based method is tree kernel (Zanzotto and Moschitti, 2006), which rely on the cross-pair similarity between two pairs (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;). Secondly, some approaches extract the knowledge in T-H pair and check if the knowledge in T contains the knowledge in H. Hickl (2008) transformed the T-H pair into discourse commitments, reducing the RTE task to the identification of the commitments from a T which support the inference of the H. Other works map the text to logical meaning representations, and then strict logic entailment methods, possibly by invoking theorem provers. Thirdly, some works make use of statistical classifiers which leverages a wide variety of features. The language expression of each T-H pair are represented by a feature vector (f1, f2 · · · fm). The feature vector contains the scores of different similarity measures applied to the pair, and po</context>
</contexts>
<marker>Hickl, 2008</marker>
<rawString>Andrew Hickl. 2008. Using discourse commitments to recognize textual entailment. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 337–344. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
</authors>
<title>Recognizing textual entailment using lexical similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<pages>73--76</pages>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Valentin Jijkoun and Maarten de Rijke. 2005. Recognizing textual entailment using lexical similarity. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt@ sbt@ discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>323--328</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9045" citStr="Lin and Pantel, 2001" startWordPosition="1451" endWordPosition="1455">sfully extracted. 2.2 Distant supervision with YAGO The goal of distant supervision is to use the knowledge of YAGO (Mahdisoltani et al., 2014) to help the textual entailment recognition task. The facts in YAGO have various type of connections with each other. We think this connection is very useful for RTE. Therefore, the predicates in the T-H pair should be matched to the YAGO facts for making advantage of the connection information. Since YAGO is very large, the common predicates can easily matched to YAGO facts in most cases. However, YAGO cannot contain every predicate in T. We run DIRT (Lin and Pantel, 2001) system on 1GB text random sampled from Gigaword corpus and for each predicate we choose the top-10 similar predicates as its synonymous predicate. If the origin predicate cannot be found in YAGO, we instead check for the top-10 similar predicates. If we still cannot find a match, that means this predicate has very little connection with other predicates and cannot be supervised by YAGO. 2.3 Probabilistic Inference The goal of MLN (Richardson and Domingos, 2006) is to implement the probabilistic inference, or “soft” logic inference. MLN is constructed by an inference rule base. Each rule has a</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt@ sbt@ discovery of inference rules from text. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 323–328. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 41–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farzaneh Mahdisoltani</author>
<author>Joanna Biega</author>
<author>Fabian Suchanek</author>
</authors>
<title>Yago3: A knowledge base from multilingual wikipedias.</title>
<date>2014</date>
<booktitle>In 7th Biennial Conference on Innovative Data Systems Research. CIDR</booktitle>
<contexts>
<context position="8567" citStr="Mahdisoltani et al., 2014" startWordPosition="1369" endWordPosition="1372">ents to predicates. For example, the commitments in Figure 2 can be transformed to the predicates (or triples) shown in Figure 3. We use REVERB (Fader et al., 2011) to extract the triples (predicate + 2 Arguments). To make the inference process in the next section more convenient, we order that all of the arguments should be NPs. Therefore, we check if the arguments in the triples contain or have overlap with any of the NPs, replace it with that NP, and the predicates are successfully extracted. 2.2 Distant supervision with YAGO The goal of distant supervision is to use the knowledge of YAGO (Mahdisoltani et al., 2014) to help the textual entailment recognition task. The facts in YAGO have various type of connections with each other. We think this connection is very useful for RTE. Therefore, the predicates in the T-H pair should be matched to the YAGO facts for making advantage of the connection information. Since YAGO is very large, the common predicates can easily matched to YAGO facts in most cases. However, YAGO cannot contain every predicate in T. We run DIRT (Lin and Pantel, 2001) system on 1GB text random sampled from Gigaword corpus and for each predicate we choose the top-10 similar predicates as </context>
</contexts>
<marker>Mahdisoltani, Biega, Suchanek, 2014</marker>
<rawString>Farzaneh Mahdisoltani, Joanna Biega, and Fabian Suchanek. 2014. Yago3: A knowledge base from multilingual wikipedias. In 7th Biennial Conference on Innovative Data Systems Research. CIDR 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prodromos Malakasiotis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Learning textual entailment using svms and string similarity measures.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2248" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="326" endWordPosition="329">d (entailed) from the other one (T)(Dagan et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T-H pairs. Then the RTE task is reduced to the identification of the commitments from</context>
<context position="14427" citStr="Malakasiotis and Androutsopoulos, 2007" startWordPosition="2327" endWordPosition="2330">Therefore, some T-H pairs which had to be judged by semantic reasoning can be corrected by our framework. For instance, T is “Hughes loved his wife, Gracia, and was absolutely obsessed with his little daughter Elicia.” and H is “Gracia’s daughter is Elicia.” It is not easy for the former baselines to recognize this entailment, but our framework can easily recognize it to be “true”. In this way, our framework has achieved a higher result. 4 Related work Textual Entailment Recognizing (RTE) task has been widely studied by many previous works. Firstly, the method based on similarity and overlap (Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006). This kind of methods can help solve the paraphrase recognition problem, which is a subset of RTE. Another important similarity-based method is tree kernel (Zanzotto and Moschitti, 2006), which rely on the cross-pair similarity between two pairs (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;). Secondly, some approaches extract the knowledge in T-H pair and check if the knowledge in T contains the knowledge in H. Hickl (2008) transformed the T-H pair into discourse commitments, reducing the RTE task to the identification of the commitments from a T which support the infe</context>
</contexts>
<marker>Malakasiotis, Androutsopoulos, 2007</marker>
<rawString>Prodromos Malakasiotis and Ion Androutsopoulos. 2007. Learning textual entailment using svms and string similarity measures. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 42–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4753" citStr="Mintz et al., 2009" startWordPosition="722" endWordPosition="725">on Senna was married to a doctor who lives in Austin, the capital of Texas, in 1998. H: Ayrton Senna lives in Texas. T: R1(e11,e12) R2(e21,e22) R3(e31,e32) H: RH(eH1,eH2) YAGO Inference rules R R R T ... R H R1(e11,e12)^R2(e21,e22)=&gt;R3(e31,e32) R4(e41,e42)^R5(e51,e52)=&gt;R6(e61,e62) R7(e71,e72)^R8(e81,e82)=&gt;R9(e91,e92) . .. Rp(ep1,ep2)^Rq(eq1,eq2)=&gt;Rr(er1,er2) Construct Markov Logic Network facts P(RH) H is True or false? R R R AIME MLN ... R train Discourse commitment extract pose to use the predicate-argument structure to represent the extracted discourse commitments. Inspired by the work of (Mintz et al., 2009), we make use of the external knowledge YAGO and borrow the distant supervision technique to mine implicit facts for the extracted predicates. For example, Ayrton Senna was married to a doctor who lives in Austin, the capital of Texas, in 1998. We translate this example into the predicateargument structures such as bemarried(Senna, doctor), livein(doctor, Austin), captial(Austin, Texas). Then through distant supervision, we can get some new facts livein(Senna, Austin), livein(Senna, Texas). To judge the confidence of the new facts, we construct a probabilistic network with all the facts and ad</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="6194" citStr="Richardson and Domingos, 2006" startWordPosition="958" endWordPosition="961">nces and lessen the effect brought by natural language’s vagueness, we design a RTE system which is composed of three stages, as shown in Figure 1. First, we decompose the sentences in T-H pair to a series of discourse commitments as Hickl (2008) did. Since the syntax of these commitments are very simple, we can directly transform them to predicates (or 3-arg tuples). Then we use YAGO, a large semantic database including several thousands relations, to provide distant supervision. The predicates in T are matched to YAGO facts due to some metrics. At last, we use the Markov Logic Network(MLN) (Richardson and Domingos, 2006) to infer the correctness of the predicates in H. The MLN is constructed using the inference rules (“soft” rules) generated by AMIE system (Gal´arraga et al., 2013) on YAGO. Each rule has a weight which should be trained by real world facts. Using this framework, we can apply the “soft” logic to the textual entailment recognition task. 2.1 Extracting Predicates from Sentences Discourse commitment is our baseline system proposed by Hickl (2008) which can decompose one sentence to a series of shorter and simpler sentences which completely contain the origin sentence’s information. One of the adv</context>
<context position="9511" citStr="Richardson and Domingos, 2006" startWordPosition="1528" endWordPosition="1531">y large, the common predicates can easily matched to YAGO facts in most cases. However, YAGO cannot contain every predicate in T. We run DIRT (Lin and Pantel, 2001) system on 1GB text random sampled from Gigaword corpus and for each predicate we choose the top-10 similar predicates as its synonymous predicate. If the origin predicate cannot be found in YAGO, we instead check for the top-10 similar predicates. If we still cannot find a match, that means this predicate has very little connection with other predicates and cannot be supervised by YAGO. 2.3 Probabilistic Inference The goal of MLN (Richardson and Domingos, 2006) is to implement the probabilistic inference, or “soft” logic inference. MLN is constructed by an inference rule base. Each rule has a weight which needs to be well trained by real word facts. We use AMIE to mine inference rules from YAGO. AMIE1 (Gal´arraga et al., 2013) is a state-ofthe-art inference rule mining system. The motivation of AMIE is that KBs themselves often already contain enough information to derive and add new facts. If, for example, a KB contains the fact that a child has a mother, then the mother’s husband is most likely the father. motherOf(m, c)∧marriedTo(m, f) ⇒ fatherOf</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine learning, 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Rios</author>
<author>Lucia Specia</author>
<author>Alexander Gelbukh</author>
<author>Ruslan Mitkov</author>
</authors>
<title>Statistical relational learning to recognise textual entailment.</title>
<date>2014</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>330--339</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="15625" citStr="Rios et al. (2014)" startWordPosition="2528" endWordPosition="2531">ich support the inference of the H. Other works map the text to logical meaning representations, and then strict logic entailment methods, possibly by invoking theorem provers. Thirdly, some works make use of statistical classifiers which leverages a wide variety of features. The language expression of each T-H pair are represented by a feature vector (f1, f2 · · · fm). The feature vector contains the scores of different similarity measures applied to the pair, and possibly other features. There are also other works based on predicateargument representations with Markov Logic for RTE, such as Rios et al. (2014) and Beltagy et al. (2013). However, they did not use discourse commitments to extract predicate-argument triples, which may lead to severe information loss. 5 Conclusion This paper introduced a new framework to solve the Textual Entailment Recognizing task. This 1623 framework makes full use of Markov logic network for probabilistic inference. We hold that probabilistic inference is better than strict logic method since transforming from language form to strict logic form could lose a lot of information. Therefore it is extremely hard for the theorem provers to perform well. In addition, we u</context>
</contexts>
<marker>Rios, Specia, Gelbukh, Mitkov, 2014</marker>
<rawString>Miguel Rios, Lucia Specia, Alexander Gelbukh, and Ruslan Mitkov. 2014. Statistical relational learning to recognise textual entailment. In Computational Linguistics and Intelligent Text Processing, pages 330–339. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Dan Moldovan</author>
</authors>
<title>A logicbased semantic approach to recognizing textual entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>819--826</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3520" citStr="Tatu and Moldovan, 2006" startWordPosition="541" endWordPosition="544"> of the commitments from H. From the work of Hickl (2008), we can see that a deep understanding of text is critical to the RTE performance and discourse commitments can serve a good media to understanding text. However, the limitation of Hickl (2008)’s work is, the extracted discourse commitments are still from the original text and do not explore the implicit meaning latent behind the text. Another kind of deep methods involves first transferring natural language to logic representation and then conducting strict logic inference based on the logic representations (de Salvo Braz et al., 2006; Tatu and Moldovan, 2006; Wotzlaw and Coote, 2013). Through logic inference, some implicit knowledge behind the text can be mined. However, it is not easy to translate the natural language text into formal logic expressions and the translation process inevitably suffer from great information loss. Through analysis above, in our work, we pro1620 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1620–1625, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: The Framework of our RTE system Align the predicates to the YAGO databas</context>
<context position="11776" citStr="Tatu and Moldovan, 2006" startWordPosition="1900" endWordPosition="1903">asChild (Cliton, Hillary)”, “hasChild (Hillary, Chelsea)”, “IsMarriedto (Cliton, Chelsea)”, etc. Obviously, the probability of “hasChild (Hillary, Chelsea)” may be the highest, so that it is most likely to be true. The MLN constructing and inferring can be implemented by 1http://www.mpi-inf.mpg.de/departments/databases-andinformation-systems/research/yago-naga/amie/ 1622 Approach Accuracy Term overlap (Zanzotto and Moschitti, 2006) 67.50 % Graph Matching (MacCartney et al., 2006) 65.33 % Classification-Based (Hickl et al., 2006) 77.00 % Discourse Commitment (Hickl, 2008) 84.93 % Strict logic (Tatu and Moldovan, 2006) 71.59 % Our Framework 85.16 % Table 1: Performance of Inference based RTE Alchemy2, which provides a series of algorithms for statistical relational learning and probabilistic logic inference, based on the Markov logic representation. After the new facts are inferred, we check if these facts can cover the predicates in H. If so, we decide T entails H. 3 Experiment We evaluate the performance of our framework for RTE on the PASCAL RTE-23 and RTE-34 datasets, which has 1600 examples. We use the YAGO2 for aligning predicates and mining inference rules. YAGO2 contains more than 940K facts and abo</context>
<context position="13168" citStr="Tatu and Moldovan (2006)" startWordPosition="2134" endWordPosition="2137">ion of related inference rules to construct MLN. The chosen rules must contain at least one predicate which occurred in the predicates of T-H pair. We only use the MLN to infer when the discourse commitment paraphrasing cannot identify a T-H pair as ”Entailment”, which is a back-off method. We compare our result with 5 baseline systems: (1) Zanzotto and Moschitti (2006)’s simple termoverlap measure, (2) MacCartney et al. (2006)’s semantic graph-mapping approach, (3) Hickl et al. (2006)’s classification-based term alignment approach. (4) Hickl (2008)’s discourse commitment based Alignment, (5) Tatu and Moldovan (2006)’s strict logic based method. The comparison of the 5 baselines and our framework is shown in Table 1. Since we only need to judge “Yes” or “No” for the 1600 examples, the precision is equal to the recall, so that we only report the precision. According to the Table 1, the performance of our framework is higher than Hickl (2008)’s baseline, which is significant (Wilcoxon signed-rank test, p &lt; 0.05). The reason is that we have 2http://alchemy.cs.washington.edu/ 3http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/ 4http://pascallin.ecs.soton.ac.uk/Challenges/RTE3/ added the inference portion to Hi</context>
</contexts>
<marker>Tatu, Moldovan, 2006</marker>
<rawString>Marta Tatu and Dan Moldovan. 2006. A logicbased semantic approach to recognizing textual entailment. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 819–826. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
<author>Arul Menezes</author>
<author>Rion Snow</author>
</authors>
<title>Microsoft research at rte-2: Syntactic contributions in the entailment task: an implementation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop.</booktitle>
<contexts>
<context position="2180" citStr="Vanderwende et al., 2006" startWordPosition="317" endWordPosition="320"> the meaning of one text (denoted as H) can be inferred (entailed) from the other one (T)(Dagan et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T-H pairs. Then t</context>
</contexts>
<marker>Vanderwende, Menezes, Snow, 2006</marker>
<rawString>Lucy Vanderwende, Arul Menezes, and Rion Snow. 2006. Microsoft research at rte-2: Syntactic contributions in the entailment task: an implementation. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Using dependency-based features to take the para-farce out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<volume>volume</volume>
<contexts>
<context position="14474" citStr="Wan et al., 2006" startWordPosition="2337" endWordPosition="2340">ng can be corrected by our framework. For instance, T is “Hughes loved his wife, Gracia, and was absolutely obsessed with his little daughter Elicia.” and H is “Gracia’s daughter is Elicia.” It is not easy for the former baselines to recognize this entailment, but our framework can easily recognize it to be “true”. In this way, our framework has achieved a higher result. 4 Related work Textual Entailment Recognizing (RTE) task has been widely studied by many previous works. Firstly, the method based on similarity and overlap (Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006). This kind of methods can help solve the paraphrase recognition problem, which is a subset of RTE. Another important similarity-based method is tree kernel (Zanzotto and Moschitti, 2006), which rely on the cross-pair similarity between two pairs (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;). Secondly, some approaches extract the knowledge in T-H pair and check if the knowledge in T contains the knowledge in H. Hickl (2008) transformed the T-H pair into discourse commitments, reducing the RTE task to the identification of the commitments from a T which support the inference of the H. Other works map the text to log</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2006. Using dependency-based features to take the para-farce out of paraphrase. In Proceedings of the Australasian Language Technology Workshop, volume 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Wotzlaw</author>
<author>Ravi Coote</author>
</authors>
<title>A logicbased approach for recognizing textual entailment supported by ontological background knowledge. arXiv preprint arXiv:1310.4938.</title>
<date>2013</date>
<contexts>
<context position="3546" citStr="Wotzlaw and Coote, 2013" startWordPosition="545" endWordPosition="548">H. From the work of Hickl (2008), we can see that a deep understanding of text is critical to the RTE performance and discourse commitments can serve a good media to understanding text. However, the limitation of Hickl (2008)’s work is, the extracted discourse commitments are still from the original text and do not explore the implicit meaning latent behind the text. Another kind of deep methods involves first transferring natural language to logic representation and then conducting strict logic inference based on the logic representations (de Salvo Braz et al., 2006; Tatu and Moldovan, 2006; Wotzlaw and Coote, 2013). Through logic inference, some implicit knowledge behind the text can be mined. However, it is not easy to translate the natural language text into formal logic expressions and the translation process inevitably suffer from great information loss. Through analysis above, in our work, we pro1620 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1620–1625, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: The Framework of our RTE system Align the predicates to the YAGO database YAGO T: Ayrton Senna was</context>
</contexts>
<marker>Wotzlaw, Coote, 2013</marker>
<rawString>Andreas Wotzlaw and Ravi Coote. 2013. A logicbased approach for recognizing textual entailment supported by ontological background knowledge. arXiv preprint arXiv:1310.4938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In International conference on computational linguistics and the 44th Annual meeting of the Association for computational linguistics,</booktitle>
<volume>1</volume>
<pages>401--408</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11587" citStr="Zanzotto and Moschitti, 2006" startWordPosition="1871" endWordPosition="1874"> permutations of all the ground atoms (or entities). For example, if we give the facts “hasChild (Cliton, Chelsea)” and “IsMarriedto (Cliton, Hillary)”, the MLN will output the probability of “hasChild (Cliton, Hillary)”, “hasChild (Hillary, Chelsea)”, “IsMarriedto (Cliton, Chelsea)”, etc. Obviously, the probability of “hasChild (Hillary, Chelsea)” may be the highest, so that it is most likely to be true. The MLN constructing and inferring can be implemented by 1http://www.mpi-inf.mpg.de/departments/databases-andinformation-systems/research/yago-naga/amie/ 1622 Approach Accuracy Term overlap (Zanzotto and Moschitti, 2006) 67.50 % Graph Matching (MacCartney et al., 2006) 65.33 % Classification-Based (Hickl et al., 2006) 77.00 % Discourse Commitment (Hickl, 2008) 84.93 % Strict logic (Tatu and Moldovan, 2006) 71.59 % Our Framework 85.16 % Table 1: Performance of Inference based RTE Alchemy2, which provides a series of algorithms for statistical relational learning and probabilistic logic inference, based on the Markov logic representation. After the new facts are inferred, we check if these facts can cover the predicates in H. If so, we decide T entails H. 3 Experiment We evaluate the performance of our framewor</context>
<context position="12916" citStr="Zanzotto and Moschitti (2006)" startWordPosition="2100" endWordPosition="2103">igning predicates and mining inference rules. YAGO2 contains more than 940K facts and about 470K entities. We run the AMIE system on YAGO2 for only one time to get all inference rules (about more than 1.8K in total). For each T-H pair, we only choose a portion of related inference rules to construct MLN. The chosen rules must contain at least one predicate which occurred in the predicates of T-H pair. We only use the MLN to infer when the discourse commitment paraphrasing cannot identify a T-H pair as ”Entailment”, which is a back-off method. We compare our result with 5 baseline systems: (1) Zanzotto and Moschitti (2006)’s simple termoverlap measure, (2) MacCartney et al. (2006)’s semantic graph-mapping approach, (3) Hickl et al. (2006)’s classification-based term alignment approach. (4) Hickl (2008)’s discourse commitment based Alignment, (5) Tatu and Moldovan (2006)’s strict logic based method. The comparison of the 5 baselines and our framework is shown in Table 1. Since we only need to judge “Yes” or “No” for the 1600 examples, the precision is equal to the recall, so that we only report the precision. According to the Table 1, the performance of our framework is higher than Hickl (2008)’s baseline, which</context>
<context position="14661" citStr="Zanzotto and Moschitti, 2006" startWordPosition="2365" endWordPosition="2368">ter is Elicia.” It is not easy for the former baselines to recognize this entailment, but our framework can easily recognize it to be “true”. In this way, our framework has achieved a higher result. 4 Related work Textual Entailment Recognizing (RTE) task has been widely studied by many previous works. Firstly, the method based on similarity and overlap (Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006). This kind of methods can help solve the paraphrase recognition problem, which is a subset of RTE. Another important similarity-based method is tree kernel (Zanzotto and Moschitti, 2006), which rely on the cross-pair similarity between two pairs (T&apos;, H&apos;) and (T&apos;&apos;, H&apos;&apos;). Secondly, some approaches extract the knowledge in T-H pair and check if the knowledge in T contains the knowledge in H. Hickl (2008) transformed the T-H pair into discourse commitments, reducing the RTE task to the identification of the commitments from a T which support the inference of the H. Other works map the text to logical meaning representations, and then strict logic entailment methods, possibly by invoking theorem provers. Thirdly, some works make use of statistical classifiers which leverages a wid</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In International conference on computational linguistics and the 44th Annual meeting of the Association for computational linguistics, volume 1, pages 401–408. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008833">
<title confidence="0.9975525">
Answering Elementary Science Questions by Constructing Coherent
Scenes using Background Knowledge
</title>
<author confidence="0.977466">
Yang Li∗ Peter Clark
</author>
<affiliation confidence="0.505328">
UC Santa Barbara Allen Institute for Artificial Intelligence
Santa Barbara, CA, USA Seattle, WA, USA
</affiliation>
<email confidence="0.994805">
yangli@cs.ucsb.edu peterc@allenai.org
</email>
<sectionHeader confidence="0.993775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999915483870968">
Much of what we understand from text is
not explicitly stated. Rather, the reader
uses his/her knowledge to fill in gaps
and create a coherent, mental picture or
“scene” depicting what text appears to
convey. The scene constitutes an under-
standing of the text, and can be used to an-
swer questions that go beyond the text.
Our goal is to answer elementary science
questions, where this requirement is per-
vasive; A question will often give a partial
description of a scene and ask the student
about implicit information. We show that
by using a simple “knowledge graph” rep-
resentation of the question, we can lever-
age several large-scale linguistic resources
to provide missing background knowl-
edge, somewhat alleviating the knowledge
bottleneck in previous approaches. The
coherence of the best resulting scene, built
from a question/answer-candidate pair, re-
flects the confidence that the answer can-
didate is correct, and thus can be used to
answer multiple choice questions. Our ex-
periments show that this approach outper-
forms competitive algorithms on several
datasets tested. The significance of this
work is thus to show that a simple “knowl-
edge graph” representation allows a ver-
sion of “interpretation as scene construc-
tion” to be made viable.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.987131725490196">
Elementary grade science tests are challenging as
they test a wide variety of commonsense knowl-
edge that human beings largely take for granted,
yet are very difficult for machines (Clark, 2015).
For example, consider a question from a NY Re-
gents 4th Grade science test:
∗Work was done while the author was an intern at Allen
Institute for Artificial Intelligence.
Question 1 “When a baby shakes a rattle, it
makes a noise. Which form of energy was changed
to sound energy?” [Answer: mechanical energy]
Science questions are typically quite different
from the entity-centric factoid questions exten-
sively studied in the question answering (QA)
community, e.g., “In which year was Bill Clinton
born?” (Ferrucci et al., 2010; Yao and Van Durme,
2014). While factoid questions are usually an-
swerable from text search or fact databases, sci-
ence questions typically require deeper analysis.
A full understanding of the above question in-
volves not just parsing and semantic interpreta-
tion; it involves adding implicit information to cre-
ate an overall picture of the “scene” that the text is
intended to convey, including facts such as: noise
is a kind of sound, the baby is holding the rattle,
shaking involves movement, the rattle is making
the noise, movement involves mechanical energy,
etc. This mental ability to create a scene from par-
tial information is at the heart of natural language
understanding (NLU), which is essential for an-
swering these kinds of question. It is also very dif-
ficult for a machine because it requires substantial
world knowledge, and there are often many ways
a scene can be elaborated.
We present a method for answering multiple-
choice questions that implements a simple ver-
sion of this. A scene is represented as a “knowl-
edge graph” of nodes (words) and relations, and
the scene is elaborated with (node,relation,node)
tuples drawn from three large-scale linguistic
knowledge resources: WordNet (Miller, 1995),
DART (Clark and Harrison, 2009), and the Free-
Association database (Nelson et al., 2004). These
elaborations reflect the mental process of “filling
in the gaps”, and multiple choice questions can
then be answered by finding which answer option
creates the most coherent scene.
The notion of NLU as constructing a most co-
herent scene is not new, and has has been stud-
ied in several contexts including work on scripts
(Schank and Abelson, 1977), interpretation as ab-
</bodyText>
<page confidence="0.936624">
2007
</page>
<note confidence="0.650399">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999591318181818">
duction (Hobbs et al., 1988; Hobbs, 1979; Ovchin-
nikova et al., 2014), bridging anaphora (Asher and
Lascarides, 1998; Fan et al., 2005), and para-
graph understanding (Zadrozny and Jensen, 1991;
Harabagiu and Moldovan, 1997). These meth-
ods are inspiring, but have previously been limited
by the lack of background knowledge to supply
implicit information, and with the complexity of
their representations. To make progress, we have
chosen to work with a simple “knowledge graph”
representation of nodes (words) and edges (rela-
tions). Although we lose some subtlety of expres-
sion, we gain the ability to leverage several vast
resources of world knowledge to supply implicit
information. The significance of this work is thus
to show that, by working with a simple “knowl-
edge graph” representation, we can make a viable
version of “interpretation as scene construction”.
Although the approach makes several simplifying
assumptions, our experiments show that it outper-
forms competitive algorithms on several datasets
of (real) elementary science questions.
</bodyText>
<sectionHeader confidence="0.98232" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999972347826087">
The input to our question-answering system is
a multiple choice question Q, a set of an-
swer options ak, and one or more background
knowledge base(s) each containing a set of
(wordi, relation, wordj) tuples, each denoting
that words is plausibly related to wordj by
relation. The output is a ranked list of the K an-
swer options.
We define a scene S as a “knowledge graph”
of nodes (words) and edges (relations between
words), where all (wordi, relation, wordj) edges
are sanctioned by (contained in) at least one of
the background knowledge bases. Each scene
node has an associated measure of coherence (de-
scribed shortly), denoting how well-connected it
is. The question-answering objective is, for each
answer option ak, to find the most coherent scene
containing (at least) the question keywords kwi ∈
Q and answer option ak, and then return the an-
swer option with the overall highest coherence
score. Our implementation approximates this ob-
jective using a simple elaborate-and-prune algo-
rithm, illustrated in Figure 11 and now described.
</bodyText>
<footnote confidence="0.718174">
1The system constructs 4 alternative graphs, each contains
</footnote>
<figureCaption confidence="0.9768965">
only one answer option plus some additional related nodes.
Figure 1 shows just one of these 4 graphs, namely the graph
containing answer option ”food”.
Figure 1: (1) Question keywords are extracted to
</figureCaption>
<bodyText confidence="0.969865714285714">
form the initial scene. (2) The scene is elaborated
with background knowledge to add plausible rela-
tionships. (3) For each answer option, it is added
into the scene and connected with additional rela-
tionships. Then the scene is pruned. (4) A score is
derived from the final scene, reflecting confidence
that the answer option is correct.
</bodyText>
<subsectionHeader confidence="0.998998">
2.1 Question Analyzer
</subsectionHeader>
<bodyText confidence="0.999580285714286">
The initial scene is simply the keywords (non-stop
words) KW = {kwi} in the question Q, along
with a measure of importance IS(kwi) for each
word kwi. For our purposes we compute impor-
tance by sending the question to Google, grouping
the top 20 result snippets into a document d, and
computing:
</bodyText>
<equation confidence="0.9716175">
tfd(kw)
IS(kw) = dfQ(kw), (1)
</equation>
<bodyText confidence="0.99950075">
where tfd(kw) is the term frequency of kw in doc-
ument d, and dfQ(kw) is the document frequency
of kw in question set Q containing all the available
elementary science questions. The intuition here is
</bodyText>
<page confidence="0.975909">
2008
</page>
<table confidence="0.98930875">
KB Size Examples
(# tuples)
WordNet 235k (dog,isa,animal)
(sunlight,isa,energy)
DART 2.3M (nutrients,in,food)
(animal,eat,food)
FreeAssoc 64k (car,relate to,tire)
(ice,relate to,cold)
</table>
<tableCaption confidence="0.999299">
Table 1: Knowledge Bases Used
</tableCaption>
<bodyText confidence="0.999896714285714">
that the words frequently mentioned in variations
of the question should be important (reflected by
”tf”), while the descriptive words (e.g. ”follow-
ing”, ”example”) which are widely used in many
questions should be penalized (reflected by ”idf”).
Other methods could equally be used to compute
importance.
</bodyText>
<subsectionHeader confidence="0.986816">
2.2 Builder
</subsectionHeader>
<bodyText confidence="0.998612285714286">
In this step our goal is to inject implicit knowl-
edge from the background KBs to form an elab-
orated knowledge graph. To do this, we first
fetch all the triples (kw, relation, w) that are
directly connected with any keyword kw E
KW from the background KBs, as well as all
(kwi, relation, kwj) triples between keywords.
In our experiments we use three background
knowledge bases to supply implicit knowledge, al-
though in principle any triple store could be used:
WordNet (Miller, 1995), DART (Clark and Har-
rison, 2009), and the FreeAssociation database
(Nelson et al., 2004). Table 1 shows examples of
each 2.
These triples introduce new nodes w into the
graph. As we may get a large number of such
nodes, we score them and retain only the top scor-
ing ones (and all edges connecting to it). Infor-
mally, a new word w is preferred if it is connected
to many important keywords kw with strong rela-
tionships. Formally, the scoring function is:
</bodyText>
<equation confidence="0.9913965">
�score(w) _ IS(kw) ∗ rel(kw, w) (2)
kwEK
</equation>
<bodyText confidence="0.999797333333333">
where IS(kw) is the importance score of keyword
kw and rel(kw, w) is the relatedness score be-
tween kw and w. In this work we use the co-
sine similarity between word2vec (Mikolov et al.,
2013) word vectors to measure two words’ related-
ness, as a rough proxy for the strength of related-
</bodyText>
<listItem confidence="0.768097666666667">
2WordNet: all relationships types are used. DART: The
NVN and NPN databases with frequency counts &gt; 10 are
used. FreeAssoc: The top 3 associations per word were used.
</listItem>
<bodyText confidence="0.9982515">
ness in the KBs (the KBs themselves do not pro-
vide meaningful strengths of relationship). After
the ranking, the top N x|KW  |neighbor words w
are retained3, along with their edges to keywords
kw and each other.
Note that at this point the elaboration process
is independent of any answer option; rather, the
graph depicts the question scenario.
</bodyText>
<subsectionHeader confidence="0.996103">
2.3 Elaborate and Prune
</subsectionHeader>
<bodyText confidence="0.9999882">
To score the K different answer options, the sys-
tem now builds K alternative elaborations of the
scene so far, each one with answer option ak
added, and assesses the coherence of the addition.
The answer option ak that fits “most coherently”
with the scene is returned as the answer to the
question.
To do this for a given option ak, we add ak to
the graph along with all triples (wi, relation, ak)
in the KBs that relate any node wi in the graph
to ak. Now that the focus ak of the question is
known, some of the earlier added nodes w in the
graph may be only weakly relevant to the question
and answer, and so we add a pruning step to re-
move these nodes. The goal of this pruning is to
find a dense subgraph (i.e. the coherent scene) that
would ideally contain all the question keywords
kw, the answer option ak, and extra words wk that
are highly connected with them.
Inspired by Sozio et al’s work (Sozio and Gio-
nis, 2010) on finding strongly interconnected sub-
groups in social networks, we have developed an
iterative node removal algorithm for extracting
this subgraph. We define the coherence of a node
as the summed weight of its incident edges:
</bodyText>
<equation confidence="0.9971545">
coherence(w) _ E rel(w, w&apos;) (3)
w1E{(w,r,w1)}
</equation>
<bodyText confidence="0.999817454545455">
where rel(w, w&apos;) is the weight of edge (w, r, w&apos;)
in the graph, again computed using cosine similar-
ity between w and w&apos;. We then iteratively remove
the non-keyword node (and attached edges) with
least coherence until the answer option ak is about
to removed. The resulting graph is thus maximally
pruned, subject to the constraint it must still de-
scribe the question plus answer option.
Finally, we use the coherence of the answer op-
tion ak in this final scene as the confidence that
ak is the correct answer. The system repeats this
</bodyText>
<footnote confidence="0.9770265">
3The optimal N (here 6) was selected using an indepen-
dent set of training questions
</footnote>
<page confidence="0.997977">
2009
</page>
<bodyText confidence="0.995886">
for all K answer options and selects the ak with
highest confidence.
</bodyText>
<sectionHeader confidence="0.997742" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999906727272727">
The system was developed using a dataset of natu-
ral (unedited) elementary science exam questions,
and then tested on three similar, unseen (hidden)
datasets. Its performance was compared with two
other state-of-the-art systems for this task. As our
system only fields questions where the answer op-
tions are all single words, we evaluate it, and the
other systems, only on these subsets. These sub-
sets are in general easier than other questions, but
this advantage is the same for all systems being
compared so it is still a fair test.
</bodyText>
<subsectionHeader confidence="0.994769">
3.1 Evaluation Datasets
</subsectionHeader>
<bodyText confidence="0.999918">
The datasets used are the non-diagram, multiple-
choice questions with single-word answer options
drawn from the following exams:
</bodyText>
<listItem confidence="0.981964111111111">
• Dev (System Development): New York Re-
gents 4th Grade Science 4 (47 questions in 6
years)
• Test1: New York Regents 4th Grade Science
(23 questions in 3 years)
• Test2: Additional 4th Grade Science (from
multiple States) (26 questions)
• Test3: 5th Grade Science (from multiple
States) (197 questions)
</listItem>
<bodyText confidence="0.997419666666667">
Although these datasets are small (real exam ques-
tions of this type are in limited supply), the num-
bers are large enough to draw conclusions.
</bodyText>
<subsectionHeader confidence="0.981127">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.9933335">
We compared our system (called SceneQA) with
two other state-of-the-art systems for this task:
</bodyText>
<listItem confidence="0.953791916666667">
• LSModel (Lexical semantics model): SVM
combination of several language models (likeli-
hood of answer given question) and information
retrieval scores (score of top retrieved sentence
matching question plus answer), trained on a set
of questions plus answers. (An expanded ver-
sion of Section 4.3 of (Jansen et al., 2014))
• A*Rules: “Prove” the answer option from the
question by applying lexical inference rules au-
tomatically extracted from science texts. Select
the option with the strongest “proof”. (Clark et
al., 2014)
</listItem>
<footnote confidence="0.909596">
4http://www.nysedregents.org/Grade4/Science/home.html
</footnote>
<table confidence="0.999189">
Dev Test1 Test2 Test3
LSModel 65.96 58.70 28.85 30.08
A*Rules 65.96 67.00 47.00 29.22
SceneQA 83.51 66.30 65.38 55.20
</table>
<tableCaption confidence="0.902922666666667">
Table 2: SceneQA outperforms two competitive
systems on two of the three test sets. The high-
lighted improvements are statistically significant.
</tableCaption>
<bodyText confidence="0.995545227272727">
The results (% scores, Table 2) show SceneQA
significantly outperforms the two other systems on
two of the three test sets, including the largest
(Test3, 197 questions), suggesting the approach
has merit.
We also performed some case studies to identify
what kinds of questions SceneQA does well on,
relative to the baselines. In general, SceneQA per-
forms well when the question words and the (cor-
rect) answer can be tightly related by background
knowledge, including through intermediate nodes
(words). For example, in Question 2 below:
Question 2 Which type of energy does a person
use to pedal a bicycle? (A) light (B) sound (C)
mechanical (D) electrical
the KB relates the correct answer ”mechanical” to
the question words ”energy”, ”pedal”, ”bicycle”,
and the intermediate node ”power” forming a tight
graph. In contrast, the other algorithms select the
wrong answer ”light” due to frequent mentions of
”bicycle lights” in their supporting text corpora
that confuses their algorithms.
</bodyText>
<subsectionHeader confidence="0.999249">
3.3 Ablations
</subsectionHeader>
<bodyText confidence="0.999269">
We also performed ablations to assess which parts
of our method are contributing the most:
</bodyText>
<listItem confidence="0.999934">
• -NewNodes: Only add edges but no new nodes
w during the Build step.
• -Prune: Do not prune nodes during the Elabo-
rate and Prune step.
• -Both: No new nodes, no pruning
</listItem>
<table confidence="0.9990184">
Dev Test1 Test2 Test3
SceneQA 83.51 66.30 65.38 55.20
-NewNodes 65.96 69.57 42.31 51.78
-Prune 70.74 57.61 47.12 50.13
-Both 59.57 65.22 42.31 50.25
</table>
<tableCaption confidence="0.996103">
Table 3: SceneQA outperforms all the ablations
</tableCaption>
<bodyText confidence="0.934306">
on two of the three test sets. The highlighted im-
provements are statistically significant.
The results (% scores, Table 3) suggest that the
two most important algorithmic features - adding
</bodyText>
<page confidence="0.974628">
2010
</page>
<bodyText confidence="0.9998335">
concepts implied but not explicitly stated in the
text (NewNodes), and later removing implied in-
formation that is of low relevance to the answer
(Prune) - are important for answering the ques-
tions correctly. (The small gain without adding
NewNodes on Test1 is not statistically significant).
</bodyText>
<subsectionHeader confidence="0.94139">
3.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.997205333333333">
We also examined cases where SceneQA gave the
wrong answer. Two problems were particularly
common:
</bodyText>
<listItem confidence="0.862723333333333">
(1) There were two answer options with oppo-
site meanings, and one of them was correct. For
example:
</listItem>
<construct confidence="0.806191">
Question 3 An animal that has a backbone is
called a(n) (A) invertebrate (B) vertebrate (C) ex-
oskeleton (D) sponge
</construct>
<bodyText confidence="0.997855444444444">
Since the relatedness measure we use (i.e.
word2vec) cannot distinguish words with similar
distributional semantics (a common property of
antonyms), our method cannot confidently iden-
tify which of the opposites (e.g., here, vertebrate
vs. invertebrate) is correct.
(2) The word ordering in the question is particu-
larly important, e.g., questions about processes or
sequences. For example:
</bodyText>
<figureCaption confidence="0.772345">
Question 4 The process that changes a gas to liq-
uid is called (A) condensation (B) melting (C)
evaporation (D) vaporization
</figureCaption>
<bodyText confidence="0.999920333333333">
Because our method ignores word order (the
knowledge graph is initially populated with key-
words in the question), the representation is inher-
ently incapable of capturing sequential informa-
tion (e.g., here, gas to liquid vs. liquid to gas).
As a result, it struggles with such questions.
</bodyText>
<sectionHeader confidence="0.998349" genericHeader="discussions">
4 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999995068965517">
Our goal is to answer simple science questions.
Unlike entity-centric factoid QA tasks, science
questions typically involve general concepts, and
answering them requires identifying implicit re-
lationships in the question. Our approach is to
view question-answering as constructing a coher-
ent scene. While the notion of scene construction
is not new, our insight is that this can be done
with a simple “knowledge graph” representation,
allowing several massive background KBs to be
applied, somewhat alleviating the knowledge bot-
tleneck. Our contribution is to show this works
well in the elementary science domain.
Despite this, there are clearly many limitations
with our approach: we are largely ignoring syn-
tactic structure in the questions; the KBs are
noisy, contributing errors to the scenes; the graph
representation has limited expressivity (e.g., no
quantification or negation); the word2vec measure
of relationship strength does not account for the
question context; and contradictions are not de-
tected within the scene. These all contributed to
QA failures in the tests. However, the approach is
appealing as it takes a step towards a richer picture
of language understanding, the empirical results
are encouraging, and there are many ways these
initial limitations can be addressed going forward.
We are confident that this is a rich and exciting
space, worthy of further exploration.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996915">
The research was supported by the Allen Insti-
tute for Artificial Intelligence (AI2). We thank the
Aristo team at AI2 for invaluable discussions, and
the anonymous reviewers for helpful comments.
</bodyText>
<sectionHeader confidence="0.998125" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999422740740741">
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15(1):83–113.
Peter Clark and Phil Harrison. 2009. Large-scale
extraction and use of knowledge from text. In
Proceedings of the fifth international conference on
Knowledge capture, pages 153–160.
Peter Clark, Niranjan Balasubramanian, Sum-
ithra Bhakthavatsalam, Kevin Humphreys, Jesse
Kinkead, Ashish Sabharwal, and Oyvind Tafjord.
2014. Automatic construction of inference-
supporting knowledge bases. In Proceedings of
AKBC.
Peter Clark. 2015. Elementary school science and
math tests as a driver for ai: Take the aristo chal-
lenge! In Twenty-Seventh IAAI Conference.
James Fan, Ken Barker, and Bruce Porter. 2005. Indi-
rect anaphora resolution as semantic path search. In
Proceedings of the 3rd international conference on
Knowledge capture, pages 153–160. ACM.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building watson: An overview
of the deepqa project. AI magazine, 31(3):59–79.
Sanda M Harabagiu and Dan I Moldovan. 1997.
Textnet–a text-based intelligent system. Natural
Language Engineering, 3(02):171–190.
</reference>
<page confidence="0.818319">
2011
</page>
<reference confidence="0.999861634146342">
Jerry R Hobbs, Mark Stickel, Paul Martin, and Dou-
glas Edwards. 1988. Interpretation as abduction. In
Proceedings of the 26th annual meeting on Associ-
ation for Computational Linguistics, pages 95–103.
Association for Computational Linguistics.
Jerry R Hobbs. 1979. Coherence and coreference.
Cognitive science, 3(1):67–90.
Peter Jansen, Mihai Surdeanu, and Peter Clark. 2014.
Discourse complements lexical semantics for non-
factoid answer reranking. In Proceedings of ACL,
pages 977–986.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Douglas L Nelson, Cathy L McEvoy, and Thomas A
Schreiber. 2004. The university of south florida free
association, rhyme, and word fragment norms. Be-
havior Research Methods, Instruments, &amp; Comput-
ers, 36(3):402–407.
Ekaterina Ovchinnikova, Niloofar Montazeri,
Theodore Alexandrov, Jerry R Hobbs, Michael C
McCord, and Rutu Mulkar-Mehta. 2014. Abductive
reasoning with a large knowledge base for dis-
course processing. In Computing Meaning, pages
107–127. Springer.
Roger C Schank and Robert P Abelson. 1977. Scripts,
Plans, Goals and Understanding. Erlbaum, Hills-
dale, NJ.
Mauro Sozio and Aristides Gionis. 2010. The
community-search problem and how to plan a suc-
cessful cocktail party. In Proceedings of SIGKDD,
pages 939–948.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of ACL.
Wlodek Zadrozny and Karen Jensen. 1991. Se-
mantics of paragraphs. Computational Linguistics,
17(2):171–209.
</reference>
<page confidence="0.995216">
2012
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.496000">
<title confidence="0.9128795">Answering Elementary Science Questions by Constructing Scenes using Background Knowledge</title>
<author confidence="0.561446">Clark</author>
<affiliation confidence="0.997288">UC Santa Barbara Allen Institute for Artificial Intelligence</affiliation>
<address confidence="0.990116">Santa Barbara, CA, USA Seattle, WA, USA</address>
<email confidence="0.992156">yangli@cs.ucsb.edupeterc@allenai.org</email>
<abstract confidence="0.99792759375">Much of what we understand from text is not explicitly stated. Rather, the reader uses his/her knowledge to fill in gaps and create a coherent, mental picture or “scene” depicting what text appears to convey. The scene constitutes an understanding of the text, and can be used to answer questions that go beyond the text. Our goal is to answer elementary science questions, where this requirement is pervasive; A question will often give a partial description of a scene and ask the student about implicit information. We show that by using a simple “knowledge graph” representation of the question, we can leverage several large-scale linguistic resources to provide missing background knowledge, somewhat alleviating the knowledge bottleneck in previous approaches. The coherence of the best resulting scene, built from a question/answer-candidate pair, reflects the confidence that the answer candidate is correct, and thus can be used to answer multiple choice questions. Our experiments show that this approach outperforms competitive algorithms on several datasets tested. The significance of this work is thus to show that a simple “knowledge graph” representation allows a version of “interpretation as scene construction” to be made viable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<date>1998</date>
<journal>Bridging. Journal of Semantics,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="4247" citStr="Asher and Lascarides, 1998" startWordPosition="667" endWordPosition="670">e gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resources of world knowledge to supply implicit information. The significance of this work is</context>
</contexts>
<marker>Asher, Lascarides, 1998</marker>
<rawString>Nicholas Asher and Alex Lascarides. 1998. Bridging. Journal of Semantics, 15(1):83–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Phil Harrison</author>
</authors>
<title>Large-scale extraction and use of knowledge from text.</title>
<date>2009</date>
<booktitle>In Proceedings of the fifth international conference on Knowledge capture,</booktitle>
<pages>153--160</pages>
<contexts>
<context position="3500" citStr="Clark and Harrison, 2009" startWordPosition="552" endWordPosition="555">rmation is at the heart of natural language understanding (NLU), which is essential for answering these kinds of question. It is also very difficult for a machine because it requires substantial world knowledge, and there are often many ways a scene can be elaborated. We present a method for answering multiplechoice questions that implements a simple version of this. A scene is represented as a “knowledge graph” of nodes (words) and relations, and the scene is elaborated with (node,relation,node) tuples drawn from three large-scale linguistic knowledge resources: WordNet (Miller, 1995), DART (Clark and Harrison, 2009), and the FreeAssociation database (Nelson et al., 2004). These elaborations reflect the mental process of “filling in the gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association</context>
<context position="8432" citStr="Clark and Harrison, 2009" startWordPosition="1336" endWordPosition="1340">ld be penalized (reflected by ”idf”). Other methods could equally be used to compute importance. 2.2 Builder In this step our goal is to inject implicit knowledge from the background KBs to form an elaborated knowledge graph. To do this, we first fetch all the triples (kw, relation, w) that are directly connected with any keyword kw E KW from the background KBs, as well as all (kwi, relation, kwj) triples between keywords. In our experiments we use three background knowledge bases to supply implicit knowledge, although in principle any triple store could be used: WordNet (Miller, 1995), DART (Clark and Harrison, 2009), and the FreeAssociation database (Nelson et al., 2004). Table 1 shows examples of each 2. These triples introduce new nodes w into the graph. As we may get a large number of such nodes, we score them and retain only the top scoring ones (and all edges connecting to it). Informally, a new word w is preferred if it is connected to many important keywords kw with strong relationships. Formally, the scoring function is: �score(w) _ IS(kw) ∗ rel(kw, w) (2) kwEK where IS(kw) is the importance score of keyword kw and rel(kw, w) is the relatedness score between kw and w. In this work we use the cosi</context>
</contexts>
<marker>Clark, Harrison, 2009</marker>
<rawString>Peter Clark and Phil Harrison. 2009. Large-scale extraction and use of knowledge from text. In Proceedings of the fifth international conference on Knowledge capture, pages 153–160.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Peter Clark</author>
<author>Niranjan Balasubramanian</author>
<author>Sumithra Bhakthavatsalam</author>
<author>Kevin Humphreys</author>
<author>Jesse Kinkead</author>
</authors>
<title>Ashish Sabharwal, and Oyvind Tafjord.</title>
<marker>Clark, Balasubramanian, Bhakthavatsalam, Humphreys, Kinkead, </marker>
<rawString>Peter Clark, Niranjan Balasubramanian, Sumithra Bhakthavatsalam, Kevin Humphreys, Jesse Kinkead, Ashish Sabharwal, and Oyvind Tafjord.</rawString>
</citation>
<citation valid="true">
<title>Automatic construction of inferencesupporting knowledge bases.</title>
<date>2014</date>
<booktitle>In Proceedings of AKBC.</booktitle>
<marker>2014</marker>
<rawString>2014. Automatic construction of inferencesupporting knowledge bases. In Proceedings of AKBC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
</authors>
<title>Elementary school science and math tests as a driver for ai: Take the aristo challenge!</title>
<date>2015</date>
<booktitle>In Twenty-Seventh IAAI Conference.</booktitle>
<contexts>
<context position="1725" citStr="Clark, 2015" startWordPosition="267" endWordPosition="268">ate pair, reflects the confidence that the answer candidate is correct, and thus can be used to answer multiple choice questions. Our experiments show that this approach outperforms competitive algorithms on several datasets tested. The significance of this work is thus to show that a simple “knowledge graph” representation allows a version of “interpretation as scene construction” to be made viable. 1 Introduction Elementary grade science tests are challenging as they test a wide variety of commonsense knowledge that human beings largely take for granted, yet are very difficult for machines (Clark, 2015). For example, consider a question from a NY Regents 4th Grade science test: ∗Work was done while the author was an intern at Allen Institute for Artificial Intelligence. Question 1 “When a baby shakes a rattle, it makes a noise. Which form of energy was changed to sound energy?” [Answer: mechanical energy] Science questions are typically quite different from the entity-centric factoid questions extensively studied in the question answering (QA) community, e.g., “In which year was Bill Clinton born?” (Ferrucci et al., 2010; Yao and Van Durme, 2014). While factoid questions are usually answerab</context>
</contexts>
<marker>Clark, 2015</marker>
<rawString>Peter Clark. 2015. Elementary school science and math tests as a driver for ai: Take the aristo challenge! In Twenty-Seventh IAAI Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Fan</author>
<author>Ken Barker</author>
<author>Bruce Porter</author>
</authors>
<title>Indirect anaphora resolution as semantic path search.</title>
<date>2005</date>
<booktitle>In Proceedings of the 3rd international conference on Knowledge capture,</booktitle>
<pages>153--160</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4266" citStr="Fan et al., 2005" startWordPosition="671" endWordPosition="674"> questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resources of world knowledge to supply implicit information. The significance of this work is thus to show that,</context>
</contexts>
<marker>Fan, Barker, Porter, 2005</marker>
<rawString>James Fan, Ken Barker, and Bruce Porter. 2005. Indirect anaphora resolution as semantic path search. In Proceedings of the 3rd international conference on Knowledge capture, pages 153–160. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya A Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
<author>Eric Nyberg</author>
<author>John Prager</author>
</authors>
<title>Building watson: An overview of the deepqa project.</title>
<date>2010</date>
<journal>AI magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="2253" citStr="Ferrucci et al., 2010" startWordPosition="350" endWordPosition="353">that human beings largely take for granted, yet are very difficult for machines (Clark, 2015). For example, consider a question from a NY Regents 4th Grade science test: ∗Work was done while the author was an intern at Allen Institute for Artificial Intelligence. Question 1 “When a baby shakes a rattle, it makes a noise. Which form of energy was changed to sound energy?” [Answer: mechanical energy] Science questions are typically quite different from the entity-centric factoid questions extensively studied in the question answering (QA) community, e.g., “In which year was Bill Clinton born?” (Ferrucci et al., 2010; Yao and Van Durme, 2014). While factoid questions are usually answerable from text search or fact databases, science questions typically require deeper analysis. A full understanding of the above question involves not just parsing and semantic interpretation; it involves adding implicit information to create an overall picture of the “scene” that the text is intended to convey, including facts such as: noise is a kind of sound, the baby is holding the rattle, shaking involves movement, the rattle is making the noise, movement involves mechanical energy, etc. This mental ability to create a s</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. 2010. Building watson: An overview of the deepqa project. AI magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Dan I Moldovan</author>
</authors>
<title>Textnet–a text-based intelligent system.</title>
<date>1997</date>
<journal>Natural Language Engineering,</journal>
<volume>3</volume>
<issue>02</issue>
<contexts>
<context position="4353" citStr="Harabagiu and Moldovan, 1997" startWordPosition="683" endWordPosition="686">most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resources of world knowledge to supply implicit information. The significance of this work is thus to show that, by working with a simple “knowledge graph” representation, we can make a viable versio</context>
</contexts>
<marker>Harabagiu, Moldovan, 1997</marker>
<rawString>Sanda M Harabagiu and Dan I Moldovan. 1997. Textnet–a text-based intelligent system. Natural Language Engineering, 3(02):171–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Mark Stickel</author>
<author>Paul Martin</author>
<author>Douglas Edwards</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>95--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4159" citStr="Hobbs et al., 1988" startWordPosition="654" endWordPosition="657">n et al., 2004). These elaborations reflect the mental process of “filling in the gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resou</context>
</contexts>
<marker>Hobbs, Stickel, Martin, Edwards, 1988</marker>
<rawString>Jerry R Hobbs, Mark Stickel, Paul Martin, and Douglas Edwards. 1988. Interpretation as abduction. In Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 95–103. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Coherence and coreference.</title>
<date>1979</date>
<journal>Cognitive science,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="4172" citStr="Hobbs, 1979" startWordPosition="658" endWordPosition="659">se elaborations reflect the mental process of “filling in the gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resources of world</context>
</contexts>
<marker>Hobbs, 1979</marker>
<rawString>Jerry R Hobbs. 1979. Coherence and coreference. Cognitive science, 3(1):67–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Jansen</author>
<author>Mihai Surdeanu</author>
<author>Peter Clark</author>
</authors>
<title>Discourse complements lexical semantics for nonfactoid answer reranking.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>977--986</pages>
<contexts>
<context position="13182" citStr="Jansen et al., 2014" startWordPosition="2164" endWordPosition="2167">Science (from multiple States) (197 questions) Although these datasets are small (real exam questions of this type are in limited supply), the numbers are large enough to draw conclusions. 3.2 Experiments We compared our system (called SceneQA) with two other state-of-the-art systems for this task: • LSModel (Lexical semantics model): SVM combination of several language models (likelihood of answer given question) and information retrieval scores (score of top retrieved sentence matching question plus answer), trained on a set of questions plus answers. (An expanded version of Section 4.3 of (Jansen et al., 2014)) • A*Rules: “Prove” the answer option from the question by applying lexical inference rules automatically extracted from science texts. Select the option with the strongest “proof”. (Clark et al., 2014) 4http://www.nysedregents.org/Grade4/Science/home.html Dev Test1 Test2 Test3 LSModel 65.96 58.70 28.85 30.08 A*Rules 65.96 67.00 47.00 29.22 SceneQA 83.51 66.30 65.38 55.20 Table 2: SceneQA outperforms two competitive systems on two of the three test sets. The highlighted improvements are statistically significant. The results (% scores, Table 2) show SceneQA significantly outperforms the two o</context>
</contexts>
<marker>Jansen, Surdeanu, Clark, 2014</marker>
<rawString>Peter Jansen, Mihai Surdeanu, and Peter Clark. 2014. Discourse complements lexical semantics for nonfactoid answer reranking. In Proceedings of ACL, pages 977–986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of ICLR.</booktitle>
<contexts>
<context position="9085" citStr="Mikolov et al., 2013" startWordPosition="1458" endWordPosition="1461">se (Nelson et al., 2004). Table 1 shows examples of each 2. These triples introduce new nodes w into the graph. As we may get a large number of such nodes, we score them and retain only the top scoring ones (and all edges connecting to it). Informally, a new word w is preferred if it is connected to many important keywords kw with strong relationships. Formally, the scoring function is: �score(w) _ IS(kw) ∗ rel(kw, w) (2) kwEK where IS(kw) is the importance score of keyword kw and rel(kw, w) is the relatedness score between kw and w. In this work we use the cosine similarity between word2vec (Mikolov et al., 2013) word vectors to measure two words’ relatedness, as a rough proxy for the strength of related2WordNet: all relationships types are used. DART: The NVN and NPN databases with frequency counts &gt; 10 are used. FreeAssoc: The top 3 associations per word were used. ness in the KBs (the KBs themselves do not provide meaningful strengths of relationship). After the ranking, the top N x|KW |neighbor words w are retained3, along with their edges to keywords kw and each other. Note that at this point the elaboration process is independent of any answer option; rather, the graph depicts the question scena</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3467" citStr="Miller, 1995" startWordPosition="549" endWordPosition="550">ene from partial information is at the heart of natural language understanding (NLU), which is essential for answering these kinds of question. It is also very difficult for a machine because it requires substantial world knowledge, and there are often many ways a scene can be elaborated. We present a method for answering multiplechoice questions that implements a simple version of this. A scene is represented as a “knowledge graph” of nodes (words) and relations, and the scene is elaborated with (node,relation,node) tuples drawn from three large-scale linguistic knowledge resources: WordNet (Miller, 1995), DART (Clark and Harrison, 2009), and the FreeAssociation database (Nelson et al., 2004). These elaborations reflect the mental process of “filling in the gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 S</context>
<context position="8399" citStr="Miller, 1995" startWordPosition="1333" endWordPosition="1334">n many questions should be penalized (reflected by ”idf”). Other methods could equally be used to compute importance. 2.2 Builder In this step our goal is to inject implicit knowledge from the background KBs to form an elaborated knowledge graph. To do this, we first fetch all the triples (kw, relation, w) that are directly connected with any keyword kw E KW from the background KBs, as well as all (kwi, relation, kwj) triples between keywords. In our experiments we use three background knowledge bases to supply implicit knowledge, although in principle any triple store could be used: WordNet (Miller, 1995), DART (Clark and Harrison, 2009), and the FreeAssociation database (Nelson et al., 2004). Table 1 shows examples of each 2. These triples introduce new nodes w into the graph. As we may get a large number of such nodes, we score them and retain only the top scoring ones (and all edges connecting to it). Informally, a new word w is preferred if it is connected to many important keywords kw with strong relationships. Formally, the scoring function is: �score(w) _ IS(kw) ∗ rel(kw, w) (2) kwEK where IS(kw) is the importance score of keyword kw and rel(kw, w) is the relatedness score between kw an</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<title>The university of south florida free association, rhyme, and word fragment norms.</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="3556" citStr="Nelson et al., 2004" startWordPosition="561" endWordPosition="564">U), which is essential for answering these kinds of question. It is also very difficult for a machine because it requires substantial world knowledge, and there are often many ways a scene can be elaborated. We present a method for answering multiplechoice questions that implements a simple version of this. A scene is represented as a “knowledge graph” of nodes (words) and relations, and the scene is elaborated with (node,relation,node) tuples drawn from three large-scale linguistic knowledge resources: WordNet (Miller, 1995), DART (Clark and Harrison, 2009), and the FreeAssociation database (Nelson et al., 2004). These elaborations reflect the mental process of “filling in the gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1</context>
<context position="8488" citStr="Nelson et al., 2004" startWordPosition="1345" endWordPosition="1348">ally be used to compute importance. 2.2 Builder In this step our goal is to inject implicit knowledge from the background KBs to form an elaborated knowledge graph. To do this, we first fetch all the triples (kw, relation, w) that are directly connected with any keyword kw E KW from the background KBs, as well as all (kwi, relation, kwj) triples between keywords. In our experiments we use three background knowledge bases to supply implicit knowledge, although in principle any triple store could be used: WordNet (Miller, 1995), DART (Clark and Harrison, 2009), and the FreeAssociation database (Nelson et al., 2004). Table 1 shows examples of each 2. These triples introduce new nodes w into the graph. As we may get a large number of such nodes, we score them and retain only the top scoring ones (and all edges connecting to it). Informally, a new word w is preferred if it is connected to many important keywords kw with strong relationships. Formally, the scoring function is: �score(w) _ IS(kw) ∗ rel(kw, w) (2) kwEK where IS(kw) is the importance score of keyword kw and rel(kw, w) is the relatedness score between kw and w. In this work we use the cosine similarity between word2vec (Mikolov et al., 2013) wo</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 2004</marker>
<rawString>Douglas L Nelson, Cathy L McEvoy, and Thomas A Schreiber. 2004. The university of south florida free association, rhyme, and word fragment norms. Behavior Research Methods, Instruments, &amp; Computers, 36(3):402–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Ovchinnikova</author>
<author>Niloofar Montazeri</author>
<author>Theodore Alexandrov</author>
<author>Jerry R Hobbs</author>
<author>Michael C McCord</author>
<author>Rutu Mulkar-Mehta</author>
</authors>
<title>Abductive reasoning with a large knowledge base for discourse processing.</title>
<date>2014</date>
<booktitle>In Computing Meaning,</booktitle>
<pages>107--127</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4200" citStr="Ovchinnikova et al., 2014" startWordPosition="660" endWordPosition="664">ns reflect the mental process of “filling in the gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resources of world knowledge to supply implici</context>
</contexts>
<marker>Ovchinnikova, Montazeri, Alexandrov, Hobbs, McCord, Mulkar-Mehta, 2014</marker>
<rawString>Ekaterina Ovchinnikova, Niloofar Montazeri, Theodore Alexandrov, Jerry R Hobbs, Michael C McCord, and Rutu Mulkar-Mehta. 2014. Abductive reasoning with a large knowledge base for discourse processing. In Computing Meaning, pages 107–127. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<date>1977</date>
<booktitle>Scripts, Plans, Goals and Understanding. Erlbaum,</booktitle>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="3910" citStr="Schank and Abelson, 1977" startWordPosition="621" endWordPosition="624">raph” of nodes (words) and relations, and the scene is elaborated with (node,relation,node) tuples drawn from three large-scale linguistic knowledge resources: WordNet (Miller, 1995), DART (Clark and Harrison, 2009), and the FreeAssociation database (Nelson et al., 2004). These elaborations reflect the mental process of “filling in the gaps”, and multiple choice questions can then be answered by finding which answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity o</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C Schank and Robert P Abelson. 1977. Scripts, Plans, Goals and Understanding. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Sozio</author>
<author>Aristides Gionis</author>
</authors>
<title>The community-search problem and how to plan a successful cocktail party.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGKDD,</booktitle>
<pages>939--948</pages>
<contexts>
<context position="10640" citStr="Sozio and Gionis, 2010" startWordPosition="1742" endWordPosition="1746">do this for a given option ak, we add ak to the graph along with all triples (wi, relation, ak) in the KBs that relate any node wi in the graph to ak. Now that the focus ak of the question is known, some of the earlier added nodes w in the graph may be only weakly relevant to the question and answer, and so we add a pruning step to remove these nodes. The goal of this pruning is to find a dense subgraph (i.e. the coherent scene) that would ideally contain all the question keywords kw, the answer option ak, and extra words wk that are highly connected with them. Inspired by Sozio et al’s work (Sozio and Gionis, 2010) on finding strongly interconnected subgroups in social networks, we have developed an iterative node removal algorithm for extracting this subgraph. We define the coherence of a node as the summed weight of its incident edges: coherence(w) _ E rel(w, w&apos;) (3) w1E{(w,r,w1)} where rel(w, w&apos;) is the weight of edge (w, r, w&apos;) in the graph, again computed using cosine similarity between w and w&apos;. We then iteratively remove the non-keyword node (and attached edges) with least coherence until the answer option ak is about to removed. The resulting graph is thus maximally pruned, subject to the constr</context>
</contexts>
<marker>Sozio, Gionis, 2010</marker>
<rawString>Mauro Sozio and Aristides Gionis. 2010. The community-search problem and how to plan a successful cocktail party. In Proceedings of SIGKDD, pages 939–948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with freebase.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wlodek Zadrozny</author>
<author>Karen Jensen</author>
</authors>
<title>Semantics of paragraphs.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="4322" citStr="Zadrozny and Jensen, 1991" startWordPosition="679" endWordPosition="682"> answer option creates the most coherent scene. The notion of NLU as constructing a most coherent scene is not new, and has has been studied in several contexts including work on scripts (Schank and Abelson, 1977), interpretation as ab2007 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2007–2012, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. duction (Hobbs et al., 1988; Hobbs, 1979; Ovchinnikova et al., 2014), bridging anaphora (Asher and Lascarides, 1998; Fan et al., 2005), and paragraph understanding (Zadrozny and Jensen, 1991; Harabagiu and Moldovan, 1997). These methods are inspiring, but have previously been limited by the lack of background knowledge to supply implicit information, and with the complexity of their representations. To make progress, we have chosen to work with a simple “knowledge graph” representation of nodes (words) and edges (relations). Although we lose some subtlety of expression, we gain the ability to leverage several vast resources of world knowledge to supply implicit information. The significance of this work is thus to show that, by working with a simple “knowledge graph” representati</context>
</contexts>
<marker>Zadrozny, Jensen, 1991</marker>
<rawString>Wlodek Zadrozny and Karen Jensen. 1991. Semantics of paragraphs. Computational Linguistics, 17(2):171–209.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
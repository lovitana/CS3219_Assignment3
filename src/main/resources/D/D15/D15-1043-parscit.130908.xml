<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000607">
<title confidence="0.999149">
An Empirical Comparison Between N-gram and Syntactic Language
Models for Word Ordering
</title>
<author confidence="0.999161">
Jiangming Liu and Yue Zhang
</author>
<affiliation confidence="0.999759">
Singapore University of Technology and Design,
</affiliation>
<address confidence="0.967827">
8 Somapah Road, Singapore, 487372
</address>
<email confidence="0.988985">
{jiangming liu, yue zhang}@sutd.edu.sg
</email>
<sectionHeader confidence="0.997183" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999852222222222">
Syntactic language models and N-gram
language models have both been used in
word ordering. In this paper, we give
an empirical comparison between N-gram
and syntactic language models on word or-
der task. Our results show that the quality
of automatically-parsed training data has a
relatively small impact on syntactic mod-
els. Both of syntactic and N-gram mod-
els can benefit from large-scale raw text.
Compared with N-gram models, syntac-
tic models give overall better performance,
but they require much more training time.
In addition, the two models lead to differ-
ent error distributions in word ordering. A
combination of the two models integrates
the advantages of each model, achieving
the best result in a standard benchmark.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954596153846">
N-gram language models have been used in a wide
range of the generation tasks, such as machine
translation (Koehn et al., 2003; Chiang, 2007;
Galley et al., 2004), text summarization (Barzilay
and McKeown, 2005) and realization (Guo et al.,
2011). Such models are trained from large-scale
raw text, capturing distributions of local word N-
grams, which can be used to improve the fluency
of synthesized text.
More recently, syntactic language models have
been used as a complement or alternative to N-
gram language models for machine translation
(Charniak et al., 2003; Shen et al., 2008; Schwartz
et al., 2011), syntactic analysis (Chen et al.,
2012) and tree linearization (Song et al., 2014).
Compared with N-gram models, syntactic mod-
els capture rich structural information, and can be
more effective in improving the fluency of large
constituents, long-range dependencies and over-
all sentential grammaticality. However, Syntactic
models require annotated syntactic structures for
training, which are expensive to obtain manually.
In addition, they can be slower compared to N-
gram models.
In this paper, we make an empirical compari-
son between syntactic and N-gram language mod-
els on the task of word ordering (Wan et al., 2009;
Zhang and Clark, 2011a; De Gispert et al., 2014),
which is to order a set of input words into a gram-
matical and fluent sentence. The task can be re-
garded as an abstract language modeling problem,
although methods have been explored extending it
for tree linearization (Zhang, 2013), broader text
generation (Song et al., 2014) and machine trans-
lation (Zhang et al., 2014).
We choose the model of Liu et al.(2015) as the
syntactic language model. There has been two
main types of syntactic language models in the
literature, the first being relatively more oriented
to syntactic structure, without an explicit empha-
sis on word orders (Shen et al., 2008; Chen et al.,
2012). As a result, this type of syntactic language
models are typically used jointly with N-gram
model for text-to-text tasks. The second type mod-
els syntactic structures incrementally, thereby can
be used to directly score surface orders (Schwartz
et al., 2011; Liu et al., 2015). We choose the dis-
criminative model of Liu et al. (2015), which gives
state-of-the-art results for word ordering.
We try to answer the following research ques-
tions by comparing the syntactic model and the N-
gram model using the same search algorithm.
</bodyText>
<listItem confidence="0.7789555">
• What is the influence of automatically-
parsed training data on the performance of
</listItem>
<bodyText confidence="0.999853428571429">
syntactic models. Because manual syntac-
tic annotations are relatively limited and highly
expensive, it is necessary to use large-scale
automatically-parsed sentences for training syn-
tactic language models. As a result, the syntac-
tic structures that a word ordering system learns
can be inaccurate. However, this might not affect
</bodyText>
<page confidence="0.9857">
369
</page>
<note confidence="0.991439">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 369–378,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.975524">
Initial State ([], set(1...n), Ø)
Final State ([], Ø, A)
Induction Rules:
(σ,p,A)
SHIFT
L-ARC ([σ|i],p,A∪{j←i})
</figure>
<figureCaption confidence="0.9977295">
Figure 1: Deduction system for transition-based
linearization.
</figureCaption>
<bodyText confidence="0.928999">
the quality of the synthesized output, which is a
string only. We quantitatively study the influence
of parsing accuracy of syntactic training data on
word ordering output.
</bodyText>
<listItem confidence="0.9923638">
• What is the influence of data scale on the
performance. N-gram language models can be
trained efficiently over large numbers of raw sen-
tences. In contrast, syntactic language models can
be much slower to train due to rich features. We
compare the output quality of the two models on
different scales of training data, and also on differ-
ent amounts of training time.
• What are the errors characteristics of each
model. Syntactic language models can poten-
tially be better in capturing larger constituents and
overall sentence structures. However, compared
with N-gram models, little work has been done
to quantify the difference between the two mod-
els. We characterise the outputs using a set of dif-
ferent measures, and show empirically the relative
strength and weakness of each model.
• What is the effect of model combination.
Finally, because the two models make different
types of errors, they can be combined to give bet-
ter outputs. We develop a combined model by dis-
cretizing probability from N-gram model, and us-
ing them as features in the syntactic model. The
combined model gives the best results in a stan-
dard benchmark.
</listItem>
<sectionHeader confidence="0.995147" genericHeader="introduction">
2 Systems
</sectionHeader>
<subsectionHeader confidence="0.996326">
2.1 Syntactic word ordering
</subsectionHeader>
<bodyText confidence="0.999570166666667">
Syntactic word ordering algorithms take a multi-
set of input words constructing an output sen-
tence and its syntactic derivation simultaneously.
Transition-based syntactic word ordering can be
modelled as an extension to transition-based pars-
ing (Liu et al., 2015), with the main difference be-
</bodyText>
<table confidence="0.556018714285714">
step action a p A
init [] (0 1 2) Ø
0 shift [1] (0 2)
1 shift [1 2] (0)
2 L-arc [2] (0) A U {1 , 21
3 shift [2 0] ()
4 R-arc [2] () A U {2 , 01
</table>
<figureCaption confidence="0.9813645">
Figure 2: Transition-based process for ordering
{“potatoes0”, “Tom1”, “likes2”}.
</figureCaption>
<bodyText confidence="0.999283567567568">
ing that the order of words is not given in the input,
which leads to a much larger search space.
We take the system of Liu, et al.1, which gives
state-of-the-art performance and efficiencies in
standard word ordering benchmark. It maintains
outputs in stack σ, and orders the unprocessed in-
coming words in a set p. Given an input bag of
words, p is initialized to the input and σ is ini-
tialized as empty. The system repeatedly applies
transition actions to consume words from p and
construct output on σ.
Figure 1 shows the deduction system, where p
is unordered and any word in p can be shifted onto
the stack σ. The set of actions are SHIFT, L-ARC
and R-ARC. The SHIFT actions add a word to the
stack. For the L-ARC and R-ARC actions, new
arcs {j ← i} and {j → i} are constructed re-
spectively. Under these possible actions, the un-
ordered word set “potatoes0 Tom1 likes2” is gen-
erated as shown in Figure 2, and the result is
“Tom1 ←likes2→potatoes0”.
We apply the learning and search framework
of Zhang and Clark (2011a). Pseudocode of the
search algorithm is shown in Algorithm 1. [] refers
to an empty stack, and set(1...n) represents the
full set of input words W and n is the number of
distinct words. candidates stores possible states,
and agenda stores temporary states transited from
possible actions. GETACTIONS generates a set of
possible actions depending on the current state s.
APPLY generates a new state by applying action on
the current state s. N-BEST produces the top k can-
didates in agenda. Finally, the algorithm returns
the highest-score state best in the agenda.
A global linear model is used to score search
hypotheses. Given a hypothesis h, its score is cal-
culated by:
</bodyText>
<equation confidence="0.9961395">
Score(h) = Φ(h) · B,
1http://sourceforge.net/projects/zgen/
([σ|i],p−{i},A)
([σ|j i],p,A)
R-ARC ([σ|i],p,A∪{j→i})
([σ|j i],p,A)
</equation>
<page confidence="0.973948">
370
</page>
<bodyText confidence="0.344797">
Algorithm 1 Transition-based linearisation
Input: W, a set of input word
Output: the highest-scored final state
</bodyText>
<listItem confidence="0.993214">
1: candidates ← ([], set(1..n), 0)
2: agenda ← 0
3: N ← 2n
4: for i ← 1..N do
5: for s in candidates do
6: for action in GETACTIONS(s) do
7: agenda ← APPLY(s, action)
8: end for
9: end for
10: candidates ← N-BEST(agenda)
11: agenda ← 0
12: end for
13: best ← BEST(candidates)
14: return best
</listItem>
<bodyText confidence="0.999985352941177">
where Φ(h) is the feature vector of h, extracted
by using the same feature templates as Liu et
al.(2015), which are shown in Table 1 and θ� is
the parameter vector of the model. The feature
templates essentially represents a syntactic lan-
guage model. As shown in Figure 2, from the hy-
potheses produced in steps 2 and 4, the features
“Tom1 ← likes2” and “likes2 → potatoes0” are
extracted, which corresponds to P(Tom1|likes2)
and P(potatoes0|likes2) respectively in the de-
pendency language model of Chen et al.,(2012).
Training. We apply perceptron with early-update
(Collins and Roark, 2004), and iteratively tune re-
lated parameters on a set of development data. For
each iteration, we measure the performance on the
development data, and choose best parameters for
final tests.
</bodyText>
<subsectionHeader confidence="0.997787">
2.2 N-gram word ordering
</subsectionHeader>
<bodyText confidence="0.999993785714286">
We build an N-gram word ordering system under
the same beam-search framework as the syntac-
tic word ordering system. In particular, search is
performed incrementally, from left to right, adding
one word at each step. The decoding process can
be regarded as a simplified version of Algorithm 1,
with only SHIFT being returned by GETACTIONS,
and the score of each transition is given by a stan-
dard N-gram language model. We use the same
beam size for both N-gram and the syntactic word
ordering. Compared with the syntactic model,
the N-gram model has less information for disam-
biguation, but also has less structural ambiguities,
and therefore a smaller search space.
</bodyText>
<equation confidence="0.997477772727273">
Unigram
S0w; S0p; S0,lw; S0,lp; S0,rw; S0,rp;
S0,l2w; S0,l2p; S0,r2w; S0,r2p;
S1w; S1p; S1,lw; S1,lp; S1,rw; S1,rp;
S1,l2w; S1,l2p; S1,r2w; S1,r2p;
Bigram
S0wS0,lw; S0wS0,lp; S0pS0,lw; S0pS0,lpS0,lp;
S0wS0,rw; S0wS0,rp; S0pS0,rw; S0pS0,rpS0,rp;
S1wS1,lw; S1wS1,lp; S1pS1,lw; S1pS1,lpS1,lp;
S1wS1,rw; S1wS1,rp; S1pS1,rw; S1pS1,rpS1,rp;
S0wS1w; S0wS1p; S0pS1w; S0pS1p;
Trigram
S0wS0pS0,lw; S0wS0,lwS0,lp; S0wS0pS0,lp;
S0pS0,lwS0,lp; S0wS0pS0,rw; S0wS0,lwS0,rp;
S0wS0pS0,rp; S0pS0,rwS0,rp;
S1wS1pS1,lw; S1wS1,lwS1,lp; S1wS1pS1,lp;
S1pS1,lwS1,lp; S1wS1pS1,rw; S1wS1,lwS1,rp;
S1wS1pS1,rp; S1pS1,rwS1,rp;
Linearization
w0; p0; w−1w0; p−1p0; w−2w−1w0; p−2p−1p0;
S0,lS0,l2w; S0,lpS0,l2p; S0,r2wS0,rw; S0,r2pS0,rp;
S1,lS1,l2w; S1,lpS1,l2p; S1,r2wS1,rw; S1,r2pS1,rp;
</equation>
<tableCaption confidence="0.997341">
Table 1: Feature templates.
</tableCaption>
<table confidence="0.999519222222222">
name domain # of sents # of tokens
training data
AFP News 35,390,025 844,395,322
XIN News 18,095,371 401,769,616
WSJ Finance 39,832 950,028
testing data
WSJ Finance 2,416 56,684
WPB News 2,000 43,712
SANCL Blog 1,015 20,356
</table>
<tableCaption confidence="0.997639">
Table 2: Data.
</tableCaption>
<bodyText confidence="0.9997825">
Training. We train N-gram language models
from raw text using modified Kneser-Ney smooth-
ing without pruning. The text is true-case tok-
enized, and we train 4-gram language modes using
KenLM2, which gives high efficiencies in standard
N-gram language model construction.
</bodyText>
<sectionHeader confidence="0.993085" genericHeader="method">
3 Experimental settings
</sectionHeader>
<subsectionHeader confidence="0.989169">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.998666">
For training data, we use the Wall Street Journal
(WSJ) sections 1-22 of the Penn Treebank (Mar-
</bodyText>
<footnote confidence="0.967622">
2https://kheafield.com/code/kenlm/
</footnote>
<page confidence="0.997543">
371
</page>
<bodyText confidence="0.999604916666667">
domain sentence example
Finance The $ 409 million bid includes the assum-
ption of an estimated $ 300 million in sec-
ured liabilities on those properties , accor-
ding to those making the bid.
News But after rising steadily during the quarter-
century following World War II , wages ha-
ve stagnated since the manufacturing sector
began to contract.
Blog The freaky thing here is that these bozos
are seriously claiming the moral high grou-
nd ?
</bodyText>
<tableCaption confidence="0.993391">
Table 3: Domain examples.
</tableCaption>
<bodyText confidence="0.999739">
cus et al., 1993), and the Agence France-Presse
(AFP) and Xinhua News Agency (XIN) subsets of
the English Giga Word Fifth Edition (Parker et al.,
2011). As the development data, we use WSJ sec-
tion 0 for parameter tuning. For testing, we use
data from various domain, which consist of WSJ
section 23, Washington Post/Bloomberg(WPB)
subsets of the English Giga Word Fifth Edition and
SANCL blog data, as shown in Table 2. Example
sentence in various test domains are shown in Ta-
ble 3.
</bodyText>
<subsectionHeader confidence="0.99906">
3.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999997142857143">
We follow previous work and use the BLEU met-
ric (Papineni et al., 2002) for evaluation. Since
BLEU only scores N-gram precisions, it can be in
favour of N-gram language models. We addition-
ally use METEOR3(Denkowski and Lavie, 2010)
to evaluate the system performances. The BLEU
metric measures the fluency of generated sentence
without considering long range ordering. The ME-
TEOR metric can potentially fix this problem us-
ing a set of mapping between generated sentences
and references to evaluate distortion. The fol-
lowing example illustrates the difference between
BLEU and METEOR on long range reordering,
where the reference is
</bodyText>
<listItem confidence="0.621702428571429">
(1) [The document is necessary for developer j0 [so you
can not follow this document to get right options .]1
and the generated output sentence is
(2) [so you can not follow this document to get right op-
tions .]1 [The document is necessary for developer j0 .
There is a big distortion in the output. The BLEU
metric gives a score of 90.09 out of 100, while
</listItem>
<footnote confidence="0.922261">
3http://www.cs.cmu.edu/—alavie/METEOR/
</footnote>
<table confidence="0.9993485">
ID # training sent # iter Avg F1
set57 900 1 57.31
set66 1800 1 66.82
set78 9000 1 78.73
set83 all 1 83.93
set88 all 30 88.10
</table>
<tableCaption confidence="0.999783">
Table 4: Parsing accuracy settings.
</tableCaption>
<bodyText confidence="0.9999549">
the METEOR gives a score of 61.34 out of 100.
This is because that METEOR is based on ex-
plicit word-to-word matches over the whole sen-
tence. For word ordering, word-to-word matches
are unique, which facilitates METEOR evaluation
between generated sentences and references. As
can bee seen from the example, long range dis-
tortion can highly influence the METEOR scores
making the METEOR metric more suitable for
evaluating word ordering distortions.
</bodyText>
<subsectionHeader confidence="0.997914">
3.3 Data preparation
</subsectionHeader>
<bodyText confidence="0.999962925925926">
For all the experiments, we assume that the in-
put is a bag of words without order, and the out-
put is a fully ordered sentence. Following previ-
ous work (Wan et al., 2009; Zhang, 2013; Liu et
al., 2015), we treat base noun phrases (i.e. noun
phrases do not contains other noun phrases, such
as ‘Pierre Iinken’ and ‘a big cat’) as a single
word. This avoids unnecessary ambiguities in
combination between their subcomponents.
The syntactic model requires that the train-
ing sentences have syntactic dependency struc-
ture. However, only the WSJ data contains gold-
standard annotations. In order to obtain automati-
cally annotated dependency trees, we train a con-
stituent parser using the gold-standard bracketed
sentences from WSJ, and automatically parse the
Giga Word data. The results are turned into de-
pendency trees using Penn2Malt4, after base noun
phrases are extracted. In our experiments, we use
ZPar5 (Zhu et al., 2013) for automatic constituent
parsing.
In order to study the influence of parsing ac-
curacy of the training data, we also use ten-fold
jackknifing to construct WSJ training data with
different accuracies. The data is randomly split
into ten equal-size subsets, and each subset is auto-
matically parsed with a parser trained on the other
</bodyText>
<footnote confidence="0.995486666666667">
4http://stp.lingfil.uu.se/—nivre/research/Penn2Malt.html
5http://people.sutd.edu.sg/—yue zhang/doc/doc/con-
parser.html
</footnote>
<page confidence="0.966665">
372
</page>
<table confidence="0.999705">
in-domain on WSJ test cross-domain on WPB test cross-domain on SANCL test
BLEU (%) METEOR (%) BLEU (%) METEOR (%) BLEU (%) METEOR (%)
syntax-set57 48.76 48.98 37.31 46.78 37.60 46.79
syntax-set66 48.79 48.98 37.52 46.81 38.28 46.90
syntax-set78 49.27 49.08 38.10 46.89 38.76 46.96
syntax-set83 49.74 49.16 37.68 46.84 38.67 46.93
syntax-set88 49.73 49.17 38.27 46.92 38.52 46.93
syntax-gold 50.82 49.33 37.76 46.84 39.97 47.26
</table>
<tableCaption confidence="0.999255">
Table 5: Influence result of parsing accuracy.
</tableCaption>
<bodyText confidence="0.999506">
nine subset. In order to obtain datasets with dif-
ferent parsing accuracies, we randomly sample a
small number of sentences from each training sub-
set, as shown in Table 4. The dependency trees
of each set are derived from these bracketed sen-
tences using Penn2Malt after base noun phrase are
extracted as a single word.
</bodyText>
<sectionHeader confidence="0.994599" genericHeader="method">
4 Influence of parsing accuracy
</sectionHeader>
<subsectionHeader confidence="0.917256">
4.1 In-domain word ordering
</subsectionHeader>
<bodyText confidence="0.999947928571428">
We train the syntactic models on the WSJ training
parsing data with different accuracies. The WSJ
development data are used to find out the optimal
number of training iterations for each experiments,
and the WSJ test results are shown in Table 5.
Table 5 shows that the parsing accuracy can af-
fect the performance of the syntactic model. A
higher parsing accuracy can lead to a better syn-
tactic language model. It conforms to the intu-
ition that syntactic quality affects the fluency of
surface texts. On the other hand, the influence is
not huge, the BLEU scores decrease by 1.0 points
as the parsing accuracy decreases from 88.10% to
57.31%
</bodyText>
<subsectionHeader confidence="0.988993">
4.2 Cross-domain word ordering
</subsectionHeader>
<bodyText confidence="0.999991136363636">
The influence of parsing accuracy of the training
data on cross-domain word ordering is measured
by using the same training settings, but testing on
the WPB and SANCL test sets. Table 5 shows
that the performance on cross-domain word order-
ing cannot reach that of in-domain word ordering
using the syntactic models. Compared with the
cross-domain experiments, the influence of pars-
ing accuracy becomes smaller. In the WPB test,
the fluctuation of performance decline to about 0.9
BLEU points, and in the SANCL test, the fluctua-
tion is about 1.1 BLEU points.
In conclusion, the experiments show that pars-
ing accuracies have a relatively small influence on
the syntactic models. This suggests that it is possi-
ble to use large automatically-parsed data to train
syntactic models. On the other hand, when the
training data scale increases, syntactic models can
become much slower to train compared with N-
gram models. The influence on data scale, which
includes output quality and training time, is further
studied in the next section.
</bodyText>
<sectionHeader confidence="0.975431" genericHeader="method">
5 Influence of data scale
</sectionHeader>
<bodyText confidence="0.999684714285714">
We use the AFP news data as the training data
for the experiments of this section. The syntac-
tic models are trained using automatically-parsed
trees derived from ZPar, as described in Section
3.3. The WPB test data is used to measure in-
domain performance, and the SANCL blog data
is used to measure cross-domain performance.
</bodyText>
<subsectionHeader confidence="0.968018">
5.1 Influence on BLEU and METEOR
</subsectionHeader>
<bodyText confidence="0.999928736842105">
The Figure 3 and 4 shows that using both the
BLEU and the METEOR metrics, the perfor-
mance of the syntactic model is better than that
of the N-gram models. It suggests that sentences
generated by the syntactic model have both bet-
ter fluency and better ordering. The performance
of the syntactic models is not highly weakened in
cross-domain tests.
The grey dot in each figure shows the perfor-
mance of the syntactic model trained on the gold
WSJ training data, and evaluated on the same
WPB and SANCL test data sets. A comparison
between the grey dots and the dashed lines shows
that the syntactic model trained on the WSJ data
perform better than the syntactic model trained on
similar amounts of AFP data. This again shows
the effect of syntactic quality of the training data.
On the other hand, as the scale of automatically-
parsed AFP data increases, the performance of the
</bodyText>
<page confidence="0.997664">
373
</page>
<figure confidence="0.98566725">
45
BLEU(%)
40
35
48
METEOR(%)
47.5
47
46.5
46
0 1 2 3 4
training data size (million sentences)
0 1 2 3 4
training data size (million sentences)
4-gram
syntax
WSJ
4-gram
syntax
WSJ
</figure>
<figureCaption confidence="0.9999555">
Figure 3: In-domain results on different training data size.
Figure 4: Cross-domain results on different training data sizes.
</figureCaption>
<figure confidence="0.992090736842105">
0 1 2 3 4
training data size (million sentences)
0 1 2 3 4
training data size (million sentences)
METEOR(%)
48
47
46
4-gram
syntax
WSJ
BLEU(%)
45
40
35
4-gram
syntax
WSJ
training time (log seconds)
</figure>
<figureCaption confidence="0.999973">
Figure 5: BLEU on different training times.
</figureCaption>
<bodyText confidence="0.998635">
syntactic model rapidly increases, surpassing the
syntactic model trained on the high-quality WSJ
data. This observation is important, showing that
large-scale data can be used to alleviate the prob-
lem of lower syntactic quality in automatically-
parsed data, which can be leveraged to address the
scarcity issue of manually annotated data in both
in-domain and cross-domain settings.
</bodyText>
<subsectionHeader confidence="0.995083">
5.2 Influence on training time
</subsectionHeader>
<bodyText confidence="0.999961428571429">
The training time of both syntactic models and
N-gram models increases as the size of training
data increases. Figure 5 shows the BLEU of the
two systems under different amounts of training
time. There is no result reported for the syntac-
tic model beyond 1 million training sentences, be-
cause training becomes infeasibly slow 6. On the
</bodyText>
<footnote confidence="0.935750666666667">
6Our experiments are carried on a single thread of
3.60GHz CPU. If the training time is over 90 hours for a
model, we consider it infeasible.
</footnote>
<bodyText confidence="0.999601944444445">
other hand, the N-gram model can be trained using
all the WSJ, AFP, XIN training sentences, which
are 53 millions, within 103.2 seconds. As a result,
there is no overlap between the syntactic model
and the N-gram model curves.
As can be seen from the figure, the syntactic
model is much slower to train. However, it ben-
efits more from the scale of the training data, with
the slope of the dashed curve being steeper than
that of the solid curve. The N-gram model can
be trained with more data thanks to the fast train-
ing speed. However, the performance of the N-
gram model flattens when the training data size
reaches beyond 3 million. Projection of the solid
curve suggests that the performance of the N-gram
model may not surpass that of the syntactic model
even if sufficiently large data is available for train-
ing the N-gram model in more time.
</bodyText>
<sectionHeader confidence="0.996117" genericHeader="method">
6 Error analysis
</sectionHeader>
<bodyText confidence="0.9994852">
Although giving overall better performance, the
syntactic model does not perform better than the
N-gram model in all cases. Here we analyze the
strength of each model via more fine-grained com-
parison.
In this set of experiments, the syntactic model is
trained using gold-standard annotated WSJ train-
ing parse trees, and the N-gram model is trained
using the data containing WSJ training data, AFP
and XIN. The WSJ test data, which contains
</bodyText>
<figure confidence="0.993763222222222">
1 2 3 4 5 6
BLEU (%)
50
40
35
30
5
4-gram
syntax
</figure>
<page confidence="0.924899">
374
</page>
<tableCaption confidence="0.833125">
average length of test sentences
average length of test sentences
</tableCaption>
<figure confidence="0.999524375">
5 10 15 20 25 30
BLEU(%)
100
60
40
0
4-gram
syntax
5 10 15 20 25 30
METEOR(%)
100
80
60
40
4-gram
syntax
</figure>
<figureCaption confidence="0.997541">
Figure 6: Performance on sentences with different length.
</figureCaption>
<figure confidence="0.9553035">
0.2 0.4 0.6 0.8 1
distortion rate
</figure>
<figureCaption confidence="0.999984">
Figure 7: The distribution of distortion.
</figureCaption>
<bodyText confidence="0.998584">
golden constituent trees, is used to analyze errors
in different aspects.
</bodyText>
<subsectionHeader confidence="0.999886">
6.1 Sentence length
</subsectionHeader>
<bodyText confidence="0.999951375">
The BLEU and METEOR scores of the two sys-
tems on various sentence lengths are shown in
Figure 6. The results are measured by binning
sentences according to their lengths, so that each
bin contains about the same number of sentences.
As shown by the figure, the N-gram model per-
forms better on short sentences (less than 8 to-
kens), and the syntactic model performs better on
longer sentences. This can be explained by the
fact that longer sentences have richer underlying
syntactic structures, which can better captured by
the syntactic model. In contrast, for shorter sen-
tences, the syntactic structure is relatively simple,
and therefore the N-gram model can give better
performance based on string patterns, which form
smaller search spaces.
</bodyText>
<subsectionHeader confidence="0.999824">
6.2 Distortion range
</subsectionHeader>
<bodyText confidence="0.999693">
We measure the average distortion rate of output
word w using the following metric:
</bodyText>
<equation confidence="0.9812625">
distortion(w) = |iw − i&apos;w |,
len(
</equation>
<bodyText confidence="0.985614">
where iw is index of word w in the output sentence
5w, i&apos;w is the index of the word w in the refer-
ence sentence. len(5w) is the number of tokens in
</bodyText>
<table confidence="0.998994166666667">
Template distribution
NLM-LOW set 1 if p &lt; e−12.5, else 0
NLM-20 use 20 bins to scatter probability
NLM-10 use 10 bins to scatter probability
NLM-5 use 5 bins to scatter probability
NLM-2 use 2 bins to scatter probability
</table>
<tableCaption confidence="0.998359">
Table 6: NLM feature templates.
</tableCaption>
<bodyText confidence="0.999938">
sentence 5w. Figure 7 shows distributions of dis-
tortion respectively by the syntactic and N-gram
model. The N-gram model makes relatively fewer
short-range distortions, but more long-range dis-
tortions. This can be explained by the local scor-
ing nature of the N-gram model. In contrast, the
syntactic model makes less long-range distortions,
which can suggest better sentence structure.
</bodyText>
<subsectionHeader confidence="0.999783">
6.3 Constituent span
</subsectionHeader>
<bodyText confidence="0.999368761904762">
We further evaluate sentence structure correctness
by evaluating the recalls of discovered constituent
span in output two systems, respectively. As
shown in Figure 8. The syntactic model performs
better in most constituent labels. However, the
N-gram model performs better in WHPP, SBARQ
and WHNP.
In the test data, WHPP, SBARQ and WHNP
are much less than PP, NP, VP, ADJP, ADVP and
CONJP, on which the syntactic model gives bet-
ter recalls. WHNP spans are small and most of
them consist of a question word (WP$) and one or
two nouns (e.g. “whose (WP$) parents (NNS)”).
WHPP spans are also small and usually consist
of a preposition (IN) and a WHNP span (e.g “at
(IN) what level (WHNP)”). The N-gram model
performs better on these small spans. The syntac-
tic model also performs better on S, which covers
the whole sentence structure. This verifies the hy-
pothesis introduce that syntactic language models
better capture overall sentential grammaticality.
</bodyText>
<figure confidence="0.985458666666667">
8,000
6,000
4,000
2,000
4-gram
syntax
</figure>
<page confidence="0.538011">
375
</page>
<figure confidence="0.9994405">
constituents recall (%)
100
50
0
4-gram
syntax
</figure>
<figureCaption confidence="0.999908">
Figure 8: Recalls of different constituents.
</figureCaption>
<table confidence="0.9990938">
in-domain on WSJ test cross-domain on WPB test cross-domain on SANCL test # of
BLEU (%) METEOR (%) BLEU (%) METEOR (%) BLEU (%) METEOR (%) sent/s
syntax 50.82 49.33 37.76 46.84 39.97 47.26 17.9
4-gram 42.26 48.00 37.71 46.90 39.72 47.08 177.0
combined 52.38 49.66 39.12 47.07 40.60 47.38 15.4
</table>
<tableCaption confidence="0.99523">
Table 7: Final results on various domains.
</tableCaption>
<sectionHeader confidence="0.981742" genericHeader="method">
7 Combining the syntactic and N-gram
models
</sectionHeader>
<bodyText confidence="0.999582">
The results above show the respective error char-
acteristics of each model, which are complimen-
tary. This suggests that better results can be
achieved by model combination.
</bodyText>
<subsectionHeader confidence="0.995835">
7.1 N-gram language model feature
</subsectionHeader>
<bodyText confidence="0.999816083333333">
We integrate the two types of models by using
N-gram language model probabilities as features
in the syntactic model. N-gram language model
probabilities, which ranges from 0 to 1. Direct
use of real value probabilities as features does not
work well in our experiments, and we use dis-
cretized features instead. For the L-ARC and R-
ARC actions, because no words are pushed onto
the stack, The NLM feature is set to NULL by de-
fault. For the SHIFT action, different feature values
are extracted depending on the NLM from 0 to 1.
In order to measure the N-gram probabilities
on our data, we train the 4-gram language model
WSJ, AFP and XIN data, and randomly sample 4-
gram probabilities from the syntactic model output
on the WSJ development data, finding that most
of 4-gram probabilities p are larger than 10−12.5.
In this way, if p lower than 10−12.5, NLM feature
value is set to LOW. As for p larger than 10−12.5,
we extract the discrete features by assigning them
into different bins. We bin the 4-gram probabil-
ities with different granularities without overlap
features. As shown in Table 6, NLM-20, NLM-
10, NLM-5 and NLM-2 respectively use 20, 10, 5
</bodyText>
<table confidence="0.9977555">
BLEU (%) on WSJ test
Wan et al. (2009) 33.70*
Zhang and Clark (2011b) 40.10*
Zhang et al. (2012) 43.80*
Zhang (2013) 44.70
syntax (Liu et al., 2015) 50.82
4-gram 42.26
combined 52.38
</table>
<tableCaption confidence="0.9679225">
Table 8: Final results of all systems, where “*”
means that the system uses extra POS input.
</tableCaption>
<bodyText confidence="0.926701">
and 2 bins to capture NLM feature values.
</bodyText>
<subsectionHeader confidence="0.986525">
7.2 Final results
</subsectionHeader>
<bodyText confidence="0.999991727272727">
We use the WSJ, AFP and XIN for training the N-
gram model7. The same WSJ, WPB and SANCL
test data are used to measure performances on dif-
ferent domains.
The experimental results are shown in Tables
7 and 8. In both in-domain and cross-domain
test data, the combined system outperforms all
other systems, with a BLEU score of 52.38 been
achieved in the WSJ domain. It would be overly
expensive to obtain a human oracle on discusses.
However, according to Papineni (2002), a BLEU
</bodyText>
<footnote confidence="0.988584857142857">
7For the combined model, we used the WSJ training data
for training, because the syntactic model is slower to train us-
ing large data. However, we did a set of experiments to scale
up the training data by sampling 900k sentences from AFP.
Results show that the combined model gives BLEU scores of
42.86 and 44.44 on the WPB and SANCL tests, respectively.
Cross-domain BLEU on WSJ, however falls to 49.84.
</footnote>
<page confidence="0.997683">
376
</page>
<bodyText confidence="0.910712538461539">
BLEU sentences
ref For weeks , the market had been nervous
about takeovers , after Campeau Corp. ’s cash
crunch spurred concern about the prospects for
future highly leveraged takeovers .
41.37 For weeks , Campeau Corp. ’s cash had
the prospects for takeovers after the market
crunch spurred concern about future highly
leveraged takeovers , nervous been about.
ref Now , at 3:07 , one of the market ’s post-
crash “ reforms ” took hold as the S&amp;P
500 futures contract had plunged 12 points
, equivalent to around a 100-point drop in
</bodyText>
<table confidence="0.9983793125">
the Dow industrials.
51.39 Now , one of the market ’s reforms plunged
12 points in the Dow industrials as “ post-
crash, the S&amp;P 500 futures contract,
equivalent to 3:07 took hold at around a
100-point drop had. ”
ref Canadian Utilities had 1988 revenue of C$
1.16 billion , mainly from its natural gas
and electric utility businesses in Alberta,
where the company serves about 800,000
customers .
64.38 Canadian Utilities , Alberta, where the
company had 1988 revenue of C$ 1.16
billion in its natural gas and electric utility
businesses serves mainly from about
800,000 customers .
</table>
<tableCaption confidence="0.999803">
Table 9: Output samples.
</tableCaption>
<bodyText confidence="0.999918666666667">
score of over 52.38 indicate an easily understood
sentence. Some sample outputs with different
BLEU scores are shown in Table 9
In addition, Table 7 shows that the N-gram
model is the fastest among the models due to its
small search space. The running time of the com-
bined system is larger than the pure syntactic sys-
tem, because of N-gram probability computation.
Table 8 compare our results with different previ-
ous methods on word ordering. Our combined
model gives the best reported performance on this
standard benchmarks.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999974857142857">
We empirically compared the strengths and er-
ror distributions of syntactic and N-gram lan-
guage models on word ordering, showing that both
can benefit from large-scale raw text. The influ-
ence of parsing accuracies has relatively small im-
pact on the syntactic language model trained on
automatically-parsed data, which enables scaling
up of training data for syntactic language mod-
els. However, as the size of training data in-
creases, syntactic language models can become in-
tolerantly slow to train, making them benefit less
from the scale of training data, as compared with
N-gram models.
Syntactic models give better performance com-
pared with N-gram models, despite trained with
less data. On the other hand, the two models lead
to different error distributions in word ordering.
As a result, we combined the advantages of both
systems by integrating a syntactic model trained
with relatively small data and an N-gram model
trained with relatively large data. The resulting
model gives better performance than both single
models and achieves the best reported scores in a
standard benchmark for word ordering.
We release our code under GPL at https://
github.com/SUTDNLP/ZGen. Future work
includes application of the system on text-to-text
problem such as machine translation.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999595">
The research is funded by the Singapore min-
istry of education (MOE) ACRF Tier 2 project
T2MOE201301. We thank the anonymous review-
ers for their detailed comments.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999366">
Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297–
328.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of MT Summit
IX, pages 40–46. Citeseer.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012.
Utilizing dependency language models for graph-
based dependency parsing models. In Proceedings
ofACL, pages 213–222.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201–228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, page 111.
</reference>
<page confidence="0.980334">
377
</page>
<reference confidence="0.999766109589041">
A De Gispert, M Tomalin, and W Byrne. 2014. Word
ordering with phrase-based grammars. In Proceed-
ings of EACL, pages 259–268.
Michael Denkowski and Alon Lavie. 2010. Extending
the meteor machine translation evaluation metric to
the phrase level. In HLT/NAACL, pages 250–253.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule.
Technical report, DTIC Document.
Yuqing Guo, Haifeng Wang, and Josef Van Genabith.
2011. Dependency-based n-gram models for gen-
eral purpose sentence realisation. Natural Language
Engineering, 17(04):455–483.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL, pages 48–54.
Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.
2015. Transition-based syntactic linearization. In
Proceedings of NAACL/HLT, pages 113–122, Den-
ver, Colorado, May–June.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311–318. Association for Computa-
tional Linguistics.
Robert Parker, David Graff, Junbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword
fifth edition, june. Linguistic Data Consortium,
LDC2011T07.
Lane Schwartz, Chris Callison-Burch, William
Schuler, and Stephen Wu. 2011. Incremental syn-
tactic language models for phrase-based translation.
In Proceedings of ACL/HLT, pages 620–631.
Libin Shen, Jinxi Xu, and Ralph M Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL, pages 577–585.
Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.
2014. Joint morphological generation and syntactic
linearization. In Twenty-Eighth AAAI Conference on
Artificial Intelligence.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of EACL, pages 852–
860.
Yue Zhang and Stephen Clark. 2011a. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
Yue Zhang and Stephen Clark. 2011b. Syntax-based
grammaticality improvement using ccg and guided
search. In Proceedings of EMNLP, pages 1147–
1157.
Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating
a large-scale language model. In Proceedings of
EACL, pages 736–746. Association for Computa-
tional Linguistics.
Yue Zhang, Kai Song, Linfeng Song, Jingbo Zhu, and
Qun Liu. 2014. Syntactic smt using a discriminative
text generation model. In Proceedings of EMNLP,
pages 177–182, Doha, Qatar, October.
Yue Zhang. 2013. Partial-tree linearization: general-
ized word ordering for text synthesis. In Proceed-
ings of IJCAI, pages 2232–2238. AAAI Press.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In ACL, pages 434–443.
</reference>
<page confidence="0.998304">
378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.490292">
<title confidence="0.9968825">Empirical Comparison Between and Syntactic Models for Word Ordering</title>
<author confidence="0.867355">Liu</author>
<affiliation confidence="0.998389">Singapore University of Technology and</affiliation>
<address confidence="0.728954">8 Somapah Road, Singapore,</address>
<email confidence="0.731291">liu,yue</email>
<abstract confidence="0.997937052631579">language models and language models have both been used in word ordering. In this paper, we give empirical comparison between and syntactic language models on word order task. Our results show that the quality of automatically-parsed training data has a relatively small impact on syntactic mod- Both of syntactic and models can benefit from large-scale raw text. with models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>328</pages>
<contexts>
<context position="1200" citStr="Barzilay and McKeown, 2005" startWordPosition="182" endWordPosition="185">h of syntactic and N-gram models can benefit from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improv</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297– 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX,</booktitle>
<pages>40--46</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1554" citStr="Charniak et al., 2003" startWordPosition="239" endWordPosition="242">the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine translation. In Proceedings of MT Summit IX, pages 40–46. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Utilizing dependency language models for graphbased dependency parsing models.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>213--222</pages>
<contexts>
<context position="1637" citStr="Chen et al., 2012" startWordPosition="253" endWordPosition="256">en used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2</context>
<context position="2890" citStr="Chen et al., 2012" startWordPosition="457" endWordPosition="460">ch is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering. We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm. • What is the influence of automaticallyparsed training data on the perfo</context>
</contexts>
<marker>Chen, Zhang, Li, 2012</marker>
<rawString>Wenliang Chen, Min Zhang, and Haizhou Li. 2012. Utilizing dependency language models for graphbased dependency parsing models. In Proceedings ofACL, pages 213–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation. computational linguistics,</title>
<date>2007</date>
<pages>33--2</pages>
<contexts>
<context position="1129" citStr="Chiang, 2007" startWordPosition="174" endWordPosition="175">a has a relatively small impact on syntactic models. Both of syntactic and N-gram models can benefit from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models c</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. computational linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>111</pages>
<contexts>
<context position="8882" citStr="Collins and Roark, 2004" startWordPosition="1463" endWordPosition="1466">BEST(candidates) 14: return best where Φ(h) is the feature vector of h, extracted by using the same feature templates as Liu et al.(2015), which are shown in Table 1 and θ� is the parameter vector of the model. The feature templates essentially represents a syntactic language model. As shown in Figure 2, from the hypotheses produced in steps 2 and 4, the features “Tom1 ← likes2” and “likes2 → potatoes0” are extracted, which corresponds to P(Tom1|likes2) and P(potatoes0|likes2) respectively in the dependency language model of Chen et al.,(2012). Training. We apply perceptron with early-update (Collins and Roark, 2004), and iteratively tune related parameters on a set of development data. For each iteration, we measure the performance on the development data, and choose best parameters for final tests. 2.2 N-gram word ordering We build an N-gram word ordering system under the same beam-search framework as the syntactic word ordering system. In particular, search is performed incrementally, from left to right, adding one word at each step. The decoding process can be regarded as a simplified version of Algorithm 1, with only SHIFT being returned by GETACTIONS, and the score of each transition is given by a s</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of ACL, page 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A De Gispert</author>
<author>M Tomalin</author>
<author>W Byrne</author>
</authors>
<title>Word ordering with phrase-based grammars.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>259--268</pages>
<marker>De Gispert, Tomalin, Byrne, 2014</marker>
<rawString>A De Gispert, M Tomalin, and W Byrne. 2014. Word ordering with phrase-based grammars. In Proceedings of EACL, pages 259–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Extending the meteor machine translation evaluation metric to the phrase level.</title>
<date>2010</date>
<booktitle>In HLT/NAACL,</booktitle>
<pages>250--253</pages>
<contexts>
<context position="12419" citStr="Denkowski and Lavie, 2010" startWordPosition="1995" endWordPosition="1998">Word Fifth Edition (Parker et al., 2011). As the development data, we use WSJ section 0 for parameter tuning. For testing, we use data from various domain, which consist of WSJ section 23, Washington Post/Bloomberg(WPB) subsets of the English Giga Word Fifth Edition and SANCL blog data, as shown in Table 2. Example sentence in various test domains are shown in Table 3. 3.2 Evaluation metrics We follow previous work and use the BLEU metric (Papineni et al., 2002) for evaluation. Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models. We additionally use METEOR3(Denkowski and Lavie, 2010) to evaluate the system performances. The BLEU metric measures the fluency of generated sentence without considering long range ordering. The METEOR metric can potentially fix this problem using a set of mapping between generated sentences and references to evaluate distortion. The following example illustrates the difference between BLEU and METEOR on long range reordering, where the reference is (1) [The document is necessary for developer j0 [so you can not follow this document to get right options .]1 and the generated output sentence is (2) [so you can not follow this document to get righ</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. Extending the meteor machine translation evaluation metric to the phrase level. In HLT/NAACL, pages 250–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule.</title>
<date>2004</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="1151" citStr="Galley et al., 2004" startWordPosition="176" endWordPosition="179">vely small impact on syntactic models. Both of syntactic and N-gram models can benefit from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Haifeng Wang</author>
<author>Josef Van Genabith</author>
</authors>
<title>Dependency-based n-gram models for general purpose sentence realisation.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>04</issue>
<marker>Guo, Wang, Van Genabith, 2011</marker>
<rawString>Yuqing Guo, Haifeng Wang, and Josef Van Genabith. 2011. Dependency-based n-gram models for general purpose sentence realisation. Natural Language Engineering, 17(04):455–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1115" citStr="Koehn et al., 2003" startWordPosition="170" endWordPosition="173">-parsed training data has a relatively small impact on syntactic models. Both of syntactic and N-gram models can benefit from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, synt</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijia Liu</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Bing Qin</author>
</authors>
<title>Transition-based syntactic linearization.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL/HLT,</booktitle>
<pages>113--122</pages>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="3166" citStr="Liu et al., 2015" startWordPosition="501" endWordPosition="504"> machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering. We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm. • What is the influence of automaticallyparsed training data on the performance of syntactic models. Because manual syntactic annotations are relatively limited and highly expensive, it is necessary to use large-scale automatically-parsed sentences for training syntactic language models. As a result, the syntactic structures that a word ordering s</context>
<context position="5807" citStr="Liu et al., 2015" startWordPosition="916" endWordPosition="919">l combination. Finally, because the two models make different types of errors, they can be combined to give better outputs. We develop a combined model by discretizing probability from N-gram model, and using them as features in the syntactic model. The combined model gives the best results in a standard benchmark. 2 Systems 2.1 Syntactic word ordering Syntactic word ordering algorithms take a multiset of input words constructing an output sentence and its syntactic derivation simultaneously. Transition-based syntactic word ordering can be modelled as an extension to transition-based parsing (Liu et al., 2015), with the main difference bestep action a p A init [] (0 1 2) Ø 0 shift [1] (0 2) 1 shift [1 2] (0) 2 L-arc [2] (0) A U {1 , 21 3 shift [2 0] () 4 R-arc [2] () A U {2 , 01 Figure 2: Transition-based process for ordering {“potatoes0”, “Tom1”, “likes2”}. ing that the order of words is not given in the input, which leads to a much larger search space. We take the system of Liu, et al.1, which gives state-of-the-art performance and efficiencies in standard word ordering benchmark. It maintains outputs in stack σ, and orders the unprocessed incoming words in a set p. Given an input bag of words, p</context>
<context position="14048" citStr="Liu et al., 2015" startWordPosition="2272" endWordPosition="2275"> because that METEOR is based on explicit word-to-word matches over the whole sentence. For word ordering, word-to-word matches are unique, which facilitates METEOR evaluation between generated sentences and references. As can bee seen from the example, long range distortion can highly influence the METEOR scores making the METEOR metric more suitable for evaluating word ordering distortions. 3.3 Data preparation For all the experiments, we assume that the input is a bag of words without order, and the output is a fully ordered sentence. Following previous work (Wan et al., 2009; Zhang, 2013; Liu et al., 2015), we treat base noun phrases (i.e. noun phrases do not contains other noun phrases, such as ‘Pierre Iinken’ and ‘a big cat’) as a single word. This avoids unnecessary ambiguities in combination between their subcomponents. The syntactic model requires that the training sentences have syntactic dependency structure. However, only the WSJ data contains goldstandard annotations. In order to obtain automatically annotated dependency trees, we train a constituent parser using the gold-standard bracketed sentences from WSJ, and automatically parse the Giga Word data. The results are turned into depe</context>
<context position="26905" citStr="Liu et al., 2015" startWordPosition="4430" endWordPosition="4433">from the syntactic model output on the WSJ development data, finding that most of 4-gram probabilities p are larger than 10−12.5. In this way, if p lower than 10−12.5, NLM feature value is set to LOW. As for p larger than 10−12.5, we extract the discrete features by assigning them into different bins. We bin the 4-gram probabilities with different granularities without overlap features. As shown in Table 6, NLM-20, NLM10, NLM-5 and NLM-2 respectively use 20, 10, 5 BLEU (%) on WSJ test Wan et al. (2009) 33.70* Zhang and Clark (2011b) 40.10* Zhang et al. (2012) 43.80* Zhang (2013) 44.70 syntax (Liu et al., 2015) 50.82 4-gram 42.26 combined 52.38 Table 8: Final results of all systems, where “*” means that the system uses extra POS input. and 2 bins to capture NLM feature values. 7.2 Final results We use the WSJ, AFP and XIN for training the Ngram model7. The same WSJ, WPB and SANCL test data are used to measure performances on different domains. The experimental results are shown in Tables 7 and 8. In both in-domain and cross-domain test data, the combined system outperforms all other systems, with a BLEU score of 52.38 been achieved in the WSJ domain. It would be overly expensive to obtain a human or</context>
</contexts>
<marker>Liu, Zhang, Che, Qin, 2015</marker>
<rawString>Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin. 2015. Transition-based syntactic linearization. In Proceedings of NAACL/HLT, pages 113–122, Denver, Colorado, May–June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12259" citStr="Papineni et al., 2002" startWordPosition="1970" endWordPosition="1973">oral high ground ? Table 3: Domain examples. cus et al., 1993), and the Agence France-Presse (AFP) and Xinhua News Agency (XIN) subsets of the English Giga Word Fifth Edition (Parker et al., 2011). As the development data, we use WSJ section 0 for parameter tuning. For testing, we use data from various domain, which consist of WSJ section 23, Washington Post/Bloomberg(WPB) subsets of the English Giga Word Fifth Edition and SANCL blog data, as shown in Table 2. Example sentence in various test domains are shown in Table 3. 3.2 Evaluation metrics We follow previous work and use the BLEU metric (Papineni et al., 2002) for evaluation. Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models. We additionally use METEOR3(Denkowski and Lavie, 2010) to evaluate the system performances. The BLEU metric measures the fluency of generated sentence without considering long range ordering. The METEOR metric can potentially fix this problem using a set of mapping between generated sentences and references to evaluate distortion. The following example illustrates the difference between BLEU and METEOR on long range reordering, where the reference is (1) [The document is necessary for deve</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2011</date>
<booktitle>Linguistic Data Consortium, LDC2011T07.</booktitle>
<note>English gigaword fifth edition,</note>
<contexts>
<context position="11833" citStr="Parker et al., 2011" startWordPosition="1895" endWordPosition="1898">lm/ 371 domain sentence example Finance The $ 409 million bid includes the assumption of an estimated $ 300 million in secured liabilities on those properties , according to those making the bid. News But after rising steadily during the quartercentury following World War II , wages have stagnated since the manufacturing sector began to contract. Blog The freaky thing here is that these bozos are seriously claiming the moral high ground ? Table 3: Domain examples. cus et al., 1993), and the Agence France-Presse (AFP) and Xinhua News Agency (XIN) subsets of the English Giga Word Fifth Edition (Parker et al., 2011). As the development data, we use WSJ section 0 for parameter tuning. For testing, we use data from various domain, which consist of WSJ section 23, Washington Post/Bloomberg(WPB) subsets of the English Giga Word Fifth Edition and SANCL blog data, as shown in Table 2. Example sentence in various test domains are shown in Table 3. 3.2 Evaluation metrics We follow previous work and use the BLEU metric (Papineni et al., 2002) for evaluation. Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models. We additionally use METEOR3(Denkowski and Lavie, 2010) to evaluate t</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword fifth edition, june. Linguistic Data Consortium, LDC2011T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
<author>Chris Callison-Burch</author>
<author>William Schuler</author>
<author>Stephen Wu</author>
</authors>
<title>Incremental syntactic language models for phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL/HLT,</booktitle>
<pages>620--631</pages>
<contexts>
<context position="1597" citStr="Schwartz et al., 2011" startWordPosition="247" endWordPosition="250"> Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word orderi</context>
<context position="3147" citStr="Schwartz et al., 2011" startWordPosition="497" endWordPosition="500">(Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering. We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm. • What is the influence of automaticallyparsed training data on the performance of syntactic models. Because manual syntactic annotations are relatively limited and highly expensive, it is necessary to use large-scale automatically-parsed sentences for training syntactic language models. As a result, the syntactic structures tha</context>
</contexts>
<marker>Schwartz, Callison-Burch, Schuler, Wu, 2011</marker>
<rawString>Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language models for phrase-based translation. In Proceedings of ACL/HLT, pages 620–631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph M Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1573" citStr="Shen et al., 2008" startWordPosition="243" endWordPosition="246">andard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on</context>
<context position="2870" citStr="Shen et al., 2008" startWordPosition="453" endWordPosition="456"> et al., 2014), which is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering. We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm. • What is the influence of automaticallyparsed traini</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph M Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linfeng Song</author>
<author>Yue Zhang</author>
<author>Kai Song</author>
<author>Qun Liu</author>
</authors>
<title>Joint morphological generation and syntactic linearization.</title>
<date>2014</date>
<booktitle>In Twenty-Eighth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1680" citStr="Song et al., 2014" startWordPosition="260" endWordPosition="263">asks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2011a; De Gispert et al., 2014), which is to</context>
</contexts>
<marker>Song, Zhang, Song, Liu, 2014</marker>
<rawString>Linfeng Song, Yue Zhang, Kai Song, and Qun Liu. 2014. Joint morphological generation and syntactic linearization. In Twenty-Eighth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>852--860</pages>
<contexts>
<context position="2217" citStr="Wan et al., 2009" startWordPosition="342" endWordPosition="345">ntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2011a; De Gispert et al., 2014), which is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without a</context>
<context position="14016" citStr="Wan et al., 2009" startWordPosition="2266" endWordPosition="2269">re of 61.34 out of 100. This is because that METEOR is based on explicit word-to-word matches over the whole sentence. For word ordering, word-to-word matches are unique, which facilitates METEOR evaluation between generated sentences and references. As can bee seen from the example, long range distortion can highly influence the METEOR scores making the METEOR metric more suitable for evaluating word ordering distortions. 3.3 Data preparation For all the experiments, we assume that the input is a bag of words without order, and the output is a fully ordered sentence. Following previous work (Wan et al., 2009; Zhang, 2013; Liu et al., 2015), we treat base noun phrases (i.e. noun phrases do not contains other noun phrases, such as ‘Pierre Iinken’ and ‘a big cat’) as a single word. This avoids unnecessary ambiguities in combination between their subcomponents. The syntactic model requires that the training sentences have syntactic dependency structure. However, only the WSJ data contains goldstandard annotations. In order to obtain automatically annotated dependency trees, we train a constituent parser using the gold-standard bracketed sentences from WSJ, and automatically parse the Giga Word data. </context>
<context position="26795" citStr="Wan et al. (2009)" startWordPosition="4411" endWordPosition="4414">our data, we train the 4-gram language model WSJ, AFP and XIN data, and randomly sample 4- gram probabilities from the syntactic model output on the WSJ development data, finding that most of 4-gram probabilities p are larger than 10−12.5. In this way, if p lower than 10−12.5, NLM feature value is set to LOW. As for p larger than 10−12.5, we extract the discrete features by assigning them into different bins. We bin the 4-gram probabilities with different granularities without overlap features. As shown in Table 6, NLM-20, NLM10, NLM-5 and NLM-2 respectively use 20, 10, 5 BLEU (%) on WSJ test Wan et al. (2009) 33.70* Zhang and Clark (2011b) 40.10* Zhang et al. (2012) 43.80* Zhang (2013) 44.70 syntax (Liu et al., 2015) 50.82 4-gram 42.26 combined 52.38 Table 8: Final results of all systems, where “*” means that the system uses extra POS input. and 2 bins to capture NLM feature values. 7.2 Final results We use the WSJ, AFP and XIN for training the Ngram model7. The same WSJ, WPB and SANCL test data are used to measure performances on different domains. The experimental results are shown in Tables 7 and 8. In both in-domain and cross-domain test data, the combined system outperforms all other systems,</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2009</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2009. Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model. In Proceedings of EACL, pages 852– 860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2240" citStr="Zhang and Clark, 2011" startWordPosition="346" endWordPosition="349">Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2011a; De Gispert et al., 2014), which is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on </context>
<context position="7081" citStr="Zhang and Clark (2011" startWordPosition="1162" endWordPosition="1165">empty. The system repeatedly applies transition actions to consume words from p and construct output on σ. Figure 1 shows the deduction system, where p is unordered and any word in p can be shifted onto the stack σ. The set of actions are SHIFT, L-ARC and R-ARC. The SHIFT actions add a word to the stack. For the L-ARC and R-ARC actions, new arcs {j ← i} and {j → i} are constructed respectively. Under these possible actions, the unordered word set “potatoes0 Tom1 likes2” is generated as shown in Figure 2, and the result is “Tom1 ←likes2→potatoes0”. We apply the learning and search framework of Zhang and Clark (2011a). Pseudocode of the search algorithm is shown in Algorithm 1. [] refers to an empty stack, and set(1...n) represents the full set of input words W and n is the number of distinct words. candidates stores possible states, and agenda stores temporary states transited from possible actions. GETACTIONS generates a set of possible actions depending on the current state s. APPLY generates a new state by applying action on the current state s. N-BEST produces the top k candidates in agenda. Finally, the algorithm returns the highest-score state best in the agenda. A global linear model is used to s</context>
<context position="26824" citStr="Zhang and Clark (2011" startWordPosition="4416" endWordPosition="4419">gram language model WSJ, AFP and XIN data, and randomly sample 4- gram probabilities from the syntactic model output on the WSJ development data, finding that most of 4-gram probabilities p are larger than 10−12.5. In this way, if p lower than 10−12.5, NLM feature value is set to LOW. As for p larger than 10−12.5, we extract the discrete features by assigning them into different bins. We bin the 4-gram probabilities with different granularities without overlap features. As shown in Table 6, NLM-20, NLM10, NLM-5 and NLM-2 respectively use 20, 10, 5 BLEU (%) on WSJ test Wan et al. (2009) 33.70* Zhang and Clark (2011b) 40.10* Zhang et al. (2012) 43.80* Zhang (2013) 44.70 syntax (Liu et al., 2015) 50.82 4-gram 42.26 combined 52.38 Table 8: Final results of all systems, where “*” means that the system uses extra POS input. and 2 bins to capture NLM feature values. 7.2 Final results We use the WSJ, AFP and XIN for training the Ngram model7. The same WSJ, WPB and SANCL test data are used to measure performances on different domains. The experimental results are shown in Tables 7 and 8. In both in-domain and cross-domain test data, the combined system outperforms all other systems, with a BLEU score of 52.38 b</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011a. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntax-based grammaticality improvement using ccg and guided search.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1147--1157</pages>
<contexts>
<context position="2240" citStr="Zhang and Clark, 2011" startWordPosition="346" endWordPosition="349">Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2011a; De Gispert et al., 2014), which is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on </context>
<context position="7081" citStr="Zhang and Clark (2011" startWordPosition="1162" endWordPosition="1165">empty. The system repeatedly applies transition actions to consume words from p and construct output on σ. Figure 1 shows the deduction system, where p is unordered and any word in p can be shifted onto the stack σ. The set of actions are SHIFT, L-ARC and R-ARC. The SHIFT actions add a word to the stack. For the L-ARC and R-ARC actions, new arcs {j ← i} and {j → i} are constructed respectively. Under these possible actions, the unordered word set “potatoes0 Tom1 likes2” is generated as shown in Figure 2, and the result is “Tom1 ←likes2→potatoes0”. We apply the learning and search framework of Zhang and Clark (2011a). Pseudocode of the search algorithm is shown in Algorithm 1. [] refers to an empty stack, and set(1...n) represents the full set of input words W and n is the number of distinct words. candidates stores possible states, and agenda stores temporary states transited from possible actions. GETACTIONS generates a set of possible actions depending on the current state s. APPLY generates a new state by applying action on the current state s. N-BEST produces the top k candidates in agenda. Finally, the algorithm returns the highest-score state best in the agenda. A global linear model is used to s</context>
<context position="26824" citStr="Zhang and Clark (2011" startWordPosition="4416" endWordPosition="4419">gram language model WSJ, AFP and XIN data, and randomly sample 4- gram probabilities from the syntactic model output on the WSJ development data, finding that most of 4-gram probabilities p are larger than 10−12.5. In this way, if p lower than 10−12.5, NLM feature value is set to LOW. As for p larger than 10−12.5, we extract the discrete features by assigning them into different bins. We bin the 4-gram probabilities with different granularities without overlap features. As shown in Table 6, NLM-20, NLM10, NLM-5 and NLM-2 respectively use 20, 10, 5 BLEU (%) on WSJ test Wan et al. (2009) 33.70* Zhang and Clark (2011b) 40.10* Zhang et al. (2012) 43.80* Zhang (2013) 44.70 syntax (Liu et al., 2015) 50.82 4-gram 42.26 combined 52.38 Table 8: Final results of all systems, where “*” means that the system uses extra POS input. and 2 bins to capture NLM feature values. 7.2 Final results We use the WSJ, AFP and XIN for training the Ngram model7. The same WSJ, WPB and SANCL test data are used to measure performances on different domains. The experimental results are shown in Tables 7 and 8. In both in-domain and cross-domain test data, the combined system outperforms all other systems, with a BLEU score of 52.38 b</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011b. Syntax-based grammaticality improvement using ccg and guided search. In Proceedings of EMNLP, pages 1147– 1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Graeme Blackwood</author>
<author>Stephen Clark</author>
</authors>
<title>Syntax-based word ordering incorporating a large-scale language model.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>736--746</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26853" citStr="Zhang et al. (2012)" startWordPosition="4421" endWordPosition="4424">d XIN data, and randomly sample 4- gram probabilities from the syntactic model output on the WSJ development data, finding that most of 4-gram probabilities p are larger than 10−12.5. In this way, if p lower than 10−12.5, NLM feature value is set to LOW. As for p larger than 10−12.5, we extract the discrete features by assigning them into different bins. We bin the 4-gram probabilities with different granularities without overlap features. As shown in Table 6, NLM-20, NLM10, NLM-5 and NLM-2 respectively use 20, 10, 5 BLEU (%) on WSJ test Wan et al. (2009) 33.70* Zhang and Clark (2011b) 40.10* Zhang et al. (2012) 43.80* Zhang (2013) 44.70 syntax (Liu et al., 2015) 50.82 4-gram 42.26 combined 52.38 Table 8: Final results of all systems, where “*” means that the system uses extra POS input. and 2 bins to capture NLM feature values. 7.2 Final results We use the WSJ, AFP and XIN for training the Ngram model7. The same WSJ, WPB and SANCL test data are used to measure performances on different domains. The experimental results are shown in Tables 7 and 8. In both in-domain and cross-domain test data, the combined system outperforms all other systems, with a BLEU score of 52.38 been achieved in the WSJ domai</context>
</contexts>
<marker>Zhang, Blackwood, Clark, 2012</marker>
<rawString>Yue Zhang, Graeme Blackwood, and Stephen Clark. 2012. Syntax-based word ordering incorporating a large-scale language model. In Proceedings of EACL, pages 736–746. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Kai Song</author>
<author>Linfeng Song</author>
<author>Jingbo Zhu</author>
<author>Qun Liu</author>
</authors>
<title>Syntactic smt using a discriminative text generation model.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>177--182</pages>
<location>Doha, Qatar,</location>
<contexts>
<context position="2590" citStr="Zhang et al., 2014" startWordPosition="406" endWordPosition="409">training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2011a; De Gispert et al., 2014), which is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discrimi</context>
</contexts>
<marker>Zhang, Song, Song, Zhu, Liu, 2014</marker>
<rawString>Yue Zhang, Kai Song, Linfeng Song, Jingbo Zhu, and Qun Liu. 2014. Syntactic smt using a discriminative text generation model. In Proceedings of EMNLP, pages 177–182, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
</authors>
<title>Partial-tree linearization: generalized word ordering for text synthesis.</title>
<date>2013</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>2232--2238</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2500" citStr="Zhang, 2013" startWordPosition="393" endWordPosition="394">ammaticality. However, Syntactic models require annotated syntactic structures for training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2011a; De Gispert et al., 2014), which is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to direc</context>
<context position="14029" citStr="Zhang, 2013" startWordPosition="2270" endWordPosition="2271"> 100. This is because that METEOR is based on explicit word-to-word matches over the whole sentence. For word ordering, word-to-word matches are unique, which facilitates METEOR evaluation between generated sentences and references. As can bee seen from the example, long range distortion can highly influence the METEOR scores making the METEOR metric more suitable for evaluating word ordering distortions. 3.3 Data preparation For all the experiments, we assume that the input is a bag of words without order, and the output is a fully ordered sentence. Following previous work (Wan et al., 2009; Zhang, 2013; Liu et al., 2015), we treat base noun phrases (i.e. noun phrases do not contains other noun phrases, such as ‘Pierre Iinken’ and ‘a big cat’) as a single word. This avoids unnecessary ambiguities in combination between their subcomponents. The syntactic model requires that the training sentences have syntactic dependency structure. However, only the WSJ data contains goldstandard annotations. In order to obtain automatically annotated dependency trees, we train a constituent parser using the gold-standard bracketed sentences from WSJ, and automatically parse the Giga Word data. The results a</context>
<context position="26873" citStr="Zhang (2013)" startWordPosition="4426" endWordPosition="4427">mple 4- gram probabilities from the syntactic model output on the WSJ development data, finding that most of 4-gram probabilities p are larger than 10−12.5. In this way, if p lower than 10−12.5, NLM feature value is set to LOW. As for p larger than 10−12.5, we extract the discrete features by assigning them into different bins. We bin the 4-gram probabilities with different granularities without overlap features. As shown in Table 6, NLM-20, NLM10, NLM-5 and NLM-2 respectively use 20, 10, 5 BLEU (%) on WSJ test Wan et al. (2009) 33.70* Zhang and Clark (2011b) 40.10* Zhang et al. (2012) 43.80* Zhang (2013) 44.70 syntax (Liu et al., 2015) 50.82 4-gram 42.26 combined 52.38 Table 8: Final results of all systems, where “*” means that the system uses extra POS input. and 2 bins to capture NLM feature values. 7.2 Final results We use the WSJ, AFP and XIN for training the Ngram model7. The same WSJ, WPB and SANCL test data are used to measure performances on different domains. The experimental results are shown in Tables 7 and 8. In both in-domain and cross-domain test data, the combined system outperforms all other systems, with a BLEU score of 52.38 been achieved in the WSJ domain. It would be overl</context>
</contexts>
<marker>Zhang, 2013</marker>
<rawString>Yue Zhang. 2013. Partial-tree linearization: generalized word ordering for text synthesis. In Proceedings of IJCAI, pages 2232–2238. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and accurate shiftreduce constituent parsing.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>434--443</pages>
<contexts>
<context position="14769" citStr="Zhu et al., 2013" startWordPosition="2385" endWordPosition="2388">en’ and ‘a big cat’) as a single word. This avoids unnecessary ambiguities in combination between their subcomponents. The syntactic model requires that the training sentences have syntactic dependency structure. However, only the WSJ data contains goldstandard annotations. In order to obtain automatically annotated dependency trees, we train a constituent parser using the gold-standard bracketed sentences from WSJ, and automatically parse the Giga Word data. The results are turned into dependency trees using Penn2Malt4, after base noun phrases are extracted. In our experiments, we use ZPar5 (Zhu et al., 2013) for automatic constituent parsing. In order to study the influence of parsing accuracy of the training data, we also use ten-fold jackknifing to construct WSJ training data with different accuracies. The data is randomly split into ten equal-size subsets, and each subset is automatically parsed with a parser trained on the other 4http://stp.lingfil.uu.se/—nivre/research/Penn2Malt.html 5http://people.sutd.edu.sg/—yue zhang/doc/doc/conparser.html 372 in-domain on WSJ test cross-domain on WPB test cross-domain on SANCL test BLEU (%) METEOR (%) BLEU (%) METEOR (%) BLEU (%) METEOR (%) syntax-set57</context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shiftreduce constituent parsing. In ACL, pages 434–443.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
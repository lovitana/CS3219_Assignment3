<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000171">
<title confidence="0.990637">
Reverse-engineering Language: A Study on the Semantic
Compositionality of German Compounds
</title>
<author confidence="0.979329">
Corina Dima
</author>
<affiliation confidence="0.98335">
Collaborative Research Center 833
University of T¨ubingen, Germany
</affiliation>
<email confidence="0.447904">
corina.dima@uni-tuebingen.de
</email>
<sectionHeader confidence="0.983094" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999118692307692">
In this paper we analyze the perfor-
mance of different composition models
on a large dataset of German compound
nouns. Given a vector space model for the
German language, we try to reconstruct
the observed representation (the corpus-
estimated vector) of a compound by com-
posing the observed representations of its
two immediate constituents. We explore
the composition models proposed in the
literature and also present a new, simple
model that achieves the best performance
on our dataset.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984186440678">
Vector space models of language like the ones
presented in (Collobert et al., 2011b; Mikolov et
al., 2013; Pennington et al., 2014) create good
representations for the individual words of a lan-
guage. However, the words in a language can
be combined into infinitely many distinct, well-
formed phrases and sentences. Creating meaning-
ful, reusable representations for such longer word
sequences is still an open problem.
In this paper we focus on building represen-
tations for syntactic units just above the word
level, by exploring compositional models for com-
pounds. Bauer (2001) defines a compound as “a
lexical unit made up of two or more elements,
each of which can function as a lexeme indepen-
dent of the other(s) in other contexts” (e.g. apple
tree). The vast majority of compounds are com-
positional, i.e. we can understand the meaning of
the compound if we know the meaning of its con-
stituent words. We would like to equip the vector
space model with a composition function able to
construct a composite representation for apple tree
from the representations of apple and tree. The
composite representation should ideally be indis-
tinguishable from its observed representation, i.e.
the representation learned directly by the language
model if the compound is part of the dictionary.
We situate our investigations in the context of
the German language, a language where com-
pounds represent an important fraction of the vo-
cabulary. Baroni et al. (2002) analyzed the 28 mil-
lion words German APA news corpus and discov-
ered that compounds account for 47% of the word
types but only 7% of the overall token count, with
83% of compounds having a corpus frequency of 5
or lower. The high productivity of the compound-
ing process makes the compositional approach the
most tractable way to create meaningful represen-
tations for all the compounds that have been or will
be coined by the speakers of the German language.
German compounds have a strategic advantage
for our study: they are generally written as a
contiguous word, irrespective of how many con-
stituents they have. Our example English com-
pound, apple tree, translates into the German com-
pound Apfelbaum, with the head Baum “tree” and
the modifier Apfel “apple”. Because the com-
pound is written as a single word, we can di-
rectly learn the representations for the compound
and for its constituents. Given a large dataset of
German compounds together with their immedi-
ate constituents, and the corresponding distributed
representations for each of the individual words,
one can try to reverse-engineer the compounding
process and learn the parameters of a function that
combines the representation of the constituents
into the representation of the compound. More
formally, we are interested in learning a compo-
sition function f such that
</bodyText>
<equation confidence="0.836599">
ccomp = f(mobs, hobs)
</equation>
<bodyText confidence="0.9999448">
where ccomp ∈ Rn is the composite representa-
tion of the compound and mobs, hobs ∈ Rn are
the observed representations of its modifier and its
head. The function should minimize J, the mean
squared error between the composite (ccomp) and
</bodyText>
<page confidence="0.93316">
1637
</page>
<note confidence="0.6516555">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1637–1642,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.998432">
the observed (cobs) representations of the |C |com-
pounds in the training set:
</bodyText>
<equation confidence="0.494012">
(C comp− cobs
l ij ij )2
</equation>
<bodyText confidence="0.999369">
Several compositionality models have already
been proposed in the literature (Mitchell and La-
pata, 2010; Baroni and Zamparelli, 2010; Socher
et al., 2012). In this paper we evaluate several
of the proposed composition functions and also
present a new composition model which outper-
forms all previous models on a dataset of German
compounds.
</bodyText>
<sectionHeader confidence="0.7603125" genericHeader="method">
2 Word Representations and
Compounds Dataset
</sectionHeader>
<bodyText confidence="0.99995296969697">
We trained 4 vector space language models for
German (with 50, 100, 200 and 300 dimensions
respectively) using the GloVe package (Penning-
ton et al., 2014) and a 10 billion token raw-text
corpus extracted from the DECOW14AX corpus
(Sch¨afer, 2015). We use a vocabulary of 1,029,270
(1M) words, obtained by selecting all the words
with a minimum frequency of 100 (the full vocab-
ulary had 50M unique words). We used the default
GloVe training parameters, the only modifications
being the use of a symmetric context when con-
structing the co-occurence matrix (10 words to the
left and to the right of the target word) and training
each model for 15 iterations. All the vector spaces
were normalized to the L2-norm, first across fea-
tures then across samples using scikit-learn (Pe-
dregosa et al., 2011).
The German compounds dataset used in the ex-
periments is a subset of the 54759 compounds
available in GermaNet 9.01. The compounds
in the list were automatically split and manu-
ally post-corrected (Henrich and Hinrichs, 2011).
Each entry in the list is a triple of the form (com-
pound, modifier, head). We filtered the entries in
the list, keeping only those where all three words
have a minimum frequency of 500 in the support
corpus used to create the vector space represen-
tations. The reason for the filtering step is that
a “well-learned” representation (based on a suffi-
ciently large number of contexts) should allow for
a more accurate reconstruction than a representa-
tion based only on a few contexts. The filtered
dataset contains 34497 entries. This dataset was
</bodyText>
<footnote confidence="0.890249">
1http://www.sfs.uni-tuebingen.de/lsd/compounds.shtml
</footnote>
<bodyText confidence="0.999622666666667">
randomized and partitioned into train, test
and dev splits according to the 70-20-10 rule. The
dataset contains 8580 unique modifiers and heads,
and a dictionary of 41732 unique words. 1345
compounds appear as the modifier or head of an-
other compound.
</bodyText>
<sectionHeader confidence="0.683765" genericHeader="method">
3 12 ways to Represent A Compound
</sectionHeader>
<bodyText confidence="0.998969166666667">
We adopt a notation similar to the one introduced
in (Mitchell and Lapata, 2010), where the compos-
ite representation p is the result of applying a com-
position function f to the vectors u and v. In this
study we tested the following composition func-
tions:
</bodyText>
<listItem confidence="0.682969216216216">
1. p = v, the second constituent of the com-
pound
2. p = u, the first constituent of the compound
3. p = u O v, component-wise vector multipli-
cation
4. p = (u · u)v + (A − 1)(u · v)u, dilation
5. p = 0.5u + 0.5v, vector addition
6. p = Au + Qv, weighted vector addition,
where the A and Q are estimated using the
training set. Models 1 through 6 were intro-
duced in (Mitchell and Lapata, 2010).
7. p = Uv, where v E Rn is the vectorial
representation of the head word (given) and
U E Rn×n is a matrix representation for the
modifier, estimated with the help of the train-
ing data. The model estimates one matrix for
each word that is used as a modifier. Referred
to as alm in (Baroni and Zamparelli, 2010)
and as Lexfunc in (Dinu et al., 2013b).
8. p = M1u + M2v, where M1, M2 E Rn×n
are two matrices that modify the first and the
second constituent vectors, respectively. In
contrast to the previous model, this model es-
timates just one matrix for all the modifiers
and one matrix for all the head words. Ref-
ered to as EAM in (Zanzotto et al., 2010) and
as Fulladd in (Dinu et al., 2013b).
9. p = g(W [u; v]), where: [u; v] E R2n×1 is the
concatenation of the individual word vectors;
W E Rn×2n is a global matrix that: (i) com-
bines the individual dimensions of the con-
catenated input vector [u; v]; (ii) brings the
composite representation back into the Rn×1
space; g is an element-wise function, in our
experiments the hyperbolic tangent tanh. In-
troduced in (Socher et al., 2010).
10. p = g(W[Vu; Uv]). Introduced in (Socher
</listItem>
<equation confidence="0.991911857142857">
1
n
J =
� |C|
i=1
n
j=1
</equation>
<page confidence="0.916678">
1638
</page>
<bodyText confidence="0.999659333333333">
et al., 2012), it is a generalization of model 7.
Each word is represented using an Rn×n ma-
trix and a Rn vector. The vectors are given,
while the matrices are estimated using the
training data. Referred to as Fulllex in (Dinu
et al., 2013b).
</bodyText>
<listItem confidence="0.560912">
11. p = uOu0 +v Ov00, the additive mask model
(Addmask) and
12. p = g(W [u O u0; v O v00]), the global ma-
trix mask model (Wmask), both presented in
subsection 3.1.
</listItem>
<bodyText confidence="0.9999594">
Models 1 through 8 were tested using the im-
plementations available in the DISSECT toolkit
(Dinu et al., 2013a). As a side note, the Lex-
func implementation in DISSECT does not pro-
duce a composite representation for 11.5% of the
our test data, where a word does not appear as a
modifier during training. Therefore, we reimple-
mented the Lexfunc model and solved the missing
training material problem by initializing the ma-
trix for all the words in the dictionary with I + E,
the identity matrix plus a small amount of Gaus-
sian noise. This type of initialization was proposed
by (Socher et al., 2012), and allows the model to
back-off to the model p = v when there is no data
to estimate the parameters of the modifier matrix.
We also reimplemented models 9 and 10, which
were used in (Socher et al., 2010; Socher et al.,
2012), as the existing implementations are part of
a more complex recursive architecture aimed at
constructing representations for full sentences.
</bodyText>
<subsectionHeader confidence="0.990732">
3.1 The mask models
</subsectionHeader>
<bodyText confidence="0.999984377777778">
The newly introduced mask models build upon the
idea that when a word w enters a composition pro-
cess, there is some variation in its meaning de-
pending on whether it is the first or the second el-
ement of the composition. Think, for instance, of
the compounds company car and car factory. In
the first case, car has its primary denotation, that
of a road vehicle. In the second case, what mat-
ters more about the car is its product aspect, the
fact that it is an “artifact produced in a factory”. A
good representation of the word car should encode
both aspects. Likewise, a good composition model
should be able to select from the individual word
representations only those aspects that are relevant
for the composition process.
We want to give the composition model the pos-
sibility to deal with these slight sense variations,
so we train, for each word in the dictionary, two
masks, one for the case when it is the first word
in the composition process and one for when it is
the second word. The masks of the word w rep-
resented by u E Rn are two vectors u0, u00 E Rn.
The mask vectors are initialized with a vector of
all ones, 1, and estimated with the help of the train-
ing data. Each time w is the first word in the com-
position process, it is represented as the element-
wise multiplication of the vector u and the mask
u0, u O u0. When w is the second word in the
composition, it is represented by the element-wise
multiplication of u and the mask u00, u O u00.
It is important to note that the initial vector rep-
resentations remain fixed during the learning pro-
cess. The learning process only affects the mask
vectors. The composite representation of a com-
pound like car factory is obtained by combin-
ing the masked representations, ucar O u0car and
vfactory O v00factory. We tried two different combi-
nation methods: (i) p = u O u0 + v O v00, called
Addmask (model 11), where the masked represen-
tations are combined via component-wise addi-
tion, and (ii) p = g(W [u O u0; v O v00]), called
Wmask (model 12), where the combination of the
masked representations is made via a global ma-
trix W E Rn×2n and a nonlinearity g (tanh), sim-
ilar to model 10.
</bodyText>
<subsectionHeader confidence="0.999125">
3.2 Implementing composition models
</subsectionHeader>
<bodyText confidence="0.999948590909091">
Models 7, 9 and 10 and the mask models were im-
plemented using neural network architectures in
the Torch7 library (Collobert et al., 2011a). We
use the mean squared error as a training criterion,
and optimize all models using Adagrad (Duchi et
al., 2011) and a mini-batch of 100 samples. The
hyperparameters were chosen by testing different
parameter values and evaluating their performance
on the dev set. To avoid overfitting we used early
stopping (Prechelt, 1998). All the implemented
models keep the input vectors fixed during the
composition process.
Training the mask models entails estimating
modifier and head masks for every word in the dic-
tionary D. The two types of masks to be learned
can be formalized as two matrices WM, WH E
Rn×|D|, where n is the size of the initial word
representations. The masks of the word wi E D
are the ith rows in WM and WH. In Torch7 such
representations can be learned using lookup table
layers (Collobert et al., 2011b), which map matrix
indices to the corresponding row vector.
</bodyText>
<page confidence="0.986285">
1639
</page>
<bodyText confidence="0.999940352941177">
The masked representation of the modifier is
obtained by first feeding the index of the word to
LTWM, the modifier lookup table, to obtain the
modifier mask, and then multiplying the modifier
mask with the initial representation for the modi-
fier. The masked representation of the head is ob-
tained in a similar manner via a lookup operation
in LTWH, the head lookup table. The Addmask
and Wmask models differ only in the composi-
tion method used after the masking process: the
masked representations are directly added together
in the case of Addmask and are passed through a
composition matrix W ∈ Rn×2n and a nonlin-
earity g in the case of Wmask. The two matri-
ces WM, WH are initialized with all ones and are
modified via backpropagation during the training
process.
</bodyText>
<sectionHeader confidence="0.994862" genericHeader="method">
4 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999987554216868">
The twelve composition models presented in Sec-
tion 3 were evaluated using word representations
of increasing size (described in Section 2). All
the models are trained on the train split and
tested on the test split. We used the rank eval-
uation method proposed by (Baroni and Zampar-
elli, 2010) for a similar task: first, we generate
a composite representation for each of the 6901
compounds in the test set; then, we use the co-
sine similarity to rank each composite representa-
tion with respect to the observed representations
of the 41732 unique words in the dataset dictio-
nary. If the observed representation is the nearest
neighbour, the composition is assigned the rank 1.
Similar to (Baroni and Zamparelli, 2010), we as-
sign the rank 1000 (≥1K) when the observed rep-
resentation is not one of the nearest 1000 neigh-
bours of the composite representation. We then
compute the first, second and third quartiles (Q1,
Q2, Q3) across all the compounds in the test set.
A Q1 value of 2 means that the first 25% of the
data was only assigned ranks 1 and 2. Similarly,
Q2 and Q3 refer to the ranks assigned to the first
50% and 75% of data, respectively. The results of
our evaluation are displayed in Table 1.
The observed representation of the head (model
1) was used as a strong baseline for the compound
composition task. Two of the tested models, mul-
tiplicative (model 3) and dilation (model 4) score
worse than the head baseline, while the additive
models (5 and 6) score only slightly above it. The
fact that the worst performing model is the multi-
plicative model is surprising considering its good
performance in previous studies (Mitchell and La-
pata, 2010). This might be either a side-effect of
the normalization procedure, or a genuine incom-
patibility of this compositionality model with the
vectorial representations produced by GloVe.
The new Addmask and Wmask models (intro-
duced in Section 3.1) perform very well, with
Wmask producing the best results on the test
dataset across all dimensions. It is interesting to
note that the linguistically motivated Lexfunc and
Fulllex models, which build dedicated representa-
tions for each individual constituent, are outper-
formed by a simple model like Fulladd, that only
learns two modification matrices, one for each po-
sition. The explanation is, in our opinion, that the
available training material is not enough for train-
ing all the parameters of the complex Lexfunc and
Fulllex models, but good enough for the more sim-
ple Fulladd.
The mask models are computationally cheaper
than models like Lexfunc and Fulllex, as they they
only train 2n parameters for each word in the vo-
cabulary, and not n2 parameters like the aforemen-
tioned models. They manage to strike a balance
and learn a dedicated representation for each con-
stituent with a small number of parameters, thus
performing better than the more complex models.
We used non-parametric statistical tests to de-
tect significant differences between the results ob-
tained by the models. We focused our analysis on
the best performing 4 models: model 9, which we
will label the Matrix model, Fulladd (model 8),
Addmask (model 11) and Wmask (model 12). The
comparison takes into account two separate fac-
tors: (i) differences between the models using rep-
resentations of the same size; (ii) differences in the
performance of the same model using representa-
tions of different sizes.
A Friedman test on the ranks obtained by the
4 selected models on representations of size 300
showed that there is a significant difference be-
tween the models (p &lt; 0.01). Pairwise compar-
isons (using the Wilcoxon signed rank test and
Bonferroni corrections) showed that there is a sig-
nificant difference (p &lt; 0.01) between all but one
pair of models, namely the Matrix and the Ad-
dmask models (p = 0.9). The same test confirmed
that there are significant differences in the perfor-
mance of the best model Wmask when using repre-
sentations of different sizes (p &lt; 0.01). Pairwise
</bodyText>
<page confidence="0.946936">
1640
</page>
<figure confidence="0.9678972">
no f I 50d 100d 200d 300d
Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3
1 p = v D 66 445 AK 36 202 AK 33 197 989 29 174 884
2 p = u D 445 AK AK 215 AK AK 171 917 AK 144 808 AK
3 p = u O v D AK AK AK AK AK AK AK AK AK AK AK AK
4 p = (u · u)v + (A − 1)(u · v)u D 75 492 AK 38 213 AK 35 209 AK 30 181 926
5 p = 0.5u + 0.5v D 85 408 AK 29 137 600.5 28 140 637 24 120 553
6 p = Au + βv D 62 329 AK 23.5 118 556 23 121 568 20 105 503
7 p = Uv (on 88.5% of test data) D 38 415 AK 15 147 AK 9 61 636 8 47 443
7 p = Uv R 10 88 829 8 64 595 7 48 479.5 7 51 526.5
10 p = g(W[Vu; Uv]) R 3 18 178 3 12 111 3 16 188 4 26 334
9 p = g(W [u; v]) R 4 19 137 3 11 64 2 7 33 2 7 29
11 p = u O u&apos; + v O v&apos;&apos; N 3 12 85 3 8 45 3 7 30 3 7 27
8 p = M1u + M2v D 4 19 135 3 10 61 2 7 33 2 6 27
12 p = g(W[u O u&apos;; v O v&apos;&apos;]) N 2 9 62 2 7 35 2 6 25 2 6 24
</figure>
<tableCaption confidence="0.697165">
Table 1: Quartiles for the 6901 composite representations in the test set, ranked with respect to the
observed representations. Best possible rank is 1. D marks the models tested with DISSECT, R marks
reimplementations of existing models and N marks new models.
</tableCaption>
<bodyText confidence="0.929701454545454">
comparisons showed that Wmask model signifi-
cantly improves its performance (p &lt; 0.01) when
using word representations of increasing size (50,
100, 200 and 300 dimensions).
The twelve composition models were also com-
pared in terms of the mean squared error (MSE)
objective function, by computing the MSE be-
tween the composite and the observed represen-
tation of the compounds in the test set. The best
scoring models in the rank evaluation were also
the best in the MSE evaluation. However, the dif-
ference in performance between the best and the
worst models was considerably smaller: the MSE
of the multiplicative model is only twice as large
as the MSE of the best performing Wmask model.
This is in contrast to the rank evaluation where the
multiplicative model assigned the observed repre-
sentations in the test set only ranks ≥ 1000, while
Wmask assigned ranks ≤ 25 to 75% of the test
data. Additional investigations are necessary to es-
timate the impact of different objective functions
on the performance of compositional models.
</bodyText>
<sectionHeader confidence="0.886229" genericHeader="method">
5 Comparison to related work
</sectionHeader>
<bodyText confidence="0.999937083333334">
The experiments reported in this paper are, to the
best of our knowledge, the first large scale experi-
ments on the composition of German compounds.
Other studies (Kisselew et al., 2015; Lazaridou
et al., 2013) focused on morphologically complex
words in German and English respectively. In
terms of the size of the training and test material,
our experiments are closest to the adjective-noun
experiments in (Baroni and Zamparelli, 2010) and
(Dinu et al., 2013b) where the lexical function
model performed the best, with lowest reported
median ranks (Q2) above 100.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999982416666667">
Twelve composition models were evaluated on the
task of building compositional representations for
German compounds. The best results (median
rank 6) were obtained by the newly introduced
Wmask model, p = g(W [u O u&apos;; v O v&apos;&apos;]). The re-
sults show that it is possible to learn a composition
function specific to compounds, an idea which we
would like to further explore using existing com-
pound datasets for English ( O´ S´eaghdha, 2008;
Tratz and Hovy, 2010). The implementation of
the newly introduced composition methods can be
downloaded from the author’s website.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998667111111111">
The author would like to thank Emanuel Dima,
Erhard Hinrichs, Dani¨el de Kok, D¨orte de Kok
and Jianqiang Ma, as well as the anonymous re-
viewers for their insightful comments and sugges-
tions. Financial support for the research reported
in this paper was provided by the German Re-
search Foundation (DFG) as part of the Collabo-
rative Research Center “Emergence of Meaning”
(SFB 833), project A3.
</bodyText>
<sectionHeader confidence="0.986662" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.404806">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Represent-
ing adjective-noun constructions in semantic space.
In Proceedings of the Conference on Empirical
</reference>
<page confidence="0.962968">
1641
</page>
<reference confidence="0.997519457943925">
Methods in Natural Language Processing (EMNLP
2010), pages 1183–1193.
Marco Baroni, Johannes Matiasek, and Harald Trost.
2002. Predicting the components of German nom-
inal compounds. In F. van Harmelen, editor, Pro-
ceedings of the 15th European Conference on Artifi-
cial Intelligence (ECAI), pages 470–474.
Laurie Bauer. 2001. Compounding. In Martin
Haspelmath, editor, Language Typology and Lan-
guage Universals. Mouton de Gruyter, The Hague.
Ronan Collobert, Koray Kavukcuoglu, and Cl´ement
Farabet. 2011a. Torch7: A Matlab-like environment
for machine learning. In BigLearn, NIPS Workshop,
number EPFL-CONF-192376.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011b. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Georgiana Dinu, The Pham Nghia, and Marco Baroni.
2013a. DISSECT - DIStributional SEmantics Com-
position Toolkit. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 31–36, Sofia, Bulgaria.
Georgiana Dinu, The Pham Nghia, and Marco Baroni.
2013b. General estimation and evaluation of com-
positional distributional semantic models. In Work-
shop on Continuous Vector Space Models and their
Compositionality, Sofia, Bulgaria.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Verena Henrich and Erhard W. Hinrichs. 2011. De-
termining Immediate Constituents of Compounds in
GermaNet. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2011), pages
420–426, Hissar, Bulgaria.
Max Kisselew, Sebastian Pad´o, Alexis Palmer, and Jan
ˇSnajder. 2015. Obtaining a Better Understand-
ing of Distributional Models of German Deriva-
tional Morphology. In Proceedings of the 11th In-
ternational Conference on Computational Seman-
tics (IWCS 2015), pages 58–63, London, UK.
Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
Derived Representations of Morphologically Com-
plex Words in Distributional Semantics. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL 2013), pages
1517–1526, Sofia, Bulgaria.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive sci-
ence, 34(8):1388–1429.
Diarmuid O´ S´eaghdha. 2008. Learning compound
noun semantics. Ph.D. thesis, Computer Laboratory,
University of Cambridge. Published as University
of Cambridge Computer Laboratory Technical Re-
port 735.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors
for word representation. In Proceedings of the Em-
piricial Methods in Natural Language Processing
(EMNLP 2014), volume 12.
Lutz Prechelt. 1998. Early stopping - but when? In
Neural Networks: Tricks of the trade, pages 55–69.
Springer.
Roland Sch¨afer. 2015. Processing and querying large
web corpora with the COW14 architecture. In
Challenges in the Management of Large Corpora
(CMLC-3).
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1–9.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-10), Uppsala, Sweden.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating Linear Models for Compositional Distri-
butional Semantics. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 1263–1271.
</reference>
<page confidence="0.994169">
1642
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806807">
<title confidence="0.952942">Reverse-engineering Language: A Study on the Compositionality of German Compounds</title>
<author confidence="0.832035">Corina</author>
<affiliation confidence="0.999026">Collaborative Research Center University of T¨ubingen,</affiliation>
<email confidence="0.984615">corina.dima@uni-tuebingen.de</email>
<abstract confidence="0.999667571428571">In this paper we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpusestimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="4195" citStr="Baroni and Zamparelli, 2010" startWordPosition="666" endWordPosition="669">ion of the compound and mobs, hobs ∈ Rn are the observed representations of its modifier and its head. The function should minimize J, the mean squared error between the composite (ccomp) and 1637 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1637–1642, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. the observed (cobs) representations of the |C |compounds in the training set: (C comp− cobs l ij ij )2 Several compositionality models have already been proposed in the literature (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012). In this paper we evaluate several of the proposed composition functions and also present a new composition model which outperforms all previous models on a dataset of German compounds. 2 Word Representations and Compounds Dataset We trained 4 vector space language models for German (with 50, 100, 200 and 300 dimensions respectively) using the GloVe package (Pennington et al., 2014) and a 10 billion token raw-text corpus extracted from the DECOW14AX corpus (Sch¨afer, 2015). We use a vocabulary of 1,029,270 (1M) words, obtained by selecting all the words with a minimum fr</context>
<context position="7310" citStr="Baroni and Zamparelli, 2010" startWordPosition="1208" endWordPosition="1211">ituent of the compound 3. p = u O v, component-wise vector multiplication 4. p = (u · u)v + (A − 1)(u · v)u, dilation 5. p = 0.5u + 0.5v, vector addition 6. p = Au + Qv, weighted vector addition, where the A and Q are estimated using the training set. Models 1 through 6 were introduced in (Mitchell and Lapata, 2010). 7. p = Uv, where v E Rn is the vectorial representation of the head word (given) and U E Rn×n is a matrix representation for the modifier, estimated with the help of the training data. The model estimates one matrix for each word that is used as a modifier. Referred to as alm in (Baroni and Zamparelli, 2010) and as Lexfunc in (Dinu et al., 2013b). 8. p = M1u + M2v, where M1, M2 E Rn×n are two matrices that modify the first and the second constituent vectors, respectively. In contrast to the previous model, this model estimates just one matrix for all the modifiers and one matrix for all the head words. Refered to as EAM in (Zanzotto et al., 2010) and as Fulladd in (Dinu et al., 2013b). 9. p = g(W [u; v]), where: [u; v] E R2n×1 is the concatenation of the individual word vectors; W E Rn×2n is a global matrix that: (i) combines the individual dimensions of the concatenated input vector [u; v]; (ii)</context>
<context position="13847" citStr="Baroni and Zamparelli, 2010" startWordPosition="2391" endWordPosition="2395">asking process: the masked representations are directly added together in the case of Addmask and are passed through a composition matrix W ∈ Rn×2n and a nonlinearity g in the case of Wmask. The two matrices WM, WH are initialized with all ones and are modified via backpropagation during the training process. 4 Evaluation and Results The twelve composition models presented in Section 3 were evaluated using word representations of increasing size (described in Section 2). All the models are trained on the train split and tested on the test split. We used the rank evaluation method proposed by (Baroni and Zamparelli, 2010) for a similar task: first, we generate a composite representation for each of the 6901 compounds in the test set; then, we use the cosine similarity to rank each composite representation with respect to the observed representations of the 41732 unique words in the dataset dictionary. If the observed representation is the nearest neighbour, the composition is assigned the rank 1. Similar to (Baroni and Zamparelli, 2010), we assign the rank 1000 (≥1K) when the observed representation is not one of the nearest 1000 neighbours of the composite representation. We then compute the first, second and</context>
<context position="20089" citStr="Baroni and Zamparelli, 2010" startWordPosition="3574" endWordPosition="3577">75% of the test data. Additional investigations are necessary to estimate the impact of different objective functions on the performance of compositional models. 5 Comparison to related work The experiments reported in this paper are, to the best of our knowledge, the first large scale experiments on the composition of German compounds. Other studies (Kisselew et al., 2015; Lazaridou et al., 2013) focused on morphologically complex words in German and English respectively. In terms of the size of the training and test material, our experiments are closest to the adjective-noun experiments in (Baroni and Zamparelli, 2010) and (Dinu et al., 2013b) where the lexical function model performed the best, with lowest reported median ranks (Q2) above 100. 6 Conclusions Twelve composition models were evaluated on the task of building compositional representations for German compounds. The best results (median rank 6) were obtained by the newly introduced Wmask model, p = g(W [u O u&apos;; v O v&apos;&apos;]). The results show that it is possible to learn a composition function specific to compounds, an idea which we would like to further explore using existing compound datasets for English ( O´ S´eaghdha, 2008; Tratz and Hovy, 2010).</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2010), pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Johannes Matiasek</author>
<author>Harald Trost</author>
</authors>
<title>Predicting the components of German nominal compounds.</title>
<date>2002</date>
<booktitle>Proceedings of the 15th European Conference on Artificial Intelligence (ECAI),</booktitle>
<pages>470--474</pages>
<editor>In F. van Harmelen, editor,</editor>
<contexts>
<context position="2165" citStr="Baroni et al. (2002)" startWordPosition="337" endWordPosition="340">ning of the compound if we know the meaning of its constituent words. We would like to equip the vector space model with a composition function able to construct a composite representation for apple tree from the representations of apple and tree. The composite representation should ideally be indistinguishable from its observed representation, i.e. the representation learned directly by the language model if the compound is part of the dictionary. We situate our investigations in the context of the German language, a language where compounds represent an important fraction of the vocabulary. Baroni et al. (2002) analyzed the 28 million words German APA news corpus and discovered that compounds account for 47% of the word types but only 7% of the overall token count, with 83% of compounds having a corpus frequency of 5 or lower. The high productivity of the compounding process makes the compositional approach the most tractable way to create meaningful representations for all the compounds that have been or will be coined by the speakers of the German language. German compounds have a strategic advantage for our study: they are generally written as a contiguous word, irrespective of how many constitue</context>
</contexts>
<marker>Baroni, Matiasek, Trost, 2002</marker>
<rawString>Marco Baroni, Johannes Matiasek, and Harald Trost. 2002. Predicting the components of German nominal compounds. In F. van Harmelen, editor, Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pages 470–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurie Bauer</author>
</authors>
<date>2001</date>
<booktitle>Language Typology and Language Universals. Mouton de Gruyter, The Hague.</booktitle>
<editor>Compounding. In Martin Haspelmath, editor,</editor>
<contexts>
<context position="1288" citStr="Bauer (2001)" startWordPosition="193" endWordPosition="194">ance on our dataset. 1 Introduction Vector space models of language like the ones presented in (Collobert et al., 2011b; Mikolov et al., 2013; Pennington et al., 2014) create good representations for the individual words of a language. However, the words in a language can be combined into infinitely many distinct, wellformed phrases and sentences. Creating meaningful, reusable representations for such longer word sequences is still an open problem. In this paper we focus on building representations for syntactic units just above the word level, by exploring compositional models for compounds. Bauer (2001) defines a compound as “a lexical unit made up of two or more elements, each of which can function as a lexeme independent of the other(s) in other contexts” (e.g. apple tree). The vast majority of compounds are compositional, i.e. we can understand the meaning of the compound if we know the meaning of its constituent words. We would like to equip the vector space model with a composition function able to construct a composite representation for apple tree from the representations of apple and tree. The composite representation should ideally be indistinguishable from its observed representati</context>
</contexts>
<marker>Bauer, 2001</marker>
<rawString>Laurie Bauer. 2001. Compounding. In Martin Haspelmath, editor, Language Typology and Language Universals. Mouton de Gruyter, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Koray Kavukcuoglu</author>
<author>Cl´ement Farabet</author>
</authors>
<title>Torch7: A Matlab-like environment for machine learning.</title>
<date>2011</date>
<booktitle>In BigLearn, NIPS Workshop, number EPFL-CONF-192376.</booktitle>
<contexts>
<context position="794" citStr="Collobert et al., 2011" startWordPosition="113" endWordPosition="116">rina.dima@uni-tuebingen.de Abstract In this paper we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpusestimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset. 1 Introduction Vector space models of language like the ones presented in (Collobert et al., 2011b; Mikolov et al., 2013; Pennington et al., 2014) create good representations for the individual words of a language. However, the words in a language can be combined into infinitely many distinct, wellformed phrases and sentences. Creating meaningful, reusable representations for such longer word sequences is still an open problem. In this paper we focus on building representations for syntactic units just above the word level, by exploring compositional models for compounds. Bauer (2001) defines a compound as “a lexical unit made up of two or more elements, each of which can function as a le</context>
<context position="11873" citStr="Collobert et al., 2011" startWordPosition="2053" endWordPosition="2056">mbining the masked representations, ucar O u0car and vfactory O v00factory. We tried two different combination methods: (i) p = u O u0 + v O v00, called Addmask (model 11), where the masked representations are combined via component-wise addition, and (ii) p = g(W [u O u0; v O v00]), called Wmask (model 12), where the combination of the masked representations is made via a global matrix W E Rn×2n and a nonlinearity g (tanh), similar to model 10. 3.2 Implementing composition models Models 7, 9 and 10 and the mask models were implemented using neural network architectures in the Torch7 library (Collobert et al., 2011a). We use the mean squared error as a training criterion, and optimize all models using Adagrad (Duchi et al., 2011) and a mini-batch of 100 samples. The hyperparameters were chosen by testing different parameter values and evaluating their performance on the dev set. To avoid overfitting we used early stopping (Prechelt, 1998). All the implemented models keep the input vectors fixed during the composition process. Training the mask models entails estimating modifier and head masks for every word in the dictionary D. The two types of masks to be learned can be formalized as two matrices WM, W</context>
</contexts>
<marker>Collobert, Kavukcuoglu, Farabet, 2011</marker>
<rawString>Ronan Collobert, Koray Kavukcuoglu, and Cl´ement Farabet. 2011a. Torch7: A Matlab-like environment for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="794" citStr="Collobert et al., 2011" startWordPosition="113" endWordPosition="116">rina.dima@uni-tuebingen.de Abstract In this paper we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpusestimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset. 1 Introduction Vector space models of language like the ones presented in (Collobert et al., 2011b; Mikolov et al., 2013; Pennington et al., 2014) create good representations for the individual words of a language. However, the words in a language can be combined into infinitely many distinct, wellformed phrases and sentences. Creating meaningful, reusable representations for such longer word sequences is still an open problem. In this paper we focus on building representations for syntactic units just above the word level, by exploring compositional models for compounds. Bauer (2001) defines a compound as “a lexical unit made up of two or more elements, each of which can function as a le</context>
<context position="11873" citStr="Collobert et al., 2011" startWordPosition="2053" endWordPosition="2056">mbining the masked representations, ucar O u0car and vfactory O v00factory. We tried two different combination methods: (i) p = u O u0 + v O v00, called Addmask (model 11), where the masked representations are combined via component-wise addition, and (ii) p = g(W [u O u0; v O v00]), called Wmask (model 12), where the combination of the masked representations is made via a global matrix W E Rn×2n and a nonlinearity g (tanh), similar to model 10. 3.2 Implementing composition models Models 7, 9 and 10 and the mask models were implemented using neural network architectures in the Torch7 library (Collobert et al., 2011a). We use the mean squared error as a training criterion, and optimize all models using Adagrad (Duchi et al., 2011) and a mini-batch of 100 samples. The hyperparameters were chosen by testing different parameter values and evaluating their performance on the dev set. To avoid overfitting we used early stopping (Prechelt, 1998). All the implemented models keep the input vectors fixed during the composition process. Training the mask models entails estimating modifier and head masks for every word in the dictionary D. The two types of masks to be learned can be formalized as two matrices WM, W</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011b. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>The Pham Nghia</author>
<author>Marco Baroni</author>
</authors>
<title>DISSECT - DIStributional SEmantics Composition Toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>31--36</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="7347" citStr="Dinu et al., 2013" startWordPosition="1216" endWordPosition="1219">ise vector multiplication 4. p = (u · u)v + (A − 1)(u · v)u, dilation 5. p = 0.5u + 0.5v, vector addition 6. p = Au + Qv, weighted vector addition, where the A and Q are estimated using the training set. Models 1 through 6 were introduced in (Mitchell and Lapata, 2010). 7. p = Uv, where v E Rn is the vectorial representation of the head word (given) and U E Rn×n is a matrix representation for the modifier, estimated with the help of the training data. The model estimates one matrix for each word that is used as a modifier. Referred to as alm in (Baroni and Zamparelli, 2010) and as Lexfunc in (Dinu et al., 2013b). 8. p = M1u + M2v, where M1, M2 E Rn×n are two matrices that modify the first and the second constituent vectors, respectively. In contrast to the previous model, this model estimates just one matrix for all the modifiers and one matrix for all the head words. Refered to as EAM in (Zanzotto et al., 2010) and as Fulladd in (Dinu et al., 2013b). 9. p = g(W [u; v]), where: [u; v] E R2n×1 is the concatenation of the individual word vectors; W E Rn×2n is a global matrix that: (i) combines the individual dimensions of the concatenated input vector [u; v]; (ii) brings the composite representation </context>
<context position="8676" citStr="Dinu et al., 2013" startWordPosition="1469" endWordPosition="1472">ced in (Socher et al., 2010). 10. p = g(W[Vu; Uv]). Introduced in (Socher 1 n J = � |C| i=1 n j=1 1638 et al., 2012), it is a generalization of model 7. Each word is represented using an Rn×n matrix and a Rn vector. The vectors are given, while the matrices are estimated using the training data. Referred to as Fulllex in (Dinu et al., 2013b). 11. p = uOu0 +v Ov00, the additive mask model (Addmask) and 12. p = g(W [u O u0; v O v00]), the global matrix mask model (Wmask), both presented in subsection 3.1. Models 1 through 8 were tested using the implementations available in the DISSECT toolkit (Dinu et al., 2013a). As a side note, the Lexfunc implementation in DISSECT does not produce a composite representation for 11.5% of the our test data, where a word does not appear as a modifier during training. Therefore, we reimplemented the Lexfunc model and solved the missing training material problem by initializing the matrix for all the words in the dictionary with I + E, the identity matrix plus a small amount of Gaussian noise. This type of initialization was proposed by (Socher et al., 2012), and allows the model to back-off to the model p = v when there is no data to estimate the parameters of the mo</context>
<context position="20112" citStr="Dinu et al., 2013" startWordPosition="3579" endWordPosition="3582">nvestigations are necessary to estimate the impact of different objective functions on the performance of compositional models. 5 Comparison to related work The experiments reported in this paper are, to the best of our knowledge, the first large scale experiments on the composition of German compounds. Other studies (Kisselew et al., 2015; Lazaridou et al., 2013) focused on morphologically complex words in German and English respectively. In terms of the size of the training and test material, our experiments are closest to the adjective-noun experiments in (Baroni and Zamparelli, 2010) and (Dinu et al., 2013b) where the lexical function model performed the best, with lowest reported median ranks (Q2) above 100. 6 Conclusions Twelve composition models were evaluated on the task of building compositional representations for German compounds. The best results (median rank 6) were obtained by the newly introduced Wmask model, p = g(W [u O u&apos;; v O v&apos;&apos;]). The results show that it is possible to learn a composition function specific to compounds, an idea which we would like to further explore using existing compound datasets for English ( O´ S´eaghdha, 2008; Tratz and Hovy, 2010). The implementation of </context>
</contexts>
<marker>Dinu, Nghia, Baroni, 2013</marker>
<rawString>Georgiana Dinu, The Pham Nghia, and Marco Baroni. 2013a. DISSECT - DIStributional SEmantics Composition Toolkit. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 31–36, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>The Pham Nghia</author>
<author>Marco Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="7347" citStr="Dinu et al., 2013" startWordPosition="1216" endWordPosition="1219">ise vector multiplication 4. p = (u · u)v + (A − 1)(u · v)u, dilation 5. p = 0.5u + 0.5v, vector addition 6. p = Au + Qv, weighted vector addition, where the A and Q are estimated using the training set. Models 1 through 6 were introduced in (Mitchell and Lapata, 2010). 7. p = Uv, where v E Rn is the vectorial representation of the head word (given) and U E Rn×n is a matrix representation for the modifier, estimated with the help of the training data. The model estimates one matrix for each word that is used as a modifier. Referred to as alm in (Baroni and Zamparelli, 2010) and as Lexfunc in (Dinu et al., 2013b). 8. p = M1u + M2v, where M1, M2 E Rn×n are two matrices that modify the first and the second constituent vectors, respectively. In contrast to the previous model, this model estimates just one matrix for all the modifiers and one matrix for all the head words. Refered to as EAM in (Zanzotto et al., 2010) and as Fulladd in (Dinu et al., 2013b). 9. p = g(W [u; v]), where: [u; v] E R2n×1 is the concatenation of the individual word vectors; W E Rn×2n is a global matrix that: (i) combines the individual dimensions of the concatenated input vector [u; v]; (ii) brings the composite representation </context>
<context position="8676" citStr="Dinu et al., 2013" startWordPosition="1469" endWordPosition="1472">ced in (Socher et al., 2010). 10. p = g(W[Vu; Uv]). Introduced in (Socher 1 n J = � |C| i=1 n j=1 1638 et al., 2012), it is a generalization of model 7. Each word is represented using an Rn×n matrix and a Rn vector. The vectors are given, while the matrices are estimated using the training data. Referred to as Fulllex in (Dinu et al., 2013b). 11. p = uOu0 +v Ov00, the additive mask model (Addmask) and 12. p = g(W [u O u0; v O v00]), the global matrix mask model (Wmask), both presented in subsection 3.1. Models 1 through 8 were tested using the implementations available in the DISSECT toolkit (Dinu et al., 2013a). As a side note, the Lexfunc implementation in DISSECT does not produce a composite representation for 11.5% of the our test data, where a word does not appear as a modifier during training. Therefore, we reimplemented the Lexfunc model and solved the missing training material problem by initializing the matrix for all the words in the dictionary with I + E, the identity matrix plus a small amount of Gaussian noise. This type of initialization was proposed by (Socher et al., 2012), and allows the model to back-off to the model p = v when there is no data to estimate the parameters of the mo</context>
<context position="20112" citStr="Dinu et al., 2013" startWordPosition="3579" endWordPosition="3582">nvestigations are necessary to estimate the impact of different objective functions on the performance of compositional models. 5 Comparison to related work The experiments reported in this paper are, to the best of our knowledge, the first large scale experiments on the composition of German compounds. Other studies (Kisselew et al., 2015; Lazaridou et al., 2013) focused on morphologically complex words in German and English respectively. In terms of the size of the training and test material, our experiments are closest to the adjective-noun experiments in (Baroni and Zamparelli, 2010) and (Dinu et al., 2013b) where the lexical function model performed the best, with lowest reported median ranks (Q2) above 100. 6 Conclusions Twelve composition models were evaluated on the task of building compositional representations for German compounds. The best results (median rank 6) were obtained by the newly introduced Wmask model, p = g(W [u O u&apos;; v O v&apos;&apos;]). The results show that it is possible to learn a composition function specific to compounds, an idea which we would like to further explore using existing compound datasets for English ( O´ S´eaghdha, 2008; Tratz and Hovy, 2010). The implementation of </context>
</contexts>
<marker>Dinu, Nghia, Baroni, 2013</marker>
<rawString>Georgiana Dinu, The Pham Nghia, and Marco Baroni. 2013b. General estimation and evaluation of compositional distributional semantic models. In Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="11990" citStr="Duchi et al., 2011" startWordPosition="2073" endWordPosition="2076">(i) p = u O u0 + v O v00, called Addmask (model 11), where the masked representations are combined via component-wise addition, and (ii) p = g(W [u O u0; v O v00]), called Wmask (model 12), where the combination of the masked representations is made via a global matrix W E Rn×2n and a nonlinearity g (tanh), similar to model 10. 3.2 Implementing composition models Models 7, 9 and 10 and the mask models were implemented using neural network architectures in the Torch7 library (Collobert et al., 2011a). We use the mean squared error as a training criterion, and optimize all models using Adagrad (Duchi et al., 2011) and a mini-batch of 100 samples. The hyperparameters were chosen by testing different parameter values and evaluating their performance on the dev set. To avoid overfitting we used early stopping (Prechelt, 1998). All the implemented models keep the input vectors fixed during the composition process. Training the mask models entails estimating modifier and head masks for every word in the dictionary D. The two types of masks to be learned can be formalized as two matrices WM, WH E Rn×|D|, where n is the size of the initial word representations. The masks of the word wi E D are the ith rows in</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Henrich</author>
<author>Erhard W Hinrichs</author>
</authors>
<title>Determining Immediate Constituents of Compounds in GermaNet.</title>
<date>2011</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP 2011),</booktitle>
<pages>420--426</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="5469" citStr="Henrich and Hinrichs, 2011" startWordPosition="876" endWordPosition="879">que words). We used the default GloVe training parameters, the only modifications being the use of a symmetric context when constructing the co-occurence matrix (10 words to the left and to the right of the target word) and training each model for 15 iterations. All the vector spaces were normalized to the L2-norm, first across features then across samples using scikit-learn (Pedregosa et al., 2011). The German compounds dataset used in the experiments is a subset of the 54759 compounds available in GermaNet 9.01. The compounds in the list were automatically split and manually post-corrected (Henrich and Hinrichs, 2011). Each entry in the list is a triple of the form (compound, modifier, head). We filtered the entries in the list, keeping only those where all three words have a minimum frequency of 500 in the support corpus used to create the vector space representations. The reason for the filtering step is that a “well-learned” representation (based on a sufficiently large number of contexts) should allow for a more accurate reconstruction than a representation based only on a few contexts. The filtered dataset contains 34497 entries. This dataset was 1http://www.sfs.uni-tuebingen.de/lsd/compounds.shtml ra</context>
</contexts>
<marker>Henrich, Hinrichs, 2011</marker>
<rawString>Verena Henrich and Erhard W. Hinrichs. 2011. Determining Immediate Constituents of Compounds in GermaNet. In Proceedings of Recent Advances in Natural Language Processing (RANLP 2011), pages 420–426, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Kisselew</author>
<author>Sebastian Pad´o</author>
<author>Alexis Palmer</author>
<author>Jan ˇSnajder</author>
</authors>
<title>Obtaining a Better Understanding of Distributional Models of German Derivational Morphology.</title>
<date>2015</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015),</booktitle>
<pages>58--63</pages>
<location>London, UK.</location>
<marker>Kisselew, Pad´o, Palmer, ˇSnajder, 2015</marker>
<rawString>Max Kisselew, Sebastian Pad´o, Alexis Palmer, and Jan ˇSnajder. 2015. Obtaining a Better Understanding of Distributional Models of German Derivational Morphology. In Proceedings of the 11th International Conference on Computational Semantics (IWCS 2015), pages 58–63, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Marco Marelli</author>
<author>Roberto Zamparelli</author>
<author>Marco Baroni</author>
</authors>
<title>Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1517--1526</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="19861" citStr="Lazaridou et al., 2013" startWordPosition="3540" endWordPosition="3543"> MSE of the best performing Wmask model. This is in contrast to the rank evaluation where the multiplicative model assigned the observed representations in the test set only ranks ≥ 1000, while Wmask assigned ranks ≤ 25 to 75% of the test data. Additional investigations are necessary to estimate the impact of different objective functions on the performance of compositional models. 5 Comparison to related work The experiments reported in this paper are, to the best of our knowledge, the first large scale experiments on the composition of German compounds. Other studies (Kisselew et al., 2015; Lazaridou et al., 2013) focused on morphologically complex words in German and English respectively. In terms of the size of the training and test material, our experiments are closest to the adjective-noun experiments in (Baroni and Zamparelli, 2010) and (Dinu et al., 2013b) where the lexical function model performed the best, with lowest reported median ranks (Q2) above 100. 6 Conclusions Twelve composition models were evaluated on the task of building compositional representations for German compounds. The best results (median rank 6) were obtained by the newly introduced Wmask model, p = g(W [u O u&apos;; v O v&apos;&apos;]). </context>
</contexts>
<marker>Lazaridou, Marelli, Zamparelli, Baroni, 2013</marker>
<rawString>Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, and Marco Baroni. 2013. Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1517–1526, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="817" citStr="Mikolov et al., 2013" startWordPosition="117" endWordPosition="120">e Abstract In this paper we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpusestimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset. 1 Introduction Vector space models of language like the ones presented in (Collobert et al., 2011b; Mikolov et al., 2013; Pennington et al., 2014) create good representations for the individual words of a language. However, the words in a language can be combined into infinitely many distinct, wellformed phrases and sentences. Creating meaningful, reusable representations for such longer word sequences is still an open problem. In this paper we focus on building representations for syntactic units just above the word level, by exploring compositional models for compounds. Bauer (2001) defines a compound as “a lexical unit made up of two or more elements, each of which can function as a lexeme independent of the</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="4166" citStr="Mitchell and Lapata, 2010" startWordPosition="661" endWordPosition="665">s the composite representation of the compound and mobs, hobs ∈ Rn are the observed representations of its modifier and its head. The function should minimize J, the mean squared error between the composite (ccomp) and 1637 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1637–1642, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. the observed (cobs) representations of the |C |compounds in the training set: (C comp− cobs l ij ij )2 Several compositionality models have already been proposed in the literature (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012). In this paper we evaluate several of the proposed composition functions and also present a new composition model which outperforms all previous models on a dataset of German compounds. 2 Word Representations and Compounds Dataset We trained 4 vector space language models for German (with 50, 100, 200 and 300 dimensions respectively) using the GloVe package (Pennington et al., 2014) and a 10 billion token raw-text corpus extracted from the DECOW14AX corpus (Sch¨afer, 2015). We use a vocabulary of 1,029,270 (1M) words, obtained by selecting al</context>
<context position="6433" citStr="Mitchell and Lapata, 2010" startWordPosition="1032" endWordPosition="1035"> on a sufficiently large number of contexts) should allow for a more accurate reconstruction than a representation based only on a few contexts. The filtered dataset contains 34497 entries. This dataset was 1http://www.sfs.uni-tuebingen.de/lsd/compounds.shtml randomized and partitioned into train, test and dev splits according to the 70-20-10 rule. The dataset contains 8580 unique modifiers and heads, and a dictionary of 41732 unique words. 1345 compounds appear as the modifier or head of another compound. 3 12 ways to Represent A Compound We adopt a notation similar to the one introduced in (Mitchell and Lapata, 2010), where the composite representation p is the result of applying a composition function f to the vectors u and v. In this study we tested the following composition functions: 1. p = v, the second constituent of the compound 2. p = u, the first constituent of the compound 3. p = u O v, component-wise vector multiplication 4. p = (u · u)v + (A − 1)(u · v)u, dilation 5. p = 0.5u + 0.5v, vector addition 6. p = Au + Qv, weighted vector addition, where the A and Q are estimated using the training set. Models 1 through 6 were introduced in (Mitchell and Lapata, 2010). 7. p = Uv, where v E Rn is the v</context>
<context position="15212" citStr="Mitchell and Lapata, 2010" startWordPosition="2630" endWordPosition="2634">ned ranks 1 and 2. Similarly, Q2 and Q3 refer to the ranks assigned to the first 50% and 75% of data, respectively. The results of our evaluation are displayed in Table 1. The observed representation of the head (model 1) was used as a strong baseline for the compound composition task. Two of the tested models, multiplicative (model 3) and dilation (model 4) score worse than the head baseline, while the additive models (5 and 6) score only slightly above it. The fact that the worst performing model is the multiplicative model is surprising considering its good performance in previous studies (Mitchell and Lapata, 2010). This might be either a side-effect of the normalization procedure, or a genuine incompatibility of this compositionality model with the vectorial representations produced by GloVe. The new Addmask and Wmask models (introduced in Section 3.1) perform very well, with Wmask producing the best results on the test dataset across all dimensions. It is interesting to note that the linguistically motivated Lexfunc and Fulllex models, which build dedicated representations for each individual constituent, are outperformed by a simple model like Fulladd, that only learns two modification matrices, one </context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Learning compound noun semantics.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Laboratory, University of Cambridge. Published as University of Cambridge Computer Laboratory</institution>
<marker>S´eaghdha, 2008</marker>
<rawString>Diarmuid O´ S´eaghdha. 2008. Learning compound noun semantics. Ph.D. thesis, Computer Laboratory, University of Cambridge. Published as University of Cambridge Computer Laboratory Technical Report 735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="5244" citStr="Pedregosa et al., 2011" startWordPosition="839" endWordPosition="843">token raw-text corpus extracted from the DECOW14AX corpus (Sch¨afer, 2015). We use a vocabulary of 1,029,270 (1M) words, obtained by selecting all the words with a minimum frequency of 100 (the full vocabulary had 50M unique words). We used the default GloVe training parameters, the only modifications being the use of a symmetric context when constructing the co-occurence matrix (10 words to the left and to the right of the target word) and training each model for 15 iterations. All the vector spaces were normalized to the L2-norm, first across features then across samples using scikit-learn (Pedregosa et al., 2011). The German compounds dataset used in the experiments is a subset of the 54759 compounds available in GermaNet 9.01. The compounds in the list were automatically split and manually post-corrected (Henrich and Hinrichs, 2011). Each entry in the list is a triple of the form (compound, modifier, head). We filtered the entries in the list, keeping only those where all three words have a minimum frequency of 500 in the support corpus used to create the vector space representations. The reason for the filtering step is that a “well-learned” representation (based on a sufficiently large number of co</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<volume>12</volume>
<contexts>
<context position="843" citStr="Pennington et al., 2014" startWordPosition="121" endWordPosition="124">er we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpusestimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset. 1 Introduction Vector space models of language like the ones presented in (Collobert et al., 2011b; Mikolov et al., 2013; Pennington et al., 2014) create good representations for the individual words of a language. However, the words in a language can be combined into infinitely many distinct, wellformed phrases and sentences. Creating meaningful, reusable representations for such longer word sequences is still an open problem. In this paper we focus on building representations for syntactic units just above the word level, by exploring compositional models for compounds. Bauer (2001) defines a compound as “a lexical unit made up of two or more elements, each of which can function as a lexeme independent of the other(s) in other context</context>
<context position="4603" citStr="Pennington et al., 2014" startWordPosition="731" endWordPosition="735"> representations of the |C |compounds in the training set: (C comp− cobs l ij ij )2 Several compositionality models have already been proposed in the literature (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012). In this paper we evaluate several of the proposed composition functions and also present a new composition model which outperforms all previous models on a dataset of German compounds. 2 Word Representations and Compounds Dataset We trained 4 vector space language models for German (with 50, 100, 200 and 300 dimensions respectively) using the GloVe package (Pennington et al., 2014) and a 10 billion token raw-text corpus extracted from the DECOW14AX corpus (Sch¨afer, 2015). We use a vocabulary of 1,029,270 (1M) words, obtained by selecting all the words with a minimum frequency of 100 (the full vocabulary had 50M unique words). We used the default GloVe training parameters, the only modifications being the use of a symmetric context when constructing the co-occurence matrix (10 words to the left and to the right of the target word) and training each model for 15 iterations. All the vector spaces were normalized to the L2-norm, first across features then across samples us</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), volume 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lutz Prechelt</author>
</authors>
<title>Early stopping - but when?</title>
<date>1998</date>
<booktitle>In Neural Networks: Tricks of the trade,</booktitle>
<pages>55--69</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="12203" citStr="Prechelt, 1998" startWordPosition="2107" endWordPosition="2108">e masked representations is made via a global matrix W E Rn×2n and a nonlinearity g (tanh), similar to model 10. 3.2 Implementing composition models Models 7, 9 and 10 and the mask models were implemented using neural network architectures in the Torch7 library (Collobert et al., 2011a). We use the mean squared error as a training criterion, and optimize all models using Adagrad (Duchi et al., 2011) and a mini-batch of 100 samples. The hyperparameters were chosen by testing different parameter values and evaluating their performance on the dev set. To avoid overfitting we used early stopping (Prechelt, 1998). All the implemented models keep the input vectors fixed during the composition process. Training the mask models entails estimating modifier and head masks for every word in the dictionary D. The two types of masks to be learned can be formalized as two matrices WM, WH E Rn×|D|, where n is the size of the initial word representations. The masks of the word wi E D are the ith rows in WM and WH. In Torch7 such representations can be learned using lookup table layers (Collobert et al., 2011b), which map matrix indices to the corresponding row vector. 1639 The masked representation of the modifi</context>
</contexts>
<marker>Prechelt, 1998</marker>
<rawString>Lutz Prechelt. 1998. Early stopping - but when? In Neural Networks: Tricks of the trade, pages 55–69. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
</authors>
<title>Processing and querying large web corpora with the COW14 architecture.</title>
<date>2015</date>
<booktitle>In Challenges in the Management of Large Corpora (CMLC-3).</booktitle>
<marker>Sch¨afer, 2015</marker>
<rawString>Roland Sch¨afer. 2015. Processing and querying large web corpora with the COW14 architecture. In Challenges in the Management of Large Corpora (CMLC-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="8087" citStr="Socher et al., 2010" startWordPosition="1353" endWordPosition="1356">pectively. In contrast to the previous model, this model estimates just one matrix for all the modifiers and one matrix for all the head words. Refered to as EAM in (Zanzotto et al., 2010) and as Fulladd in (Dinu et al., 2013b). 9. p = g(W [u; v]), where: [u; v] E R2n×1 is the concatenation of the individual word vectors; W E Rn×2n is a global matrix that: (i) combines the individual dimensions of the concatenated input vector [u; v]; (ii) brings the composite representation back into the Rn×1 space; g is an element-wise function, in our experiments the hyperbolic tangent tanh. Introduced in (Socher et al., 2010). 10. p = g(W[Vu; Uv]). Introduced in (Socher 1 n J = � |C| i=1 n j=1 1638 et al., 2012), it is a generalization of model 7. Each word is represented using an Rn×n matrix and a Rn vector. The vectors are given, while the matrices are estimated using the training data. Referred to as Fulllex in (Dinu et al., 2013b). 11. p = uOu0 +v Ov00, the additive mask model (Addmask) and 12. p = g(W [u O u0; v O v00]), the global matrix mask model (Wmask), both presented in subsection 3.1. Models 1 through 8 were tested using the implementations available in the DISSECT toolkit (Dinu et al., 2013a). As a si</context>
<context position="9369" citStr="Socher et al., 2010" startWordPosition="1596" endWordPosition="1599"> a composite representation for 11.5% of the our test data, where a word does not appear as a modifier during training. Therefore, we reimplemented the Lexfunc model and solved the missing training material problem by initializing the matrix for all the words in the dictionary with I + E, the identity matrix plus a small amount of Gaussian noise. This type of initialization was proposed by (Socher et al., 2012), and allows the model to back-off to the model p = v when there is no data to estimate the parameters of the modifier matrix. We also reimplemented models 9 and 10, which were used in (Socher et al., 2010; Socher et al., 2012), as the existing implementations are part of a more complex recursive architecture aimed at constructing representations for full sentences. 3.1 The mask models The newly introduced mask models build upon the idea that when a word w enters a composition process, there is some variation in its meaning depending on whether it is the first or the second element of the composition. Think, for instance, of the compounds company car and car factory. In the first case, car has its primary denotation, that of a road vehicle. In the second case, what matters more about the car is</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="4217" citStr="Socher et al., 2012" startWordPosition="670" endWordPosition="673"> hobs ∈ Rn are the observed representations of its modifier and its head. The function should minimize J, the mean squared error between the composite (ccomp) and 1637 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1637–1642, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. the observed (cobs) representations of the |C |compounds in the training set: (C comp− cobs l ij ij )2 Several compositionality models have already been proposed in the literature (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Socher et al., 2012). In this paper we evaluate several of the proposed composition functions and also present a new composition model which outperforms all previous models on a dataset of German compounds. 2 Word Representations and Compounds Dataset We trained 4 vector space language models for German (with 50, 100, 200 and 300 dimensions respectively) using the GloVe package (Pennington et al., 2014) and a 10 billion token raw-text corpus extracted from the DECOW14AX corpus (Sch¨afer, 2015). We use a vocabulary of 1,029,270 (1M) words, obtained by selecting all the words with a minimum frequency of 100 (the fu</context>
<context position="9164" citStr="Socher et al., 2012" startWordPosition="1556" endWordPosition="1559">nted in subsection 3.1. Models 1 through 8 were tested using the implementations available in the DISSECT toolkit (Dinu et al., 2013a). As a side note, the Lexfunc implementation in DISSECT does not produce a composite representation for 11.5% of the our test data, where a word does not appear as a modifier during training. Therefore, we reimplemented the Lexfunc model and solved the missing training material problem by initializing the matrix for all the words in the dictionary with I + E, the identity matrix plus a small amount of Gaussian noise. This type of initialization was proposed by (Socher et al., 2012), and allows the model to back-off to the model p = v when there is no data to estimate the parameters of the modifier matrix. We also reimplemented models 9 and 10, which were used in (Socher et al., 2010; Socher et al., 2012), as the existing implementations are part of a more complex recursive architecture aimed at constructing representations for full sentences. 3.1 The mask models The newly introduced mask models build upon the idea that when a word w enters a composition process, there is some variation in its meaning depending on whether it is the first or the second element of the comp</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A taxonomy, dataset, and classifier for automatic noun compound interpretation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<location>Uppsala,</location>
<marker>Tratz, Hovy, 2010</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2010. A taxonomy, dataset, and classifier for automatic noun compound interpretation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Fallucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating Linear Models for Compositional Distributional Semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1263--1271</pages>
<contexts>
<context position="7655" citStr="Zanzotto et al., 2010" startWordPosition="1275" endWordPosition="1278">ectorial representation of the head word (given) and U E Rn×n is a matrix representation for the modifier, estimated with the help of the training data. The model estimates one matrix for each word that is used as a modifier. Referred to as alm in (Baroni and Zamparelli, 2010) and as Lexfunc in (Dinu et al., 2013b). 8. p = M1u + M2v, where M1, M2 E Rn×n are two matrices that modify the first and the second constituent vectors, respectively. In contrast to the previous model, this model estimates just one matrix for all the modifiers and one matrix for all the head words. Refered to as EAM in (Zanzotto et al., 2010) and as Fulladd in (Dinu et al., 2013b). 9. p = g(W [u; v]), where: [u; v] E R2n×1 is the concatenation of the individual word vectors; W E Rn×2n is a global matrix that: (i) combines the individual dimensions of the concatenated input vector [u; v]; (ii) brings the composite representation back into the Rn×1 space; g is an element-wise function, in our experiments the hyperbolic tangent tanh. Introduced in (Socher et al., 2010). 10. p = g(W[Vu; Uv]). Introduced in (Socher 1 n J = � |C| i=1 n j=1 1638 et al., 2012), it is a generalization of model 7. Each word is represented using an Rn×n matr</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>Fabio Massimo Zanzotto, Ioannis Korkontzelos, Francesca Fallucchi, and Suresh Manandhar. 2010. Estimating Linear Models for Compositional Distributional Semantics. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1263–1271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
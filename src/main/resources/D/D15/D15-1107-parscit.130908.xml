<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.997674">
Auto-Sizing Neural Networks:
With Applications to n-gram Language Models
</title>
<author confidence="0.992928">
Kenton Murray and David Chiang
</author>
<affiliation confidence="0.9965555">
Department of Computer Science and Engineering
University of Notre Dame
</affiliation>
<email confidence="0.99822">
{kmurray4,dchiang}@nd.edu
</email>
<sectionHeader confidence="0.997379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999216">
Neural networks have been shown to
improve performance across a range of
natural-language tasks. However, design-
ing and training them can be complicated.
Frequently, researchers resort to repeated
experimentation to pick optimal settings.
In this paper, we address the issue of
choosing the correct number of units in
hidden layers. We introduce a method for
automatically adjusting network size by
pruning out hidden units through E∞,1 and
E2,1 regularization. We apply this method
to language modeling and demonstrate its
ability to correctly choose the number of
hidden units while maintaining perplexity.
We also include these models in a machine
translation decoder and show that these
smaller neural models maintain the signif-
icant improvements of their unpruned ver-
sions.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978810810811">
Neural networks have proven to be highly ef-
fective at many tasks in natural language. For
example, neural language models and joint lan-
guage/translation models improve machine trans-
lation quality significantly (Vaswani et al., 2013;
Devlin et al., 2014). However, neural networks can
be complicated to design and train well. Many de-
cisions need to be made, and performance can be
highly dependent on making them correctly. Yet
the optimal settings are non-obvious and can be
laborious to find, often requiring an extensive grid
search involving numerous experiments.
In this paper, we focus on the choice of the
sizes of hidden layers. We introduce a method
for automatically pruning out hidden layer units,
by adding a sparsity-inducing regularizer that en-
courages units to deactivate if not needed, so that
they can be removed from the network. Thus, af-
ter training with more units than necessary, a net-
work is produced that has hidden layers correctly
sized, saving both time and memory when actually
putting the network to use.
Using a neural n-gram language model (Bengio
et al., 2003), we are able to show that our novel
auto-sizing method is able to learn models that are
smaller than models trained without the method,
while maintaining nearly the same perplexity. The
method has only a single hyperparameter to adjust
(as opposed to adjusting the sizes of each of the
hidden layers), and we find that the same setting
works consistently well across different training
data sizes, vocabulary sizes, and n-gram sizes. In
addition, we show that incorporating these mod-
els into a machine translation decoder still results
in large BLEU point improvements. The result is
that fewer experiments are needed to obtain mod-
els that perform well and are correctly sized.
</bodyText>
<sectionHeader confidence="0.995481" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999953947368421">
Language models are often used in natural lan-
guage processing tasks involving generation of
text. For instance, in machine translation, the lan-
guage model helps to output fluent translations,
and in speech recognition, the language model
helps to disambiguate among possible utterances.
Current language models are usually n-gram
models, which look at the previous (n − 1) words
to predict the nth word in a sequence, based
on (smoothed) counts of n-grams collected from
training data. These models are simple but very
effective in improving the performance of natural
language systems.
However, n-gram models suffer from some lim-
itations, such as data sparsity and memory usage.
As an alternative, researchers have begun explor-
ing the use of neural networks for language mod-
eling. For modeling n-grams, the most common
approach is the feedforward network of Bengio et
</bodyText>
<page confidence="0.959765">
908
</page>
<note confidence="0.9873915">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 908–916,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.97219425">
al. (2003), shown in Figure 1.
Each node represents a unit or “neuron,” which
has a real valued activation. The units are orga-
nized into real-vector valued layers. The activa-
tions at each layer are computed as follows. (We
assume n = 3; the generalization is easy.) The two
preceding words, w1, w2, are mapped into lower-
dimensional word embeddings,
</bodyText>
<equation confidence="0.9814952">
x1 = A:w1
x2 = A:w2
then passed through two hidden layers,
y = f(B1x1 + B2x2 + b)
z = f(Cy + c)
</equation>
<bodyText confidence="0.999874166666667">
where f is an elementwise nonlinear activation
(or transfer) function. Commonly used activation
functions are the hyperbolic tangent, logistic func-
tion, and rectified linear units, to name a few. Fi-
nally, the result is mapped via a softmax to an out-
put probability distribution,
</bodyText>
<equation confidence="0.984512">
P(wn  |w1 ··· wn−1) a exp([Dz + d]wn).
</equation>
<bodyText confidence="0.999966222222222">
The parameters of the model are A, B1, B2, b,
C, c, D, and d, which are learned by minimizing
the negative log-likelihood of the the training data
using stochastic gradient descent (also known as
backpropagation) or variants.
Vaswani et al. (2013) showed that this model,
with some improvements, can be used effectively
during decoding in machine translation. In this pa-
per, we use and extend their implementation.
</bodyText>
<sectionHeader confidence="0.998726" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.9999575625">
Our method is focused on the challenge of choos-
ing the number of units in the hidden layers of a
feed-forward neural network. The networks used
for different tasks require different numbers of
units, and the layers in a single network also re-
quire different numbers of units. Choosing too few
units can impair the performance of the network,
and choosing too many units can lead to overfit-
ting. It can also slow down computations with the
network, which can be a major concern for many
applications such as integrating neural language
models into a machine translation decoder.
Our method starts out with a large number of
units in each layer and then jointly trains the net-
work while pruning out individual units when pos-
sible. The goal is to end up with a trained network
</bodyText>
<figureCaption confidence="0.90803">
Figure 1: Neural probabilistic language model
(Bengio et al., 2003), adapted from Vaswani et al.
(2013).
</figureCaption>
<bodyText confidence="0.991291928571428">
that also has the optimal number of units in each
layer.
We do this by adding a regularizer to the ob-
jective function. For simplicity, consider a single
layer without bias, y = f(Wx). Let L(W) be
the negative log-likelihood of the model. Instead
of minimizing L(W) alone, we want to mini-
mize L(W) + AR(W), where R(W) is a con-
vex regularizer. The E1 norm, R(W) = IIWII1 =
Ei,j |Wij|, is a common choice for pushing pa-
rameters to zero, which can be useful for prevent-
ing overfitting and reducing model size. However,
we are interested not only in reducing the number
of parameters but the number of units. To do this,
we need a different regularizer.
We assume activation functions that satisfy
f(0) = 0, such as the hyperbolic tangent or rec-
tified linear unit (f(x) = max{0, x}). Then, if we
push the incoming weights of a unit yi to zero, that
is, Wij = 0 for all j (as well as the bias, if any:
bi = 0), then yi = f(0) = 0 is independent of the
previous layers and contributes nothing to subse-
quent layers. So the unit can be removed without
affecting the network at all. Therefore, we need a
regularizer that pushes all the incoming connec-
tion weights to a unit together towards zero.
Here, we experiment with two, the E2,1 norm
and the E∞,1 norm.1 The E2,1 norm on a ma-
</bodyText>
<footnote confidence="0.9137715">
1In the notation fp,q, the subscript p corresponds to the
norm over each group of parameters, and q corresponds to
the norm over the group norms. Contrary to more common
usage, in this paper, the groups are rows, not columns.
</footnote>
<figure confidence="0.962991066666667">
output
P(w3  |w1w2)
D
hidden
z
C
hidden
y
input
embeddings
x1, x2
words
w1, w2
B1 B2
A
</figure>
<page confidence="0.905881">
909
</page>
<figureCaption confidence="0.877932666666667">
Figure 2: The (unsquared) `2 norm and `oo norm
both have sharp tips at the origin that encourage
sparsity.
</figureCaption>
<equation confidence="0.994526">
trix W is
� � ⎛ ⎞ 1 . (1)
R(W) = kWi:k2 = ⎝J:W2⎠ 2
i i ij
j
</equation>
<bodyText confidence="0.998471692307692">
(If there are biases bi, they should be included as
well.) This puts equal pressure on each row, but
within each row, the larger values contribute more,
and therefore there is more pressure on larger val-
ues towards zero. The `oo,1 norm is
Again, this puts equal pressure on each row, but
within each row, only the maximum value (or val-
ues) matter, and therefore the pressure towards
zero is entirely on the maximum value(s).
Figure 2 visualizes the sparsity-inducing behav-
ior of the two regularizers on a single row. Both
have a sharp tip at the origin that encourages all
the parameters in a row to become exactly zero.
</bodyText>
<sectionHeader confidence="0.996962" genericHeader="method">
4 Optimization
</sectionHeader>
<bodyText confidence="0.999982142857143">
However, this also means that sparsity-inducing
regularizers are not differentiable at zero, mak-
ing gradient-based optimization methods trickier
to apply. The methods we use are discussed in
detail elsewhere (Duchi et al., 2008; Duchi and
Singer, 2009); in this section, we include a short
description of these methods for completeness.
</bodyText>
<subsectionHeader confidence="0.990785">
4.1 Proximal gradient method
</subsectionHeader>
<bodyText confidence="0.9999823">
Most work on learning with regularizers, includ-
ing this work, can be thought of as instances of
the proximal gradient method (Parikh and Boyd,
2014). Our objective function can be split into two
parts, a convex and differentiable part (L) and a
convex but non-differentiable part (λR). In prox-
imal gradient descent, we alternate between im-
proving L alone and λR alone. Let u be the pa-
rameter values from the previous iteration. We
compute new parameter values w using:
</bodyText>
<equation confidence="0.972126">
v ← u − η∇L(u) (3)
� 1 �
2η kw − vk2 + λR(w) (4)
</equation>
<bodyText confidence="0.999669909090909">
and repeat until convergence. The first update is
just a standard gradient descent update on L; the
second is known as the proximal operator for λR
and in many cases has a closed-form solution. In
the rest of this section, we provide some justifica-
tion for this method, and in Sections 4.2 and 4.3
we show how to compute the proximal operator
for the `2 and `oo norms.
We can think of the gradient descent update (3)
on L as follows. Approximate L around u by the
tangent plane,
</bodyText>
<equation confidence="0.999504">
L(v) = L(u) + ∇L(u)(v − u) (5)
</equation>
<bodyText confidence="0.8098825">
and move v to minimize L, but don’t move it too
far from u; that is, minimize
</bodyText>
<equation confidence="0.9981965">
F(v) = 2η kv − uk2 + ¯L(v).
1
</equation>
<bodyText confidence="0.951609">
Setting partial derivatives to zero, we get
</bodyText>
<equation confidence="0.993611666666667">
1 (v − u) + ∇L(u) = 0
η
v = u − η∇L(u).
</equation>
<bodyText confidence="0.99955775">
By a similar strategy, we can derive the second
step (4). Again we want to move w to minimize
the objective function, but don’t want to move it
too far from u; that is, we want to minimize:
</bodyText>
<equation confidence="0.9979155">
G(w) = 2η kw − uk2 + ¯L(w) + λR(w).
1
</equation>
<bodyText confidence="0.999294">
Note that we have not approximated R by a tan-
gent plane. We can simplify this by substituting
in (3). The first term becomes
</bodyText>
<equation confidence="0.9971701875">
1kw−uk2= 1kw−v−η∇L(u)k2
2η 2η
= 2ηkw − vk2 − ∇L(u)(w − v)
1
η
+ 2k∇L(u)k2
x1 x1
`2 `oo
x2
x2
R(W) =� �kWi:koo = max |Wij|. (2)
i i j
w ← arg max
W
∂F
∂v =
</equation>
<page confidence="0.956955">
910
</page>
<figure confidence="0.634906125">
and the second term becomes
L(w) = L(u) + ∇L(u)(w − u)
= L(u) + ∇L(u)(w − v − q∇L(u)).
The ∇L(u)(w − v) terms cancel out, and we can
ignore terms not involving w, giving
1
G(w) = kw − vk2 + AR(w) + const.
2q
</figure>
<bodyText confidence="0.9485905">
which is minimized by the update (4). Thus, we
have split the optimization step into two easier
steps: first, do the update for L (3), then do the
update for AR (4). The latter can often be done
exactly (without approximating R by a tangent
plane). We show next how to do this for the E2
and E∞ norms.
4.2 E2 and E2,1 regularization
Since the E2,1 norm on matrices (1) is separable
into the E2 norm of each row, we can treat each
row separately. Thus, for simplicity, assume that
we have a single row and want to minimize
</bodyText>
<equation confidence="0.9945605">
G(w) = 2qkw − vk2 + Akwk + const.
1
</equation>
<bodyText confidence="0.841354666666667">
The minimum is either at w = 0 (the tip of
the cone) or where the partial derivatives are zero
(Figure 3):
</bodyText>
<equation confidence="0.96963">
�(w − v) + Akwk = 0.
w
</equation>
<bodyText confidence="0.9298378">
Clearly, w and v must have the same direction and
differ only in magnitude, that is, w = α v
kvk. Sub-
stituting this into the above equation, we get the
solution
</bodyText>
<equation confidence="0.83445725">
α = kvk − qA.
Therefore the update is
v
4.3 E∞ and E∞,1 regularization
</equation>
<bodyText confidence="0.99820325">
As above, since the E∞,1 norm on matrices (2) is
separable into the E∞ norm of each row, we can
treat each row separately; thus, we want to mini-
mize
</bodyText>
<equation confidence="0.931874666666667">
G(w) = 1 k w − vk2 + A max  |xj  |+ const.
2q
kwk &gt; 0 kwk = 0
</equation>
<figureCaption confidence="0.999661">
Figure 3: Examples of the two possible cases for
the E2 gradient update. Point v is drawn with a hol-
low dot, and point w is drawn with a solid dot.
Figure 4: The proximal operator for the E∞ norm
</figureCaption>
<bodyText confidence="0.990338741935484">
(with strength qA) decreases the maximal compo-
nents until the total decrease sums to qA. Projec-
tion onto the E1-ball (of radius qA) decreases each
component by an equal amount until they sum
to qA.
Intuitively, the solution can be characterized as:
Decrease all of the maximal |xj |until the total de-
crease reaches qA or all the xj are zero. See Fig-
ure 4.
If we pre-sort the |xj |in nonincreasing order,
it’s easy to see how to compute this: for p =
1, ... , n, see if there is a value ξ ≤ xρ such that
decreasing all the x1, ... , xρ to ξ amounts to a to-
tal decrease of qA. The largest p for which this is
possible gives the correct solution.
But this situation seems similar to another op-
timization problem, projection onto the E1-ball,
which Duchi et al. (2008) solve in linear time
without pre-sorting. In fact, the two problems can
be solved by nearly identical algorithms, because
they are convex conjugates of each other (Duchi
and Singer, 2009; Bach et al., 2012). Intuitively,
the E1 projection of v is exactly what is cut out
by the E∞ proximal operator, and vice versa (Fig-
ure 4).
Duchi et al.’s algorithm modified for the present
problem is shown as Algorithm 1. It partitions the
xj about a pivot element (line 6) and tests whether
it and the elements to its left can be decreased to a
value ξ such that the total decrease is δ (line 8). If
so, it recursively searches the right side; if not, the
</bodyText>
<equation confidence="0.9660332">
before E∞ prox. op. E1 projection
∂G
∂w =
w = α kvk
α = max(0, kvk − qA).
</equation>
<page confidence="0.974753">
911
</page>
<bodyText confidence="0.993492846153846">
left side. At the conclusion of the algorithm, ρ is
set to the largest value that passes the test (line 13),
and finally the new xj are computed (line 16) – the
only difference from Duchi et al.’s algorithm.
This algorithm is asymptotically faster than that
of Quattoni et al. (2009). They reformulate `∞,1
regularization as a constrained optimization prob-
lem (in which the `∞,1 norm is bounded by µ) and
provide a solution in O(n log n) time. The method
shown here is simpler and faster because it can
work on each row separately.
Algorithm 1 Linear-time algorithm for the proxi-
mal operator of the `∞ norm.
</bodyText>
<listItem confidence="0.983210666666667">
1: procedure UPDATE(w, δ)
2: lo, hi ← 1, n
3: s ← 0
4: while lo ≤ hi do
5: select md randomly from lo, ... , hi
6: ρ ← PARTITIO`N(w, lo, md, hi)
7: ξ ← 1 (( \S + EP lo |xi |− δ)
ρ
8: if ξ ≤ |xρ |then
s ← s + Eρ
9: i=lo |xi|
10: lo ← ρ + 1
11: else
12: hi← ρ − 1
13: ρ ← hi
14: ξ ← 1 ρ (s − δ)
15: for i ← 1,...,n do
16: xi ← min(max(xi, −ξ),ξ)
17: procedure PARTITION(w, lo, md, hi)
18: swap xlo and xmd
19: i ← lo + 1
20: for j ← lo + 1, ... , hi do
21: if xj ≥ xlo then
22: swap xi and xj
</listItem>
<sectionHeader confidence="0.999115" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999693428571429">
We evaluate our model using the open-source
NPLM toolkit released by Vaswani et al. (2013),
extending it to use the additional regularizers as
described in this paper.2 We use a vocabulary size
of 100k and word embeddings with 50 dimen-
sions. We use two hidden layers of rectified linear
units (Nair and Hinton, 2010).
</bodyText>
<footnote confidence="0.933363">
2These extensions have been contributed to the NPLM
project.
</footnote>
<bodyText confidence="0.9999872">
We train neural language models (LMs) on two
natural language corpora, Europarl v7 English and
the AFP portion of English Gigaword 5. After tok-
enization, Europarl has 56M tokens and Gigaword
AFP has 870M tokens. For both corpora, we hold
out a validation set of 5,000 tokens. We train each
model for 10 iterations over the training data.
Our experiments break down into three parts.
First, we look at the impact of our pruning method
on perplexity of a held-out validation set, across a
variety of settings. Second, we take a closer look
at how the model evolves through the training pro-
cess. Finally, we explore the downstream impact
of our method on a statistical phrase-based ma-
chine translation system.
</bodyText>
<subsectionHeader confidence="0.998603">
5.1 Evaluating perplexity and network size
</subsectionHeader>
<bodyText confidence="0.999991275862069">
We first look at the impact that the `∞,1 regular-
izer has on the perplexity of our validation set. The
main results are shown in Table 1. For λ ≤ 0.01,
the regularizer seems to have little impact: no hid-
den units are pruned, and perplexity is also not af-
fected. For λ = 1, on the other hand, most hidden
units are pruned – apparently too many, since per-
plexity is worse. But for λ = 0.1, we see that we
are able to prune out many hidden units: up to half
of the first layer, with little impact on perplexity.
We found this to be consistent across all our exper-
iments, varying n-gram size, initial hidden layer
size, and vocabulary size.
Table 2 shows the same information for 5-gram
models trained on the larger Gigaword AFP cor-
pus. These numbers look very similar to those on
Europarl: again λ = 0.1 works best, and, counter
to expectation, even the final number of units is
similar.
Table 3 shows the result of varying the vocabu-
lary size: again λ = 0.1 works best, and, although
it is not shown in the table, we also found that the
final number of units did not depend strongly on
the vocabulary size.
Table 4 shows results using the `2,1 norm (Eu-
roparl corpus, 5-grams, 100k vocabulary). Since
this is a different regularizer, there isn’t any rea-
son to expect that λ behaves the same way, and
indeed, a smaller value of λ seems to work best.
</bodyText>
<subsectionHeader confidence="0.999424">
5.2 A closer look at training
</subsectionHeader>
<bodyText confidence="0.998777666666667">
We also studied the evolution of the network over
the training process to gain some insights into how
the method works. The first question we want to
</bodyText>
<figure confidence="0.315572333333333">
i ← i + 1
24: swap xlo and xi−1
25: return i − 1
</figure>
<page confidence="0.60867">
912
</page>
<table confidence="0.996327142857143">
A layer 1 2-gram ppl layer 1 3-gram ppl layer 1 5-gram ppl
layer 2 layer 2 layer 2
0 1,000 50 103 1,000 50 66 1,000 50 55
0.001 1,000 50 104 1,000 50 66 1,000 50 54
0.01 1,000 50 104 1,000 50 63 1,000 50 55
0.1 499 47 105 652 49 66 784 50 55
1.0 50 24 111 128 32 76 144 29 68
</table>
<tableCaption confidence="0.82419725">
Table 1: Comparison of E∞,1 regularization on 2-gram, 3-gram, and 5-gram neural language models. The
network initially started with 1,000 units in the first hidden layer and 50 in the second. A regularization
strength of A = 0.1 consistently is able to prune units while maintaining perplexity, even though the final
number of units varies considerably across models. The vocabulary size is 100k.
</tableCaption>
<table confidence="0.987283">
A layer 1 layer 2 perplexity
0 1,000 50 100
0.001 1,000 50 99
0.01 1,000 50 101
0.1 742 50 107
1.0 24 17 173
</table>
<tableCaption confidence="0.993833">
Table 2: Results from training a 5-gram neural LM
</tableCaption>
<bodyText confidence="0.94223025">
on the AFP portion of the Gigaword dataset. As
with the smaller Europarl corpus (Table 1), a reg-
ularization strength of A = 0.1 is able to prune
units while maintaining perplexity.
</bodyText>
<table confidence="0.972219875">
vocabulary
size
A 10k 25k 50k 100k
0 47 60 54 55
0.001 47 54 54 54
0.01 47 58 55 55
0.1 48 62 55 55
1.0 61 64 65 68
</table>
<tableCaption confidence="0.905573">
Table 3: A regularization strength of A = 0.1 is
best across different vocabulary sizes.
</tableCaption>
<table confidence="0.999750833333333">
A layer 1 layer 2 perplexity
0 1,000 50 100
0.0001 1,000 50 54
0.001 1,000 50 55
0.01 616 50 57
0.1 199 32 65
</table>
<tableCaption confidence="0.999483">
Table 4: Results using E2,1 regularization.
</tableCaption>
<figure confidence="0.449205">
epoch
</figure>
<figureCaption confidence="0.841489">
Figure 5: Number of units in first hidden layer over
</figureCaption>
<bodyText confidence="0.978868363636364">
time, with various starting sizes (A = 0.1). If we
start with too many units, we end up with the same
number, although if we start with a smaller number
of units, a few are still pruned away.
answer is whether the method is simply remov-
ing units, or converging on an optimal number of
units. Figure 5 suggests that it is a little of both:
if we start with too many units (900 or 1000), the
method converges to the same number regardless
of how many extra units there were initially. But
if we start with a smaller number of units, the
method still prunes away about 50 units.
Next, we look at the behavior over time of dif-
ferent regularization strengths A. We found that
not only does A = 1 prune out too many units, it
does so at the very first iteration (Figure 6, above),
perhaps prematurely. By contrast, the A = 0.1
run prunes out units gradually. By plotting these
curves together with perplexity (Figure 6, below),
we can see that the A = 0.1 run is fitting the model
and pruning it at the same time, which seems
preferable to fitting without any pruning (A =
</bodyText>
<figure confidence="0.991919">
1000
900
800
700
0 2 4 6 8 10
1,000
500
0
nonzero units in hidden layer 1
</figure>
<page confidence="0.650458">
913
</page>
<figure confidence="0.99516875">
neural LM
A none Europarl Gigaword AFP
0 (none)
0.1
23.2 24.7 (+1.5) 25.2 (+2.0)
24.6 (+1.4) 24.9 (+1.7)
epoch
epoch
</figure>
<figureCaption confidence="0.858479">
Figure 6: Above: Number of units in first hid-
den layer over time, for various regularization
strengths A. A regularization strength of &lt; 0.01
does not zero out any rows, while a strength of 1
</figureCaption>
<bodyText confidence="0.996038117647059">
zeros out rows right away. Below: Perplexity over
time. The runs with A &lt; 0.1 have very similar
learning curves, whereas A = 1 is worse from the
beginning.
Table 5: The improvements in translation accuracy
due to the neural LM (shown in parentheses) are
affected only slightly by E∞,1 regularization. For
the Europarl LM, there is no statistically signifi-
cant difference, and for the Gigaword AFP LM, a
statistically significant but small decrease of −0.3.
0.01) or pruning first and then fitting (A = 1).
We can also visualize the weight matrix itself
over time (Figure 7), for A = 0.1. It is striking
that although this setting fits the model and prunes
it at the same time, as argued above, by the first
iteration it already seems to have decided roughly
how many units it will eventually prune.
</bodyText>
<subsectionHeader confidence="0.998771">
5.3 Evaluating on machine translation
</subsectionHeader>
<bodyText confidence="0.9997397">
We also looked at the impact of our method on
statistical machine translation systems. We used
the Moses toolkit (Koehn et al., 2007) to build a
phrase based machine translation system with a
traditional 5-gram LM trained on the target side
of our bitext. We augmented this system with neu-
ral LMs trained on the Europarl data and the Gi-
gaword AFP data. Based on the results from the
perplexity experiments, we looked at models both
built with a A = 0.1 regularizer, and without regu-
larization (A = 0).
We built our system using the newscommentary
dataset v8. We tuned our model using newstest13
and evaluated using newstest14. After standard
cleaning and tokenization, there were 155k paral-
lel sentences in the newscommentary dataset, and
3,000 sentences each for the tuning and test sets.
Table 5 shows that the addition of a neural
LM helps substantially over the baseline, with im-
provements of up to 2 BLEU. Using the Europarl
model, the BLEU scores obtained without and
with regularization were not significantly differ-
ent (p ≥ 0.05), consistent with the negligible per-
plexity difference between these models. On the
Gigaword AFP model, regularization did decrease
the BLEU score by 0.3, consistent with the small
perplexity increase of the regularized model. The
decrease is statistically significant, but small com-
pared with the overall benefit of adding a neu-
ral LM.
</bodyText>
<figure confidence="0.975131588235294">
0 2 4 6 8 10
A &lt; 0.01
A = 0.1
A = 1
1,000
500
0
nonzero units in hidden layer 1
0 2 4 6 8 10
perplexity 100
50
0
A = 0.01
A = 0.1
A = 1
914
1 iteration 5 iterations 10 iterations
</figure>
<figureCaption confidence="0.999769">
Figure 7: Evolution of the first hidden layer weight matrix after 1, 5, and 10 iterations (with rows sorted
</figureCaption>
<bodyText confidence="0.792264666666667">
by `∞ norm). A nonlinear color scale is used to show small values more clearly. The four vertical blocks
correspond to the four context words. The light bar at the bottom is the rows that are close to zero, and
the white bar is the rows that are exactly zero.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999984775862069">
Researchers have been exploring the use of neu-
ral networks for language modeling for a long
time. Schmidhuber and Heil (1996) proposed a
character n-gram model using neural networks
which they used for text compression. Xu and
Rudnicky (2000) proposed a word-based proba-
bility model using a softmax output layer trained
using cross-entropy, but only for bigrams. Bengio
et al. (2003) defined a probabilistic word n-gram
model and demonstrated improvements over con-
ventional smoothed language models. Mnih and
Teh (2012) sped up training of log-bilinear lan-
guage models through the use of noise-contrastive
estimation (NCE). Vaswani et al. (2013) also
used NCE to train the architecture of Bengio et
al. (2003), and were able to integrate a large-
vocabulary language model directly into a ma-
chine translation decoder. Baltescu et al. (2014)
describe a similar model, with extensions like a
hierarchical softmax (based on Brown clustering)
and direct n-gram features.
Beyond feed-forward neural network lan-
guage models, researchers have explored using
more complicated neural network architectures.
RNNLM is an open-source implementation of a
language model using recurrent neural networks
(RNN) where connections between units can form
directed cycles (Mikolov et al., 2011). Sunder-
meyer et al. (2015) use the long-short term mem-
ory (LSTM) neural architecture to show a per-
plexity improvement over the RNNLM toolkit.
In future work, we plan on exploring how our
method could improve these more complicated
neural models as well.
Automatically limiting the size of neural net-
works is an old idea. The “Optimal Brain Dam-
age” (OBD) technique (LeCun et al., 1989) com-
putes a saliency based on the second derivative of
the objective function with respect to each parame-
ter. The parameters are then sorted by saliency, and
the lowest-saliency parameters are pruned. The
pruning process is separate from the training pro-
cess, whereas regularization performs training and
pruning simultaneously. Regularization in neural
networks is also an old idea; for example, Now-
land and Hinton (1992) mention both `22 and `0
regularization. Our method develops on this idea
by using a mixed norm to prune units, rather than
parameters.
Srivastava et al. introduce a method called
dropout in which units are directly deactivated at
random during training (Srivastava et al., 2014),
which induces sparsity in the hidden unit activa-
tions. However, at the end of training, all units
are reactivated, as the goal of dropout is to re-
duce overfitting, not to reduce network size. Thus,
dropout and our method seem to be complemen-
tary.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999361">
We have presented a method for auto-sizing a neu-
ral network during training by removing units us-
ing a `∞,1 regularizer. This regularizer drives a
unit’s input weights as a group down to zero, al-
lowing the unit to be pruned. We can thus prune
units out of our network during training with min-
imal impact to held-out perplexity or downstream
performance of a machine translation system.
Our results showed empirically that the choice
</bodyText>
<page confidence="0.995544">
915
</page>
<bodyText confidence="0.999962736842105">
of a regularization coefficient of 0.1 was robust to
initial configuration parameters of initial network
size, vocabulary size, n-gram order, and training
corpus. Furthermore, imposing a single regularizer
on the objective function can tune all of the hidden
layers of a network with one setting. This reduces
the need to conduct expensive, multi-dimensional
grid searches in order to determine optimal sizes.
We have demonstrated the power and efficacy
of this method on a feed-forward neural network
for language modeling though experiments on per-
plexity and machine translation. However, this
method is general enough that it should be applica-
ble to other domains, both inside natural language
processing and outside. As neural models become
more pervasive in natural language processing, the
ability to auto-size networks for fast experimen-
tation and quick exploration will become increas-
ingly important.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999953">
We would like to thank Tomer Levinboim, Anto-
nios Anastasopoulos, and Ashish Vaswani for their
helpful discussions, as well as the reviewers for
their assistance and feedback.
</bodyText>
<sectionHeader confidence="0.999402" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999695368421053">
Francis Bach, Rodolphe Jenatton, Julien Mairal, and
Guillaume Obozinski. 2012. Optimization
with sparsity-inducing penalties. Foundations and
Trends in Machine Learning, 4(1):1–106.
Paul Baltescu, Phil Blunsom, and Hieu Hoang. 2014.
OxLM: A neural language modelling framework for
machine translation. Prague Bulletin of Mathemati-
cal Linguistics, 102(1):81–92.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic
language model. J. Machine Learning Research,
3:1137–1155.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proc. ACL, pages
1370–1380.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
J. Machine Learning Research, 10:2899–2934.
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and
Tushar Chandra. 2008. Efficient projections onto
the `1-ball for learning in high dimensions. In Proc.
ICML, pages 272–279.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proc. ACL, Interactive Poster and Demon-
stration Sessions, pages 177–180.
Yann LeCun, John S. Denker, Sara A. Solla, Richard E.
Howard, and Lawrence D. Jackel. 1989. Optimal
brain damage. In Proc. NIPS, volume 2, pages 598–
605.
Tomas Mikolov, Stefan Kombrink, Anoop Deoras,
Lukar Burget, and Jan Cernocky. 2011. RNNLM -
recurrent neural network language modeling toolkit.
In Proc. ASRU, pages 196–201.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proc. ICML, pages 1751–1758.
Vinod Nair and Geoffrey E Hinton. 2010. Recti-
fied linear units improve Restricted Boltzmann Ma-
chines. In Proc. ICML, pages 807–814.
Steven J. Nowland and Geoffrey E. Hinton. 1992.
Simplifying neural networks by soft weight-sharing.
Neural Computation, 4:473–493.
Neal Parikh and Stephen Boyd. 2014. Proximal al-
gorithms. Foundations and Trends in Optimization,
1(3):127–239.
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projection
for l1,∞ regularization. In Proc. ICML, pages 857–
864.
Jurgen Schmidhuber and Stefan Heil. 1996. Sequen-
tial neural text compression. IEEE Transactions on
Neural Networks, 7:142–146.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. J. Machine Learning Research,
15(1):1929–1958.
Martin Sundermeyer, Hermann Ney, and Ralf Schl¨uter.
2015. From feedforward to recurrent LSTM neu-
ral networks for language modeling. Trans. Audio,
Speech, and Language, 23(3):517–529.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
Proc. EMNLP, pages 1387–1392.
Wei Xu and Alexander I. Rudnicky. 2000. Can ar-
tificial neural networks learn language models? In
Proc. International Conference on Statistical Lan-
guage Processing, pages M1–13.
</reference>
<page confidence="0.998588">
916
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818320">
<title confidence="0.997993">Auto-Sizing Neural Networks: Applications to Language Models</title>
<author confidence="0.904116">Murray</author>
<affiliation confidence="0.991832">Department of Computer Science and University of Notre</affiliation>
<abstract confidence="0.99621">Neural networks have been shown to improve performance across a range of natural-language tasks. However, designing and training them can be complicated. Frequently, researchers resort to repeated experimentation to pick optimal settings. In this paper, we address the issue of choosing the correct number of units in hidden layers. We introduce a method for automatically adjusting network size by out hidden units through and We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bach</author>
<author>Rodolphe Jenatton</author>
<author>Julien Mairal</author>
<author>Guillaume Obozinski</author>
</authors>
<title>Optimization with sparsity-inducing penalties.</title>
<date>2012</date>
<booktitle>Foundations and Trends in Machine Learning,</booktitle>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="12912" citStr="Bach et al., 2012" startWordPosition="2318" endWordPosition="2321">4. If we pre-sort the |xj |in nonincreasing order, it’s easy to see how to compute this: for p = 1, ... , n, see if there is a value ξ ≤ xρ such that decreasing all the x1, ... , xρ to ξ amounts to a total decrease of qA. The largest p for which this is possible gives the correct solution. But this situation seems similar to another optimization problem, projection onto the E1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting. In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012). Intuitively, the E1 projection of v is exactly what is cut out by the E∞ proximal operator, and vice versa (Figure 4). Duchi et al.’s algorithm modified for the present problem is shown as Algorithm 1. It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8). If so, it recursively searches the right side; if not, the before E∞ prox. op. E1 projection ∂G ∂w = w = α kvk α = max(0, kvk − qA). 911 left side. At the conclusion of the algorithm, ρ is set to the largest value that p</context>
</contexts>
<marker>Bach, Jenatton, Mairal, Obozinski, 2012</marker>
<rawString>Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. 2012. Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, 4(1):1–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Baltescu</author>
<author>Phil Blunsom</author>
<author>Hieu Hoang</author>
</authors>
<title>OxLM: A neural language modelling framework for machine translation.</title>
<date>2014</date>
<journal>Prague Bulletin of Mathematical Linguistics,</journal>
<volume>102</volume>
<issue>1</issue>
<contexts>
<context position="23746" citStr="Baltescu et al. (2014)" startWordPosition="4357" endWordPosition="4360">on. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams. Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder. Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features. Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures. RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011). Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit. In future work, we plan</context>
</contexts>
<marker>Baltescu, Blunsom, Hoang, 2014</marker>
<rawString>Paul Baltescu, Phil Blunsom, and Hieu Hoang. 2014. OxLM: A neural language modelling framework for machine translation. Prague Bulletin of Mathematical Linguistics, 102(1):81–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2095" citStr="Bengio et al., 2003" startWordPosition="320" endWordPosition="323">n be laborious to find, often requiring an extensive grid search involving numerous experiments. In this paper, we focus on the choice of the sizes of hidden layers. We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that they can be removed from the network. Thus, after training with more units than necessary, a network is produced that has hidden layers correctly sized, saving both time and memory when actually putting the network to use. Using a neural n-gram language model (Bengio et al., 2003), we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity. The method has only a single hyperparameter to adjust (as opposed to adjusting the sizes of each of the hidden layers), and we find that the same setting works consistently well across different training data sizes, vocabulary sizes, and n-gram sizes. In addition, we show that incorporating these models into a machine translation decoder still results in large BLEU point improvements. The result is that fewer expe</context>
<context position="5881" citStr="Bengio et al., 2003" startWordPosition="947" endWordPosition="950">ingle network also require different numbers of units. Choosing too few units can impair the performance of the network, and choosing too many units can lead to overfitting. It can also slow down computations with the network, which can be a major concern for many applications such as integrating neural language models into a machine translation decoder. Our method starts out with a large number of units in each layer and then jointly trains the network while pruning out individual units when possible. The goal is to end up with a trained network Figure 1: Neural probabilistic language model (Bengio et al., 2003), adapted from Vaswani et al. (2013). that also has the optimal number of units in each layer. We do this by adding a regularizer to the objective function. For simplicity, consider a single layer without bias, y = f(Wx). Let L(W) be the negative log-likelihood of the model. Instead of minimizing L(W) alone, we want to minimize L(W) + AR(W), where R(W) is a convex regularizer. The E1 norm, R(W) = IIWII1 = Ei,j |Wij|, is a common choice for pushing parameters to zero, which can be useful for preventing overfitting and reducing model size. However, we are interested not only in reducing the numb</context>
<context position="23291" citStr="Bengio et al. (2003)" startWordPosition="4286" endWordPosition="4289">s used to show small values more clearly. The four vertical blocks correspond to the four context words. The light bar at the bottom is the rows that are close to zero, and the white bar is the rows that are exactly zero. 6 Related Work Researchers have been exploring the use of neural networks for language modeling for a long time. Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams. Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder. Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features. Beyond feed-forwar</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1370--1380</pages>
<contexts>
<context position="1258" citStr="Devlin et al., 2014" startWordPosition="181" endWordPosition="184">ough E∞,1 and E2,1 regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions. 1 Introduction Neural networks have proven to be highly effective at many tasks in natural language. For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014). However, neural networks can be complicated to design and train well. Many decisions need to be made, and performance can be highly dependent on making them correctly. Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments. In this paper, we focus on the choice of the sizes of hidden layers. We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that they can be removed from the network. Thus, </context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proc. ACL, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Yoram Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>J. Machine Learning Research,</journal>
<pages>10--2899</pages>
<contexts>
<context position="8562" citStr="Duchi and Singer, 2009" startWordPosition="1442" endWordPosition="1445">ssure on each row, but within each row, only the maximum value (or values) matter, and therefore the pressure towards zero is entirely on the maximum value(s). Figure 2 visualizes the sparsity-inducing behavior of the two regularizers on a single row. Both have a sharp tip at the origin that encourages all the parameters in a row to become exactly zero. 4 Optimization However, this also means that sparsity-inducing regularizers are not differentiable at zero, making gradient-based optimization methods trickier to apply. The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness. 4.1 Proximal gradient method Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014). Our objective function can be split into two parts, a convex and differentiable part (L) and a convex but non-differentiable part (λR). In proximal gradient descent, we alternate between improving L alone and λR alone. Let u be the parameter values from the previous iteration. We compute new parameter values w using: v ← u − η∇L(u) (3</context>
<context position="12892" citStr="Duchi and Singer, 2009" startWordPosition="2314" endWordPosition="2317">xj are zero. See Figure 4. If we pre-sort the |xj |in nonincreasing order, it’s easy to see how to compute this: for p = 1, ... , n, see if there is a value ξ ≤ xρ such that decreasing all the x1, ... , xρ to ξ amounts to a total decrease of qA. The largest p for which this is possible gives the correct solution. But this situation seems similar to another optimization problem, projection onto the E1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting. In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012). Intuitively, the E1 projection of v is exactly what is cut out by the E∞ proximal operator, and vice versa (Figure 4). Duchi et al.’s algorithm modified for the present problem is shown as Algorithm 1. It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8). If so, it recursively searches the right side; if not, the before E∞ prox. op. E1 projection ∂G ∂w = w = α kvk α = max(0, kvk − qA). 911 left side. At the conclusion of the algorithm, ρ is set to the </context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. J. Machine Learning Research, 10:2899–2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
<author>Tushar Chandra</author>
</authors>
<title>Efficient projections onto the `1-ball for learning in high dimensions.</title>
<date>2008</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>272--279</pages>
<contexts>
<context position="8537" citStr="Duchi et al., 2008" startWordPosition="1438" endWordPosition="1441"> this puts equal pressure on each row, but within each row, only the maximum value (or values) matter, and therefore the pressure towards zero is entirely on the maximum value(s). Figure 2 visualizes the sparsity-inducing behavior of the two regularizers on a single row. Both have a sharp tip at the origin that encourages all the parameters in a row to become exactly zero. 4 Optimization However, this also means that sparsity-inducing regularizers are not differentiable at zero, making gradient-based optimization methods trickier to apply. The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness. 4.1 Proximal gradient method Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014). Our objective function can be split into two parts, a convex and differentiable part (L) and a convex but non-differentiable part (λR). In proximal gradient descent, we alternate between improving L alone and λR alone. Let u be the parameter values from the previous iteration. We compute new parameter values w</context>
<context position="12705" citStr="Duchi et al. (2008)" startWordPosition="2284" endWordPosition="2287">h component by an equal amount until they sum to qA. Intuitively, the solution can be characterized as: Decrease all of the maximal |xj |until the total decrease reaches qA or all the xj are zero. See Figure 4. If we pre-sort the |xj |in nonincreasing order, it’s easy to see how to compute this: for p = 1, ... , n, see if there is a value ξ ≤ xρ such that decreasing all the x1, ... , xρ to ξ amounts to a total decrease of qA. The largest p for which this is possible gives the correct solution. But this situation seems similar to another optimization problem, projection onto the E1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting. In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012). Intuitively, the E1 projection of v is exactly what is cut out by the E∞ proximal operator, and vice versa (Figure 4). Duchi et al.’s algorithm modified for the present problem is shown as Algorithm 1. It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8). If so, it rec</context>
</contexts>
<marker>Duchi, Shalev-Shwartz, Singer, Chandra, 2008</marker>
<rawString>John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. 2008. Efficient projections onto the `1-ball for learning in high dimensions. In Proc. ICML, pages 272–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL, Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="21105" citStr="Koehn et al., 2007" startWordPosition="3897" endWordPosition="3900">nificant difference, and for the Gigaword AFP LM, a statistically significant but small decrease of −0.3. 0.01) or pruning first and then fitting (A = 1). We can also visualize the weight matrix itself over time (Figure 7), for A = 0.1. It is striking that although this setting fits the model and prunes it at the same time, as argued above, by the first iteration it already seems to have decided roughly how many units it will eventually prune. 5.3 Evaluating on machine translation We also looked at the impact of our method on statistical machine translation systems. We used the Moses toolkit (Koehn et al., 2007) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext. We augmented this system with neural LMs trained on the Europarl data and the Gigaword AFP data. Based on the results from the perplexity experiments, we looked at models both built with a A = 0.1 regularizer, and without regularization (A = 0). We built our system using the newscommentary dataset v8. We tuned our model using newstest13 and evaluated using newstest14. After standard cleaning and tokenization, there were 155k parallel sentences in the newscommentary dataset</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL, Interactive Poster and Demonstration Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>John S Denker</author>
<author>Sara A Solla</author>
<author>Richard E Howard</author>
<author>Lawrence D Jackel</author>
</authors>
<title>Optimal brain damage.</title>
<date>1989</date>
<booktitle>In Proc. NIPS,</booktitle>
<volume>2</volume>
<pages>598--605</pages>
<contexts>
<context position="24565" citStr="LeCun et al., 1989" startWordPosition="4483" endWordPosition="4486">ored using more complicated neural network architectures. RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011). Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit. In future work, we plan on exploring how our method could improve these more complicated neural models as well. Automatically limiting the size of neural networks is an old idea. The “Optimal Brain Damage” (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter. The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned. The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously. Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both `22 and `0 regularization. Our method develops on this idea by using a mixed norm to prune units, rather than parameters. Srivastava et al. introduce a method calle</context>
</contexts>
<marker>LeCun, Denker, Solla, Howard, Jackel, 1989</marker>
<rawString>Yann LeCun, John S. Denker, Sara A. Solla, Richard E. Howard, and Lawrence D. Jackel. 1989. Optimal brain damage. In Proc. NIPS, volume 2, pages 598– 605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Anoop Deoras</author>
<author>Lukar Burget</author>
<author>Jan Cernocky</author>
</authors>
<title>RNNLM -recurrent neural network language modeling toolkit.</title>
<date>2011</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>196--201</pages>
<contexts>
<context position="24180" citStr="Mikolov et al., 2011" startWordPosition="4417" endWordPosition="4420">used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder. Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features. Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures. RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011). Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit. In future work, we plan on exploring how our method could improve these more complicated neural models as well. Automatically limiting the size of neural networks is an old idea. The “Optimal Brain Damage” (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter. The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned. The pruning pr</context>
</contexts>
<marker>Mikolov, Kombrink, Deoras, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and Jan Cernocky. 2011. RNNLM -recurrent neural network language modeling toolkit. In Proc. ASRU, pages 196–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>1751--1758</pages>
<contexts>
<context position="23427" citStr="Mnih and Teh (2012)" startWordPosition="4305" endWordPosition="4308">he rows that are close to zero, and the white bar is the rows that are exactly zero. 6 Related Work Researchers have been exploring the use of neural networks for language modeling for a long time. Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams. Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder. Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features. Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures. RNNLM is an open-source</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proc. ICML, pages 1751–1758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve Restricted Boltzmann Machines.</title>
<date>2010</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="14858" citStr="Nair and Hinton, 2010" startWordPosition="2724" endWordPosition="2727"> s ← s + Eρ 9: i=lo |xi| 10: lo ← ρ + 1 11: else 12: hi← ρ − 1 13: ρ ← hi 14: ξ ← 1 ρ (s − δ) 15: for i ← 1,...,n do 16: xi ← min(max(xi, −ξ),ξ) 17: procedure PARTITION(w, lo, md, hi) 18: swap xlo and xmd 19: i ← lo + 1 20: for j ← lo + 1, ... , hi do 21: if xj ≥ xlo then 22: swap xi and xj 5 Experiments We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.2 We use a vocabulary size of 100k and word embeddings with 50 dimensions. We use two hidden layers of rectified linear units (Nair and Hinton, 2010). 2These extensions have been contributed to the NPLM project. We train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5. After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens. For both corpora, we hold out a validation set of 5,000 tokens. We train each model for 10 iterations over the training data. Our experiments break down into three parts. First, we look at the impact of our pruning method on perplexity of a held-out validation set, across a variety of settings. Second, we take a closer look</context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve Restricted Boltzmann Machines. In Proc. ICML, pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J Nowland</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Simplifying neural networks by soft weight-sharing.</title>
<date>1992</date>
<journal>Neural Computation,</journal>
<pages>4--473</pages>
<contexts>
<context position="24987" citStr="Nowland and Hinton (1992)" startWordPosition="4547" endWordPosition="4551">w our method could improve these more complicated neural models as well. Automatically limiting the size of neural networks is an old idea. The “Optimal Brain Damage” (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter. The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned. The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously. Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both `22 and `0 regularization. Our method develops on this idea by using a mixed norm to prune units, rather than parameters. Srivastava et al. introduce a method called dropout in which units are directly deactivated at random during training (Srivastava et al., 2014), which induces sparsity in the hidden unit activations. However, at the end of training, all units are reactivated, as the goal of dropout is to reduce overfitting, not to reduce network size. Thus, dropout and our method seem to be complementary. 7 Conclusion We have presented a method for auto-sizing a neural network</context>
</contexts>
<marker>Nowland, Hinton, 1992</marker>
<rawString>Steven J. Nowland and Geoffrey E. Hinton. 1992. Simplifying neural networks by soft weight-sharing. Neural Computation, 4:473–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neal Parikh</author>
<author>Stephen Boyd</author>
</authors>
<date>2014</date>
<booktitle>Proximal algorithms. Foundations and Trends in Optimization,</booktitle>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="8824" citStr="Parikh and Boyd, 2014" startWordPosition="1484" endWordPosition="1487"> sharp tip at the origin that encourages all the parameters in a row to become exactly zero. 4 Optimization However, this also means that sparsity-inducing regularizers are not differentiable at zero, making gradient-based optimization methods trickier to apply. The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness. 4.1 Proximal gradient method Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014). Our objective function can be split into two parts, a convex and differentiable part (L) and a convex but non-differentiable part (λR). In proximal gradient descent, we alternate between improving L alone and λR alone. Let u be the parameter values from the previous iteration. We compute new parameter values w using: v ← u − η∇L(u) (3) � 1 � 2η kw − vk2 + λR(w) (4) and repeat until convergence. The first update is just a standard gradient descent update on L; the second is known as the proximal operator for λR and in many cases has a closed-form solution. In the rest of this section, we prov</context>
</contexts>
<marker>Parikh, Boyd, 2014</marker>
<rawString>Neal Parikh and Stephen Boyd. 2014. Proximal algorithms. Foundations and Trends in Optimization, 1(3):127–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Trevor Darrell</author>
</authors>
<title>An efficient projection for l1,∞ regularization.</title>
<date>2009</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>857--864</pages>
<contexts>
<context position="13712" citStr="Quattoni et al. (2009)" startWordPosition="2473" endWordPosition="2476"> is shown as Algorithm 1. It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8). If so, it recursively searches the right side; if not, the before E∞ prox. op. E1 projection ∂G ∂w = w = α kvk α = max(0, kvk − qA). 911 left side. At the conclusion of the algorithm, ρ is set to the largest value that passes the test (line 13), and finally the new xj are computed (line 16) – the only difference from Duchi et al.’s algorithm. This algorithm is asymptotically faster than that of Quattoni et al. (2009). They reformulate `∞,1 regularization as a constrained optimization problem (in which the `∞,1 norm is bounded by µ) and provide a solution in O(n log n) time. The method shown here is simpler and faster because it can work on each row separately. Algorithm 1 Linear-time algorithm for the proximal operator of the `∞ norm. 1: procedure UPDATE(w, δ) 2: lo, hi ← 1, n 3: s ← 0 4: while lo ≤ hi do 5: select md randomly from lo, ... , hi 6: ρ ← PARTITIO`N(w, lo, md, hi) 7: ξ ← 1 (( \S + EP lo |xi |− δ) ρ 8: if ξ ≤ |xρ |then s ← s + Eρ 9: i=lo |xi| 10: lo ← ρ + 1 11: else 12: hi← ρ − 1 13: ρ ← hi 14</context>
</contexts>
<marker>Quattoni, Carreras, Collins, Darrell, 2009</marker>
<rawString>Ariadna Quattoni, Xavier Carreras, Michael Collins, and Trevor Darrell. 2009. An efficient projection for l1,∞ regularization. In Proc. ICML, pages 857– 864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Schmidhuber</author>
<author>Stefan Heil</author>
</authors>
<title>Sequential neural text compression.</title>
<date>1996</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<pages>7--142</pages>
<contexts>
<context position="23033" citStr="Schmidhuber and Heil (1996)" startWordPosition="4246" endWordPosition="4249">s in hidden layer 1 0 2 4 6 8 10 perplexity 100 50 0 A = 0.01 A = 0.1 A = 1 914 1 iteration 5 iterations 10 iterations Figure 7: Evolution of the first hidden layer weight matrix after 1, 5, and 10 iterations (with rows sorted by `∞ norm). A nonlinear color scale is used to show small values more clearly. The four vertical blocks correspond to the four context words. The light bar at the bottom is the rows that are close to zero, and the white bar is the rows that are exactly zero. 6 Related Work Researchers have been exploring the use of neural networks for language modeling for a long time. Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams. Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able </context>
</contexts>
<marker>Schmidhuber, Heil, 1996</marker>
<rawString>Jurgen Schmidhuber and Stefan Heil. 1996. Sequential neural text compression. IEEE Transactions on Neural Networks, 7:142–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>J. Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. J. Machine Learning Research, 15(1):1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Hermann Ney</author>
<author>Ralf Schl¨uter</author>
</authors>
<title>From feedforward to recurrent LSTM neural networks for language modeling.</title>
<date>2015</date>
<journal>Trans. Audio, Speech, and Language,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Sundermeyer, Ney, Schl¨uter, 2015</marker>
<rawString>Martin Sundermeyer, Hermann Ney, and Ralf Schl¨uter. 2015. From feedforward to recurrent LSTM neural networks for language modeling. Trans. Audio, Speech, and Language, 23(3):517–529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with large-scale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1387--1392</pages>
<contexts>
<context position="1236" citStr="Vaswani et al., 2013" startWordPosition="177" endWordPosition="180">g out hidden units through E∞,1 and E2,1 regularization. We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity. We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions. 1 Introduction Neural networks have proven to be highly effective at many tasks in natural language. For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014). However, neural networks can be complicated to design and train well. Many decisions need to be made, and performance can be highly dependent on making them correctly. Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments. In this paper, we focus on the choice of the sizes of hidden layers. We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that they can be removed fr</context>
<context position="4862" citStr="Vaswani et al. (2013)" startWordPosition="773" endWordPosition="776"> two hidden layers, y = f(B1x1 + B2x2 + b) z = f(Cy + c) where f is an elementwise nonlinear activation (or transfer) function. Commonly used activation functions are the hyperbolic tangent, logistic function, and rectified linear units, to name a few. Finally, the result is mapped via a softmax to an output probability distribution, P(wn |w1 ··· wn−1) a exp([Dz + d]wn). The parameters of the model are A, B1, B2, b, C, c, D, and d, which are learned by minimizing the negative log-likelihood of the the training data using stochastic gradient descent (also known as backpropagation) or variants. Vaswani et al. (2013) showed that this model, with some improvements, can be used effectively during decoding in machine translation. In this paper, we use and extend their implementation. 3 Methods Our method is focused on the challenge of choosing the number of units in the hidden layers of a feed-forward neural network. The networks used for different tasks require different numbers of units, and the layers in a single network also require different numbers of units. Choosing too few units can impair the performance of the network, and choosing too many units can lead to overfitting. It can also slow down compu</context>
<context position="14632" citStr="Vaswani et al. (2013)" startWordPosition="2685" endWordPosition="2688"> operator of the `∞ norm. 1: procedure UPDATE(w, δ) 2: lo, hi ← 1, n 3: s ← 0 4: while lo ≤ hi do 5: select md randomly from lo, ... , hi 6: ρ ← PARTITIO`N(w, lo, md, hi) 7: ξ ← 1 (( \S + EP lo |xi |− δ) ρ 8: if ξ ≤ |xρ |then s ← s + Eρ 9: i=lo |xi| 10: lo ← ρ + 1 11: else 12: hi← ρ − 1 13: ρ ← hi 14: ξ ← 1 ρ (s − δ) 15: for i ← 1,...,n do 16: xi ← min(max(xi, −ξ),ξ) 17: procedure PARTITION(w, lo, md, hi) 18: swap xlo and xmd 19: i ← lo + 1 20: for j ← lo + 1, ... , hi do 21: if xj ≥ xlo then 22: swap xi and xj 5 Experiments We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.2 We use a vocabulary size of 100k and word embeddings with 50 dimensions. We use two hidden layers of rectified linear units (Nair and Hinton, 2010). 2These extensions have been contributed to the NPLM project. We train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5. After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens. For both corpora, we hold out a validation set of 5,000 tokens. We train each model for 10 iteratio</context>
<context position="23553" citStr="Vaswani et al. (2013)" startWordPosition="4324" endWordPosition="4327">ploring the use of neural networks for language modeling for a long time. Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams. Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder. Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features. Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures. RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cy</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. In Proc. EMNLP, pages 1387–1392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Can artificial neural networks learn language models?</title>
<date>2000</date>
<booktitle>In Proc. International Conference on Statistical Language Processing,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="23150" citStr="Xu and Rudnicky (2000)" startWordPosition="4264" endWordPosition="4267">ure 7: Evolution of the first hidden layer weight matrix after 1, 5, and 10 iterations (with rows sorted by `∞ norm). A nonlinear color scale is used to show small values more clearly. The four vertical blocks correspond to the four context words. The light bar at the bottom is the rows that are close to zero, and the white bar is the rows that are exactly zero. 6 Related Work Researchers have been exploring the use of neural networks for language modeling for a long time. Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression. Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams. Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models. Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE). Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder. Baltescu et al. (2014) des</context>
</contexts>
<marker>Xu, Rudnicky, 2000</marker>
<rawString>Wei Xu and Alexander I. Rudnicky. 2000. Can artificial neural networks learn language models? In Proc. International Conference on Statistical Language Processing, pages M1–13.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
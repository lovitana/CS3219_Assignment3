<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.7610036">
Keyboard Logs as Natural Annotations for Word Segmentation
Fumihiko Takahashi*
Yahoo Japan Corporation
Midtown Tower, 9-7-1 Akasaka, Minato-ku,
Tokyo, Japan
</note>
<email confidence="0.601472">
ftakahas@yahoo-corp.jp
</email>
<author confidence="0.763863">
Shinsuke Mori
</author>
<affiliation confidence="0.799897">
ACCMS, Kyoto University
</affiliation>
<address confidence="0.520717">
Yoshida Honmachi, Sakyo-ku,
Kyoto, Japan
</address>
<email confidence="0.99611">
forest@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.994796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999217210526316">
In this paper we propose a framework to
improve word segmentation accuracy us-
ing input method logs. An input method
is software used to type sentences in lan-
guages which have far more characters
than the number of keys on a keyboard.
The main contributions of this paper are:
1) an input method server that proposes
word candidates which are not included in
the vocabulary, 2) a publicly usable input
method that logs user behavior (like typ-
ing and selection of word candidates), and
3) a method for improving word segmen-
tation by using these logs. We conducted
word segmentation experiments on tweets
from Twitter, and showed that our method
improves accuracy in this domain. Our
method itself is domain-independent and
only needs logs from the target domain.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999259551724138">
The first step of almost all natural language
processing (NLP) for languages with ambiguous
word boundaries (such as Japanese and Chinese)
is solving the problem of word identification am-
biguity. This task is called word segmentation
(WS) and the accuracy of state-of-the-art methods
based on machine learning techniques is more than
98% for Japanese and 95% for Chinese (Neubig
et al., 2011; Yang and Vozila, 2014). Compared
to languages like English with clear word bound-
aries, this ambiguity poses an additional problem
for NLP tasks in these languages. To make matters
worse, the domains of the available training data
often differ from domains where there is a high
demand for NLP, which causes a severe degrada-
tion in WS performance. Examples include ma-
&apos;This work was done when the first author was at Kyoto
University.
chine translation of patents, text mining of med-
ical texts, and marketing on the micro-blog site
Twitter&apos;. Some papers have reported low accuracy
on WS or the joint task of WS and part-of-speech
(POS) tagging of Japanese or Chinese in these do-
mains (Mori and Neubig, 2014; Kaji and Kitsure-
gawa, 2014; Liu et al., 2014)
To cope with this problem, we propose a way
to collect information from people as they type
Japanese or Chinese on computers. These lan-
guages use far more characters than the number of
keys on a keyboard, so users use software called an
input method (IM) to type text in these languages.
Unlike written texts in these languages, which lack
word boundary information, text entered with an
IM can provide word boundary information that
can used by NLP systems. As we show in this pa-
per, logs collected from IMs are a valuable source
of word boundary information.
An IM consists of a client (front-end) and a
server (back-end). The client receives a key se-
quence typed by the user and sends a phoneme
sequence (kana in Japanese or pinyin in Chinese)
or some predefined commands to the server. The
server converts the phoneme sequence into normal
written text as a word sequence or proposes word
candidates for the phoneme sequence in the region
specified by the user. We noticed that the actions
performed by people using the IM (such as typ-
ing and selecting word candidates) provide infor-
mation about word boundaries, including context
information.
In this paper, we first describe an IM for
Japanese which allows us to collect this informa-
tion. We then propose an automatic word seg-
menter that uses IM logs as a language resource to
improve its performance. Finally, we report exper-
imental results showing that our method increases
the accuracy of a word segmenter on Twitter texts
by using logs collected from a browser add-on ver-
</bodyText>
<footnote confidence="0.640136">
lhttps://twitter.com/ (accessed in 2015 May).
</footnote>
<page confidence="0.901828">
1186
</page>
<note confidence="0.9854185">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1186–1196,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9841495">
sion of our IM.
The three main contributions of this paper are:
</bodyText>
<listItem confidence="0.987789125">
• an IM server that proposes word candidates
which are not included in the vocabulary
(Section 3),
• a publicly usable IM that logs user behavior
(such as typing and selection of word candi-
dates) (Section 4),
• a method for improving word segmentation
by using these logs (Section 5).
</listItem>
<bodyText confidence="0.996333333333333">
To the best of our knowledge, this is the first paper
proposing a method for using IM logs to success-
fully improve WS.
</bodyText>
<sectionHeader confidence="0.999831" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999966892857143">
The main focus of this paper is WS. Corpus-based,
or empirical, methods were proposed in the early
90’s (Nagata, 1994). Then (Mori and Kurata,
2005) extended it by lexicalizing the states like
many researches in that era, grouping the word-
POS pairs into clusters inspired by the class-based
n-gram model (Brown et al., 1992), and making
the history length variable like a POS tagger in
English (Ron et al., 1996). In parallel, there were
attempts at solving Chinese WS in a similar way
(Sproat and Chang, 1996). WS or the joint task of
WS and POS tagging can be seen as a sequence
labeling problem. So conditional random fields
(CRFs) (Peng et al., 2004; Lafferty et al., 2001)
have been applied to this task and showed bet-
ter performance than POS-based Markov models
(Kudo et al., 2004). The training time of sequence-
based methods tends to be long, especially when
we use partially annotated data. Thus a simple
method based on pointwise classification has been
shown to be as accurate as sequence-based meth-
ods and fast enough to make active learning prac-
tically possible (Neubig et al., 2011). Since the
pointwise method decides whether there is a word
boundary or not between two characters without
referring to other word boundary decisions in the
same sentence, it is straightforward to train the
model from partially annotated sentences. We
adopt this WS system for our experiments.
Along with the evolution of models, the NLP
community has become increasingly aware of the
importance of language resources (Neubig and
Mori, 2010; Mori and Neubig, 2014). So Mori
and Oda (2009) proposed to incorpolate dictio-
naries for human into a WS system with a differ-
ent word definition. CRFs were also extended to
enable training from partially annotated sentences
(Tsuboi et al., 2008). When using partially anno-
tated sentences for WS training data, word bound-
ary information exists only between some charac-
ter pairs and is absent for others. This extension
was adopted in Chinese WS to make use of so-
called natural annotations (Yang and Vozila, 2014;
Jiang et al., 2013). In that work, tags in hyper-texts
were regarded as annotations and used to improve
WS performance. The IM logs used in this paper
are also classified as natural annotations, but con-
tain much more noise. In addition, we need an IM
that is specifically designed to collect logs as nat-
ural annotations.
Server design is the most important factor in
capturing information from IM logs. The most
popular IM servers are based on statistical lan-
guage modeling (Mori et al., 1999; Chen and
Lee, 2000; Maeta and Mori, 2012). Their param-
eters are trained from manually segmented sen-
tences whose words are annotated with phoneme
sequences, and from sentences automatically an-
notated with NLP tools which are also based on
machine learning models trained on the annotated
sentences. Thus normal IM servers are not capa-
ble of presenting out-of-vocabulary (OOV) words
(which provide large amounts of information on
word boundaries) as conversion candidates. To
make our IM server capable of presenting OOV
words, we extend a statistical IM server based on
(Mori et al., 2006), and ensure that it is compu-
tationally efficient enough for practical use by the
public.
The target domain in our experiments is Twit-
ter, a site where users post short messages called
tweets. Since tweets are an immediate and power-
ful reflection of public attitudes and social trends,
there have been numerous attempts at extracting
information from them. Examples include infor-
mation analysis of disasters (Sakai et al., 2010),
estimation of depressive tendencies (Tsugawa et
al., 2013), speech diarization (Higashinaka et al.,
2011), and many others. These works require pre-
processing of tweets with NLP tools, and WS is
the first step. So it is clear that there is strong de-
mand for improving WS accuracy. Another reason
why we have chosen Twitter for the test domain is
that the tweets typed using our server are open and
</bodyText>
<page confidence="0.989288">
1187
</page>
<bodyText confidence="0.740385333333333">
we can avoid privacy problems. Our method does
not utilize any other characteristics of tweets. So
it also works in other domains such as blogs.
</bodyText>
<sectionHeader confidence="0.97284" genericHeader="method">
3 Input Method Suggesting OOV Words
</sectionHeader>
<bodyText confidence="0.999874666666667">
In this section we propose a practical statistical IM
server that suggests OOV word candidates in ad-
dition to words in its vocabulary.
</bodyText>
<subsectionHeader confidence="0.999012">
3.1 Statistical Input Method
</subsectionHeader>
<bodyText confidence="0.9933485">
An input method (IM) is software which converts
a phoneme sequence into a word sequence. This is
useful for languages which contain far more char-
acters than keys on a keyboard. Since there are
some ambiguities in conversion, a conversion en-
gine based on a word n-gram model has been pro-
posed (Chen and Lee, 2000). Today, almost all IM
engines are based on statistical methods.
For the LM unit, instead of words we propose to
adopt word-pronunciation pairs u = (y, w). Thus
given a phoneme sequence yl 1 = y1y2 · · · yl as
the input, the goal of our IM engine is to output
a word sequence �wm1 that maximizes the probabil-
ity P(w, yl1) as follows:
</bodyText>
<equation confidence="0.98348825">
wm1 = argmax
w
P(ui|ui−1
i−n+1),
</equation>
<bodyText confidence="0.999834111111111">
where the concatenation of yi in each ui is equal to
the input: yl1 = y1y2 · · · ym. In addition uj (j &lt;
0) are special symbols introduced to simplify the
notation and um+1 is a special symbol indicating
a sentence boundary.
As in existing statistical IM engines, parame-
ters are estimated from a corpus whose sentences
are segmented into words annotated with their pro-
nunciations as follows:
</bodyText>
<equation confidence="0.99970275">
F (ui i−n+1)
P(ui|ui−1
i−n+1) = i−1 , (1)
F (ui−n+1 )
</equation>
<bodyText confidence="0.999988166666667">
where F(·) denotes the frequency of a pair se-
quence in the corpus. In contrast to IM engines
based on a word n-gram model, ours does not re-
quire an additional model describing relationships
between words and pronunciations, and thus it is
much simpler and more practical.
Existing statistical IM engines only need an ac-
curate automatic word segmenter to estimate the
parameters of the word n-gram model. As the
equation above shows, our pair-based engine also
needs an accurate way of automatically estimat-
ing pronunciation (phoneme sequences). How-
ever, recently an automatic pronunciation estima-
tor (Mori and Neubig, 2011) that delivers as accu-
rate as state-of-the-art word segmenters has been
proposed. As we explain in Section 6, in our ex-
periments both our IM engine and existing ones
delivered accuracy of 91%.
</bodyText>
<subsectionHeader confidence="0.9922855">
3.2 Enumerating Substrings as Candidate
Words
</subsectionHeader>
<bodyText confidence="0.999936583333333">
Essentially, the IM engine which we have ex-
plained above does not have the ability to enumer-
ate words which are unknown to the word seg-
menter and the pronunciation estimator used to
build the training data. The aim of our research is
to gather language information from user behav-
ior as they use an IM. So we extend the basic IM
engine to enumerate all the substrings in a corpus
with all possible pronunciations. For that purpose,
we adopt the notion of a stochastically segmented
corpus (SSC) (Mori and Takuma, 2004) and ex-
tend it to the pronunciation annotation to words.
</bodyText>
<subsectionHeader confidence="0.947921">
3.2.1 Stochastically Segmented Corpora
</subsectionHeader>
<bodyText confidence="0.999071136363636">
An SSC is defined as a combination of a raw cor-
pus Cr (hereafter referred to as the character se-
quence xnr
1 ) and word boundary probabilities of
the form Pi, which is the probability that a word
boundary exists between two characters xi and
xi+1. These probabilities are estimated by a model
based on logistic regression (LR)(Fan et al., 2008)
trained on a manually segmented corpus referring
to the same features as those used in (Neubig et
al., 2011). Since there are word boundaries be-
fore the first character and after the last character
of the corpus, P0 = Pnr = 1. Then word n-gram
frequencies on an SSC are calculated as follows:
Word 0-gram frequency: This is defined as the
expected number of words in the SSC:
Word n-gram frequency (n &gt; 1): Consider the
situation in which a word sequence wn1 occurs
in the SSC as a subsequence beginning at the
(i + 1)-th character and ending at the k-th char-
acter and each word wm in the word sequence is
equal to the character sequence beginning at the
</bodyText>
<equation confidence="0.990623">
P(w, yl1),
m+1
P(w, yl1) =
i=1
A·) = 1 + nr−1� Pi.
i=1
1188
xi xb1 xe1
xb2 xe2
xbn xbn+1 xen xk+1
� Y �
w1
� Y �
w2
� Y �
wn
fr(wn1 ) = Pi(1 − Pb1)Pe1(1 − Pb2)Pe2 ··· (1 − Pbn)(1 − Pbn+1)Pen
</equation>
<figureCaption confidence="0.989315">
Figure 1: Word n-gram frequency in a stochastically segmented corpus.
</figureCaption>
<bodyText confidence="0.944379">
bm-th character and ending at the em-th charac-
</bodyText>
<equation confidence="0.810159">
ter (xem
bm = wm, 1 &lt; bm &lt; n; em + 1 =
</equation>
<bodyText confidence="0.986922857142857">
bm+1, 1 &lt; bm &lt; n − 1; b1 = i + 1; en = k)
(See Figure 1 for an example). The word n-
gram frequency of a word sequence fr(wn1) in
the SSC is defined by the summation of the
stochastic frequency at each occurrence of the
character sequence of the word sequence wn1
over all of the occurrences in the SSC:
</bodyText>
<equation confidence="0.960571571428572">

∑ ∏n
fr(wn 1) = Pi 
(i,en1 )EOn m=1
where en1 = (e1, e2, , en) and On =
{(i, eem
n1 bm = wm,1 &lt; m &lt; n}.
</equation>
<bodyText confidence="0.999871">
We calculate word n-gram probabilities by divid-
ing word n-gram frequencies by word (n − 1)-
gram frequencies. For a detailed explanation and
a mathematical proof of this method, please refer
to (Mori and Takuma, 2004).
</bodyText>
<subsectionHeader confidence="0.8611725">
3.2.2 Pseudo-Stochastically Segmented
Corpora
</subsectionHeader>
<bodyText confidence="0.999929">
The computational costs (in terms of both time and
space) for calculating an n-gram model from an
SSC are very high2, so it is not a practical tech-
nique for implementing an IM engine. In order
to reduce the computational costs we approximate
an SSC using a deterministically tagged corpus,
which is called a pseudo-stochastically segmented
corpus (pSSC) (Kameko et al., 2015). The follow-
ing is the method for producing a pSSC from an
SSC.
</bodyText>
<listItem confidence="0.9994348">
• For i = 1 to nr − 1
1. output a character xi,
2. generate a random number 0 &lt; p &lt; 1,
3. output a word boundary if p &lt; Pi or
output nothing otherwise.
</listItem>
<bodyText confidence="0.998817">
Now we have a corpus in the same format as
a standard segmented corpus with variable (non-
constant) segmentation.
</bodyText>
<footnote confidence="0.904428333333333">
2This is because an SSC has many words and word frag-
ments. Additionally, word n-gram frequencies must be cal-
culated using floating point numbers instead of integers.
</footnote>
<subsectionHeader confidence="0.841422">
3.2.3 Pseudo-Stochastically Tagged Corpora
</subsectionHeader>
<bodyText confidence="0.998835076923077">
We can annotate a word with its all possi-
ble pronunciations and their probabilities, as is
done in an SSC. We call a corpus containing
sequences of words (w1w2 • • • wi • • • ) annotated
with a sequence of pairs of a pronunciation and
its probability ((yi,1,pi,1), (yi,2,pi,2), • • •, where
∑j pi,j = 1, for bi) a stochastically tagged cor-
pus (STC)3. We can estimate these probabilities
using an LR model built from sentences annotated
with pronunciations (Mori and Neubig, 2011).
Similar to pSSC we then produce a pseudo-
stochastically tagged sentence (pSTC) from an
STC as follows:
</bodyText>
<listItem confidence="0.996689">
• For each wi in the sentence
1. generate a random number 0 &lt; p &lt; 1,
2. annotate wi with its j-th phoneme se-
quence yi,j, where ∑j−1
</listItem>
<equation confidence="0.7552675">
1 pi,j &lt; p &lt;
∑j1 pi,j
</equation>
<bodyText confidence="0.999911125">
Now we have a corpus in the same format as a
standard corpus annotated with variable pronunci-
ation.
By estimating the parameters in Equation (1)
from a pSTC derived from a pSSC, our IM en-
gine can also suggest OOV word candidates with
various possible segmentation and pronunciations
without incurring high computational costs.
</bodyText>
<subsectionHeader confidence="0.950596">
3.2.4 Suggestion of OOV Words
</subsectionHeader>
<bodyText confidence="0.999884454545454">
Here we give an intuitive explanation why our
IM engine can suggest OOV words for a certain
phoneme sequence. Let us take an OOV word
example: “0ア9/yo-ko-a-ri,” an abbreviation of
“0{Aア9—t” (Yokohama city arena). A WS
system tends to segment it into “0” (side) and “
ア9” (ant) because they are frequent nouns. In
a pSSC, however, some occurrences of the string
“0ア9” are remain concatenated as the correct
word. For pronunciation, the first character has
two possible pronunciations “yo-ko” and “o-u.”
</bodyText>
<footnote confidence="0.889805">
3Because the existence or non-existence of a word bound-
ary information can also be expressed as a tag, a stochasti-
cally tagged corpus includes stochastic segmentation.
</footnote>
<figure confidence="0.980133818181818">
 

Pem  ,




em−1
∏
(1 − Pj)
j=bm
</figure>
<page confidence="0.685621">
1189
</page>
<figureCaption confidence="0.999206">
Figure 2: Input method for collecting logs.
</figureCaption>
<bodyText confidence="0.999927416666667">
So deterministic pronunciation estimation of this
new word has the risk of outputting the erroneous
result “o-u-a-ri.” This prevents our engine from
presenting “0ア9” as a conversion candidate for
the input “yo-ko-a-ri.” The pSTC, however, con-
tains two possible pronunciations for this word
and allows our engine to present the OOV word
“0ア9” for the input “yo-ko-a-ri.”
Thus when the user of our IM engine types “yo-
ko-a-ri-ni-i-ku” and selects “0ア9 6:. (to) 47く
(go),” the engine can learn an OOV word “0ア
9/yo-ko-a-ri” with context “6:./ni 47く/i-ku”.
</bodyText>
<sectionHeader confidence="0.998063" genericHeader="method">
4 Input Method Logs
</sectionHeader>
<bodyText confidence="0.99991525">
In this section we first propose an IM which al-
lows us to collect user logs. We then examine the
characteristics of these logs and some difficulties
in using them as language resources.
</bodyText>
<subsectionHeader confidence="0.999491">
4.1 Collecting Logs from an Input Method
</subsectionHeader>
<bodyText confidence="0.993858259259259">
As Figure 2 shows, the client of our IM, running
on the user’s PC, is used to input characters and
to modify conversion results. The server logs both
input from the client and the results of conversions
performed in response to requests from the client.
Our IM has two phases: phoneme typing and
conversion result editing. In each phase, the client
sends the typed keys to the server with a timestamp
and its IP address.
Phoneme typing: First the user inputs ASCII
characters for a phoneme sequence. If the
phoneme sequence itself is what the user
wants to write, the user may not go to the
next phase. The server records the keys typed
to enter the phoneme sequence, cursor move-
ments, and the phoneme sequence if the user
selects it as-is.
Conversion result editing: Then the user presses
a space key to make the IM engine con-
vert the phoneme sequence to the most likely
word sequence based on Equation (1). Some-
times the user changes some word bound-
aries, makes the IM engine enumerate can-
didate words covering the region, and selects
the intended one from the list of candidates.
The server records a space key and the final
word sequence.
</bodyText>
<subsectionHeader confidence="0.999266">
4.2 Characteristics of Input Method Logs
</subsectionHeader>
<bodyText confidence="0.999716888888889">
Table 1 shows an example of interesting log mes-
sages from the same IP address4. In many cases,
users type sentence fragments but not a complete
sentence. So in the example there are six frag-
ments within a short period indicated by the times-
tamps. If the user selects the phoneme sequence
as-is without going to the conversion result editing
phase, we can expect that there are word bound-
aries on both sides of the phoneme sequence. In-
</bodyText>
<footnote confidence="0.9949275">
4In reality, logs from different IPs are stored in the order
that they were received.
</footnote>
<page confidence="0.991618">
1190
</page>
<tableCaption confidence="0.955563">
Table 1: Input method logs of a tweet ‘横アリに比べると安めかと’ (It is cheap compared with Yoko-
hama arena).
</tableCaption>
<table confidence="0.999690428571429">
Timestamp Phoneme sequence Edit result Note
18:37:11.21 よこありに/yo-ko-a-ri-ni 横アリ/yo-ko-a-ri に/ni (with Yokohama arena)
18:37:12.60 くらっべる/ku-ra-b-be-ru くらっ/ku-ra-b ベル/be-ru Mistyping
18:37:14.94 くらべる/ku-ra-be-ru 比べ/ku-ra-be る/ru Revised input (compare)
18:37:15.32 と/to N/A (inflectional ending)
18:37:19.82 ものの/mo-no-no N/A Discarded in the twitter
18:37:22.42 やすめかと/ya-su-me-ka-to 安め/ya-su-me か/ka と/to (cheap)
</table>
<bodyText confidence="0.999739952380952">
side the phoneme sequence, however, there is no
information. If the user goes to the conversion
result editing phase, we can expect that the final
word sequence has correct word boundary infor-
mation.
There are two main problems that make it dif-
ficult to directly use IM logs as a training cor-
pus for word segmentation. The first problem is
fragmentation. IM users send the phoneme se-
quences for sentence fragments to the engine to
avoid editing long conversion results that require
many cursor movements. Thus the phoneme se-
quence and the final word sequence tend to be
sentence fragments (as we noted above) and as a
result they lose context information. The second
problem is noise. Word boundary information is
unreliable even when it is present because of mis-
takenly selected conversions or words entered sep-
arately. From these observations, the IM logs are
treated as partially segmented sentence fragments
that include noise.
</bodyText>
<sectionHeader confidence="0.969709" genericHeader="method">
5 Word Segmentation Using Input
</sectionHeader>
<subsectionHeader confidence="0.52593">
Method Logs
</subsectionHeader>
<bodyText confidence="0.930957">
In this section we first explain various ways to
generate language resources for a word segmenter
from IM logs. We then describe an automatic word
segmenter which utilizes these resources. In the
examples below we use the three-valued notation
(Mori and Oda, 2009) to denote partial segmenta-
tion as follows:
 |: there is a word boundary,
- : there is not a word boundary,
: there is no information.
</bodyText>
<subsectionHeader confidence="0.6464725">
5.1 Input Method Logs as Language
Resources
</subsectionHeader>
<bodyText confidence="0.963118666666667">
The phoneme sequences and edit results in the fi-
nal selection themselves are considered to be par-
tially segmented sentences. We call the corpus
generated directly from the logs “Log-as-is.” Ex-
amples in Table 1 are converted as following.
✓ Example of Log-as-is (12 annotations) ✏
</bodyText>
<equation confidence="0.99082025">
横-ア-リ|に と
く-ら-っ|ベ-ル
比-べ|る 安-め|か|と
✒ ✑
</equation>
<bodyText confidence="0.927702523809524">
Here the number of annotations is the sum of “-”
and “|”. In this example, one entry corresponds
to one entry of the training data for the word seg-
menter. As you can easily imagine, Log-as-is may
contain mistaken results (noise) and short entries
(fragmentation). Both are harmful for a word seg-
menter.
To cope with the fragmentation problem, we
propose to connect some logs based on their times-
tamps. If the difference between the timestamps of
two sequential logs is short, both logs are proba-
bly from the same sentence. So we connect two
sequential logs if the time difference between the
last key of the first log and the first key of the sec-
ond log is smaller than a certain threshold s. In the
experiment we set s = 500[ms] based on observa-
tions of our behaviors. This method is referred to
as “Log-chunk.” Using this method, we obtain
the following from the examples in Table 1.
✓ Example of Log-chunk (15 annotations) ✏横-ア-リ|に|く-ら-っ|ベ-ル
比-べ|る|と|も の の
</bodyText>
<equation confidence="0.8523465">
安-め|か|と
✒
</equation>
<footnote confidence="0.699209">
5The results were stable for s in preliminary experiments.
</footnote>
<equation confidence="0.946179">
も の の
</equation>
<bodyText confidence="0.995837111111111">
✑
We see that Log-chunk contains more context in-
formation than Log-as-is.
For preventing the noise problem, we propose
to filter out logs with a small number of conver-
sions. We expect that an edited sentence will have
many OOV words and not much noise. Therefore
we use logs which were converted more than n,
times. In the experiment we set n, = 2 based on
</bodyText>
<page confidence="0.998468">
1191
</page>
<tableCaption confidence="0.99798">
Table 2: Corpus specifications.
</tableCaption>
<table confidence="0.996474375">
#sent. #words #char.
Training
BCCWJ 56,753 1,324,951 1,911,660
Newspaper 8,164 240,097 361,843
Conversation 11,700 147,809 197,941
Test
BCCWJ-test 6,025 148,929 212,261
TWI-test 2,976 37,010 58,316
</table>
<bodyText confidence="0.85739925">
observations of our behavior6. This method is re-
ferred to as “Log-mconv.” Using this method, the
examples in Table 1 becomes the following.
r Example of Log-mconv (3 annotations) �
</bodyText>
<equation confidence="0.945826">
横-ア-リ|に � �
</equation>
<bodyText confidence="0.999965444444444">
As this example shows, Log-mconv contains short
entries (fragmentation) like Log-as-is. However,
we expect that the annotated tweets do not include
mistaken boundaries or conversions that were dis-
carded.
Obviously we can combine Log-chunk and
Log-mconv to avoid both the fragmentation and
noise problems. This combination is referred to as
“Log-chunk-mconv.”
</bodyText>
<subsectionHeader confidence="0.999268">
5.2 Training a Word Segmenter on Logs
</subsectionHeader>
<bodyText confidence="0.999973333333333">
The IM logs give us partially segmented sentence
fragments, so we need a word segmenter capa-
ble of learning from them. We can use a word
segmenter based on a sequence classifier (Tsuboi
et al., 2008; Yang and Vozila, 2014; Jiang et al.,
2013) or one based on a pointwise classifier (Neu-
big et al., 2011). Although both types are viable,
we adopt the latter in the experiments because it
requires much less training time while delivering
comparable accuracy.
Here is a brief explanation of the word seg-
menter based on the pointwise method. For more
detail the reader may refer to (Neubig et al., 2011).
The input is an unsegmented character sequence
X = x1x2 · · · xk. The word segmenter decides if
there is a word boundary tz = 1 or not tz = 0
by using support vector machines (SVMs) (Fan et
al., 2008)7. The features are character n-grams
</bodyText>
<footnote confidence="0.994913333333333">
6The results were stable for n. in the preliminary experi-
ments.
7The reason why we use SVM for word segmentation is
that the accuracy is generally higher than that based on LR. It
was so in the experiments of this paper. The F-measure of LR
on TWI-test was 91.30 (Recall = 89.50, Precision = 93.17),
</footnote>
<tableCaption confidence="0.987152">
Table 3: Language resources derived from logs.
</tableCaption>
<table confidence="0.995302666666667">
#sentence #annotations
fragments
Log-as-is 32,119 39,708
Log-chunk 8,685 63,144
Log-mconv 4,610 10,852
Log-chunk-mconv 1,218 14,242
</table>
<bodyText confidence="0.996684181818182">
and character type n-grams (n = 1, 2,3) around
the decision points in a window with a width of
6 characters. Additional features are triggered if
character n-grams in the window match with char-
acter sequences in the dictionary. This approach is
called pointwise because the word boundary deci-
sion is made without referring to the other deci-
sions on the points j =� i. As you can see from the
explanation given above, we can also use partially
segmented sentences from IM logs for training in
the standard way.
</bodyText>
<sectionHeader confidence="0.99525" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999975428571428">
As an evaluation of our methods, we measured
the accuracy of WS without using logs (the base-
line) and using logs converted by several methods.
There are two test corpora: one is the general do-
main corpus from which we built the baseline WS,
and the other is the same domain that the IM logs
were collected from, Twitter.
</bodyText>
<subsectionHeader confidence="0.995036">
6.1 Corpora
</subsectionHeader>
<bodyText confidence="0.821886210526316">
The annotated corpus we used to build the base-
line word segmenter is the manually annotated
part (core data) of the Balanced Corpus of Con-
temporary Written Japanese (BCCWJ) (Maekawa,
2008), plus newspaper articles and daily conver-
sation sentences. We also used a 234,652-word
dictionary (UniDic) provided with the BCCWJ. A
small portion of the BCCWJ core data is reserved
for testing. In addition, we manually segmented
sentences randomly obtained from Twitter8 during
the same period as the log collection for the test
corpus. Table 2 shows the details of these corpora.
which is lower than that of SVM (see Table 4). To make an
SSC, however, we use an LR model because we need word
boundary probabilities.
8We extracted body text from 1,592 tweets excluding
mentions, hash tags, URLs, and ticker symbols. Then we
divided the body text into sentences by separating on newline
characters, resulting in 2,976 sentences.
</bodyText>
<page confidence="0.997568">
1192
</page>
<tableCaption confidence="0.998989">
Table 4: WS accuracy on the tweets.
</tableCaption>
<table confidence="0.999845833333333">
Recall [%] Precision [%] F-measure
Baseline 90.31 94.05 92.14
+ Log-as-is 90.33 93.77 92.02
+ Log-chunk 91.04 94.29 92.64
+ Log-mconv 90.62 94.09 92.32
+ Log-chunk-mconv 91.40 94.45 92.90
</table>
<tableCaption confidence="0.976972">
Table 5: WS accuracy on BCCWJ.
</tableCaption>
<table confidence="0.999876833333333">
Recall [%] Precision [%] F-measure
Baseline 99.01 98.97 98.99
+ Log-as-is 99.02 98.87 98.94
+ Log-chunk 99.05 98.88 98.96
+ Log-mconv 98.98 98.91 98.95
+ Log-chunk-mconv 98.98 98.92 98.95
</table>
<subsectionHeader confidence="0.998976">
6.2 Models using Input Method Logs
</subsectionHeader>
<bodyText confidence="0.999983">
To make the training data for our IM server, we
first chose randomly selected tweets (786,331 sen-
tences) in addition to the unannotated part of the
BCCWJ (358,078 sentences). We then trained LR
models which estimate word boundary probabili-
ties and pronunciation probabilities for words (and
word candidates) from the training data shown in
Table 2 and UniDic. We made a pSTC for our
IM engine from 1,207,182 sentences randomly ob-
tained from Twitter by following the procedure
which we explained in Subsection 3.2.39.
We launched our IM as a browser add-on for
Twitter and collected 19,770 IM logs from 7 users
between April 24 and December 31, 2014. Fol-
lowing the procedures in Section 5.1, we obtained
the language resources shown in Table 3. We com-
bined them with the training corpus and dictionar-
ies to build four WSs, which we compared with
the baseline.
</bodyText>
<subsectionHeader confidence="0.978275">
6.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9994164">
Following the standard in WS experiments, the
evaluation criteria are recall, precision, and F-
measure (their harmonic mean). Recall is the
number of correctly segmented words divided by
the number of words in the test corpus. Preci-
sion is the number of correctly segmented words
divided by the number of words in the system out-
put.
Table 4 and 5 show WS accuracy on TWI-test
and BCCWJ-test, respectively. The difference in
</bodyText>
<footnote confidence="0.948325">
9There is no overlap with the test data.
</footnote>
<bodyText confidence="0.99439803030303">
accuracy of the baseline method on BCCWJ-test
and TWI-test shows that WS of tweets is very dif-
ficult. The fact that the precision on TWI-test is
much higher than the recall indicates that the base-
line model suffers from over-segmentation. This
over-segmentation problem is mainly caused by
OOV words being divided into known words. For
example, “Aア9” (Yokohama arena) is divided
into the two know words “A” (side) and “ア9”
(ant).
When we compare the F-measures on TWI-test,
all the models referring to the IM logs outperform
the baseline model trained only from the BCCWJ.
The highest is the Log-chunk-mconv model and
the improvement over the baseline is statistically
significant (significance level: 1%). In addition
the accuracies of the five methods on the BCCWJ
(Table 5) are almost the same and there is no statis-
tical significance (significance level: 1%) between
any two of them.
We analyzed the words misrecognized by the
WSs, which we call error words. Table 6 shows
the number of error words, the number of OOV
words, and the ratio of OOV words to error words.
Here the vocabulary is the set of the words appear-
ing in the training data or in UniDic (see Table 2).
Although the result of the WS trained on Log-as-
is contains more error words than the baseline, the
OOV ratio is less than the baseline. This means
that the IM logs have a potential to reduce errors
caused by OOV words.
Table 6 also indicates that the best method Log-
chunk-mconv had the greatest success in reducing
</bodyText>
<page confidence="0.994185">
1193
</page>
<tableCaption confidence="0.996287">
Table 6: Ratio of OOV words in error words.
</tableCaption>
<table confidence="0.9981875">
#Error words #OOV words (ratio[%])
Baseline 446 103 (23.09)
+ Log-as-is 467 89 (19.06)
+ Log-chunk 428 81 (18.93)
+ Log-mconv 443 88 (19.86)
+ Log-chunk-mconv 413 74 (17.79)
</table>
<figure confidence="0.9989733">
0 20000 40000 60000 80000 100000
#characters
F-measure
92.9
92.7
92.5
92.3
92.1
Log-chunk-mconv
Log-chunk
</figure>
<figureCaption confidence="0.9876045">
Figure 3: Relationship between WS accuracy on
the tweets and log size.
</figureCaption>
<bodyText confidence="0.99975316">
errors caused by OOV words. However, the ma-
jority of error words are in-vocabulary words. It
can be said that our log chunking method (Log-
chunk or Log-chunk-mconv) enabled the WSs to
eliminate many known word errors by using con-
text information.
To investigate the impact of the log size, we
measured WS accuracy on TWI-test when vary-
ing the log size during training. Figure 3 shows
the results. Table 4 says that Log-chunk-mconv
and Log-chunk increase the accuracy nicely. The
graph, however, clarifies that Log-chunk-mconv
achieves high accuracy with fewer training data
converted from logs. In other words, the method
Log-chunk-mconv is good at distilling the in-
formative parts and filtering out the noisy parts.
These characteristics are very important properties
to have as we consider deploying our IM to a wider
audience. An IM is needed to type Japanese and
the number of Japanese speakers is more than 100
million. If we can use input logs of even 1% of
them for the same or longer periodlo, the idea we
propose in this paper can improve WS accuracy on
various domains efficiently and automatically.
As a final remark, this paper describes a suc-
</bodyText>
<footnote confidence="0.529571">
10The number of users using our system in this paper is 7
for 8 months.
</footnote>
<bodyText confidence="0.9961562">
cessful example of how to build a useful tool for
the NLP community. This process has three steps:
1) design a useful NLP application that can collect
user logs, 2) deploy it for public use, and 3) devise
a method for mining data from the logs.
</bodyText>
<sectionHeader confidence="0.998961" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999973647058824">
This paper described the design of a publicly us-
able IM which collects natural annotations for use
as training data for another system. Specifically,
we (1) described how to construct an IM server
that suggests OOV word candidates, (2) designed
a publicly usable IM that collects logs of user
behavior, and (3) proposed a method for using
this data to improve word segmenters. Tweets
from Twitter are a promising source of data with
great potential for NLP, which is one reason why
we used them as the target domain for our ex-
periments. The experimental results showed that
our methods improve accuracy in this domain.
Our method itself is domain-independent and only
needs logs from the target domain, so it is worth
testing on other domains and with much longer pe-
riods of data collection.
</bodyText>
<sectionHeader confidence="0.996983" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999681428571428">
This work was supported by JSPS Grants-in-Aid
for Scientific Research Grant Number 26280084
and Microsoft CORE project. We thank Dr.
Hisami Suzuki, Dr. Koichiro Yoshino, and Mr.
Daniel Flannery for their valuable comments and
suggestions on the manuscript. We are also grate-
ful to the anonymous users of our input method.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977162833333333">
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jennifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479.
Zheng Chen and Kai-Fu Lee. 2000. A new statistical
approach to Chinese pinyin input. In Proceedings
</reference>
<page confidence="0.962827">
1194
</page>
<reference confidence="0.999032943925233">
of the 38th Annual Meeting of the Association for
Computational Linguistics, pages 241–247.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Ryuichiro Higashinaka, Noriaki Kawamae, Kugatsu
Sadamitsu, Yasuhiro Minami, Toyomi Meguro, Ko-
hji Dohsaka, and Hirohito Inagaki. 2011. Building a
conversational model from two-tweets. IEEE Trans-
actions on ASRU, pages 330–335.
Wenbin Jiang, Meng Sun, Yajuan Lu, Yating Yang, and
Qun Liu. 2013. Discriminative learning with natu-
ral annotations: Word segmentation as a case study.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 761–
769.
Nobuhiro Kaji and Masaru Kitsuregawa. 2014. Ac-
curate word segmentation and POS tagging for
Japanese microblogs: Corpus annotation and joint
modeling with lexical normalization. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing, pages 99–109.
Hirotaka Kameko, Shinsuke Mori, and Yoshimasa Tsu-
ruoka. 2015. Can symbol grounding improve low-
level NLP? Word segmentation as a case study. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 230–237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth
ICML, pages 282–289.
Yijia Liu, Yue Zhang, Wangxiang Che, Ting Liu, and
Fan Wu. 2014. Domain adaptation for CRF-based
Chinese word segmentation using free annotations.
In Proceedings of the 2014 Conference on Empiri-
cal Methods in Natural Language Processing, pages
864–874.
Kikuo Maekawa. 2008. Balanced corpus of con-
temporary written Japanese. In Proceedings of the
6th Workshop on Asian Language Resources, pages
101–102.
Hirokuni Maeta and Shinsuke Mori. 2012. Statistical
input method based on a phrase class n-gram model.
In Workshop on Advances in Text Input Methods.
Shinsuke Mori and Gakuto Kurata. 2005. Class-based
variable memory length markov model. In Proceed-
ings of the InterSpeech2005, pages 13–16.
Shinsuke Mori and Graham Neubig. 2011. A point-
wise approach to pronunciation estimation for a TTS
front-end. In Proceedings of the InterSpeech2011,
pages 2181–2184.
Shinsuke Mori and Graham Neubig. 2014. Language
resource addition: Dictionary or corpus? In Pro-
ceedings of the Nineth International Conference on
Language Resources and Evaluation, pages 1631–
1636.
Shinsuke Mori and Hiroki Oda. 2009. Automatic word
segmentation using three types of dictionaries. In
Proceedings of the Eighth International Conference
Pacific Association for Computational Linguistics,
pages 1–6.
Shinsuke Mori and Daisuke Takuma. 2004. Word
n-gram probability estimation from a Japanese raw
corpus. In Proceedings of the Eighth International
Conference on Speech and Language Processing,
pages 1037–1040.
Shinsuke Mori, Tsuchiya Masatoshi, Osamu Yamaji,
and Makoto Nagao. 1999. Kana-kanji conversion
by a stochastic model. Transactions of Information
Processing Society of Japan, 40(7):2946–2953. (in
Japanese).
Shinsuke Mori, Daisuke Takuma, and Gakuto Kurata.
2006. Phoneme-to-text transcription system with an
infinite vocabulary. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics,
pages 729–736.
Masaaki Nagata. 1994. A stochastic Japanese mor-
phological analyzer using a forward-DP backward-
A* n-best search algorithm. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 201–207.
Graham Neubig and Shinsuke Mori. 2010. Word-
based partial annotation for efficient corpus con-
struction. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation, pages 2723–2727.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust, adaptable
Japanese morphological analysis. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics, pages 529–533.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics, pages 562–568.
Dana Ron, Yoram Singer, and Naftali Tishby. 1996.
The power of amnesia: Learning probabilistic au-
tomata with variable memory length. Machine
Learning, 25:117–149.
</reference>
<page confidence="0.853717">
1195
</page>
<reference confidence="0.998578291666667">
Takeshi Sakai, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: Real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, WWW ’10, pages 851–860.
Richard Sproat and Chilin Shih William Gale Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377–404.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations.
In Proceedings of the 22nd International Conference
on Computational Linguistics, pages 897–904.
Sho Tsugawa, Yukiko Mogi, Yusuke Kikuchi, Fumio
Kishino, Kazuyuki Fujita, Yuichi Itoh, and Hiroyuki
Ohsaki. 2013. On estimating depressive tendencies
of Twitter users utilizing their tweet data. In VR’13,
pages 1–4.
Fan Yang and Paul Vozila. 2014. Semi-supervised
Chinese word segmentation using partial-label
learning with conditional random fields. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing, pages 90–98.
</reference>
<page confidence="0.994314">
1196
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.042375">
<title confidence="0.999148">Keyboard Logs as Natural Annotations for Word Segmentation</title>
<author confidence="0.9323">Yahoo Japan</author>
<affiliation confidence="0.545978">Midtown Tower, 9-7-1 Akasaka,</affiliation>
<address confidence="0.800833">Tokyo,</address>
<email confidence="0.988898">ftakahas@yahoo-corp.jp</email>
<title confidence="0.221532">Shinsuke ACCMS, Kyoto</title>
<author confidence="0.512551">Yoshida Honmachi</author>
<email confidence="0.6878195">Kyoto,forest@i.kyoto-u.ac.jp</email>
<abstract confidence="0.99961415">In this paper we propose a framework to improve word segmentation accuracy using input method logs. An input method is software used to type sentences in languages which have far more characters than the number of keys on a keyboard. The main contributions of this paper are: 1) an input method server that proposes word candidates which are not included in the vocabulary, 2) a publicly usable input method that logs user behavior (like typing and selection of word candidates), and 3) a method for improving word segmentation by using these logs. We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="4795" citStr="Brown et al., 1992" startWordPosition="785" endWordPosition="788">at logs user behavior (such as typing and selection of word candidates) (Section 4), • a method for improving word segmentation by using these logs (Section 5). To the best of our knowledge, this is the first paper proposing a method for using IM logs to successfully improve WS. 2 Related Work The main focus of this paper is WS. Corpus-based, or empirical, methods were proposed in the early 90’s (Nagata, 1994). Then (Mori and Kurata, 2005) extended it by lexicalizing the states like many researches in that era, grouping the wordPOS pairs into clusters inspired by the class-based n-gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed better performance than POS-based Markov models (Kudo et al., 2004). The training time of sequencebased methods tends to be long, especially when we use partially annotated data. Thus a simple method base</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Kai-Fu Lee</author>
</authors>
<title>A new statistical approach to Chinese pinyin input.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>241--247</pages>
<contexts>
<context position="7032" citStr="Chen and Lee, 2000" startWordPosition="1163" endWordPosition="1166">s extension was adopted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs used in this paper are also classified as natural annotations, but contain much more noise. In addition, we need an IM that is specifically designed to collect logs as natural annotations. Server design is the most important factor in capturing information from IM logs. The most popular IM servers are based on statistical language modeling (Mori et al., 1999; Chen and Lee, 2000; Maeta and Mori, 2012). Their parameters are trained from manually segmented sentences whose words are annotated with phoneme sequences, and from sentences automatically annotated with NLP tools which are also based on machine learning models trained on the annotated sentences. Thus normal IM servers are not capable of presenting out-of-vocabulary (OOV) words (which provide large amounts of information on word boundaries) as conversion candidates. To make our IM server capable of presenting OOV words, we extend a statistical IM server based on (Mori et al., 2006), and ensure that it is comput</context>
<context position="9091" citStr="Chen and Lee, 2000" startWordPosition="1506" endWordPosition="1509">hod does not utilize any other characteristics of tweets. So it also works in other domains such as blogs. 3 Input Method Suggesting OOV Words In this section we propose a practical statistical IM server that suggests OOV word candidates in addition to words in its vocabulary. 3.1 Statistical Input Method An input method (IM) is software which converts a phoneme sequence into a word sequence. This is useful for languages which contain far more characters than keys on a keyboard. Since there are some ambiguities in conversion, a conversion engine based on a word n-gram model has been proposed (Chen and Lee, 2000). Today, almost all IM engines are based on statistical methods. For the LM unit, instead of words we propose to adopt word-pronunciation pairs u = (y, w). Thus given a phoneme sequence yl 1 = y1y2 · · · yl as the input, the goal of our IM engine is to output a word sequence �wm1 that maximizes the probability P(w, yl1) as follows: wm1 = argmax w P(ui|ui−1 i−n+1), where the concatenation of yi in each ui is equal to the input: yl1 = y1y2 · · · ym. In addition uj (j &lt; 0) are special symbols introduced to simplify the notation and um+1 is a special symbol indicating a sentence boundary. As in ex</context>
</contexts>
<marker>Chen, Lee, 2000</marker>
<rawString>Zheng Chen and Kai-Fu Lee. 2000. A new statistical approach to Chinese pinyin input. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 241–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="11724" citStr="Fan et al., 2008" startWordPosition="1966" endWordPosition="1969">umerate all the substrings in a corpus with all possible pronunciations. For that purpose, we adopt the notion of a stochastically segmented corpus (SSC) (Mori and Takuma, 2004) and extend it to the pronunciation annotation to words. 3.2.1 Stochastically Segmented Corpora An SSC is defined as a combination of a raw corpus Cr (hereafter referred to as the character sequence xnr 1 ) and word boundary probabilities of the form Pi, which is the probability that a word boundary exists between two characters xi and xi+1. These probabilities are estimated by a model based on logistic regression (LR)(Fan et al., 2008) trained on a manually segmented corpus referring to the same features as those used in (Neubig et al., 2011). Since there are word boundaries before the first character and after the last character of the corpus, P0 = Pnr = 1. Then word n-gram frequencies on an SSC are calculated as follows: Word 0-gram frequency: This is defined as the expected number of words in the SSC: Word n-gram frequency (n &gt; 1): Consider the situation in which a word sequence wn1 occurs in the SSC as a subsequence beginning at the (i + 1)-th character and ending at the k-th character and each word wm in the word seque</context>
<context position="23939" citStr="Fan et al., 2008" startWordPosition="4099" endWordPosition="4102"> et al., 2008; Yang and Vozila, 2014; Jiang et al., 2013) or one based on a pointwise classifier (Neubig et al., 2011). Although both types are viable, we adopt the latter in the experiments because it requires much less training time while delivering comparable accuracy. Here is a brief explanation of the word segmenter based on the pointwise method. For more detail the reader may refer to (Neubig et al., 2011). The input is an unsegmented character sequence X = x1x2 · · · xk. The word segmenter decides if there is a word boundary tz = 1 or not tz = 0 by using support vector machines (SVMs) (Fan et al., 2008)7. The features are character n-grams 6The results were stable for n. in the preliminary experiments. 7The reason why we use SVM for word segmentation is that the accuracy is generally higher than that based on LR. It was so in the experiments of this paper. The F-measure of LR on TWI-test was 91.30 (Recall = 89.50, Precision = 93.17), Table 3: Language resources derived from logs. #sentence #annotations fragments Log-as-is 32,119 39,708 Log-chunk 8,685 63,144 Log-mconv 4,610 10,852 Log-chunk-mconv 1,218 14,242 and character type n-grams (n = 1, 2,3) around the decision points in a window with</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
</authors>
<title>Noriaki Kawamae, Kugatsu Sadamitsu, Yasuhiro Minami, Toyomi Meguro, Kohji Dohsaka, and Hirohito Inagaki.</title>
<date>2011</date>
<journal>IEEE Transactions on ASRU,</journal>
<pages>330--335</pages>
<marker>Higashinaka, 2011</marker>
<rawString>Ryuichiro Higashinaka, Noriaki Kawamae, Kugatsu Sadamitsu, Yasuhiro Minami, Toyomi Meguro, Kohji Dohsaka, and Hirohito Inagaki. 2011. Building a conversational model from two-tweets. IEEE Transactions on ASRU, pages 330–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Meng Sun</author>
<author>Yajuan Lu</author>
<author>Yating Yang</author>
<author>Qun Liu</author>
</authors>
<title>Discriminative learning with natural annotations: Word segmentation as a case study.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="6539" citStr="Jiang et al., 2013" startWordPosition="1078" endWordPosition="1081">ecome increasingly aware of the importance of language resources (Neubig and Mori, 2010; Mori and Neubig, 2014). So Mori and Oda (2009) proposed to incorpolate dictionaries for human into a WS system with a different word definition. CRFs were also extended to enable training from partially annotated sentences (Tsuboi et al., 2008). When using partially annotated sentences for WS training data, word boundary information exists only between some character pairs and is absent for others. This extension was adopted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs used in this paper are also classified as natural annotations, but contain much more noise. In addition, we need an IM that is specifically designed to collect logs as natural annotations. Server design is the most important factor in capturing information from IM logs. The most popular IM servers are based on statistical language modeling (Mori et al., 1999; Chen and Lee, 2000; Maeta and Mori, 2012). Their parameters are trained from manually segmented sentences whose words are ann</context>
<context position="23379" citStr="Jiang et al., 2013" startWordPosition="3995" endWordPosition="3998">s, Log-mconv contains short entries (fragmentation) like Log-as-is. However, we expect that the annotated tweets do not include mistaken boundaries or conversions that were discarded. Obviously we can combine Log-chunk and Log-mconv to avoid both the fragmentation and noise problems. This combination is referred to as “Log-chunk-mconv.” 5.2 Training a Word Segmenter on Logs The IM logs give us partially segmented sentence fragments, so we need a word segmenter capable of learning from them. We can use a word segmenter based on a sequence classifier (Tsuboi et al., 2008; Yang and Vozila, 2014; Jiang et al., 2013) or one based on a pointwise classifier (Neubig et al., 2011). Although both types are viable, we adopt the latter in the experiments because it requires much less training time while delivering comparable accuracy. Here is a brief explanation of the word segmenter based on the pointwise method. For more detail the reader may refer to (Neubig et al., 2011). The input is an unsegmented character sequence X = x1x2 · · · xk. The word segmenter decides if there is a word boundary tz = 1 or not tz = 0 by using support vector machines (SVMs) (Fan et al., 2008)7. The features are character n-grams 6T</context>
</contexts>
<marker>Jiang, Sun, Lu, Yang, Liu, 2013</marker>
<rawString>Wenbin Jiang, Meng Sun, Yajuan Lu, Yating Yang, and Qun Liu. 2013. Discriminative learning with natural annotations: Word segmentation as a case study. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761– 769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Accurate word segmentation and POS tagging for Japanese microblogs: Corpus annotation and joint modeling with lexical normalization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>99--109</pages>
<contexts>
<context position="2192" citStr="Kaji and Kitsuregawa, 2014" startWordPosition="345" endWordPosition="349">s an additional problem for NLP tasks in these languages. To make matters worse, the domains of the available training data often differ from domains where there is a high demand for NLP, which causes a severe degradation in WS performance. Examples include ma&apos;This work was done when the first author was at Kyoto University. chine translation of patents, text mining of medical texts, and marketing on the micro-blog site Twitter&apos;. Some papers have reported low accuracy on WS or the joint task of WS and part-of-speech (POS) tagging of Japanese or Chinese in these domains (Mori and Neubig, 2014; Kaji and Kitsuregawa, 2014; Liu et al., 2014) To cope with this problem, we propose a way to collect information from people as they type Japanese or Chinese on computers. These languages use far more characters than the number of keys on a keyboard, so users use software called an input method (IM) to type text in these languages. Unlike written texts in these languages, which lack word boundary information, text entered with an IM can provide word boundary information that can used by NLP systems. As we show in this paper, logs collected from IMs are a valuable source of word boundary information. An IM consists of a</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2014</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2014. Accurate word segmentation and POS tagging for Japanese microblogs: Corpus annotation and joint modeling with lexical normalization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 99–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirotaka Kameko</author>
<author>Shinsuke Mori</author>
<author>Yoshimasa Tsuruoka</author>
</authors>
<title>Can symbol grounding improve lowlevel NLP? Word segmentation as a case study.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="13775" citStr="Kameko et al., 2015" startWordPosition="2366" endWordPosition="2369">gram probabilities by dividing word n-gram frequencies by word (n − 1)- gram frequencies. For a detailed explanation and a mathematical proof of this method, please refer to (Mori and Takuma, 2004). 3.2.2 Pseudo-Stochastically Segmented Corpora The computational costs (in terms of both time and space) for calculating an n-gram model from an SSC are very high2, so it is not a practical technique for implementing an IM engine. In order to reduce the computational costs we approximate an SSC using a deterministically tagged corpus, which is called a pseudo-stochastically segmented corpus (pSSC) (Kameko et al., 2015). The following is the method for producing a pSSC from an SSC. • For i = 1 to nr − 1 1. output a character xi, 2. generate a random number 0 &lt; p &lt; 1, 3. output a word boundary if p &lt; Pi or output nothing otherwise. Now we have a corpus in the same format as a standard segmented corpus with variable (nonconstant) segmentation. 2This is because an SSC has many words and word fragments. Additionally, word n-gram frequencies must be calculated using floating point numbers instead of integers. 3.2.3 Pseudo-Stochastically Tagged Corpora We can annotate a word with its all possible pronunciations an</context>
</contexts>
<marker>Kameko, Mori, Tsuruoka, 2015</marker>
<rawString>Hirotaka Kameko, Shinsuke Mori, and Yoshimasa Tsuruoka. 2015. Can symbol grounding improve lowlevel NLP? Word segmentation as a case study. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="5258" citStr="Kudo et al., 2004" startWordPosition="868" endWordPosition="871">alizing the states like many researches in that era, grouping the wordPOS pairs into clusters inspired by the class-based n-gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed better performance than POS-based Markov models (Kudo et al., 2004). The training time of sequencebased methods tends to be long, especially when we use partially annotated data. Thus a simple method based on pointwise classification has been shown to be as accurate as sequence-based methods and fast enough to make active learning practically possible (Neubig et al., 2011). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the same sentence, it is straightforward to train the model from partially annotated sentences. We adopt this WS system for our experiment</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="5148" citStr="Lafferty et al., 2001" startWordPosition="849" endWordPosition="852">pirical, methods were proposed in the early 90’s (Nagata, 1994). Then (Mori and Kurata, 2005) extended it by lexicalizing the states like many researches in that era, grouping the wordPOS pairs into clusters inspired by the class-based n-gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed better performance than POS-based Markov models (Kudo et al., 2004). The training time of sequencebased methods tends to be long, especially when we use partially annotated data. Thus a simple method based on pointwise classification has been shown to be as accurate as sequence-based methods and fast enough to make active learning practically possible (Neubig et al., 2011). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the same sentence, it is str</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijia Liu</author>
<author>Yue Zhang</author>
<author>Wangxiang Che</author>
<author>Ting Liu</author>
<author>Fan Wu</author>
</authors>
<title>Domain adaptation for CRF-based Chinese word segmentation using free annotations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>864--874</pages>
<contexts>
<context position="2211" citStr="Liu et al., 2014" startWordPosition="350" endWordPosition="353">NLP tasks in these languages. To make matters worse, the domains of the available training data often differ from domains where there is a high demand for NLP, which causes a severe degradation in WS performance. Examples include ma&apos;This work was done when the first author was at Kyoto University. chine translation of patents, text mining of medical texts, and marketing on the micro-blog site Twitter&apos;. Some papers have reported low accuracy on WS or the joint task of WS and part-of-speech (POS) tagging of Japanese or Chinese in these domains (Mori and Neubig, 2014; Kaji and Kitsuregawa, 2014; Liu et al., 2014) To cope with this problem, we propose a way to collect information from people as they type Japanese or Chinese on computers. These languages use far more characters than the number of keys on a keyboard, so users use software called an input method (IM) to type text in these languages. Unlike written texts in these languages, which lack word boundary information, text entered with an IM can provide word boundary information that can used by NLP systems. As we show in this paper, logs collected from IMs are a valuable source of word boundary information. An IM consists of a client (front-end)</context>
</contexts>
<marker>Liu, Zhang, Che, Liu, Wu, 2014</marker>
<rawString>Yijia Liu, Yue Zhang, Wangxiang Che, Ting Liu, and Fan Wu. 2014. Domain adaptation for CRF-based Chinese word segmentation using free annotations. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 864–874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
</authors>
<title>Balanced corpus of contemporary written Japanese.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Workshop on Asian Language Resources,</booktitle>
<pages>101--102</pages>
<contexts>
<context position="25500" citStr="Maekawa, 2008" startWordPosition="4367" endWordPosition="4368">se partially segmented sentences from IM logs for training in the standard way. 6 Evaluation As an evaluation of our methods, we measured the accuracy of WS without using logs (the baseline) and using logs converted by several methods. There are two test corpora: one is the general domain corpus from which we built the baseline WS, and the other is the same domain that the IM logs were collected from, Twitter. 6.1 Corpora The annotated corpus we used to build the baseline word segmenter is the manually annotated part (core data) of the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), plus newspaper articles and daily conversation sentences. We also used a 234,652-word dictionary (UniDic) provided with the BCCWJ. A small portion of the BCCWJ core data is reserved for testing. In addition, we manually segmented sentences randomly obtained from Twitter8 during the same period as the log collection for the test corpus. Table 2 shows the details of these corpora. which is lower than that of SVM (see Table 4). To make an SSC, however, we use an LR model because we need word boundary probabilities. 8We extracted body text from 1,592 tweets excluding mentions, hash tags, URLs, a</context>
</contexts>
<marker>Maekawa, 2008</marker>
<rawString>Kikuo Maekawa. 2008. Balanced corpus of contemporary written Japanese. In Proceedings of the 6th Workshop on Asian Language Resources, pages 101–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirokuni Maeta</author>
<author>Shinsuke Mori</author>
</authors>
<title>Statistical input method based on a phrase class n-gram model.</title>
<date>2012</date>
<booktitle>In Workshop on Advances in Text Input Methods.</booktitle>
<contexts>
<context position="7055" citStr="Maeta and Mori, 2012" startWordPosition="1167" endWordPosition="1170">ted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs used in this paper are also classified as natural annotations, but contain much more noise. In addition, we need an IM that is specifically designed to collect logs as natural annotations. Server design is the most important factor in capturing information from IM logs. The most popular IM servers are based on statistical language modeling (Mori et al., 1999; Chen and Lee, 2000; Maeta and Mori, 2012). Their parameters are trained from manually segmented sentences whose words are annotated with phoneme sequences, and from sentences automatically annotated with NLP tools which are also based on machine learning models trained on the annotated sentences. Thus normal IM servers are not capable of presenting out-of-vocabulary (OOV) words (which provide large amounts of information on word boundaries) as conversion candidates. To make our IM server capable of presenting OOV words, we extend a statistical IM server based on (Mori et al., 2006), and ensure that it is computationally efficient eno</context>
</contexts>
<marker>Maeta, Mori, 2012</marker>
<rawString>Hirokuni Maeta and Shinsuke Mori. 2012. Statistical input method based on a phrase class n-gram model. In Workshop on Advances in Text Input Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Gakuto Kurata</author>
</authors>
<title>Class-based variable memory length markov model.</title>
<date>2005</date>
<booktitle>In Proceedings of the InterSpeech2005,</booktitle>
<pages>13--16</pages>
<contexts>
<context position="4619" citStr="Mori and Kurata, 2005" startWordPosition="756" endWordPosition="759">ur IM. The three main contributions of this paper are: • an IM server that proposes word candidates which are not included in the vocabulary (Section 3), • a publicly usable IM that logs user behavior (such as typing and selection of word candidates) (Section 4), • a method for improving word segmentation by using these logs (Section 5). To the best of our knowledge, this is the first paper proposing a method for using IM logs to successfully improve WS. 2 Related Work The main focus of this paper is WS. Corpus-based, or empirical, methods were proposed in the early 90’s (Nagata, 1994). Then (Mori and Kurata, 2005) extended it by lexicalizing the states like many researches in that era, grouping the wordPOS pairs into clusters inspired by the class-based n-gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed better performance than POS-</context>
</contexts>
<marker>Mori, Kurata, 2005</marker>
<rawString>Shinsuke Mori and Gakuto Kurata. 2005. Class-based variable memory length markov model. In Proceedings of the InterSpeech2005, pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Graham Neubig</author>
</authors>
<title>A pointwise approach to pronunciation estimation for a TTS front-end.</title>
<date>2011</date>
<booktitle>In Proceedings of the InterSpeech2011,</booktitle>
<pages>2181--2184</pages>
<contexts>
<context position="10527" citStr="Mori and Neubig, 2011" startWordPosition="1759" endWordPosition="1762">here F(·) denotes the frequency of a pair sequence in the corpus. In contrast to IM engines based on a word n-gram model, ours does not require an additional model describing relationships between words and pronunciations, and thus it is much simpler and more practical. Existing statistical IM engines only need an accurate automatic word segmenter to estimate the parameters of the word n-gram model. As the equation above shows, our pair-based engine also needs an accurate way of automatically estimating pronunciation (phoneme sequences). However, recently an automatic pronunciation estimator (Mori and Neubig, 2011) that delivers as accurate as state-of-the-art word segmenters has been proposed. As we explain in Section 6, in our experiments both our IM engine and existing ones delivered accuracy of 91%. 3.2 Enumerating Substrings as Candidate Words Essentially, the IM engine which we have explained above does not have the ability to enumerate words which are unknown to the word segmenter and the pronunciation estimator used to build the training data. The aim of our research is to gather language information from user behavior as they use an IM. So we extend the basic IM engine to enumerate all the subs</context>
<context position="14792" citStr="Mori and Neubig, 2011" startWordPosition="2552" endWordPosition="2555">nally, word n-gram frequencies must be calculated using floating point numbers instead of integers. 3.2.3 Pseudo-Stochastically Tagged Corpora We can annotate a word with its all possible pronunciations and their probabilities, as is done in an SSC. We call a corpus containing sequences of words (w1w2 • • • wi • • • ) annotated with a sequence of pairs of a pronunciation and its probability ((yi,1,pi,1), (yi,2,pi,2), • • •, where ∑j pi,j = 1, for bi) a stochastically tagged corpus (STC)3. We can estimate these probabilities using an LR model built from sentences annotated with pronunciations (Mori and Neubig, 2011). Similar to pSSC we then produce a pseudostochastically tagged sentence (pSTC) from an STC as follows: • For each wi in the sentence 1. generate a random number 0 &lt; p &lt; 1, 2. annotate wi with its j-th phoneme sequence yi,j, where ∑j−1 1 pi,j &lt; p &lt; ∑j1 pi,j Now we have a corpus in the same format as a standard corpus annotated with variable pronunciation. By estimating the parameters in Equation (1) from a pSTC derived from a pSSC, our IM engine can also suggest OOV word candidates with various possible segmentation and pronunciations without incurring high computational costs. 3.2.4 Suggestio</context>
</contexts>
<marker>Mori, Neubig, 2011</marker>
<rawString>Shinsuke Mori and Graham Neubig. 2011. A pointwise approach to pronunciation estimation for a TTS front-end. In Proceedings of the InterSpeech2011, pages 2181–2184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Graham Neubig</author>
</authors>
<title>Language resource addition: Dictionary or corpus?</title>
<date>2014</date>
<booktitle>In Proceedings of the Nineth International Conference on Language Resources and Evaluation,</booktitle>
<pages>1631--1636</pages>
<contexts>
<context position="2164" citStr="Mori and Neubig, 2014" startWordPosition="341" endWordPosition="344">es, this ambiguity poses an additional problem for NLP tasks in these languages. To make matters worse, the domains of the available training data often differ from domains where there is a high demand for NLP, which causes a severe degradation in WS performance. Examples include ma&apos;This work was done when the first author was at Kyoto University. chine translation of patents, text mining of medical texts, and marketing on the micro-blog site Twitter&apos;. Some papers have reported low accuracy on WS or the joint task of WS and part-of-speech (POS) tagging of Japanese or Chinese in these domains (Mori and Neubig, 2014; Kaji and Kitsuregawa, 2014; Liu et al., 2014) To cope with this problem, we propose a way to collect information from people as they type Japanese or Chinese on computers. These languages use far more characters than the number of keys on a keyboard, so users use software called an input method (IM) to type text in these languages. Unlike written texts in these languages, which lack word boundary information, text entered with an IM can provide word boundary information that can used by NLP systems. As we show in this paper, logs collected from IMs are a valuable source of word boundary info</context>
<context position="6031" citStr="Mori and Neubig, 2014" startWordPosition="992" endWordPosition="995"> classification has been shown to be as accurate as sequence-based methods and fast enough to make active learning practically possible (Neubig et al., 2011). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the same sentence, it is straightforward to train the model from partially annotated sentences. We adopt this WS system for our experiments. Along with the evolution of models, the NLP community has become increasingly aware of the importance of language resources (Neubig and Mori, 2010; Mori and Neubig, 2014). So Mori and Oda (2009) proposed to incorpolate dictionaries for human into a WS system with a different word definition. CRFs were also extended to enable training from partially annotated sentences (Tsuboi et al., 2008). When using partially annotated sentences for WS training data, word boundary information exists only between some character pairs and is absent for others. This extension was adopted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS perf</context>
</contexts>
<marker>Mori, Neubig, 2014</marker>
<rawString>Shinsuke Mori and Graham Neubig. 2014. Language resource addition: Dictionary or corpus? In Proceedings of the Nineth International Conference on Language Resources and Evaluation, pages 1631– 1636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Hiroki Oda</author>
</authors>
<title>Automatic word segmentation using three types of dictionaries.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eighth International Conference Pacific Association for Computational Linguistics,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="6055" citStr="Mori and Oda (2009)" startWordPosition="997" endWordPosition="1000">own to be as accurate as sequence-based methods and fast enough to make active learning practically possible (Neubig et al., 2011). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the same sentence, it is straightforward to train the model from partially annotated sentences. We adopt this WS system for our experiments. Along with the evolution of models, the NLP community has become increasingly aware of the importance of language resources (Neubig and Mori, 2010; Mori and Neubig, 2014). So Mori and Oda (2009) proposed to incorpolate dictionaries for human into a WS system with a different word definition. CRFs were also extended to enable training from partially annotated sentences (Tsuboi et al., 2008). When using partially annotated sentences for WS training data, word boundary information exists only between some character pairs and is absent for others. This extension was adopted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs use</context>
<context position="20425" citStr="Mori and Oda, 2009" startWordPosition="3494" endWordPosition="3497">ult they lose context information. The second problem is noise. Word boundary information is unreliable even when it is present because of mistakenly selected conversions or words entered separately. From these observations, the IM logs are treated as partially segmented sentence fragments that include noise. 5 Word Segmentation Using Input Method Logs In this section we first explain various ways to generate language resources for a word segmenter from IM logs. We then describe an automatic word segmenter which utilizes these resources. In the examples below we use the three-valued notation (Mori and Oda, 2009) to denote partial segmentation as follows: |: there is a word boundary, - : there is not a word boundary, : there is no information. 5.1 Input Method Logs as Language Resources The phoneme sequences and edit results in the final selection themselves are considered to be partially segmented sentences. We call the corpus generated directly from the logs “Log-as-is.” Examples in Table 1 are converted as following. ✓ Example of Log-as-is (12 annotations) ✏ 横-ア-リ|に と く-ら-っ|ベ-ル 比-べ|る 安-め|か|と ✒ ✑ Here the number of annotations is the sum of “-” and “|”. In this example, one entry corresponds to one </context>
</contexts>
<marker>Mori, Oda, 2009</marker>
<rawString>Shinsuke Mori and Hiroki Oda. 2009. Automatic word segmentation using three types of dictionaries. In Proceedings of the Eighth International Conference Pacific Association for Computational Linguistics, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Daisuke Takuma</author>
</authors>
<title>Word n-gram probability estimation from a Japanese raw corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eighth International Conference on Speech and Language Processing,</booktitle>
<pages>1037--1040</pages>
<contexts>
<context position="11284" citStr="Mori and Takuma, 2004" startWordPosition="1890" endWordPosition="1893">ur IM engine and existing ones delivered accuracy of 91%. 3.2 Enumerating Substrings as Candidate Words Essentially, the IM engine which we have explained above does not have the ability to enumerate words which are unknown to the word segmenter and the pronunciation estimator used to build the training data. The aim of our research is to gather language information from user behavior as they use an IM. So we extend the basic IM engine to enumerate all the substrings in a corpus with all possible pronunciations. For that purpose, we adopt the notion of a stochastically segmented corpus (SSC) (Mori and Takuma, 2004) and extend it to the pronunciation annotation to words. 3.2.1 Stochastically Segmented Corpora An SSC is defined as a combination of a raw corpus Cr (hereafter referred to as the character sequence xnr 1 ) and word boundary probabilities of the form Pi, which is the probability that a word boundary exists between two characters xi and xi+1. These probabilities are estimated by a model based on logistic regression (LR)(Fan et al., 2008) trained on a manually segmented corpus referring to the same features as those used in (Neubig et al., 2011). Since there are word boundaries before the first </context>
<context position="13352" citStr="Mori and Takuma, 2004" startWordPosition="2300" endWordPosition="2303">, 1 &lt; bm &lt; n − 1; b1 = i + 1; en = k) (See Figure 1 for an example). The word ngram frequency of a word sequence fr(wn1) in the SSC is defined by the summation of the stochastic frequency at each occurrence of the character sequence of the word sequence wn1 over all of the occurrences in the SSC:  ∑ ∏n fr(wn 1) = Pi  (i,en1 )EOn m=1 where en1 = (e1, e2, , en) and On = {(i, eem n1 bm = wm,1 &lt; m &lt; n}. We calculate word n-gram probabilities by dividing word n-gram frequencies by word (n − 1)- gram frequencies. For a detailed explanation and a mathematical proof of this method, please refer to (Mori and Takuma, 2004). 3.2.2 Pseudo-Stochastically Segmented Corpora The computational costs (in terms of both time and space) for calculating an n-gram model from an SSC are very high2, so it is not a practical technique for implementing an IM engine. In order to reduce the computational costs we approximate an SSC using a deterministically tagged corpus, which is called a pseudo-stochastically segmented corpus (pSSC) (Kameko et al., 2015). The following is the method for producing a pSSC from an SSC. • For i = 1 to nr − 1 1. output a character xi, 2. generate a random number 0 &lt; p &lt; 1, 3. output a word boundary </context>
</contexts>
<marker>Mori, Takuma, 2004</marker>
<rawString>Shinsuke Mori and Daisuke Takuma. 2004. Word n-gram probability estimation from a Japanese raw corpus. In Proceedings of the Eighth International Conference on Speech and Language Processing, pages 1037–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Tsuchiya Masatoshi</author>
<author>Osamu Yamaji</author>
<author>Makoto Nagao</author>
</authors>
<title>Kana-kanji conversion by a stochastic model.</title>
<date>1999</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<volume>40</volume>
<issue>7</issue>
<note>(in Japanese).</note>
<contexts>
<context position="7012" citStr="Mori et al., 1999" startWordPosition="1159" endWordPosition="1162">ent for others. This extension was adopted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs used in this paper are also classified as natural annotations, but contain much more noise. In addition, we need an IM that is specifically designed to collect logs as natural annotations. Server design is the most important factor in capturing information from IM logs. The most popular IM servers are based on statistical language modeling (Mori et al., 1999; Chen and Lee, 2000; Maeta and Mori, 2012). Their parameters are trained from manually segmented sentences whose words are annotated with phoneme sequences, and from sentences automatically annotated with NLP tools which are also based on machine learning models trained on the annotated sentences. Thus normal IM servers are not capable of presenting out-of-vocabulary (OOV) words (which provide large amounts of information on word boundaries) as conversion candidates. To make our IM server capable of presenting OOV words, we extend a statistical IM server based on (Mori et al., 2006), and ensu</context>
</contexts>
<marker>Mori, Masatoshi, Yamaji, Nagao, 1999</marker>
<rawString>Shinsuke Mori, Tsuchiya Masatoshi, Osamu Yamaji, and Makoto Nagao. 1999. Kana-kanji conversion by a stochastic model. Transactions of Information Processing Society of Japan, 40(7):2946–2953. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Daisuke Takuma</author>
<author>Gakuto Kurata</author>
</authors>
<title>Phoneme-to-text transcription system with an infinite vocabulary.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics,</booktitle>
<pages>729--736</pages>
<contexts>
<context position="7602" citStr="Mori et al., 2006" startWordPosition="1253" endWordPosition="1256"> modeling (Mori et al., 1999; Chen and Lee, 2000; Maeta and Mori, 2012). Their parameters are trained from manually segmented sentences whose words are annotated with phoneme sequences, and from sentences automatically annotated with NLP tools which are also based on machine learning models trained on the annotated sentences. Thus normal IM servers are not capable of presenting out-of-vocabulary (OOV) words (which provide large amounts of information on word boundaries) as conversion candidates. To make our IM server capable of presenting OOV words, we extend a statistical IM server based on (Mori et al., 2006), and ensure that it is computationally efficient enough for practical use by the public. The target domain in our experiments is Twitter, a site where users post short messages called tweets. Since tweets are an immediate and powerful reflection of public attitudes and social trends, there have been numerous attempts at extracting information from them. Examples include information analysis of disasters (Sakai et al., 2010), estimation of depressive tendencies (Tsugawa et al., 2013), speech diarization (Higashinaka et al., 2011), and many others. These works require preprocessing of tweets wi</context>
</contexts>
<marker>Mori, Takuma, Kurata, 2006</marker>
<rawString>Shinsuke Mori, Daisuke Takuma, and Gakuto Kurata. 2006. Phoneme-to-text transcription system with an infinite vocabulary. In Proceedings of the 21st International Conference on Computational Linguistics, pages 729–736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-DP backwardA* n-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="4589" citStr="Nagata, 1994" startWordPosition="753" endWordPosition="754">inguistics. sion of our IM. The three main contributions of this paper are: • an IM server that proposes word candidates which are not included in the vocabulary (Section 3), • a publicly usable IM that logs user behavior (such as typing and selection of word candidates) (Section 4), • a method for improving word segmentation by using these logs (Section 5). To the best of our knowledge, this is the first paper proposing a method for using IM logs to successfully improve WS. 2 Related Work The main focus of this paper is WS. Corpus-based, or empirical, methods were proposed in the early 90’s (Nagata, 1994). Then (Mori and Kurata, 2005) extended it by lexicalizing the states like many researches in that era, grouping the wordPOS pairs into clusters inspired by the class-based n-gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showe</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backwardA* n-best search algorithm. In Proceedings of the 15th International Conference on Computational Linguistics, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Shinsuke Mori</author>
</authors>
<title>Wordbased partial annotation for efficient corpus construction.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation,</booktitle>
<pages>2723--2727</pages>
<contexts>
<context position="6007" citStr="Neubig and Mori, 2010" startWordPosition="988" endWordPosition="991">thod based on pointwise classification has been shown to be as accurate as sequence-based methods and fast enough to make active learning practically possible (Neubig et al., 2011). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the same sentence, it is straightforward to train the model from partially annotated sentences. We adopt this WS system for our experiments. Along with the evolution of models, the NLP community has become increasingly aware of the importance of language resources (Neubig and Mori, 2010; Mori and Neubig, 2014). So Mori and Oda (2009) proposed to incorpolate dictionaries for human into a WS system with a different word definition. CRFs were also extended to enable training from partially annotated sentences (Tsuboi et al., 2008). When using partially annotated sentences for WS training data, word boundary information exists only between some character pairs and is absent for others. This extension was adopted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and</context>
</contexts>
<marker>Neubig, Mori, 2010</marker>
<rawString>Graham Neubig and Shinsuke Mori. 2010. Wordbased partial annotation for efficient corpus construction. In Proceedings of the Seventh International Conference on Language Resources and Evaluation, pages 2723–2727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Yosuke Nakata</author>
<author>Shinsuke Mori</author>
</authors>
<title>Pointwise prediction for robust, adaptable Japanese morphological analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>529--533</pages>
<contexts>
<context position="1458" citStr="Neubig et al., 2011" startWordPosition="221" endWordPosition="224"> We conducted word segmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain. 1 Introduction The first step of almost all natural language processing (NLP) for languages with ambiguous word boundaries (such as Japanese and Chinese) is solving the problem of word identification ambiguity. This task is called word segmentation (WS) and the accuracy of state-of-the-art methods based on machine learning techniques is more than 98% for Japanese and 95% for Chinese (Neubig et al., 2011; Yang and Vozila, 2014). Compared to languages like English with clear word boundaries, this ambiguity poses an additional problem for NLP tasks in these languages. To make matters worse, the domains of the available training data often differ from domains where there is a high demand for NLP, which causes a severe degradation in WS performance. Examples include ma&apos;This work was done when the first author was at Kyoto University. chine translation of patents, text mining of medical texts, and marketing on the micro-blog site Twitter&apos;. Some papers have reported low accuracy on WS or the joint </context>
<context position="5566" citStr="Neubig et al., 2011" startWordPosition="919" endWordPosition="922">imilar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed better performance than POS-based Markov models (Kudo et al., 2004). The training time of sequencebased methods tends to be long, especially when we use partially annotated data. Thus a simple method based on pointwise classification has been shown to be as accurate as sequence-based methods and fast enough to make active learning practically possible (Neubig et al., 2011). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the same sentence, it is straightforward to train the model from partially annotated sentences. We adopt this WS system for our experiments. Along with the evolution of models, the NLP community has become increasingly aware of the importance of language resources (Neubig and Mori, 2010; Mori and Neubig, 2014). So Mori and Oda (2009) proposed to incorpolate dictionaries for human into a WS system with a different word definition. CRFs were al</context>
<context position="11833" citStr="Neubig et al., 2011" startWordPosition="1985" endWordPosition="1988">tion of a stochastically segmented corpus (SSC) (Mori and Takuma, 2004) and extend it to the pronunciation annotation to words. 3.2.1 Stochastically Segmented Corpora An SSC is defined as a combination of a raw corpus Cr (hereafter referred to as the character sequence xnr 1 ) and word boundary probabilities of the form Pi, which is the probability that a word boundary exists between two characters xi and xi+1. These probabilities are estimated by a model based on logistic regression (LR)(Fan et al., 2008) trained on a manually segmented corpus referring to the same features as those used in (Neubig et al., 2011). Since there are word boundaries before the first character and after the last character of the corpus, P0 = Pnr = 1. Then word n-gram frequencies on an SSC are calculated as follows: Word 0-gram frequency: This is defined as the expected number of words in the SSC: Word n-gram frequency (n &gt; 1): Consider the situation in which a word sequence wn1 occurs in the SSC as a subsequence beginning at the (i + 1)-th character and ending at the k-th character and each word wm in the word sequence is equal to the character sequence beginning at the P(w, yl1), m+1 P(w, yl1) = i=1 A·) = 1 + nr−1� Pi. i=</context>
<context position="23440" citStr="Neubig et al., 2011" startWordPosition="4006" endWordPosition="4010">-as-is. However, we expect that the annotated tweets do not include mistaken boundaries or conversions that were discarded. Obviously we can combine Log-chunk and Log-mconv to avoid both the fragmentation and noise problems. This combination is referred to as “Log-chunk-mconv.” 5.2 Training a Word Segmenter on Logs The IM logs give us partially segmented sentence fragments, so we need a word segmenter capable of learning from them. We can use a word segmenter based on a sequence classifier (Tsuboi et al., 2008; Yang and Vozila, 2014; Jiang et al., 2013) or one based on a pointwise classifier (Neubig et al., 2011). Although both types are viable, we adopt the latter in the experiments because it requires much less training time while delivering comparable accuracy. Here is a brief explanation of the word segmenter based on the pointwise method. For more detail the reader may refer to (Neubig et al., 2011). The input is an unsegmented character sequence X = x1x2 · · · xk. The word segmenter decides if there is a word boundary tz = 1 or not tz = 0 by using support vector machines (SVMs) (Fan et al., 2008)7. The features are character n-grams 6The results were stable for n. in the preliminary experiments.</context>
</contexts>
<marker>Neubig, Nakata, Mori, 2011</marker>
<rawString>Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011. Pointwise prediction for robust, adaptable Japanese morphological analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 529–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>562--568</pages>
<contexts>
<context position="5124" citStr="Peng et al., 2004" startWordPosition="845" endWordPosition="848">Corpus-based, or empirical, methods were proposed in the early 90’s (Nagata, 1994). Then (Mori and Kurata, 2005) extended it by lexicalizing the states like many researches in that era, grouping the wordPOS pairs into clusters inspired by the class-based n-gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed better performance than POS-based Markov models (Kudo et al., 2004). The training time of sequencebased methods tends to be long, especially when we use partially annotated data. Thus a simple method based on pointwise classification has been shown to be as accurate as sequence-based methods and fast enough to make active learning practically possible (Neubig et al., 2011). Since the pointwise method decides whether there is a word boundary or not between two characters without referring to other word boundary decisions in the </context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the 20th International Conference on Computational Linguistics, pages 562–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Ron</author>
<author>Yoram Singer</author>
<author>Naftali Tishby</author>
</authors>
<title>The power of amnesia: Learning probabilistic automata with variable memory length.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>25--117</pages>
<contexts>
<context position="4883" citStr="Ron et al., 1996" startWordPosition="801" endWordPosition="804">ethod for improving word segmentation by using these logs (Section 5). To the best of our knowledge, this is the first paper proposing a method for using IM logs to successfully improve WS. 2 Related Work The main focus of this paper is WS. Corpus-based, or empirical, methods were proposed in the early 90’s (Nagata, 1994). Then (Mori and Kurata, 2005) extended it by lexicalizing the states like many researches in that era, grouping the wordPOS pairs into clusters inspired by the class-based n-gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed better performance than POS-based Markov models (Kudo et al., 2004). The training time of sequencebased methods tends to be long, especially when we use partially annotated data. Thus a simple method based on pointwise classification has been shown to be as accurate as sequence-based methods</context>
</contexts>
<marker>Ron, Singer, Tishby, 1996</marker>
<rawString>Dana Ron, Yoram Singer, and Naftali Tishby. 1996. The power of amnesia: Learning probabilistic automata with variable memory length. Machine Learning, 25:117–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakai</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes Twitter users: Real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web, WWW ’10,</booktitle>
<pages>851--860</pages>
<contexts>
<context position="8030" citStr="Sakai et al., 2010" startWordPosition="1322" endWordPosition="1325">arge amounts of information on word boundaries) as conversion candidates. To make our IM server capable of presenting OOV words, we extend a statistical IM server based on (Mori et al., 2006), and ensure that it is computationally efficient enough for practical use by the public. The target domain in our experiments is Twitter, a site where users post short messages called tweets. Since tweets are an immediate and powerful reflection of public attitudes and social trends, there have been numerous attempts at extracting information from them. Examples include information analysis of disasters (Sakai et al., 2010), estimation of depressive tendencies (Tsugawa et al., 2013), speech diarization (Higashinaka et al., 2011), and many others. These works require preprocessing of tweets with NLP tools, and WS is the first step. So it is clear that there is strong demand for improving WS accuracy. Another reason why we have chosen Twitter for the test domain is that the tweets typed using our server are open and 1187 we can avoid privacy problems. Our method does not utilize any other characteristics of tweets. So it also works in other domains such as blogs. 3 Input Method Suggesting OOV Words In this section</context>
</contexts>
<marker>Sakai, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakai, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: Real-time event detection by social sensors. In Proceedings of the 19th International Conference on World Wide Web, WWW ’10, pages 851–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih William Gale Nancy Chang</author>
</authors>
<title>A stochastic finite-state wordsegmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="4981" citStr="Sproat and Chang, 1996" startWordPosition="818" endWordPosition="821">owledge, this is the first paper proposing a method for using IM logs to successfully improve WS. 2 Related Work The main focus of this paper is WS. Corpus-based, or empirical, methods were proposed in the early 90’s (Nagata, 1994). Then (Mori and Kurata, 2005) extended it by lexicalizing the states like many researches in that era, grouping the wordPOS pairs into clusters inspired by the class-based n-gram model (Brown et al., 1992), and making the history length variable like a POS tagger in English (Ron et al., 1996). In parallel, there were attempts at solving Chinese WS in a similar way (Sproat and Chang, 1996). WS or the joint task of WS and POS tagging can be seen as a sequence labeling problem. So conditional random fields (CRFs) (Peng et al., 2004; Lafferty et al., 2001) have been applied to this task and showed better performance than POS-based Markov models (Kudo et al., 2004). The training time of sequencebased methods tends to be long, especially when we use partially annotated data. Thus a simple method based on pointwise classification has been shown to be as accurate as sequence-based methods and fast enough to make active learning practically possible (Neubig et al., 2011). Since the poi</context>
</contexts>
<marker>Sproat, Chang, 1996</marker>
<rawString>Richard Sproat and Chilin Shih William Gale Nancy Chang. 1996. A stochastic finite-state wordsegmentation algorithm for Chinese. Computational Linguistics, 22(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
<author>Hisashi Kashima</author>
<author>Shinsuke Mori</author>
<author>Hiroki Oda</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Training conditional random fields using incomplete annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>897--904</pages>
<contexts>
<context position="6253" citStr="Tsuboi et al., 2008" startWordPosition="1029" endWordPosition="1032">ry or not between two characters without referring to other word boundary decisions in the same sentence, it is straightforward to train the model from partially annotated sentences. We adopt this WS system for our experiments. Along with the evolution of models, the NLP community has become increasingly aware of the importance of language resources (Neubig and Mori, 2010; Mori and Neubig, 2014). So Mori and Oda (2009) proposed to incorpolate dictionaries for human into a WS system with a different word definition. CRFs were also extended to enable training from partially annotated sentences (Tsuboi et al., 2008). When using partially annotated sentences for WS training data, word boundary information exists only between some character pairs and is absent for others. This extension was adopted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs used in this paper are also classified as natural annotations, but contain much more noise. In addition, we need an IM that is specifically designed to collect logs as natural annotations. Server desig</context>
<context position="23335" citStr="Tsuboi et al., 2008" startWordPosition="3987" endWordPosition="3990">otations) � 横-ア-リ|に � � As this example shows, Log-mconv contains short entries (fragmentation) like Log-as-is. However, we expect that the annotated tweets do not include mistaken boundaries or conversions that were discarded. Obviously we can combine Log-chunk and Log-mconv to avoid both the fragmentation and noise problems. This combination is referred to as “Log-chunk-mconv.” 5.2 Training a Word Segmenter on Logs The IM logs give us partially segmented sentence fragments, so we need a word segmenter capable of learning from them. We can use a word segmenter based on a sequence classifier (Tsuboi et al., 2008; Yang and Vozila, 2014; Jiang et al., 2013) or one based on a pointwise classifier (Neubig et al., 2011). Although both types are viable, we adopt the latter in the experiments because it requires much less training time while delivering comparable accuracy. Here is a brief explanation of the word segmenter based on the pointwise method. For more detail the reader may refer to (Neubig et al., 2011). The input is an unsegmented character sequence X = x1x2 · · · xk. The word segmenter decides if there is a word boundary tz = 1 or not tz = 0 by using support vector machines (SVMs) (Fan et al., 2</context>
</contexts>
<marker>Tsuboi, Kashima, Mori, Oda, Matsumoto, 2008</marker>
<rawString>Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki Oda, and Yuji Matsumoto. 2008. Training conditional random fields using incomplete annotations. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 897–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sho Tsugawa</author>
</authors>
<title>Yukiko Mogi, Yusuke Kikuchi, Fumio Kishino, Kazuyuki Fujita, Yuichi Itoh, and Hiroyuki Ohsaki.</title>
<date>2013</date>
<booktitle>VR’13,</booktitle>
<pages>1--4</pages>
<marker>Tsugawa, 2013</marker>
<rawString>Sho Tsugawa, Yukiko Mogi, Yusuke Kikuchi, Fumio Kishino, Kazuyuki Fujita, Yuichi Itoh, and Hiroyuki Ohsaki. 2013. On estimating depressive tendencies of Twitter users utilizing their tweet data. In VR’13, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Yang</author>
<author>Paul Vozila</author>
</authors>
<title>Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>90--98</pages>
<contexts>
<context position="1482" citStr="Yang and Vozila, 2014" startWordPosition="225" endWordPosition="228">gmentation experiments on tweets from Twitter, and showed that our method improves accuracy in this domain. Our method itself is domain-independent and only needs logs from the target domain. 1 Introduction The first step of almost all natural language processing (NLP) for languages with ambiguous word boundaries (such as Japanese and Chinese) is solving the problem of word identification ambiguity. This task is called word segmentation (WS) and the accuracy of state-of-the-art methods based on machine learning techniques is more than 98% for Japanese and 95% for Chinese (Neubig et al., 2011; Yang and Vozila, 2014). Compared to languages like English with clear word boundaries, this ambiguity poses an additional problem for NLP tasks in these languages. To make matters worse, the domains of the available training data often differ from domains where there is a high demand for NLP, which causes a severe degradation in WS performance. Examples include ma&apos;This work was done when the first author was at Kyoto University. chine translation of patents, text mining of medical texts, and marketing on the micro-blog site Twitter&apos;. Some papers have reported low accuracy on WS or the joint task of WS and part-of-s</context>
<context position="6518" citStr="Yang and Vozila, 2014" startWordPosition="1074" endWordPosition="1077">the NLP community has become increasingly aware of the importance of language resources (Neubig and Mori, 2010; Mori and Neubig, 2014). So Mori and Oda (2009) proposed to incorpolate dictionaries for human into a WS system with a different word definition. CRFs were also extended to enable training from partially annotated sentences (Tsuboi et al., 2008). When using partially annotated sentences for WS training data, word boundary information exists only between some character pairs and is absent for others. This extension was adopted in Chinese WS to make use of socalled natural annotations (Yang and Vozila, 2014; Jiang et al., 2013). In that work, tags in hyper-texts were regarded as annotations and used to improve WS performance. The IM logs used in this paper are also classified as natural annotations, but contain much more noise. In addition, we need an IM that is specifically designed to collect logs as natural annotations. Server design is the most important factor in capturing information from IM logs. The most popular IM servers are based on statistical language modeling (Mori et al., 1999; Chen and Lee, 2000; Maeta and Mori, 2012). Their parameters are trained from manually segmented sentence</context>
<context position="23358" citStr="Yang and Vozila, 2014" startWordPosition="3991" endWordPosition="3994"> � As this example shows, Log-mconv contains short entries (fragmentation) like Log-as-is. However, we expect that the annotated tweets do not include mistaken boundaries or conversions that were discarded. Obviously we can combine Log-chunk and Log-mconv to avoid both the fragmentation and noise problems. This combination is referred to as “Log-chunk-mconv.” 5.2 Training a Word Segmenter on Logs The IM logs give us partially segmented sentence fragments, so we need a word segmenter capable of learning from them. We can use a word segmenter based on a sequence classifier (Tsuboi et al., 2008; Yang and Vozila, 2014; Jiang et al., 2013) or one based on a pointwise classifier (Neubig et al., 2011). Although both types are viable, we adopt the latter in the experiments because it requires much less training time while delivering comparable accuracy. Here is a brief explanation of the word segmenter based on the pointwise method. For more detail the reader may refer to (Neubig et al., 2011). The input is an unsegmented character sequence X = x1x2 · · · xk. The word segmenter decides if there is a word boundary tz = 1 or not tz = 0 by using support vector machines (SVMs) (Fan et al., 2008)7. The features are</context>
</contexts>
<marker>Yang, Vozila, 2014</marker>
<rawString>Fan Yang and Paul Vozila. 2014. Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 90–98.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040613">
<title confidence="0.997219">
Arabic Diacritization with Recurrent Neural Networks
</title>
<author confidence="0.996611">
Yonatan Belinkov and James Glass
</author>
<affiliation confidence="0.9916225">
Massachusetts Institute of Technology
Computer Science and Artificial Intelligence Laboratory
</affiliation>
<email confidence="0.993531">
{belinkov, glass}@mit.edu
</email>
<sectionHeader confidence="0.997315" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997430625">
Arabic, Hebrew, and similar languages are typi-
cally written without diacritics, leading to ambigu-
ity and posing a major challenge for core language
processing tasks like speech recognition. Previous
approaches to automatic diacritization employed a
variety of machine learning techniques. However,
they typically rely on existing tools like morpho-
logical analyzers and therefore cannot be easily
extended to new genres and languages. We de-
velop a recurrent neural network with long short-
term memory layers for predicting diacritics in
Arabic text. Our language-independent approach
is trained solely from diacritized text without re-
lying on external tools. We show experimentally
that our model can rival state-of-the-art methods
that have access to additional resources.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999249571428571">
Hebrew, Arabic, and other languages based on the
Arabic script usually represent only consonants in
writing and do not mark vowels. In such writ-
ing systems, diacritics are used for marking short
vowels, gemination, and other phonetic units. In
practice, diacritics are usually restricted to specific
settings such as language teaching or to religious
texts. Faced with a non-diacritized word, readers
infer missing diacritics based on their prior knowl-
edge and the context of the word in order to re-
solve ambiguities. For example, Maamouri et al.
(2006) mention several types of ambiguity for the
Arabic string � s Elm, both within and
across part-of-speech tags, and at a grammatical
</bodyText>
<table confidence="0.7901621">
Word Gloss
Ealima he knew
Eulima it was known
Eal~ama he taught
Eilomu knowledge (def.nom)
... ...
EilomK knowledge (indef.gen)
Ealamu flag (def.nom)
... ...
EalamK flag (indef.gen)
</table>
<tableCaption confidence="0.998477">
Table 1: Possible diacritized forms for e
</tableCaption>
<bodyText confidence="0.989132423076923">
Elm.
level. In practice, a morphological analyzer like
MADA (Habash et al., 2009) produces at least 13
different diacritized forms for this word, a subset
of which is shown in Table 1.1
The ambiguity in Arabic orthography presents
a problem for many language processing tasks, in-
cluding acoustic modeling for speech recognition,
language modeling, text-to-speech, and morpho-
logical analysis. Automatic methods for diacriti-
zation aim to restore diacritics in a non-diacritized
text. While earlier work used rule-based meth-
ods, more recent studies attempted to learn a di-
acritization model from diacritized text. A vari-
ety of methods have been used, including hidden
Markov models, finite-state transducers, and max-
imum entropy – see the review in (Zitouni and
Sarikaya, 2009) – and more recently, deep neu-
ral networks (Al Sallab et al., 2014). In addi-
tion to learning from diacritized text, these meth-
ods typically rely on external resources such as
part-of-speech taggers and morphological analyz-
ers like the MADA tool (Habash and Rambow,
2007). However, building such resources is a
labor-intensive task and cannot be easily extended
to new languages, dialects, and domains.
</bodyText>
<footnote confidence="0.9933835">
1Arabic transliteration follows the Buckwalter scheme:
http://www.qamus.org/transliteration.htm.
</footnote>
<page confidence="0.859569">
2281
</page>
<note confidence="0.8628665">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2281–2285,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<table confidence="0.998595888888889">
Diacritic Transliteration Transcription
X~ a /a/
X� u /u/
X� i /i/
X~ F /an/
X� N /un
X� s K /in/
X~ Gemination
X� o No vowel
</table>
<tableCaption confidence="0.999498">
Table 2: Arabic diacritics.
</tableCaption>
<bodyText confidence="0.999934125">
In this work, we propose a diacritization method
based solely on diacritized text. We treat the prob-
lem as a sequence classification task, where each
character has a corresponding diacritic label. The
sequence is modeled with a recurrent neural net-
work whose input is a sequence of characters and
whose output is a probability distribution over the
diacritics. Any RNN architecture can be used in
this framework; here we focus on long short-term
memory (LSTM) networks, which have shown re-
cent success in a number of NLP tasks. We exper-
iment with several architectures and show that we
can achieve state-of-the-art results, without rely-
ing on external resources. Error analysis demon-
strates the benefit of using LSTM over simpler
neural networks.
</bodyText>
<sectionHeader confidence="0.95786" genericHeader="method">
2 Linguistic Background
</sectionHeader>
<bodyText confidence="0.999809666666667">
Languages based on the Arabic script typically
employ an abjad writing system, where each sym-
bol represents a consonant while vowels and other
phonetic units, commonly known as diacritics, are
usually omitted in writing. In modern standard
and classical Arabic, these include the short vow-
els a, u, and i, the case endings F, N, and K, the
gemination marker ~, and the silence marker o.2
Table 2, modified from (Habash et al., 2007), lists
the diacritics. Importantly, the gemination marker
~ can combine with short vowels and case endings
(e.g. Table 1, row 3).
</bodyText>
<sectionHeader confidence="0.996632" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.9999465">
We define the following sequence classification
task, similarly to (Zitouni and Sarikaya, 2009).
</bodyText>
<footnote confidence="0.992861">
2We also include the low-frequency superscript Alif ‘ that
is usually ignored due to its limitation to fixed lexical items.
</footnote>
<figureCaption confidence="0.999594">
Figure 1: An illustration of our network topology.
</figureCaption>
<bodyText confidence="0.9999348">
Let w = (wi, ..., wT) denote a sequence of char-
acters, where each character wt is associated with
a label lt. A label may represent 0, 1, or more di-
acritics, depending on the language. Assume fur-
ther that each character w in the alphabet is rep-
resented as a real-valued vector xw. This charac-
ter embedding may be learned during training or
fixed.
Our neural network has the following structure,
illustrated in Figure 1:
</bodyText>
<listItem confidence="0.999248833333333">
• Input layer: mapping the letter sequence w to
a vector sequence x.
• Hidden layer(s): mapping the vector se-
quence x to a hidden sequence h.
• Output layer: mapping each hidden vector ht
to a probability distribution over labels l.
</listItem>
<bodyText confidence="0.999705142857143">
During training, each sequence is fed into this
network to create a prediction for each character.
As errors are back-propagated down the network,
the weights at each layer are updated. During test-
ing, the learned weights are used in a forward step
to compute a prediction over the labels. We always
take the best predicted label for evaluation.
Hidden layer Our main system relies on long
short-term memory networks (LSTM) (Hochre-
iter and Schmidhuber, 1997; Graves et al., 2013).
Here we describe a single LSTM layer and refer
to Graves et al. (2013) for the extension to bidi-
rectional LSTM (B-LSTM) and to multiple layers.
The LSTM computes the hidden representation for
</bodyText>
<figure confidence="0.948179375">
h1,...,hT
Hidden layers
xw1,...,xwT
w1,...,wT
l1,...,lT
Output layer
Softmax
Input layer Embedding
</figure>
<page confidence="0.908269">
2282
</page>
<table confidence="0.979717">
Train Dev Test
Words 470K 81K 80K
Letters 2.6M 438K 434K
</table>
<tableCaption confidence="0.99671">
Table 3: Arabic diacritization corpus statistics.
</tableCaption>
<equation confidence="0.956234571428571">
input xt with the following iterative process:
it = Q(Wxixt + Whiht−1 + W,ict−1 + bi)
ft = Q(Wxfxt + Whfht−1 + W,fct−1 + bf)
ct = ft O ct−1+
it O tanh (Wx,xt + Wh,ht−1 + b,)
ot = Q(Wxoxt + Whoht−1 + W,oct + bo)
ht = ot O tanh(ct)
</equation>
<bodyText confidence="0.99997608">
where Q is the sigmoid function, O is element-
wise multiplication, and i, f, o, and c are input,
forget, output, and memory cell activation vectors.
The crucial element is the memory cell c that is
able to store and reuse long term dependencies
over the sequence. The W matrices and b bias vec-
tors are learned during training.
Implementation details The input layer maps
the character sequence to a sequence of letter vec-
tors, initialized randomly. We also tried initializ-
ing with letter vectors trained from raw text with
word2vec (Mikolov et al., 2013a; Mikolov et al.,
2013b), but did not notice any improvement, prob-
ably due to the small letter vocabulary size. The
input layer also stacks previous and future letter
vectors, enabling the model to learn contextual in-
formation. We use a letter embedding size of 10
and a window size of 5 characters, so the input
size is 110.
We experiment with several types of hidden lay-
ers, ranging from one feed-forward layer to mul-
tiple B-LSTM layers. We also add a linear pro-
jection after the input layer. This has the effect of
learning a new representation for the letter embed-
dings. The output layer is a Softmax over labels:
</bodyText>
<equation confidence="0.9887975">
exp(yt[l])
P(l|wt) = � l� exp(yt[l0])
</equation>
<bodyText confidence="0.995769875">
where yt = Whyht + by and yt[l] is the lth element
of yt.
Training is done with stochastic gradient de-
scent with momentum, optimizing the cross-
entropy objective function. Layer sizes and other
hyper-parameters are tuned on the Dev set. Our
implementation is based on Currennt (Weninger et
al., 2015).
</bodyText>
<table confidence="0.999651">
Model DER # params
All End
Feed-forward 11.76 22.90 63K
Feed-forward (large) 11.55 23.40 908K
LSTM 6.98 10.36 838K
B-LSTM 6.16 9.85 518K
2-layer B-LSTM 5.77 9.18 916K
3-layer B-LSTM 5.08 8.14 1,498K
</table>
<tableCaption confidence="0.9817495">
Table 4: Diacrtic error rates (DERs) on the Dev
set, over all diacritics and only at word ending.
</tableCaption>
<table confidence="0.998122">
MaxEnt (only lexical) 8.1
MaxEnt (full) 5.1
3-layer B-LSTM 4.85
</table>
<tableCaption confidence="0.979167">
Table 5: Results (DER) on the Test set. MaxEnt
results from (Zitouni and Sarikaya, 2009)
</tableCaption>
<sectionHeader confidence="0.999272" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999938866666667">
Data We extract diacritizied and non-diacritized
texts from the Arabic treebank, following the
Train/Dev/Test split in (Zitouni and Sarikaya,
2009). Table 3 provides statistics for the corpus.
Every character in our corpus has a label cor-
responding to 0, 1, or 2 diacritics, in the case of
the gemination marker combining with another di-
acritic. Thus the label set almost doubles. We
opted for this formulation due to its simplicity and
generalizability to other languages, even though
previous work reported improved results by first
predicting gemination and then all other diacrit-
ics (Zitouni and Sarikaya, 2009).
Results Table 4 shows the results of our models
on the Dev set in terms of the diacritic error rate
(DER). Clearly, LSTM models perform much bet-
ter than simple feed-forward networks. To make
the comparison fair, we increased the number of
parameters in the feed-forward model to match
that of the LSTM. In this setting, the LSTM is still
much better, indicating that it is far more success-
ful at exploiting the larger parameter set. Interest-
ingly, the bidirectional LSTM works better than a
unidirectional one, despite having less parameters.
Finally, deeper models achieve the best results.
On the Test set (Table 5), our 3-layer B-LSTM
model beats the lexical variant of Zitouni and
Sarikaya (2009) by 3.25% DER, a 40% error
reduction. Moreover, we outperform their best
model, which also used a segmenter and part-of-
</bodyText>
<page confidence="0.991658">
2283
</page>
<figureCaption confidence="0.883874">
Figure 2: A confusion matrix of errors made by
our system. ”#” marks word boundary. Best
viewed in color.
</figureCaption>
<bodyText confidence="0.997939666666667">
speech tagger. This shows that our model can ef-
fectively learn to diacritize without relying on any
resources other than diacritized text.
Finally, some studies report work on a
Train/Test data split, without a dedicated Dev
set (Zitouni et al., 2006; Habash and Rambow,
2007; Rashwan et al., 2011; Al Sallab et al., 2014).
We were reluctant to follow this setting so we per-
formed all development on the Dev set of (Zi-
touni and Sarikaya, 2009). Still, we ran our best
model on the Train/Test split and achieved a DER
of 5.39% on all diacritics and 8.74% on case end-
ings. The first result is behind the state-of-the-
art (Al Sallab et al., 2014) by 2% but the second
one is better by 3%. Given that we did not tune the
system for this data set, this result is encouraging.
Error Analysis A quantitative analysis of the er-
rors produced by one of our models on the Dev set
is shown in Figure 2. The heat map denotes the
number of errors produced. The major source of
errors comes from confusing the short vowels a,
i, and u, among themselves and with no diacritic.
This is expected due to the high rate of short vow-
els in Arabic compared to other diacritics. It also
explains why methods that take the confusion ma-
trix into account in their classification algorithm
do quite well (Al Sallab et al., 2014).
We also analyzed some errors qualitatively. Fig-
ure 3 shows the errors produced by several of our
diacritization models on a sample sentence. In-
</bodyText>
<table confidence="0.985161523809523">
Model Diacritization
Gold AiEotabara Almudiyru AlEAm—u l ”
Aln—ahAri ” juborAn tuwayoniy— Aan
Alt—a$okiylAti AlqaDA}iy—apa jA’at lita-
moyiyEi milaf—i maHaT—api Al ” Aim. tiy
. fiy . ”
AiEotabara Almudiyru AlEAm—u l ”
Aln—ahAr ” jaborAn tuwayoniy Aan
Alt—a$okiylAti AlqaDA}iy—api jA’at
litamayiyEi malaf—i maHaT—api Al ” A m
. tiy . fiy . ”
LSTM AiEotabara Almudiyru AlEAm—u l ”
Aln—ahAri ” juborAn t w yoniy Ain
Alt—a$okiylAti AlqaDA}iy—apa jA’at
litamoyiyEa milaf—i maHaT—api Al ” Aim
. tiy . fiy . ”
B-LSTM AiEotabara Almudiyru AlEAm—u l ”
Aln—ahAri ” juborAn t wayoniy Aan
Alt—a$okiylAti AlqaDA}iy—apa jA’at lita-
moyiyEi milaf—i maHaT—api Al ” Aim. tiy
. fiy . ”
</table>
<figureCaption confidence="0.924460333333333">
Figure 3: Sample errors by selected diacritization
models. Wrong predicted diacritics are underlined
and in red; missing diacritics are noted by under-
score. Translation: ”The editor of An Nahar, Ge-
bran Tueni, thought that the judicial formations
came to dilute the issue of MTV station”.
</figureCaption>
<bodyText confidence="0.9999297">
terestingly, the simple feed-forward model fails
to predict the correct case ending on the word
AlqaDA}iy—ap (“judicial”), while both LSTM
models succeed. This may indicate that LSTM in-
deed captures the kind of long-distance dependen-
cies that are responsible for case marking. Other
errors are more difficult to explain, but note that all
models struggle with the proper name tuwayoniy—
(“Tueini”), which is difficult to solve without ex-
ternal resources.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.957711722222222">
In this work, we develop a recurrent neural net-
work that predicts diacritics in non-diacritized
texts. Our model is language agnostic: it is
trained solely from diacritized text without relying
on additional resources. Using LSTM units, we
demonstrate that our model can effectively learn
to diacritize Arabic texts and rivals state-of-the-art
methods that rely on language-specific tools.
In future work, we intend to incorporate our di-
acritization system in a speech recognizer. Recent
work has shown improvements in Arabic speech
recognition by diacritizing with MADA (Al Hanai
and Glass, 2014). Since creating such tools is a
labor-intensive task, we expect our diacritization
approach to promote the development of speech
recognizers for other languages and dialects.
Feed-
forward
</bodyText>
<page confidence="0.945993">
2284
</page>
<sectionHeader confidence="0.999002" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.809487">
This research was supported by the Qatar Comput-
ing Research Institute (QCRI).
</bodyText>
<sectionHeader confidence="0.994775" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999132431034483">
Tuka Al Hanai and James Glass. 2014. Lexical Mod-
eling for Arabic ASR: A Systematic Approach. In
Proceedings of INTERSPEECH.
Ahmad Al Sallab, Mohsen Rashwan, Hazem
M. Raafat, and Ahmed Rafea. 2014. Auto-
matic Arabic diacritics restoration based on deep
nets. In Proceedings of the EMNLP 2014 Workshop
on Arabic Natural Language Processing (ANLP).
Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In Proceedings of ICASSP.
Nizar Habash and Owen Rambow. 2007. Arabic Dia-
critization through Full Morphological Tagging. In
Proceedings of HLT-NAACL.
Nizar Habash, Abdelhadi Soudi, and Timothy Buck-
walter. 2007. On Arabic Transliteration. In Ab-
delhadi Soudi, G¨unter Neumann, and Antal Van den
Bosch, editors, Arabic Computational Morphology:
Knowledge-based and Empirical Methods, pages
15–22. Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A Toolkit for Arabic Tokeniza-
tion, Diacritization, Morphological Disambiguation,
POS Tagging, Stemming and Lemmatization. In
Proceedings of the Second International Conference
on Arabic Language Resources and Tools.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long Short-Term Memory. Neural computation,
9(8):1735–1780.
Mohamed Maamouri, Ann Bies, and Seth Kulick.
2006. Diacritization: A challenge to Arabic tree-
bank annotation and parsing. In Proceedings of the
British Computer Society Arabic NLP/MT Confer-
ence.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of Work-
shop at ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed Repre-
sentations of Words and Phrases and their Composi-
tionality. In Proceedings of NIPS.
M.A.A. Rashwan, M.A.S.A.A. Al-Badrashiny, M. At-
tia, S.M. Abdou, and A. Rafea. 2011. A Stochastic
Arabic Diacritizer Based on a Hybrid of Factorized
and Unfactorized Textual Features. IEEE Transac-
tions on Audio, Speech, and Language Processing,
19(1):166–175, Jan.
Felix Weninger, Johannes Bergmann, and Bj¨orn
Schuller. 2015. Introducing CURRENNT: The Mu-
nich Open-Source CUDA RecurREnt Neural Net-
work Toolkit. JMLR, 16:547–551.
Imed Zitouni and Ruhi Sarikaya. 2009. Arabic Dia-
critic Restoration Approach Based on Maximum En-
tropy Models. Comput. Speech Lang., 23(3).
Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.
2006. Maximum Entropy Based Restoration of Ara-
bic Diacritics. In Proceedings of COLING andACL.
</reference>
<page confidence="0.991156">
2285
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.628234">
<title confidence="0.992871">Arabic Diacritization with Recurrent Neural Networks</title>
<author confidence="0.670932">Belinkov</author>
<affiliation confidence="0.8973775">Massachusetts Institute of Computer Science and Artificial Intelligence</affiliation>
<abstract confidence="0.999143117647059">Arabic, Hebrew, and similar languages are typically written without diacritics, leading to ambiguity and posing a major challenge for core language processing tasks like speech recognition. Previous approaches to automatic diacritization employed a variety of machine learning techniques. However, they typically rely on existing tools like morphological analyzers and therefore cannot be easily extended to new genres and languages. We develop a recurrent neural network with long shortterm memory layers for predicting diacritics in Arabic text. Our language-independent approach is trained solely from diacritized text without relying on external tools. We show experimentally that our model can rival state-of-the-art methods that have access to additional resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tuka Al Hanai</author>
<author>James Glass</author>
</authors>
<title>Lexical Modeling for Arabic ASR: A Systematic Approach.</title>
<date>2014</date>
<booktitle>In Proceedings of INTERSPEECH.</booktitle>
<marker>Hanai, Glass, 2014</marker>
<rawString>Tuka Al Hanai and James Glass. 2014. Lexical Modeling for Arabic ASR: A Systematic Approach. In Proceedings of INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Al Sallab</author>
<author>Mohsen Rashwan</author>
<author>Hazem M Raafat</author>
<author>Ahmed Rafea</author>
</authors>
<title>Automatic Arabic diacritics restoration based on deep nets.</title>
<date>2014</date>
<booktitle>In Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing (ANLP).</booktitle>
<contexts>
<context position="2752" citStr="Sallab et al., 2014" startWordPosition="411" endWordPosition="414"> presents a problem for many language processing tasks, including acoustic modeling for speech recognition, language modeling, text-to-speech, and morphological analysis. Automatic methods for diacritization aim to restore diacritics in a non-diacritized text. While earlier work used rule-based methods, more recent studies attempted to learn a diacritization model from diacritized text. A variety of methods have been used, including hidden Markov models, finite-state transducers, and maximum entropy – see the review in (Zitouni and Sarikaya, 2009) – and more recently, deep neural networks (Al Sallab et al., 2014). In addition to learning from diacritized text, these methods typically rely on external resources such as part-of-speech taggers and morphological analyzers like the MADA tool (Habash and Rambow, 2007). However, building such resources is a labor-intensive task and cannot be easily extended to new languages, dialects, and domains. 1Arabic transliteration follows the Buckwalter scheme: http://www.qamus.org/transliteration.htm. 2281 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2281–2285, Lisbon, Portugal, 17-21 September 2015. c�2015 Association</context>
<context position="10765" citStr="Sallab et al., 2014" startWordPosition="1747" endWordPosition="1750">-LSTM model beats the lexical variant of Zitouni and Sarikaya (2009) by 3.25% DER, a 40% error reduction. Moreover, we outperform their best model, which also used a segmenter and part-of2283 Figure 2: A confusion matrix of errors made by our system. ”#” marks word boundary. Best viewed in color. speech tagger. This shows that our model can effectively learn to diacritize without relying on any resources other than diacritized text. Finally, some studies report work on a Train/Test data split, without a dedicated Dev set (Zitouni et al., 2006; Habash and Rambow, 2007; Rashwan et al., 2011; Al Sallab et al., 2014). We were reluctant to follow this setting so we performed all development on the Dev set of (Zitouni and Sarikaya, 2009). Still, we ran our best model on the Train/Test split and achieved a DER of 5.39% on all diacritics and 8.74% on case endings. The first result is behind the state-of-theart (Al Sallab et al., 2014) by 2% but the second one is better by 3%. Given that we did not tune the system for this data set, this result is encouraging. Error Analysis A quantitative analysis of the errors produced by one of our models on the Dev set is shown in Figure 2. The heat map denotes the number </context>
</contexts>
<marker>Sallab, Rashwan, Raafat, Rafea, 2014</marker>
<rawString>Ahmad Al Sallab, Mohsen Rashwan, Hazem M. Raafat, and Ahmed Rafea. 2014. Automatic Arabic diacritics restoration based on deep nets. In Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing (ANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Abdel-rahman Mohamed</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Speech recognition with deep recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="6287" citStr="Graves et al., 2013" startWordPosition="985" endWordPosition="988">ing the vector sequence x to a hidden sequence h. • Output layer: mapping each hidden vector ht to a probability distribution over labels l. During training, each sequence is fed into this network to create a prediction for each character. As errors are back-propagated down the network, the weights at each layer are updated. During testing, the learned weights are used in a forward step to compute a prediction over the labels. We always take the best predicted label for evaluation. Hidden layer Our main system relies on long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013). Here we describe a single LSTM layer and refer to Graves et al. (2013) for the extension to bidirectional LSTM (B-LSTM) and to multiple layers. The LSTM computes the hidden representation for h1,...,hT Hidden layers xw1,...,xwT w1,...,wT l1,...,lT Output layer Softmax Input layer Embedding 2282 Train Dev Test Words 470K 81K 80K Letters 2.6M 438K 434K Table 3: Arabic diacritization corpus statistics. input xt with the following iterative process: it = Q(Wxixt + Whiht−1 + W,ict−1 + bi) ft = Q(Wxfxt + Whfht−1 + W,fct−1 + bf) ct = ft O ct−1+ it O tanh (Wx,xt + Wh,ht−1 + b,) ot = Q(Wxoxt + Whoht−</context>
</contexts>
<marker>Graves, Mohamed, Hinton, 2013</marker>
<rawString>Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic Diacritization through Full Morphological Tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2955" citStr="Habash and Rambow, 2007" startWordPosition="443" endWordPosition="446">tization aim to restore diacritics in a non-diacritized text. While earlier work used rule-based methods, more recent studies attempted to learn a diacritization model from diacritized text. A variety of methods have been used, including hidden Markov models, finite-state transducers, and maximum entropy – see the review in (Zitouni and Sarikaya, 2009) – and more recently, deep neural networks (Al Sallab et al., 2014). In addition to learning from diacritized text, these methods typically rely on external resources such as part-of-speech taggers and morphological analyzers like the MADA tool (Habash and Rambow, 2007). However, building such resources is a labor-intensive task and cannot be easily extended to new languages, dialects, and domains. 1Arabic transliteration follows the Buckwalter scheme: http://www.qamus.org/transliteration.htm. 2281 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2281–2285, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Diacritic Transliteration Transcription X~ a /a/ X� u /u/ X� i /i/ X~ F /an/ X� N /un X� s K /in/ X~ Gemination X� o No vowel Table 2: Arabic diacritics. In this work, we </context>
<context position="10718" citStr="Habash and Rambow, 2007" startWordPosition="1738" endWordPosition="1741"> results. On the Test set (Table 5), our 3-layer B-LSTM model beats the lexical variant of Zitouni and Sarikaya (2009) by 3.25% DER, a 40% error reduction. Moreover, we outperform their best model, which also used a segmenter and part-of2283 Figure 2: A confusion matrix of errors made by our system. ”#” marks word boundary. Best viewed in color. speech tagger. This shows that our model can effectively learn to diacritize without relying on any resources other than diacritized text. Finally, some studies report work on a Train/Test data split, without a dedicated Dev set (Zitouni et al., 2006; Habash and Rambow, 2007; Rashwan et al., 2011; Al Sallab et al., 2014). We were reluctant to follow this setting so we performed all development on the Dev set of (Zitouni and Sarikaya, 2009). Still, we ran our best model on the Train/Test split and achieved a DER of 5.39% on all diacritics and 8.74% on case endings. The first result is behind the state-of-theart (Al Sallab et al., 2014) by 2% but the second one is better by 3%. Given that we did not tune the system for this data set, this result is encouraging. Error Analysis A quantitative analysis of the errors produced by one of our models on the Dev set is show</context>
</contexts>
<marker>Habash, Rambow, 2007</marker>
<rawString>Nizar Habash and Owen Rambow. 2007. Arabic Diacritization through Full Morphological Tagging. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Abdelhadi Soudi</author>
<author>Timothy Buckwalter</author>
</authors>
<title>On Arabic Transliteration.</title>
<date>2007</date>
<booktitle>Arabic Computational Morphology: Knowledge-based and Empirical Methods,</booktitle>
<pages>15--22</pages>
<editor>In Abdelhadi Soudi, G¨unter Neumann, and Antal Van den Bosch, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="4741" citStr="Habash et al., 2007" startWordPosition="724" endWordPosition="727">we can achieve state-of-the-art results, without relying on external resources. Error analysis demonstrates the benefit of using LSTM over simpler neural networks. 2 Linguistic Background Languages based on the Arabic script typically employ an abjad writing system, where each symbol represents a consonant while vowels and other phonetic units, commonly known as diacritics, are usually omitted in writing. In modern standard and classical Arabic, these include the short vowels a, u, and i, the case endings F, N, and K, the gemination marker ~, and the silence marker o.2 Table 2, modified from (Habash et al., 2007), lists the diacritics. Importantly, the gemination marker ~ can combine with short vowels and case endings (e.g. Table 1, row 3). 3 Approach We define the following sequence classification task, similarly to (Zitouni and Sarikaya, 2009). 2We also include the low-frequency superscript Alif ‘ that is usually ignored due to its limitation to fixed lexical items. Figure 1: An illustration of our network topology. Let w = (wi, ..., wT) denote a sequence of characters, where each character wt is associated with a label lt. A label may represent 0, 1, or more diacritics, depending on the language. A</context>
</contexts>
<marker>Habash, Soudi, Buckwalter, 2007</marker>
<rawString>Nizar Habash, Abdelhadi Soudi, and Timothy Buckwalter. 2007. On Arabic Transliteration. In Abdelhadi Soudi, G¨unter Neumann, and Antal Van den Bosch, editors, Arabic Computational Morphology: Knowledge-based and Empirical Methods, pages 15–22. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second International Conference on Arabic Language Resources and Tools.</booktitle>
<contexts>
<context position="1992" citStr="Habash et al., 2009" startWordPosition="292" endWordPosition="295"> non-diacritized word, readers infer missing diacritics based on their prior knowledge and the context of the word in order to resolve ambiguities. For example, Maamouri et al. (2006) mention several types of ambiguity for the Arabic string � s Elm, both within and across part-of-speech tags, and at a grammatical Word Gloss Ealima he knew Eulima it was known Eal~ama he taught Eilomu knowledge (def.nom) ... ... EilomK knowledge (indef.gen) Ealamu flag (def.nom) ... ... EalamK flag (indef.gen) Table 1: Possible diacritized forms for e Elm. level. In practice, a morphological analyzer like MADA (Habash et al., 2009) produces at least 13 different diacritized forms for this word, a subset of which is shown in Table 1.1 The ambiguity in Arabic orthography presents a problem for many language processing tasks, including acoustic modeling for speech recognition, language modeling, text-to-speech, and morphological analysis. Automatic methods for diacritization aim to restore diacritics in a non-diacritized text. While earlier work used rule-based methods, more recent studies attempted to learn a diacritization model from diacritized text. A variety of methods have been used, including hidden Markov models, f</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization. In Proceedings of the Second International Conference on Arabic Language Resources and Tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<date>1997</date>
<booktitle>Long Short-Term Memory. Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="6265" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="980" endWordPosition="984">equence x. • Hidden layer(s): mapping the vector sequence x to a hidden sequence h. • Output layer: mapping each hidden vector ht to a probability distribution over labels l. During training, each sequence is fed into this network to create a prediction for each character. As errors are back-propagated down the network, the weights at each layer are updated. During testing, the learned weights are used in a forward step to compute a prediction over the labels. We always take the best predicted label for evaluation. Hidden layer Our main system relies on long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013). Here we describe a single LSTM layer and refer to Graves et al. (2013) for the extension to bidirectional LSTM (B-LSTM) and to multiple layers. The LSTM computes the hidden representation for h1,...,hT Hidden layers xw1,...,xwT w1,...,wT l1,...,lT Output layer Softmax Input layer Embedding 2282 Train Dev Test Words 470K 81K 80K Letters 2.6M 438K 434K Table 3: Arabic diacritization corpus statistics. input xt with the following iterative process: it = Q(Wxixt + Whiht−1 + W,ict−1 + bi) ft = Q(Wxfxt + Whfht−1 + W,fct−1 + bf) ct = ft O ct−1+ it O tanh (Wx,xt + Wh,ht−1 + b,)</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long Short-Term Memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Seth Kulick</author>
</authors>
<title>Diacritization: A challenge to Arabic treebank annotation and parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the British Computer Society Arabic NLP/MT Conference.</booktitle>
<contexts>
<context position="1555" citStr="Maamouri et al. (2006)" startWordPosition="222" endWordPosition="225">art methods that have access to additional resources. 1 Introduction Hebrew, Arabic, and other languages based on the Arabic script usually represent only consonants in writing and do not mark vowels. In such writing systems, diacritics are used for marking short vowels, gemination, and other phonetic units. In practice, diacritics are usually restricted to specific settings such as language teaching or to religious texts. Faced with a non-diacritized word, readers infer missing diacritics based on their prior knowledge and the context of the word in order to resolve ambiguities. For example, Maamouri et al. (2006) mention several types of ambiguity for the Arabic string � s Elm, both within and across part-of-speech tags, and at a grammatical Word Gloss Ealima he knew Eulima it was known Eal~ama he taught Eilomu knowledge (def.nom) ... ... EilomK knowledge (indef.gen) Ealamu flag (def.nom) ... ... EalamK flag (indef.gen) Table 1: Possible diacritized forms for e Elm. level. In practice, a morphological analyzer like MADA (Habash et al., 2009) produces at least 13 different diacritized forms for this word, a subset of which is shown in Table 1.1 The ambiguity in Arabic orthography presents a problem for</context>
</contexts>
<marker>Maamouri, Bies, Kulick, 2006</marker>
<rawString>Mohamed Maamouri, Ann Bies, and Seth Kulick. 2006. Diacritization: A challenge to Arabic treebank annotation and parsing. In Proceedings of the British Computer Society Arabic NLP/MT Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="7474" citStr="Mikolov et al., 2013" startWordPosition="1195" endWordPosition="1198">,ht−1 + b,) ot = Q(Wxoxt + Whoht−1 + W,oct + bo) ht = ot O tanh(ct) where Q is the sigmoid function, O is elementwise multiplication, and i, f, o, and c are input, forget, output, and memory cell activation vectors. The crucial element is the memory cell c that is able to store and reuse long term dependencies over the sequence. The W matrices and b bias vectors are learned during training. Implementation details The input layer maps the character sequence to a sequence of letter vectors, initialized randomly. We also tried initializing with letter vectors trained from raw text with word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), but did not notice any improvement, probably due to the small letter vocabulary size. The input layer also stacks previous and future letter vectors, enabling the model to learn contextual information. We use a letter embedding size of 10 and a window size of 5 characters, so the input size is 110. We experiment with several types of hidden layers, ranging from one feed-forward layer to multiple B-LSTM layers. We also add a linear projection after the input layer. This has the effect of learning a new representation for the letter embeddings. The output layer is a So</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<booktitle>2013b. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS.</booktitle>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A A Rashwan</author>
<author>M A S A A Al-Badrashiny</author>
<author>M Attia</author>
<author>S M Abdou</author>
<author>A Rafea</author>
</authors>
<title>A Stochastic Arabic Diacritizer Based on a Hybrid of Factorized and Unfactorized Textual Features.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="10740" citStr="Rashwan et al., 2011" startWordPosition="1742" endWordPosition="1745"> (Table 5), our 3-layer B-LSTM model beats the lexical variant of Zitouni and Sarikaya (2009) by 3.25% DER, a 40% error reduction. Moreover, we outperform their best model, which also used a segmenter and part-of2283 Figure 2: A confusion matrix of errors made by our system. ”#” marks word boundary. Best viewed in color. speech tagger. This shows that our model can effectively learn to diacritize without relying on any resources other than diacritized text. Finally, some studies report work on a Train/Test data split, without a dedicated Dev set (Zitouni et al., 2006; Habash and Rambow, 2007; Rashwan et al., 2011; Al Sallab et al., 2014). We were reluctant to follow this setting so we performed all development on the Dev set of (Zitouni and Sarikaya, 2009). Still, we ran our best model on the Train/Test split and achieved a DER of 5.39% on all diacritics and 8.74% on case endings. The first result is behind the state-of-theart (Al Sallab et al., 2014) by 2% but the second one is better by 3%. Given that we did not tune the system for this data set, this result is encouraging. Error Analysis A quantitative analysis of the errors produced by one of our models on the Dev set is shown in Figure 2. The hea</context>
</contexts>
<marker>Rashwan, Al-Badrashiny, Attia, Abdou, Rafea, 2011</marker>
<rawString>M.A.A. Rashwan, M.A.S.A.A. Al-Badrashiny, M. Attia, S.M. Abdou, and A. Rafea. 2011. A Stochastic Arabic Diacritizer Based on a Hybrid of Factorized and Unfactorized Textual Features. IEEE Transactions on Audio, Speech, and Language Processing, 19(1):166–175, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Weninger</author>
<author>Johannes Bergmann</author>
<author>Bj¨orn Schuller</author>
</authors>
<date>2015</date>
<booktitle>Introducing CURRENNT: The Munich Open-Source CUDA RecurREnt Neural Network Toolkit. JMLR,</booktitle>
<pages>16--547</pages>
<contexts>
<context position="8430" citStr="Weninger et al., 2015" startWordPosition="1362" endWordPosition="1365">riment with several types of hidden layers, ranging from one feed-forward layer to multiple B-LSTM layers. We also add a linear projection after the input layer. This has the effect of learning a new representation for the letter embeddings. The output layer is a Softmax over labels: exp(yt[l]) P(l|wt) = � l� exp(yt[l0]) where yt = Whyht + by and yt[l] is the lth element of yt. Training is done with stochastic gradient descent with momentum, optimizing the crossentropy objective function. Layer sizes and other hyper-parameters are tuned on the Dev set. Our implementation is based on Currennt (Weninger et al., 2015). Model DER # params All End Feed-forward 11.76 22.90 63K Feed-forward (large) 11.55 23.40 908K LSTM 6.98 10.36 838K B-LSTM 6.16 9.85 518K 2-layer B-LSTM 5.77 9.18 916K 3-layer B-LSTM 5.08 8.14 1,498K Table 4: Diacrtic error rates (DERs) on the Dev set, over all diacritics and only at word ending. MaxEnt (only lexical) 8.1 MaxEnt (full) 5.1 3-layer B-LSTM 4.85 Table 5: Results (DER) on the Test set. MaxEnt results from (Zitouni and Sarikaya, 2009) 4 Experiments Data We extract diacritizied and non-diacritized texts from the Arabic treebank, following the Train/Dev/Test split in (Zitouni and Sa</context>
</contexts>
<marker>Weninger, Bergmann, Schuller, 2015</marker>
<rawString>Felix Weninger, Johannes Bergmann, and Bj¨orn Schuller. 2015. Introducing CURRENNT: The Munich Open-Source CUDA RecurREnt Neural Network Toolkit. JMLR, 16:547–551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Arabic Diacritic Restoration Approach Based on Maximum Entropy Models.</title>
<date>2009</date>
<journal>Comput. Speech Lang.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2685" citStr="Zitouni and Sarikaya, 2009" startWordPosition="398" endWordPosition="401"> subset of which is shown in Table 1.1 The ambiguity in Arabic orthography presents a problem for many language processing tasks, including acoustic modeling for speech recognition, language modeling, text-to-speech, and morphological analysis. Automatic methods for diacritization aim to restore diacritics in a non-diacritized text. While earlier work used rule-based methods, more recent studies attempted to learn a diacritization model from diacritized text. A variety of methods have been used, including hidden Markov models, finite-state transducers, and maximum entropy – see the review in (Zitouni and Sarikaya, 2009) – and more recently, deep neural networks (Al Sallab et al., 2014). In addition to learning from diacritized text, these methods typically rely on external resources such as part-of-speech taggers and morphological analyzers like the MADA tool (Habash and Rambow, 2007). However, building such resources is a labor-intensive task and cannot be easily extended to new languages, dialects, and domains. 1Arabic transliteration follows the Buckwalter scheme: http://www.qamus.org/transliteration.htm. 2281 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 22</context>
<context position="4978" citStr="Zitouni and Sarikaya, 2009" startWordPosition="760" endWordPosition="763">ally employ an abjad writing system, where each symbol represents a consonant while vowels and other phonetic units, commonly known as diacritics, are usually omitted in writing. In modern standard and classical Arabic, these include the short vowels a, u, and i, the case endings F, N, and K, the gemination marker ~, and the silence marker o.2 Table 2, modified from (Habash et al., 2007), lists the diacritics. Importantly, the gemination marker ~ can combine with short vowels and case endings (e.g. Table 1, row 3). 3 Approach We define the following sequence classification task, similarly to (Zitouni and Sarikaya, 2009). 2We also include the low-frequency superscript Alif ‘ that is usually ignored due to its limitation to fixed lexical items. Figure 1: An illustration of our network topology. Let w = (wi, ..., wT) denote a sequence of characters, where each character wt is associated with a label lt. A label may represent 0, 1, or more diacritics, depending on the language. Assume further that each character w in the alphabet is represented as a real-valued vector xw. This character embedding may be learned during training or fixed. Our neural network has the following structure, illustrated in Figure 1: • I</context>
<context position="8881" citStr="Zitouni and Sarikaya, 2009" startWordPosition="1438" endWordPosition="1441">optimizing the crossentropy objective function. Layer sizes and other hyper-parameters are tuned on the Dev set. Our implementation is based on Currennt (Weninger et al., 2015). Model DER # params All End Feed-forward 11.76 22.90 63K Feed-forward (large) 11.55 23.40 908K LSTM 6.98 10.36 838K B-LSTM 6.16 9.85 518K 2-layer B-LSTM 5.77 9.18 916K 3-layer B-LSTM 5.08 8.14 1,498K Table 4: Diacrtic error rates (DERs) on the Dev set, over all diacritics and only at word ending. MaxEnt (only lexical) 8.1 MaxEnt (full) 5.1 3-layer B-LSTM 4.85 Table 5: Results (DER) on the Test set. MaxEnt results from (Zitouni and Sarikaya, 2009) 4 Experiments Data We extract diacritizied and non-diacritized texts from the Arabic treebank, following the Train/Dev/Test split in (Zitouni and Sarikaya, 2009). Table 3 provides statistics for the corpus. Every character in our corpus has a label corresponding to 0, 1, or 2 diacritics, in the case of the gemination marker combining with another diacritic. Thus the label set almost doubles. We opted for this formulation due to its simplicity and generalizability to other languages, even though previous work reported improved results by first predicting gemination and then all other diacritic</context>
<context position="10213" citStr="Zitouni and Sarikaya (2009)" startWordPosition="1653" endWordPosition="1656">f the diacritic error rate (DER). Clearly, LSTM models perform much better than simple feed-forward networks. To make the comparison fair, we increased the number of parameters in the feed-forward model to match that of the LSTM. In this setting, the LSTM is still much better, indicating that it is far more successful at exploiting the larger parameter set. Interestingly, the bidirectional LSTM works better than a unidirectional one, despite having less parameters. Finally, deeper models achieve the best results. On the Test set (Table 5), our 3-layer B-LSTM model beats the lexical variant of Zitouni and Sarikaya (2009) by 3.25% DER, a 40% error reduction. Moreover, we outperform their best model, which also used a segmenter and part-of2283 Figure 2: A confusion matrix of errors made by our system. ”#” marks word boundary. Best viewed in color. speech tagger. This shows that our model can effectively learn to diacritize without relying on any resources other than diacritized text. Finally, some studies report work on a Train/Test data split, without a dedicated Dev set (Zitouni et al., 2006; Habash and Rambow, 2007; Rashwan et al., 2011; Al Sallab et al., 2014). We were reluctant to follow this setting so we</context>
</contexts>
<marker>Zitouni, Sarikaya, 2009</marker>
<rawString>Imed Zitouni and Ruhi Sarikaya. 2009. Arabic Diacritic Restoration Approach Based on Maximum Entropy Models. Comput. Speech Lang., 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
<author>Jeffrey S Sorensen</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Maximum Entropy Based Restoration of Arabic Diacritics.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING andACL.</booktitle>
<contexts>
<context position="10693" citStr="Zitouni et al., 2006" startWordPosition="1734" endWordPosition="1737">odels achieve the best results. On the Test set (Table 5), our 3-layer B-LSTM model beats the lexical variant of Zitouni and Sarikaya (2009) by 3.25% DER, a 40% error reduction. Moreover, we outperform their best model, which also used a segmenter and part-of2283 Figure 2: A confusion matrix of errors made by our system. ”#” marks word boundary. Best viewed in color. speech tagger. This shows that our model can effectively learn to diacritize without relying on any resources other than diacritized text. Finally, some studies report work on a Train/Test data split, without a dedicated Dev set (Zitouni et al., 2006; Habash and Rambow, 2007; Rashwan et al., 2011; Al Sallab et al., 2014). We were reluctant to follow this setting so we performed all development on the Dev set of (Zitouni and Sarikaya, 2009). Still, we ran our best model on the Train/Test split and achieved a DER of 5.39% on all diacritics and 8.74% on case endings. The first result is behind the state-of-theart (Al Sallab et al., 2014) by 2% but the second one is better by 3%. Given that we did not tune the system for this data set, this result is encouraging. Error Analysis A quantitative analysis of the errors produced by one of our mode</context>
</contexts>
<marker>Zitouni, Sorensen, Sarikaya, 2006</marker>
<rawString>Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya. 2006. Maximum Entropy Based Restoration of Arabic Diacritics. In Proceedings of COLING andACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018438">
<title confidence="0.9989905">
Investigating Continuous Space Language Models for Machine
Translation Quality Estimation
</title>
<author confidence="0.997599">
Kashif Shah§, Raymond W. M. Ng§, Fethi Bougares†, Lucia Specia§
</author>
<affiliation confidence="0.999025">
§Department of Computer Science, University of Sheffield, UK
</affiliation>
<email confidence="0.970808">
{kashif.shah, wm.ng, l.specia}@sheffield.ac.uk
</email>
<affiliation confidence="0.905955">
†LIUM, University of Le Mans, France
</affiliation>
<email confidence="0.994735">
fethi.bougares@lium.univ-lemans.fr
</email>
<sectionHeader confidence="0.993774" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999410684210527">
We present novel features designed with a
deep neural network for Machine Trans-
lation (MT) Quality Estimation (QE). The
features are learned with a Continuous
Space Language Model to estimate the
probabilities of the source and target seg-
ments. These new features, along with
standard MT system-independent features,
are benchmarked on a series of datasets
with various quality labels, including post-
editing effort, human translation edit rate,
post-editing time and METEOR. Results
show significant improvements in predic-
tion over the baseline, as well as over sys-
tems trained on state of the art feature sets
for all datasets. More notably, the addition
of the newly proposed features improves
over the best QE systems in WMT12 and
WMT14 by a significant margin.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882344827586">
Quality Estimation (QE) is concerned with pre-
dicting the quality of Machine Translation (MT)
output without reference translations. QE is ad-
dressed with various features indicating fluency,
adequacy and complexity of the translation pair.
These features are used by a machine learning al-
gorithm along with quality labels given by humans
to learn models to predict the quality of unseen
translations.
A variety of features play a key role in QE.
A wide range of features from source segments
and their translated segments, extracted with the
help of external resources and tools, have been
proposed. These go from simple, language-
independent features, to advanced, linguistically
motivated features. They include features that
summarise how the MT systems generate transla-
tions, as well as features that are oblivious to the
systems. The majority of the features in the lit-
erature are extracted from each sentence pair in
isolation, ignoring the context of the text. QE
performance usually differs depending on the lan-
guage pair, the specific quality score being opti-
mised (e.g., post-editing time vs translation ad-
equacy) and the feature set. Features based on
n-gram language models, despite their simplicity,
are among those with the best performance in most
QE tasks (Shah et al., 2013b). However, they may
not generalise well due to the underlying discrete
nature of words in n-gram modelling.
Continuous Space Language Models (CSLM),
on the other hand, have shown their potential
to capture long distance dependencies among
words (Schwenk, 2012; Mikolov et al., 2013). The
assumption of these models is that semantically or
grammatically related words are mapped to simi-
lar geometric locations in a high-dimensional con-
tinuous space. The probability distribution is thus
much smoother and therefore the model has a bet-
ter generalisation power on unseen events. The
representations are learned in a continuous space
to estimate the probabilities using neural networks
with single (called shallow networks) or multi-
ple (called deep networks) hidden layers. Deep
neural networks have been shown to perform bet-
ter than shallow ones due to their capability to
learn higher-level, abstract representations of the
input (Arisoy et al., 2012). In this paper, we ex-
plore the potential of these models in context of
QE for MT. We obtain more robust features with
CSLM and improve the overall prediction power
for translation quality.
The paper is organised as follows: In Section
2 we briefly present the related work. Section 3
describes the CSLM model training and its vari-
ous settings. In Section 4 we propose the use of
CSLM features for QE. In Section 5 we present
our experiments along with their results.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.934074">
For a detailed overview of various features and
algorithms for QE, we refer the reader to the
</bodyText>
<page confidence="0.920709">
1073
</page>
<note confidence="0.657788">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1073–1078,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99982245945946">
WMT12-14 shared tasks on QE (Callison-Burch
et al., 2012; Bojar et al., 2013; Ling et al., 2014).
Most of the research work lies on deciding which
aspects of quality are more relevant for a given
task and designing feature extractors for them.
While simple features such as counts of tokens
and language model scores can be easily extracted,
feature engineering for more advanced and useful
information can be quite labour-intensive.
Since their introduction in (Bengio et al.,
2003), neural network language models have
been successfully exploited in many speech and
language processing problems, including auto-
matic speech recognition (Schwenk and Gau-
vain, 2005; Schwenk, 2007) and machine trans-
lation (Schwenk, 2012).
Recently, (Banchs et al., 2015) used a Latent
Semantic Indexing approach to model sentences
as bag-of-words in a continuous space to measure
cross language adequacy. (Tan et al., 2015) pro-
posed to train models with deep regression for ma-
chine translation evaluation in a task to measure
semantic similarity between sentences. They re-
ported positive results on simple features; larger
feature sets did not improve these results.
In this paper, we propose to estimate the prob-
abilities of source and target segments with con-
tinuous space language models based on a deep
architecture and to use these estimated probabili-
ties as features along with standard feature sets in
a supervised learning framework. To the best of
our knowledge, such approach has not been stud-
ied before in the context of QE for MT. The result
shows significant improvements in many predic-
tion tasks, despite its simplicity. Monolingual data
for source and target language is the only resource
required to extract these features.
</bodyText>
<sectionHeader confidence="0.953351" genericHeader="method">
3 Continuous Space Language Models
</sectionHeader>
<bodyText confidence="0.9994095">
A key factor for quality inference of a translated
text is to determine the fluency of such a text and
how well it conforms to the linguistic regularities
of the target language. It involves grammatical
correctness, idiomatic and stylistic word choices
that can be derived by using n-gram language
models. However, in high-order n-grams, the pa-
rameter space is sparse and conventional mod-
elling is inefficient. Neural networks model the
non-linear relationship between the input features
and target outputs. They often outperform con-
ventional techniques in difficult machine learning
tasks. Neural network language models (CSLM)
alleviate the curse of dimensionality by projecting
words into a continuous space, and modelling and
estimating probabilities in this space.
The architecture of a deep CSLM is illus-
trated in Figure 1. The inputs to a CSLM
model are the (K − 1) left-context words
(wi−x+1, . . . , wi−2, wi−1) to predict wi. A one-
hot vector encoding scheme is used to repre-
sent the input wi−k with an N-dimensional vec-
tor. The output of CSLM is a vector of pos-
terior probabilities for all words in vocabulary,
P(wi|wi−1, wi−2, . . . , wi−x+1). Due to the large
output layer (vocabulary size), the complexity of a
basic neural network language model is very high.
Schwenk (2007) proposed efficient training strate-
gies in order to reduce the computational complex-
ity and speed up the training time. They process
several examples at once and use a short-list vo-
cabulary V with only the most frequent words.
</bodyText>
<figureCaption confidence="0.998918">
Figure 1: Deep CSLM architecture.
</figureCaption>
<bodyText confidence="0.9999831875">
Following the settings mentioned in (Schwenk
et al., 2014), all CSLM experiments described
in this paper are performed using deep networks
with four hidden layers: first layer for the projec-
tion (320 units for each context word) and three
hidden layers of 1024 units with tanh activation.
At the output layer, we use a softmax activation
function applied to a short-list of the 32k most
frequent words. The probabilities of the out-of-
vocabulary words are obtained from a standard
back-off n-gram language model. The projection
of the words onto the continuous space and the
training of the neural network is done by the stan-
dard back-propagation algorithm and outputs are
the converged posterior probabilities. The model
parameters are optimised on a development set.
</bodyText>
<sectionHeader confidence="0.981002" genericHeader="method">
4 CSLM and Quality Estimation
</sectionHeader>
<bodyText confidence="0.9997365">
In the context of MT, CSLMs are generally trained
on the target side of a given language pair to ex-
</bodyText>
<page confidence="0.984485">
1074
</page>
<bodyText confidence="0.999827157894737">
press the probability that the generated sentence
is “correct” or “likely”, without looking at the
source sentence. However, QE is also concerned
with how well the source segments can be trans-
lated. Therefore, we trained two models, one for
each side of a given language pair. We extracted
the probabilities for QE training and test sets for
both source and its translation with their respec-
tive models and used them as features, along with
other features, in a supervised learning setting.
Finally, we also used CSLM in a spoken lan-
guage translation (SLT) task. In SLT, an auto-
matic speech recogniser (ASR) is used to decode
the source language text from audio. This creates
an extra source of variability, where different ASR
models and configurations give different outputs.
In this paper, we use QE to exploit different ASR
outputs (i.e. MT inputs) which in turn can lead to
different MT outputs.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999975923076923">
We focus on experiments with sentence level QE
tasks. Our English-Spanish experiments are based
on the WMT QE shared task data from 2012 to
2015.1 These tasks are diverse in nature, with dif-
ferent sizes and labels such as post-editing effort
(PEE), post-editing time (PET) and human trans-
lation error rate (HTER). The results reported in
Section 5.5 are directly comparable with the of-
ficial systems submitted for each of the respec-
tive tasks. We also performed experiments on the
IWSLT 2014 English-French SLT task 2 to study
the applicability of our models on n-best ASR
(MT inputs) comparison.
</bodyText>
<subsectionHeader confidence="0.974765">
5.1 QE Datasets
</subsectionHeader>
<bodyText confidence="0.9990939">
In Table 1 we summarise the data and tasks for our
experiments. We refer readers to the WMT and
IWSLT websites for detailed descriptions of these
datasets. All datasets are publicly available.
WMT12: English-Spanish news sentence trans-
lations produced by a Moses “baseline” statisti-
cal MT (SMT) system, and judged for perceived
post-editing effort in 1–5 (highest-lowest), taking
a weighted average of three annotators (Callison-
Burch et al., 2012).
</bodyText>
<listItem confidence="0.9033755">
WMT13 (Task-1): English-Spanish sentence
translations of news texts produced by a Moses
</listItem>
<footnote confidence="0.9985245">
1http://www.statmt.org/wmt[12,13,14,
15]/quality-estimation-task.html
2https://sites.google.com/site/
iwsltevaluation2014/slt-track
</footnote>
<bodyText confidence="0.953254861111111">
“baseline” SMT system. These were then post-
edited by a professional translator and labelled
using HTER. This is a superset of the WMT12
dataset, with 500 additional sentences for test, and
a different quality label (Bojar et al., 2013).
WMT14 (Task-1.1): English-Spanish news
sentence translations. The dataset contains source
sentences and their human translations, as well
as three versions of machine translations: by an
SMT system, a rule-based system system and a
hybrid system. Each translation was labelled by
professional translators with 1-3 (lowest-highest)
scores for perceived post-editing effort.
WMT14 (Task-1.3): English-Spanish news
sentence translations post-edited by a professional
translator, with the post-editing time collected on a
sentence-basis and used as label (in milliseconds).
WMT15 (Task-1): Large English-Spanish news
dataset containing source sentences, their machine
translations by an online SMT system, and the
post-editions of the translation by crowdsourced
translators, with HTER used as label.
IWSLT14: English-French dataset containing
source language data from the 10-best (sentences)
ASR system output. On the target side, the 1-
best MT translation is used. The ASR system
leads to different source segments, which in turn
lead to different translations. METEOR (Banerjee
and Lavie, 2005) is used to label these alternative
translations against a reference (human) transla-
tion. Both ASR and MT outputs come from a sys-
tem submission in IWSLT 2014 (Ng et al., 2014).
The ASR system is a multi-pass deep neural net-
work tandem system with feature and model adap-
tation and rescoring. The MT system is a phrase-
based SMT system produced using Moses.
</bodyText>
<table confidence="0.999712142857143">
Train Test Label
832 422 PEE 1-5
254 500 HTER 0-1
816 600 PEE 1-3
650 208 PET (ms)
11, 271 1, 817 HTER 0-1
8, 180 11, 240 MET. 0-1
</table>
<tableCaption confidence="0.999906">
Table 1: QE datasets: # sentences and labels.
</tableCaption>
<subsectionHeader confidence="0.978964">
5.2 CSLM Dataset
</subsectionHeader>
<bodyText confidence="0.999525333333333">
The dataset used for CSLM training consists of
Europarl, News-commentary and News-crawl cor-
pus. We used a data selection method (Moore
</bodyText>
<figure confidence="0.985038714285714">
Dataset Lang.
WMT12 en-es
WMT13 en-es
WMT14task1.1 en-es
WMT14task1.3 en-es
WMT15 en-es
IWSLT14 en-fr
</figure>
<page confidence="0.973603">
1075
</page>
<bodyText confidence="0.999345555555555">
and Lewis, 2010) to select the most relevant train-
ing data with respect to a development set. For
English-Spanish, the development data is the con-
catenation of newstest2012 and newstest2013 of
the WMT translation track. For English-French,
the development set is the concatenation of the
IWSLT dev2010 and eval2010. In Table 2 we
show statistics on the selected monolingual data
used to train back-off LM and CSLM.
</bodyText>
<table confidence="0.9916485">
Lang. Train Dev LM ppl CSLM ppl
en 4.3G 137.7k 164.63 116.58 (29.18%)
fr 464.7M 54K 99.34 64.88 (34.68%)
es 21.2M 149.4k 145.49 87.14 (40.10%)
</table>
<tableCaption confidence="0.71674075">
Table 2: Training data size (number of tokens) and
language models perplexity (ppl). The values in
parentheses in last column shows percentage de-
crease in perplexity.
</tableCaption>
<subsectionHeader confidence="0.989982">
5.3 Feature Sets
</subsectionHeader>
<bodyText confidence="0.993944666666667">
We use the QuEst 3 toolkit (Specia et al., 2013;
Shah et al., 2013a) to extract two feature sets for
each dataset:
</bodyText>
<listItem confidence="0.938607">
• BL: 17 features used as baseline in the WMT
shared tasks on QE.
• AF: 80 augmented MT system-independent
features4 (superset of BL). For the En-Fr SLT
</listItem>
<bodyText confidence="0.9786332">
task, we have additional 36 features (21 ASR
+ 15 MT-dependent features)
The resources used to extract these features (cor-
pora, etc.) are also available as part of the WMT
shared tasks on QE. The CSLM features for each
of the source and target segments are extracted us-
ing the procedure described in Section 3 with the
CSLM toolkit. 5
We trained QE models with following combina-
tion of features:
</bodyText>
<listItem confidence="0.969291333333333">
• BL + CSLMsrc,tgt: CSLM features for
source and target segments, plus the baseline
features.
• AF + CSLMsrc,tgt: CSLM features for
source and target segments, plus all available
features.
</listItem>
<bodyText confidence="0.944142">
For the WMT12 task, we performed further exper-
iments to analyse the improvements with CSLM:
</bodyText>
<listItem confidence="0.99923725">
• CSLMsrc: Source side CSLM feature only.
• CSLMtgt: Target side CSLM feature only.
• CSLMsrc,tgt: Source and target CSLM fea-
tures by themselves.
</listItem>
<footnote confidence="0.9996895">
3http://www.quest.dcs.shef.ac.uk/
480 features http://www.quest.dcs.shef.ac.
uk/quest_files/features_blackbox
5http://www-lium.univ-lemans.fr/cslm/
</footnote>
<listItem confidence="0.98628975">
• FS(AF) + CSLMsrc,tgt: CSLM features in
addition to the best performing feature set
(FS(AF)) selected as described in (Shah et
al., 2013b; Shah et al., 2015).
</listItem>
<subsectionHeader confidence="0.997556">
5.4 Learning algorithms
</subsectionHeader>
<bodyText confidence="0.99551025">
We use the Support Vector Machines implementa-
tion of the scikit-learn toolkit to perform re-
gression (SVR) with either Radial Basis Function
(RBF) or linear kernel and parameters optimised
via grid search. To evaluate the prediction models
we use Mean Absolute Error (MAE), its squared
version – Root Mean Squared Error (RMSE), and
Pearson’s correlation (r) score.
</bodyText>
<table confidence="0.99956837037037">
Task System #feats MAE RMSE r
WMT12 BL 17 0.6821 0.8117 0.5595
AF 80 0.6717 0.8103 0.5645
BL + CSLMsrc,tgt 19 0.6463 0.7977 0.5805
AF + CSLMsrc,tgt 82 0.6462 0.7946 0.5825
WMT13 BL 17 0.1411 0.1812 0.4612
AF 80 0.1399 0.1789 0.4751
BL + CSLMsrc,tgt 19 0.1401 0.1791 0.4771
AF + CSLMsrc,tgt 82 0.1371 0.1750 0.4820
WMT14 BL 17 0.5241 0.6591 0.2502
Task 1.1
AF 80 0.4896 0.6349 0.3310
BL + CSLMsrc,tgt 19 0.4931 0.6351 0.3545
AF + CSLMsrc,tgt 82 0.4628* 0.6165* 0.3824*
WMT14 BL 17 0.1798 0.2865 0.5661
Task 1.3
AF 80 0.1753 0.2815 0.5871
BL + CSLMsrc,tgt 19 0.1740 0.2758 0.6243
AF + CSLMsrc,tgt 82 0.1701** 0.2734 0.6201
WMT15 BL 17 0.1562 0.2036 0.1382
AF 80 0.1541 0.1995 0.2205
BL + CSLMsrc,tgt 19 0.1501 0.1971 0.2611
AF + CSLMsrc,tgt 82 0.1471 0.1934 0.2862
IWSLT14 BL 17 0.1390 0.1791 0.5012
AF 116 0.1361 0.1775 0.5211
BL + CSLMsrc,tgt 19 0.1358 0.1750 0.5321
AF + CSLMsrc,tgt 118 0.1337 0.1728 0.5445
</table>
<tableCaption confidence="0.9929908">
Table 3: Results for datasets with various feature
sets. Figures with * beat the official best systems,
and with ** are second best. Results with CSLM
features are significantly better than BL and AF on
all tasks (paired t-test with p G 0.05).
</tableCaption>
<table confidence="0.998327666666667">
Task System #feats MAE RMSE r
WMT12 BL + CSLMsrc 18 0.6751 0.8125 0.5626
BL + CSLMtgt 18 0.6694 0.8023 0.5815
CSLMsrc,tgt 2 0.6882 0.8430 0.5314
FS(AF) 19 0.6131 0.7598 0.6296
FS(AF) + CSLMsrc,tgt 21 0.5950* 0.7442* 0.6482*
</table>
<tableCaption confidence="0.9278274">
Table 4: Impact of different combinations of
CSLM features on the WMT12 task. Figures with
* beat the official best system. Results with CSLM
features are significantly better than BL and AF on
all tasks (paired t-test with p G 0.05).
</tableCaption>
<page confidence="0.996081">
1076
</page>
<sectionHeader confidence="0.720915" genericHeader="evaluation">
5.5 Results
</sectionHeader>
<bodyText confidence="0.9999756">
Table 3 presents the results with different feature
sets for data from various shared tasks. It can be
noted that CSLM features always bring significant
improvements whenever added to either baseline
or augmented feature set. A reduction in both error
scores (MAE and RMSE) as well as an increase
in Pearson’s correlation with human labels can be
observed on all tasks. It is also worth noticing
that the CSLM features bring improvements over
all tasks with different labels, evidencing that dif-
ferent optimisation objectives and language pairs
can benefit from these features. However, the im-
provements are more visible when predicting post-
editing effort for WMT12 and WMT14’s Task 1.1.
For these two tasks, we are able to achieve state-
of-the-art performance by adding the two CSLM
features to all available or selected feature sets.
For WMT12, we performed another set of ex-
periments to study the effect of CSLM features
by themselves and in combination. The results
in Table 4 show that the target side CSLM fea-
ture bring larger improvements than its source side
counterpart. We believe that it is because the tar-
get side feature directly reflects the fluency of the
translation, whereas the source side feature (re-
garded as a translation complexity feature) only
has indirect effect on quality. Interestingly, the
two CSLM features alone give comparable re-
sults (slightly worse) than the BL feature set 6 de-
spite the fact that these 17 features cover many
complexity, adequacy and fluency quality aspects.
CSLM features bring further improvements on
pre-selected feature sets, as shown in Table 3. We
also performed feature selection over the full fea-
ture set along with CSLM features, following the
procedure in (Shah et al., 2013b). Interestingly,
both CSLM features were selected among the top
ranked features, confirming their relevance.
In order to investigate whether our CSLM fea-
tures results hold for other feature sets, we ex-
perimented with the feature sets provided by most
teams participating in the WMT12 QE shared task.
These feature sets are very diverse in terms of the
types of features, resources used, and their sizes.
Table 5 shows the official results from the shared
task (Off.) (Callison-Burch et al., 2012), those
from training an SVR on these features with and
without CSLM features. Note that the official
scores are often different from the results obtained
with our SVR models because of differences in
</bodyText>
<footnote confidence="0.66504">
6We compare results in terms of MAE scores only.
</footnote>
<bodyText confidence="0.949096666666667">
the learning algorithms. As shown in Table 5,
we observed similar improvements with additional
CSLM features over all of these feature sets.
</bodyText>
<table confidence="0.998911727272727">
System #feats Off. SVR SVR
without CSLM with CSLM
SDL 15 0.61 0.6115 0.5993
UU 82 0.64 0.6513 0.6371
Loria 49 0.68 0.6978 0.6729
UEdin 56 0.68 0.6879 0.6724
TCD 43 0.68 0.6972 0.6715
WL-SH 147 0.69 0.6791 0.6678
UPC 57 0.84 0.8419 0.8310
DCU 308 0.75 0.6825 0.6812
PRHLT 497 0.70 0.6699 0.6649
</table>
<tableCaption confidence="0.983855">
Table 5: MAE score on official WMT12 feature
sets using SVR with and without CSLM features.
</tableCaption>
<sectionHeader confidence="0.998994" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99997878125">
We proposed novel features for machine transla-
tion quality estimation obtained using a deep con-
tinuous space language models. The proposed fea-
tures led to significant improvements over stan-
dard feature sets for a variety of datasets, outper-
forming the state-of-art on two official WMT QE
tasks. These results showed that different opti-
misation objectives and language pairs can bene-
fit from the proposed features. The proposed fea-
tures have been shown to also perform well on QE
within a spoken language translation task.
Both source and target CSLM features improve
prediction quality, either when used separately
or in combination. They proved complementary
when used together with other feature sets and
produce comparable results to high performing
baseline features when used alone for prediction.
Finally, results comparing all official WMT12 QE
feature sets showed significant improvements in
the predictions when CSLM features were added
to those submitted by participating teams. These
findings provide evidence that the proposed fea-
tures bring valuable information into prediction
models, despite their simplicity and the fact that
they require only monolingual data as resource,
which is available in abundance for many lan-
guages.
As future work, it would be interesting to ex-
plore various distributed word representations for
quality estimation and joint models that look at
both the source and the target sentences simulta-
neously.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.710001">
This work was supported by the QT21 (H2020
No. 645452), Cracker (H2020 No. 645357) and
DARPA Bolt projects.
</bodyText>
<page confidence="0.99691">
1077
</page>
<sectionHeader confidence="0.990157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982802173913">
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model? On
the Future of Language Modeling for HLT, pages
20–28, Montreal, Canada.
Rafael E Banchs, Luis F D’Haro, and Haizhou Li.
2015. Adequacy–fluency metrics: Evaluating mt
in the continuous space model framework. Au-
dio, Speech, and Language Processing, IEEE/ACM
Transactions on, 23(3):472–482.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In ACL work-
shop on intrinsic and extrinsic evaluation measures
for machine translation and/or summarization, vol-
ume 29, pages 65–72.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Eighth
Workshop on Statistical Machine Translation, pages
1–44, Sofia, Bulgaria.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 WMT. In Seventh Workshop
on Statistical Machine Translation, pages 10–51,
Montr´eal, Canada.
Wang Ling, Luis Marujo, Chris Dyer, Alan Black, and
Isabel Trancoso. 2014. Crowdsourcing high-quality
parallel data extraction from twitter. In Ninth Work-
shop on Statistical Machine Translation, WMT14,
pages 426–436, Baltimore, USA.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ’10, pages 220–224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Raymond W. N. Ng, Mortaza Doulaty, Rama Dod-
dipatla, Oscar Saz, Madina Hasan, Thomas Hain,
Wilker Aziz, Kashif Shaf, and Lucia Specia. 2014.
The USFD spoken language translation system for
IWSLT 2014. Proc. IWSLT, pages 86–91.
Holger Schwenk and Jean-Luc Gauvain. 2005. Train-
ing neural network language models on very large
corpora. In Conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 201–208.
Holger Schwenk, Fethi Bougares, and Loic Barrault.
2014. Efficient training strategies for deep neural
network language models. In NIPS workshop on
Deep Learning and Representation Learning.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech &amp; Language, 21(3):492–
518.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In COLING (Posters), pages 1071–1080.
Kashif Shah, Eleftherios Avramidis, Ergun Bic¸icic, and
Lucia Specia. 2013a. Quest - design, implemen-
tation and extensions of a framework for machine
translation quality estimation. The Prague Bulletin
of Mathematical Linguistics, 100:19–30.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2013b.
An investigation on the effectiveness of features for
translation quality estimation. In Machine Transla-
tion Summit, volume 14, pages 167–174.
Kashif Shah, Trevor Cohn, and Lucia Specia. 2015.
A bayesian non-linear method for feature selection
in machine translation quality estimation. Machine
Translation, 29(2):101–125.
Lucia Specia, Kashif Shah, Jos´e G. C. de Souza, and
Trevor Cohn. 2013. QuEst - A translation qual-
ity estimation framework. In 51st Annual Meeting
of the Association for Computational Linguistics:
Demo Session, pages 79–84, Sofia, Bulgaria.
Liling Tan, Carolina Scarton, Lucia Specia, and Josef
van Genabith. 2015. Usaar-sheffield: Semantic
textual similarity with deep regression and machine
translation evaluation metrics. In Proceedings of the
9th International Workshop on Semantic Evaluation,
pages 85–89, Denver, Colorado.
</reference>
<page confidence="0.995882">
1078
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.558713">
<title confidence="0.999827">Investigating Continuous Space Language Models for Translation Quality Estimation</title>
<author confidence="0.999881">Raymond W M Fethi Lucia</author>
<affiliation confidence="0.993923">of Computer Science, University of Sheffield,</affiliation>
<email confidence="0.663708">wm.ng,</email>
<affiliation confidence="0.991154">University of Le Mans,</affiliation>
<email confidence="0.99619">fethi.bougares@lium.univ-lemans.fr</email>
<abstract confidence="0.9925324">We present novel features designed with a deep neural network for Machine Translation (MT) Quality Estimation (QE). The features are learned with a Continuous Space Language Model to estimate the probabilities of the source and target segments. These new features, along with standard MT system-independent features, are benchmarked on a series of datasets with various quality labels, including postediting effort, human translation edit rate, post-editing time and METEOR. Results show significant improvements in prediction over the baseline, as well as over systems trained on state of the art feature sets for all datasets. More notably, the addition of the newly proposed features improves over the best QE systems in WMT12 and WMT14 by a significant margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ebru Arisoy</author>
<author>Tara N Sainath</author>
<author>Brian Kingsbury</author>
<author>Bhuvana Ramabhadran</author>
</authors>
<title>Deep neural network language models.</title>
<date>2012</date>
<booktitle>In NAACL-HLT</booktitle>
<pages>20--28</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3362" citStr="Arisoy et al., 2012" startWordPosition="507" endWordPosition="510">ically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space. The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events. The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multiple (called deep networks) hidden layers. Deep neural networks have been shown to perform better than shallow ones due to their capability to learn higher-level, abstract representations of the input (Arisoy et al., 2012). In this paper, we explore the potential of these models in context of QE for MT. We obtain more robust features with CSLM and improve the overall prediction power for translation quality. The paper is organised as follows: In Section 2 we briefly present the related work. Section 3 describes the CSLM model training and its various settings. In Section 4 we propose the use of CSLM features for QE. In Section 5 we present our experiments along with their results. 2 Related Work For a detailed overview of various features and algorithms for QE, we refer the reader to the 1073 Proceedings of the</context>
</contexts>
<marker>Arisoy, Sainath, Kingsbury, Ramabhadran, 2012</marker>
<rawString>Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. 2012. Deep neural network language models. In NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 20–28, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael E Banchs</author>
<author>Luis F D’Haro</author>
<author>Haizhou Li</author>
</authors>
<title>Adequacy–fluency metrics: Evaluating mt in the continuous space model framework. Audio, Speech, and Language Processing,</title>
<date>2015</date>
<journal>IEEE/ACM Transactions on,</journal>
<pages>23--3</pages>
<marker>Banchs, D’Haro, Li, 2015</marker>
<rawString>Rafael E Banchs, Luis F D’Haro, and Haizhou Li. 2015. Adequacy–fluency metrics: Evaluating mt in the continuous space model framework. Audio, Speech, and Language Processing, IEEE/ACM Transactions on, 23(3):472–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In ACL workshop on</booktitle>
<volume>29</volume>
<pages>65--72</pages>
<contexts>
<context position="11876" citStr="Banerjee and Lavie, 2005" startWordPosition="1849" endWordPosition="1852">ator, with the post-editing time collected on a sentence-basis and used as label (in milliseconds). WMT15 (Task-1): Large English-Spanish news dataset containing source sentences, their machine translations by an online SMT system, and the post-editions of the translation by crowdsourced translators, with HTER used as label. IWSLT14: English-French dataset containing source language data from the 10-best (sentences) ASR system output. On the target side, the 1- best MT translation is used. The ASR system leads to different source segments, which in turn lead to different translations. METEOR (Banerjee and Lavie, 2005) is used to label these alternative translations against a reference (human) translation. Both ASR and MT outputs come from a system submission in IWSLT 2014 (Ng et al., 2014). The ASR system is a multi-pass deep neural network tandem system with feature and model adaptation and rescoring. The MT system is a phrasebased SMT system produced using Moses. Train Test Label 832 422 PEE 1-5 254 500 HTER 0-1 816 600 PEE 1-3 650 208 PET (ms) 11, 271 1, 817 HTER 0-1 8, 180 11, 240 MET. 0-1 Table 1: QE datasets: # sentences and labels. 5.2 CSLM Dataset The dataset used for CSLM training consists of Euro</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, volume 29, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="4622" citStr="Bengio et al., 2003" startWordPosition="713" endWordPosition="716"> Natural Language Processing, pages 1073–1078, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and machine translation (Schwenk, 2012). Recently, (Banchs et al., 2015) used a Latent Semantic Indexing approach to model sentences as bag-of-words in a continuous space to measure cross language adequacy. (Tan et al., 2015) proposed to train models with deep regression for machine translation evaluation in a task to measure semantic similarity between sentences. They reported positive results on si</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="4215" citStr="Bojar et al., 2013" startWordPosition="648" endWordPosition="651"> 2 we briefly present the related work. Section 3 describes the CSLM model training and its various settings. In Section 4 we propose the use of CSLM features for QE. In Section 5 we present our experiments along with their results. 2 Related Work For a detailed overview of various features and algorithms for QE, we refer the reader to the 1073 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1073–1078, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvain, 2005; Schwenk, 20</context>
<context position="10779" citStr="Bojar et al., 2013" startWordPosition="1698" endWordPosition="1701"> MT (SMT) system, and judged for perceived post-editing effort in 1–5 (highest-lowest), taking a weighted average of three annotators (CallisonBurch et al., 2012). WMT13 (Task-1): English-Spanish sentence translations of news texts produced by a Moses 1http://www.statmt.org/wmt[12,13,14, 15]/quality-estimation-task.html 2https://sites.google.com/site/ iwsltevaluation2014/slt-track “baseline” SMT system. These were then postedited by a professional translator and labelled using HTER. This is a superset of the WMT12 dataset, with 500 additional sentences for test, and a different quality label (Bojar et al., 2013). WMT14 (Task-1.1): English-Spanish news sentence translations. The dataset contains source sentences and their human translations, as well as three versions of machine translations: by an SMT system, a rule-based system system and a hybrid system. Each translation was labelled by professional translators with 1-3 (lowest-highest) scores for perceived post-editing effort. WMT14 (Task-1.3): English-Spanish news sentence translations post-edited by a professional translator, with the post-editing time collected on a sentence-basis and used as label (in milliseconds). WMT15 (Task-1): Large Englis</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 WMT. In Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="4195" citStr="Callison-Burch et al., 2012" startWordPosition="644" endWordPosition="647">anised as follows: In Section 2 we briefly present the related work. Section 3 describes the CSLM model training and its various settings. In Section 4 we propose the use of CSLM features for QE. In Section 5 we present our experiments along with their results. 2 Related Work For a detailed overview of various features and algorithms for QE, we refer the reader to the 1073 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1073–1078, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvai</context>
<context position="19058" citStr="Callison-Burch et al., 2012" startWordPosition="3042" endWordPosition="3045">also performed feature selection over the full feature set along with CSLM features, following the procedure in (Shah et al., 2013b). Interestingly, both CSLM features were selected among the top ranked features, confirming their relevance. In order to investigate whether our CSLM features results hold for other feature sets, we experimented with the feature sets provided by most teams participating in the WMT12 QE shared task. These feature sets are very diverse in terms of the types of features, resources used, and their sizes. Table 5 shows the official results from the shared task (Off.) (Callison-Burch et al., 2012), those from training an SVR on these features with and without CSLM features. Note that the official scores are often different from the results obtained with our SVR models because of differences in 6We compare results in terms of MAE scores only. the learning algorithms. As shown in Table 5, we observed similar improvements with additional CSLM features over all of these feature sets. System #feats Off. SVR SVR without CSLM with CSLM SDL 15 0.61 0.6115 0.5993 UU 82 0.64 0.6513 0.6371 Loria 49 0.68 0.6978 0.6729 UEdin 56 0.68 0.6879 0.6724 TCD 43 0.68 0.6972 0.6715 WL-SH 147 0.69 0.6791 0.66</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 WMT. In Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Luis Marujo</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Crowdsourcing high-quality parallel data extraction from twitter.</title>
<date>2014</date>
<booktitle>In Ninth Workshop on Statistical Machine Translation, WMT14,</booktitle>
<pages>426--436</pages>
<location>Baltimore, USA.</location>
<contexts>
<context position="4235" citStr="Ling et al., 2014" startWordPosition="652" endWordPosition="655">t the related work. Section 3 describes the CSLM model training and its various settings. In Section 4 we propose the use of CSLM features for QE. In Section 5 we present our experiments along with their results. 2 Related Work For a detailed overview of various features and algorithms for QE, we refer the reader to the 1073 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1073–1078, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and machine tran</context>
</contexts>
<marker>Ling, Marujo, Dyer, Black, Trancoso, 2014</marker>
<rawString>Wang Ling, Luis Marujo, Chris Dyer, Alan Black, and Isabel Trancoso. 2014. Crowdsourcing high-quality parallel data extraction from twitter. In Ninth Workshop on Statistical Machine Translation, WMT14, pages 426–436, Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="2695" citStr="Mikolov et al., 2013" startWordPosition="405" endWordPosition="408">t of the text. QE performance usually differs depending on the language pair, the specific quality score being optimised (e.g., post-editing time vs translation adequacy) and the feature set. Features based on n-gram language models, despite their simplicity, are among those with the best performance in most QE tasks (Shah et al., 2013b). However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling. Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words (Schwenk, 2012; Mikolov et al., 2013). The assumption of these models is that semantically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space. The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events. The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multiple (called deep networks) hidden layers. Deep neural networks have been shown to perform better than shallow ones due to their capability to learn higher</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746– 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>220--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 220–224, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond W N Ng</author>
<author>Mortaza Doulaty</author>
<author>Rama Doddipatla</author>
<author>Oscar Saz</author>
<author>Madina Hasan</author>
<author>Thomas Hain</author>
<author>Wilker Aziz</author>
<author>Kashif Shaf</author>
<author>Lucia Specia</author>
</authors>
<title>The USFD spoken language translation system for IWSLT</title>
<date>2014</date>
<booktitle>Proc. IWSLT,</booktitle>
<pages>86--91</pages>
<contexts>
<context position="12051" citStr="Ng et al., 2014" startWordPosition="1880" endWordPosition="1883">r machine translations by an online SMT system, and the post-editions of the translation by crowdsourced translators, with HTER used as label. IWSLT14: English-French dataset containing source language data from the 10-best (sentences) ASR system output. On the target side, the 1- best MT translation is used. The ASR system leads to different source segments, which in turn lead to different translations. METEOR (Banerjee and Lavie, 2005) is used to label these alternative translations against a reference (human) translation. Both ASR and MT outputs come from a system submission in IWSLT 2014 (Ng et al., 2014). The ASR system is a multi-pass deep neural network tandem system with feature and model adaptation and rescoring. The MT system is a phrasebased SMT system produced using Moses. Train Test Label 832 422 PEE 1-5 254 500 HTER 0-1 816 600 PEE 1-3 650 208 PET (ms) 11, 271 1, 817 HTER 0-1 8, 180 11, 240 MET. 0-1 Table 1: QE datasets: # sentences and labels. 5.2 CSLM Dataset The dataset used for CSLM training consists of Europarl, News-commentary and News-crawl corpus. We used a data selection method (Moore Dataset Lang. WMT12 en-es WMT13 en-es WMT14task1.1 en-es WMT14task1.3 en-es WMT15 en-es IWS</context>
</contexts>
<marker>Ng, Doulaty, Doddipatla, Saz, Hasan, Hain, Aziz, Shaf, Specia, 2014</marker>
<rawString>Raymond W. N. Ng, Mortaza Doulaty, Rama Doddipatla, Oscar Saz, Madina Hasan, Thomas Hain, Wilker Aziz, Kashif Shaf, and Lucia Specia. 2014. The USFD spoken language translation system for IWSLT 2014. Proc. IWSLT, pages 86–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Training neural network language models on very large corpora.</title>
<date>2005</date>
<booktitle>In Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>201--208</pages>
<contexts>
<context position="4802" citStr="Schwenk and Gauvain, 2005" startWordPosition="737" endWordPosition="741">Burch et al., 2012; Bojar et al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and machine translation (Schwenk, 2012). Recently, (Banchs et al., 2015) used a Latent Semantic Indexing approach to model sentences as bag-of-words in a continuous space to measure cross language adequacy. (Tan et al., 2015) proposed to train models with deep regression for machine translation evaluation in a task to measure semantic similarity between sentences. They reported positive results on simple features; larger feature sets did not improve these results. In this paper, we propose to estimate the probabilities of source and target segments with continuous space langua</context>
</contexts>
<marker>Schwenk, Gauvain, 2005</marker>
<rawString>Holger Schwenk and Jean-Luc Gauvain. 2005. Training neural network language models on very large corpora. In Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Fethi Bougares</author>
<author>Loic Barrault</author>
</authors>
<title>Efficient training strategies for deep neural network language models.</title>
<date>2014</date>
<booktitle>In NIPS workshop on Deep Learning and Representation Learning.</booktitle>
<contexts>
<context position="7503" citStr="Schwenk et al., 2014" startWordPosition="1179" endWordPosition="1182">nt the input wi−k with an N-dimensional vector. The output of CSLM is a vector of posterior probabilities for all words in vocabulary, P(wi|wi−1, wi−2, . . . , wi−x+1). Due to the large output layer (vocabulary size), the complexity of a basic neural network language model is very high. Schwenk (2007) proposed efficient training strategies in order to reduce the computational complexity and speed up the training time. They process several examples at once and use a short-list vocabulary V with only the most frequent words. Figure 1: Deep CSLM architecture. Following the settings mentioned in (Schwenk et al., 2014), all CSLM experiments described in this paper are performed using deep networks with four hidden layers: first layer for the projection (320 units for each context word) and three hidden layers of 1024 units with tanh activation. At the output layer, we use a softmax activation function applied to a short-list of the 32k most frequent words. The probabilities of the out-ofvocabulary words are obtained from a standard back-off n-gram language model. The projection of the words onto the continuous space and the training of the neural network is done by the standard back-propagation algorithm an</context>
</contexts>
<marker>Schwenk, Bougares, Barrault, 2014</marker>
<rawString>Holger Schwenk, Fethi Bougares, and Loic Barrault. 2014. Efficient training strategies for deep neural network language models. In NIPS workshop on Deep Learning and Representation Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>518</pages>
<contexts>
<context position="4818" citStr="Schwenk, 2007" startWordPosition="742" endWordPosition="743">t al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and machine translation (Schwenk, 2012). Recently, (Banchs et al., 2015) used a Latent Semantic Indexing approach to model sentences as bag-of-words in a continuous space to measure cross language adequacy. (Tan et al., 2015) proposed to train models with deep regression for machine translation evaluation in a task to measure semantic similarity between sentences. They reported positive results on simple features; larger feature sets did not improve these results. In this paper, we propose to estimate the probabilities of source and target segments with continuous space language models based </context>
<context position="7184" citStr="Schwenk (2007)" startWordPosition="1129" endWordPosition="1130">ecting words into a continuous space, and modelling and estimating probabilities in this space. The architecture of a deep CSLM is illustrated in Figure 1. The inputs to a CSLM model are the (K − 1) left-context words (wi−x+1, . . . , wi−2, wi−1) to predict wi. A onehot vector encoding scheme is used to represent the input wi−k with an N-dimensional vector. The output of CSLM is a vector of posterior probabilities for all words in vocabulary, P(wi|wi−1, wi−2, . . . , wi−x+1). Due to the large output layer (vocabulary size), the complexity of a basic neural network language model is very high. Schwenk (2007) proposed efficient training strategies in order to reduce the computational complexity and speed up the training time. They process several examples at once and use a short-list vocabulary V with only the most frequent words. Figure 1: Deep CSLM architecture. Following the settings mentioned in (Schwenk et al., 2014), all CSLM experiments described in this paper are performed using deep networks with four hidden layers: first layer for the projection (320 units for each context word) and three hidden layers of 1024 units with tanh activation. At the output layer, we use a softmax activation f</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech &amp; Language, 21(3):492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>1071--1080</pages>
<contexts>
<context position="2672" citStr="Schwenk, 2012" startWordPosition="403" endWordPosition="404">ring the context of the text. QE performance usually differs depending on the language pair, the specific quality score being optimised (e.g., post-editing time vs translation adequacy) and the feature set. Features based on n-gram language models, despite their simplicity, are among those with the best performance in most QE tasks (Shah et al., 2013b). However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling. Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words (Schwenk, 2012; Mikolov et al., 2013). The assumption of these models is that semantically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space. The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events. The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multiple (called deep networks) hidden layers. Deep neural networks have been shown to perform better than shallow ones due to their cap</context>
<context position="4858" citStr="Schwenk, 2012" startWordPosition="748" endWordPosition="749"> the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including automatic speech recognition (Schwenk and Gauvain, 2005; Schwenk, 2007) and machine translation (Schwenk, 2012). Recently, (Banchs et al., 2015) used a Latent Semantic Indexing approach to model sentences as bag-of-words in a continuous space to measure cross language adequacy. (Tan et al., 2015) proposed to train models with deep regression for machine translation evaluation in a task to measure semantic similarity between sentences. They reported positive results on simple features; larger feature sets did not improve these results. In this paper, we propose to estimate the probabilities of source and target segments with continuous space language models based on a deep architecture and to use these </context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In COLING (Posters), pages 1071–1080.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kashif Shah</author>
</authors>
<title>Eleftherios Avramidis, Ergun Bic¸icic, and Lucia Specia. 2013a. Quest - design, implementation and extensions of a framework for machine translation quality estimation. The Prague Bulletin of Mathematical Linguistics,</title>
<pages>100--19</pages>
<marker>Shah, </marker>
<rawString>Kashif Shah, Eleftherios Avramidis, Ergun Bic¸icic, and Lucia Specia. 2013a. Quest - design, implementation and extensions of a framework for machine translation quality estimation. The Prague Bulletin of Mathematical Linguistics, 100:19–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>An investigation on the effectiveness of features for translation quality estimation.</title>
<date>2013</date>
<booktitle>In Machine Translation Summit,</booktitle>
<volume>14</volume>
<pages>167--174</pages>
<contexts>
<context position="2411" citStr="Shah et al., 2013" startWordPosition="362" endWordPosition="365">tically motivated features. They include features that summarise how the MT systems generate translations, as well as features that are oblivious to the systems. The majority of the features in the literature are extracted from each sentence pair in isolation, ignoring the context of the text. QE performance usually differs depending on the language pair, the specific quality score being optimised (e.g., post-editing time vs translation adequacy) and the feature set. Features based on n-gram language models, despite their simplicity, are among those with the best performance in most QE tasks (Shah et al., 2013b). However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling. Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words (Schwenk, 2012; Mikolov et al., 2013). The assumption of these models is that semantically or grammatically related words are mapped to similar geometric locations in a high-dimensional continuous space. The probability distribution is thus much smoother and therefore the model has a better generalisation power on unseen events. The representations ar</context>
<context position="13475" citStr="Shah et al., 2013" startWordPosition="2125" endWordPosition="2128">f the WMT translation track. For English-French, the development set is the concatenation of the IWSLT dev2010 and eval2010. In Table 2 we show statistics on the selected monolingual data used to train back-off LM and CSLM. Lang. Train Dev LM ppl CSLM ppl en 4.3G 137.7k 164.63 116.58 (29.18%) fr 464.7M 54K 99.34 64.88 (34.68%) es 21.2M 149.4k 145.49 87.14 (40.10%) Table 2: Training data size (number of tokens) and language models perplexity (ppl). The values in parentheses in last column shows percentage decrease in perplexity. 5.3 Feature Sets We use the QuEst 3 toolkit (Specia et al., 2013; Shah et al., 2013a) to extract two feature sets for each dataset: • BL: 17 features used as baseline in the WMT shared tasks on QE. • AF: 80 augmented MT system-independent features4 (superset of BL). For the En-Fr SLT task, we have additional 36 features (21 ASR + 15 MT-dependent features) The resources used to extract these features (corpora, etc.) are also available as part of the WMT shared tasks on QE. The CSLM features for each of the source and target segments are extracted using the procedure described in Section 3 with the CSLM toolkit. 5 We trained QE models with following combination of features: • </context>
<context position="14785" citStr="Shah et al., 2013" startWordPosition="2329" endWordPosition="2332"> AF + CSLMsrc,tgt: CSLM features for source and target segments, plus all available features. For the WMT12 task, we performed further experiments to analyse the improvements with CSLM: • CSLMsrc: Source side CSLM feature only. • CSLMtgt: Target side CSLM feature only. • CSLMsrc,tgt: Source and target CSLM features by themselves. 3http://www.quest.dcs.shef.ac.uk/ 480 features http://www.quest.dcs.shef.ac. uk/quest_files/features_blackbox 5http://www-lium.univ-lemans.fr/cslm/ • FS(AF) + CSLMsrc,tgt: CSLM features in addition to the best performing feature set (FS(AF)) selected as described in (Shah et al., 2013b; Shah et al., 2015). 5.4 Learning algorithms We use the Support Vector Machines implementation of the scikit-learn toolkit to perform regression (SVR) with either Radial Basis Function (RBF) or linear kernel and parameters optimised via grid search. To evaluate the prediction models we use Mean Absolute Error (MAE), its squared version – Root Mean Squared Error (RMSE), and Pearson’s correlation (r) score. Task System #feats MAE RMSE r WMT12 BL 17 0.6821 0.8117 0.5595 AF 80 0.6717 0.8103 0.5645 BL + CSLMsrc,tgt 19 0.6463 0.7977 0.5805 AF + CSLMsrc,tgt 82 0.6462 0.7946 0.5825 WMT13 BL 17 0.141</context>
<context position="18560" citStr="Shah et al., 2013" startWordPosition="2962" endWordPosition="2965">t side feature directly reflects the fluency of the translation, whereas the source side feature (regarded as a translation complexity feature) only has indirect effect on quality. Interestingly, the two CSLM features alone give comparable results (slightly worse) than the BL feature set 6 despite the fact that these 17 features cover many complexity, adequacy and fluency quality aspects. CSLM features bring further improvements on pre-selected feature sets, as shown in Table 3. We also performed feature selection over the full feature set along with CSLM features, following the procedure in (Shah et al., 2013b). Interestingly, both CSLM features were selected among the top ranked features, confirming their relevance. In order to investigate whether our CSLM features results hold for other feature sets, we experimented with the feature sets provided by most teams participating in the WMT12 QE shared task. These feature sets are very diverse in terms of the types of features, resources used, and their sizes. Table 5 shows the official results from the shared task (Off.) (Callison-Burch et al., 2012), those from training an SVR on these features with and without CSLM features. Note that the official </context>
</contexts>
<marker>Shah, Cohn, Specia, 2013</marker>
<rawString>Kashif Shah, Trevor Cohn, and Lucia Specia. 2013b. An investigation on the effectiveness of features for translation quality estimation. In Machine Translation Summit, volume 14, pages 167–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>A bayesian non-linear method for feature selection in machine translation quality estimation.</title>
<date>2015</date>
<journal>Machine Translation,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="14806" citStr="Shah et al., 2015" startWordPosition="2333" endWordPosition="2336">SLM features for source and target segments, plus all available features. For the WMT12 task, we performed further experiments to analyse the improvements with CSLM: • CSLMsrc: Source side CSLM feature only. • CSLMtgt: Target side CSLM feature only. • CSLMsrc,tgt: Source and target CSLM features by themselves. 3http://www.quest.dcs.shef.ac.uk/ 480 features http://www.quest.dcs.shef.ac. uk/quest_files/features_blackbox 5http://www-lium.univ-lemans.fr/cslm/ • FS(AF) + CSLMsrc,tgt: CSLM features in addition to the best performing feature set (FS(AF)) selected as described in (Shah et al., 2013b; Shah et al., 2015). 5.4 Learning algorithms We use the Support Vector Machines implementation of the scikit-learn toolkit to perform regression (SVR) with either Radial Basis Function (RBF) or linear kernel and parameters optimised via grid search. To evaluate the prediction models we use Mean Absolute Error (MAE), its squared version – Root Mean Squared Error (RMSE), and Pearson’s correlation (r) score. Task System #feats MAE RMSE r WMT12 BL 17 0.6821 0.8117 0.5595 AF 80 0.6717 0.8103 0.5645 BL + CSLMsrc,tgt 19 0.6463 0.7977 0.5805 AF + CSLMsrc,tgt 82 0.6462 0.7946 0.5825 WMT13 BL 17 0.1411 0.1812 0.4612 AF 80</context>
</contexts>
<marker>Shah, Cohn, Specia, 2015</marker>
<rawString>Kashif Shah, Trevor Cohn, and Lucia Specia. 2015. A bayesian non-linear method for feature selection in machine translation quality estimation. Machine Translation, 29(2):101–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jos´e G C de Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>QuEst - A translation quality estimation framework.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics: Demo Session,</booktitle>
<pages>79--84</pages>
<location>Sofia, Bulgaria.</location>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jos´e G. C. de Souza, and Trevor Cohn. 2013. QuEst - A translation quality estimation framework. In 51st Annual Meeting of the Association for Computational Linguistics: Demo Session, pages 79–84, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Carolina Scarton</author>
<author>Lucia Specia</author>
<author>Josef van Genabith</author>
</authors>
<title>Usaar-sheffield: Semantic textual similarity with deep regression and machine translation evaluation metrics.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation,</booktitle>
<pages>85--89</pages>
<location>Denver, Colorado.</location>
<marker>Tan, Scarton, Specia, van Genabith, 2015</marker>
<rawString>Liling Tan, Carolina Scarton, Lucia Specia, and Josef van Genabith. 2015. Usaar-sheffield: Semantic textual similarity with deep regression and machine translation evaluation metrics. In Proceedings of the 9th International Workshop on Semantic Evaluation, pages 85–89, Denver, Colorado.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
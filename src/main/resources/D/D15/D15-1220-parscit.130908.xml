<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002337">
<title confidence="0.998665">
Concept-based Summarization using Integer Linear Programming:
From Concept Pruning to Multiple Optimal Solutions
</title>
<author confidence="0.5620275">
Florian Boudins Hugo Mougards Benoit Favre2
s LINA - UMR CNRS 6241, Universit´e de Nantes, France
</author>
<affiliation confidence="0.315338">
{florian.boudin,hugo.mougard}@univ-nantes.fr
</affiliation>
<address confidence="0.43019">
2 LIF - UMR CNRS 7279, Universit´e Aix-Marseille, France
</address>
<email confidence="0.989576">
benoit.favre@lif.univ-mrs.fr
</email>
<sectionHeader confidence="0.996985" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999304">
In concept-based summarization, sentence
selection is modelled as a budgeted maxi-
mum coverage problem. As this problem
is NP-hard, pruning low-weight concepts
is required for the solver to find optimal
solutions efficiently. This work shows that
reducing the number of concepts in the
model leads to lower ROUGE scores, and
more importantly to the presence of mul-
tiple optimal solutions. We address these
issues by extending the model to provide a
single optimal solution, and eliminate the
need for concept pruning using an approx-
imation algorithm that achieves compara-
ble performance to exact inference.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999682">
Recent years have witnessed increased interest in
global inference methods for extractive summa-
rization. These methods formulate summarization
as a combinatorial optimization problem, i.e. se-
lecting a subset of sentences that maximizes an
objective function under a length constraint, and
use Integer Linear Programming (ILP) to solve it
exactly (McDonald, 2007).
In this work, we focus on the concept-based ILP
model for summarization introduced by (Gillick
and Favre, 2009). In their model, a summary is
generated by assembling the subset of sentences
that maximizes a function of the unique concepts it
covers. Selecting the optimal subset of sentences
is then cast as an instance of the budgeted maxi-
mum coverage problem1.
As this problem is NP-hard, pruning low-weight
concepts is required for the ILP solver to find opti-
mal solutions efficiently (Gillick and Favre, 2009;
</bodyText>
<footnote confidence="0.5508405">
1Given a collection S of sets with associated costs and a
budget L, find a subset S&apos; ⊆ S such that the total cost of
sets in S&apos; does not exceed L, and the total weight of elements
covered by S&apos; is maximized (Khuller et al., 1999).
</footnote>
<bodyText confidence="0.989853388888889">
Qian and Liu, 2013; Li et al., 2013). However, re-
ducing the number of concepts in the model has
two undesirable consequences. First, it forces the
model to only use a limited number of concepts to
rank summaries, resulting in lower ROUGE scores.
Second, by reducing the number of items from
which sentence scores are derived, it allows dif-
ferent sentences to have the same score, and ulti-
mately leads to multiple optimal summaries.
To our knowledge, no previous work has men-
tioned these problems, and only results corre-
sponding to the first optimal solution found by the
solver are reported. However, as we will show
through experiments, these multiple optimal so-
lutions cause a substantial amount of variation in
ROUGE scores, which, if not accounted for, could
lead to incorrect conclusions. More specifically,
the contributions of this work are as follows:
</bodyText>
<listItem confidence="0.9971517">
• We evaluate (Gillick and Favre, 2009)’s sum-
marization model at various concept pruning
levels. In doing so, we quantify the impact of
pruning on running time, ROUGE scores and
the number of optimal solutions.
• We extend the model to address the prob-
lem of multiple optimal solutions, and we
sidestep the need for concept pruning by de-
veloping a fast approximation algorithm that
achieves near-optimal performance.
</listItem>
<sectionHeader confidence="0.93039" genericHeader="method">
2 Concept-based ILP Summarization
</sectionHeader>
<subsectionHeader confidence="0.990928">
2.1 Model definition
</subsectionHeader>
<bodyText confidence="0.999956555555556">
Gillick and Favre (2009) introduce a concept-
based ILP model for summarization that casts sen-
tence selection as a maximum coverage problem.
The key assumption of their model is that the value
of a summary is defined as the sum of the weights
of the unique concepts it contains. That way, re-
dundancy within the summary is addressed im-
plicitly at a sub-sentence level: a summary only
benefits from including each concept once.
</bodyText>
<page confidence="0.955845">
1914
</page>
<note confidence="0.655404">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1914–1918,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999155571428571">
Formally, let wi be the weight of concept i, ci
and sj two binary variables indicating the pres-
ence of concept i and sentence j in the summary,
Occij an indicator of the occurrence of concept i
in sentence j, lj the length of sentence j and L the
length limit for the summary, the concept-based
ILP model is described as:
</bodyText>
<equation confidence="0.9983564">
�max wici (1)
i
�s.t. ljsj :5 L (2)
j
sjOccij :5 ci, Vi, j (3)
�
sjOccij �! ci, Vi (4)
j
ci E {0, 11 Vi
sj E {0, 11 Vj
</equation>
<bodyText confidence="0.999834571428571">
The constraints formalized in equations 3 and 4
ensure the consistency of the solution: selecting a
sentence leads to the selection of all the concepts it
contains, and selecting a concept is only possible
if it is present in at least one selected sentence.
Choosing a suitable definition for concepts and
a method to estimate their weights are the two
key factors that affect the performance of this
model. Bigrams of words are usually used as a
proxy for concepts (Gillick and Favre, 2009; Berg-
Kirkpatrick et al., 2011). Concept weights are
either estimated by heuristic counting, e.g. docu-
ment frequency in (Gillick and Favre, 2009), or
obtained by supervised learning (Li et al., 2013).
</bodyText>
<subsectionHeader confidence="0.99976">
2.2 Pruning to reduce complexity
</subsectionHeader>
<bodyText confidence="0.99999484375">
The concept-level formulation of (Gillick and
Favre, 2009) is an instance of the budgeted maxi-
mum coverage problem, and solving such a prob-
lem is NP-hard (Khuller et al., 1999). Keeping the
number of variables and constraints small is then
critical to reduce the model complexity.
In previous work, efficient summarization was
achieved by pruning concepts. One way to re-
duce the number of concepts in the model is to
remove those concepts that have a weight below
a given threshold (Gillick and Favre, 2009). An-
other way is to consider only the top-n highest
weighted concepts (Li et al., 2013). Once low-
weight concepts are pruned, sentences that do not
contain any remaining concepts are removed, fur-
ther reducing the number of variables and con-
straints in the model. As such, this can be regarded
as a way to approximate the problem.
Pruning concepts to reduce complexity also cuts
down the number of items from which summary
scores are derived. As we will see in Section 3.2,
this results in a lower ROUGE scores and leads to
the production of multiple optimal summaries.
The concept weighting function also plays an
important role in the presence of multiple opti-
mal solutions. Limited-range functions, such as
frequency-based ones, yield many ties and in-
crease the likelihood that different sentences have
the same score. Redundancy within the set of
input sentences exacerbate this problem, since
highly similar sentences are likely to contain the
same concepts.
</bodyText>
<subsectionHeader confidence="0.999554">
2.3 Summarization parameters
</subsectionHeader>
<bodyText confidence="0.997079962962963">
For comparison purposes, we use the same system
pipeline as in (Gillick et al., 2009), which is de-
scribed below.
Step 1: clean input documents; a set of rules is
used to remove bylines and format markup.
Step 2: split the text into sentences; we use
splitta2 (Gillick, 2009) and re-attach multi-
sentence quotations.
Step 3: compute parameters needed by the
model; we extract and weight the concepts.
Step 4: prune sentences shorter than 10 words,
duplicate sentences and those that begin and end
with a quotation mark.
Step 5: map to ILP format and solve; we use an
off-the-shelf ILP solver3.
Step 6: order selected sentences for inclusion in
the summary, first by source and then by position.
Similar to previous work, we use bigrams of
words as concepts. Although bigrams are rough
approximations of concepts, they are simple to ex-
tract and match, and have been shown to perform
well at this task. Bigrams of words consisting of
two stop words4 or containing a punctuation mark
are discarded. Stemming5 is then applied to allow
more robust matching.
Concepts are weighted using document fre-
quency, i.e. the number of source documents
</bodyText>
<footnote confidence="0.9854555">
2We use splitta v1.03, https://code.google.
com/p/splitta/
3We use glpk v4.52, https://www.gnu.org/
software/glpk/
4We use the stoplist in nltk, http://www.nltk.org/
5We use the Porter stemmer in nltk.
</footnote>
<page confidence="0.887217">
1915
</page>
<table confidence="0.999301166666667">
DUC’04 TAC’08
DF 1 2 3 4 1 2 3 4
# solutions 1.3 1.3 1.5 1.5 1.2 1.3 1.8 4.8
# concepts 2 955 676 247 107 2 909 393 127 56
# sentences 184 175 159 139 174 167 149 129
Avg. time (sec) 22.3 1.7 0.5 0.3 21.5 0.8 0.3 0.2
</table>
<tableCaption confidence="0.9696645">
Table 1: Average number of optimal solutions, concepts and sentences for different minimum document
frequencies. The average time in seconds for finding the first optimal solution is also reported.
</tableCaption>
<bodyText confidence="0.999947785714286">
where the concept was seen. Document frequency
is a simple, yet effective approach to concept
weighting (Gillick and Favre, 2009; Woodsend
and Lapata, 2012; Qian and Liu, 2013). Reducing
the number of concepts in the ILP model is then
performed by pruning those concepts that occur in
fewer than a given number of documents.
ILP solvers usually provide only one solution.
To generate alternate optimal solutions, we iter-
atively add new constraints to the problem that
eliminate already found optimal solutions and re-
run the solver. We stop the iterations when the
value of the objective function returned by the
solver changes.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998966">
3.1 Datasets and evaluation measures
</subsectionHeader>
<bodyText confidence="0.999978714285714">
Experiments are conducted on the DUC’04 and
TAC’08 datasets. For DUC’04, we use the 50 top-
ics from the generic multi-document summariza-
tion task (Task 2). For TAC’08, we focus only on
the 48 topics from the non-update summarization
task. Each topic contains 10 newswire articles for
which the task is to generate a summary no longer
than 100 words (whitespace-delimited tokens).
Summaries are evaluated against reference sum-
maries using the ROUGE automatic evaluation
measures (Lin, 2004). We set the ROUGE param-
eters to those6 that lead to highest agreement with
manual evaluation (Owczarzak et al., 2012), that
is, with stemming and stopwords not removed.
</bodyText>
<subsectionHeader confidence="0.731405">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.99416388372093">
Table 1 presents the average number of optimal
solutions at different levels of concept pruning.
Overall, the average number of optimal solutions
increases along with the minimum document fre-
quency, reaching 4.8 for TAC’08 at DF = 4. Prun-
6We use ROUGE-1.5.5 with the parameters: n 4 -m -a
-l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0
ing concepts also greatly reduces the number of
variables in the ILP formulation, and consequently
improves the run-time for solving the problem.
Interestingly, we note that, even without any
pruning, the model produces multiple optimal
solutions. The choice of document frequency
for weighting concepts is responsible for this as
it generates many ties. Finer-grained concept
weighting functions such as frequency estima-
tion (Li et al., 2013) should therefore be preferred
to limit the number of multiple optimal solutions.
The mean ROUGE recall scores of the multiple
optimal solutions for different minimal document
frequencies are presented in Table 2. Here, the
higher the concept pruning threshold, the higher
the variability of the generated summaries as in-
dicated by the standard deviation. Best ROUGE
scores are achieved without concept pruning while
the best compromise between effectiveness and
run-time is given when DF ≥ 3, confirming the
findings of (Gillick and Favre, 2009).
To show in a realistic scenario how multiple
optimal solutions could lead to different conclu-
sions, we compare in Table 3 the ROUGE-1 scores
of the summaries generated from the first op-
timal solution found by three off-the-shelf ILP
solvers against that of the systems7 that partici-
pated at TAC’08. We set the minimum document
frequency to 3, which is often used in previous
work (Gillick and Favre, 2009; Li et al., 2013),
and use a two-sided Wilcoxon signed-rank to com-
pute the number of systems that obtain signifi-
cantly lower and higher ROUGE-1 recall scores8.
Despite being comparable (p-value &gt; 0.4), the
solutions found by the three solvers support differ-
ent conclusions. The solution found using GLPK
</bodyText>
<footnote confidence="0.9706606">
771 systems participated at TAC’08 but we removed
ICSI1 and ICSI2 systems which are based on the concept-
based ILP model.
8ROUGE-1 recall is most accurate metric to identify the
better summary in a pair (Owczarzak et al., 2012).
</footnote>
<page confidence="0.980226">
1916
</page>
<table confidence="0.955786333333333">
DUC’04 TAC’08
DF ROUGE-1 ROUGE-2 ROUGE-4 ROUGE-1 ROUGE-2 ROUGE-4
1 37.74 ±0.07 9.48 ±0.05 1.45 ±0.02 37.65 ±0.10 10.63 ±0.08 2.23 ±0.04
2 37.25 ±0.08 9.14 ±0.02 1.37 ±0.01 37.16 ±0.11 9.96 ±0.07 2.05 ±0.03
3 37.37 ±0.11 9.16 ±0.06 1.41 ±0.02 37.39 ±0.15 10.62 ±0.07 2.13 ±0.03
4 37.96 ±0.10 9.38 ±0.05 1.57 ±0.02 36.73 ±0.12 10.10 ±0.08 1.78 ±0.07
</table>
<tableCaption confidence="0.9911">
Table 2: Mean ROUGE recall and standard deviation for different minimum document frequencies.
</tableCaption>
<table confidence="0.999615">
Solver ROUGE-1 1 / T
GLPK 37.33 54 / 0
Gurobi 37.20 52 / 1
CPLEX 37.17 51 / 1
</table>
<tableCaption confidence="0.995457">
Table 3: ROUGE-1 recall scores for the first opti-
</tableCaption>
<bodyText confidence="0.850324272727273">
mal solution found by different solvers along with
the number of systems that obtain significantly
lower (1) or higher (T) scores (p-value &lt; 0.05).
indicates that the concept-based model achieves
state-of-the-art performance whereas the solutions
provided by Gurobi and CPLEX do not do so. The
reason for these differences is the use of differ-
ent solving strategies, involving heuristics for find-
ing feasible solutions more quickly. This exam-
ple demonstrates that multiple optimal solutions
should be considered during evaluation.
</bodyText>
<subsectionHeader confidence="0.999496">
3.3 Solving the multiple solution problem
</subsectionHeader>
<bodyText confidence="0.9999584">
Multiple optimal solutions occur when concepts
alone are not sufficient to distinguish between two
competing summary candidates. Extending the
model so that it provides a single solution can
therefore not be done without introducing a sec-
ond term in the objective function. Following the
observation that the frequency of a non-stop word
in a document set is a good predictor of a word ap-
pearing in a human summary (Nenkova and Van-
derwende, 2005), we extend equation 1 as follows:
</bodyText>
<equation confidence="0.9629335">
�max �wici + p fktk (5)
i k
</equation>
<bodyText confidence="0.999975230769231">
where fk is the frequency of non-stop word k in
the document set, and tk is a binary variable indi-
cating the presence of k in the summary. Here, we
want to induce a single solution among the multi-
ple optimal solutions given by concept weighting,
and thus set p to a small value (10−6). We add
further constraints, similar to equations 3 and 4, to
ensure the consistency of the solution.
This extended model succeeds in giving a sin-
gle solution that is at least comparable to the mean
score of the multiple optimal solutions. How-
ever, it requires about twice as much time to solve
which makes it impractical for large documents.
</bodyText>
<subsectionHeader confidence="0.993831">
3.4 Fast approximation
</subsectionHeader>
<bodyText confidence="0.999888619047619">
Instead of pruning concepts to reduce complex-
ity, one may consider using an approximation
if results are found satisfactory. Here, simi-
larly to (Takamura and Okumura, 2009; Lin and
Bilmes, 2010) we implement the greedy heuristic
proposed in (Khuller et al., 1999) that solve the
budgeted maximum coverage problem with a per-
formance guarantee 1/2 · (1 − 1/e). Table 4 com-
pares the performance of the model that achieves
the best trade off between effectiveness and run-
time, that is when DF &gt; 3, with that of the greedy
approximation without pruning.
Overall, the approximate solution is over 96%
as good as the average optimal solution. Although
the ILP solution marks an upper bound on perfor-
mance, its solving time is exponential in the num-
ber of input sentences. The approximate method
is then relevant as it marks an upper bound on
speed (less than 0.01 seconds to compute) while
having performance comparable to the ILP model
with concept pruning (p-value &gt; 0.3).
</bodyText>
<table confidence="0.987222333333333">
Dataset ROUGE-1 ROUGE-2
DUC’04 37.14 (−0.7%) 9.37 (+2.3%)
TAC’08 36.90 (−1.3%) 10.27 (−3.3%)
</table>
<tableCaption confidence="0.973831">
Table 4: ROUGE recall scores of the approxima-
</tableCaption>
<bodyText confidence="0.725251">
tion. The relative difference from the mean score
of the multiple optimal solutions is also reported.
</bodyText>
<sectionHeader confidence="0.999559" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.997775">
Multiple optimal solutions are not an issue as long
as alternate solutions are equivalent. Unfortu-
nately, summaries generated from different sets of
</bodyText>
<page confidence="0.986413">
1917
</page>
<bodyText confidence="0.999953588235294">
sentences are likely to differ. We showed through
experiments that concept pruning leads to the pres-
ence of multiple optimal solutions, and that the
latter cause a substantial amount of variation in
ROUGE scores. We proposed an extension of the
ILP that obtains unique solutions. If speed is a
concern, we showed that a near-optimal approx-
imation can be computed without pruning. The
implementation of the concept-based summariza-
tion model that we use in this study is available at
https://github.com/boudinfl/sume.
In future work, we intend to extend our study
to compressive summarization. We expect that the
number of optimal solutions will increase as mul-
tiple compression candidates, which are likely to
be similar in content, are added to the set of input
sentences.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999346">
This work was partially supported by the GOLEM
project (grant of CNRS PEPS FaSciDo 2015,
http://boudinfl.github.io/GOLEM/).
We thank the anonymous reviewers and R´emi
Bois for their insightful comments.
</bodyText>
<sectionHeader confidence="0.998867" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999753142857143">
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 481–490, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the
Workshop on Integer Linear Programming for Nat-
ural Language Processing, pages 10–18, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
icsi/utd summarization system at tac 2009. In Pro-
ceedings of the Second Text Analysis Conference,
Gaithersburg, Maryland, USA. National Institute of
Standards and Technology.
Dan Gillick. 2009. Sentence boundary detection and
the problem with the u.s. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 241–244, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Samir Khuller, Anna Moss, and Joseph (Seffi) Naor.
1999. The budgeted maximum coverage problem.
Information Processing Letters, 70(1):39 – 45.
Chen Li, Xian Qian, and Yang Liu. 2013. Using super-
vised bigram-based ilp for extractive summarization.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 1004–1013, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 912–920, Los Angeles, California,
June. Association for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European Conference on IR
Research, ECIR’07, pages 557–564, Berlin, Heidel-
berg. Springer-Verlag.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-TR-
2005-101.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment
of the accuracy of automatic evaluation in summa-
rization. In Proceedings of Workshop on Evaluation
Metrics and System Comparison forAutomatic Sum-
marization, pages 1–9, Montr´eal, Canada, June. As-
sociation for Computational Linguistics.
Xian Qian and Yang Liu. 2013. Fast joint compres-
sion and summarization via graph cuts. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1492–1502,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum cover-
age problem and its variant. In Proceedings of the
12th Conference of the European Chapter of the
ACL (EACL 2009), pages 781–789, Athens, Greece,
March. Association for Computational Linguistics.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 233–243, Jeju Island, Korea, July.
Association for Computational Linguistics.
</reference>
<page confidence="0.994941">
1918
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.226252">
<title confidence="0.8481485">Concept-based Summarization using Integer Linear From Concept Pruning to Multiple Optimal Solutions</title>
<affiliation confidence="0.4968355">UMR CNRS 6241, Universit´e de Nantes, - UMR CNRS 7279, Universit´e Aix-Marseille,</affiliation>
<email confidence="0.978387">benoit.favre@lif.univ-mrs.fr</email>
<abstract confidence="0.9981519375">In concept-based summarization, sentence selection is modelled as a budgeted maximum coverage problem. As this problem is NP-hard, pruning low-weight concepts is required for the solver to find optimal solutions efficiently. This work shows that reducing the number of concepts in the leads to lower and more importantly to the presence of multiple optimal solutions. We address these issues by extending the model to provide a single optimal solution, and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>481--490</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 481–490, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1447" citStr="Gillick and Favre, 2009" startWordPosition="202" endWordPosition="205"> and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference. 1 Introduction Recent years have witnessed increased interest in global inference methods for extractive summarization. These methods formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of sentences that maximizes an objective function under a length constraint, and use Integer Linear Programming (ILP) to solve it exactly (McDonald, 2007). In this work, we focus on the concept-based ILP model for summarization introduced by (Gillick and Favre, 2009). In their model, a summary is generated by assembling the subset of sentences that maximizes a function of the unique concepts it covers. Selecting the optimal subset of sentences is then cast as an instance of the budgeted maximum coverage problem1. As this problem is NP-hard, pruning low-weight concepts is required for the ILP solver to find optimal solutions efficiently (Gillick and Favre, 2009; 1Given a collection S of sets with associated costs and a budget L, find a subset S&apos; ⊆ S such that the total cost of sets in S&apos; does not exceed L, and the total weight of elements covered by S&apos; is </context>
<context position="2980" citStr="Gillick and Favre, 2009" startWordPosition="464" endWordPosition="467">r of items from which sentence scores are derived, it allows different sentences to have the same score, and ultimately leads to multiple optimal summaries. To our knowledge, no previous work has mentioned these problems, and only results corresponding to the first optimal solution found by the solver are reported. However, as we will show through experiments, these multiple optimal solutions cause a substantial amount of variation in ROUGE scores, which, if not accounted for, could lead to incorrect conclusions. More specifically, the contributions of this work are as follows: • We evaluate (Gillick and Favre, 2009)’s summarization model at various concept pruning levels. In doing so, we quantify the impact of pruning on running time, ROUGE scores and the number of optimal solutions. • We extend the model to address the problem of multiple optimal solutions, and we sidestep the need for concept pruning by developing a fast approximation algorithm that achieves near-optimal performance. 2 Concept-based ILP Summarization 2.1 Model definition Gillick and Favre (2009) introduce a conceptbased ILP model for summarization that casts sentence selection as a maximum coverage problem. The key assumption of their </context>
<context position="4968" citStr="Gillick and Favre, 2009" startWordPosition="808" endWordPosition="811">described as: �max wici (1) i �s.t. ljsj :5 L (2) j sjOccij :5 ci, Vi, j (3) � sjOccij �! ci, Vi (4) j ci E {0, 11 Vi sj E {0, 11 Vj The constraints formalized in equations 3 and 4 ensure the consistency of the solution: selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept is only possible if it is present in at least one selected sentence. Choosing a suitable definition for concepts and a method to estimate their weights are the two key factors that affect the performance of this model. Bigrams of words are usually used as a proxy for concepts (Gillick and Favre, 2009; BergKirkpatrick et al., 2011). Concept weights are either estimated by heuristic counting, e.g. document frequency in (Gillick and Favre, 2009), or obtained by supervised learning (Li et al., 2013). 2.2 Pruning to reduce complexity The concept-level formulation of (Gillick and Favre, 2009) is an instance of the budgeted maximum coverage problem, and solving such a problem is NP-hard (Khuller et al., 1999). Keeping the number of variables and constraints small is then critical to reduce the model complexity. In previous work, efficient summarization was achieved by pruning concepts. One way t</context>
<context position="8585" citStr="Gillick and Favre, 2009" startWordPosition="1412" endWordPosition="1415">stoplist in nltk, http://www.nltk.org/ 5We use the Porter stemmer in nltk. 1915 DUC’04 TAC’08 DF 1 2 3 4 1 2 3 4 # solutions 1.3 1.3 1.5 1.5 1.2 1.3 1.8 4.8 # concepts 2 955 676 247 107 2 909 393 127 56 # sentences 184 175 159 139 174 167 149 129 Avg. time (sec) 22.3 1.7 0.5 0.3 21.5 0.8 0.3 0.2 Table 1: Average number of optimal solutions, concepts and sentences for different minimum document frequencies. The average time in seconds for finding the first optimal solution is also reported. where the concept was seen. Document frequency is a simple, yet effective approach to concept weighting (Gillick and Favre, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). Reducing the number of concepts in the ILP model is then performed by pruning those concepts that occur in fewer than a given number of documents. ILP solvers usually provide only one solution. To generate alternate optimal solutions, we iteratively add new constraints to the problem that eliminate already found optimal solutions and rerun the solver. We stop the iterations when the value of the objective function returned by the solver changes. 3 Experiments 3.1 Datasets and evaluation measures Experiments are conducted on the DUC’04 and TAC’0</context>
<context position="11125" citStr="Gillick and Favre, 2009" startWordPosition="1820" endWordPosition="1823">ned concept weighting functions such as frequency estimation (Li et al., 2013) should therefore be preferred to limit the number of multiple optimal solutions. The mean ROUGE recall scores of the multiple optimal solutions for different minimal document frequencies are presented in Table 2. Here, the higher the concept pruning threshold, the higher the variability of the generated summaries as indicated by the standard deviation. Best ROUGE scores are achieved without concept pruning while the best compromise between effectiveness and run-time is given when DF ≥ 3, confirming the findings of (Gillick and Favre, 2009). To show in a realistic scenario how multiple optimal solutions could lead to different conclusions, we compare in Table 3 the ROUGE-1 scores of the summaries generated from the first optimal solution found by three off-the-shelf ILP solvers against that of the systems7 that participated at TAC’08. We set the minimum document frequency to 3, which is often used in previous work (Gillick and Favre, 2009; Li et al., 2013), and use a two-sided Wilcoxon signed-rank to compute the number of systems that obtain significantly lower and higher ROUGE-1 recall scores8. Despite being comparable (p-value</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 10–18, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
<author>Berndt Bohnet</author>
<author>Yang Liu</author>
<author>Shasha Xie</author>
</authors>
<title>The icsi/utd summarization system at tac</title>
<date>2009</date>
<booktitle>In Proceedings of the Second Text Analysis Conference,</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, Maryland, USA.</location>
<contexts>
<context position="6785" citStr="Gillick et al., 2009" startWordPosition="1105" endWordPosition="1108"> in Section 3.2, this results in a lower ROUGE scores and leads to the production of multiple optimal summaries. The concept weighting function also plays an important role in the presence of multiple optimal solutions. Limited-range functions, such as frequency-based ones, yield many ties and increase the likelihood that different sentences have the same score. Redundancy within the set of input sentences exacerbate this problem, since highly similar sentences are likely to contain the same concepts. 2.3 Summarization parameters For comparison purposes, we use the same system pipeline as in (Gillick et al., 2009), which is described below. Step 1: clean input documents; a set of rules is used to remove bylines and format markup. Step 2: split the text into sentences; we use splitta2 (Gillick, 2009) and re-attach multisentence quotations. Step 3: compute parameters needed by the model; we extract and weight the concepts. Step 4: prune sentences shorter than 10 words, duplicate sentences and those that begin and end with a quotation mark. Step 5: map to ILP format and solve; we use an off-the-shelf ILP solver3. Step 6: order selected sentences for inclusion in the summary, first by source and then by po</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, Bohnet, Liu, Xie, 2009</marker>
<rawString>Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, Berndt Bohnet, Yang Liu, and Shasha Xie. 2009. The icsi/utd summarization system at tac 2009. In Proceedings of the Second Text Analysis Conference, Gaithersburg, Maryland, USA. National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
</authors>
<title>Sentence boundary detection and the problem with the u.s.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>241--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="6974" citStr="Gillick, 2009" startWordPosition="1141" endWordPosition="1142">iple optimal solutions. Limited-range functions, such as frequency-based ones, yield many ties and increase the likelihood that different sentences have the same score. Redundancy within the set of input sentences exacerbate this problem, since highly similar sentences are likely to contain the same concepts. 2.3 Summarization parameters For comparison purposes, we use the same system pipeline as in (Gillick et al., 2009), which is described below. Step 1: clean input documents; a set of rules is used to remove bylines and format markup. Step 2: split the text into sentences; we use splitta2 (Gillick, 2009) and re-attach multisentence quotations. Step 3: compute parameters needed by the model; we extract and weight the concepts. Step 4: prune sentences shorter than 10 words, duplicate sentences and those that begin and end with a quotation mark. Step 5: map to ILP format and solve; we use an off-the-shelf ILP solver3. Step 6: order selected sentences for inclusion in the summary, first by source and then by position. Similar to previous work, we use bigrams of words as concepts. Although bigrams are rough approximations of concepts, they are simple to extract and match, and have been shown to pe</context>
</contexts>
<marker>Gillick, 2009</marker>
<rawString>Dan Gillick. 2009. Sentence boundary detection and the problem with the u.s. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 241–244, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samir Khuller</author>
<author>Anna Moss</author>
<author>Joseph Naor</author>
</authors>
<title>The budgeted maximum coverage problem.</title>
<date>1999</date>
<journal>Information Processing Letters,</journal>
<volume>70</volume>
<issue>1</issue>
<contexts>
<context position="2079" citStr="Khuller et al., 1999" startWordPosition="315" endWordPosition="318">model, a summary is generated by assembling the subset of sentences that maximizes a function of the unique concepts it covers. Selecting the optimal subset of sentences is then cast as an instance of the budgeted maximum coverage problem1. As this problem is NP-hard, pruning low-weight concepts is required for the ILP solver to find optimal solutions efficiently (Gillick and Favre, 2009; 1Given a collection S of sets with associated costs and a budget L, find a subset S&apos; ⊆ S such that the total cost of sets in S&apos; does not exceed L, and the total weight of elements covered by S&apos; is maximized (Khuller et al., 1999). Qian and Liu, 2013; Li et al., 2013). However, reducing the number of concepts in the model has two undesirable consequences. First, it forces the model to only use a limited number of concepts to rank summaries, resulting in lower ROUGE scores. Second, by reducing the number of items from which sentence scores are derived, it allows different sentences to have the same score, and ultimately leads to multiple optimal summaries. To our knowledge, no previous work has mentioned these problems, and only results corresponding to the first optimal solution found by the solver are reported. Howeve</context>
<context position="5378" citStr="Khuller et al., 1999" startWordPosition="874" endWordPosition="877">efinition for concepts and a method to estimate their weights are the two key factors that affect the performance of this model. Bigrams of words are usually used as a proxy for concepts (Gillick and Favre, 2009; BergKirkpatrick et al., 2011). Concept weights are either estimated by heuristic counting, e.g. document frequency in (Gillick and Favre, 2009), or obtained by supervised learning (Li et al., 2013). 2.2 Pruning to reduce complexity The concept-level formulation of (Gillick and Favre, 2009) is an instance of the budgeted maximum coverage problem, and solving such a problem is NP-hard (Khuller et al., 1999). Keeping the number of variables and constraints small is then critical to reduce the model complexity. In previous work, efficient summarization was achieved by pruning concepts. One way to reduce the number of concepts in the model is to remove those concepts that have a weight below a given threshold (Gillick and Favre, 2009). Another way is to consider only the top-n highest weighted concepts (Li et al., 2013). Once lowweight concepts are pruned, sentences that do not contain any remaining concepts are removed, further reducing the number of variables and constraints in the model. As such</context>
<context position="14634" citStr="Khuller et al., 1999" startWordPosition="2413" endWordPosition="2416">further constraints, similar to equations 3 and 4, to ensure the consistency of the solution. This extended model succeeds in giving a single solution that is at least comparable to the mean score of the multiple optimal solutions. However, it requires about twice as much time to solve which makes it impractical for large documents. 3.4 Fast approximation Instead of pruning concepts to reduce complexity, one may consider using an approximation if results are found satisfactory. Here, similarly to (Takamura and Okumura, 2009; Lin and Bilmes, 2010) we implement the greedy heuristic proposed in (Khuller et al., 1999) that solve the budgeted maximum coverage problem with a performance guarantee 1/2 · (1 − 1/e). Table 4 compares the performance of the model that achieves the best trade off between effectiveness and runtime, that is when DF &gt; 3, with that of the greedy approximation without pruning. Overall, the approximate solution is over 96% as good as the average optimal solution. Although the ILP solution marks an upper bound on performance, its solving time is exponential in the number of input sentences. The approximate method is then relevant as it marks an upper bound on speed (less than 0.01 second</context>
</contexts>
<marker>Khuller, Moss, Naor, 1999</marker>
<rawString>Samir Khuller, Anna Moss, and Joseph (Seffi) Naor. 1999. The budgeted maximum coverage problem. Information Processing Letters, 70(1):39 – 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Using supervised bigram-based ilp for extractive summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1004--1013</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2117" citStr="Li et al., 2013" startWordPosition="323" endWordPosition="326"> the subset of sentences that maximizes a function of the unique concepts it covers. Selecting the optimal subset of sentences is then cast as an instance of the budgeted maximum coverage problem1. As this problem is NP-hard, pruning low-weight concepts is required for the ILP solver to find optimal solutions efficiently (Gillick and Favre, 2009; 1Given a collection S of sets with associated costs and a budget L, find a subset S&apos; ⊆ S such that the total cost of sets in S&apos; does not exceed L, and the total weight of elements covered by S&apos; is maximized (Khuller et al., 1999). Qian and Liu, 2013; Li et al., 2013). However, reducing the number of concepts in the model has two undesirable consequences. First, it forces the model to only use a limited number of concepts to rank summaries, resulting in lower ROUGE scores. Second, by reducing the number of items from which sentence scores are derived, it allows different sentences to have the same score, and ultimately leads to multiple optimal summaries. To our knowledge, no previous work has mentioned these problems, and only results corresponding to the first optimal solution found by the solver are reported. However, as we will show through experiments</context>
<context position="5167" citStr="Li et al., 2013" startWordPosition="839" endWordPosition="842"> the solution: selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept is only possible if it is present in at least one selected sentence. Choosing a suitable definition for concepts and a method to estimate their weights are the two key factors that affect the performance of this model. Bigrams of words are usually used as a proxy for concepts (Gillick and Favre, 2009; BergKirkpatrick et al., 2011). Concept weights are either estimated by heuristic counting, e.g. document frequency in (Gillick and Favre, 2009), or obtained by supervised learning (Li et al., 2013). 2.2 Pruning to reduce complexity The concept-level formulation of (Gillick and Favre, 2009) is an instance of the budgeted maximum coverage problem, and solving such a problem is NP-hard (Khuller et al., 1999). Keeping the number of variables and constraints small is then critical to reduce the model complexity. In previous work, efficient summarization was achieved by pruning concepts. One way to reduce the number of concepts in the model is to remove those concepts that have a weight below a given threshold (Gillick and Favre, 2009). Another way is to consider only the top-n highest weight</context>
<context position="10579" citStr="Li et al., 2013" startWordPosition="1736" endWordPosition="1739">long with the minimum document frequency, reaching 4.8 for TAC’08 at DF = 4. Prun6We use ROUGE-1.5.5 with the parameters: n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0 ing concepts also greatly reduces the number of variables in the ILP formulation, and consequently improves the run-time for solving the problem. Interestingly, we note that, even without any pruning, the model produces multiple optimal solutions. The choice of document frequency for weighting concepts is responsible for this as it generates many ties. Finer-grained concept weighting functions such as frequency estimation (Li et al., 2013) should therefore be preferred to limit the number of multiple optimal solutions. The mean ROUGE recall scores of the multiple optimal solutions for different minimal document frequencies are presented in Table 2. Here, the higher the concept pruning threshold, the higher the variability of the generated summaries as indicated by the standard deviation. Best ROUGE scores are achieved without concept pruning while the best compromise between effectiveness and run-time is given when DF ≥ 3, confirming the findings of (Gillick and Favre, 2009). To show in a realistic scenario how multiple optimal</context>
</contexts>
<marker>Li, Qian, Liu, 2013</marker>
<rawString>Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1004–1013, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>912--920</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="14565" citStr="Lin and Bilmes, 2010" startWordPosition="2402" endWordPosition="2405">by concept weighting, and thus set p to a small value (10−6). We add further constraints, similar to equations 3 and 4, to ensure the consistency of the solution. This extended model succeeds in giving a single solution that is at least comparable to the mean score of the multiple optimal solutions. However, it requires about twice as much time to solve which makes it impractical for large documents. 3.4 Fast approximation Instead of pruning concepts to reduce complexity, one may consider using an approximation if results are found satisfactory. Here, similarly to (Takamura and Okumura, 2009; Lin and Bilmes, 2010) we implement the greedy heuristic proposed in (Khuller et al., 1999) that solve the budgeted maximum coverage problem with a performance guarantee 1/2 · (1 − 1/e). Table 4 compares the performance of the model that achieves the best trade off between effectiveness and runtime, that is when DF &gt; 3, with that of the greedy approximation without pruning. Overall, the approximate solution is over 96% as good as the average optimal solution. Although the ILP solution marks an upper bound on performance, its solving time is exponential in the number of input sentences. The approximate method is the</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 912–920, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="9624" citStr="Lin, 2004" startWordPosition="1580" endWordPosition="1581">en the value of the objective function returned by the solver changes. 3 Experiments 3.1 Datasets and evaluation measures Experiments are conducted on the DUC’04 and TAC’08 datasets. For DUC’04, we use the 50 topics from the generic multi-document summarization task (Task 2). For TAC’08, we focus only on the 48 topics from the non-update summarization task. Each topic contains 10 newswire articles for which the task is to generate a summary no longer than 100 words (whitespace-delimited tokens). Summaries are evaluated against reference summaries using the ROUGE automatic evaluation measures (Lin, 2004). We set the ROUGE parameters to those6 that lead to highest agreement with manual evaluation (Owczarzak et al., 2012), that is, with stemming and stopwords not removed. 3.2 Results Table 1 presents the average number of optimal solutions at different levels of concept pruning. Overall, the average number of optimal solutions increases along with the minimum document frequency, reaching 4.8 for TAC’08 at DF = 4. Prun6We use ROUGE-1.5.5 with the parameters: n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0 ing concepts also greatly reduces the number of variables in the ILP formulation, and co</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th European Conference on IR Research, ECIR’07,</booktitle>
<pages>557--564</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1334" citStr="McDonald, 2007" startWordPosition="186" endWordPosition="187"> optimal solutions. We address these issues by extending the model to provide a single optimal solution, and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference. 1 Introduction Recent years have witnessed increased interest in global inference methods for extractive summarization. These methods formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of sentences that maximizes an objective function under a length constraint, and use Integer Linear Programming (ILP) to solve it exactly (McDonald, 2007). In this work, we focus on the concept-based ILP model for summarization introduced by (Gillick and Favre, 2009). In their model, a summary is generated by assembling the subset of sentences that maximizes a function of the unique concepts it covers. Selecting the optimal subset of sentences is then cast as an instance of the budgeted maximum coverage problem1. As this problem is NP-hard, pruning low-weight concepts is required for the ILP solver to find optimal solutions efficiently (Gillick and Favre, 2009; 1Given a collection S of sets with associated costs and a budget L, find a subset S&apos;</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th European Conference on IR Research, ECIR’07, pages 557–564, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The impact of frequency on summarization. Microsoft Research,</title>
<date>2005</date>
<pages>2005--101</pages>
<location>Redmond, Washington, Tech. Rep.</location>
<contexts>
<context position="13657" citStr="Nenkova and Vanderwende, 2005" startWordPosition="2239" endWordPosition="2243">or finding feasible solutions more quickly. This example demonstrates that multiple optimal solutions should be considered during evaluation. 3.3 Solving the multiple solution problem Multiple optimal solutions occur when concepts alone are not sufficient to distinguish between two competing summary candidates. Extending the model so that it provides a single solution can therefore not be done without introducing a second term in the objective function. Following the observation that the frequency of a non-stop word in a document set is a good predictor of a word appearing in a human summary (Nenkova and Vanderwende, 2005), we extend equation 1 as follows: �max �wici + p fktk (5) i k where fk is the frequency of non-stop word k in the document set, and tk is a binary variable indicating the presence of k in the summary. Here, we want to induce a single solution among the multiple optimal solutions given by concept weighting, and thus set p to a small value (10−6). We add further constraints, similar to equations 3 and 4, to ensure the consistency of the solution. This extended model succeeds in giving a single solution that is at least comparable to the mean score of the multiple optimal solutions. However, it </context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR2005-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>John M Conroy</author>
<author>Hoa Trang Dang</author>
<author>Ani Nenkova</author>
</authors>
<title>An assessment of the accuracy of automatic evaluation in summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of Workshop on Evaluation Metrics and System Comparison forAutomatic Summarization,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="9742" citStr="Owczarzak et al., 2012" startWordPosition="1598" endWordPosition="1601">luation measures Experiments are conducted on the DUC’04 and TAC’08 datasets. For DUC’04, we use the 50 topics from the generic multi-document summarization task (Task 2). For TAC’08, we focus only on the 48 topics from the non-update summarization task. Each topic contains 10 newswire articles for which the task is to generate a summary no longer than 100 words (whitespace-delimited tokens). Summaries are evaluated against reference summaries using the ROUGE automatic evaluation measures (Lin, 2004). We set the ROUGE parameters to those6 that lead to highest agreement with manual evaluation (Owczarzak et al., 2012), that is, with stemming and stopwords not removed. 3.2 Results Table 1 presents the average number of optimal solutions at different levels of concept pruning. Overall, the average number of optimal solutions increases along with the minimum document frequency, reaching 4.8 for TAC’08 at DF = 4. Prun6We use ROUGE-1.5.5 with the parameters: n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0 ing concepts also greatly reduces the number of variables in the ILP formulation, and consequently improves the run-time for solving the problem. Interestingly, we note that, even without any pruning, the m</context>
<context position="12062" citStr="Owczarzak et al., 2012" startWordPosition="1977" endWordPosition="1980">nimum document frequency to 3, which is often used in previous work (Gillick and Favre, 2009; Li et al., 2013), and use a two-sided Wilcoxon signed-rank to compute the number of systems that obtain significantly lower and higher ROUGE-1 recall scores8. Despite being comparable (p-value &gt; 0.4), the solutions found by the three solvers support different conclusions. The solution found using GLPK 771 systems participated at TAC’08 but we removed ICSI1 and ICSI2 systems which are based on the conceptbased ILP model. 8ROUGE-1 recall is most accurate metric to identify the better summary in a pair (Owczarzak et al., 2012). 1916 DUC’04 TAC’08 DF ROUGE-1 ROUGE-2 ROUGE-4 ROUGE-1 ROUGE-2 ROUGE-4 1 37.74 ±0.07 9.48 ±0.05 1.45 ±0.02 37.65 ±0.10 10.63 ±0.08 2.23 ±0.04 2 37.25 ±0.08 9.14 ±0.02 1.37 ±0.01 37.16 ±0.11 9.96 ±0.07 2.05 ±0.03 3 37.37 ±0.11 9.16 ±0.06 1.41 ±0.02 37.39 ±0.15 10.62 ±0.07 2.13 ±0.03 4 37.96 ±0.10 9.38 ±0.05 1.57 ±0.02 36.73 ±0.12 10.10 ±0.08 1.78 ±0.07 Table 2: Mean ROUGE recall and standard deviation for different minimum document frequencies. Solver ROUGE-1 1 / T GLPK 37.33 54 / 0 Gurobi 37.20 52 / 1 CPLEX 37.17 51 / 1 Table 3: ROUGE-1 recall scores for the first optimal solution found by di</context>
</contexts>
<marker>Owczarzak, Conroy, Dang, Nenkova, 2012</marker>
<rawString>Karolina Owczarzak, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2012. An assessment of the accuracy of automatic evaluation in summarization. In Proceedings of Workshop on Evaluation Metrics and System Comparison forAutomatic Summarization, pages 1–9, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Fast joint compression and summarization via graph cuts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1492--1502</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="2099" citStr="Qian and Liu, 2013" startWordPosition="319" endWordPosition="322">erated by assembling the subset of sentences that maximizes a function of the unique concepts it covers. Selecting the optimal subset of sentences is then cast as an instance of the budgeted maximum coverage problem1. As this problem is NP-hard, pruning low-weight concepts is required for the ILP solver to find optimal solutions efficiently (Gillick and Favre, 2009; 1Given a collection S of sets with associated costs and a budget L, find a subset S&apos; ⊆ S such that the total cost of sets in S&apos; does not exceed L, and the total weight of elements covered by S&apos; is maximized (Khuller et al., 1999). Qian and Liu, 2013; Li et al., 2013). However, reducing the number of concepts in the model has two undesirable consequences. First, it forces the model to only use a limited number of concepts to rank summaries, resulting in lower ROUGE scores. Second, by reducing the number of items from which sentence scores are derived, it allows different sentences to have the same score, and ultimately leads to multiple optimal summaries. To our knowledge, no previous work has mentioned these problems, and only results corresponding to the first optimal solution found by the solver are reported. However, as we will show t</context>
<context position="8633" citStr="Qian and Liu, 2013" startWordPosition="1420" endWordPosition="1423">orter stemmer in nltk. 1915 DUC’04 TAC’08 DF 1 2 3 4 1 2 3 4 # solutions 1.3 1.3 1.5 1.5 1.2 1.3 1.8 4.8 # concepts 2 955 676 247 107 2 909 393 127 56 # sentences 184 175 159 139 174 167 149 129 Avg. time (sec) 22.3 1.7 0.5 0.3 21.5 0.8 0.3 0.2 Table 1: Average number of optimal solutions, concepts and sentences for different minimum document frequencies. The average time in seconds for finding the first optimal solution is also reported. where the concept was seen. Document frequency is a simple, yet effective approach to concept weighting (Gillick and Favre, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). Reducing the number of concepts in the ILP model is then performed by pruning those concepts that occur in fewer than a given number of documents. ILP solvers usually provide only one solution. To generate alternate optimal solutions, we iteratively add new constraints to the problem that eliminate already found optimal solutions and rerun the solver. We stop the iterations when the value of the objective function returned by the solver changes. 3 Experiments 3.1 Datasets and evaluation measures Experiments are conducted on the DUC’04 and TAC’08 datasets. For DUC’04, we use the 50 topics fro</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Fast joint compression and summarization via graph cuts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492–1502, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>781--789</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="14542" citStr="Takamura and Okumura, 2009" startWordPosition="2398" endWordPosition="2401">ple optimal solutions given by concept weighting, and thus set p to a small value (10−6). We add further constraints, similar to equations 3 and 4, to ensure the consistency of the solution. This extended model succeeds in giving a single solution that is at least comparable to the mean score of the multiple optimal solutions. However, it requires about twice as much time to solve which makes it impractical for large documents. 3.4 Fast approximation Instead of pruning concepts to reduce complexity, one may consider using an approximation if results are found satisfactory. Here, similarly to (Takamura and Okumura, 2009; Lin and Bilmes, 2010) we implement the greedy heuristic proposed in (Khuller et al., 1999) that solve the budgeted maximum coverage problem with a performance guarantee 1/2 · (1 − 1/e). Table 4 compares the performance of the model that achieves the best trade off between effectiveness and runtime, that is when DF &gt; 3, with that of the greedy approximation without pruning. Overall, the approximate solution is over 96% as good as the average optimal solution. Although the ILP solution marks an upper bound on performance, its solving time is exponential in the number of input sentences. The ap</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Hiroya Takamura and Manabu Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 781–789, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>233--243</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="8612" citStr="Woodsend and Lapata, 2012" startWordPosition="1416" endWordPosition="1419">www.nltk.org/ 5We use the Porter stemmer in nltk. 1915 DUC’04 TAC’08 DF 1 2 3 4 1 2 3 4 # solutions 1.3 1.3 1.5 1.5 1.2 1.3 1.8 4.8 # concepts 2 955 676 247 107 2 909 393 127 56 # sentences 184 175 159 139 174 167 149 129 Avg. time (sec) 22.3 1.7 0.5 0.3 21.5 0.8 0.3 0.2 Table 1: Average number of optimal solutions, concepts and sentences for different minimum document frequencies. The average time in seconds for finding the first optimal solution is also reported. where the concept was seen. Document frequency is a simple, yet effective approach to concept weighting (Gillick and Favre, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). Reducing the number of concepts in the ILP model is then performed by pruning those concepts that occur in fewer than a given number of documents. ILP solvers usually provide only one solution. To generate alternate optimal solutions, we iteratively add new constraints to the problem that eliminate already found optimal solutions and rerun the solver. We stop the iterations when the value of the objective function returned by the solver changes. 3 Experiments 3.1 Datasets and evaluation measures Experiments are conducted on the DUC’04 and TAC’08 datasets. For DUC’04, we </context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 233–243, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
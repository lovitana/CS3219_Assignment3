<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001871">
<title confidence="0.976009">
Event Detection and Factuality Assessment with Non-Expert Supervision
</title>
<author confidence="0.941553">
Kenton Leet, Yoav Artzit* Yejin Choit, and Luke Zettlemoyert
</author>
<affiliation confidence="0.747599">
t Computer Science &amp; Engineering, University of Washington, Seattle, WA 98195
</affiliation>
<email confidence="0.810029">
{kentonl, yejin, lsz}@cs.washington.edu
</email>
<affiliation confidence="0.817231">
t Dept. of Computer Science and Cornell Tech, Cornell University, New York, NY 10011
</affiliation>
<email confidence="0.997539">
yoav@cs.cornell.edu
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912647058824">
Events are communicated in natural lan-
guage with varying degrees of certainty.
For example, if you are “hoping for a
raise,” it may be somewhat less likely than
if you are “expecting” one. To study these
distinctions, we present scalable, high-
quality annotation schemes for event de-
tection and fine-grained factuality assess-
ment. We find that non-experts, with
very little training, can reliably provide
judgments about what events are men-
tioned and the extent to which the author
thinks they actually happened. We also
show how such data enables the develop-
ment of regression models for fine-grained
scalar factuality predictions that outper-
form strong baselines.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97940785">
Interpretation of events—determining what the
author claims did or did not happen—is impor-
tant for many NLP applications, such as news arti-
cle summarization or biomedical information ex-
traction. However, detecting events and assessing
their factuality is challenging. For example, while
most non-copular verbs are events, words in gen-
eral vary with use (e.g. “trade route” vs “trade
with Iraq”). Events also have widely varying,
context-dependent factuality cues, such as event
interactions (e.g. “prevent easy access) and cue
words (e.g. “ordered to” vs. “expected to”). As
shown in Figure 1, these are common challenges
that a model of event factuality must address.
In this paper, we present new data and mod-
els for these tasks, demonstrating that non-experts
can provide high-quality annotations which en-
able fine-grained, scalar judgments of factuality.
Unlike previous work, we do not use a detailed
∗Work done at the University of Washington.
</bodyText>
<listItem confidence="0.984804266666667">
(1) U.S. embassies and military installations
around the world were ordered(3.0) to set(2.6) up
barriers and tighten(2.6) security to prevent(1.8)
easy access(-2.4) by unauthorized people.
(2) Intel’s most powerful computer chip has
flaws that could delay(0.8) several computer
makers’ marketing efforts(2.6), but the “bugs”
aren’t expected(-2.6) to hurt(-2.0) Intel.
(3) President Bush on Tuesday said(3.0)
the United States may extend(1.6) its naval
quarantine(2.6) to Jordan’s Red Sea port of
Aqaba to shut(1.4) off Iraq’s last unhindered
trade route.
(4) He also said(3.0) of trade(-0.8) with Iraq:
“There are no shipments at the moment.”
</listItem>
<figureCaption confidence="0.822425333333333">
Figure 1: Example annotations with italicized
event mentions and crowdsourced scalar factuality
values u E [−3.0, 3.0]. Positive (or negative) val-
</figureCaption>
<bodyText confidence="0.981180619047619">
ues indicate the extent to which the author claims
the events happened (or not).
specification of exactly what events and factual-
ity classes should be. Instead, we simply ask
non-experts to find words describing things that
the author claims could have happened, and rate
each possibility on a scale of -3 (certainly did
not happen) to 3 (certainly did). Figure 1 shows
that non-expert workers—when their judgments
are aggregated—consistently find a wide range of
events and recognize the subtle differences in im-
plied factuality. For example, the event set gets a
score of 2.6, indicating that it likely but not cer-
tainly occurred, since it was ordered, whereas the
ordered event, gets a score of 3.0.
We gather data for event detection and factual-
ity, reusing sentences from the TempEval-3 cor-
pus (Uzzaman et al., 2013). Our approach pro-
duces high-quality labels with modest costs. We
also introduce simple but highly effective models
for both tasks that outperform strong baselines. In
</bodyText>
<page confidence="0.838029">
1643
</page>
<note confidence="0.647853">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643–1648,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999791">
particular, our factuality regression model uses a
learning objective that combines the advantages of
LASSO and support vector regression, enabling it
to effectively consider sparse lexical cues. By pro-
viding scalar factuality judgments for events, our
models enable more fine-grained reasoning than
previously considered. The corpus and learned
models are available online.1
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998766">
While event definitions have been proposed in
several prior studies, existing approaches vary in
how they model various linguistic forms such as
nominal events, stative events, generic events, and
light verbs (Pustejovsky et al., 2003; Palmer et
al., 2005; Meyers et al., 2004; Kim et al., 2009;
Song et al., 2015). Even with a formal and precise
account of events, training annotators to learn all
such linguistic intricacies remains a practical chal-
lenge. Instead of definition-driven instructions,
we propose example-driven instructions and show
their effectiveness.
Previous studies have modeled event factuality
assessment as a binary (Diab et al., 2009; Prab-
hakaran et al., 2010) or multi-class (Sauri and
Pustejovsky, 2009) classification task, and they re-
lied on expert annotators. A softer representation
was proposed and crowdsourced by de Marneffe
et al. (2012), who advocated for representing fac-
tuality from the reader’s perspective as a distribu-
tion of categories, but their annotation process re-
quires manual normalization of the text. In con-
trast, we model factuality from the author’s per-
spective with scalar values, and we have an end-
to-end crowdsourced annotation pipeline.
More recently, Soni et al. (2014) investigated a
related problem for quoted statements on Twitter,
and they also crowdsourced factuality annotations
to learn regression models. While their approach
is similar, we focus on predicting factuality for
events that occur in every sentence. Without the
restrictions of their task, we must reason about a
larger variety of contextual cues.
Our method of evaluating annotator agreement
(Section 3) is related to the crowdsourcing study
by Snow et al. (2008), who showed that pooled
non-experts can match or outperform single ex-
pert annotators. In contrast, we approximate ex-
pert judgments by independently sampling and ag-
gregating sets of non-expert judgments.
</bodyText>
<footnote confidence="0.979604">
1http://lil.cs.washington.edu/fact
</footnote>
<table confidence="0.97015825">
Data Documents Sentences Tokens
Train 192 2909 73220
Dev. 64 1060 26146
Test 20 274 7004
</table>
<figureCaption confidence="0.924586">
Figure 2: Corpus statistics.
</figureCaption>
<sectionHeader confidence="0.966568" genericHeader="method">
3 Data Annotation
</sectionHeader>
<bodyText confidence="0.999458324324325">
We use a two-stage annotation pipeline to create
the labels shown in Figure 1. Event mentions are
first detected, followed by factuality judgments.
As motivated in Section 1, we use instructions that
are easily understandable by workers with no lin-
guistic training and improve overall quality by ag-
gregating multiple judgments to get the final label.
Event Annotation Given a sentence, we high-
light one token at a time and ask workers if it refers
to an event. We use the following instructions:
We consider events to be things that may or
may not occur either in the past, present or fu-
ture (e.g., earthquake, meeting, jumping, talk-
ing, etc.). In some cases, it is not so clear
whether a word is referring to an event or not.
Consider these harder cases to be events.
along with 25 example annotations that covered a
large variety of cases such as nominals, statives,
generic events, light verbs, and non-events. These
examples include both toy sentences and sentences
from the corpus to annotate. For efficiency, we
did not annotate a short list of stop words, copular
verbs, and auxiliaries.
Factuality Annotation For factuality, we
present a sentence with one highlighted event
token at a time with the following prompt:
On a scale from 3 to -3, rate how likely the high-
lighted event did or will happen according to
the author of the sentence.
along with 17 examples to calibrate the annota-
tor’s judgments, including negated, conditional,
hedged, generic, and nested events. The responses
-3, 0, and 3 were given explicit interpretations. 3
and -3 denote respectively that the target event cer-
tainly did or did not happen according to the au-
thor. 0 denotes that the author is neutral and ex-
presses no bias towards the event’s factuality.
</bodyText>
<page confidence="0.975766">
1644
</page>
<figure confidence="0.97121">
Event Agreement Factuality Agreement
k
</figure>
<figureCaption confidence="0.790468">
Figure 3: Agreement statistics as a function of k,
the number of judgments aggregated. We choose
k = 5 in both tasks for our experiments, as de-
noted by the red square.
</figureCaption>
<bodyText confidence="0.981437657142857">
Data collection We gathered data on Crowd-
Flower.2 For quality control, annotators are ran-
domly presented test questions with known an-
swers. For each example, we collect and aggregate
5 judgments, as described below. For comparison,
we annotated TempEval-3 (Uzzaman et al., 2013),
keeping the existing test split and randomly hold-
ing out a quarter of the training examples to create
a development set. Figure 2 shows data statistics.
The annotation cost is 0.5¢ per judgment for de-
tection and 2¢ per judgment for factuality.
Aggregated Agreement We introduce a simple
scheme to measure agreement with aggregate data,
for example when the majority class from a pool
of judgments is used for the final label. Instead
of comparing individuals, we want to know how
often the aggregates will agree, if we were to have
different groups of annotators doing the task.
Formally, we assume N samples {(xi, yi)  |i =
1, ... , N}, where each xi is a token within a sen-
tence, and yi = {yji  |j = 1, ... , M} is the set of
M judgments for xi. Let Y be the set of possible
labels, Y = {−1,1} for detection and Y = [−3, 3]
for factuality. Let AGG : Yk —* Y be an ag-
gregation function, which maps k judgments to a
single aggregate one. For event detection, we set
AGG(y1, ... , yk) to return the majority value from
the set of judgments {y1, ... , yk}. For factual-
ity, we set AGG(y1, ... , yk) = 1 �k j=1 yj, which
k
computes the mean value.
To estimate the agreement between aggregates
of k judgments, we collect pairs of disjoint sub-
sets of size k from the M judgments. Given yi,
we define the set of aggregate judgment pairs:
</bodyText>
<footnote confidence="0.986518">
2http://crowdflower.com
</footnote>
<table confidence="0.997861111111111">
FactBank Labels
CT- PR- PS- PS+ PR+ CT+ CTu NA Uu
Discretized ratings -3 39 0 0 0 0 0 0 0 29
-2 29 2 0 0 0 0 0 0 44
-1 16 4 1 0 0 3 0 0 58
0 15 0 5 2 0 7 0 1 95
1 7 0 1 30 4 27 2 0 337
2 4 1 0 20 42 260 0 0 564
3 2 0 0 1 10 2760 0 0 771
</table>
<figureCaption confidence="0.9611025">
Figure 4: Confusion matrix between FactBank la-
bels and our discretized factuality ratings.
</figureCaption>
<bodyText confidence="0.999219621621622">
{(AGG(y&apos;), AGG(y&apos;&apos;))  |y&apos;, y&apos;&apos; C yi ∧ |y&apos; |=
|y&apos;&apos; |= k ∧ y&apos; n y&apos;&apos; = 0}.3 To measure how
well these aggregates agree, we treat AGG(y&apos;) as a
candidate hypothesis and AGG(y&apos;&apos;) as the gold la-
bel and compute the appropriate evaluation metric
to measure aggregate agreement. We use the F1
score for detection and Pearson’s correlation for
factuality, as described in Section 5.
We experiment with k = 1, ... , 9 for 100 sen-
tences, allowing aggregates of up to 9 judgments,
as seen in Figure 3. Aggregate agreement for both
tasks improve with larger k, but returns quickly
diminish. Therefore, we chose k = 5 for the
full data collection to reasonably trade off between
quality and quantity. In absolute terms, the agree-
ment at this level is strong (92.6% F1 for detection
and 83.1% correlation for factuality), demonstrat-
ing that aggregate non-expert judgments can pro-
duce high-quality annotations.
Comparison to FactBank We compare our fac-
tuality ratings, rounded to the nearest integer,
to FactBank annotations (author source only) for
overlapping events. The confusion matrix from
Figure 4 shows there is strong correlation between
our ratings and FactBank labels with specified cer-
tainties and polarities. These labels are CT-, PR-,
PS-, PS+, PR+, and CT+, corresponding to events
that are seen as (certainly/probably/possibly) (not
happening/happening).
We differ most significantly in events labeled
Uu (underspecified) by FactBank, which consist
largely of nested events, such as “Sandors said
he’d double his money” or “Sandors hoped he’d
double his money.” While FactBank annotators
would label both double events as Uu, our anno-
tations can indicate nuances based on the author’s
wording (i.e., said vs. hoped). The large variation
</bodyText>
<footnote confidence="0.930199">
3In practice, we sample judgment pairs rather then com-
puting all possible combinations.
</footnote>
<figure confidence="0.995984111111111">
F1 (%)
100
90
80
60
50
40
70
1 2 3 4 5 6 7 8 9
Pearson’s r (%)
100
90
80
60
50
40
70
1 2 3 4 5 6 7 8 9
</figure>
<page confidence="0.964218">
1645
</page>
<bodyText confidence="0.9997105">
in the Uu column of the confusion matrix suggests
that the factuality of an event is rarely perceived as
completely neutral, even when the author does not
commit to a belief in the event’s occurrence.
</bodyText>
<sectionHeader confidence="0.995922" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.982740888888889">
Learning For the detection task, we learn a lin-
ear SVM classification model. For the factuality
task, we assume a dataset with N examples of la-
beled events {(xi, yi)  |i = 1, ... , N}, and we
learn a regression model: yi = wTO(xi). We in-
troduce a learning objective for regression:
max(0, |yi — wTO(xi) |— c)
that combines the advantages of LASSO (Tibshi-
rani, 1996) and support vector regression (Drucker
et al., 1997). It induces sparse feature weights
while being insensitive to errors less than c.
Features For the detection model, we include
features given the input word x: (1) lemma of x,
(2) part of speech of x, (3) indicator for whether x
is a hyponym of the event synset in WordNet and
the part of speech of x, (4) Brown clusters of x and
its part of speech, and (5) all dependency paths
from x up to length 1. For the factuality model,
given the input event mention x, we include: (1)
lemma of x, (2) part of speech of x, and (3) all
dependency paths from x up to length 2.
For dependency paths, we include all edge
labels, the target word is omitted, and each node
may or may not be lexicalized; we include all
possible configurations. For example in “John
did not expect to return”, the dependency path:
not+—[neg]—expect—[xcomp]--+return, would
produce the following features:
not+—[neg]—expect—[xcomp]--+(*)
(*)+—[neg]—expect—[xcomp]--+(*)
not+—[neg]—(*)—[xcomp]--+(*)
(*)+—[neg]—(*)—[xcomp]--+(*)
These dependency features allow for context-
dependent reasoning, including many of the cases
in Figure 1 where the factuality of an event de-
pends on the identity of a neighboring verb.
</bodyText>
<sectionHeader confidence="0.998855" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999872390243903">
Baselines For detection, we include a baseline
reimplementation of the NAVYTIME (Chambers,
2013) classification detector, one of the top per-
formers in the TempEval-3 event detection task.
For factuality, we include three baselines: (1) A
one-vs.-rest multi-class classifier (DISCRETE) us-
ing our features (Section 4) and labels that are dis-
cretized by rounding to the nearest integer, (2) a
regression model (SVR) trained with the standard
SVR objective using our features, and (3) a re-
gression model (PRABHAKARAN) trained with the
standard SVR objective using features from Prab-
hakaran et al. (2010). These features are highly
informative, but their lexical features are restricted
to a small set of manually defined words.
Implementation Details The SVM models
(NAVYTIME, DISCRETE, SVR, PRABHAKARAN,
and our detection model) were trained with SVM-
Light (Joachims, 1999). We use CPLEX4 to solve
the linear program optimizing the regression ob-
jective in Section 4. All hyperparameters were
tuned on the development set.
We use the Stanford dependency parser (de
Marneffe et al., 2006) for extracting dependency
path and part-of-speech features. We use WordNet
(Miller, 1995) to generate lemma and hyponym
features. Brown clusters with 100, 320, 1000, and
3200 clusters from Turian et al. (2010) are used in
the detection features.
Evaluation Metrics We use the standard F1
score for the evaluation of detection. For event
factuality, we report two metrics, the mean abso-
lute error (MAE) relative to the gold standard la-
bels and Pearson’s correlation coefficient. While
MAE is an intuitive metric that evaluates the abso-
lute fit of the model, Pearson’s r better captures
how well a system is able to recover the varia-
tion of the annotations. Pearson’s r is also con-
veniently normalized such that r = 0 for a system
that blindly chooses the best a priori output and
r = 1 for a system that makes no error.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.99966425">
Detection Results Figure 5 shows development
and test results for detection event mentions.5 We
see a small drop in precision and large gains in
recall, but a significant increase in F1, primarily
</bodyText>
<footnote confidence="0.989314">
4http://tiny.cc/cplex
5We performed two-sided bootstrap resampling statistical
significance tests (Graham et al., 2014). In Figures 5 and 6,
asterisks indicate that the difference from the best system is
statistically significant (p &lt; 0.05).
</footnote>
<equation confidence="0.9922158">
N
i=1
IIwII1 + C
min
w
</equation>
<page confidence="0.975464">
1646
</page>
<table confidence="0.974507">
Model Dev. Test
P R F1 P R F1
Our system 90.1 90.9 90.5 85.5* 87.8 86.6
NAVYTIME 84.7* 79.6* 82.1* 87.7 78.3* 82.7*
Figure 5: Results for the detection task.
Model Dev. Test
MAE r MAE r
Our system 46.2 74.9 51.1 70.8
SVR 50.3* 74.8 57.1* 69.4
DISCRETE 50.3* 68.6* 52.4 62.2*
PRABHAKARAN 58.7* 51.1* 62.0* 50.8*
</table>
<figureCaption confidence="0.987473">
Figure 6: Results for the factuality task.
</figureCaption>
<bodyText confidence="0.999493194444444">
due to the use of distributional features and more
general dependency features.
Factuality Results Figure 6 shows development
and test results for predicting the factuality of
gold-labeled event mentions. Our system shows
an overall improvement in performance over all
baselines, demonstrating that the regression model
works well for this data. It is able to make more
graded judgments that correlate with the aggre-
gate opinions of untrained annotators. As shown
in Figure 8, which compares the mean average er-
ror for different buckets of factuality labels, we
observe the largest gains over PRABHAKARAN in
examples with low factuality, where lexical cues
are especially critical.
Error Analysis We manually studied 50 devel-
opment samples where our factuality model pro-
duced the largest absolute errors. Figure 7 summa-
rizes the error types. The biggest challenge is the
wide variety of sparse lexical cues. For example,
the sentences “Wong Kwan will be lucky to break
even” and “That sale could still fall through if fi-
nancing problems develop” require modeling the
influence of “lucky to” and “fall through.” Even
when these types of features do appear in the train-
ing data, they tend to be very rare.
We also find cases that require inference over
longer distances than our model permits. Con-
sider the sentence “Mesa had rejected a general
proposal from StatesWest to combine the two car-
riers.” To know that combine is not likely to hap-
pen, we must infer that it is conditioned on the
proposal, which was rejected. Finally, we find
that world knowledge and pragmatic inference is
sometimes required. For example, in the sen-
tence “There was no hint of trouble in the last
</bodyText>
<table confidence="0.998891833333333">
Error type %
Missed lexical cue (unseen in training) 52
Missed lexical cue (seen in training) 12
Long distance inference 16
World knowledge &amp; pragmatics 12
Annotation error 8
</table>
<figureCaption confidence="0.956007">
Figure 7: Error types for the 50 examples with the
largest absolute development error.
</figureCaption>
<figure confidence="0.705564">
Factuality Label
</figure>
<figureCaption confidence="0.943179">
Figure 8: Mean absolute error in the development
</figureCaption>
<bodyText confidence="0.962752875">
set for different labels rounded to the nearest inte-
ger. Our system’s improvement is greater when
predicting events with low factuality, which re-
quires modeling sparse lexical cues.
conversation between controllers and TWA pilot
Steven Snyder,” the pragmatic implication that
trouble likely happened requires common knowl-
edge about flights.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999975333333333">
We studied event detection and scalar factuality
prediction, demonstrating that non-expert annota-
tor can, in aggregate, provide high-quality data
and introducing simple models that perform well
on each task. There is significant room for fu-
ture work to improve the results, including jointly
modeling the factuality of multiple events and in-
tegrating factuality models into information ex-
traction and question answering systems.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999707875">
This research was supported in part by the NSF
(IIS-1252835, IIS-1408287), DARPA under the
DEFT program through the AFRL (FA8750-13-2-
0019), an Allen Distinguished Investigator Award,
and a gift from Google. The authors thank Mark
Yatskar, Luheng He, and Mike Lewis for help-
ful discussions, and the anonymous reviewers for
helpful comments.
</bodyText>
<figure confidence="0.9954355">
−3 −2 −1 0 1 2 3
MAE
5
4
3
2
0
1
Our system
PRABHAKARAN
</figure>
<page confidence="0.984356">
1647
</page>
<sectionHeader confidence="0.972266" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997962378947368">
Nathanael Chambers. 2013. Navytime: Event and
time ordering from raw text. In Second Joint Con-
ference on Lexical and Computational Semantics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation.
Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it hap-
pen? The pragmatic complexity of veridicality as-
sessment. Computational Linguistics, 38(2):301–
333.
Mona T Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei
Guo. 2009. Committed belief annotation and tag-
ging. In Proceedings of the Third Linguistic Annota-
tion Workshop, pages 68–73. Association for Com-
putational Linguistics.
Harris Drucker, Christopher JC Burges, Linda Kauf-
man, Alex J Smola, and Vladimir Vapnik. 1997.
Support vector regression machines. In Advances in
Neural Information Processing Systems, pages 155–
161.
Yvette Graham, Nitika Mathur, and Timothy Baldwin.
2014. Randomized significance tests in machine
translation. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation.
Thorsten Joachims. 1999. Svmlight: Support vec-
tor machine. SVM-Light Support Vector Machine
http://svmlight.joachims.org/, University of Dort-
mund, 19(4).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of bionlp’09 shared task on event extraction. In
Proceedings of the Workshop on Current Trends in
Biomedical Natural Language Processing: Shared
Task, pages 1–9. Association for Computational Lin-
guistics.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In HLT-NAACL 2004 workshop:
Frontiers in corpus annotation, pages 24–31.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational linguistics,
31(1):71–106.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014–1022, Beijing,
China, August. Coling 2010 Organizing Committee.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The timebank corpus. In Corpus
linguistics.
Roser Sauri and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227–268.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254–263. Association for Computational
Linguistics.
Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese,
Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick,
Neville Ryant, and Xiaoyi Ma. 2015. From
light to rich ere: Annotation of entities, relations,
and events. Proceedings of the 3rd Workshop on
EVENTS at the NAACL-HLT, pages 89–98.
Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob
Eisenstein. 2014. Modeling factuality judgments
in social media text. In Proceedings of the Asso-
ciation for Computational Linguistics (ACL), Balti-
more, MD.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267–288.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
N. Uzzaman, H. Llorens, L. Derczynski, M. Verhagen,
J. Allen, and J. Pustejovsky. 2013. Semeval-2013
task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Proceedings of the
International Workshop on Semantic Evaluation.
</reference>
<page confidence="0.993283">
1648
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.330501">
<title confidence="0.998084">Event Detection and Factuality Assessment with Non-Expert Supervision</title>
<author confidence="0.824312">Yoav</author>
<author confidence="0.824312">Luke</author>
<affiliation confidence="0.930769">tComputer Science &amp; Engineering, University of Washington, Seattle, WA</affiliation>
<email confidence="0.732293">yejin,</email>
<affiliation confidence="0.604649">tDept. of Computer Science and Cornell Tech, Cornell University, New York, NY</affiliation>
<email confidence="0.998119">yoav@cs.cornell.edu</email>
<abstract confidence="0.999048388888889">Events are communicated in natural language with varying degrees of certainty. For example, if you are “hoping for a raise,” it may be somewhat less likely than if you are “expecting” one. To study these distinctions, we present scalable, highquality annotation schemes for event detection and fine-grained factuality assessment. We find that non-experts, with very little training, can reliably provide judgments about what events are mentioned and the extent to which the author thinks they actually happened. We also show how such data enables the development of regression models for fine-grained scalar factuality predictions that outperform strong baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
</authors>
<title>Navytime: Event and time ordering from raw text.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="14199" citStr="Chambers, 2013" startWordPosition="2346" endWordPosition="2347">calized; we include all possible configurations. For example in “John did not expect to return”, the dependency path: not+—[neg]—expect—[xcomp]--+return, would produce the following features: not+—[neg]—expect—[xcomp]--+(*) (*)+—[neg]—expect—[xcomp]--+(*) not+—[neg]—(*)—[xcomp]--+(*) (*)+—[neg]—(*)—[xcomp]--+(*) These dependency features allow for contextdependent reasoning, including many of the cases in Figure 1 where the factuality of an event depends on the identity of a neighboring verb. 5 Experimental Setup Baselines For detection, we include a baseline reimplementation of the NAVYTIME (Chambers, 2013) classification detector, one of the top performers in the TempEval-3 event detection task. For factuality, we include three baselines: (1) A one-vs.-rest multi-class classifier (DISCRETE) using our features (Section 4) and labels that are discretized by rounding to the nearest integer, (2) a regression model (SVR) trained with the standard SVR objective using our features, and (3) a regression model (PRABHAKARAN) trained with the standard SVR objective using features from Prabhakaran et al. (2010). These features are highly informative, but their lexical features are restricted to a small set</context>
</contexts>
<marker>Chambers, 2013</marker>
<rawString>Nathanael Chambers. 2013. Navytime: Event and time ordering from raw text. In Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
<author>Christopher Potts</author>
</authors>
<title>Did it happen? The pragmatic complexity of veridicality assessment.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>2</issue>
<pages>333</pages>
<marker>de Marneffe, Manning, Potts, 2012</marker>
<rawString>Marie-Catherine de Marneffe, Christopher D. Manning, and Christopher Potts. 2012. Did it happen? The pragmatic complexity of veridicality assessment. Computational Linguistics, 38(2):301– 333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona T Diab</author>
<author>Lori Levin</author>
<author>Teruko Mitamura</author>
<author>Owen Rambow</author>
<author>Vinodkumar Prabhakaran</author>
<author>Weiwei Guo</author>
</authors>
<title>Committed belief annotation and tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Linguistic Annotation Workshop,</booktitle>
<pages>68--73</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5020" citStr="Diab et al., 2009" startWordPosition="750" endWordPosition="753">d in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem </context>
</contexts>
<marker>Diab, Levin, Mitamura, Rambow, Prabhakaran, Guo, 2009</marker>
<rawString>Mona T Diab, Lori Levin, Teruko Mitamura, Owen Rambow, Vinodkumar Prabhakaran, and Weiwei Guo. 2009. Committed belief annotation and tagging. In Proceedings of the Third Linguistic Annotation Workshop, pages 68–73. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harris Drucker</author>
<author>Christopher JC Burges</author>
<author>Linda Kaufman</author>
<author>Alex J Smola</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support vector regression machines.</title>
<date>1997</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>155--161</pages>
<contexts>
<context position="12903" citStr="Drucker et al., 1997" startWordPosition="2137" endWordPosition="2140">u column of the confusion matrix suggests that the factuality of an event is rarely perceived as completely neutral, even when the author does not commit to a belief in the event’s occurrence. 4 Approach Learning For the detection task, we learn a linear SVM classification model. For the factuality task, we assume a dataset with N examples of labeled events {(xi, yi) |i = 1, ... , N}, and we learn a regression model: yi = wTO(xi). We introduce a learning objective for regression: max(0, |yi — wTO(xi) |— c) that combines the advantages of LASSO (Tibshirani, 1996) and support vector regression (Drucker et al., 1997). It induces sparse feature weights while being insensitive to errors less than c. Features For the detection model, we include features given the input word x: (1) lemma of x, (2) part of speech of x, (3) indicator for whether x is a hyponym of the event synset in WordNet and the part of speech of x, (4) Brown clusters of x and its part of speech, and (5) all dependency paths from x up to length 1. For the factuality model, given the input event mention x, we include: (1) lemma of x, (2) part of speech of x, and (3) all dependency paths from x up to length 2. For dependency paths, we include </context>
</contexts>
<marker>Drucker, Burges, Kaufman, Smola, Vapnik, 1997</marker>
<rawString>Harris Drucker, Christopher JC Burges, Linda Kaufman, Alex J Smola, and Vladimir Vapnik. 1997. Support vector regression machines. In Advances in Neural Information Processing Systems, pages 155– 161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Nitika Mathur</author>
<author>Timothy Baldwin</author>
</authors>
<title>Randomized significance tests in machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="16331" citStr="Graham et al., 2014" startWordPosition="2686" endWordPosition="2689"> the absolute fit of the model, Pearson’s r better captures how well a system is able to recover the variation of the annotations. Pearson’s r is also conveniently normalized such that r = 0 for a system that blindly chooses the best a priori output and r = 1 for a system that makes no error. 6 Results Detection Results Figure 5 shows development and test results for detection event mentions.5 We see a small drop in precision and large gains in recall, but a significant increase in F1, primarily 4http://tiny.cc/cplex 5We performed two-sided bootstrap resampling statistical significance tests (Graham et al., 2014). In Figures 5 and 6, asterisks indicate that the difference from the best system is statistically significant (p &lt; 0.05). N i=1 IIwII1 + C min w 1646 Model Dev. Test P R F1 P R F1 Our system 90.1 90.9 90.5 85.5* 87.8 86.6 NAVYTIME 84.7* 79.6* 82.1* 87.7 78.3* 82.7* Figure 5: Results for the detection task. Model Dev. Test MAE r MAE r Our system 46.2 74.9 51.1 70.8 SVR 50.3* 74.8 57.1* 69.4 DISCRETE 50.3* 68.6* 52.4 62.2* PRABHAKARAN 58.7* 51.1* 62.0* 50.8* Figure 6: Results for the factuality task. due to the use of distributional features and more general dependency features. Factuality Resu</context>
</contexts>
<marker>Graham, Mathur, Baldwin, 2014</marker>
<rawString>Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2014. Randomized significance tests in machine translation. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight.joachims.org/,</title>
<date>1999</date>
<institution>University of Dortmund,</institution>
<contexts>
<context position="14972" citStr="Joachims, 1999" startWordPosition="2464" endWordPosition="2465">ti-class classifier (DISCRETE) using our features (Section 4) and labels that are discretized by rounding to the nearest integer, (2) a regression model (SVR) trained with the standard SVR objective using our features, and (3) a regression model (PRABHAKARAN) trained with the standard SVR objective using features from Prabhakaran et al. (2010). These features are highly informative, but their lexical features are restricted to a small set of manually defined words. Implementation Details The SVM models (NAVYTIME, DISCRETE, SVR, PRABHAKARAN, and our detection model) were trained with SVMLight (Joachims, 1999). We use CPLEX4 to solve the linear program optimizing the regression objective in Section 4. All hyperparameters were tuned on the development set. We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features. We use WordNet (Miller, 1995) to generate lemma and hyponym features. Brown clusters with 100, 320, 1000, and 3200 clusters from Turian et al. (2010) are used in the detection features. Evaluation Metrics We use the standard F1 score for the evaluation of detection. For event factuality, we report two metrics, the mean absol</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight.joachims.org/, University of Dortmund, 19(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of bionlp’09 shared task on event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4656" citStr="Kim et al., 2009" startWordPosition="698" endWordPosition="701"> that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for re</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of bionlp’09 shared task on event extraction. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared Task, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLT-NAACL</booktitle>
<pages>24--31</pages>
<contexts>
<context position="4638" citStr="Meyers et al., 2004" startWordPosition="694" endWordPosition="697"> a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), wh</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. In HLT-NAACL 2004 workshop: Frontiers in corpus annotation, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="15275" citStr="Miller, 1995" startWordPosition="2511" endWordPosition="2512"> features from Prabhakaran et al. (2010). These features are highly informative, but their lexical features are restricted to a small set of manually defined words. Implementation Details The SVM models (NAVYTIME, DISCRETE, SVR, PRABHAKARAN, and our detection model) were trained with SVMLight (Joachims, 1999). We use CPLEX4 to solve the linear program optimizing the regression objective in Section 4. All hyperparameters were tuned on the development set. We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features. We use WordNet (Miller, 1995) to generate lemma and hyponym features. Brown clusters with 100, 320, 1000, and 3200 clusters from Turian et al. (2010) are used in the detection features. Evaluation Metrics We use the standard F1 score for the evaluation of detection. For event factuality, we report two metrics, the mean absolute error (MAE) relative to the gold standard labels and Pearson’s correlation coefficient. While MAE is an intuitive metric that evaluates the absolute fit of the model, Pearson’s r better captures how well a system is able to recover the variation of the annotations. Pearson’s r is also conveniently </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational linguistics,</journal>
<pages>31--1</pages>
<contexts>
<context position="4617" citStr="Palmer et al., 2005" startWordPosition="690" endWordPosition="693">regression model uses a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marne</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinodkumar Prabhakaran</author>
<author>Owen Rambow</author>
<author>Mona Diab</author>
</authors>
<title>Automatic committed belief tagging.</title>
<date>2010</date>
<contexts>
<context position="5047" citStr="Prabhakaran et al., 2010" startWordPosition="754" endWordPosition="758">studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem for quoted statements on Tw</context>
<context position="14702" citStr="Prabhakaran et al. (2010)" startWordPosition="2422" endWordPosition="2426">erb. 5 Experimental Setup Baselines For detection, we include a baseline reimplementation of the NAVYTIME (Chambers, 2013) classification detector, one of the top performers in the TempEval-3 event detection task. For factuality, we include three baselines: (1) A one-vs.-rest multi-class classifier (DISCRETE) using our features (Section 4) and labels that are discretized by rounding to the nearest integer, (2) a regression model (SVR) trained with the standard SVR objective using our features, and (3) a regression model (PRABHAKARAN) trained with the standard SVR objective using features from Prabhakaran et al. (2010). These features are highly informative, but their lexical features are restricted to a small set of manually defined words. Implementation Details The SVM models (NAVYTIME, DISCRETE, SVR, PRABHAKARAN, and our detection model) were trained with SVMLight (Joachims, 1999). We use CPLEX4 to solve the linear program optimizing the regression objective in Section 4. All hyperparameters were tuned on the development set. We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features. We use WordNet (Miller, 1995) to generate lemma and hypo</context>
</contexts>
<marker>Prabhakaran, Rambow, Diab, 2010</marker>
<rawString>Vinodkumar Prabhakaran, Owen Rambow, and Mona Diab. 2010. Automatic committed belief tagging.</rawString>
</citation>
<citation valid="true">
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>1014--1022</pages>
<location>Beijing, China,</location>
<contexts>
<context position="14702" citStr="(2010)" startWordPosition="2426" endWordPosition="2426"> Setup Baselines For detection, we include a baseline reimplementation of the NAVYTIME (Chambers, 2013) classification detector, one of the top performers in the TempEval-3 event detection task. For factuality, we include three baselines: (1) A one-vs.-rest multi-class classifier (DISCRETE) using our features (Section 4) and labels that are discretized by rounding to the nearest integer, (2) a regression model (SVR) trained with the standard SVR objective using our features, and (3) a regression model (PRABHAKARAN) trained with the standard SVR objective using features from Prabhakaran et al. (2010). These features are highly informative, but their lexical features are restricted to a small set of manually defined words. Implementation Details The SVM models (NAVYTIME, DISCRETE, SVR, PRABHAKARAN, and our detection model) were trained with SVMLight (Joachims, 1999). We use CPLEX4 to solve the linear program optimizing the regression objective in Section 4. All hyperparameters were tuned on the development set. We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features. We use WordNet (Miller, 1995) to generate lemma and hypo</context>
</contexts>
<marker>2010</marker>
<rawString>In Coling 2010: Posters, pages 1014–1022, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
</authors>
<title>The timebank corpus. In Corpus linguistics.</title>
<date>2003</date>
<contexts>
<context position="4596" citStr="Pustejovsky et al., 2003" startWordPosition="686" endWordPosition="689">articular, our factuality regression model uses a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and cro</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, et al. 2003. The timebank corpus. In Corpus linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Sauri</author>
<author>James Pustejovsky</author>
</authors>
<title>Factbank: a corpus annotated with event factuality.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="5092" citStr="Sauri and Pustejovsky, 2009" startWordPosition="761" endWordPosition="764">hey model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem for quoted statements on Twitter, and they also crowdsourced factuality </context>
</contexts>
<marker>Sauri, Pustejovsky, 2009</marker>
<rawString>Roser Sauri and James Pustejovsky. 2009. Factbank: a corpus annotated with event factuality. Language Resources and Evaluation, 43(3):227–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 254–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyi Song</author>
<author>Ann Bies</author>
<author>Stephanie Strassel</author>
<author>Tom Riese</author>
<author>Justin Mott</author>
<author>Joe Ellis</author>
<author>Jonathan Wright</author>
<author>Seth Kulick</author>
<author>Neville Ryant</author>
<author>Xiaoyi Ma</author>
</authors>
<title>From light to rich ere: Annotation of entities, relations, and events.</title>
<date>2015</date>
<booktitle>Proceedings of the 3rd Workshop on EVENTS at the NAACL-HLT,</booktitle>
<pages>89--98</pages>
<contexts>
<context position="4676" citStr="Song et al., 2015" startWordPosition="702" endWordPosition="705"> advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgments for events, our models enable more fine-grained reasoning than previously considered. The corpus and learned models are available online.1 2 Related Work While event definitions have been proposed in several prior studies, existing approaches vary in how they model various linguistic forms such as nominal events, stative events, generic events, and light verbs (Pustejovsky et al., 2003; Palmer et al., 2005; Meyers et al., 2004; Kim et al., 2009; Song et al., 2015). Even with a formal and precise account of events, training annotators to learn all such linguistic intricacies remains a practical challenge. Instead of definition-driven instructions, we propose example-driven instructions and show their effectiveness. Previous studies have modeled event factuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factualit</context>
</contexts>
<marker>Song, Bies, Strassel, Riese, Mott, Ellis, Wright, Kulick, Ryant, Ma, 2015</marker>
<rawString>Zhiyi Song, Ann Bies, Stephanie Strassel, Tom Riese, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Ryant, and Xiaoyi Ma. 2015. From light to rich ere: Annotation of entities, relations, and events. Proceedings of the 3rd Workshop on EVENTS at the NAACL-HLT, pages 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandeep Soni</author>
<author>Tanushree Mitra</author>
<author>Eric Gilbert</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Modeling factuality judgments in social media text.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="5588" citStr="Soni et al. (2014)" startWordPosition="839" endWordPosition="842">ctuality assessment as a binary (Diab et al., 2009; Prabhakaran et al., 2010) or multi-class (Sauri and Pustejovsky, 2009) classification task, and they relied on expert annotators. A softer representation was proposed and crowdsourced by de Marneffe et al. (2012), who advocated for representing factuality from the reader’s perspective as a distribution of categories, but their annotation process requires manual normalization of the text. In contrast, we model factuality from the author’s perspective with scalar values, and we have an endto-end crowdsourced annotation pipeline. More recently, Soni et al. (2014) investigated a related problem for quoted statements on Twitter, and they also crowdsourced factuality annotations to learn regression models. While their approach is similar, we focus on predicting factuality for events that occur in every sentence. Without the restrictions of their task, we must reason about a larger variety of contextual cues. Our method of evaluating annotator agreement (Section 3) is related to the crowdsourcing study by Snow et al. (2008), who showed that pooled non-experts can match or outperform single expert annotators. In contrast, we approximate expert judgments by</context>
</contexts>
<marker>Soni, Mitra, Gilbert, Eisenstein, 2014</marker>
<rawString>Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob Eisenstein. 2014. Modeling factuality judgments in social media text. In Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>267--288</pages>
<contexts>
<context position="12850" citStr="Tibshirani, 1996" startWordPosition="2130" endWordPosition="2132">90 80 60 50 40 70 1 2 3 4 5 6 7 8 9 1645 in the Uu column of the confusion matrix suggests that the factuality of an event is rarely perceived as completely neutral, even when the author does not commit to a belief in the event’s occurrence. 4 Approach Learning For the detection task, we learn a linear SVM classification model. For the factuality task, we assume a dataset with N examples of labeled events {(xi, yi) |i = 1, ... , N}, and we learn a regression model: yi = wTO(xi). We introduce a learning objective for regression: max(0, |yi — wTO(xi) |— c) that combines the advantages of LASSO (Tibshirani, 1996) and support vector regression (Drucker et al., 1997). It induces sparse feature weights while being insensitive to errors less than c. Features For the detection model, we include features given the input word x: (1) lemma of x, (2) part of speech of x, (3) indicator for whether x is a hyponym of the event synset in WordNet and the part of speech of x, (4) Brown clusters of x and its part of speech, and (5) all dependency paths from x up to length 1. For the factuality model, given the input event mention x, we include: (1) lemma of x, (2) part of speech of x, and (3) all dependency paths fro</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15395" citStr="Turian et al. (2010)" startWordPosition="2529" endWordPosition="2532">estricted to a small set of manually defined words. Implementation Details The SVM models (NAVYTIME, DISCRETE, SVR, PRABHAKARAN, and our detection model) were trained with SVMLight (Joachims, 1999). We use CPLEX4 to solve the linear program optimizing the regression objective in Section 4. All hyperparameters were tuned on the development set. We use the Stanford dependency parser (de Marneffe et al., 2006) for extracting dependency path and part-of-speech features. We use WordNet (Miller, 1995) to generate lemma and hyponym features. Brown clusters with 100, 320, 1000, and 3200 clusters from Turian et al. (2010) are used in the detection features. Evaluation Metrics We use the standard F1 score for the evaluation of detection. For event factuality, we report two metrics, the mean absolute error (MAE) relative to the gold standard labels and Pearson’s correlation coefficient. While MAE is an intuitive metric that evaluates the absolute fit of the model, Pearson’s r better captures how well a system is able to recover the variation of the annotations. Pearson’s r is also conveniently normalized such that r = 0 for a system that blindly chooses the best a priori output and r = 1 for a system that makes </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Uzzaman</author>
<author>H Llorens</author>
<author>L Derczynski</author>
<author>M Verhagen</author>
<author>J Allen</author>
<author>J Pustejovsky</author>
</authors>
<title>Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="3603" citStr="Uzzaman et al., 2013" startWordPosition="546" endWordPosition="549"> describing things that the author claims could have happened, and rate each possibility on a scale of -3 (certainly did not happen) to 3 (certainly did). Figure 1 shows that non-expert workers—when their judgments are aggregated—consistently find a wide range of events and recognize the subtle differences in implied factuality. For example, the event set gets a score of 2.6, indicating that it likely but not certainly occurred, since it was ordered, whereas the ordered event, gets a score of 3.0. We gather data for event detection and factuality, reusing sentences from the TempEval-3 corpus (Uzzaman et al., 2013). Our approach produces high-quality labels with modest costs. We also introduce simple but highly effective models for both tasks that outperform strong baselines. In 1643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643–1648, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. particular, our factuality regression model uses a learning objective that combines the advantages of LASSO and support vector regression, enabling it to effectively consider sparse lexical cues. By providing scalar factuality judgm</context>
<context position="8661" citStr="Uzzaman et al., 2013" startWordPosition="1341" endWordPosition="1344">did not happen according to the author. 0 denotes that the author is neutral and expresses no bias towards the event’s factuality. 1644 Event Agreement Factuality Agreement k Figure 3: Agreement statistics as a function of k, the number of judgments aggregated. We choose k = 5 in both tasks for our experiments, as denoted by the red square. Data collection We gathered data on CrowdFlower.2 For quality control, annotators are randomly presented test questions with known answers. For each example, we collect and aggregate 5 judgments, as described below. For comparison, we annotated TempEval-3 (Uzzaman et al., 2013), keeping the existing test split and randomly holding out a quarter of the training examples to create a development set. Figure 2 shows data statistics. The annotation cost is 0.5¢ per judgment for detection and 2¢ per judgment for factuality. Aggregated Agreement We introduce a simple scheme to measure agreement with aggregate data, for example when the majority class from a pool of judgments is used for the final label. Instead of comparing individuals, we want to know how often the aggregates will agree, if we were to have different groups of annotators doing the task. Formally, we assume</context>
</contexts>
<marker>Uzzaman, Llorens, Derczynski, Verhagen, Allen, Pustejovsky, 2013</marker>
<rawString>N. Uzzaman, H. Llorens, L. Derczynski, M. Verhagen, J. Allen, and J. Pustejovsky. 2013. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Proceedings of the International Workshop on Semantic Evaluation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
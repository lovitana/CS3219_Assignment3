<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000079">
<title confidence="0.983102">
Adjective Intensity and Sentiment Analysis
</title>
<author confidence="0.999008">
Raksha Sharma, Mohit Gupta, Astha Agarwal, Pushpak Bhattacharyya
</author>
<affiliation confidence="0.9117025">
Dept. of Computer Science and Engineering
IIT Bombay, Mumbai, India
</affiliation>
<email confidence="0.990552">
{raksha,mohitgupta,astha11,pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.997286" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926736842106">
For fine-grained sentiment analysis, we
need to go beyond zero-one polarity and
find a way to compare adjectives that share
a common semantic property. In this
paper, we present a semi-supervised ap-
proach to assign intensity levels to adjec-
tives, viz. high, medium and low, where
adjectives are compared when they belong
to the same semantic category. For exam-
ple, in the semantic category of EXPER-
TISE, expert, experienced and familiar are
respectively of level high, medium and
low. We obtain an overall accuracy of 77%
for intensity assignment. We show the sig-
nificance of considering intensity informa-
tion of adjectives in predicting star-rating
of reviews. Our intensity based prediction
system results in an accuracy of 59% for a
5-star rated movie review corpus.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99961478125">
Sentence intensity becomes crucial when we need
to compare sentences having the same polarity ori-
entation. In such scenarios, we can use inten-
sity of words to judge the intensity of a sentence.
Words that bear the same semantic property can
be used interchangeably to upgrade or downgrade
the intensity of the expression. For example, good
and outstanding both are positive words from the
QUALITY category, but the latter can be used to
intensify positive expression in a sentence.
There are several manually or automatically
created lexical resources (Liu, 2010; Wilson et al.,
2005b; Wilson et al., 2005a; Taboada and Grieve,
2004) that assign a fixed positive (+1) or nega-
tive (−1) polarity to words, making no distinction
among them in terms of their intensity. This pa-
per presents a semi-supervised approach to assign
intensity levels to adjectives, viz. high, medium
and low, which share the same semantic property.
We have used the semantic frames of FrameNet-
1.5 (Baker et al., 1998) to obtain these semantic
categories. Our approach is based on the idea that
the most intense word has higher contextual simi-
larity with high intensity words than with medium
or low intensity words. We use the intensity an-
notated movie review corpus to obtain the most
intense word for a semantic category. Then, co-
sine similarity between word vectors of the most
intense word and other words of the category is
used to assign intensity levels to those words. Our
approach with the used resources is shown in fig-
ure 1.
</bodyText>
<figureCaption confidence="0.9966">
Figure 1: Intensity Analysis System
</figureCaption>
<bodyText confidence="0.998817909090909">
Our Contribution: Corpus based approaches suf-
fer from the data sparsity problem. Our approach
tackles this problem by using word vectors for in-
tensity assignment (Section 2.3). It also provides
a better overall accuracy (77%) than current state
of the art when compared with gold-standard in-
tensity levels (Section 6.2). In addition to this,
we show that accuracy of the star rating prediction
task improves when we incorporate our intensity
levels as features in addition to standard features
such as unigrams (Section 6.3).
</bodyText>
<page confidence="0.847997">
2520
</page>
<note confidence="0.841196">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2520–2526,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<sectionHeader confidence="0.897904" genericHeader="introduction">
2 Idea Used for Deriving Adjectival Scale
</sectionHeader>
<bodyText confidence="0.999228333333333">
In this paper, we dealt with 52 semantic (polar)
categories of the FrameNet data and derived the
polarity-intensity ordering among adjectives for
each category. Examples of these semantic cate-
gories with a few words that belong to the category
are as follows.
</bodyText>
<listItem confidence="0.994019333333333">
• INTELLIGENCE: Brainy, brainless, intelli-
gent, smart, dim etc.
• CANDIDNESS: Honest, dishonest, trust-
worthy, reliable, gullible etc.
• EMOTION: Sad, upset, appalled, tormented,
gleeful, happy, pleased etc.
</listItem>
<bodyText confidence="0.9009995">
Our algorithm to assign intensity levels to adjec-
tives is based on the following ideas:
</bodyText>
<subsectionHeader confidence="0.971675">
2.1 What Does Intensity Annotated Corpus
Tell About The Intensity of Words?
</subsectionHeader>
<bodyText confidence="0.999911583333333">
Rill et al. (2012) showed that an intensity anno-
tated polar corpus can be used to derive the in-
tensity of the adjectives. A high intensity word
will occur more frequently in high intensity re-
views. For example, the word excellent is found
118 times, while average is found only 16 times in
5-star rated movie reviews (Section 3). Based on
this distribution, we use a weighted mean formula
to find intensity of the words from the corpus. We
call it Weighted Normalized Polarity Intensity
(WNPI) formula. For a 5-star intensity rating cor-
pus, the WNPI formula is as follows:
</bodyText>
<equation confidence="0.9954795">
WNPI(word) =
5
E2 1 i ∗ Ci (1)
5 ∗ Ei=1 Ci
</equation>
<bodyText confidence="0.973729">
where Ci is the count of the word in i-star reviews.
</bodyText>
<subsectionHeader confidence="0.990988">
2.2 Need Significant Occurrence of A Word
</subsectionHeader>
<bodyText confidence="0.999974769230769">
The WNPI formula gives a corpus based result,
hence can give biased scores for words which oc-
cur less frequently in the corpus. For example,
in our movie review data-set, the word substan-
dard occurs only 3 times in the corpus, and these
occurrences happen to be in 1-star and 2-star re-
views only. Hence, the WNPI formula assigns a
higher score to substandard. To avoid such a bias,
we integrate WNPI formula with Chi-Square
test. Sharma and Bhattacharyya (2013) used Chi-
Square test to find significant polar words in a do-
main. We use the same categorical Chi-Square test
in our work.
</bodyText>
<subsectionHeader confidence="0.997042">
2.3 How to Get Intensity Clue for All Words?
</subsectionHeader>
<bodyText confidence="0.980416761904762">
A combination of WNPI formula and Chi-
Square test cannot assign intensity scores to ad-
jectives, which are not present in the corpus. To
overcome this data sparsity problem, we restrict
the use of WNPI formula to identify the most
intense word in each category. We explore pre-
computed context vectors of words, presented by
Mikolov et al. (2011) (Section 3), to assign in-
tensity levels to remaining words of the semantic
category:
Case-1 Words which have less number of
senses: These words will have a limited set of con-
text words. Hence, their context vectors will also
be based on these limited words. Example: excel-
lent, extraordinary, amazing, superb, great etc.
Case-2 Words which have many senses: These
words will have a large set of context words.
Hence their context vectors will be based on a set
of large number of words. Example: good, fair,
fine, average etc.
Inferences:
</bodyText>
<listItem confidence="0.882333">
1. Two words expressing similar meaning, and
satisfying case-1 will have similar context. Hence,
their word vectors will exhibit high cosine similar-
ity. Whereas a word satisfying case-2 will be less
similar to a word satisfying case-1.
2. The classical semantic bleaching theory1
states that a word which has less number of senses
(possibly one) tends to have higher intensity in
</listItem>
<bodyText confidence="0.976094181818182">
comparison to a word having more senses. Con-
sidering semantic bleaching phenomenon as a
base, we deduce that words which satisfy case-1
tend to be high intensity words while words satis-
fying case-2 are low intensity words.
Hence, we conclude that high intensity words
(case-1) have higher cosine similarity with each
other than with low or medium intensity words
(case-2). Therefore, cosine similarity with a high
intensity word can be used to obtain intensity or-
dering for remaining words of the category.
</bodyText>
<sectionHeader confidence="0.97793" genericHeader="method">
3 Data and Resources
</sectionHeader>
<bodyText confidence="0.986613">
This section gives an overview of the corpus and
lexical resources used in our approach.
Semantic Categories: We worked with frames
of FrameNet-1.5 (Baker et al., 1998). A frame
</bodyText>
<footnote confidence="0.997261">
1The semantic bleaching phenomenon in words was
reported in US edition of New York Times: http:
//www.nytimes.com/2010/07/18/magazine/
18onlanguage-anniversary.html?\_r=0
</footnote>
<page confidence="0.922883">
2521
</page>
<table confidence="0.99623175">
Rating Definition Size
0 Totally painful, unbearable 179
picture
1 Poor Show ( dont waste your 1057
money)
2 Average Movie 888
3 Excellent show, look for it 1977
4 A must see film 905
</table>
<tableCaption confidence="0.7523505">
Table 1: Review ratings with their definitions and
number of reviews.
</tableCaption>
<bodyText confidence="0.999056740740741">
represents a semantic property and contains words
bearing the property. We explored the FrameNet
data manually and found 52 frames (semantic cat-
egories) with polar semantic properties.
Intensity Annotated Corpus: To identify a
high intensity word for a semantic category, we
use a movie review corpus2 (Pang and Lee, 2005)
of 5006 files. Each review is rated on a scale of 0
to 4, where 0 indicates an unbearable movie and
4 represents a must see film. Table 1 describes the
meanings of the rating scores with the count of re-
views in each rating. We can infer that increase in
rating corresponds to increase in positive intensity
and decrease in negative intensity.
Sentiment Lexicon: To identify the polarity
orientation of words, we use a list of positive
(2006) and negative (4783) words3 (Liu, 2010).
We manually assign polarities to universally po-
lar words like enduring, creditable and nonsensi-
cal, which are missing in this lexicon, using other
standard lexicons. We found a total of 218 such
missing words.
Context Vectors: We use the precomputed
context vectors of words generated using Recur-
rent Neural Network Language Model (RNNLM)
(Mikolov et al., 2013). The RNN is trained with
320M words from the broadcast news data.
</bodyText>
<sectionHeader confidence="0.98767" genericHeader="method">
4 Gold Standard Data Preparation
</sectionHeader>
<bodyText confidence="0.998961">
We asked five annotators to assign words to differ-
ent intensity levels: high, medium, and low. An-
notators were given positive and negative words
of each category separately. The level chosen by
a majority of annotators is selected as the gold
</bodyText>
<footnote confidence="0.9816992">
2Written and rated by four authorized movie crit-
ics. Available at: http://www.cs.cornell.edu/
people/pabo/movie-review-data/
3Available at: http://www.cs.uic.edu/˜liub/
FBS/sentiment-analysis.html\#datasets
</footnote>
<bodyText confidence="0.974494833333333">
standard intensity level for the word. To compute
agreement among five annotators, we used fleiss’
kappa, and obtained a score of 0.61.
Figure 2: Intensity scale for QUALITY category,
where extraordinary was found as Pos-pivot and
awful as Neg-pivot.
</bodyText>
<sectionHeader confidence="0.962228" genericHeader="method">
5 Identification of Intensity of Adjectives
</sectionHeader>
<bodyText confidence="0.933448181818182">
In this section, we give a step-by-step description
of our approach.
Step 1: Find Intensity of Words
We calculate polarity-intensity of each word of a
semantic category using WNPI formula (eq. 1).
Based on the polarity orientation of a word, the
WNPI formula uses intensity interpretation of
star-rating as shown in table 2. The variable i of
the WNPI formula refers to these star ratings (in-
tensity levels). The polarity orientation of an ob-
served word is obtained using Bing Liu’s lexicon.
</bodyText>
<table confidence="0.8746792">
Star-Rating
❛❛❛❛❛❛❛❛❛❛ 0 1 2 3 4
Word-Orientation
Positive 1 2 3 4 5
Negative 5 4 3 2 1
</table>
<tableCaption confidence="0.8820355">
Table 2: Interpretation of star rating as intensity
scores of reviews for positive and negative words.
</tableCaption>
<bodyText confidence="0.4489635">
Step 2: Find Pivot Using Chi-Square Test
The word which gives the highest Chi-Square
</bodyText>
<page confidence="0.963308">
2522
</page>
<bodyText confidence="0.992310666666667">
score with the highest intensity score as per
WNPI is set as pivot (Pos-pivot and Neg-pivot).
The Chi-Square test helps us to exclude the biased
words, which are getting high intensity scores by
the WNPI formula, just by chance (Section 2.2).
Step 3: Obtain Similarity Scores with Pivot
Further, we compute the cosine similarity between
the context vectors of the pivot and the other words
of the category. We use Pos-pivot, if the observed
word is positive and Neg-pivot, if the observed
word is negative.
Step 4: Assign Intensity Level to Words
Finally, we arrange similarity scores obtained
above in decreasing order, and place 2 break
points in the sequence where consecutive similar-
ity scores differ the most. We set these break-
points as the thresholds for intensity levels.
Figure 2 shows the intensity scale obtained by
our approach for the QUALITY category, where
extraordinary was found as Pos-pivot and awful as
Neg-pivot.
</bodyText>
<sectionHeader confidence="0.999219" genericHeader="method">
6 Experiments And Results
</sectionHeader>
<bodyText confidence="0.9993285">
To evaluate the performance of our approach, we
consider three measures: accuracy with the gold-
standard data, comparison with state of the art and
accuracy for the star rating prediction task.
</bodyText>
<subsectionHeader confidence="0.995038">
6.1 Evaluation Using Gold Standard Data
</subsectionHeader>
<bodyText confidence="0.999991">
We compute accuracy as the fraction of adjectives
for which the predicted intensity level is the same
as the gold standard level. We obtained an overall
accuracy of 77% across 52 polar categories, con-
taining a total of 697 adjectives.
</bodyText>
<subsectionHeader confidence="0.999922">
6.2 Comparison with State of The Art
</subsectionHeader>
<bodyText confidence="0.999980416666667">
Ruppenhofer et al. (2014) showed that a cor-
pus based method called MeanStar approach per-
forms the best for intensity ordering task among
existing approaches (De Melo and Bansal, 2013;
Kim and de Marneffe, 2013; Fahrni and Klen-
ner, 2008; Dragut et al., 2010) for polar seman-
tic categories. Figure 3 shows the comparison be-
tween MeanStar and our approach for four seman-
tic categories4. For first three categories, our ap-
proach performs better than MeanStar and for EX-
PERTISE we obtain the same level of accuracy.
MeanStar approach gives an overall accuracy of
</bodyText>
<footnote confidence="0.513280333333333">
4We have used the same semantic categories and inten-
sity annotated movie review corpus in our work as used by
Ruppenhofer et al. (2014).
</footnote>
<bodyText confidence="0.995468">
73% across 52 polar categories, which is signifi-
cantly lesser than the accuracy obtained with our
approach. MeanStar approach does not assign in-
tensity score to words missing from the corpus.
While, 88 out of 122 missing words are assigned
correct intensity levels by our approach.
</bodyText>
<figureCaption confidence="0.9083635">
Figure 3: Accuracy obtained with MeanStar and
our approach
</figureCaption>
<subsectionHeader confidence="0.998727">
6.3 Evaluation Using Star Rating Prediction
</subsectionHeader>
<bodyText confidence="0.995605294117647">
There have been several successful attempts at
sentiment polarity detection in the past (Turney,
2002; Pang et al., 2002; Pang and Lee, 2004; Mo-
hammad et al., 2013; Svetlana Kiritchenko and
Mohammad, 2014). However, prediction of star
ratings still considered as a challenging task (Qu
et al., 2010; Gupta et al., 2010; Boteanu and Cher-
nova, 2013). We implemented three systems to
evaluate the significance of intensity annotated ad-
jectives in star rating prediction task.
System 1: A rule based system based on the
concept that negatively high intense words will oc-
cur more frequently in the low star reviews and
positively high intense words will occur more fre-
quently in the high star reviews. This system uses
the following function I to assign intensity score
to a review r:
</bodyText>
<equation confidence="0.981736">
I (r) = E3i= 1 i 3 CPiP �3i= 3 i * NN 2
3 * (�i=1 Ci + �i=1 Ci ) ( )
</equation>
<bodyText confidence="0.998390333333333">
where CPi and CNi respectively represent sum of
the term-frequencies of positive and negative ad-
jectives with intensity i.
Eq. 2 gives us an intensity score between −1
and +1 for each review. We need four breakpoints
on these intensity scores to map intensity scores
</bodyText>
<page confidence="0.918296">
2523
</page>
<bodyText confidence="0.972557375">
into 5-star ratings. We learn these breakpoints by
maximizing accuracy for the training data5 over all
possible breakpoints.
System 2: In this system, we consider intensity
of each adjective as +1 or −1 as per its polarity,
and then uses eq. 2 to find review intensity.
System 3: This is an SVM based system which
uses four different types of features: (a) unigrams,
</bodyText>
<listItem confidence="0.940098333333333">
(b) unigrams with the modification that if adjec-
tive belongs to our intensity annotated adjective
list, then feature value is intensity of the adjective,
(c) and (d) use the scores coming from eq. 2 as an
additional feature over those in (a) and (b) respec-
tively.
</listItem>
<table confidence="0.998988428571429">
System Accuracy(%) MSE MAE
1 42.28 0.94 0.69
2 27.33 1.12 0.86
55.81 0.63 0.50
57.21 0.56 0.47
58.71 0.57 0.46
59.21 0.54 0.45
</table>
<tableCaption confidence="0.998906">
Table 3: Comparison of rating prediction systems,
</tableCaption>
<bodyText confidence="0.999812857142857">
where MSE is the Mean Squared Error and MAE
is the Mean Absolute Error
Table 3 shows the results obtained with the
above systems. System 3(d) achieves the maxi-
mum accuracy depicting that inclusion of intensity
information with the standard features improves
the star rating prediction significantly.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.963526977272727">
Sentiment analysis on adjectives has been exten-
sively explored in NLP literature. However, most
of the works addressed the problem of finding po-
larity orientation of adjectives (Hatzivassiloglou
and McKeown, 1997; Wiebe, 2000; Fahrni and
Klenner, 2008; Dragut et al., 2010). The first work
in the direction of adjectival scale was done by
Hatzivassiloglou and McKeown (1993). They ex-
ploited linguistic knowledge available in the cor-
pora to compute similarity between adjectives.
However, their approach did not consider polarity
orientation of adjectives, they provided ordering
among non-polar adjectives like, cold, lukewarm,
warm, hot.
5We use 80% of the star-rated movie review corpus as
training data and 20% as test data. The results reported in
table 3 are based on the 20% test data.
The task of ordering adjectives according to
their polarity-intensity has recently received much
attention due to the vital role of intensity analy-
sis in several real world tasks. Kim et al. (2013)
interpreted the continuous space word representa-
tion by demonstrating that vector off-set can be
used to derive scalar relationship amongst adjec-
tives. Their approach provided relationship among
all the adjectives independent of their semantic
property. De Melo and Bansal (2013) used a pat-
tern based approach to identify intensity relation
among adjectives, but their approach had a severe
coverage problem. They also did not consider the
semantic property of adjectives, assuming one sin-
gle intensity-scale for all adjectives.
Ruppenhofer et al. (2014) provided ordering
among polar adjectives that bear the same seman-
tic property. Their approach was completely cor-
pus dependent, it was not able to derive intensity
of those adjectives which were not found in the
corpus. We have used the same star-rated movie
review corpus in our work as used by Ruppenhofer
et al. (2014) and found 122 polar adjectives which
are absent from the corpus. Our system is able
to identify intensity levels for these missing adjec-
tives. Moreover, we obtained an improvement of
4% in overall accuracy.
</bodyText>
<sectionHeader confidence="0.992849" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999962933333334">
In this paper, we have proposed an approach that
assigns intensity levels to domain independent ad-
jectives, viz. high, medium and low. The impor-
tant feature of our approach is that it is not fully
corpus dependent, hence is able to assign inten-
sity to adjectives that are absent in the corpus. We
have reported that the overall results are better than
the recently reported corpus based approach and
fairly close to human agreement on this challeng-
ing task.
The use of adjectives with their intensity infor-
mation can enrich existing sentiment analysis sys-
tems. We have shown the significance of consider-
ing intensity information of adjectives in predict-
ing the intensity of movie reviews.
</bodyText>
<sectionHeader confidence="0.994737" genericHeader="acknowledgments">
9 Acknowledgment
</sectionHeader>
<bodyText confidence="0.99947325">
We heartily thank English linguists Rajita Shukla
and Jaya Saraswati from CFILT Lab, IIT Bombay
for giving their valuable contribution in gold stan-
dard data creation.
</bodyText>
<page confidence="0.987819">
2524
</page>
<sectionHeader confidence="0.995748" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998768345454546">
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th international conference on Compu-
tational linguistics-Volume 1. Association for Com-
putational Linguistics.
Adrian Boteanu and Sonia Chernova. 2013. Unsuper-
vised rating prediction based on local and global se-
mantic models. In 2013 AAAI Fall Symposium Se-
ries.
Gerard De Melo and Mohit Bansal. 2013. Good, great,
excellent: Global inference of semantic intensities.
Transactions of the Association for Computational
Linguistics, 1.
Eduard C Dragut, Clement Yu, Prasad Sistla, and Weiyi
Meng. 2010. Construction of a sentimental word
dictionary. In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management. ACM.
Angela Fahrni and Manfred Klenner. 2008. Old wine
or warm beer: Target-specific sentiment analysis of
adjectives. In Proc. of the Symposium on Affective
Language in Human and Machine, AISB.
Narendra Gupta, Giuseppe Di Fabbrizio, and Patrick
Haffner. 2010. Capturing the stars: predicting rat-
ings for service and product reviews. In Proceed-
ings of the NAACL HLT 2010 Workshop on Semantic
Search. Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1993. Towards the automatic identification of ad-
jectival scales: Clustering adjectives according to
meaning. In Proceedings of the 31st annual meeting
on Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th annual meeting
of the association for computational linguistics and
eighth conference of the european chapter of the as-
sociation for computational linguistics. Association
for Computational Linguistics.
Joo-Kyung Kim and Marie-Catherine de Marneffe.
2013. Deriving adjectival scales from continuous
space word representations. In EMNLP.
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of natural language processing, 2.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for
training large scale neural network language mod-
els. In Automatic Speech Recognition and Under-
standing (ASRU), 2011 IEEE Workshop on. IEEE.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the seventh international workshop on
Semantic Evaluation Exercises.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing-Volume 10. Association for
Computational Linguistics.
Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.
2010. The bag-of-opinions method for review rat-
ing prediction from sparse text patterns. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics. Association for Compu-
tational Linguistics.
Sven Rill, J¨org Scheidt, Johannes Drescher, Oliver
Sch¨utz, Dirk Reinel, and Florian Wogenstein. 2012.
A generic approach to generate opinion lists of
phrases for opinion mining applications. In Pro-
ceedings of the First International Workshop on Is-
sues of Sentiment Discovery and Opinion Mining.
ACM.
Josef Ruppenhofer, Michael Wiegand, and Jasper
Brandes. 2014. Comparing methods for deriving
intensity scores for adjectives. EACL 2014, 117.
Raksha Sharma and Pushpak Bhattacharyya. 2013.
Detecting domain dedicated polar words. IJCNP
2013, pages 661–666.
Xiaodan Zhu Svetlana Kiritchenko and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. 50.
Maite Taboada and Jack Grieve. 2004. Analyzing
appraisal automatically. In Proceedings of AAAI
Spring Symposium on Exploring Attitude and Affect
in Text (AAAI Technical Re# port SS# 04# 07), Stan-
ford University, CA, pp. 158q161. AAAI Press.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th an-
nual meeting on association for computational lin-
guistics, pages 417–424. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.707099">
2525
</page>
<reference confidence="0.999664266666667">
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In AAAI/IAAI.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: A system for subjectiv-
ity analysis. In Proceedings of hlt/emnlp on interac-
tive demonstrations. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on human language technology and empir-
ical methods in natural language processing. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.989164">
2526
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.832983">
<title confidence="0.99982">Adjective Intensity and Sentiment Analysis</title>
<author confidence="0.998262">Raksha Sharma</author>
<author confidence="0.998262">Mohit Gupta</author>
<author confidence="0.998262">Astha Agarwal</author>
<author confidence="0.998262">Pushpak</author>
<affiliation confidence="0.9298975">Dept. of Computer Science and IIT Bombay, Mumbai,</affiliation>
<abstract confidence="0.99841085">For fine-grained sentiment analysis, we need to go beyond zero-one polarity and find a way to compare adjectives that share a common semantic property. In this paper, we present a semi-supervised approach to assign intensity levels to adjechigh, medium where adjectives are compared when they belong to the same semantic category. For example, in the semantic category of EXPERexperienced of level medium We obtain an overall accuracy of 77% for intensity assignment. We show the significance of considering intensity information of adjectives in predicting star-rating of reviews. Our intensity based prediction system results in an accuracy of 59% for a 5-star rated movie review corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics-Volume 1. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2006" citStr="Baker et al., 1998" startWordPosition="309" endWordPosition="312">ve words from the QUALITY category, but the latter can be used to intensify positive expression in a sentence. There are several manually or automatically created lexical resources (Liu, 2010; Wilson et al., 2005b; Wilson et al., 2005a; Taboada and Grieve, 2004) that assign a fixed positive (+1) or negative (−1) polarity to words, making no distinction among them in terms of their intensity. This paper presents a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low, which share the same semantic property. We have used the semantic frames of FrameNet1.5 (Baker et al., 1998) to obtain these semantic categories. Our approach is based on the idea that the most intense word has higher contextual similarity with high intensity words than with medium or low intensity words. We use the intensity annotated movie review corpus to obtain the most intense word for a semantic category. Then, cosine similarity between word vectors of the most intense word and other words of the category is used to assign intensity levels to those words. Our approach with the used resources is shown in figure 1. Figure 1: Intensity Analysis System Our Contribution: Corpus based approaches suf</context>
<context position="7259" citStr="Baker et al., 1998" startWordPosition="1184" endWordPosition="1187">menon as a base, we deduce that words which satisfy case-1 tend to be high intensity words while words satisfying case-2 are low intensity words. Hence, we conclude that high intensity words (case-1) have higher cosine similarity with each other than with low or medium intensity words (case-2). Therefore, cosine similarity with a high intensity word can be used to obtain intensity ordering for remaining words of the category. 3 Data and Resources This section gives an overview of the corpus and lexical resources used in our approach. Semantic Categories: We worked with frames of FrameNet-1.5 (Baker et al., 1998). A frame 1The semantic bleaching phenomenon in words was reported in US edition of New York Times: http: //www.nytimes.com/2010/07/18/magazine/ 18onlanguage-anniversary.html?\_r=0 2521 Rating Definition Size 0 Totally painful, unbearable 179 picture 1 Poor Show ( dont waste your 1057 money) 2 Average Movie 888 3 Excellent show, look for it 1977 4 A must see film 905 Table 1: Review ratings with their definitions and number of reviews. represents a semantic property and contains words bearing the property. We explored the FrameNet data manually and found 52 frames (semantic categories) with po</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics-Volume 1. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Boteanu</author>
<author>Sonia Chernova</author>
</authors>
<title>Unsupervised rating prediction based on local and global semantic models.</title>
<date>2013</date>
<booktitle>In 2013 AAAI Fall Symposium Series.</booktitle>
<contexts>
<context position="13374" citStr="Boteanu and Chernova, 2013" startWordPosition="2172" endWordPosition="2176">h. MeanStar approach does not assign intensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach. Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mohammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Chernova, 2013). We implemented three systems to evaluate the significance of intensity annotated adjectives in star rating prediction task. System 1: A rule based system based on the concept that negatively high intense words will occur more frequently in the low star reviews and positively high intense words will occur more frequently in the high star reviews. This system uses the following function I to assign intensity score to a review r: I (r) = E3i= 1 i 3 CPiP �3i= 3 i * NN 2 3 * (�i=1 Ci + �i=1 Ci ) ( ) where CPi and CNi respectively represent sum of the term-frequencies of positive and negative adje</context>
</contexts>
<marker>Boteanu, Chernova, 2013</marker>
<rawString>Adrian Boteanu and Sonia Chernova. 2013. Unsupervised rating prediction based on local and global semantic models. In 2013 AAAI Fall Symposium Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard De Melo</author>
<author>Mohit Bansal</author>
</authors>
<title>Good, great, excellent: Global inference of semantic intensities.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<marker>De Melo, Bansal, 2013</marker>
<rawString>Gerard De Melo and Mohit Bansal. 2013. Good, great, excellent: Global inference of semantic intensities. Transactions of the Association for Computational Linguistics, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard C Dragut</author>
<author>Clement Yu</author>
<author>Prasad Sistla</author>
<author>Weiyi Meng</author>
</authors>
<title>Construction of a sentimental word dictionary.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="12204" citStr="Dragut et al., 2010" startWordPosition="1981" endWordPosition="1984"> and accuracy for the star rating prediction task. 6.1 Evaluation Using Gold Standard Data We compute accuracy as the fraction of adjectives for which the predicted intensity level is the same as the gold standard level. We obtained an overall accuracy of 77% across 52 polar categories, containing a total of 697 adjectives. 6.2 Comparison with State of The Art Ruppenhofer et al. (2014) showed that a corpus based method called MeanStar approach performs the best for intensity ordering task among existing approaches (De Melo and Bansal, 2013; Kim and de Marneffe, 2013; Fahrni and Klenner, 2008; Dragut et al., 2010) for polar semantic categories. Figure 3 shows the comparison between MeanStar and our approach for four semantic categories4. For first three categories, our approach performs better than MeanStar and for EXPERTISE we obtain the same level of accuracy. MeanStar approach gives an overall accuracy of 4We have used the same semantic categories and intensity annotated movie review corpus in our work as used by Ruppenhofer et al. (2014). 73% across 52 polar categories, which is significantly lesser than the accuracy obtained with our approach. MeanStar approach does not assign intensity score to w</context>
<context position="15544" citStr="Dragut et al., 2010" startWordPosition="2547" endWordPosition="2550">n of rating prediction systems, where MSE is the Mean Squared Error and MAE is the Mean Absolute Error Table 3 shows the results obtained with the above systems. System 3(d) achieves the maximum accuracy depicting that inclusion of intensity information with the standard features improves the star rating prediction significantly. 7 Related Work Sentiment analysis on adjectives has been extensively explored in NLP literature. However, most of the works addressed the problem of finding polarity orientation of adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Fahrni and Klenner, 2008; Dragut et al., 2010). The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They exploited linguistic knowledge available in the corpora to compute similarity between adjectives. However, their approach did not consider polarity orientation of adjectives, they provided ordering among non-polar adjectives like, cold, lukewarm, warm, hot. 5We use 80% of the star-rated movie review corpus as training data and 20% as test data. The results reported in table 3 are based on the 20% test data. The task of ordering adjectives according to their polarity-intensity has recentl</context>
</contexts>
<marker>Dragut, Yu, Sistla, Meng, 2010</marker>
<rawString>Eduard C Dragut, Clement Yu, Prasad Sistla, and Weiyi Meng. 2010. Construction of a sentimental word dictionary. In Proceedings of the 19th ACM international conference on Information and knowledge management. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angela Fahrni</author>
<author>Manfred Klenner</author>
</authors>
<title>Old wine or warm beer: Target-specific sentiment analysis of adjectives.</title>
<date>2008</date>
<booktitle>In Proc. of the Symposium on Affective Language in Human and Machine, AISB.</booktitle>
<contexts>
<context position="12182" citStr="Fahrni and Klenner, 2008" startWordPosition="1976" endWordPosition="1980">ison with state of the art and accuracy for the star rating prediction task. 6.1 Evaluation Using Gold Standard Data We compute accuracy as the fraction of adjectives for which the predicted intensity level is the same as the gold standard level. We obtained an overall accuracy of 77% across 52 polar categories, containing a total of 697 adjectives. 6.2 Comparison with State of The Art Ruppenhofer et al. (2014) showed that a corpus based method called MeanStar approach performs the best for intensity ordering task among existing approaches (De Melo and Bansal, 2013; Kim and de Marneffe, 2013; Fahrni and Klenner, 2008; Dragut et al., 2010) for polar semantic categories. Figure 3 shows the comparison between MeanStar and our approach for four semantic categories4. For first three categories, our approach performs better than MeanStar and for EXPERTISE we obtain the same level of accuracy. MeanStar approach gives an overall accuracy of 4We have used the same semantic categories and intensity annotated movie review corpus in our work as used by Ruppenhofer et al. (2014). 73% across 52 polar categories, which is significantly lesser than the accuracy obtained with our approach. MeanStar approach does not assig</context>
<context position="15522" citStr="Fahrni and Klenner, 2008" startWordPosition="2543" endWordPosition="2546">54 0.45 Table 3: Comparison of rating prediction systems, where MSE is the Mean Squared Error and MAE is the Mean Absolute Error Table 3 shows the results obtained with the above systems. System 3(d) achieves the maximum accuracy depicting that inclusion of intensity information with the standard features improves the star rating prediction significantly. 7 Related Work Sentiment analysis on adjectives has been extensively explored in NLP literature. However, most of the works addressed the problem of finding polarity orientation of adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Fahrni and Klenner, 2008; Dragut et al., 2010). The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They exploited linguistic knowledge available in the corpora to compute similarity between adjectives. However, their approach did not consider polarity orientation of adjectives, they provided ordering among non-polar adjectives like, cold, lukewarm, warm, hot. 5We use 80% of the star-rated movie review corpus as training data and 20% as test data. The results reported in table 3 are based on the 20% test data. The task of ordering adjectives according to their polarity</context>
</contexts>
<marker>Fahrni, Klenner, 2008</marker>
<rawString>Angela Fahrni and Manfred Klenner. 2008. Old wine or warm beer: Target-specific sentiment analysis of adjectives. In Proc. of the Symposium on Affective Language in Human and Machine, AISB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narendra Gupta</author>
<author>Giuseppe Di Fabbrizio</author>
<author>Patrick Haffner</author>
</authors>
<title>Capturing the stars: predicting ratings for service and product reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search. Association for Computational Linguistics.</booktitle>
<marker>Gupta, Di Fabbrizio, Haffner, 2010</marker>
<rawString>Narendra Gupta, Giuseppe Di Fabbrizio, and Patrick Haffner. 2010. Capturing the stars: predicting ratings for service and product reviews. In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting on Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="15645" citStr="Hatzivassiloglou and McKeown (1993)" startWordPosition="2563" endWordPosition="2566">n Absolute Error Table 3 shows the results obtained with the above systems. System 3(d) achieves the maximum accuracy depicting that inclusion of intensity information with the standard features improves the star rating prediction significantly. 7 Related Work Sentiment analysis on adjectives has been extensively explored in NLP literature. However, most of the works addressed the problem of finding polarity orientation of adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Fahrni and Klenner, 2008; Dragut et al., 2010). The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They exploited linguistic knowledge available in the corpora to compute similarity between adjectives. However, their approach did not consider polarity orientation of adjectives, they provided ordering among non-polar adjectives like, cold, lukewarm, warm, hot. 5We use 80% of the star-rated movie review corpus as training data and 20% as test data. The results reported in table 3 are based on the 20% test data. The task of ordering adjectives according to their polarity-intensity has recently received much attention due to the vital role of intensity analysis in several real world tasks. Ki</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1993</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R McKeown. 1993. Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning. In Proceedings of the 31st annual meeting on Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th annual</booktitle>
<contexts>
<context position="15483" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="2537" endWordPosition="2540">.63 0.50 57.21 0.56 0.47 58.71 0.57 0.46 59.21 0.54 0.45 Table 3: Comparison of rating prediction systems, where MSE is the Mean Squared Error and MAE is the Mean Absolute Error Table 3 shows the results obtained with the above systems. System 3(d) achieves the maximum accuracy depicting that inclusion of intensity information with the standard features improves the star rating prediction significantly. 7 Related Work Sentiment analysis on adjectives has been extensively explored in NLP literature. However, most of the works addressed the problem of finding polarity orientation of adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Fahrni and Klenner, 2008; Dragut et al., 2010). The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They exploited linguistic knowledge available in the corpora to compute similarity between adjectives. However, their approach did not consider polarity orientation of adjectives, they provided ordering among non-polar adjectives like, cold, lukewarm, warm, hot. 5We use 80% of the star-rated movie review corpus as training data and 20% as test data. The results reported in table 3 are based on the 20% test data. The task of ordering</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th annual meeting of the association for computational linguistics and eighth conference of the european chapter of the association for computational linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joo-Kyung Kim</author>
<author>Marie-Catherine de Marneffe</author>
</authors>
<title>Deriving adjectival scales from continuous space word representations.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<marker>Kim, de Marneffe, 2013</marker>
<rawString>Joo-Kyung Kim and Marie-Catherine de Marneffe. 2013. Deriving adjectival scales from continuous space word representations. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>Handbook of natural language processing,</booktitle>
<pages>2</pages>
<contexts>
<context position="1578" citStr="Liu, 2010" startWordPosition="239" endWordPosition="240">star rated movie review corpus. 1 Introduction Sentence intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of words to judge the intensity of a sentence. Words that bear the same semantic property can be used interchangeably to upgrade or downgrade the intensity of the expression. For example, good and outstanding both are positive words from the QUALITY category, but the latter can be used to intensify positive expression in a sentence. There are several manually or automatically created lexical resources (Liu, 2010; Wilson et al., 2005b; Wilson et al., 2005a; Taboada and Grieve, 2004) that assign a fixed positive (+1) or negative (−1) polarity to words, making no distinction among them in terms of their intensity. This paper presents a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low, which share the same semantic property. We have used the semantic frames of FrameNet1.5 (Baker et al., 1998) to obtain these semantic categories. Our approach is based on the idea that the most intense word has higher contextual similarity with high intensity words than with medi</context>
<context position="8502" citStr="Liu, 2010" startWordPosition="1388" endWordPosition="1389">y Annotated Corpus: To identify a high intensity word for a semantic category, we use a movie review corpus2 (Pang and Lee, 2005) of 5006 files. Each review is rated on a scale of 0 to 4, where 0 indicates an unbearable movie and 4 represents a must see film. Table 1 describes the meanings of the rating scores with the count of reviews in each rating. We can infer that increase in rating corresponds to increase in positive intensity and decrease in negative intensity. Sentiment Lexicon: To identify the polarity orientation of words, we use a list of positive (2006) and negative (4783) words3 (Liu, 2010). We manually assign polarities to universally polar words like enduring, creditable and nonsensical, which are missing in this lexicon, using other standard lexicons. We found a total of 218 such missing words. Context Vectors: We use the precomputed context vectors of words generated using Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2013). The RNN is trained with 320M words from the broadcast news data. 4 Gold Standard Data Preparation We asked five annotators to assign words to different intensity levels: high, medium, and low. Annotators were given positive and negativ</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment analysis and subjectivity. Handbook of natural language processing, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Anoop Deoras</author>
<author>Daniel Povey</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
</authors>
<title>Strategies for training large scale neural network language models.</title>
<date>2011</date>
<booktitle>In Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on.</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="5643" citStr="Mikolov et al. (2011)" startWordPosition="919" endWordPosition="922">tandard. To avoid such a bias, we integrate WNPI formula with Chi-Square test. Sharma and Bhattacharyya (2013) used ChiSquare test to find significant polar words in a domain. We use the same categorical Chi-Square test in our work. 2.3 How to Get Intensity Clue for All Words? A combination of WNPI formula and ChiSquare test cannot assign intensity scores to adjectives, which are not present in the corpus. To overcome this data sparsity problem, we restrict the use of WNPI formula to identify the most intense word in each category. We explore precomputed context vectors of words, presented by Mikolov et al. (2011) (Section 3), to assign intensity levels to remaining words of the semantic category: Case-1 Words which have less number of senses: These words will have a limited set of context words. Hence, their context vectors will also be based on these limited words. Example: excellent, extraordinary, amazing, superb, great etc. Case-2 Words which have many senses: These words will have a large set of context words. Hence their context vectors will be based on a set of large number of words. Example: good, fair, fine, average etc. Inferences: 1. Two words expressing similar meaning, and satisfying case</context>
</contexts>
<marker>Mikolov, Deoras, Povey, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget, and Jan Cernocky. 2011. Strategies for training large scale neural network language models. In Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="8865" citStr="Mikolov et al., 2013" startWordPosition="1443" endWordPosition="1446">. We can infer that increase in rating corresponds to increase in positive intensity and decrease in negative intensity. Sentiment Lexicon: To identify the polarity orientation of words, we use a list of positive (2006) and negative (4783) words3 (Liu, 2010). We manually assign polarities to universally polar words like enduring, creditable and nonsensical, which are missing in this lexicon, using other standard lexicons. We found a total of 218 such missing words. Context Vectors: We use the precomputed context vectors of words generated using Recurrent Neural Network Language Model (RNNLM) (Mikolov et al., 2013). The RNN is trained with 320M words from the broadcast news data. 4 Gold Standard Data Preparation We asked five annotators to assign words to different intensity levels: high, medium, and low. Annotators were given positive and negative words of each category separately. The level chosen by a majority of annotators is selected as the gold 2Written and rated by four authorized movie critics. Available at: http://www.cs.cornell.edu/ people/pabo/movie-review-data/ 3Available at: http://www.cs.uic.edu/˜liub/ FBS/sentiment-analysis.html\#datasets standard intensity level for the word. To compute </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the seventh international workshop on Semantic Evaluation Exercises.</booktitle>
<contexts>
<context position="13190" citStr="Mohammad et al., 2013" startWordPosition="2143" endWordPosition="2147">ed movie review corpus in our work as used by Ruppenhofer et al. (2014). 73% across 52 polar categories, which is significantly lesser than the accuracy obtained with our approach. MeanStar approach does not assign intensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach. Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mohammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Chernova, 2013). We implemented three systems to evaluate the significance of intensity annotated adjectives in star rating prediction task. System 1: A rule based system based on the concept that negatively high intense words will occur more frequently in the low star reviews and positively high intense words will occur more frequently in the high star reviews. This system uses the following function I to assign intensity scor</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the stateof-the-art in sentiment analysis of tweets. In Proceedings of the seventh international workshop on Semantic Evaluation Exercises.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13167" citStr="Pang and Lee, 2004" startWordPosition="2139" endWordPosition="2142">nd intensity annotated movie review corpus in our work as used by Ruppenhofer et al. (2014). 73% across 52 polar categories, which is significantly lesser than the accuracy obtained with our approach. MeanStar approach does not assign intensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach. Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mohammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Chernova, 2013). We implemented three systems to evaluate the significance of intensity annotated adjectives in star rating prediction task. System 1: A rule based system based on the concept that negatively high intense words will occur more frequently in the low star reviews and positively high intense words will occur more frequently in the high star reviews. This system uses the following function I t</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8021" citStr="Pang and Lee, 2005" startWordPosition="1301" endWordPosition="1304">/ 18onlanguage-anniversary.html?\_r=0 2521 Rating Definition Size 0 Totally painful, unbearable 179 picture 1 Poor Show ( dont waste your 1057 money) 2 Average Movie 888 3 Excellent show, look for it 1977 4 A must see film 905 Table 1: Review ratings with their definitions and number of reviews. represents a semantic property and contains words bearing the property. We explored the FrameNet data manually and found 52 frames (semantic categories) with polar semantic properties. Intensity Annotated Corpus: To identify a high intensity word for a semantic category, we use a movie review corpus2 (Pang and Lee, 2005) of 5006 files. Each review is rated on a scale of 0 to 4, where 0 indicates an unbearable movie and 4 represents a must see film. Table 1 describes the meanings of the rating scores with the count of reviews in each rating. We can infer that increase in rating corresponds to increase in positive intensity and decrease in negative intensity. Sentiment Lexicon: To identify the polarity orientation of words, we use a list of positive (2006) and negative (4783) words3 (Liu, 2010). We manually assign polarities to universally polar words like enduring, creditable and nonsensical, which are missing</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13147" citStr="Pang et al., 2002" startWordPosition="2135" endWordPosition="2138">mantic categories and intensity annotated movie review corpus in our work as used by Ruppenhofer et al. (2014). 73% across 52 polar categories, which is significantly lesser than the accuracy obtained with our approach. MeanStar approach does not assign intensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach. Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mohammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Chernova, 2013). We implemented three systems to evaluate the significance of intensity annotated adjectives in star rating prediction task. System 1: A rule based system based on the concept that negatively high intense words will occur more frequently in the low star reviews and positively high intense words will occur more frequently in the high star reviews. This system uses the fo</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lizhen Qu</author>
<author>Georgiana Ifrim</author>
<author>Gerhard Weikum</author>
</authors>
<title>The bag-of-opinions method for review rating prediction from sparse text patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13325" citStr="Qu et al., 2010" startWordPosition="2164" endWordPosition="2167">he accuracy obtained with our approach. MeanStar approach does not assign intensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach. Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mohammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Chernova, 2013). We implemented three systems to evaluate the significance of intensity annotated adjectives in star rating prediction task. System 1: A rule based system based on the concept that negatively high intense words will occur more frequently in the low star reviews and positively high intense words will occur more frequently in the high star reviews. This system uses the following function I to assign intensity score to a review r: I (r) = E3i= 1 i 3 CPiP �3i= 3 i * NN 2 3 * (�i=1 Ci + �i=1 Ci ) ( ) where CPi and CNi respectively represent sum of t</context>
</contexts>
<marker>Qu, Ifrim, Weikum, 2010</marker>
<rawString>Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum. 2010. The bag-of-opinions method for review rating prediction from sparse text patterns. In Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Rill</author>
<author>J¨org Scheidt</author>
<author>Johannes Drescher</author>
<author>Oliver Sch¨utz</author>
<author>Dirk Reinel</author>
<author>Florian Wogenstein</author>
</authors>
<title>A generic approach to generate opinion lists of phrases for opinion mining applications.</title>
<date>2012</date>
<booktitle>In Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining.</booktitle>
<publisher>ACM.</publisher>
<marker>Rill, Scheidt, Drescher, Sch¨utz, Reinel, Wogenstein, 2012</marker>
<rawString>Sven Rill, J¨org Scheidt, Johannes Drescher, Oliver Sch¨utz, Dirk Reinel, and Florian Wogenstein. 2012. A generic approach to generate opinion lists of phrases for opinion mining applications. In Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Wiegand</author>
<author>Jasper Brandes</author>
</authors>
<title>Comparing methods for deriving intensity scores for adjectives. EACL</title>
<date>2014</date>
<pages>117</pages>
<contexts>
<context position="11972" citStr="Ruppenhofer et al. (2014)" startWordPosition="1941" endWordPosition="1944">, where extraordinary was found as Pos-pivot and awful as Neg-pivot. 6 Experiments And Results To evaluate the performance of our approach, we consider three measures: accuracy with the goldstandard data, comparison with state of the art and accuracy for the star rating prediction task. 6.1 Evaluation Using Gold Standard Data We compute accuracy as the fraction of adjectives for which the predicted intensity level is the same as the gold standard level. We obtained an overall accuracy of 77% across 52 polar categories, containing a total of 697 adjectives. 6.2 Comparison with State of The Art Ruppenhofer et al. (2014) showed that a corpus based method called MeanStar approach performs the best for intensity ordering task among existing approaches (De Melo and Bansal, 2013; Kim and de Marneffe, 2013; Fahrni and Klenner, 2008; Dragut et al., 2010) for polar semantic categories. Figure 3 shows the comparison between MeanStar and our approach for four semantic categories4. For first three categories, our approach performs better than MeanStar and for EXPERTISE we obtain the same level of accuracy. MeanStar approach gives an overall accuracy of 4We have used the same semantic categories and intensity annotated </context>
<context position="16815" citStr="Ruppenhofer et al. (2014)" startWordPosition="2743" endWordPosition="2746">e of intensity analysis in several real world tasks. Kim et al. (2013) interpreted the continuous space word representation by demonstrating that vector off-set can be used to derive scalar relationship amongst adjectives. Their approach provided relationship among all the adjectives independent of their semantic property. De Melo and Bansal (2013) used a pattern based approach to identify intensity relation among adjectives, but their approach had a severe coverage problem. They also did not consider the semantic property of adjectives, assuming one single intensity-scale for all adjectives. Ruppenhofer et al. (2014) provided ordering among polar adjectives that bear the same semantic property. Their approach was completely corpus dependent, it was not able to derive intensity of those adjectives which were not found in the corpus. We have used the same star-rated movie review corpus in our work as used by Ruppenhofer et al. (2014) and found 122 polar adjectives which are absent from the corpus. Our system is able to identify intensity levels for these missing adjectives. Moreover, we obtained an improvement of 4% in overall accuracy. 8 Conclusion In this paper, we have proposed an approach that assigns i</context>
</contexts>
<marker>Ruppenhofer, Wiegand, Brandes, 2014</marker>
<rawString>Josef Ruppenhofer, Michael Wiegand, and Jasper Brandes. 2014. Comparing methods for deriving intensity scores for adjectives. EACL 2014, 117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raksha Sharma</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Detecting domain dedicated polar words. IJCNP</title>
<date>2013</date>
<pages>661--666</pages>
<contexts>
<context position="5132" citStr="Sharma and Bhattacharyya (2013)" startWordPosition="828" endWordPosition="831">the WNPI formula is as follows: WNPI(word) = 5 E2 1 i ∗ Ci (1) 5 ∗ Ei=1 Ci where Ci is the count of the word in i-star reviews. 2.2 Need Significant Occurrence of A Word The WNPI formula gives a corpus based result, hence can give biased scores for words which occur less frequently in the corpus. For example, in our movie review data-set, the word substandard occurs only 3 times in the corpus, and these occurrences happen to be in 1-star and 2-star reviews only. Hence, the WNPI formula assigns a higher score to substandard. To avoid such a bias, we integrate WNPI formula with Chi-Square test. Sharma and Bhattacharyya (2013) used ChiSquare test to find significant polar words in a domain. We use the same categorical Chi-Square test in our work. 2.3 How to Get Intensity Clue for All Words? A combination of WNPI formula and ChiSquare test cannot assign intensity scores to adjectives, which are not present in the corpus. To overcome this data sparsity problem, we restrict the use of WNPI formula to identify the most intense word in each category. We explore precomputed context vectors of words, presented by Mikolov et al. (2011) (Section 3), to assign intensity levels to remaining words of the semantic category: Cas</context>
</contexts>
<marker>Sharma, Bhattacharyya, 2013</marker>
<rawString>Raksha Sharma and Pushpak Bhattacharyya. 2013. Detecting domain dedicated polar words. IJCNP 2013, pages 661–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts.</title>
<date>2014</date>
<pages>50</pages>
<contexts>
<context position="13232" citStr="Kiritchenko and Mohammad, 2014" startWordPosition="2149" endWordPosition="2152">rk as used by Ruppenhofer et al. (2014). 73% across 52 polar categories, which is significantly lesser than the accuracy obtained with our approach. MeanStar approach does not assign intensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach. Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mohammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Chernova, 2013). We implemented three systems to evaluate the significance of intensity annotated adjectives in star rating prediction task. System 1: A rule based system based on the concept that negatively high intense words will occur more frequently in the low star reviews and positively high intense words will occur more frequently in the high star reviews. This system uses the following function I to assign intensity score to a review r: I (r) = E3i= 1 i 3 CPiP �</context>
</contexts>
<marker>Kiritchenko, Mohammad, 2014</marker>
<rawString>Xiaodan Zhu Svetlana Kiritchenko and Saif M. Mohammad. 2014. Sentiment analysis of short informal texts. 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Jack Grieve</author>
</authors>
<title>Analyzing appraisal automatically.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI Spring Symposium on Exploring Attitude and Affect in Text (AAAI Technical Re# port SS# 04# 07),</booktitle>
<pages>158--161</pages>
<publisher>AAAI Press.</publisher>
<location>Stanford University, CA,</location>
<contexts>
<context position="1649" citStr="Taboada and Grieve, 2004" startWordPosition="249" endWordPosition="252"> intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of words to judge the intensity of a sentence. Words that bear the same semantic property can be used interchangeably to upgrade or downgrade the intensity of the expression. For example, good and outstanding both are positive words from the QUALITY category, but the latter can be used to intensify positive expression in a sentence. There are several manually or automatically created lexical resources (Liu, 2010; Wilson et al., 2005b; Wilson et al., 2005a; Taboada and Grieve, 2004) that assign a fixed positive (+1) or negative (−1) polarity to words, making no distinction among them in terms of their intensity. This paper presents a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low, which share the same semantic property. We have used the semantic frames of FrameNet1.5 (Baker et al., 1998) to obtain these semantic categories. Our approach is based on the idea that the most intense word has higher contextual similarity with high intensity words than with medium or low intensity words. We use the intensity annotated movie review </context>
</contexts>
<marker>Taboada, Grieve, 2004</marker>
<rawString>Maite Taboada and Jack Grieve. 2004. Analyzing appraisal automatically. In Proceedings of AAAI Spring Symposium on Exploring Attitude and Affect in Text (AAAI Technical Re# port SS# 04# 07), Stanford University, CA, pp. 158q161. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>417--424</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13128" citStr="Turney, 2002" startWordPosition="2133" endWordPosition="2134">ed the same semantic categories and intensity annotated movie review corpus in our work as used by Ruppenhofer et al. (2014). 73% across 52 polar categories, which is significantly lesser than the accuracy obtained with our approach. MeanStar approach does not assign intensity score to words missing from the corpus. While, 88 out of 122 missing words are assigned correct intensity levels by our approach. Figure 3: Accuracy obtained with MeanStar and our approach 6.3 Evaluation Using Star Rating Prediction There have been several successful attempts at sentiment polarity detection in the past (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Mohammad et al., 2013; Svetlana Kiritchenko and Mohammad, 2014). However, prediction of star ratings still considered as a challenging task (Qu et al., 2010; Gupta et al., 2010; Boteanu and Chernova, 2013). We implemented three systems to evaluate the significance of intensity annotated adjectives in star rating prediction task. System 1: A rule based system based on the concept that negatively high intense words will occur more frequently in the low star reviews and positively high intense words will occur more frequently in the high star reviews. This</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 417–424. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In AAAI/IAAI.</booktitle>
<contexts>
<context position="15496" citStr="Wiebe, 2000" startWordPosition="2541" endWordPosition="2542">0.46 59.21 0.54 0.45 Table 3: Comparison of rating prediction systems, where MSE is the Mean Squared Error and MAE is the Mean Absolute Error Table 3 shows the results obtained with the above systems. System 3(d) achieves the maximum accuracy depicting that inclusion of intensity information with the standard features improves the star rating prediction significantly. 7 Related Work Sentiment analysis on adjectives has been extensively explored in NLP literature. However, most of the works addressed the problem of finding polarity orientation of adjectives (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Fahrni and Klenner, 2008; Dragut et al., 2010). The first work in the direction of adjectival scale was done by Hatzivassiloglou and McKeown (1993). They exploited linguistic knowledge available in the corpora to compute similarity between adjectives. However, their approach did not consider polarity orientation of adjectives, they provided ordering among non-polar adjectives like, cold, lukewarm, warm, hot. 5We use 80% of the star-rated movie review corpus as training data and 20% as test data. The results reported in table 3 are based on the 20% test data. The task of ordering adjectives a</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In AAAI/IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of hlt/emnlp on interactive demonstrations. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1599" citStr="Wilson et al., 2005" startWordPosition="241" endWordPosition="244">movie review corpus. 1 Introduction Sentence intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of words to judge the intensity of a sentence. Words that bear the same semantic property can be used interchangeably to upgrade or downgrade the intensity of the expression. For example, good and outstanding both are positive words from the QUALITY category, but the latter can be used to intensify positive expression in a sentence. There are several manually or automatically created lexical resources (Liu, 2010; Wilson et al., 2005b; Wilson et al., 2005a; Taboada and Grieve, 2004) that assign a fixed positive (+1) or negative (−1) polarity to words, making no distinction among them in terms of their intensity. This paper presents a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low, which share the same semantic property. We have used the semantic frames of FrameNet1.5 (Baker et al., 1998) to obtain these semantic categories. Our approach is based on the idea that the most intense word has higher contextual similarity with high intensity words than with medium or low intensity w</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinionfinder: A system for subjectivity analysis. In Proceedings of hlt/emnlp on interactive demonstrations. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on human language technology and empirical methods in natural language processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1599" citStr="Wilson et al., 2005" startWordPosition="241" endWordPosition="244">movie review corpus. 1 Introduction Sentence intensity becomes crucial when we need to compare sentences having the same polarity orientation. In such scenarios, we can use intensity of words to judge the intensity of a sentence. Words that bear the same semantic property can be used interchangeably to upgrade or downgrade the intensity of the expression. For example, good and outstanding both are positive words from the QUALITY category, but the latter can be used to intensify positive expression in a sentence. There are several manually or automatically created lexical resources (Liu, 2010; Wilson et al., 2005b; Wilson et al., 2005a; Taboada and Grieve, 2004) that assign a fixed positive (+1) or negative (−1) polarity to words, making no distinction among them in terms of their intensity. This paper presents a semi-supervised approach to assign intensity levels to adjectives, viz. high, medium and low, which share the same semantic property. We have used the semantic frames of FrameNet1.5 (Baker et al., 1998) to obtain these semantic categories. Our approach is based on the idea that the most intense word has higher contextual similarity with high intensity words than with medium or low intensity w</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on human language technology and empirical methods in natural language processing. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
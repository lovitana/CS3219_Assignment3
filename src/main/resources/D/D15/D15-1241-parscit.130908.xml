<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014115">
<title confidence="0.985139">
Knowledge Base Inference using Bridging Entities
</title>
<author confidence="0.982506">
Bhushan Kotnis Pradeep Bansal Partha Talukdar
</author>
<affiliation confidence="0.993083">
Indian Institute of Science Indian Institute of Science Indian Institute of Science
</affiliation>
<email confidence="0.975562">
bkotnis@dese.iisc.ernet.in pradeepb@ee.iisc.ernet.in ppt@serc.iisc.in
</email>
<sectionHeader confidence="0.993275" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989034">
Large-scale Knowledge Bases (such as
NELL, Yago, Freebase, etc.) are often
sparse, i.e., a large number of valid rela-
tions between existing entities are missing.
Recent research have addressed this prob-
lem by augmenting the KB graph with ad-
ditional edges mined from a large text cor-
pus while keeping the set of nodes fixed,
and then using the Path Ranking Algo-
rithm (PRA) to perform KB inference over
this augmented graph. In this paper, we
extend this line of work by augmenting
the KB graph not only with edges, but
also with bridging entities, where both the
edges and bridging entities are mined from
a 500 million web text corpus. Through
experiments on real-world datasets, we
demonstrate the value of bridging entities
in improving the performance and running
time of PRA in the KB inference task.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98259258">
Large-scale knowledge bases (KB) like Freebase
(Bollacker et al., 2008), Yago (Suchanek et al.,
2007), NELL (Mitchell et al., 2015) can be use-
ful in a variety of applications like natural lan-
guage question answering, semantic search en-
gines, etc. These knowledge bases consist of mil-
lions of real world entities and relationships be-
tween them which are stored in the form of a di-
rected graph where links represent relations and
nodes represent the entities. Although such KBs
contain millions of entities, they are still very
sparse, i.e., they are missing a large number of
relations between existing entities (West et al.,
2014).
Performing inference over the knowledge graph
for predicting relations between two entities is one
way of densifying the KB graph. For example,
Figure 1: Example showing how addition of the
bridging entity, Brian McCain, and the two edges
incident on it can help the PRA algorithm (Lao and
Cohen, 2010) to infer the initially missing relation
instance teamPlaysSport(Yankees, BaseBall). The
original KB graph consisted only of two nodes,
Yankees and Baseball, and no edges.
from (Germany, playsinTournament, FIFA) and
(FIFA, tournamentofSport, Soccer), we can infer
(Germany, playsSport, Soccer). The Path Ranking
Algorithm (PRA) (Lao and Cohen, 2010), (Lao et
al., 2011) performs such an inference by learning
inference rules over the knowledge graph.
If the knowledge graph is sparse, i.e., if there
are a very few or no paths between source and
target entities, then PRA is unable to predict the
existence of a relation. To address this shortcom-
ing, (Lao et al., 2012) augmented the knowledge
graph with paths obtained from an external corpus.
The added paths consisted of unlexicalized depen-
dency labels obtained from a dependency parsed
external corpus. To improve the expressivity of
the added paths, instead of the unlexicalized la-
bels, (Gardner et al., 2013) augmented the KB
graph with verbs (surface relations) from a corpus
containing over 600 million Subject-Verb-Object
(SVO) triples. These verbs act as edges that con-
nect previously unconnected entities thereby in-
creasing the connectivity of the KB graph which
can potentially improve PRA performance.
However, naively adding these edges increases
the feature sparsity which degrades the discrim-
inative ability of the logistic regression classifier
</bodyText>
<page confidence="0.938817">
2038
</page>
<note confidence="0.6481705">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2038–2043,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99993356097561">
used in PRA. This can be addressed by adding la-
tent relations obtained by clustering the surface re-
lations, instead of directly adding the surface rela-
tions. This reduces feature sparsity and has been
shown to improve PRA inference (Gardner et al.,
2013) , (Gardner et al., 2014).
In this article we propose a scheme for aug-
menting the KB using paths obtained by mining
noun phrases that connect two SVO triples from
an external corpus. We term these noun phrases
as bridging entities since they bridge two KB re-
lations to form a path. This is different from the
scheme in (Gardner et al., 2013) and (Gardner et
al., 2014), which adds edges between KB nodes
by mining surface relations from an external cor-
pus. We search for such bridging entities in the
corpus by performing a limited depth DFS (depth
first search) on the corpus graph in an on-demand
fashion.
We term this procedure as On-Demand Aug-
mentation (ODA), because the search can be per-
formed during test time in an on-demand man-
ner. In contrast, the previous approaches of adding
edges or embeddings to the KB (Gardner et al.,
2013), and vector space random walk PRA (Gard-
ner et al., 2014) are batch procedures. As we shall
see in Section 4, due to a limited search space,
on-demand augmentation is much faster compared
to algorithms in (Gardner et al., 2013; Gardner
et al., 2014). Furthermore, since edges are not
added blindly, on-demand augmentation does not
increase feature sparsity which is responsible for
performance degradation. Our experiments sug-
gest that ODA provides better performance than
(Gardner et al., 2013) and nearly the same pre-
diction performance as provided by (Gardner et
al., 2014), but in both cases with the added ad-
vantage of faster running time and greater flex-
ibility due to its online and on-demand nature.
The code along with the results can be obtained
at https://github.com/malllabiisc/pra-oda.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999958714285714">
Using surface level relations and noun phrases
for extracting meaningful relational facts is not a
new idea (Hearst, 1992),(Brin, 1999), (Etzioni et
al., 2004). However, none of them make use of
Knowledge Bases for improving information ex-
traction.
The Path Ranking Algorithm (PRA) first pro-
posed in (Lao and Cohen, 2010) was used for per-
forming inference over a KB in (Lao et al., 2011).
It was extended by (Lao et al., 2012), to improve
the inference by augmenting the KB with syntactic
information obtained from a dependency parsed
corpus. Augmenting the KB for improving PRA
inference using surface relations mined from an
external corpus and using latent edge labels ob-
tained by performing PCA on the surface relations
was explored in (Gardner et al., 2013). Instead
of hard mapping of surface relations to latent em-
beddings, (Gardner et al., 2014) perform a ‘soft’
mapping using vector space random walks. This
allows the random walker to traverse an edge se-
mantically similar to the current edge type more
frequently than other edges.
Although, like others, we too use an external
corpus to augment the KB, the crucial difference
in our approach is that apart from adding surface
relations, we also add bridging entities that enable
us to create new paths in the KB. Furthermore, the
procedure is targeted so that only paths that play
a part in inferring the relations that are of interest
are added. Thus, the number of paths added in this
manner is much lower than the number of surface
relations added using the procedure in (Gardner et
al., 2013). As we shall see in Section 4, this results
in a more effective algorithm with faster runtime.
</bodyText>
<sectionHeader confidence="0.997074" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.961501">
3.1 Background: Path Ranking Algorithm
</subsectionHeader>
<bodyText confidence="0.971421181818182">
(PRA)
We first present a brief overview of the Path Rank-
ing Algorithm (PRA) (Lao and Cohen, 2010). The
PRA uses paths as features for a logistic regres-
sion classifier which predicts if the given relation
exists between a pair of entities. For a given pair
of entities s and t, the path type connecting s to t
form the feature vector. A path types 7r is an or-
dered set of relations. Paths with the same ordered
relations but different intermediate or terminal en-
tities belong to the same path type. For example,
</bodyText>
<equation confidence="0.989647666666667">
−→ t1 and s2 v1−→ x2
v0 −→ t2 belong
v1
</equation>
<bodyText confidence="0.9929822">
to path type v0−→v1
−→. The value of a feature, is
taken to be P(s → t; 7r), where P(s → t; 7r) is
the probability of reaching t from s by traversing
paths of type 7r. PRA approximates these proba-
bilities by running a random walk (RW) on the KB
graph. Let F = {7r1, 7r2,..., 7rk} be the set of all
path types. For predicting the existence of relation
r between entities s and t, the logistic regression
classifier outputs a score which is a measure of the
</bodyText>
<equation confidence="0.954929">
s1 −→ v0 x1
</equation>
<page confidence="0.977988">
2039
</page>
<table confidence="0.99916375">
Query Candidate Answer Path added by PRA-ODA with bridging entity (in bold)
sportsteamPositionForSport(right handed baseball right handed pitcher playsfor ��Chicago Cubs play
pitcher, ?) �� baseball
riverFlowsThroughCity(Moselle, ?) Koblenz �� Rhine meet at
Moselle flows into
�� Koblenz
teamPlaysInLeague(Cleveland Indians, ?) MLB �� Detroit Tigers blew
Cleveland Indians play �� MLB
</table>
<tableCaption confidence="0.999876">
Table 1: Examples of paths involving bridging entities (marked in bold) added to the KB by PRA-ODA.
</tableCaption>
<bodyText confidence="0.982775">
confidence that r exists between s and t. It does
so by first assigning weights to the features in the
training phase. The score is given by
</bodyText>
<equation confidence="0.9477935">
�S(s, t, r) = P(s → t; 7r) × θrπ (1)
π∈F
</equation>
<bodyText confidence="0.999979833333333">
where θrπ is the weight learned by the logistic re-
gression classifier during training specially for re-
lation r and path type 7r. During the test phase,
since targets are not available, the PRA gathers
candidate targets by performing a random walk
and then computes feature vectors and the score.
</bodyText>
<subsectionHeader confidence="0.981729">
3.2 PRA-SVO and PRA-VS
</subsectionHeader>
<bodyText confidence="0.999888090909091">
PRA-SVO and PRA-VS are the systems proposed
in (Gardner et al., 2013) and (Gardner et al., 2014)
respectively, where the KB graph is augmented
with edges mined from a large subject-verb-object
(SVO) triple corpus. In these two systems, only
new edges are added over the fixed set of nodes,
and the augmentation happens in a batch, offline
setting. In contrast, PRA-ODA, the method pro-
posed in the paper, also expands the set of nodes
through bridging entities, and performs the aug-
mentation in an on-demand manner.
</bodyText>
<subsectionHeader confidence="0.865586">
3.3 PRA On-Demand Augmentation
</subsectionHeader>
<bodyText confidence="0.974408370967742">
(PRA-ODA)
Training: Let s and t be any two KB entities
and let s(n) and t(n) be their corresponding noun
phrase representations or aliases. We search for
bridging entities x1, x2, ..xn by performing lim-
ited depth first search (DFS) starting with sn such
that we obtain a path s A )Ss(n)−) x1 −→ v1
vn−1 −→ t(n) ALIAS
vn
... −→ xn −→ t, where vi are verbs
present in the corpus graph. This is done for all
n ≤ dmax − 1, where dmax is the maximum depth
of DFS. We add an ‘ALIAS’ edge between the KB
entity and its noun phase representation. The use-
fulness of bridging entities is illustrated in Fig. 1.
We mine bridging entities from a corpus con-
taining over 600 million SVO triples which were
obtained from the ClueWeb09 corpus (Callan et
al., 2009) parsed using the MALT parser (Nivre et
al., 2007). We use Mongo DB to store the triples
as an adjacency list. During training time, for any
relation that is being inferred, both the source and
its corresponding target entities are known. A lim-
ited depth DFS is performed for all depths less
then dmax on the SVO graph with the aliases of
subject entity acting as the starting points. Such
aliases are available for the NELL and Freebase
knowledge bases. The DFS is said to discover a
path if the terminating entity of the path matches
any alias of the target entity. We choose to use
aliases to perform string match, since it is easy to
change the softness of the match by simply adding
more aliases. This is done for all training source-
target pairs. A few examples of added paths are
shown in Table 1.
The SVO graph is noisy since it is obtained by
parsing the ClueWeb corpus which was obtained
by scraping the web. To reduce noise, we add the
top K most frequent discovered SVO path types,
where K is a tunable parameter. By SVO path type
we refer to a set of ordered verbs mined from the
SVO corpus. There is a possibility that the bridg-
ing entities, extracted from the corpus, may be
present in the KB. If the bridging entity matches
any alias, then it is treated as an alias to an existing
KB entity. If not, then the bridging entity is added
to the KB as a new entity. To avoid overfitting
we add negative data to the training set. Further-
more, only high quality expressive bridging enti-
ties result in meaningful and discriminative paths.
Although the quality of bridging entities depend
on the corpus, low quality bridging entities can
be filtered out by adding negative training data.
Low quality bridging entities connect source tar-
get pairs from both positive and negative training
sets, and hence are eliminated by the sparse lo-
gistic regression classifier. The negative dataset is
generated using the closed world assumption by
performing a random walk.
After augmenting the KB, we run the training
phase of the PRA algorithm to obtain the feature
(path) weights computed by the logistic regression
</bodyText>
<page confidence="0.981504">
2040
</page>
<table confidence="0.9997264">
KB Relations PRA PRA- PRA- PRA-
SVO VS ODA
actorstarredinmovie 0.0 1.0 1.0 1.0
atheleteplaysforteam 1.0 1.0 1.0 1.0
citylocatedincountry 0.166 0.25 1.0 1.0
journalistwritesfor 1.0 1.0 1.0 1.0
publication 0.333 0.25 1.0 1.0
riverflowsthroughcity
sportsteamposition 1.0 1.0 1.0 1.0
forsport 1.0 1.0 1.0 1.0
stadiumlocatedincity
statehaslake 0.0 0.0 0.0 0.0
teamplaysinleague 1.0 1.0 1.0 1.0
writerwrotebook 1.0 1.0 1.0 1.0
Average (MRR) 0.649 0.75 0.9 0.9
</table>
<tableCaption confidence="0.998137">
Table 2: Comparison of Mean Reciprocal Rank
</tableCaption>
<bodyText confidence="0.968214956521739">
(MRR) metric for 10 relations from NELL (higher
is better). PRA-SVO, PRA-VS are the systems
proposed in (Gardner et al., 2013; Gardner et al.,
2014). PRA-ODA is the approach proposed in this
paper. Improvements in PRA-ODA over PRA-
SVO is statistically significant with p &lt; 0.007,
with PRA-SVO as null hypothesis.
classifier.
Query Time: The set of target entities corre-
sponding to a source entity and the relation be-
ing predicted is not available during query (test)
time. We use all the entities included in the range
of the relation being predicted as candidate target
entities. For example, if the relation is riverFlow-
sThroughCity, the candidate target set would in-
clude entities in the KB that are cities. The DFS
is now performed starting from source entities as
during training, but this time only restricting to
paths with positive weights learned during train-
ing. Any path (along with bridging entities) found
during this search are added to the KB, and the
PRA algorithm is now run over this augmented
graph.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999437333333333">
We used the implementation of PRA provided by
the authors of (Gardner et al., 2014). For our ex-
periments, we used the same 10 NELL relation
data as used in (Gardner et al., 2014). The aug-
mentation resulted in the addition of 1086 paths
during training and 1430 paths during test time.
We split the NELL data into 60% training
data, 15 % development data and 25% test
data. Values for dma,, and K, the most fre-
quent paths, were obtained by tuning on a
development set for 4 relations (athleteplaysfor-
sport,actorstarredinmovie,citylocatedincountry
</bodyText>
<table confidence="0.999231125">
Timings (seconds) PRA PRA- PRA- PRA-
SVO VS ODA
Training 635.6 574.5 564.2 913.3
Test 354.3 322.0 301.2 436.7
Batch augmentation n/a 797 797 n/a
Embedding compu- n/a n/a 812 n/a
tation
Total Time 989.9 1693.5 2474.4 1350
</table>
<tableCaption confidence="0.997102">
Table 3: Runtime comparison for the entire ex-
</tableCaption>
<bodyText confidence="0.992895142857143">
periment (lower is better). PRA-SVO, PRA-VS
are the systems proposed in (Gardner et al., 2013;
Gardner et al., 2014). PRA-ODA is the approach
proposed in this paper. Between the two top per-
forming systems, i.e., PRA-ODA and PRA-VS,
PRA-ODA is faster by a factor of 1.8.
and journalistwritesforpublication). The hyper-
parameter values dma, = 2, K = 10 reported
the highest MRR and were used for the rest of
the relations. For the L1 and L2 regularization
parameters in the logistic regression classifier, we
used the same values as used in (Gardner et al.,
2013; Gardner et al., 2014), viz., L1 = 0.005, and
L2 = 1.0. This is because the parameters were
reported to be robust, and seemed to work well
even when the knowledge base was augmented.
We compare the results (PRA-ODA) with the
PRA algorithm executed on the NELL KB, NELL
KB augmented with surface relations (PRA-SVO)
(Gardner et al., 2013) and vector space random
walk PRA (PRA-VS) (Gardner et al., 2014). The
run times, i.e, the time taken to perform an entire
experiment for PRA-SVO and PRA-VS includes
the time taken to augment NELL KB with SVO
edges. The PRA-VS runtime also includes the
time taken for generating embeddings to perform
the vector space random walk. As can be seen
from Table 2 and Table 3, our scheme, PRA-ODA,
provides performance equivalent to PRA-VS with
faster running time (speed up of 1.8). In addition
to the time taken for the full SVO augmentation,
PRA-VS takes additional time to generate embed-
dings (13 minutes) from the added verbs. We note
that the batch augmentation in case of PRA-SVO
and PRA-VS, and embedding computation in case
of PRA-VS are all specific to the relations in the
evaluation set, and hence can’t be ignored as a
one-time offline cost. In other words, these costs
are likely to increase as more relations (and their
instances) are included during training and test-
ing. Runtime gains with PRA-ODA are likely to
be even more pronounced in such settings.
</bodyText>
<page confidence="0.971215">
2041
</page>
<bodyText confidence="0.9641955">
An additional advantage of the proposed algo-
rithm is that it can also be run on the top of any
PRA based algorithm such as the PRA-SVO and
PRA-VS.
inference in a large knowledge-base using latent
syntactic cues. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2013, 18-21 October 2013,
Seattle, Washington, USA, A meeting of SIGDAT, a
Special Interest Group of the ACL, pages 833–838.
</bodyText>
<sectionHeader confidence="0.963128" genericHeader="evaluation">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978333333333">
In this paper, we investigated the usefulness of
adding paths to a Knowledge Base for improving
its connectivity by mining bridging entities from
an external corpus. While previous KB augmen-
tation methods focused only on augmentation us-
ing mined surface verbs while keeping the node
set fixed, we extended these approaches by also
adding bridging entities in an online fashion. We
used a large corpus of 500 million web text corpus
to mine these additional edges and bridging enti-
ties. Through experiments on real-world datasets,
we demonstrate that the proposed approach is not
only comparable or better than other state-of-the-
art baselines, but more importantly provides faster
overall runtime compared with the alternatives.
</bodyText>
<sectionHeader confidence="0.945642" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9937735">
This work is supported in part by a gift from
Google.
</bodyText>
<sectionHeader confidence="0.997309" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997591626666667">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A
collaboratively created graph database for structur-
ing human knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Man-
agement of Data, SIGMOD ’08, pages 1247–1250,
New York, NY, USA. ACM.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In Paolo Atzeni, Alberto
Mendelzon, and Giansalvatore Mecca, editors, The
World Wide Web and Databases, volume 1590 of
Lecture Notes in Computer Science, pages 172–183.
Springer Berlin Heidelberg.
J. Callan, M. Hoy, C. Yoo, and L. Zhao. 2009.
Clueweb09 data set. boston.lti.cs.cmu.edu.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in know-
itall: (preliminary results). In Proceedings of the
13th International Conference on World Wide Web,
WWW ’04, pages 100–110, New York, NY, USA.
ACM.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and
Matt Gardner, Partha Pratim Talukdar, Jayant Krish-
namurthy, and Tom Mitchell. 2014. Incorporat-
ing vector space similarity in random walk inference
over knowledge bases. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 397–406.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th Conference on Computational Linguistics
- Volume 2, COLING ’92, pages 539–545, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ni Lao and William W. Cohen. 2010. Relational re-
trieval using a combination ofpath constrained ran-
dom walks. Machine Learning, 81(1):53–67.
Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ’11, pages 529–539, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W. Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 1017–1026, Stroudsburg, PA,
USA. Association for Computational Linguistics.
T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Bet-
teridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel,
J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed,
N. Nakashole, E. Platanios, A. Ritter, M. Samadi,
B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen,
A. Saparov, M. Greaves, and J. Welling. 2015.
Never-ending learning. In Proceedings of AAAI.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(02):95–135.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge. In Proceedings of the 16th International Con-
ference on World Wide Web, WWW ’07, pages 697–
706, New York, NY, USA. ACM.
</reference>
<page confidence="0.861626">
2042
</page>
<reference confidence="0.999419666666667">
Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In Proceedings of the 23rd Interna-
tional Conference on World Wide Web, WWW ’14,
pages 515–526, New York, NY, USA. ACM.
</reference>
<page confidence="0.980604">
2043
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336487">
<title confidence="0.999764">Knowledge Base Inference using Bridging Entities</title>
<author confidence="0.906196">Bhushan Kotnis Pradeep Bansal Partha Talukdar</author>
<affiliation confidence="0.962252">Indian Institute of Science Indian Institute of Science Indian Institute of Science</affiliation>
<abstract confidence="0.971318318181818">bkotnis@dese.iisc.ernet.in pradeepb@ee.iisc.ernet.in ppt@serc.iisc.in Abstract Large-scale Knowledge Bases (such as NELL, Yago, Freebase, etc.) are often sparse, i.e., a large number of valid relations between existing entities are missing. Recent research have addressed this problem by augmenting the KB graph with additional edges mined from a large text corpus while keeping the set of nodes fixed, and then using the Path Ranking Algorithm (PRA) to perform KB inference over this augmented graph. In this paper, we extend this line of work by augmenting the KB graph not only with edges, but with where both the edges and bridging entities are mined from a 500 million web text corpus. Through experiments on real-world datasets, we demonstrate the value of bridging entities in improving the performance and running time of PRA in the KB inference task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1149" citStr="Bollacker et al., 2008" startWordPosition="173" endWordPosition="176">large text corpus while keeping the set of nodes fixed, and then using the Path Ranking Algorithm (PRA) to perform KB inference over this augmented graph. In this paper, we extend this line of work by augmenting the KB graph not only with edges, but also with bridging entities, where both the edges and bridging entities are mined from a 500 million web text corpus. Through experiments on real-world datasets, we demonstrate the value of bridging entities in improving the performance and running time of PRA in the KB inference task. 1 Introduction Large-scale knowledge bases (KB) like Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), NELL (Mitchell et al., 2015) can be useful in a variety of applications like natural language question answering, semantic search engines, etc. These knowledge bases consist of millions of real world entities and relationships between them which are stored in the form of a directed graph where links represent relations and nodes represent the entities. Although such KBs contain millions of entities, they are still very sparse, i.e., they are missing a large number of relations between existing entities (West et al., 2014). Performing inference over the knowledge</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD ’08, pages 1247–1250, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting patterns and relations from the world wide web.</title>
<date>1999</date>
<booktitle>The World Wide Web and Databases,</booktitle>
<volume>1590</volume>
<pages>172--183</pages>
<editor>In Paolo Atzeni, Alberto Mendelzon, and Giansalvatore Mecca, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="5667" citStr="Brin, 1999" startWordPosition="900" endWordPosition="901">e feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by (Gardner et al., 2014), but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. 2 Related Work Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992),(Brin, 1999), (Etzioni et al., 2004). However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011). It was extended by (Lao et al., 2012), to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explore</context>
</contexts>
<marker>Brin, 1999</marker>
<rawString>Sergey Brin. 1999. Extracting patterns and relations from the world wide web. In Paolo Atzeni, Alberto Mendelzon, and Giansalvatore Mecca, editors, The World Wide Web and Databases, volume 1590 of Lecture Notes in Computer Science, pages 172–183. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Callan</author>
<author>M Hoy</author>
<author>C Yoo</author>
<author>L Zhao</author>
</authors>
<date>2009</date>
<note>Clueweb09 data set. boston.lti.cs.cmu.edu.</note>
<contexts>
<context position="10537" citStr="Callan et al., 2009" startWordPosition="1758" endWordPosition="1761">or aliases. We search for bridging entities x1, x2, ..xn by performing limited depth first search (DFS) starting with sn such that we obtain a path s A )Ss(n)−) x1 −→ v1 vn−1 −→ t(n) ALIAS vn ... −→ xn −→ t, where vi are verbs present in the corpus graph. This is done for all n ≤ dmax − 1, where dmax is the maximum depth of DFS. We add an ‘ALIAS’ edge between the KB entity and its noun phase representation. The usefulness of bridging entities is illustrated in Fig. 1. We mine bridging entities from a corpus containing over 600 million SVO triples which were obtained from the ClueWeb09 corpus (Callan et al., 2009) parsed using the MALT parser (Nivre et al., 2007). We use Mongo DB to store the triples as an adjacency list. During training time, for any relation that is being inferred, both the source and its corresponding target entities are known. A limited depth DFS is performed for all depths less then dmax on the SVO graph with the aliases of subject entity acting as the starting points. Such aliases are available for the NELL and Freebase knowledge bases. The DFS is said to discover a path if the terminating entity of the path matches any alias of the target entity. We choose to use aliases to perf</context>
</contexts>
<marker>Callan, Hoy, Yoo, Zhao, 2009</marker>
<rawString>J. Callan, M. Hoy, C. Yoo, and L. Zhao. 2009. Clueweb09 data set. boston.lti.cs.cmu.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Stanley Kok</author>
<author>Ana-Maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Web-scale information extraction in knowitall: (preliminary results).</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th International Conference on World Wide Web, WWW ’04,</booktitle>
<pages>100--110</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5691" citStr="Etzioni et al., 2004" startWordPosition="902" endWordPosition="905">sity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by (Gardner et al., 2014), but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. 2 Related Work Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992),(Brin, 1999), (Etzioni et al., 2004). However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011). It was extended by (Lao et al., 2012), to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 20</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2004</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2004. Web-scale information extraction in knowitall: (preliminary results). In Proceedings of the 13th International Conference on World Wide Web, WWW ’04, pages 100–110, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
</authors>
<title>Partha Pratim Talukdar,</title>
<date>2013</date>
<location>Bryan</location>
<marker>Gardner, 2013</marker>
<rawString>Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom Mitchell. 2013. Improving learning and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
<author>Partha Pratim Talukdar</author>
<author>Jayant Krishnamurthy</author>
<author>Tom Mitchell</author>
</authors>
<title>Incorporating vector space similarity in random walk inference over knowledge bases.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>397--406</pages>
<location>Doha,</location>
<contexts>
<context position="3903" citStr="Gardner et al., 2014" startWordPosition="603" endWordPosition="606">er, naively adding these edges increases the feature sparsity which degrades the discriminative ability of the logistic regression classifier 2038 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2038–2043, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. used in PRA. This can be addressed by adding latent relations obtained by clustering the surface relations, instead of directly adding the surface relations. This reduces feature sparsity and has been shown to improve PRA inference (Gardner et al., 2013) , (Gardner et al., 2014). In this article we propose a scheme for augmenting the KB using paths obtained by mining noun phrases that connect two SVO triples from an external corpus. We term these noun phrases as bridging entities since they bridge two KB relations to form a path. This is different from the scheme in (Gardner et al., 2013) and (Gardner et al., 2014), which adds edges between KB nodes by mining surface relations from an external corpus. We search for such bridging entities in the corpus by performing a limited depth DFS (depth first search) on the corpus graph in an on-demand fashion. We term this proc</context>
<context position="5294" citStr="Gardner et al., 2014" startWordPosition="840" endWordPosition="843">r embeddings to the KB (Gardner et al., 2013), and vector space random walk PRA (Gardner et al., 2014) are batch procedures. As we shall see in Section 4, due to a limited search space, on-demand augmentation is much faster compared to algorithms in (Gardner et al., 2013; Gardner et al., 2014). Furthermore, since edges are not added blindly, on-demand augmentation does not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by (Gardner et al., 2014), but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. 2 Related Work Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992),(Brin, 1999), (Etzioni et al., 2004). However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a</context>
<context position="9339" citStr="Gardner et al., 2014" startWordPosition="1543" endWordPosition="1546">d to the KB by PRA-ODA. confidence that r exists between s and t. It does so by first assigning weights to the features in the training phase. The score is given by �S(s, t, r) = P(s → t; 7r) × θrπ (1) π∈F where θrπ is the weight learned by the logistic regression classifier during training specially for relation r and path type 7r. During the test phase, since targets are not available, the PRA gathers candidate targets by performing a random walk and then computes feature vectors and the score. 3.2 PRA-SVO and PRA-VS PRA-SVO and PRA-VS are the systems proposed in (Gardner et al., 2013) and (Gardner et al., 2014) respectively, where the KB graph is augmented with edges mined from a large subject-verb-object (SVO) triple corpus. In these two systems, only new edges are added over the fixed set of nodes, and the augmentation happens in a batch, offline setting. In contrast, PRA-ODA, the method proposed in the paper, also expands the set of nodes through bridging entities, and performs the augmentation in an on-demand manner. 3.3 PRA On-Demand Augmentation (PRA-ODA) Training: Let s and t be any two KB entities and let s(n) and t(n) be their corresponding noun phrase representations or aliases. We search </context>
<context position="13298" citStr="Gardner et al., 2014" startWordPosition="2231" endWordPosition="2234">RA- PRASVO VS ODA actorstarredinmovie 0.0 1.0 1.0 1.0 atheleteplaysforteam 1.0 1.0 1.0 1.0 citylocatedincountry 0.166 0.25 1.0 1.0 journalistwritesfor 1.0 1.0 1.0 1.0 publication 0.333 0.25 1.0 1.0 riverflowsthroughcity sportsteamposition 1.0 1.0 1.0 1.0 forsport 1.0 1.0 1.0 1.0 stadiumlocatedincity statehaslake 0.0 0.0 0.0 0.0 teamplaysinleague 1.0 1.0 1.0 1.0 writerwrotebook 1.0 1.0 1.0 1.0 Average (MRR) 0.649 0.75 0.9 0.9 Table 2: Comparison of Mean Reciprocal Rank (MRR) metric for 10 relations from NELL (higher is better). PRA-SVO, PRA-VS are the systems proposed in (Gardner et al., 2013; Gardner et al., 2014). PRA-ODA is the approach proposed in this paper. Improvements in PRA-ODA over PRASVO is statistically significant with p &lt; 0.007, with PRA-SVO as null hypothesis. classifier. Query Time: The set of target entities corresponding to a source entity and the relation being predicted is not available during query (test) time. We use all the entities included in the range of the relation being predicted as candidate target entities. For example, if the relation is riverFlowsThroughCity, the candidate target set would include entities in the KB that are cities. The DFS is now performed starting from</context>
<context position="15108" citStr="Gardner et al., 2014" startWordPosition="2535" endWordPosition="2538">ta into 60% training data, 15 % development data and 25% test data. Values for dma,, and K, the most frequent paths, were obtained by tuning on a development set for 4 relations (athleteplaysforsport,actorstarredinmovie,citylocatedincountry Timings (seconds) PRA PRA- PRA- PRASVO VS ODA Training 635.6 574.5 564.2 913.3 Test 354.3 322.0 301.2 436.7 Batch augmentation n/a 797 797 n/a Embedding compu- n/a n/a 812 n/a tation Total Time 989.9 1693.5 2474.4 1350 Table 3: Runtime comparison for the entire experiment (lower is better). PRA-SVO, PRA-VS are the systems proposed in (Gardner et al., 2013; Gardner et al., 2014). PRA-ODA is the approach proposed in this paper. Between the two top performing systems, i.e., PRA-ODA and PRA-VS, PRA-ODA is faster by a factor of 1.8. and journalistwritesforpublication). The hyperparameter values dma, = 2, K = 10 reported the highest MRR and were used for the rest of the relations. For the L1 and L2 regularization parameters in the logistic regression classifier, we used the same values as used in (Gardner et al., 2013; Gardner et al., 2014), viz., L1 = 0.005, and L2 = 1.0. This is because the parameters were reported to be robust, and seemed to work well even when the kno</context>
</contexts>
<marker>Gardner, Talukdar, Krishnamurthy, Mitchell, 2014</marker>
<rawString>Matt Gardner, Partha Pratim Talukdar, Jayant Krishnamurthy, and Tom Mitchell. 2014. Incorporating vector space similarity in random walk inference over knowledge bases. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 397–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th Conference on Computational Linguistics - Volume 2, COLING ’92,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5654" citStr="Hearst, 1992" startWordPosition="899" endWordPosition="900">oes not increase feature sparsity which is responsible for performance degradation. Our experiments suggest that ODA provides better performance than (Gardner et al., 2013) and nearly the same prediction performance as provided by (Gardner et al., 2014), but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. 2 Related Work Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992),(Brin, 1999), (Etzioni et al., 2004). However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011). It was extended by (Lao et al., 2012), to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relation</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th Conference on Computational Linguistics - Volume 2, COLING ’92, pages 539–545, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Relational retrieval using a combination ofpath constrained random walks.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<volume>81</volume>
<issue>1</issue>
<contexts>
<context position="2012" citStr="Lao and Cohen, 2010" startWordPosition="317" endWordPosition="320">d relationships between them which are stored in the form of a directed graph where links represent relations and nodes represent the entities. Although such KBs contain millions of entities, they are still very sparse, i.e., they are missing a large number of relations between existing entities (West et al., 2014). Performing inference over the knowledge graph for predicting relations between two entities is one way of densifying the KB graph. For example, Figure 1: Example showing how addition of the bridging entity, Brian McCain, and the two edges incident on it can help the PRA algorithm (Lao and Cohen, 2010) to infer the initially missing relation instance teamPlaysSport(Yankees, BaseBall). The original KB graph consisted only of two nodes, Yankees and Baseball, and no edges. from (Germany, playsinTournament, FIFA) and (FIFA, tournamentofSport, Soccer), we can infer (Germany, playsSport, Soccer). The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), (Lao et al., 2011) performs such an inference by learning inference rules over the knowledge graph. If the knowledge graph is sparse, i.e., if there are a very few or no paths between source and target entities, then PRA is unable to predict the exi</context>
<context position="5853" citStr="Lao and Cohen, 2010" startWordPosition="929" endWordPosition="932"> prediction performance as provided by (Gardner et al., 2014), but in both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. 2 Related Work Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992),(Brin, 1999), (Etzioni et al., 2004). However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011). It was extended by (Lao et al., 2012), to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013). Instead of hard mapping of surface relations to latent embeddings, (Gardner et al., 2014) perform a ‘soft’ mapping using vector space random walks. This allo</context>
<context position="7330" citStr="Lao and Cohen, 2010" startWordPosition="1183" endWordPosition="1186"> surface relations, we also add bridging entities that enable us to create new paths in the KB. Furthermore, the procedure is targeted so that only paths that play a part in inferring the relations that are of interest are added. Thus, the number of paths added in this manner is much lower than the number of surface relations added using the procedure in (Gardner et al., 2013). As we shall see in Section 4, this results in a more effective algorithm with faster runtime. 3 Method 3.1 Background: Path Ranking Algorithm (PRA) We first present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010). The PRA uses paths as features for a logistic regression classifier which predicts if the given relation exists between a pair of entities. For a given pair of entities s and t, the path type connecting s to t form the feature vector. A path types 7r is an ordered set of relations. Paths with the same ordered relations but different intermediate or terminal entities belong to the same path type. For example, −→ t1 and s2 v1−→ x2 v0 −→ t2 belong v1 to path type v0−→v1 −→. The value of a feature, is taken to be P(s → t; 7r), where P(s → t; 7r) is the probability of reaching t from s by travers</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W. Cohen. 2010. Relational retrieval using a combination ofpath constrained random walks. Machine Learning, 81(1):53–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Tom Mitchell</author>
<author>William W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>529--539</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2381" citStr="Lao et al., 2011" startWordPosition="368" endWordPosition="371">icting relations between two entities is one way of densifying the KB graph. For example, Figure 1: Example showing how addition of the bridging entity, Brian McCain, and the two edges incident on it can help the PRA algorithm (Lao and Cohen, 2010) to infer the initially missing relation instance teamPlaysSport(Yankees, BaseBall). The original KB graph consisted only of two nodes, Yankees and Baseball, and no edges. from (Germany, playsinTournament, FIFA) and (FIFA, tournamentofSport, Soccer), we can infer (Germany, playsSport, Soccer). The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), (Lao et al., 2011) performs such an inference by learning inference rules over the knowledge graph. If the knowledge graph is sparse, i.e., if there are a very few or no paths between source and target entities, then PRA is unable to predict the existence of a relation. To address this shortcoming, (Lao et al., 2012) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, (Gardner et al., 2013) augm</context>
<context position="5919" citStr="Lao et al., 2011" startWordPosition="943" endWordPosition="946"> both cases with the added advantage of faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. 2 Related Work Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992),(Brin, 1999), (Etzioni et al., 2004). However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011). It was extended by (Lao et al., 2012), to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013). Instead of hard mapping of surface relations to latent embeddings, (Gardner et al., 2014) perform a ‘soft’ mapping using vector space random walks. This allows the random walker to traverse an edge semantically similar to t</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>Ni Lao, Tom Mitchell, and William W. Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 529–539, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>William W Cohen</author>
</authors>
<title>Reading the web with learned syntactic-semantic inference rules.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12,</booktitle>
<pages>1017--1026</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2681" citStr="Lao et al., 2012" startWordPosition="421" endWordPosition="424">amPlaysSport(Yankees, BaseBall). The original KB graph consisted only of two nodes, Yankees and Baseball, and no edges. from (Germany, playsinTournament, FIFA) and (FIFA, tournamentofSport, Soccer), we can infer (Germany, playsSport, Soccer). The Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), (Lao et al., 2011) performs such an inference by learning inference rules over the knowledge graph. If the knowledge graph is sparse, i.e., if there are a very few or no paths between source and target entities, then PRA is unable to predict the existence of a relation. To address this shortcoming, (Lao et al., 2012) augmented the knowledge graph with paths obtained from an external corpus. The added paths consisted of unlexicalized dependency labels obtained from a dependency parsed external corpus. To improve the expressivity of the added paths, instead of the unlexicalized labels, (Gardner et al., 2013) augmented the KB graph with verbs (surface relations) from a corpus containing over 600 million Subject-Verb-Object (SVO) triples. These verbs act as edges that connect previously unconnected entities thereby increasing the connectivity of the KB graph which can potentially improve PRA performance. Howe</context>
<context position="5958" citStr="Lao et al., 2012" startWordPosition="951" endWordPosition="954"> faster running time and greater flexibility due to its online and on-demand nature. The code along with the results can be obtained at https://github.com/malllabiisc/pra-oda. 2 Related Work Using surface level relations and noun phrases for extracting meaningful relational facts is not a new idea (Hearst, 1992),(Brin, 1999), (Etzioni et al., 2004). However, none of them make use of Knowledge Bases for improving information extraction. The Path Ranking Algorithm (PRA) first proposed in (Lao and Cohen, 2010) was used for performing inference over a KB in (Lao et al., 2011). It was extended by (Lao et al., 2012), to improve the inference by augmenting the KB with syntactic information obtained from a dependency parsed corpus. Augmenting the KB for improving PRA inference using surface relations mined from an external corpus and using latent edge labels obtained by performing PCA on the surface relations was explored in (Gardner et al., 2013). Instead of hard mapping of surface relations to latent embeddings, (Gardner et al., 2014) perform a ‘soft’ mapping using vector space random walks. This allows the random walker to traverse an edge semantically similar to the current edge type more frequently th</context>
</contexts>
<marker>Lao, Subramanya, Pereira, Cohen, 2012</marker>
<rawString>Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W. Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12, pages 1017–1026, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
<author>W Cohen</author>
<author>E Hruschka</author>
<author>P Talukdar</author>
<author>J Betteridge</author>
<author>A Carlson</author>
<author>B Dalvi</author>
<author>M Gardner</author>
<author>B Kisiel</author>
<author>J Krishnamurthy</author>
<author>N Lao</author>
<author>K Mazaitis</author>
<author>T Mohamed</author>
<author>N Nakashole</author>
<author>E Platanios</author>
<author>A Ritter</author>
<author>M Samadi</author>
<author>B Settles</author>
<author>R Wang</author>
<author>D Wijaya</author>
<author>A Gupta</author>
<author>X Chen</author>
<author>A Saparov</author>
<author>M Greaves</author>
<author>J Welling</author>
</authors>
<title>Never-ending learning.</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="1209" citStr="Mitchell et al., 2015" startWordPosition="183" endWordPosition="186">hen using the Path Ranking Algorithm (PRA) to perform KB inference over this augmented graph. In this paper, we extend this line of work by augmenting the KB graph not only with edges, but also with bridging entities, where both the edges and bridging entities are mined from a 500 million web text corpus. Through experiments on real-world datasets, we demonstrate the value of bridging entities in improving the performance and running time of PRA in the KB inference task. 1 Introduction Large-scale knowledge bases (KB) like Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), NELL (Mitchell et al., 2015) can be useful in a variety of applications like natural language question answering, semantic search engines, etc. These knowledge bases consist of millions of real world entities and relationships between them which are stored in the form of a directed graph where links represent relations and nodes represent the entities. Although such KBs contain millions of entities, they are still very sparse, i.e., they are missing a large number of relations between existing entities (West et al., 2014). Performing inference over the knowledge graph for predicting relations between two entities is one </context>
</contexts>
<marker>Mitchell, Cohen, Hruschka, Talukdar, Betteridge, Carlson, Dalvi, Gardner, Kisiel, Krishnamurthy, Lao, Mazaitis, Mohamed, Nakashole, Platanios, Ritter, Samadi, Settles, Wang, Wijaya, Gupta, Chen, Saparov, Greaves, Welling, 2015</marker>
<rawString>T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and J. Welling. 2015. Never-ending learning. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<contexts>
<context position="10587" citStr="Nivre et al., 2007" startWordPosition="1767" endWordPosition="1770"> ..xn by performing limited depth first search (DFS) starting with sn such that we obtain a path s A )Ss(n)−) x1 −→ v1 vn−1 −→ t(n) ALIAS vn ... −→ xn −→ t, where vi are verbs present in the corpus graph. This is done for all n ≤ dmax − 1, where dmax is the maximum depth of DFS. We add an ‘ALIAS’ edge between the KB entity and its noun phase representation. The usefulness of bridging entities is illustrated in Fig. 1. We mine bridging entities from a corpus containing over 600 million SVO triples which were obtained from the ClueWeb09 corpus (Callan et al., 2009) parsed using the MALT parser (Nivre et al., 2007). We use Mongo DB to store the triples as an adjacency list. During training time, for any relation that is being inferred, both the source and its corresponding target entities are known. A limited depth DFS is performed for all depths less then dmax on the SVO graph with the aliases of subject entity acting as the starting points. Such aliases are available for the NELL and Freebase knowledge bases. The DFS is said to discover a path if the terminating entity of the path matches any alias of the target entity. We choose to use aliases to perform string match, since it is easy to change the s</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web, WWW ’07,</booktitle>
<pages>697--706</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1179" citStr="Suchanek et al., 2007" startWordPosition="178" endWordPosition="181"> the set of nodes fixed, and then using the Path Ranking Algorithm (PRA) to perform KB inference over this augmented graph. In this paper, we extend this line of work by augmenting the KB graph not only with edges, but also with bridging entities, where both the edges and bridging entities are mined from a 500 million web text corpus. Through experiments on real-world datasets, we demonstrate the value of bridging entities in improving the performance and running time of PRA in the KB inference task. 1 Introduction Large-scale knowledge bases (KB) like Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), NELL (Mitchell et al., 2015) can be useful in a variety of applications like natural language question answering, semantic search engines, etc. These knowledge bases consist of millions of real world entities and relationships between them which are stored in the form of a directed graph where links represent relations and nodes represent the entities. Although such KBs contain millions of entities, they are still very sparse, i.e., they are missing a large number of relations between existing entities (West et al., 2014). Performing inference over the knowledge graph for predicting relation</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowledge. In Proceedings of the 16th International Conference on World Wide Web, WWW ’07, pages 697– 706, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert West</author>
<author>Evgeniy Gabrilovich</author>
<author>Kevin Murphy</author>
<author>Shaohua Sun</author>
<author>Rahul Gupta</author>
<author>Dekang Lin</author>
</authors>
<title>Knowledge base completion via search-based question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd International Conference on World Wide Web, WWW ’14,</booktitle>
<pages>515--526</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1708" citStr="West et al., 2014" startWordPosition="267" endWordPosition="270">knowledge bases (KB) like Freebase (Bollacker et al., 2008), Yago (Suchanek et al., 2007), NELL (Mitchell et al., 2015) can be useful in a variety of applications like natural language question answering, semantic search engines, etc. These knowledge bases consist of millions of real world entities and relationships between them which are stored in the form of a directed graph where links represent relations and nodes represent the entities. Although such KBs contain millions of entities, they are still very sparse, i.e., they are missing a large number of relations between existing entities (West et al., 2014). Performing inference over the knowledge graph for predicting relations between two entities is one way of densifying the KB graph. For example, Figure 1: Example showing how addition of the bridging entity, Brian McCain, and the two edges incident on it can help the PRA algorithm (Lao and Cohen, 2010) to infer the initially missing relation instance teamPlaysSport(Yankees, BaseBall). The original KB graph consisted only of two nodes, Yankees and Baseball, and no edges. from (Germany, playsinTournament, FIFA) and (FIFA, tournamentofSport, Soccer), we can infer (Germany, playsSport, Soccer). T</context>
</contexts>
<marker>West, Gabrilovich, Murphy, Sun, Gupta, Lin, 2014</marker>
<rawString>Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014. Knowledge base completion via search-based question answering. In Proceedings of the 23rd International Conference on World Wide Web, WWW ’14, pages 515–526, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
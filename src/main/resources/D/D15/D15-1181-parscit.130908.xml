<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9992685">
Multi-Perspective Sentence Similarity Modeling
with Convolutional Neural Networks
</title>
<author confidence="0.999587">
Hua He,1 Kevin Gimpel,2 and Jimmy Lin&apos;
</author>
<affiliation confidence="0.8216305">
1 Department of Computer Science, University of Maryland, College Park
2 Toyota Technological Institute at Chicago
</affiliation>
<address confidence="0.859006">
&apos; David R. Cheriton School of Computer Science, University of Waterloo
</address>
<email confidence="0.997275">
huah@cs.umd.edu,kgimpel@ttic.edu,jimmylin@uwaterloo.ca
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956842105263">
Modeling sentence similarity is compli-
cated by the ambiguity and variability of
linguistic expression. To cope with these
challenges, we propose a model for com-
paring sentences that uses a multiplicity of
perspectives. We first model each sentence
using a convolutional neural network that
extracts features at multiple levels of gran-
ularity and uses multiple types of pooling.
We then compare our sentence representa-
tions at several granularities using multi-
ple similarity metrics. We apply our model
to three tasks, including the Microsoft Re-
search paraphrase identification task and
two SemEval semantic textual similarity
tasks. We obtain strong performance on all
tasks, rivaling or exceeding the state of the
art without using external resources such
as WordNet or parsers.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987052">
Measuring the semantic relatedness of two pieces
of text is a fundamental problem in language
processing tasks like plagiarism detection, query
ranking, and question answering. In this paper, we
address the sentence similarity measurement prob-
lem: given a query sentence S1 and a comparison
sentence S2, the task is to compute their similar-
ity in terms of a score sim(S1, S2). This simi-
larity score can be used within a system that de-
termines whether two sentences are paraphrases,
e.g., by comparing it to a threshold.
Measuring sentence similarity is challenging
because of the variability of linguistic expression
and the limited amount of annotated training data.
This makes it difficult to use sparse, hand-crafted
features as in conventional approaches in NLP. Re-
cent successes in sentence similarity have been ob-
tained by using neural networks (Tai et al., 2015;
Yin and Sch¨utze, 2015). Our approach is also
based on neural networks: we propose a modular
functional architecture with two components, sen-
tence modeling and similarity measurement.
For sentence modeling, we use a convolutional
neural network featuring convolution filters with
multiple granularities and window sizes, followed
by multiple types of pooling. We experiment with
two types of word embeddings as well as part-
of-speech tag embeddings (Sec. 4). For similar-
ity measurement, we compare pairs of local re-
gions of the sentence representations, using multi-
ple distance functions: cosine distance, Euclidean
distance, and element-wise difference (Sec. 5).
We demonstrate state-of-the-art performance on
two SemEval semantic relatedness tasks (Agirre et
al., 2012; Marelli et al., 2014), and highly com-
petitive performance on the Microsoft Research
paraphrase (MSRP) identification task (Dolan et
al., 2004). On the SemEval-2014 task, we match
the state-of-the-art dependency tree Long Short-
Term Memory (LSTM) neural networks of Tai
et al. (2015) without using parsers or part-of-
speech taggers. On the MSRP task, we outper-
form the recently-proposed convolutional neural
network model of Yin and Sch¨utze (2015) with-
out any pretraining. In addition, we perform ab-
lation experiments to show the contribution of our
modeling decisions for all three datasets, demon-
strating clear benefits from our use of multiple per-
spectives both in sentence modeling and structured
similarity measurement.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999660714285714">
Most previous work on modeling sentence simi-
larity has focused on feature engineering. Sev-
eral types of sparse features have been found use-
ful, including: (1) string-based, including n-gram
overlap features on both the word and character
levels (Wan et al., 2006) and features based on
machine translation evaluation metrics (Madnani
</bodyText>
<page confidence="0.932529">
1576
</page>
<note confidence="0.9847885">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999937684931507">
et al., 2012); (2) knowledge-based, using exter-
nal lexical resources such as WordNet (Fellbaum,
1998; Fern and Stevenson, 2008); (3) syntax-
based, e.g., modeling divergence of dependency
syntax between the two sentences (Das and Smith,
2009); (4) corpus-based, using distributional mod-
els such as latent semantic analysis to obtain fea-
tures (Hassan, 2011; Guo and Diab, 2012).
Several strongly-performing approaches used
system combination (Das and Smith, 2009; Mad-
nani et al., 2012) or multi-task learning. Xu et
al. (2014) developed a feature-rich multi-instance
learning model that jointly learns paraphrase rela-
tions between word and sentence pairs.
Recent work has moved away from hand-
crafted features and towards modeling with dis-
tributed representations and neural network archi-
tectures. Collobert and Weston (2008) used con-
volutional neural networks in a multitask setting,
where their model is trained jointly for multiple
NLP tasks with shared weights. Kalchbrenner et
al. (2014) introduced a convolutional neural net-
work for sentence modeling that uses dynamic
k-max pooling to better model inputs of varying
sizes. Kim (2014) proposed several modifications
to the convolutional neural network architecture of
Collobert and Weston (2008), including the use of
both fixed and learned word vectors and varying
window sizes of the convolution filters.
For the MSRP task, Socher et al. (2011) used
a recursive neural network to model each sen-
tence, recursively computing the representation
for the sentence from the representations of its
constituents in a binarized constituent parse. Ji
and Eisenstein (2013) used matrix factorization
techniques to obtain sentence representations, and
combined them with fine-tuned sparse features us-
ing an SVM classifier for similarity prediction.
Both Socher et al. and Ji and Eisenstein incor-
porated sparse features to improve performance,
which we do not use in this work.
Hu et al. (2014) used convolutional neural net-
works that combine hierarchical sentence mod-
eling with layer-by-layer composition and pool-
ing. While they performed comparisons directly
over entire sentence representations, we instead
develop a structured similarity measurement layer
to compare local regions. A variety of other neural
network models have been proposed for similarity
tasks (Weston et al., 2011; Huang et al., 2013; An-
drew et al., 2013; Bromley et al., 1993).
Most recently, Tai et al. (2015) and Zhu et al.
(2015) concurrently proposed a tree-based LSTM
neural network architecture for sentence model-
ing. Unlike them, we do not use syntactic parsers,
yet our performance matches Tai et al. (2015)
on the similarity task. This result is appealing
because high-quality parsers are difficult to ob-
tain for low-resource languages or specialized do-
mains. Yin and Sch¨utze (2015) concurrently de-
veloped a convolutional neural network architec-
ture for paraphrase identification, which we com-
pare to in our experiments. Their best results rely
on an unsupervised pretraining step, which we do
not need to match their performance.
Our model architecture differs from previous
work in several ways. We exploit multiple per-
spectives of input sentences in order to maxi-
mize information utilization and perform struc-
tured comparisons over particular regions of the
sentence representations. We now proceed to de-
scribe our model in detail, and we compare to the
above related work in our experimental evaluation.
</bodyText>
<sectionHeader confidence="0.960032" genericHeader="method">
3 Model Overview
</sectionHeader>
<bodyText confidence="0.999871777777778">
Modeling textual similarity is complicated by the
ambiguity and variability of linguistic expression.
We designed a model with these phenomena in
mind, exploiting multiple types of input which are
processed by multiple types of convolution and
pooling. Our similarity architecture likewise uses
multiple similarity functions.
To summarize, our model (shown in Figure 1)
consists of two main components:
</bodyText>
<listItem confidence="0.9675897">
1. A sentence model for converting a sentence
into a representation for similarity measure-
ment; we use a convolutional neural network
architecture with multiple types of convolution
and pooling in order to capture different granu-
larities of information in the inputs.
2. A similarity measurement layer using multi-
ple similarity measurements, which compare lo-
cal regions of the sentence representations from
the sentence model.
</listItem>
<bodyText confidence="0.9980965">
Our model has a “Siamese” structure (Bromley
et al., 1993) with two subnetworks each process-
ing a sentence in parallel. The subnetworks share
all of their weights, and are joined by the simi-
larity measurement layer, then followed by a fully
connected layer for similarity score output.
</bodyText>
<page confidence="0.9885">
1577
</page>
<figure confidence="0.711121">
Output: Similarity Score
Fully Connected Layer
Structured Similarity Measurement Layer
</figure>
<figureCaption confidence="0.999203">
Figure 1: Model overview. Two input sentences
</figureCaption>
<bodyText confidence="0.989856842105263">
(on the bottom) are processed in parallel by iden-
tical neural networks, outputting sentence repre-
sentations. The sentence representations are com-
pared by the structured similarity measurement
layer. The similarity features are then passed to a
fully-connected layer for computing the similarity
score (top).
Importantly, we do not require resources like
WordNet or syntactic parsers for the language of
interest; we only use optional part-of-speech tags
and pretrained word embeddings. The main dif-
ference from prior work lies in our use of multiple
types of convolution, pooling, and structured sim-
ilarity measurement over local regions. We show
later in our experiments that the bulk of our perfor-
mance comes from this use of multiple “perspec-
tives” of the input sentences.
We describe our sentence model in Section 4
and our similarity measurement layer in Section 5.
</bodyText>
<sectionHeader confidence="0.987525" genericHeader="method">
4 Sentence Modeling
</sectionHeader>
<bodyText confidence="0.996479333333333">
In this section we describe our convolutional neu-
ral network for modeling each sentence. We use
two types of convolution filters defined on differ-
ent perspectives of the input (Sec. 4.1), and also
use multiple types of pooling (Sec. 4.2).
Our inputs are streams of tokens, which can be
interpreted as a temporal sequence where nearby
words are likely to be correlated. Let sent E
Rlen×Dien be a sequence of len input words rep-
resented by Dim-dimensional word embeddings,
where senti E RDien is the embedding of the i-th
word in the sequence and senti:j represents the
concatenation of embeddings from word i up to
and including word j. We denote the k-th dimen-
sion of the i-th word vector by sent[k]
i and we de-
note the vector containing the k-th dimension of
words i to j by sent[k]
</bodyText>
<equation confidence="0.841693">
i:j.
w1 w2 w3 w4 w5 w1 w2 w3 w4 w5
</equation>
<bodyText confidence="0.51661925">
Figure 2: Left: a holistic filter matches entire word
vectors (here, ws = 2). Right: per-dimension fil-
ters match against each dimension of the word em-
beddings independently.
</bodyText>
<subsectionHeader confidence="0.999691">
4.1 Convolution on Multiple Perspectives
</subsectionHeader>
<bodyText confidence="0.998165181818182">
We define a convolution filter F as a tuple
(ws, wF, bF, hF), where ws is the sliding window
width, wF E Rws×Dien is the weight vector for
the filter, bF E R is the bias, and hF is the activa-
tion function (a nonlinear function such as tanh).
When filter F is applied to sequence sent, the
inner product is computed between wF and each
possible window of word embeddings of length
ws in sent, then the bias is added and the activa-
tion function is applied. This results in an output
vector outF E R1+len−ws where entry i equals
</bodyText>
<equation confidence="0.999144">
outF[i] = hF(wF - senti:i+ws−1 + bF) (1)
</equation>
<bodyText confidence="0.999888157894737">
where i E [1,1 + len − ws]. This filter can be
viewed as performing “temporal” convolution, as
it matches against regions of the word sequence.
Since these filters consider the entirety of each
word embedding at each position, we call them
holistic filters; see the left half of Figure 2.
In addition, we target information at a finer
granularity by constructing per-dimension filters
F [k] for each dimension k of the word embed-
dings, where wF[k] E Rws. See the right half
of Figure 2. The per-dimension filters are simi-
lar to “spatial convolution” filters except that we
limit each to a single, predefined dimension. We
include separate per-dimension filters for each di-
mension of the input word embeddings.
Applying a per-dimension filter F[k] =
(ws, wF[k], bF[k], hF[k]) for dimension k re-
sults in an output vector outF[k] E R1+len−ws
where entry i (for i E [1,1 + len − ws]) equals
</bodyText>
<equation confidence="0.999582">
outF[k][i] = hF[k](wF[k] - sent[k]
i:i+ws−1 + bF[k])
</equation>
<bodyText confidence="0.998705">
Our use of word embeddings in both ways allows
more information to be extracted for richer sen-
tence modeling. While we typically do not expect
individual dimensions of neural word embeddings
</bodyText>
<figure confidence="0.953197">
•
�
� �
� �
Cats Sit On The Mat
•
•
� �
� �
On The Mat There Sit Cats
� � � � � � � �
1578
</figure>
<figureCaption confidence="0.79397">
Figure 3: Each building block consists of multiple
</figureCaption>
<bodyText confidence="0.986442666666667">
independent pooling layers and convolution layers
with width ws1. Left: blockA operates on entire
vectors of word embeddings. Right: blockB oper-
ates on individual dimensions of word vectors to
capture information of a finer granularity.
to be interpretable to humans, there may still be
distinct information captured by the different di-
mensions that our model could exploit. Further-
more, if we update the word embeddings during
learning, different dimensions could be encour-
aged further to capture distinct information.
We define a convolution layer as a set of con-
volution filters that share the same type (holistic
or per-dimension), activation function, and width
ws. The type, width, activation function, and num-
ber of filters numFilter in the layer are chosen by
the modeler and the weights of each filter (wF and
bF) are learned.
</bodyText>
<subsectionHeader confidence="0.998585">
4.2 Multiple Pooling Types
</subsectionHeader>
<bodyText confidence="0.990596954545455">
The output vector outF of a convolution filter F is
typically converted to a scalar for subsequent use
by the model using some method of pooling. For
example, “max-pooling” applies a max operation
across the entries of outF and returns the max-
imum value. In this paper, we experiment with
two additional types of pooling: “min-pooling”
and “mean-pooling”.
A group, denoted group(ws, pooling, sent), is
an object that contains a convolution layer with
width ws, uses pooling function pooling, and op-
erates on sentence sent. We define a building
block to be a set of groups. We use two types of
building blocks, blockA and blockB, as shown in
Figure 3. We define blockA as
{groupA(wsa,p, sent) : p E {max, min, mean}}.
That is, an instance of blockA has three convolu-
tion layers, one corresponding to each of the three
pooling functions; all have the same window size
wsa. An alternative choice would be to use the
multiple types of pooling on the same filters (Ren-
nie et al., 2014); we instead use independent sets
of filters for the different pooling types.1 We use
blocks of type A for all holistic convolution layers.
We define blockB as
{groupB(wsb, p, sent) : p E {max, min}}.
That is, blockB contains two groups of convolu-
tion layers of width wsb, one with max-pooling
and one with min-pooling. Each groupB(*) con-
tains a convolution layer with Dim per-dimension
convolution filters. That is, we use blocks of type
B for convolution layers that operate on individual
dimensions of word vectors.
We use these multiple types of pooling to ex-
tract different types of information from each type
of filter. The design of each group(*) allows a
pooling function to interact with its own underly-
ing convolution layers independently, so each con-
volution layer can learn to recognize distinct phe-
nomena of the input for richer sentence modeling.
For a groupA(wsa, poolinga, sent) with a con-
volution layer with numFilterA filters, we define
the output oGA as a vector of length numFilterA
where entry j is
</bodyText>
<equation confidence="0.997434">
oGA[j] = poolinga(outFj) (2)
</equation>
<bodyText confidence="0.998270555555556">
where filters are indexed as Fj. That is, the output
of groupA(*) is a numFilterA-length vector con-
taining the output of applying the pooling function
on each filter’s vector of filter match outputs.2
A component groupB(*) of blockB contains
Dim filters, each operating on a particular di-
mension of the word embeddings. We define the
output oGB of groupB(wsb, poolingb, sent) as a
Dim x numFilterB matrix where entry [k][j] is
</bodyText>
<equation confidence="0.977596">
oGB[k][j] = poolingb(outF[k] )
j
</equation>
<bodyText confidence="0.998738">
where filter F[k] jis filter j for dimension k.
</bodyText>
<subsectionHeader confidence="0.998539">
4.3 Multiple Window Sizes
</subsectionHeader>
<bodyText confidence="0.999959">
Similar to traditional n-gram-based models, we
use multiple window sizes ws in our building
blocks in order to learn features of different
lengths. For example, in Figure 4 we use four
building blocks, each with one window size ws =
</bodyText>
<footnote confidence="0.948510666666667">
1We note that max and min are not both strictly necessary
when using certain activation functions, but they still may
help us find a more felicitous local optimum.
2We note that there is no pooling across multiple filters
in a layer/group, or across groups. Each pooling operation is
performed independently on the matches of a single filter.
</footnote>
<figure confidence="0.96937016">
Max Pooling
Holistic
Filters
wsl
Building Block A Building Block B
Min Pooling
Holistic
Filters
wsl
Mean Pooling
Holistic
Filters
wsl
Max Pooling
Per-
Dimension
Filters
wsl
Min Pooling
Per-
Dimension
Filters
ws1
1579
Cats Sit On The Mat
</figure>
<figureCaption confidence="0.94153">
Figure 4: Example neural network architecture for
</figureCaption>
<bodyText confidence="0.988384166666667">
a single sentence, containing 3 instances of blockA
(with 3 types of pooling) and 2 instances of blockB
(with 2 types) on varying window sizes ws = 1, 2
and ws = ∞; blockA operates on entire word vec-
tors while blockB contains filters that operate on
individual dimensions independently.
1 or 2 for its own convolution layers. In order to
retain the original information in the sentences, we
also include the entire matrix of word embeddings
in the sentence, which essentially corresponds to
ws = ∞.
The width ws represents how many words are
matched by a filter, so using larger values of ws
corresponds to matching longer n-grams in the
input sentences. The ranges of ws values and
the numbers of filters numFilter of blockA and
blockB are empirical choices tuned based on vali-
dation data.
</bodyText>
<sectionHeader confidence="0.987565" genericHeader="method">
5 Similarity Measurement Layer
</sectionHeader>
<bodyText confidence="0.999951457142857">
In this section we describe the second part of our
model, the similarity measurement layer.
Given two input sentences, the first part of our
model computes sentence representations for each
of them in parallel. One straightforward way to
compare them is to flatten the sentence represen-
tations into two vectors, then use standard met-
rics like cosine similarity. However, this may
not be optimal because different regions of the
flattened sentence representations are from differ-
ent underlying sources (e.g., groups of different
widths, types of pooling, dimensions of word vec-
tors, etc.). Flattening might discard useful com-
positional information for computing similarity.
We therefore perform structured comparisons over
particular regions of the sentence representations.
One important consideration is how to iden-
tify suitable local regions for comparison so that
we can best utilize the compositional information
in the sentence representations. There are many
possible ways to group local comparison regions.
In doing so, we consider the following four as-
pects: 1) whether from the same building block; 2)
whether from convolutional layers with the same
window size; 3) whether from the same pooling
layer; 4) whether from the same filter of the under-
lying convolution layers.3 We focus on comparing
regions that share at least two of these conditions.
To concretize this, we provide two algorithms
below to identify meaningful local regions. While
there exist other sets of comparable regions that
share the above conditions, we do not explore
them all due to concerns about learning efficiency;
we find that the subset we consider performs
strongly in practice.
</bodyText>
<subsectionHeader confidence="0.977041">
5.1 Similarity Comparison Units
</subsectionHeader>
<bodyText confidence="0.9998575">
We define two comparison units for comparing
two local regions in the sentence representations:
</bodyText>
<equation confidence="0.997134">
comU1(−→x , →−Y ) _ {cos(−→x , →−Y ), L2Euclid(−→x , →−Y ),
|−→x − →−Y |} (3)
comU2(−→x , →−Y ) _ {cos(−→x , →−Y ), L2Euclid(−→x , →−Y )} (4)
</equation>
<bodyText confidence="0.9990314">
Cosine distance (cos) measures the distance of
two vectors according to the angle between them,
while L2 Euclidean distance (L2Euclid) and
element-wise absolute difference measure magni-
tude differences.
</bodyText>
<subsectionHeader confidence="0.999376">
5.2 Comparison over Local Regions
</subsectionHeader>
<bodyText confidence="0.999987615384615">
Algorithms 1 and 2 show how the two sentence
representations are compared in our model. Algo-
rithm 1 works on the output of blockA only, while
Algorithm 2 deals with both blockA and blockB,
focusing on regions from the output of the same
pooling type and same block type, but with differ-
ent filters and window sizes of convolution layers.
Given two sentences 51 and 52, we set the max-
imum window size ws of blockA and blockB to be
n, let regM* represent a numFilterA by n + 1 ma-
trix, and assume that each group* outputs its cor-
responding oG*. The output features are accumu-
lated in a final vector fea.
</bodyText>
<subsectionHeader confidence="0.991537">
5.3 One Simplified Example
</subsectionHeader>
<bodyText confidence="0.999974">
We provide a simplified working example to show
how the two algorithms compare outputs of blockA
only. If we arrange the sentence representations
into the shape of sentence matrices as in Figure 5,
</bodyText>
<footnote confidence="0.99095975">
3We note that since we apply the same network to both
sentences, the same filters are used to match both sentences,
so we can directly compare filter matches of individual filters
across the two sentences.
</footnote>
<figure confidence="0.290899630434783">
• • • • •
• • • • •
• • • • •
Window Size
ws = ∞
Building Block A
Window Size
ws = 1
Window Size
ws = 2
Window Size
ws = 1
Building Block B
Window Size
ws = 2
1580
Algorithm 1 Horizontal Comparison
1: for each pooling p = max, min, mean do
2: for each width ws1 = 1...n, ∞ do
3: regM1[∗][ws1] = groupA(ws1, p, S1)
4: regM2[∗][ws1] = groupA(ws1, p, S2)
5: end for
6: for each i = 1...numFilterA do
7: feah = comU2(regM1[i], regM2[i])
8: accumulate feah for final layer
9: end for
10: end for
Algorithm 2 Vertical Comparison
1: for each pooling p = max, min, mean do
2: for each width ws1 = 1...n, ∞ do
3: oG1A = groupA(ws1, p, S1)
4: for each width ws2 = 1...n, ∞ do
5: oG2A = groupA(ws2, p, S2)
6: feaa = comU1(oG1A, oG2A)
7: accumulate feaa for final layer
8: end for
9: end for
10: for each width ws1 = 1...n do
11: oG1B = groupB(ws1, p, S1)
12: oG2B = groupB(ws1, p, S2)
13: for each i = 1...numFilterB do
14: fea6 =comU1(oG1B[∗][i], oG2B[∗][i])
15: accumulate fea6 for final layer
16: end for
17: end for
18: end for
</figure>
<bodyText confidence="0.996889529411765">
then in Algorithms 1 and 2 we are essentially com-
paring local regions of the two matrices in two di-
rections: along rows and columns.
In Figure 5, each column of the max/min/mean
groups is compared with all columns of the same
pooling group for the other sentence. This is
shown in red dotted lines in the Figure and listed in
lines 2 to 9 in Algorithm 2. Note that both ws1 and
ws2 columns within each pooling group should be
compared using red dotted lines, but we omit this
from the figure for clarity.
In the horizontal direction, each equal-sized
max/min/mean group is extracted as a vector and
is compared to the corresponding one for the other
sentence. This process is repeated for all rows and
comparisons are shown in green solid lines, as per-
formed by Algorithm 1.
</bodyText>
<sectionHeader confidence="0.508951" genericHeader="method">
5.4 Other Model Details
</sectionHeader>
<bodyText confidence="0.929585142857143">
Output Fully-Connected Layer. On top of the
similarity measurement layer (which outputs a
vector containing all fea∗), we stack two linear
layers with an activation layer in between, fol-
lowed by a log-softmax layer as the final output
layer, which outputs the similarity score.
Activation Layers. We used element-wise tanh
</bodyText>
<figureCaption confidence="0.877817125">
Figure 5: Simplified example of local region com-
parisons over two sentence representations that
use blockA only. The “horizontal comparison”
(Algorithm 1) is shown with green solid lines and
“vertical comparison” (Algorithm 2) with red dot-
ted lines. Each sentence representation uses win-
dow sizes ws1 and ws2 with max/min/mean pool-
ing and numFilterA = 3 filters.
</figureCaption>
<bodyText confidence="0.996794333333333">
as the activation function for all convolution filters
and for the activation layer placed between the fi-
nal two layers.
</bodyText>
<sectionHeader confidence="0.998308" genericHeader="evaluation">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999748666666667">
Everything necessary to replicate our experimen-
tal results can be found in our open-source code
repository.4
</bodyText>
<subsectionHeader confidence="0.997524">
6.1 Tasks and Datasets
</subsectionHeader>
<bodyText confidence="0.998047">
We consider three sentence pair similarity tasks:
</bodyText>
<listItem confidence="0.988343090909091">
1. Microsoft Research Paraphrase Corpus
(MSRP). This data was collected from news
sources (Dolan et al., 2004) and contains
5,801 pairs of sentences, with 4,076 for
training and the remaining 1,725 for testing.
Each sentence pair is annotated with a binary
label indicating whether the two sentences
are paraphrases, so the task here is binary
classification.
2. Sentences Involving Compositional Knowl-
edge (SICK) dataset. This data was collected
</listItem>
<bodyText confidence="0.967572555555556">
for the 2014 SemEval competition (Marelli
et al., 2014) and consists of 9,927 sentence
pairs, with 4,500 for training, 500 as a devel-
opment set, and the remaining 4,927 in the
test set. The sentences are drawn from image
and video descriptions. Each sentence pair is
annotated with a relatedness score ∈ [1, 5],
with higher scores indicating the two sen-
tences are more closely-related.
</bodyText>
<footnote confidence="0.708809">
4http://hohocode.github.io/textSimilarityConvNet/
</footnote>
<figure confidence="0.998895227272727">
�
⊗
0
Max
0
⊗
0
Min
�
0 ⊗
Mean
�
⊗
0
Max
0
⊗
0
Min
�
0 ⊗
Mean
</figure>
<page confidence="0.935714">
1581
</page>
<bodyText confidence="0.9687184">
3. Microsoft Video Paraphrase Corpus
(MSRVID). This dataset was collected
for the 2012 SemEval competition and
consists of 1,500 pairs of short video de-
scriptions which were then annotated (Agirre
et al., 2012). Half of it is for training and the
other half is for testing. Each sentence pair
has a relatedness score E [0, 5], with higher
scores indicating the two sentences are more
closely-related.
</bodyText>
<subsectionHeader confidence="0.997735">
6.2 Training
</subsectionHeader>
<bodyText confidence="0.999834">
We use a hinge loss for the MSRP paraphrase
identification task. This is simpler than log loss
since it only penalizes misclassified cases. The
training objective is to minimize the following loss
(summed over examples (x , ygold)):
</bodyText>
<equation confidence="0.991423666666667">
loss(θ, x, ygold) =
� max(0, 1 + fe(x, y�) − fe(x, ygold)) (5)
yYygold
</equation>
<bodyText confidence="0.999952666666667">
where ygold is the ground truth label, input x is
the pair of sentences x = {S1, S21, θ is the
model weight vector to be trained, and the func-
tion fe(x, y) is the output of our model.
We use regularized KL-divergence loss for the
semantic relatedness tasks (SICK and MSRVID),
since the goal is to predict the similarity of the two
sentences. The training objective is to minimize
the KL-divergence loss plus an L2 regularizer:
</bodyText>
<equation confidence="0.992149666666667">
� �
KL fk  ||�fk+ λ 2||θ||2 2 (6)
e
</equation>
<bodyText confidence="0.9999855">
where fe is the predicted distribution with model
weight vector θ, f is the ground truth, m is the
number of training examples, and λ is the regu-
larization parameter. Note that we use the same
KL-loss function and same sparse target distribu-
tion technique as Tai et al. (2015).
</bodyText>
<subsectionHeader confidence="0.997113">
6.3 Experiment Settings
</subsectionHeader>
<bodyText confidence="0.999956839285715">
We conduct experiments with ws values in the
range [1, 3] as well as ws = oc (no convolution).
We use multiple kinds of embeddings to rep-
resent each sentence, both on words and part-of-
speech (POS) tags. We use the Dimg = 300-
dimensional GloVe word embeddings (Pennington
et al., 2014) trained on 840 billion tokens. We
use Dimk = 25-dimensional PARAGRAM vec-
tors (Wieting et al., 2015) only on the MSRP task
since they were developed for paraphrase tasks,
having been trained on word pairs from the Para-
phrase Database (Ganitkevitch et al., 2013). For
POS embeddings, we run the Stanford POS tag-
ger (Manning et al., 2014) on the English side
of the Xinhua machine translation parallel cor-
pus, which consists of Xinhua news articles with
approximately 25 million words. We then train
Dimg, = 200-dimensional POS embeddings us-
ing the word2vec toolkit (Mikolov et al., 2013).
Adding POS embeddings is expected to retain syn-
tactic information which is reported to be effec-
tive for paraphrase identification (Das and Smith,
2009). We use POS embeddings only for the
MSRP task.
Therefore for MSRP, we concatenate all word
and POS embeddings and obtain Dim = Dimg +
Dimg, + Dimk = 525-dimension vectors for each
input word; for SICK and MSRVID we only use
Dim = 300-dimension GloVe embeddings.
We use 5-fold cross validation on the MSRP
training data for tuning, then largely re-use the
same hyperparameters for the other two datasets.
However, there are two changes: 1) for the MSRP
task we update word embeddings during train-
ing but not so on SICK and MSRVID tasks; 2)
we set the fully connected layer to contain 250
hidden units for MSRP, and 150 for SICK and
MSRVID. These changes were done to speed up
our experimental cycle on SICK and MSRVID; on
SICK data they are the same experimental settings
as used by Tai et al. (2015), which makes for a
cleaner empirical comparison.
We set the number of holistic filters in blockA
to be the same as the input word embeddings,
therefore numFilterA = 525 for MSRP and
numFilterA = 300 for SICK and MSRVID. We
set the number of per-dimension filters in blockB
to be numFilterB = 20 per dimension for all
three datasets, which corresponds to 20 * Dim fil-
ters in total.
We perform optimization using stochastic gra-
dient descent (Bottou, 1998). The backpropaga-
tion algorithm is used to compute gradients for
all parameters during training (Goller and Kuch-
ler, 1996). We fix the learning rate to 0.01 and
regularization parameter λ = 10−4.
</bodyText>
<subsectionHeader confidence="0.950609">
6.4 Results on Three Datasets
</subsectionHeader>
<bodyText confidence="0.9850795">
Results on MSRP Data. We report F1 scores
and accuracies from prior work in Table 1. Ap-
</bodyText>
<equation confidence="0.9879736">
1
loss(θ) =
m
�m
k=1
</equation>
<page confidence="0.975837">
1582
</page>
<table confidence="0.985527055555555">
Model Acc. F1
Hu et al. (2014) ARC-I 69.6% 80.3%
Hu et al. (2014) ARC-II 69.9% 80.9%
Blacoe and Lapata (2012) 73.0% 82.3%
Fern and Stevenson (2008) 74.1% 82.4%
Finch (2005) 75.0% 82.7%
Das and Smith (2009) 76.1% 82.7%
Wan et al. (2006) 75.6% 83.0%
Socher et al. (2011) 76.8% 83.6%
Madnani et al. (2012) 77.4% 84.1%
Ji and Eisenstein (2013) 80.41% 85.96%
Yin and Sch¨utze (2015) 72.5% 81.4%
(without pretraining)
Yin and Sch¨utze (2015) 78.1% 84.4%
(with pretraining)
Yin and Sch¨utze (2015) 78.4% 84.6%
(pretraining+sparse features)
This work 78.60% 84.73%
</table>
<tableCaption confidence="0.793129333333333">
Table 1: Test set results on MSRP for paraphrase
identification. Rows in grey are neural network-
based approaches.
</tableCaption>
<bodyText confidence="0.99816425">
proaches shown in gray rows of the table are
based on neural networks. The recent approach
by Yin and Sch¨utze (2015) includes a pretraining
technique which significantly improves results, as
shown in the table. We do not use any pretrain-
ing but still slightly outperform their best results
which use both pretraining and additional sparse
features from Madnani et al. (2012).
When comparing to their model without pre-
training, we outperform them by 6% absolute in
accuracy and 3% in F1. Our model is also supe-
rior to other recent neural network models (Hu et
al., 2014; Socher et al., 2011) without requiring
sparse features or unlabeled data as in (Yin and
Sch¨utze, 2015; Socher et al., 2011). The best re-
sult on MSRP is from Ji and Eisenstein (2013)
which uses unsupervised learning on the MSRP
test set and rich sparse features.
Results on SICK Data. Our results on the SICK
task are summarized in Table 2, showing Pearson’s
r, Spearman’s p, and mean squared error (MSE).
We include results from the literature as reported
by Tai et al. (2015), including prior work using re-
current neural networks (RNNs), the best submis-
sions in the SemEval-2014 competition, and vari-
ants of LSTMs. When measured by Pearson’s r,
the previous state-of-the-art approach uses a tree-
structured LSTM (Tai et al., 2015); note that their
best results require a dependency parser.
On the contrary, our approach does not rely on
parse trees, nor do we use POS/PARAGRAM em-
beddings for this task. The word embeddings,
</bodyText>
<table confidence="0.978931214285714">
Model r ρ MSE
Socher et al. (2014) DT-RNN 0.7863 0.7305 0.3983
Socher et al. (2014) SDT-RNN 0.7886 0.7280 0.3859
Lai and Hockenmaier (2014) 0.7993 0.7538 0.3692
Jimenez et al. (2014) 0.8070 0.7489 0.3550
Bjerva et al. (2014) 0.8268 0.7721 0.3224
Zhao et al. (2014) 0.8414 - -
LSTM 0.8477 0.7921 0.2949
Bi-LSTM 0.8522 0.7952 0.2850
2-layer LSTM 0.8411 0.7849 0.2980
2-layer Bidirectional LSTM 0.8488 0.7926 0.2893
Tai et al. (2015) Const. LSTM 0.8491 0.7873 0.2852
Tai et al. (2015) Dep. LSTM 0.8676 0.8083 0.2532
This work 0.8686 0.8047 0.2606
</table>
<tableCaption confidence="0.966379714285714">
Table 2: Test set results on SICK, as reported
by Tai et al. (2015), grouped as: (1) RNN vari-
ants; (2) SemEval 2014 systems; (3) sequential
LSTM variants; (4) dependency and constituency
tree LSTMs (Tai et al., 2015). Evaluation metrics
are Pearson’s r, Spearman’s p, and mean squared
error (MSE).
</tableCaption>
<table confidence="0.999850285714286">
Model Pearson’s r
Rios et al. (2012) 0.7060
Wang and Cer (2012) 0.8037
Beltagy et al. (2014) 0.8300
B¨ar et al. (2012) 0.8730
ˇSari´c et al. (2012) 0.8803
This work 0.9090
</table>
<tableCaption confidence="0.998747">
Table 3: Test set results on MSRVID data. The B¨ar
</tableCaption>
<bodyText confidence="0.997170333333333">
et al. (2012) and ˇSari´c et al. (2012) results were
the top two submissions in the Semantic Textual
Similarity task at the SemEval-2012 competition.
sparse distribution targets, and KL loss function
are exactly the same as used by Tai et al. (2015),
therefore representing comparable conditions.
Results on MSRVID Data. Our results on the
MSRVID data are summarized in Table 3, which
includes the top 2 submissions in the Seman-
tic Textual Similarity (STS) task from SemEval-
2012. We find that we outperform the top system
from the task by nearly 3 points in Pearson’s r.
</bodyText>
<subsectionHeader confidence="0.997099">
6.5 Model Ablation Study
</subsectionHeader>
<bodyText confidence="0.99991">
We report the results of an ablation study in Ta-
ble 4. We identify nine major components of our
approach, remove one at a time (if applicable),
and perform re-training and re-testing for all three
tasks. We use the same experimental settings in
Sec. 6.3 and report differences (in accuracy for
MSRP, Pearson’s r for SICK/MSRVID) compared
to our results in Tables 1–3.
</bodyText>
<page confidence="0.918288">
1583
</page>
<table confidence="0.999322916666667">
Gp ID Ablation Component MSRP MSRVID SICK
Accuracy Pearson Pearson
Diff. Diff. Diff.
1 1 Remove POS embeddings (Sec. 6.3) -0.81 NA NA
2 Remove PARAGRAM embeddings (Sec. 6.3) -1.33 NA NA
2 3 Remove per-dimension embeddings, building block A only (Sec. 4.1) -0.75 -0.0067 -0.0014
4 Remove min and mean pooling, use max pooling only (Sec. 4.2) -0.58 -0.0112 +0.0001
5 Remove multiple widths, ws = 1 and ws = ∞ only (Sec. 4.3) -2.14 -0.0048 -0.0012
3 6 Remove cosine and L2Euclid distance in comU∗ (Sec. 5.1) -2.31 -0.0188 -0.0309
4 7 Remove Horizontal Algorithm (Sec. 5.2) -0.92 -0.0097 -0.0117
8 Remove Vertical Algorithm (Sec. 5.2) -2.15 -0.0063 -0.0027
9 Remove similarity layer (completely flatten) (Sec. 5) -1.90 -0.0121 -0.0288
</table>
<tableCaption confidence="0.998722">
Table 4: Ablation study over test sets of all three datasets. Nine components are divided into four groups.
</tableCaption>
<bodyText confidence="0.9315158125">
We remove components one at a time and show differences.
The nine components can be divided into four
groups: (1) input embeddings (components 1–2);
(2) sentence modeling (components 3–5); (3) sim-
ilarity measurement metrics (component 6); (4)
similarity measurement layer (components 7–9).
For MSRP, we use all nine components. For SICK
and MSRVID, we use components 3–9 (as de-
scribed in Sec. 6.3).
From Table 4 we find drops in performance for
all components, with the largest differences ap-
pearing when removing components of the simi-
larity measurement layer. For example, conduct-
ing comparisons over flattened sentence represen-
tations (removing component 9) leads to large
drops across tasks, because this ignores struc-
tured information within sentence representations.
Groups (1) and (2) are also useful, particularly for
the MSRP task, demonstrating the extra benefit
obtained from our multi-perspective approach in
sentence modeling.
We see consistent drops when ablating the Ver-
tical/Horizontal algorithms that target particular
regions for comparison. Also, removing group
(3) hinders both the Horizontal and Vertical al-
gorithms (as described in Section 5.1), so its
removal similarly causes large drops in perfor-
mance. Though convolutional neural networks al-
ready perform strongly when followed by flattened
vector comparison, we are able to leverage the
full richness of the sentence models by performing
structured similarity modeling on their outputs.
</bodyText>
<sectionHeader confidence="0.998179" genericHeader="discussions">
7 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.99998059375">
On the SICK dataset, the dependency tree
LSTM (Tai et al., 2015) and our model achieve
comparable performance despite taking very dif-
ferent approaches. Tai et al. use syntactic parse
trees and gating mechanisms to convert each sen-
tence into a vector, while we use large sets of flex-
ible feature extractors in the form of convolution
filters, then compare particular subsets of features
in our similarity measurement layer.
Our model architecture, with its many paths of
information flow, is admittedly complex. Though
we have removed hand engineering of features,
we have added a substantial amount of functional
architecture engineering. This may be necessary
when using the small training sets provided for the
tasks we consider here. We conjecture that a sim-
pler, deeper neural network architecture may out-
perform our model when given large amounts of
training data, but we leave an investigation of this
direction to future work.
In summary, we developed a novel model for
sentence similarity based on convolutional neural
networks. We improved both sentence modeling
and similarity measurement. Our model achieves
highly competitive performance on three datasets.
Ablation experiments show that the performance
improvement comes from our use of multiple per-
spectives in both sentence modeling and structured
similarity measurement over local regions of sen-
tence representations. Future work could extend
this model to related tasks including question an-
swering and information retrieval.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99942975">
This work was supported by the U.S. National Sci-
ence Foundation under awards IIS-1218043 and
CNS-1405688. Any opinions, findings, conclu-
sions, or recommendations expressed are those of
the authors and do not necessarily reflect the views
of the sponsor. We would like to thank the anony-
mous reviewers for their feedback and CLIP lab-
mates for their support.
</bodyText>
<page confidence="0.993809">
1584
</page>
<sectionHeader confidence="0.996085" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998640888888889">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: a
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics, pages 385–393.
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen
Livescu. 2013. Deep canonical correlation analysis.
In Proceedings of the 30th International Conference
on Machine Learning, pages 1247–1255.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the First
Joint Conference on Lexical and Computational Se-
mantics, pages 435–440.
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014. Probabilistic soft logic for semantic textual
similarity. Proceedings of 52nd Annual Meeting
of the Association for Computational Linguistics,
pages 1210–1219.
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The meaning factory: for-
mal semantics for recognizing textual entailment
and determining semantic similarity. International
Workshop on Semantic Evaluation.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546–556.
L´eon Bottou. 1998. Online learning and stochastic ap-
proximations. On-line learning in neural networks,
17(9):142.
Jane Bromley, James W Bentz, L´eon Bottou, Is-
abelle Guyon, Yann LeCun, Cliff Moore, Eduard
S¨ackinger, and Roopak Shah. 1993. Signature ver-
ification using a “siamese” time delay neural net-
work. International Journal of Pattern Recognition
and Artificial Intelligence, 7(4):669–688.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine learning, pages 160–167.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 468–476.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Samuel Fern and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection.
In Computational Linguistics UK 11th Annual Re-
search Colloquium.
Andrew Finch. 2005. Using machine translation eval-
uation techniques to determine sentence-level se-
mantic equivalence. In Proceedings of the Interna-
tional Workshop on Paraphrasing.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: the Paraphrase
Database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Proceedings
of the International Conference on Neural Networks,
pages 347–352.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics, pages 864–872.
Samer Hassan. 2011. Measuring Semantic Related-
ness Using Salient Encyclopedic Concepts. Ph.D.
thesis, University of North Texas, Denton, Texas,
USA.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems, pages 2042–2050.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proceedings of the 22nd ACM
International Conference on Information &amp; Knowl-
edge Management, pages 2333–2338.
Yangfeng Ji and Jacob Eisenstein. 2013. Discrimina-
tive improvements to distributional sentence similar-
ity. In Proceedings of the 2013 Conference on Em-
pirical Methods for Natural Language Processing,
pages 891–896.
Sergio Jimenez, George Duenas, Julia Baquero,
Alexander Gelbukh, Av Juan Dios B´atiz, and
Av Mendiz´abal. 2014. UNAL-NLP: combining soft
cardinality features for semantic textual similarity,
relatedness and entailment. International Workshop
on Semantic Evaluation.
</reference>
<page confidence="0.804099">
1585
</page>
<reference confidence="0.999861315315316">
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods for Natural Lan-
guage Processing.
Alice Lai and Julia Hockenmaier. 2014. Illinois-LH:
a denotational and distributional approach to seman-
tics. International Workshop on Semantic Evalua-
tion.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 182–190.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. SemEval-2014 task 1: evaluation
of compositional distributional semantic models on
full sentences through semantic relatedness and tex-
tual entailment. International Workshop on Seman-
tic Evaluation.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of Workshop at
International Conference on Learning Representa-
tions.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1532–1543.
Steven Rennie, Vaibhava Goel, and Samuel Thomas.
2014. Deep order statistic networks. In Proceedings
of the IEEE Workshop on Spoken Language Technol-
ogy.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2012.
UOW: semantically informed text similarity. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics, pages 673–678.
Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: systems
for measuring semantic text similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 441–448.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.
Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics.
Stephen Wan, Mark Dras, Robert Dale, and Cecile
Paris. 2006. Using Dependency-based Features to
Take the ”Para-farce” out of Paraphrase. In Aus-
tralasian Language Technology Workshop, pages
131–138.
Mengqiu Wang and Daniel Cer. 2012. Probabilistic
edit distance metrics for STS. In Proceedings of
the First Joint Conference on Lexical and Compu-
tational Semantics, pages 648–654.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: scaling up to large vocabulary im-
age annotation. In International Joint Conference
on Artificial Intelligence, pages 2764–2770.
John Wieting, Mohit Bansal, Kevin Gimpel, Karen
Livescu, and Dan Roth. 2015. From paraphrase
database to compositional paraphrase model and
back. Transactions of the Association for Compu-
tational Linguistics, 3:345–358.
Wei Xu, Alan Ritter, Chris Callison-Burch, William B.
Dolan, and Yangfeng Ji. 2014. Extracting lexi-
cally divergent paraphrases from Twitter. Transac-
tions of the Association for Computational Linguis-
tics, 2:435–448.
Wenpeng Yin and Hinrich Sch¨utze. 2015. Convolu-
tional neural network for paraphrase identification.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 901–911.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014.
ECNU: one stone two birds: ensemble of heteroge-
nous measures for semantic relatedness and textual
entailment. International Workshop on Semantic
Evaluation.
Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over recursive
structures. In Proceedings of the 32nd International
Conference on Machine Learning, pages 1604–
1612.
</reference>
<page confidence="0.992929">
1586
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608425">
<title confidence="0.9997235">Multi-Perspective Sentence Similarity with Convolutional Neural Networks</title>
<author confidence="0.987829">Kevin</author>
<author confidence="0.987829">Jimmy</author>
<affiliation confidence="0.782807">1Department of Computer Science, University of Maryland, College 2Toyota Technological Institute at Chicago R. Cheriton School of Computer Science, University of Waterloo</affiliation>
<abstract confidence="0.9991069">Modeling sentence similarity is complicated by the ambiguity and variability of linguistic expression. To cope with these challenges, we propose a model for comparing sentences that uses a multiplicity of perspectives. We first model each sentence using a convolutional neural network that extracts features at multiple levels of granularity and uses multiple types of pooling. We then compare our sentence representations at several granularities using multiple similarity metrics. We apply our model to three tasks, including the Microsoft Research paraphrase identification task and two SemEval semantic textual similarity tasks. We obtain strong performance on all tasks, rivaling or exceeding the state of the art without using external resources such as WordNet or parsers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 task 6: a pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>385--393</pages>
<contexts>
<context position="2805" citStr="Agirre et al., 2012" startWordPosition="411" endWordPosition="414">eling and similarity measurement. For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonst</context>
<context position="24859" citStr="Agirre et al., 2012" startWordPosition="4071" endWordPosition="4074"> with 4,500 for training, 500 as a development set, and the remaining 4,927 in the test set. The sentences are drawn from image and video descriptions. Each sentence pair is annotated with a relatedness score ∈ [1, 5], with higher scores indicating the two sentences are more closely-related. 4http://hohocode.github.io/textSimilarityConvNet/ � ⊗ 0 Max 0 ⊗ 0 Min � 0 ⊗ Mean � ⊗ 0 Max 0 ⊗ 0 Min � 0 ⊗ Mean 1581 3. Microsoft Video Paraphrase Corpus (MSRVID). This dataset was collected for the 2012 SemEval competition and consists of 1,500 pairs of short video descriptions which were then annotated (Agirre et al., 2012). Half of it is for training and the other half is for testing. Each sentence pair has a relatedness score E [0, 5], with higher scores indicating the two sentences are more closely-related. 6.2 Training We use a hinge loss for the MSRP paraphrase identification task. This is simpler than log loss since it only penalizes misclassified cases. The training objective is to minimize the following loss (summed over examples (x , ygold)): loss(θ, x, ygold) = � max(0, 1 + fe(x, y�) − fe(x, ygold)) (5) yYygold where ygold is the ground truth label, input x is the pair of sentences x = {S1, S21, θ is t</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: a pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Raman Arora</author>
<author>Jeff Bilmes</author>
<author>Karen Livescu</author>
</authors>
<title>Deep canonical correlation analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning,</booktitle>
<pages>1247--1255</pages>
<contexts>
<context position="6448" citStr="Andrew et al., 2013" startWordPosition="955" endWordPosition="959">VM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare local regions. A variety of other neural network models have been proposed for similarity tasks (Weston et al., 2011; Huang et al., 2013; Andrew et al., 2013; Bromley et al., 1993). Most recently, Tai et al. (2015) and Zhu et al. (2015) concurrently proposed a tree-based LSTM neural network architecture for sentence modeling. Unlike them, we do not use syntactic parsers, yet our performance matches Tai et al. (2015) on the similarity task. This result is appealing because high-quality parsers are difficult to obtain for low-resource languages or specialized domains. Yin and Sch¨utze (2015) concurrently developed a convolutional neural network architecture for paraphrase identification, which we compare to in our experiments. Their best results rel</context>
</contexts>
<marker>Andrew, Arora, Bilmes, Livescu, 2013</marker>
<rawString>Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. 2013. Deep canonical correlation analysis. In Proceedings of the 30th International Conference on Machine Learning, pages 1247–1255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Probabilistic soft logic for semantic textual similarity.</title>
<date>2014</date>
<booktitle>Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1210--1219</pages>
<contexts>
<context position="31849" citStr="Beltagy et al. (2014)" startWordPosition="5278" endWordPosition="5281">522 0.7952 0.2850 2-layer LSTM 0.8411 0.7849 0.2980 2-layer Bidirectional LSTM 0.8488 0.7926 0.2893 Tai et al. (2015) Const. LSTM 0.8491 0.7873 0.2852 Tai et al. (2015) Dep. LSTM 0.8676 0.8083 0.2532 This work 0.8686 0.8047 0.2606 Table 2: Test set results on SICK, as reported by Tai et al. (2015), grouped as: (1) RNN variants; (2) SemEval 2014 systems; (3) sequential LSTM variants; (4) dependency and constituency tree LSTMs (Tai et al., 2015). Evaluation metrics are Pearson’s r, Spearman’s p, and mean squared error (MSE). Model Pearson’s r Rios et al. (2012) 0.7060 Wang and Cer (2012) 0.8037 Beltagy et al. (2014) 0.8300 B¨ar et al. (2012) 0.8730 ˇSari´c et al. (2012) 0.8803 This work 0.9090 Table 3: Test set results on MSRVID data. The B¨ar et al. (2012) and ˇSari´c et al. (2012) results were the top two submissions in the Semantic Textual Similarity task at the SemEval-2012 competition. sparse distribution targets, and KL loss function are exactly the same as used by Tai et al. (2015), therefore representing comparable conditions. Results on MSRVID Data. Our results on the MSRVID data are summarized in Table 3, which includes the top 2 submissions in the Semantic Textual Similarity (STS) task from Se</context>
</contexts>
<marker>Beltagy, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Katrin Erk, and Raymond Mooney. 2014. Probabilistic soft logic for semantic textual similarity. Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, pages 1210–1219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Bjerva</author>
<author>Johan Bos</author>
<author>Rob van der Goot</author>
<author>Malvina Nissim</author>
</authors>
<title>The meaning factory: formal semantics for recognizing textual entailment and determining semantic similarity.</title>
<date>2014</date>
<booktitle>International Workshop on Semantic Evaluation.</booktitle>
<marker>Bjerva, Bos, van der Goot, Nissim, 2014</marker>
<rawString>Johannes Bjerva, Johan Bos, Rob van der Goot, and Malvina Nissim. 2014. The meaning factory: formal semantics for recognizing textual entailment and determining semantic similarity. International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<contexts>
<context position="28857" citStr="Blacoe and Lapata (2012)" startWordPosition="4779" endWordPosition="4782">e numFilterB = 20 per dimension for all three datasets, which corresponds to 20 * Dim filters in total. We perform optimization using stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural networkbased approaches. proaches shown in gray rows of the tab</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Online learning and stochastic approximations.</title>
<date>1998</date>
<booktitle>On-line learning in neural networks,</booktitle>
<pages>17--9</pages>
<contexts>
<context position="28409" citStr="Bottou, 1998" startWordPosition="4699" endWordPosition="4700">hese changes were done to speed up our experimental cycle on SICK and MSRVID; on SICK data they are the same experimental settings as used by Tai et al. (2015), which makes for a cleaner empirical comparison. We set the number of holistic filters in blockA to be the same as the input word embeddings, therefore numFilterA = 525 for MSRP and numFilterA = 300 for SICK and MSRVID. We set the number of per-dimension filters in blockB to be numFilterB = 20 per dimension for all three datasets, which corresponds to 20 * Dim filters in total. We perform optimization using stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al.</context>
</contexts>
<marker>Bottou, 1998</marker>
<rawString>L´eon Bottou. 1998. Online learning and stochastic approximations. On-line learning in neural networks, 17(9):142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Bromley</author>
<author>James W Bentz</author>
<author>L´eon Bottou</author>
<author>Isabelle Guyon</author>
<author>Yann LeCun</author>
<author>Cliff Moore</author>
<author>Eduard S¨ackinger</author>
<author>Roopak Shah</author>
</authors>
<title>Signature verification using a “siamese” time delay neural network.</title>
<date>1993</date>
<journal>International Journal of Pattern Recognition and Artificial Intelligence,</journal>
<volume>7</volume>
<issue>4</issue>
<marker>Bromley, Bentz, Bottou, Guyon, LeCun, Moore, S¨ackinger, Shah, 1993</marker>
<rawString>Jane Bromley, James W Bentz, L´eon Bottou, Isabelle Guyon, Yann LeCun, Cliff Moore, Eduard S¨ackinger, and Roopak Shah. 1993. Signature verification using a “siamese” time delay neural network. International Journal of Pattern Recognition and Artificial Intelligence, 7(4):669–688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="4900" citStr="Collobert and Weston (2008)" startWordPosition="721" endWordPosition="724">etween the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neura</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine learning, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>468--476</pages>
<contexts>
<context position="4319" citStr="Das and Smith, 2009" startWordPosition="636" endWordPosition="639"> (1) string-based, including n-gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional</context>
<context position="27155" citStr="Das and Smith, 2009" startWordPosition="4473" endWordPosition="4476">ask since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dimg, = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphrase identification (Das and Smith, 2009). We use POS embeddings only for the MSRP task. Therefore for MSRP, we concatenate all word and POS embeddings and obtain Dim = Dimg + Dimg, + Dimk = 525-dimension vectors for each input word; for SICK and MSRVID we only use Dim = 300-dimension GloVe embeddings. We use 5-fold cross validation on the MSRP training data for tuning, then largely re-use the same hyperparameters for the other two datasets. However, there are two changes: 1) for the MSRP task we update word embeddings during training but not so on SICK and MSRVID tasks; 2) we set the fully connected layer to contain 250 hidden units</context>
<context position="28953" citStr="Das and Smith (2009)" startWordPosition="4795" endWordPosition="4798">al. We perform optimization using stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural networkbased approaches. proaches shown in gray rows of the table are based on neural networks. The recent approach by Yin and Sch¨utze (2015) includes a pretr</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 468–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2949" citStr="Dolan et al., 2004" startWordPosition="432" endWordPosition="435">larities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits from our use of multiple perspectives both in sentence modeling and structured similarity measurement. 2 Related Work Most</context>
<context position="23809" citStr="Dolan et al., 2004" startWordPosition="3896" endWordPosition="3899">nes and “vertical comparison” (Algorithm 2) with red dotted lines. Each sentence representation uses window sizes ws1 and ws2 with max/min/mean pooling and numFilterA = 3 filters. as the activation function for all convolution filters and for the activation layer placed between the final two layers. 6 Experiments and Results Everything necessary to replicate our experimental results can be found in our open-source code repository.4 6.1 Tasks and Datasets We consider three sentence pair similarity tasks: 1. Microsoft Research Paraphrase Corpus (MSRP). This data was collected from news sources (Dolan et al., 2004) and contains 5,801 pairs of sentences, with 4,076 for training and the remaining 1,725 for testing. Each sentence pair is annotated with a binary label indicating whether the two sentences are paraphrases, so the task here is binary classification. 2. Sentences Involving Compositional Knowledge (SICK) dataset. This data was collected for the 2014 SemEval competition (Marelli et al., 2014) and consists of 9,927 sentence pairs, with 4,500 for training, 500 as a development set, and the remaining 4,927 in the test set. The sentences are drawn from image and video descriptions. Each sentence pair</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4179" citStr="Fellbaum, 1998" startWordPosition="617" endWordPosition="618">n modeling sentence similarity has focused on feature engineering. Several types of sparse features have been found useful, including: (1) string-based, including n-gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted fea</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Fern</author>
<author>Mark Stevenson</author>
</authors>
<title>A semantic similarity approach to paraphrase detection.</title>
<date>2008</date>
<booktitle>In Computational Linguistics UK 11th Annual Research Colloquium.</booktitle>
<contexts>
<context position="4206" citStr="Fern and Stevenson, 2008" startWordPosition="619" endWordPosition="622">nce similarity has focused on feature engineering. Several types of sparse features have been found useful, including: (1) string-based, including n-gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling </context>
<context position="28895" citStr="Fern and Stevenson (2008)" startWordPosition="4785" endWordPosition="4788">ll three datasets, which corresponds to 20 * Dim filters in total. We perform optimization using stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural networkbased approaches. proaches shown in gray rows of the table are based on neural networks. The r</context>
</contexts>
<marker>Fern, Stevenson, 2008</marker>
<rawString>Samuel Fern and Mark Stevenson. 2008. A semantic similarity approach to paraphrase detection. In Computational Linguistics UK 11th Annual Research Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
</authors>
<title>Using machine translation evaluation techniques to determine sentence-level semantic equivalence.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Paraphrasing.</booktitle>
<contexts>
<context position="28920" citStr="Finch (2005)" startWordPosition="4791" endWordPosition="4792">o 20 * Dim filters in total. We perform optimization using stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural networkbased approaches. proaches shown in gray rows of the table are based on neural networks. The recent approach by Yin and</context>
</contexts>
<marker>Finch, 2005</marker>
<rawString>Andrew Finch. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Proceedings of the International Workshop on Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: the Paraphrase Database.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: the Paraphrase Database. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Neural Networks,</booktitle>
<pages>347--352</pages>
<contexts>
<context position="28531" citStr="Goller and Kuchler, 1996" startWordPosition="4715" endWordPosition="4719">xperimental settings as used by Tai et al. (2015), which makes for a cleaner empirical comparison. We set the number of holistic filters in blockA to be the same as the input word embeddings, therefore numFilterA = 525 for MSRP and numFilterA = 300 for SICK and MSRVID. We set the number of per-dimension filters in blockB to be numFilterB = 20 per dimension for all three datasets, which corresponds to 20 * Dim filters in total. We perform optimization using stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5%</context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Proceedings of the International Conference on Neural Networks, pages 347–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>864--872</pages>
<contexts>
<context position="4453" citStr="Guo and Diab, 2012" startWordPosition="657" endWordPosition="660">hine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 864–872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
</authors>
<title>Measuring Semantic Relatedness Using Salient Encyclopedic Concepts.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>University of North Texas,</institution>
<location>Denton, Texas, USA.</location>
<contexts>
<context position="4432" citStr="Hassan, 2011" startWordPosition="655" endWordPosition="656">s based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared </context>
</contexts>
<marker>Hassan, 2011</marker>
<rawString>Samer Hassan. 2011. Measuring Semantic Relatedness Using Salient Encyclopedic Concepts. Ph.D. thesis, University of North Texas, Denton, Texas, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2042--2050</pages>
<contexts>
<context position="6014" citStr="Hu et al. (2014)" startWordPosition="892" endWordPosition="895">indow sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse features using an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare local regions. A variety of other neural network models have been proposed for similarity tasks (Weston et al., 2011; Huang et al., 2013; Andrew et al., 2013; Bromley et al., 1993). Most recently, Tai et al. (2015) and Zhu et al. (2015) concurrently proposed a tree-based LSTM neural network architecture for sentence model</context>
<context position="28778" citStr="Hu et al. (2014)" startWordPosition="4765" endWordPosition="4768">K and MSRVID. We set the number of per-dimension filters in blockB to be numFilterB = 20 per dimension for all three datasets, which corresponds to 20 * Dim filters in total. We perform optimization using stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in </context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems, pages 2042–2050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Po-Sen Huang</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Alex Acero</author>
<author>Larry Heck</author>
</authors>
<title>Learning deep structured semantic models for web search using clickthrough data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management,</booktitle>
<pages>2333--2338</pages>
<contexts>
<context position="6427" citStr="Huang et al., 2013" startWordPosition="951" endWordPosition="954"> features using an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare local regions. A variety of other neural network models have been proposed for similarity tasks (Weston et al., 2011; Huang et al., 2013; Andrew et al., 2013; Bromley et al., 1993). Most recently, Tai et al. (2015) and Zhu et al. (2015) concurrently proposed a tree-based LSTM neural network architecture for sentence modeling. Unlike them, we do not use syntactic parsers, yet our performance matches Tai et al. (2015) on the similarity task. This result is appealing because high-quality parsers are difficult to obtain for low-resource languages or specialized domains. Yin and Sch¨utze (2015) concurrently developed a convolutional neural network architecture for paraphrase identification, which we compare to in our experiments. T</context>
</contexts>
<marker>Huang, He, Gao, Deng, Acero, Heck, 2013</marker>
<rawString>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management, pages 2333–2338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Discriminative improvements to distributional sentence similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods for Natural Language Processing,</booktitle>
<pages>891--896</pages>
<contexts>
<context position="5695" citStr="Ji and Eisenstein (2013)" startWordPosition="842" endWordPosition="845">roduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse features using an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare loca</context>
<context position="29087" citStr="Ji and Eisenstein (2013)" startWordPosition="4819" endWordPosition="4822">dients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural networkbased approaches. proaches shown in gray rows of the table are based on neural networks. The recent approach by Yin and Sch¨utze (2015) includes a pretraining technique which significantly improves results, as shown in the table. We do not use any pretraining but still slightly outperf</context>
</contexts>
<marker>Ji, Eisenstein, 2013</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2013. Discriminative improvements to distributional sentence similarity. In Proceedings of the 2013 Conference on Empirical Methods for Natural Language Processing, pages 891–896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>George Duenas</author>
<author>Julia Baquero</author>
<author>Alexander Gelbukh</author>
<author>Av Juan Dios B´atiz</author>
<author>Av Mendiz´abal</author>
</authors>
<title>UNAL-NLP: combining soft cardinality features for semantic textual similarity, relatedness and entailment.</title>
<date>2014</date>
<booktitle>International Workshop on Semantic Evaluation.</booktitle>
<marker>Jimenez, Duenas, Baquero, Gelbukh, B´atiz, Mendiz´abal, 2014</marker>
<rawString>Sergio Jimenez, George Duenas, Julia Baquero, Alexander Gelbukh, Av Juan Dios B´atiz, and Av Mendiz´abal. 2014. UNAL-NLP: combining soft cardinality features for semantic textual similarity, relatedness and entailment. International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5067" citStr="Kalchbrenner et al. (2014)" startWordPosition="747" endWordPosition="750"> Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent par</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="5215" citStr="Kim (2014)" startWordPosition="772" endWordPosition="773">veloped a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse featu</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Lai</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Illinois-LH: a denotational and distributional approach to semantics.</title>
<date>2014</date>
<booktitle>International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="31054" citStr="Lai and Hockenmaier (2014)" startWordPosition="5145" endWordPosition="5148">rature as reported by Tai et al. (2015), including prior work using recurrent neural networks (RNNs), the best submissions in the SemEval-2014 competition, and variants of LSTMs. When measured by Pearson’s r, the previous state-of-the-art approach uses a treestructured LSTM (Tai et al., 2015); note that their best results require a dependency parser. On the contrary, our approach does not rely on parse trees, nor do we use POS/PARAGRAM embeddings for this task. The word embeddings, Model r ρ MSE Socher et al. (2014) DT-RNN 0.7863 0.7305 0.3983 Socher et al. (2014) SDT-RNN 0.7886 0.7280 0.3859 Lai and Hockenmaier (2014) 0.7993 0.7538 0.3692 Jimenez et al. (2014) 0.8070 0.7489 0.3550 Bjerva et al. (2014) 0.8268 0.7721 0.3224 Zhao et al. (2014) 0.8414 - - LSTM 0.8477 0.7921 0.2949 Bi-LSTM 0.8522 0.7952 0.2850 2-layer LSTM 0.8411 0.7849 0.2980 2-layer Bidirectional LSTM 0.8488 0.7926 0.2893 Tai et al. (2015) Const. LSTM 0.8491 0.7873 0.2852 Tai et al. (2015) Dep. LSTM 0.8676 0.8083 0.2532 This work 0.8686 0.8047 0.2606 Table 2: Test set results on SICK, as reported by Tai et al. (2015), grouped as: (1) RNN variants; (2) SemEval 2014 systems; (3) sequential LSTM variants; (4) dependency and constituency tree LST</context>
</contexts>
<marker>Lai, Hockenmaier, 2014</marker>
<rawString>Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: a denotational and distributional approach to semantics. International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="4561" citStr="Madnani et al., 2012" startWordPosition="671" endWordPosition="675">in Natural Language Processing, pages 1576–1586, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooli</context>
<context position="29050" citStr="Madnani et al. (2012)" startWordPosition="4813" endWordPosition="4816">n algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural networkbased approaches. proaches shown in gray rows of the table are based on neural networks. The recent approach by Yin and Sch¨utze (2015) includes a pretraining technique which significantly improves results, as shown in the table. We do not use any p</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="26752" citStr="Manning et al., 2014" startWordPosition="4409" endWordPosition="4412">We conduct experiments with ws values in the range [1, 3] as well as ws = oc (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dimg = 300- dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dimk = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dimg, = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphrase identification (Das and Smith, 2009). We use POS embeddings only for the MSRP task. Therefore for MSRP, we concatenate all word and POS embeddings and obtain Dim = Dimg + Dimg, + Dimk = 525-dimension vectors for each input word; for </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 task 1: evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="2828" citStr="Marelli et al., 2014" startWordPosition="415" endWordPosition="418">measurement. For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits f</context>
<context position="24201" citStr="Marelli et al., 2014" startWordPosition="3956" endWordPosition="3959">be found in our open-source code repository.4 6.1 Tasks and Datasets We consider three sentence pair similarity tasks: 1. Microsoft Research Paraphrase Corpus (MSRP). This data was collected from news sources (Dolan et al., 2004) and contains 5,801 pairs of sentences, with 4,076 for training and the remaining 1,725 for testing. Each sentence pair is annotated with a binary label indicating whether the two sentences are paraphrases, so the task here is binary classification. 2. Sentences Involving Compositional Knowledge (SICK) dataset. This data was collected for the 2014 SemEval competition (Marelli et al., 2014) and consists of 9,927 sentence pairs, with 4,500 for training, 500 as a development set, and the remaining 4,927 in the test set. The sentences are drawn from image and video descriptions. Each sentence pair is annotated with a relatedness score ∈ [1, 5], with higher scores indicating the two sentences are more closely-related. 4http://hohocode.github.io/textSimilarityConvNet/ � ⊗ 0 Max 0 ⊗ 0 Min � 0 ⊗ Mean � ⊗ 0 Max 0 ⊗ 0 Min � 0 ⊗ Mean 1581 3. Microsoft Video Paraphrase Corpus (MSRVID). This dataset was collected for the 2012 SemEval competition and consists of 1,500 pairs of short video de</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. SemEval-2014 task 1: evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>Proceedings of Workshop at International Conference on Learning Representations.</booktitle>
<contexts>
<context position="27002" citStr="Mikolov et al., 2013" startWordPosition="4449" endWordPosition="4452">mbeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dimk = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dimg, = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphrase identification (Das and Smith, 2009). We use POS embeddings only for the MSRP task. Therefore for MSRP, we concatenate all word and POS embeddings and obtain Dim = Dimg + Dimg, + Dimk = 525-dimension vectors for each input word; for SICK and MSRVID we only use Dim = 300-dimension GloVe embeddings. We use 5-fold cross validation on the MSRP training data for tuning, then largely re-use the same hyperparameters for the other two datasets. However, there are two changes: 1) for the</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. Proceedings of Workshop at International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1532--1543</pages>
<contexts>
<context position="26416" citStr="Pennington et al., 2014" startWordPosition="4351" endWordPosition="4354">: � � KL fk ||�fk+ λ 2||θ||2 2 (6) e where fe is the predicted distribution with model weight vector θ, f is the ground truth, m is the number of training examples, and λ is the regularization parameter. Note that we use the same KL-loss function and same sparse target distribution technique as Tai et al. (2015). 6.3 Experiment Settings We conduct experiments with ws values in the range [1, 3] as well as ws = oc (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dimg = 300- dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dimk = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dimg, = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS e</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1532–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Rennie</author>
<author>Vaibhava Goel</author>
<author>Samuel Thomas</author>
</authors>
<title>Deep order statistic networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the IEEE Workshop on Spoken Language Technology.</booktitle>
<contexts>
<context position="14392" citStr="Rennie et al., 2014" startWordPosition="2285" endWordPosition="2289">oup, denoted group(ws, pooling, sent), is an object that contains a convolution layer with width ws, uses pooling function pooling, and operates on sentence sent. We define a building block to be a set of groups. We use two types of building blocks, blockA and blockB, as shown in Figure 3. We define blockA as {groupA(wsa,p, sent) : p E {max, min, mean}}. That is, an instance of blockA has three convolution layers, one corresponding to each of the three pooling functions; all have the same window size wsa. An alternative choice would be to use the multiple types of pooling on the same filters (Rennie et al., 2014); we instead use independent sets of filters for the different pooling types.1 We use blocks of type A for all holistic convolution layers. We define blockB as {groupB(wsb, p, sent) : p E {max, min}}. That is, blockB contains two groups of convolution layers of width wsb, one with max-pooling and one with min-pooling. Each groupB(*) contains a convolution layer with Dim per-dimension convolution filters. That is, we use blocks of type B for convolution layers that operate on individual dimensions of word vectors. We use these multiple types of pooling to extract different types of information </context>
</contexts>
<marker>Rennie, Goel, Thomas, 2014</marker>
<rawString>Steven Rennie, Vaibhava Goel, and Samuel Thomas. 2014. Deep order statistic networks. In Proceedings of the IEEE Workshop on Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Rios</author>
<author>Wilker Aziz</author>
<author>Lucia Specia</author>
</authors>
<title>UOW: semantically informed text similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>673--678</pages>
<contexts>
<context position="31793" citStr="Rios et al. (2012)" startWordPosition="5268" endWordPosition="5271">014) 0.8414 - - LSTM 0.8477 0.7921 0.2949 Bi-LSTM 0.8522 0.7952 0.2850 2-layer LSTM 0.8411 0.7849 0.2980 2-layer Bidirectional LSTM 0.8488 0.7926 0.2893 Tai et al. (2015) Const. LSTM 0.8491 0.7873 0.2852 Tai et al. (2015) Dep. LSTM 0.8676 0.8083 0.2532 This work 0.8686 0.8047 0.2606 Table 2: Test set results on SICK, as reported by Tai et al. (2015), grouped as: (1) RNN variants; (2) SemEval 2014 systems; (3) sequential LSTM variants; (4) dependency and constituency tree LSTMs (Tai et al., 2015). Evaluation metrics are Pearson’s r, Spearman’s p, and mean squared error (MSE). Model Pearson’s r Rios et al. (2012) 0.7060 Wang and Cer (2012) 0.8037 Beltagy et al. (2014) 0.8300 B¨ar et al. (2012) 0.8730 ˇSari´c et al. (2012) 0.8803 This work 0.9090 Table 3: Test set results on MSRVID data. The B¨ar et al. (2012) and ˇSari´c et al. (2012) results were the top two submissions in the Semantic Textual Similarity task at the SemEval-2012 competition. sparse distribution targets, and KL loss function are exactly the same as used by Tai et al. (2015), therefore representing comparable conditions. Results on MSRVID Data. Our results on the MSRVID data are summarized in Table 3, which includes the top 2 submissio</context>
</contexts>
<marker>Rios, Aziz, Specia, 2012</marker>
<rawString>Miguel Rios, Wilker Aziz, and Lucia Specia. 2012. UOW: semantically informed text similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 673–678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Sari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>441--448</pages>
<marker>Sari´c, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. TakeLab: systems for measuring semantic text similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5477" citStr="Socher et al. (2011)" startWordPosition="810" endWordPosition="813"> architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse features using an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical s</context>
<context position="29016" citStr="Socher et al. (2011)" startWordPosition="4807" endWordPosition="4810">Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural networkbased approaches. proaches shown in gray rows of the table are based on neural networks. The recent approach by Yin and Sch¨utze (2015) includes a pretraining technique which significantly improves results, as shown</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--207</pages>
<contexts>
<context position="30949" citStr="Socher et al. (2014)" startWordPosition="5129" endWordPosition="5132">, showing Pearson’s r, Spearman’s p, and mean squared error (MSE). We include results from the literature as reported by Tai et al. (2015), including prior work using recurrent neural networks (RNNs), the best submissions in the SemEval-2014 competition, and variants of LSTMs. When measured by Pearson’s r, the previous state-of-the-art approach uses a treestructured LSTM (Tai et al., 2015); note that their best results require a dependency parser. On the contrary, our approach does not rely on parse trees, nor do we use POS/PARAGRAM embeddings for this task. The word embeddings, Model r ρ MSE Socher et al. (2014) DT-RNN 0.7863 0.7305 0.3983 Socher et al. (2014) SDT-RNN 0.7886 0.7280 0.3859 Lai and Hockenmaier (2014) 0.7993 0.7538 0.3692 Jimenez et al. (2014) 0.8070 0.7489 0.3550 Bjerva et al. (2014) 0.8268 0.7721 0.3224 Zhao et al. (2014) 0.8414 - - LSTM 0.8477 0.7921 0.2949 Bi-LSTM 0.8522 0.7952 0.2850 2-layer LSTM 0.8411 0.7849 0.2980 2-layer Bidirectional LSTM 0.8488 0.7926 0.2893 Tai et al. (2015) Const. LSTM 0.8491 0.7873 0.2852 Tai et al. (2015) Dep. LSTM 0.8676 0.8083 0.2532 This work 0.8686 0.8047 0.2606 Table 2: Test set results on SICK, as reported by Tai et al. (2015), grouped as: (1) RNN v</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2034" citStr="Tai et al., 2015" startWordPosition="300" endWordPosition="303">n a query sentence S1 and a comparison sentence S2, the task is to compute their similarity in terms of a score sim(S1, S2). This similarity score can be used within a system that determines whether two sentences are paraphrases, e.g., by comparing it to a threshold. Measuring sentence similarity is challenging because of the variability of linguistic expression and the limited amount of annotated training data. This makes it difficult to use sparse, hand-crafted features as in conventional approaches in NLP. Recent successes in sentence similarity have been obtained by using neural networks (Tai et al., 2015; Yin and Sch¨utze, 2015). Our approach is also based on neural networks: we propose a modular functional architecture with two components, sentence modeling and similarity measurement. For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine dis</context>
<context position="6505" citStr="Tai et al. (2015)" startWordPosition="966" endWordPosition="969"> and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare local regions. A variety of other neural network models have been proposed for similarity tasks (Weston et al., 2011; Huang et al., 2013; Andrew et al., 2013; Bromley et al., 1993). Most recently, Tai et al. (2015) and Zhu et al. (2015) concurrently proposed a tree-based LSTM neural network architecture for sentence modeling. Unlike them, we do not use syntactic parsers, yet our performance matches Tai et al. (2015) on the similarity task. This result is appealing because high-quality parsers are difficult to obtain for low-resource languages or specialized domains. Yin and Sch¨utze (2015) concurrently developed a convolutional neural network architecture for paraphrase identification, which we compare to in our experiments. Their best results rely on an unsupervised pretraining step, which we do not ne</context>
<context position="26105" citStr="Tai et al. (2015)" startWordPosition="4296" endWordPosition="4299">trained, and the function fe(x, y) is the output of our model. We use regularized KL-divergence loss for the semantic relatedness tasks (SICK and MSRVID), since the goal is to predict the similarity of the two sentences. The training objective is to minimize the KL-divergence loss plus an L2 regularizer: � � KL fk ||�fk+ λ 2||θ||2 2 (6) e where fe is the predicted distribution with model weight vector θ, f is the ground truth, m is the number of training examples, and λ is the regularization parameter. Note that we use the same KL-loss function and same sparse target distribution technique as Tai et al. (2015). 6.3 Experiment Settings We conduct experiments with ws values in the range [1, 3] as well as ws = oc (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dimg = 300- dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dimk = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run</context>
<context position="27955" citStr="Tai et al. (2015)" startWordPosition="4618" endWordPosition="4621">put word; for SICK and MSRVID we only use Dim = 300-dimension GloVe embeddings. We use 5-fold cross validation on the MSRP training data for tuning, then largely re-use the same hyperparameters for the other two datasets. However, there are two changes: 1) for the MSRP task we update word embeddings during training but not so on SICK and MSRVID tasks; 2) we set the fully connected layer to contain 250 hidden units for MSRP, and 150 for SICK and MSRVID. These changes were done to speed up our experimental cycle on SICK and MSRVID; on SICK data they are the same experimental settings as used by Tai et al. (2015), which makes for a cleaner empirical comparison. We set the number of holistic filters in blockA to be the same as the input word embeddings, therefore numFilterA = 525 for MSRP and numFilterA = 300 for SICK and MSRVID. We set the number of per-dimension filters in blockB to be numFilterB = 20 per dimension for all three datasets, which corresponds to 20 * Dim filters in total. We perform optimization using stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning ra</context>
<context position="30467" citStr="Tai et al. (2015)" startWordPosition="5048" endWordPosition="5051">outperform them by 6% absolute in accuracy and 3% in F1. Our model is also superior to other recent neural network models (Hu et al., 2014; Socher et al., 2011) without requiring sparse features or unlabeled data as in (Yin and Sch¨utze, 2015; Socher et al., 2011). The best result on MSRP is from Ji and Eisenstein (2013) which uses unsupervised learning on the MSRP test set and rich sparse features. Results on SICK Data. Our results on the SICK task are summarized in Table 2, showing Pearson’s r, Spearman’s p, and mean squared error (MSE). We include results from the literature as reported by Tai et al. (2015), including prior work using recurrent neural networks (RNNs), the best submissions in the SemEval-2014 competition, and variants of LSTMs. When measured by Pearson’s r, the previous state-of-the-art approach uses a treestructured LSTM (Tai et al., 2015); note that their best results require a dependency parser. On the contrary, our approach does not rely on parse trees, nor do we use POS/PARAGRAM embeddings for this task. The word embeddings, Model r ρ MSE Socher et al. (2014) DT-RNN 0.7863 0.7305 0.3983 Socher et al. (2014) SDT-RNN 0.7886 0.7280 0.3859 Lai and Hockenmaier (2014) 0.7993 0.753</context>
<context position="32229" citStr="Tai et al. (2015)" startWordPosition="5344" endWordPosition="5347">; (4) dependency and constituency tree LSTMs (Tai et al., 2015). Evaluation metrics are Pearson’s r, Spearman’s p, and mean squared error (MSE). Model Pearson’s r Rios et al. (2012) 0.7060 Wang and Cer (2012) 0.8037 Beltagy et al. (2014) 0.8300 B¨ar et al. (2012) 0.8730 ˇSari´c et al. (2012) 0.8803 This work 0.9090 Table 3: Test set results on MSRVID data. The B¨ar et al. (2012) and ˇSari´c et al. (2012) results were the top two submissions in the Semantic Textual Similarity task at the SemEval-2012 competition. sparse distribution targets, and KL loss function are exactly the same as used by Tai et al. (2015), therefore representing comparable conditions. Results on MSRVID Data. Our results on the MSRVID data are summarized in Table 3, which includes the top 2 submissions in the Semantic Textual Similarity (STS) task from SemEval2012. We find that we outperform the top system from the task by nearly 3 points in Pearson’s r. 6.5 Model Ablation Study We report the results of an ablation study in Table 4. We identify nine major components of our approach, remove one at a time (if applicable), and perform re-training and re-testing for all three tasks. We use the same experimental settings in Sec. 6.3</context>
<context position="35344" citStr="Tai et al., 2015" startWordPosition="5840" endWordPosition="5843"> We see consistent drops when ablating the Vertical/Horizontal algorithms that target particular regions for comparison. Also, removing group (3) hinders both the Horizontal and Vertical algorithms (as described in Section 5.1), so its removal similarly causes large drops in performance. Though convolutional neural networks already perform strongly when followed by flattened vector comparison, we are able to leverage the full richness of the sentence models by performing structured similarity modeling on their outputs. 7 Discussion and Conclusion On the SICK dataset, the dependency tree LSTM (Tai et al., 2015) and our model achieve comparable performance despite taking very different approaches. Tai et al. use syntactic parse trees and gating mechanisms to convert each sentence into a vector, while we use large sets of flexible feature extractors in the form of convolution filters, then compare particular subsets of features in our similarity measurement layer. Our model architecture, with its many paths of information flow, is admittedly complex. Though we have removed hand engineering of features, we have added a substantial amount of functional architecture engineering. This may be necessary whe</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>Cecile Paris</author>
</authors>
<title>Using Dependency-based Features to Take the ”Para-farce” out of Paraphrase.</title>
<date>2006</date>
<booktitle>In Australasian Language Technology Workshop,</booktitle>
<pages>131--138</pages>
<contexts>
<context position="3808" citStr="Wan et al., 2006" startWordPosition="566" endWordPosition="569">convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits from our use of multiple perspectives both in sentence modeling and structured similarity measurement. 2 Related Work Most previous work on modeling sentence similarity has focused on feature engineering. Several types of sparse features have been found useful, including: (1) string-based, including n-gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtai</context>
<context position="28983" citStr="Wan et al. (2006)" startWordPosition="4801" endWordPosition="4804"> stochastic gradient descent (Bottou, 1998). The backpropagation algorithm is used to compute gradients for all parameters during training (Goller and Kuchler, 1996). We fix the learning rate to 0.01 and regularization parameter λ = 10−4. 6.4 Results on Three Datasets Results on MSRP Data. We report F1 scores and accuracies from prior work in Table 1. Ap1 loss(θ) = m �m k=1 1582 Model Acc. F1 Hu et al. (2014) ARC-I 69.6% 80.3% Hu et al. (2014) ARC-II 69.9% 80.9% Blacoe and Lapata (2012) 73.0% 82.3% Fern and Stevenson (2008) 74.1% 82.4% Finch (2005) 75.0% 82.7% Das and Smith (2009) 76.1% 82.7% Wan et al. (2006) 75.6% 83.0% Socher et al. (2011) 76.8% 83.6% Madnani et al. (2012) 77.4% 84.1% Ji and Eisenstein (2013) 80.41% 85.96% Yin and Sch¨utze (2015) 72.5% 81.4% (without pretraining) Yin and Sch¨utze (2015) 78.1% 84.4% (with pretraining) Yin and Sch¨utze (2015) 78.4% 84.6% (pretraining+sparse features) This work 78.60% 84.73% Table 1: Test set results on MSRP for paraphrase identification. Rows in grey are neural networkbased approaches. proaches shown in gray rows of the table are based on neural networks. The recent approach by Yin and Sch¨utze (2015) includes a pretraining technique which signifi</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and Cecile Paris. 2006. Using Dependency-based Features to Take the ”Para-farce” out of Paraphrase. In Australasian Language Technology Workshop, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Daniel Cer</author>
</authors>
<title>Probabilistic edit distance metrics for STS.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>648--654</pages>
<contexts>
<context position="31820" citStr="Wang and Cer (2012)" startWordPosition="5273" endWordPosition="5276">7 0.7921 0.2949 Bi-LSTM 0.8522 0.7952 0.2850 2-layer LSTM 0.8411 0.7849 0.2980 2-layer Bidirectional LSTM 0.8488 0.7926 0.2893 Tai et al. (2015) Const. LSTM 0.8491 0.7873 0.2852 Tai et al. (2015) Dep. LSTM 0.8676 0.8083 0.2532 This work 0.8686 0.8047 0.2606 Table 2: Test set results on SICK, as reported by Tai et al. (2015), grouped as: (1) RNN variants; (2) SemEval 2014 systems; (3) sequential LSTM variants; (4) dependency and constituency tree LSTMs (Tai et al., 2015). Evaluation metrics are Pearson’s r, Spearman’s p, and mean squared error (MSE). Model Pearson’s r Rios et al. (2012) 0.7060 Wang and Cer (2012) 0.8037 Beltagy et al. (2014) 0.8300 B¨ar et al. (2012) 0.8730 ˇSari´c et al. (2012) 0.8803 This work 0.9090 Table 3: Test set results on MSRVID data. The B¨ar et al. (2012) and ˇSari´c et al. (2012) results were the top two submissions in the Semantic Textual Similarity task at the SemEval-2012 competition. sparse distribution targets, and KL loss function are exactly the same as used by Tai et al. (2015), therefore representing comparable conditions. Results on MSRVID Data. Our results on the MSRVID data are summarized in Table 3, which includes the top 2 submissions in the Semantic Textual </context>
</contexts>
<marker>Wang, Cer, 2012</marker>
<rawString>Mengqiu Wang and Daniel Cer. 2012. Probabilistic edit distance metrics for STS. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 648–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Wsabie: scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2764--2770</pages>
<contexts>
<context position="6407" citStr="Weston et al., 2011" startWordPosition="947" endWordPosition="950">ith fine-tuned sparse features using an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare local regions. A variety of other neural network models have been proposed for similarity tasks (Weston et al., 2011; Huang et al., 2013; Andrew et al., 2013; Bromley et al., 1993). Most recently, Tai et al. (2015) and Zhu et al. (2015) concurrently proposed a tree-based LSTM neural network architecture for sentence modeling. Unlike them, we do not use syntactic parsers, yet our performance matches Tai et al. (2015) on the similarity task. This result is appealing because high-quality parsers are difficult to obtain for low-resource languages or specialized domains. Yin and Sch¨utze (2015) concurrently developed a convolutional neural network architecture for paraphrase identification, which we compare to i</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: scaling up to large vocabulary image annotation. In International Joint Conference on Artificial Intelligence, pages 2764–2770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Wieting</author>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
<author>Dan Roth</author>
</authors>
<title>From paraphrase database to compositional paraphrase model and back.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--345</pages>
<contexts>
<context position="26516" citStr="Wieting et al., 2015" startWordPosition="4368" endWordPosition="4371"> f is the ground truth, m is the number of training examples, and λ is the regularization parameter. Note that we use the same KL-loss function and same sparse target distribution technique as Tai et al. (2015). 6.3 Experiment Settings We conduct experiments with ws values in the range [1, 3] as well as ws = oc (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dimg = 300- dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dimk = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dimg, = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphra</context>
</contexts>
<marker>Wieting, Bansal, Gimpel, Livescu, Roth, 2015</marker>
<rawString>John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu, and Dan Roth. 2015. From paraphrase database to compositional paraphrase model and back. Transactions of the Association for Computational Linguistics, 3:345–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
<author>Yangfeng Ji</author>
</authors>
<title>Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--435</pages>
<contexts>
<context position="4602" citStr="Xu et al. (2014)" startWordPosition="679" endWordPosition="682">6, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying size</context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. 2014. Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics, 2:435–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenpeng Yin</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Convolutional neural network for paraphrase identification.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>901--911</pages>
<marker>Yin, Sch¨utze, 2015</marker>
<rawString>Wenpeng Yin and Hinrich Sch¨utze. 2015. Convolutional neural network for paraphrase identification. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 901–911.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Tian Tian Zhu</author>
<author>Man Lan</author>
</authors>
<title>ECNU: one stone two birds: ensemble of heterogenous measures for semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="31179" citStr="Zhao et al. (2014)" startWordPosition="5166" endWordPosition="5169">Eval-2014 competition, and variants of LSTMs. When measured by Pearson’s r, the previous state-of-the-art approach uses a treestructured LSTM (Tai et al., 2015); note that their best results require a dependency parser. On the contrary, our approach does not rely on parse trees, nor do we use POS/PARAGRAM embeddings for this task. The word embeddings, Model r ρ MSE Socher et al. (2014) DT-RNN 0.7863 0.7305 0.3983 Socher et al. (2014) SDT-RNN 0.7886 0.7280 0.3859 Lai and Hockenmaier (2014) 0.7993 0.7538 0.3692 Jimenez et al. (2014) 0.8070 0.7489 0.3550 Bjerva et al. (2014) 0.8268 0.7721 0.3224 Zhao et al. (2014) 0.8414 - - LSTM 0.8477 0.7921 0.2949 Bi-LSTM 0.8522 0.7952 0.2850 2-layer LSTM 0.8411 0.7849 0.2980 2-layer Bidirectional LSTM 0.8488 0.7926 0.2893 Tai et al. (2015) Const. LSTM 0.8491 0.7873 0.2852 Tai et al. (2015) Dep. LSTM 0.8676 0.8083 0.2532 This work 0.8686 0.8047 0.2606 Table 2: Test set results on SICK, as reported by Tai et al. (2015), grouped as: (1) RNN variants; (2) SemEval 2014 systems; (3) sequential LSTM variants; (4) dependency and constituency tree LSTMs (Tai et al., 2015). Evaluation metrics are Pearson’s r, Spearman’s p, and mean squared error (MSE). Model Pearson’s r Rios</context>
</contexts>
<marker>Zhao, Zhu, Lan, 2014</marker>
<rawString>Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. ECNU: one stone two birds: ensemble of heterogenous measures for semantic relatedness and textual entailment. International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Parinaz Sobhani</author>
<author>Hongyu Guo</author>
</authors>
<title>Long short-term memory over recursive structures.</title>
<date>2015</date>
<booktitle>In Proceedings of the 32nd International Conference on Machine Learning,</booktitle>
<pages>1604--1612</pages>
<contexts>
<context position="6527" citStr="Zhu et al. (2015)" startWordPosition="971" endWordPosition="974"> incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare local regions. A variety of other neural network models have been proposed for similarity tasks (Weston et al., 2011; Huang et al., 2013; Andrew et al., 2013; Bromley et al., 1993). Most recently, Tai et al. (2015) and Zhu et al. (2015) concurrently proposed a tree-based LSTM neural network architecture for sentence modeling. Unlike them, we do not use syntactic parsers, yet our performance matches Tai et al. (2015) on the similarity task. This result is appealing because high-quality parsers are difficult to obtain for low-resource languages or specialized domains. Yin and Sch¨utze (2015) concurrently developed a convolutional neural network architecture for paraphrase identification, which we compare to in our experiments. Their best results rely on an unsupervised pretraining step, which we do not need to match their perf</context>
</contexts>
<marker>Zhu, Sobhani, Guo, 2015</marker>
<rawString>Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over recursive structures. In Proceedings of the 32nd International Conference on Machine Learning, pages 1604– 1612.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000502">
<title confidence="0.932672">
RELLY: Inferring Hypernym Relationships Between Relational Phrases
</title>
<author confidence="0.973079">
Adam Grycner, Gerhard Weikum
</author>
<affiliation confidence="0.911064">
Max-Planck Institute for Informatics
</affiliation>
<address confidence="0.737555333333333">
Campus E1.4, 66123
Saarbr¨ucken, Germany
{agrycner, weikum}
</address>
<email confidence="0.722738">
@mpi-inf.mpg.de
</email>
<author confidence="0.998071">
Jay Pujara, James Foulds, Lise Getoor
</author>
<affiliation confidence="0.9989555">
Computer Science Department
University of California, Santa Cruz
</affiliation>
<address confidence="0.946008">
Santa Cruz, CA 95064
</address>
<email confidence="0.7257505">
{jpujara,jfoulds,getoor}
@soe.ucsc.edu
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869482758621">
Relational phrases (e.g., “got married to”)
and their hypernyms (e.g., “is a relative
of”) are central for many tasks including
question answering, open information ex-
traction, paraphrasing, and entailment de-
tection. This has motivated the develop-
ment of several linguistic resources (e.g.
DIRT, PATTY, and WiseNet) which sys-
tematically collect and organize relational
phrases. These resources have demonstra-
ble practical benefits, but are each limited
due to noise, sparsity, or size. We present a
new general-purpose method, RELLY, for
constructing a large hypernymy graph of
relational phrases with high-quality sub-
sumptions using collective probabilistic
programming techniques. Our graph in-
duction approach integrates small high-
precision knowledge bases together with
large automatically curated resources, and
reasons collectively to combine these re-
sources into a consistent graph. Using
RELLY, we construct a high-coverage,
high-precision hypernymy graph consist-
ing of 20K relational phrases and 35K hy-
pernymy links. Our evaluation indicates
a hypernymy link precision of 78%, and
demonstrates the value of this resource for
a document-relevance ranking task.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986387755102">
One of the many challenges in natural lan-
guage understanding is interpreting the multi-
word phrases that denote relationships between
entities. Semantically organizing the complex re-
lationships between diverse phrases is crucial to
applications including question answering, open
information extraction, paraphrasing, and entail-
ment detection (Yahya et al., 2012; Fader et al.,
2011; Madnani et al., 2012; Dagan et al., 2005).
For example, a corpus containing the phrase
“George Burns was married to Gracie Allen” al-
lows us to answer the query “Who was the spouse
of George Burns?” However, “Jay Z is in a re-
lationship with Beyonc´e” provides insufficient in-
formation to determine whether the couple is mar-
ried. To capture the knowledge found in text, rela-
tional phrases need to be systematically organized
with lexical links like synonymy (“married to” and
“spouse of”) and hypernymy (“in a relationship”
generalizing “married to”).
Many projects address the challenge of under-
standing relational phrases, but existing linguis-
tic resources are often limited to synonymy, suffer
from low precision, or have low coverage. Sys-
tems such as DIRT (Lin and Pantel, 2001), RE-
SOLVER (Yates and Etzioni, 2009), and WiseNet
(Moro and Navigli, 2012) have used sophisticated
clustering techniques to determine synonymous
phrases, but do not provide subsumption informa-
tion. The PATTY (Nakashole et al., 2012) project
goes beyond clustering and introduces a subsump-
tion hierarchy, but suffers from sparsity and con-
tains few hypernymy links. The HARPY (Gryc-
ner and Weikum, 2014) project extended PATTY,
generating 600K hypernymy links, but with low
precision. Berant et al. (2011) introduced en-
tailment graphs that provided a high-quality sub-
sumption hierarchy. This method required parti-
tioning the graph and the largest component con-
sisted of 120 relations. A number of manually-
curated relational taxonomies such as WordNet
(Fellbaum, 1998), VerbNet (Kipper et al., 2008),
and FrameNet (Baker et al., 1998) also offer high-
precision hierarchies with limited coverage.
In this paper, we introduce RELLY, a method
for producing a hypernymy graph that has both
high coverage and precision. We build on pre-
vious work, integrating the high-precision knowl-
edge in resources such as YAGO (Suchanek et al.,
</bodyText>
<page confidence="0.971791">
971
</page>
<note confidence="0.9850835">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 971–981,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999882448979592">
2007) and WordNet with noisy statistical informa-
tion from OpenIE projects PATTY and HARPY.
RELLY maintains a consistent graph by includ-
ing collective global constraints such as transitiv-
ity, asymmetry, and acyclicity. Scalability is of-
ten a concern when employing collective reason-
ing over large corpora, but our system can pro-
duce graphs with over 100K edges on conven-
tional hardware. As a result, we produce a large,
complete, and high-precision hypernym graph that
includes alignments and type information.
RELLY leverages probabilistic soft logic (PSL)
(Bach et al., 2015), a popular probabilistic mod-
eling framework, to collectively infer hypernymy
links at scale. PSL uses continuously-valued vari-
ables and evidence, allowing easy integration of
uncertain statistical information while encoding
dependencies between variables using a first-order
logic syntax. We define a PSL model with rules
that combine statistical features, semantic infor-
mation, and structural constraints. Statistical fea-
tures, such as argument overlap and alignments
to WordNet verbs senses, allow RELLY to learn
from large text collections. Semantic informa-
tion, such as type information for relation argu-
ments, improves precision of the resulting infer-
ences. Structural constraints, such as transitivity
and acyclicity, enforce a complete and consistent
set of edges. Using this PSL model, we learn rule
weights with a small amount of training data and
then perform joint inference over all hypernymy
links in the graph.
We highlight three major contributions of our
work. First, we introduce RELLY, a scalable
method for integrating statistical and semantic sig-
nals to produce a hypernymy graph. RELLY is ex-
tensible and can easily incorporate additional in-
formation sources and features. Second, we gener-
ate a complete and precise hypernymy graph over
20K relational phrases and 35K hypernymy links.
We have publicly released this hypernymy graph
as a resource for the NLP community. Third, we
present a thorough empirical evaluation to mea-
sure the precision of the hypernymy graph as well
as demonstrate its usefulness in a real-world docu-
ment ranking task. Our results show a high preci-
sion (0.78) and superior performance in document
ranking compared to state-of-the-art models such
as word2vec (Mikolov et al., 2013).
</bodyText>
<sectionHeader confidence="0.981126" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999908666666667">
Before describing the details of RELLY, we be-
gin with necessary background information on the
task of semantically organizing relational phrases,
as well as the probabilistic soft logic modeling lan-
guage which we use to develop our hypernymy
graph construction method.
</bodyText>
<subsectionHeader confidence="0.985871">
2.1 Relational Phrases
</subsectionHeader>
<bodyText confidence="0.999821642857143">
Relational phrases are textual representations of
relations which occur between named entities
(e.g., “Terry Pratchett”) or noun phrases (e.g., “the
great writer”). Nakashole et al. (2012) identify re-
lational phrases with the semantic type signature
of the relation, i.e. the fine-grained lexical types
of left- and right-hand side arguments. For ex-
ample, “Terry Pratchett published his new novel
The Colour of Magic” is an instance of the re-
lational phrase “&lt;person&gt; published his * ADJ
novel &lt;book&gt;.” In this case, the left-hand ar-
gument (the domain of the relation) has the type
&lt;person&gt; and the right-hand argument (the range
of the relation) has the type &lt;book&gt;.
Several projects from the Open Information
Extraction community have addressed the task
of finding synonyms of relational phrases us-
ing clustering algorithms. The biggest collec-
tion of relational phrases and their synonyms is
currently the PATTY project (Nakashole et al.,
2012), with around 350,000 semantically typed
relational phrases. Prominent alternatives are
WiseNet (Moro and Navigli, 2012), which offers
40,000 synsets of relational phrases, PPDB (Gan-
itkevitch et al., 2013), which contains over 220
million paraphrase pairs, as well as DIRT and Ver-
bOcean (Lin and Pantel, 2001; Chklovski and Pan-
tel, 2004) which inspired the approach and results
pursued here.
Relational phrases can be further organized into
a hierarchical structure according to their hyper-
nymy (subsumption) relationships. For example,
“&lt;person&gt; moves to &lt;country&gt;” is a hypernym
of the relational phrase “&lt;musician&gt; emigrates to
&lt;country&gt;.” Of the aforementioned collections,
only PATTY attempts to automatically create a
subsumption hierarchy for the extracted relational
phrases. The authors of the HARPY system ar-
gue that the sparseness of PATTY’s graph comes
from the lack of general phrases in the source
corpus. As a solution, they propose using the
WordNet verb hierarchy (which contains general
</bodyText>
<page confidence="0.996866">
972
</page>
<bodyText confidence="0.993057615384615">
verb senses) to construct a similar hierarchy with
PATTY’s relational phrases. The graph obtained
by HARPY consists of around 600,000 hyper-
nymy links for around 20,000 relational phrases.
However, the final graph was not evaluated for pre-
cision; rather, the evaluation was instead concen-
trated on the alignment between verb senses and
relations.
In this paper we will make use of several con-
cepts that are closely related to hypernymy, which
we define below. Note that although the following
definitions concern verbs, we also apply them to
relational phrases:
</bodyText>
<listItem confidence="0.997255153846154">
• hypernym: the verb Y is a hypernym of the
verb X if Y is more general than X. To per-
ceive is a hypernym of to listen (Bai et al.,
2010).
• troponym: the verb Y is a troponym of the verb
X if doing Y is doing X, in some manner. To
lisp is a troponym of to talk (Bai et al., 2010).
Troponym is a verb counterpart for hyponym,
which applies to nouns. In this work we use
these two terms interchangeably.
• entailment: the verb Y is entailed by X if, by
doing X, you must be doing Y . To sleep is
entailed by to snore (Bai et al., 2010).
</listItem>
<subsectionHeader confidence="0.998614">
2.2 Probabilistic Soft Logic
</subsectionHeader>
<bodyText confidence="0.999944625">
Our approach is based on probabilistic soft logic
(PSL), a popular statistical relational learning sys-
tem which we briefly describe here. PSL is a tem-
plating language for a class of graphical models
known as hinge-loss Markov random fields. PSL
models are specified using rules in first-order logic
syntax, expressing dependencies between interre-
lated variables. For example, the PSL rule
</bodyText>
<equation confidence="0.997547">
w : HYPERNYM(P1, P2) ∧ HYPERNYM(P2, P3)
⇒ HYPERNYM(P1, P3)
</equation>
<bodyText confidence="0.999765947368421">
expresses the transitivity of hypernyms: if phrase
P1 is a hypernym of phrase P2 and P2 is a hyper-
nym of P3, then P1 is a hypernym of P3. Rules are
weighted (w) to indicate their importance in the
model, and weight learning in PSL allows these
weights to be learned from training data.
Each rule is ground by substituting the variables
in the rule with constants, e.g. ”married to” and
”relative of” for P1 and P2. However, unlike pre-
vious approaches such as Markov logic networks,
the atoms in each logical rule take values in the
[0,1] continuous domain. In addition to provid-
ing a natural way of incorporating uncertainty and
similarity into models, continuous-valued vari-
ables allow the inference objective to be formu-
lated as convex optimization making MAP infer-
ence extremely efficient, with empirical perfor-
mance that scales linearly with the number of
ground rules.
</bodyText>
<sectionHeader confidence="0.963763" genericHeader="method">
3 Hypernymy Graph Construction
</sectionHeader>
<bodyText confidence="0.99957112">
In this section we detail RELLY, our system for
constructing a hypernymy graph. RELLY incor-
porates semantic and statistical information from
sources such as YAGO, WordNet, PATTY, and
HARPY, and uses PSL to combine and reason
over these sources. For each source, we in-
troduce a PSL predicate (Table 1). The predi-
cates are divided into three categories: statisti-
cal (continuous-valued features arising from sta-
tistical methods), semantic (binary predicates ac-
quired from knowledge bases) and output (the tar-
get variables). We relate these predicates with a
series of rules which combine alignment links, ar-
gument similarity, and hierarchical information.
The collection of rules defines the PSL model,
which we describe in Section 3.1 and Table 2.
In the resulting hypernymy graph, an edge from
a relational phrase R1 to a relational phrase R2 de-
notes that R1 is more specific than R2, i.e. R2 is a
hypernym of R1. For example, there is an edge
from R1 =“&lt;musician&gt; emigrates to &lt;coun-
try&gt;” to R2 = “&lt;person&gt; moves to &lt;country&gt;.”
In the PSL model the strength of this edge is rep-
resented by the confidence score of the predicate
hyponym(R1, R2).
</bodyText>
<subsectionHeader confidence="0.998226">
3.1 PSL Rules
</subsectionHeader>
<bodyText confidence="0.999719928571429">
The PSL rules that define the model are shown in
Table 2. Each of the rules is additionally supplied
with a weight which describes its importance in
the model. The weights are learned from a small
hand-crafted hierarchy of relational phrases. The
full PSL model combines multiple statistical and
semantic signals into the hypernymy graph.
Our model includes rules to encode signals that
provide evidence for hypernymy, as well as rules
to encode consistency in the graph. One statistical
signal for phrase subsumption is argument over-
lap. If the arguments to a relational phrase R1
are also found as arguments to another relational
phrase R2, R1 and R2 may be synonymous or
</bodyText>
<page confidence="0.999221">
973
</page>
<tableCaption confidence="0.997423">
Table 1: PSL predicates;
</tableCaption>
<equation confidence="0.773967923076923">
R1, R2 are relational phrases, V b1, V b2 WordNet verb senses and TL1, TR1, T1, T2 YAGO types
PSL predicate Type Description
2003)
patty5ubsumption(R1, R2) statistical PATTY subsumption (Nakashole et al., 2012)
harpy(R1, V b1) statistical alignment links between relational phrases and Word-
Net verb senses (Grycner and Weikum, 2014)
wordnetHyponym(V b1, V b2) semantic hyponymy link between WordNet verb senses
lType(R1, TL1) semantic left (domain) type of arguments of a relational phrase
rType(R1, TR1) semantic right (range) type of arguments of a relational phrase
yagoHyponym(T 1, T2) semantic T1 is a subtype of T2 in YAGO hierarchy
candidateHyponym(R1, R2) output relational phrase R1 is more specific than R2 (without
enforcing consistent argument types)
hyponym(R1, R2) output relational phrase R1 is more specific than R2
</equation>
<bodyText confidence="0.940987575757576">
weedsInclusion(R1, R2) statistical degree of inclusion of sets of argument pairs of re-
lations defined assRlnArsR2�
ArgsR1
|Ar (Weeds and Weir,
gg
R2 may be a hypernym of R1. We use two mea-
sures of argument overlap, weedsInclusion and
patty5ubsumption, in rules 1 and 2, respectively,
to capture the relationship between argument over-
lap and subsumption. Another signal, used in rule
3, is the alignment between relational phrases and
WordNet verb senses. If relational phrases R1 and
R2 are aligned to WordNet verb senses V b1 and
V b2 which are in a hyponymy relationship, then
this is evidence that R1 is more specific than R2.
An example of using HARPY alignment links and
WordNet hierarchy is shown in Figure 1.
We encode local consistency requirements us-
ing Rules 4–6. Rule 4 (types compatibility) is a
constraint to restrict hypernymy links to be be-
tween relations whose types are compatible, i.e
they are identical or the types of the more specific
relation are subtypes of the types of the more gen-
eral relation. Rules 5 and 6 create a transitive clo-
sure of both WordNet and YAGO hierarchies. As a
result of these rules, we can use indirect hyponyms
(in rule 3) or indirect subtypes (in rule 4).
Finally, rules 7, 8 and 9 shape the structure of
the output graph with collective global constraints.
Rule 7 (asymmetry) removes bidirectional links,
rule 8 (transitivity) creates a transitive closure of
the graph and rule 9 (acyclicity) prevents the cre-
ation of small cycles in the graph.
</bodyText>
<subsectionHeader confidence="0.994347">
3.2 RELLY Overview
</subsectionHeader>
<bodyText confidence="0.993002">
RELLY has four stages: data pre-processing, rule
weight learning, inference, and thresholding.
</bodyText>
<figure confidence="0.9001515">
HARPY alignment
&lt;person&gt; created a &lt;artifact&gt; make.03
</figure>
<figureCaption confidence="0.999913">
Figure 1: HARPY alignment usage
</figureCaption>
<bodyText confidence="0.999821826086957">
First, in the data pre-processing stage, we as-
sign confidence scores of 0 or 1 for the binary-
valued semantic predicates in the PSL model.
For example, the wordnetHyponym(V b1,V b2)
confidence score is set to 1 if there is a hy-
ponymy link between verb senses V b1 and V b2
and 0 otherwise. In other cases, the confidence
is set to a similarity score of a feature which
is represented by a predicate. For example, the
weedsInclusion(R1, R2) confidence is equal to
the Weeds inclusion score between relations R1
and R2.
In the next stage the weights of the PSL rules
described in Table 2 are learned from a small
handcrafted graph of relational phrases. The
weight learning is performed using an EM al-
gorithm. Later, the most-probable explanation
(MPE) state of the output predicates is inferred.
Finally, we export the inferred confidence
scores of the predicate hyponym and perform ad-
ditional cleaning. Whenever two links contradict
each other (e.g. we have both hyponym(R1, R2)
and hyponym(R2, R1)) we remove the link with
</bodyText>
<figure confidence="0.99608525">
create_verbally.01
WordNet
hierarchy
WordNet
hierarchy
HARPY alignment
&lt;person&gt; wrote a poem &lt;artifact&gt; write.01
hyponym
</figure>
<page confidence="0.99315">
974
</page>
<tableCaption confidence="0.977921">
Table 2: PSL rules
</tableCaption>
<table confidence="0.999383866666667">
Id Feature PSL rule
1 Weeds inclusion weedsInclusion(R1, R2) ⇒ candidateHyponym(R1, R2)
2 Patty subsumption pattySubsumption(R1, R2) ⇒ candidateHyponym(R1, R2)
3 Harpy alignment wordnetHyponym(Vb1,Vb2) ∧ harpy(R1,V b1) ∧ harpy(R2,Vb2)
⇒ candidateHyponym(R1, R2)
4 Types candidateHyponym(R1, R2) ∧ lType(R1, TL1) ∧ rType(R1, TR1)
compatibility ∧lType(R2, TL2) ∧ rType(R2, TR2) ∧ yagoHyponym(TL1, TL2)
∧yagoHyponym(TR1, TR2) ⇒ hyponym(R1, R2)
5 WordNet wordnetHyponym(V b1,V b2) ∧ wordnetHyponym(V b2, V b3)
hierarchy ⇒ wordnetHyponym(Vb1,Vb3)
6 Yago hierarchy yagoHyponym(T1,T2) ∧ yagoHyponym(T2,T3)
⇒ yagoHyponym(T 1, T3)
7 Asymmetry hyponym(R1, R2) ⇒ ¬hyponym(R2, R1)
8 Transitivity hyponym(R1, R2) ∧ hyponym(R2, R3) ⇒ hyponym(R1, R3)
9 Acyclicity hyponym(R1, R2) ∧ hyponym(R2, R3) ⇒ ¬hyponym(R3, R1)
</table>
<bodyText confidence="0.9905584">
the lower confidence score. If both predicates have
the same confidence score we exclude them both
from the final graph. Additionally, we only con-
sider links with a confidence score above an em-
pirically chosen threshold of 0.2.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999971785714286">
In our experiments, we use a large corpus of rela-
tional phrases to construct a hypernymy graph us-
ing RELLY. We evaluate RELLY using both intrin-
sic and extrinsic evaluation. In the intrinsic evalua-
tion, we asked human annotators to judge the rela-
tionship between two relational phrases and com-
pared results from several hypernymy graphs. In
the extrinsic evaluation, we used the hypernymy
graph for a real-world document ranking task and
measured the mean reciprocal rank (MRR) for a
number of methods. In both evaluations, the hy-
pernymy graph constructed by RELLY demon-
strates significantly better performance than com-
peting algorithms.
</bodyText>
<subsectionHeader confidence="0.965853">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999839678571429">
We use RELLY to build a hypernymy graph with
data from the PATTY and HARPY projects. The
input to our system consists of 20,812 relational
phrases and the associated argument types ex-
tracted from the English-language Wikipedia web-
site using the PATTY system. For simplicity, we
only include relational phrases that contain exactly
one verb (e.g. “took the throne”), excluding noun
phrases (e.g. “member of”) and phrases contain-
ing multiple verbs (e.g. “hit and run”). The verb
“to be” and modal verbs were not considered in
the dataset. We also include HARPY alignments
to the corresponding verb senses in WordNet for
each phrase in the corpus. Additionally, we use
a subset of the type-subsumption hierarchy from
YAGO consisting of 144 types and 323 subsump-
tion relationships.
During graph inference, RELLY evaluated
7.9M possible hypernymy links using 9.7M
ground logical rules and constraints. Ultimately,
RELLY produced 35,613 hypernymy links be-
tween relational phrases with confidence scores
above 0.2. The hypernymy graph consisted of
3,730 roots. Running RELLY on a multi-core
2.27GHz server with 64GB of RAM required ap-
proximately 20 hours. For comparison, PATTY
produced 8,162 subsumption links out of 350,569
phrases with approximately 2,300 roots.
</bodyText>
<subsectionHeader confidence="0.936214">
4.2 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.999929">
In our intrinsic evaluation, we assess the precision
of hypernymy links inferred by RELLY and com-
pare with the precision of hypernymy graphs of
PATTY and HARPY. In this evaluation, we mea-
sure precision for both the most confident hyper-
nymy links in the system (precision@100) and the
precision of a random sample of 100 hypernymy
links. Each set of hypernymy links were presented
to several human annotators for labeling.
</bodyText>
<footnote confidence="0.532074">
To measure precision@100, we choose the top
100 hypernymy links using the confidence scores
reported by PSL. We similarly choose the top 100
links from PATTY using the PATTY subsumption
</footnote>
<page confidence="0.995838">
975
</page>
<bodyText confidence="0.996649294117647">
score. Since HARPY does not provide confidence
scores, we were unable to compute precision@100
for HARPY.
For each of the three systems, we used the full
set of hypernymy links they produce, which con-
sisted of 8K links from PATTY, 600K links from
HARPY and 35K links from RELLY. We ran-
domly sampled 100 hypernymy links from each
of these systems.
We presented the selected hypernymy links to
several human annotators. The labeling task re-
quired the annotator to judge the relationship be-
tween two relational phrases in a hypernymy link.
For each relational phrase, we provided annota-
tors with type information about the phrase argu-
ments (domain and range) and examples of sen-
tences that use the relational phrase. Based on
this information, annotators could make one of
four judgments: (1) the phrases are unrelated;
(2) the phrases are synonymous; (3) the first
phrase is more specific than the second phrase;
(4) the second phrase is more specific than the
first phrase. This evaluation task had good inter-
annotator agreement, with a Cohen’s Kappa of
0.624. Separately, the precision@100 dataset had
Cohen’s Kappa of 0.708 and the randomly sam-
pled dataset had Cohen’s Kappa of 0.521.
We show the results of the intrinsic evaluation in
Table 3 with 0.9-confidence Wilson score interval
(Brown et al., 2001). In comparison to HARPY
and PATTY, RELLY has higher precision for both
precision@100 and random evaluations. Precision
in RELLY is comparable to PATTY, but RELLY
has more than four times as many hypernym links.
HARPY has far more hypernymy links, but with a
precision of 0.43, we find that many of these links
are incorrect.
Table 4 includes example hypernymy links from
RELLY. There are examples where PATTY’s sub-
sumption is a dominant signal (“&lt;person&gt; pub-
licly accused &lt;person&gt;” ⇒ “&lt;person&gt; accused
&lt;person&gt;”). We also observe YAGO type hier-
archy influence (“&lt;athlete&gt; played for &lt;team&gt;”
⇒ “&lt;person&gt; played for &lt;organization&gt;”), as
well as the influence of combined WordNet hierar-
chy with HARPY alignments (“&lt;person&gt; marry
daughter &lt;person&gt;” ⇒ “&lt;person&gt; joins &lt;per-
son&gt;”). The advantage of RELLY is that it com-
putes the final graph jointly and incorporates tran-
sitivity, asymmetry and acyclicity rules. It leads
to less semantic drift in longer hypernymy chains
</bodyText>
<tableCaption confidence="0.995325">
Table 3: Intrinsic evaluation
</tableCaption>
<table confidence="0.998876375">
Prec. Range Cvg.
precision@100
RELLY 0.87 0.81 - 0.92 35K
PATTY 0.83 0.76 - 0.90 8K
random sample
RELLY 0.78 0.71 - 0.84 35K
PATTY 0.75 0.68 - 0.82 8K
HARPY 0.43 0.35 - 0.52 600K
</table>
<figureCaption confidence="0.937161">
(e.g. Figure 2) compared with PATTY where
“&lt;organization&gt; merged &lt;organization&gt;” can
lead to “&lt;team&gt; beat &lt;team&gt;”.
&lt;organization&gt; acquires &lt;organization&gt;
&lt;organization&gt; purchased share &lt;organization&gt;
&lt;organization&gt; bought half of &lt;company&gt;
&lt;company&gt; bought half of &lt;company&gt;
&lt;company&gt; later bought half of &lt;company&gt;
Figure 2: Chain of hypernymy
</figureCaption>
<subsectionHeader confidence="0.999808">
4.3 Ablation Study
</subsectionHeader>
<bodyText confidence="0.999888">
Two advantages of RELLY that we have high-
lighted are easily incorporating new information
sources and collectively enforcing global con-
straints. To analyze the influence of these sys-
tem components, we performed an ablation study
where we omitted PSL rules corresponding to
specific model features. Using this approach,
we quantify the importance of these features to
RELLY’s performance.
First, we demonstrate the value of type informa-
tion in determining hypernymy. The YAGO type
hierarchy allows RELLY to detect hypernymy
links between relational phrases where types do
not match exactly, but are compatible through
type subsumption. When the YAGO type hierar-
chy rules are omitted from the model, coverage
is reduced dramatically; the resulting hypernymy
graph contains only 12,000 hypernymy links in
contrast to the 35,000 links in the original model.
Additionally, removing YAGO type information
harms precision, with a precision of 0.75 f 0.09
</bodyText>
<page confidence="0.999202">
976
</page>
<tableCaption confidence="0.999243">
Table 4: Example RELLY hypernymy links
</tableCaption>
<table confidence="0.648864571428571">
Hyponym relational phrase Hypernym relational phrase
Domain Text pattern Range Domain Text pattern Range
head of state abdicated in favor of sovereign person resigns as person
person publicly accused person person accused person
person marry daughter person person joins person
person had paid person person interacted with person
athlete played for team person played for organization
</table>
<tableCaption confidence="0.944677">
Table 5: Results for Entailment graphs induction
</tableCaption>
<table confidence="0.940702">
Prec. Rec. F1
Berant et al. (2011) 0.422 0.434 0.428
PSL 0.461 0.435 0.447
</table>
<bodyText confidence="0.995445066666667">
with 0.9-confidence Wilson score interval for a
random sample of 100 examples.
Next, we show how global constraints on the
hypernymy graph such as anti-symmetry and
acyclicity improve the quality of the hypernymy
graph. Since the relational phrases generated
by PATTY are clustered to find synonymous re-
lations, these global constraints prevent RELLY
from merging clusters. When the anti-symmetry
and acyclicity rules were removed from the model,
the resulting hypernymy graph included approxi-
mately 500 additional hypernymy links, while 10
existing links were removed. We manually evalu-
ated the newly introduced links, and found that the
majority of links were false positives.
</bodyText>
<subsectionHeader confidence="0.996979">
4.4 Entailment Graph Induction
</subsectionHeader>
<bodyText confidence="0.99889296">
We compared the performance of PSL against the
Integer Linear Programming (ILP) formulation by
(Berant et al., 2011). The comparison was per-
formed on the task of creating entailment graphs
as described in (Berant et al., 2011). This task is
strongly related to finding hypernyms of relational
phrases. The experiments were executed on the
dataset of 10 manually annotated graphs. In to-
tal this dataset contains 3,427 positive and 35,585
negative examples. Our model uses the transi-
tivity rule (entails(A, B) ∧ entails(B, C) ⇒
entails(A, C)). We also include the local en-
tailment scores (score(A, B) ⇒ entails(A, B))
which were released by (Berant et al., 2011). Ta-
ble 5 presents micro-averaged precision, recall and
F1 scores for this comparison.
PSL was much faster than the other exact meth-
ods used for this problem. To compare efficiency
we measured the run-time of our method. With-
out any graph decomposition it took on average
232 seconds. The experiments were performed
on a multi-core 2.67GHz server with 32GB of
RAM. The methods reported in (Berant et al.,
2012), which did not utilize graph decomposition
method, had run-time above 5000 seconds.
</bodyText>
<subsectionHeader confidence="0.958068">
4.5 Extrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.999928806451613">
The ultimate goal of producing a high-quality hy-
pernymy graph is to deepen our understanding of
natural language and improve performance on the
many NLP applications. One such application is
document retrieval, where billions of queries are
performed each day through search engines. In our
extrinsic evaluation, we demonstrate how a hyper-
nymy graph can improve performance on a docu-
ment ranking and retrieval task.
We consider a task where an input query
document is compared to a corpus of docu-
ments with the aim of finding the most relevant
related documents. To isolate the evaluation
to relational phrases, we anonymize the doc-
uments, by replacing all named entities and
noun phrases with placeholders. For example,
the sentence “The villain has already
fled to the Republica de Isthmus”
is anonymized to “* has already fled
to *.” Anonymized retrieval has potential appli-
cations in security and for sensitive documents.
We collected a dataset consisting of movie plot
summaries from two different websites, Wikipedia
and the Internet Movie Database (IMDB). We
chose plot synopses from 25 James Bond movies
and 23 movies based on the Marvel Comics char-
acters. For each plot synopsis, we have two plot
descriptions: one from Wikipedia and another
from IMDB. Given a query in the form of an
anonymized plot description from one website, the
task is to rank the anonymized plot descriptions
</bodyText>
<page confidence="0.994766">
977
</page>
<bodyText confidence="0.999968333333333">
from the other dataset using relational phrase sim-
ilarity. For example, given a query plot description
of “Iron Man” from Wikipedia, rank plot descrip-
tions from IMDB with the goal of maximizing
the ranking of the corresponding “Iron Man” plot
summary. We evaluate the quality of these rank-
ings using the mean reciprocal rank (MRR) score,
MRR = Q � Q 1 1ranki. Here, Q is the number of
documents in the collection (i.e. 2*48 = 96) and
rankz is the position of the counterpart document
in the ranking of document i.
As baseline algorithms, we use a unigram
word2vec model and a bigram model. In the uni-
gram word2vec model documents are represented
by the average of the 300-dimensional word vec-
tors trained on part of Google News dataset (about
100 billion words) (Mikolov et al., 2013). We
could not use the bigram word2vec model because
of the frequent occurrence of the placeholder sym-
bol. In the bigram model, documents are rep-
resented by vectors in the bag-of-bigrams model
with bigram frequency weights. The similarity
measure in both cases is the cosine similarity mea-
sure.
As the first of our approaches we proposed a
solution purely based on relational phrases. In
the relational phrases model we extract relational
phrases from a text and we map them to their
synsets from PATTY (clusters of synonyms). A
phrase is mapped to a synset if the Jaccard simi-
larity between tokens of extracted relation and to-
kens of one of the phrases in the synset is above
a threshold. Next we represent the document as
a vector of the relational phrase synsets weighted
by the frequency of the synset in the document
(bag-of-relational phrases). The similarity score
between two documents is the cosine similarity
between two vectors representing two documents.
The ranking is created based on the similarity
scores. In the relational phrases + hypernyms
model we add hypernyms of the extracted rela-
tional phrases to the document vector (based on
the hypernymy graph). Hypernyms are addition-
ally weighted by the confidence score produced by
the algorithm described in the Section 3. In the
second approach we combine relational phrases
models with the best of the baselines. The similar-
ity score is then equal to Asim1+(1−A)sim2. The
A parameter is trained on a different dataset (2*8
plot descriptions of Harry Potter movies). Train-
ing was performed by maximization of the MRR
</bodyText>
<tableCaption confidence="0.991233">
Table 6: Extrinsic evaluation (Bond &amp; Marvel)
</tableCaption>
<table confidence="0.999828">
MRR score
word2vec 0.26
bigram 0.55
relational phrases 0.28
+ hypernyms 0.25
+ bigrams 0.58
+ hypernyms + bigrams 0.60
</table>
<bodyText confidence="0.998534454545455">
score using grid search. We consider the combina-
tion of the bigram model with relational phrases,
as well as the combination of the bigram model
with relational phrases + hypernyms.
The results of the experiment are presented in
Table 6. The best MRR score was obtained by re-
lational phrases + hypernyms + bigrams model.
The number of samples, 96, was large enough for
statistical significance. We performed a paired t-
test for MRR between each of these methods.
The obtained p-values were below 0.05.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999940807692308">
The biggest sources of hypernyms, subsumptions,
and hierarchical structure can be found in exist-
ing knowledge bases. Examples of these are Free-
base (Bollacker et al., 2008), YAGO, DBPedia
(Lehmann et al., 2014), and Google Knowledge
Vault (Dong et al., 2014). However, these knowl-
edge bases are mainly concentrated on named enti-
ties and noun phrases, and the variety of relations
between entities is much smaller. Relations and
information about them are underrepresented.
Open Information Extraction systems try to
solve this problem by extracting new relations
from natural text. These new relations do not
necessarily follow the standard schema of knowl-
edge bases. Additionally, these systems often or-
ganize the newly extracted relations by clustering
or hierarchy construction. A first attempt to ex-
tract and cluster similar relations was presented in
DIRT. This work was followed by projects such
as ReVerb, PATTY, WiseNet, NELL (Carlson et
al., 2010), and RESOLVER (Yates and Etzioni,
2009). PATTY and WiseNet also introduced se-
mantic types to their concept of relational phrases.
All of these systems rely on the co-occurrence of
arguments of clustered relations. A different ap-
proach was presented in PPDB, where the authors
</bodyText>
<page confidence="0.99652">
978
</page>
<bodyText confidence="0.999654765625">
cluster phrases based on the similarity of transla-
tions to other languages.
Of these systems, only PATTY attempted to cre-
ate a hierarchy of relations and the result was very
sparse. HARPY aimed to overcome this problem
by disambiguating and aligning relational phrases
with WordNet, and performing a simple recon-
struction of the WordNet hierarchy on top of rela-
tional phrases from PATTY. A very similar prob-
lem was addressed in the entailment graph project
(Levy et al., 2014). The authors automatically
created graphs of entailments between proposi-
tions, using Integer Linear Programing as one of
the main components. Propositions can be en-
coded as triples of form (subject, relation, ob-
ject). Edges in the entailment graph occur between
these triples, whereas edges connect typed rela-
tions in PATTY and HARPY. Moreover, the rela-
tions in the propositions were mainly limited to
single verbs, whereas in our case we also consider
longer relational phrases. Relations with semantic
types were also used in typed entailment graphs
(Berant et al., 2011). However, the type hierarchy
was not considered there, which prevented from
creating links between two relations with different
semantic types. The input dataset was also smaller
– the biggest graph consisted of 118 relations.
Although there is a scarcity of automatically
created taxonomies of relations, there exist several
manually curated taxonomies. Manually crafted
verb or relation hierarchies are available in Word-
Net, VerbNet and FrameNet. WordNet has 13,767
verb synsets, which are organized into a hierarchy
with 13,239 hypernymy links.
Automatic construction of taxonomies of
named entities or noun phrases has received much
more attention than organization of verbs or rela-
tions. In (Snow et al., 2006), the WordNet taxon-
omy was extended by 10,000 novel noun synsets
with hypernym-hyponym links. In (Bansal et al.,
2014), the authors reconstructed WordNet’s noun
hypernymy/hyponymy hierarchy from scratch us-
ing a probabilistic graphical model formulation.
Another method of organizing noun phrases was
proposed in (Mehdad et al., 2013), where an en-
tailment graph of noun phrases was constructed.
Building a hypernymy graph for relational
phrases is strongly related with the textual entail-
ment task (Dagan et al., 2010). This concept was
introduced in the Recognizing Textual Entailment
(RTE) shared task (Dagan et al., 2005). Instead of
short typed relational phrases, the input data are
two texts – the entailing text T and the hypothesis
text H. According to (Dagan et al., 2005)’s defi-
nition, “T entails H if, typically, a human reading
T would infer that H is most probably true.”
In RELLY, we use probabilistic soft logic (PSL)
as the main ingredient of our approach. PSL was
successfully used for numerous other applications
including knowledge graph construction (Pujara
et al., 2013), trust in social networks (Huang et
al., 2012b), ontology alignment (Broecheler and
Getoor, 2009), and social group modeling (Huang
et al., 2012a).
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999797666666667">
This paper presents RELLY, a scalable method for
integrating statistical and semantic signals to pro-
duce a hypernymy graph of relational phrases. We
used RELLY to create a hypernymy graph that has
both high coverage and precision, as shown in our
evaluation. RELLY is extensible and can easily in-
corporate additional information sources and fea-
tures. The hypernymy graph of relational phrases
could potentially be useful for many problems of
natural language processing and information re-
trieval. For example, we applied the hypernymy
graph to a document-relevance task, which we
used to evaluate RELLY extrinsically. As a future
work, RELLY can incorporate more information
sources and statistical signals and be expanded
to infer multi-verb or noun relational phrases.
The RELLY resource is publicly available at
www.mpi-inf.mpg.de/yago-naga/patty/.
Acknowledgments: This work was partially
supported by National Science Foundation (NSF)
grant IIS1218488 and by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D12PC00337.
The U.S. Government is authorized to reproduce
and distribute reprints for governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of NSF, IARPA, DoI/NBC, or the U.S. Gov-
ernment.
</bodyText>
<page confidence="0.998078">
979
</page>
<sectionHeader confidence="0.996299" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999406401869159">
Stephen H. Bach, Matthias Broecheler, Ben. Huang,
and Lise Getoor. 2015. Hinge-loss Markov random
fields and probabilistic soft logic. arXiv:1505.04406
[cs.LG].
Rujiang Bai, Xiaoyue Wang, and Junhua Liao. 2010.
Extract semantic information from wordnet to im-
prove text classification performance. In Advances
in Computer Science and Information Technology,
pages 409–420.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Pro-
ceedings of Association for Computational Linguis-
tics and Conference on Computational Linguistics
(COLING/ACL), pages 86–90.
Mohit Bansal, David Burkett, Gerard de Melo, and
Dan Klein. 2014. Structured learning for taxonomy
induction with belief propagation. In Proceedings
ofAssociation for Computational Linguistics (ACL),
pages 1041–1051.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of Association for Computational Lin-
guistics (ACL), pages 610–619.
Jonathan Berant, Ido Dagan, Meni Adler, and Jacob
Goldberger. 2012. Efficient tree-based approxima-
tion for entailment graph learning. In Proceedings
ofAssociation for Computational Linguistics (ACL).
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of ACM SIGMOD
International Conference on Management of Data,
pages 1247–1250.
Matthias Broecheler and Lise Getoor. 2009. Proba-
bilistic similarity logic. In International Workshop
on Statistical Relational Learning.
Lawrence D. Brown, T. Tony Cai, and Anirban Das-
gupta. 2001. Interval estimation for a binomial pro-
portion. Statistical Science, 16:101–133.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka, and Tom M. Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In Proceedings of Association for
the Advancement of Artificial Intelligence (AAAI).
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP), pages 33–
40.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entail-
ment challenge. In In Proceedings of the PASCAL
Challenges Workshop on Recognising Textual En-
tailment.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ra-
tional, evaluation and approaches erratum. Natural
Language Engineering, 16:105–105, 1.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowledge
Vault: A web-scale approach to probabilistic knowl-
edge fusion. In Proceedings ofACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 601–610.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information
extraction. In Proceedings of Empirical Methods
in Natural Language Processing (EMNLP), pages
1535–1545.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguis-
tics Human Language Technologies (NAACL-HLT),
pages 758–764.
Adam Grycner and Gerhard Weikum. 2014. HARPY:
Hypernyms and alignment of relational paraphrases.
In Proceedings of Conference on Computational
Linguistics (COLING), pages 2195–2204.
Bert Huang, Stephen H. Bach, Eric Norris, Jay Pujara,
and Lise Getoor. 2012a. Social group modeling
with probabilistic soft logic. In NIPS Workshop on
Social Network and Social Media Analysis: Meth-
ods, Models, and Applications.
Bert Huang, Angelika Kimmig, Lise Getoor, and Jen-
nifer Golbeck. 2012b. Probabilistic soft logic for
trust analysis in social networks. In International
Workshop on Statistical Relational Artificial Intelli-
gence (StaRAI2012).
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of english verbs. Language Resources and Evalua-
tion, 42(1):21–40.
Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo Mendes, Se-
bastian Hellmann, Mohamed Morsey, Patrick van
Kleef, S¨oren Auer, and Chris Bizer. 2014. DBpe-
dia - a large-scale, multilingual knowledge base ex-
tracted from wikipedia. Semantic Web Journal.
Omer Levy, Ido Dagan, and Jacob Goldberger. 2014.
Focused entailment graphs for Open IE proposi-
tions. In Proceedings of Conference on Computa-
tional Natural Language Learning (CoNLL), pages
87–97.
</reference>
<page confidence="0.975766">
980
</page>
<reference confidence="0.998187033333333">
Dekang Lin and Patrick Pantel. 2001. DIRT
@SBT@discovery of inference rules from text. In
Proceedings of ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
pages 323–328.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of North
American Chapter of the Association for Computa-
tional Linguistics Human Language Technologies
(NAACL-HLT), pages 182–190.
Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng,
and Shafiq Joty. 2013. Towards topic labeling with
phrase entailment and aggregation. In Proceedings
of North American Chapter of the Association for
Computational Linguistics Human Language Tech-
nologies (NAACL-HLT), pages 179–189.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Andrea Moro and Roberto Navigli. 2012. WiseNet:
building a wikipedia-based semantic network with
ontologized relations. In Proceedings ofACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 1672–1676.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. PATTY: A taxonomy of relational
patterns with semantic types. In Proceedings of
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning EMNLP-CoNLL, pages 1135–
1145.
Jay Pujara, Hui Miao, Lise Getoor, and William Cohen.
2013. Knowledge graph identification. In Interna-
tional Semantic Web Conference (ISWC).
Rion Snow, Daniel Jurafsky, and Y. Andrew Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings ofAssociation for Compu-
tational Linguistics and International Conference on
Computational Linguistics (COLING/ACL), pages
801–808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of International Conference
on World Wide Web (WWW), pages 697–706.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
Empirical Methods in Natural Language Processing
(EMNLP), pages 81–88.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In Proceedings of Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
EMNLP-CoNLL, pages 379–390.
Alexander Yates and Oren Etzioni. 2009. Unsuper-
vised methods for determining object and relation
synonyms on the web. Journal of Artificial Intelli-
gence Research, 34(1):255–296.
</reference>
<page confidence="0.99817">
981
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.477553">
<title confidence="0.99976">RELLY: Inferring Hypernym Relationships Between Relational Phrases</title>
<author confidence="0.992875">Adam Grycner</author>
<author confidence="0.992875">Gerhard</author>
<affiliation confidence="0.986958">Max-Planck Institute for</affiliation>
<address confidence="0.985385">Campus E1.4,</address>
<email confidence="0.7618825">Saarbr¨ucken,@mpi-inf.mpg.de</email>
<author confidence="0.991479">Jay Pujara</author>
<author confidence="0.991479">James Foulds</author>
<author confidence="0.991479">Lise</author>
<affiliation confidence="0.9930865">Computer Science University of California, Santa</affiliation>
<address confidence="0.960293">Santa Cruz, CA</address>
<email confidence="0.999139">@soe.ucsc.edu</email>
<abstract confidence="0.9995474">Relational phrases (e.g., “got married to”) and their hypernyms (e.g., “is a relative of”) are central for many tasks including question answering, open information extraction, paraphrasing, and entailment detection. This has motivated the development of several linguistic resources (e.g. DIRT, PATTY, and WiseNet) which systematically collect and organize relational phrases. These resources have demonstrable practical benefits, but are each limited due to noise, sparsity, or size. We present a new general-purpose method, RELLY, for constructing a large hypernymy graph of relational phrases with high-quality subsumptions using collective probabilistic programming techniques. Our graph induction approach integrates small highprecision knowledge bases together with large automatically curated resources, and reasons collectively to combine these resources into a consistent graph. Using RELLY, we construct a high-coverage, high-precision hypernymy graph consisting of 20K relational phrases and 35K hypernymy links. Our evaluation indicates a hypernymy link precision of 78%, and demonstrates the value of this resource for a document-relevance ranking task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Huang</author>
<author>Lise Getoor</author>
</authors>
<title>Hinge-loss Markov random fields and probabilistic soft logic.</title>
<date>2015</date>
<note>arXiv:1505.04406 [cs.LG].</note>
<marker>Huang, Getoor, 2015</marker>
<rawString>Stephen H. Bach, Matthias Broecheler, Ben. Huang, and Lise Getoor. 2015. Hinge-loss Markov random fields and probabilistic soft logic. arXiv:1505.04406 [cs.LG].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rujiang Bai</author>
<author>Xiaoyue Wang</author>
<author>Junhua Liao</author>
</authors>
<title>Extract semantic information from wordnet to improve text classification performance.</title>
<date>2010</date>
<booktitle>In Advances in Computer Science and Information Technology,</booktitle>
<pages>409--420</pages>
<contexts>
<context position="9308" citStr="Bai et al., 2010" startWordPosition="1402" endWordPosition="1405">e graph obtained by HARPY consists of around 600,000 hypernymy links for around 20,000 relational phrases. However, the final graph was not evaluated for precision; rather, the evaluation was instead concentrated on the alignment between verb senses and relations. In this paper we will make use of several concepts that are closely related to hypernymy, which we define below. Note that although the following definitions concern verbs, we also apply them to relational phrases: • hypernym: the verb Y is a hypernym of the verb X if Y is more general than X. To perceive is a hypernym of to listen (Bai et al., 2010). • troponym: the verb Y is a troponym of the verb X if doing Y is doing X, in some manner. To lisp is a troponym of to talk (Bai et al., 2010). Troponym is a verb counterpart for hyponym, which applies to nouns. In this work we use these two terms interchangeably. • entailment: the verb Y is entailed by X if, by doing X, you must be doing Y . To sleep is entailed by to snore (Bai et al., 2010). 2.2 Probabilistic Soft Logic Our approach is based on probabilistic soft logic (PSL), a popular statistical relational learning system which we briefly describe here. PSL is a templating language for a</context>
</contexts>
<marker>Bai, Wang, Liao, 2010</marker>
<rawString>Rujiang Bai, Xiaoyue Wang, and Junhua Liao. 2010. Extract semantic information from wordnet to improve text classification performance. In Advances in Computer Science and Information Technology, pages 409–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of Association for Computational Linguistics and Conference on Computational Linguistics (COLING/ACL),</booktitle>
<pages>86--90</pages>
<contexts>
<context position="3557" citStr="Baker et al., 1998" startWordPosition="515" endWordPosition="518">kashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the largest component consisted of 120 relations. A number of manuallycurated relational taxonomies such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998) also offer highprecision hierarchies with limited coverage. In this paper, we introduce RELLY, a method for producing a hypernymy graph that has both high coverage and precision. We build on previous work, integrating the high-precision knowledge in resources such as YAGO (Suchanek et al., 971 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 971–981, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2007) and WordNet with noisy statistical information from OpenIE projects PATTY and HARPY. RELLY maintains a co</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of Association for Computational Linguistics and Conference on Computational Linguistics (COLING/ACL), pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>David Burkett</author>
<author>Gerard de Melo</author>
<author>Dan Klein</author>
</authors>
<title>Structured learning for taxonomy induction with belief propagation.</title>
<date>2014</date>
<booktitle>In Proceedings ofAssociation for Computational Linguistics (ACL),</booktitle>
<pages>1041--1051</pages>
<marker>Bansal, Burkett, de Melo, Klein, 2014</marker>
<rawString>Mohit Bansal, David Burkett, Gerard de Melo, and Dan Klein. 2014. Structured learning for taxonomy induction with belief propagation. In Proceedings ofAssociation for Computational Linguistics (ACL), pages 1041–1051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL),</booktitle>
<pages>610--619</pages>
<contexts>
<context position="3229" citStr="Berant et al. (2011)" startWordPosition="465" endWordPosition="468">ften limited to synonymy, suffer from low precision, or have low coverage. Systems such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), and WiseNet (Moro and Navigli, 2012) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the largest component consisted of 120 relations. A number of manuallycurated relational taxonomies such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998) also offer highprecision hierarchies with limited coverage. In this paper, we introduce RELLY, a method for producing a hypernymy graph that has both high coverage and precision. We build on previous work, integrating the high-precision knowledge in resources such as YAG</context>
<context position="24535" citStr="Berant et al. (2011)" startWordPosition="3851" endWordPosition="3854">iginal model. Additionally, removing YAGO type information harms precision, with a precision of 0.75 f 0.09 976 Table 4: Example RELLY hypernymy links Hyponym relational phrase Hypernym relational phrase Domain Text pattern Range Domain Text pattern Range head of state abdicated in favor of sovereign person resigns as person person publicly accused person person accused person person marry daughter person person joins person person had paid person person interacted with person athlete played for team person played for organization Table 5: Results for Entailment graphs induction Prec. Rec. F1 Berant et al. (2011) 0.422 0.434 0.428 PSL 0.461 0.435 0.447 with 0.9-confidence Wilson score interval for a random sample of 100 examples. Next, we show how global constraints on the hypernymy graph such as anti-symmetry and acyclicity improve the quality of the hypernymy graph. Since the relational phrases generated by PATTY are clustered to find synonymous relations, these global constraints prevent RELLY from merging clusters. When the anti-symmetry and acyclicity rules were removed from the model, the resulting hypernymy graph included approximately 500 additional hypernymy links, while 10 existing links wer</context>
<context position="25946" citStr="Berant et al., 2011" startWordPosition="4072" endWordPosition="4075">st the Integer Linear Programming (ILP) formulation by (Berant et al., 2011). The comparison was performed on the task of creating entailment graphs as described in (Berant et al., 2011). This task is strongly related to finding hypernyms of relational phrases. The experiments were executed on the dataset of 10 manually annotated graphs. In total this dataset contains 3,427 positive and 35,585 negative examples. Our model uses the transitivity rule (entails(A, B) ∧ entails(B, C) ⇒ entails(A, C)). We also include the local entailment scores (score(A, B) ⇒ entails(A, B)) which were released by (Berant et al., 2011). Table 5 presents micro-averaged precision, recall and F1 scores for this comparison. PSL was much faster than the other exact methods used for this problem. To compare efficiency we measured the run-time of our method. Without any graph decomposition it took on average 232 seconds. The experiments were performed on a multi-core 2.67GHz server with 32GB of RAM. The methods reported in (Berant et al., 2012), which did not utilize graph decomposition method, had run-time above 5000 seconds. 4.5 Extrinsic Evaluation The ultimate goal of producing a high-quality hypernymy graph is to deepen our u</context>
<context position="33198" citStr="Berant et al., 2011" startWordPosition="5260" endWordPosition="5263">e entailment graph project (Levy et al., 2014). The authors automatically created graphs of entailments between propositions, using Integer Linear Programing as one of the main components. Propositions can be encoded as triples of form (subject, relation, object). Edges in the entailment graph occur between these triples, whereas edges connect typed relations in PATTY and HARPY. Moreover, the relations in the propositions were mainly limited to single verbs, whereas in our case we also consider longer relational phrases. Relations with semantic types were also used in typed entailment graphs (Berant et al., 2011). However, the type hierarchy was not considered there, which prevented from creating links between two relations with different semantic types. The input dataset was also smaller – the biggest graph consisted of 118 relations. Although there is a scarcity of automatically created taxonomies of relations, there exist several manually curated taxonomies. Manually crafted verb or relation hierarchies are available in WordNet, VerbNet and FrameNet. WordNet has 13,767 verb synsets, which are organized into a hierarchy with 13,239 hypernymy links. Automatic construction of taxonomies of named entit</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of Association for Computational Linguistics (ACL), pages 610–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Meni Adler</author>
<author>Jacob Goldberger</author>
</authors>
<title>Efficient tree-based approximation for entailment graph learning.</title>
<date>2012</date>
<booktitle>In Proceedings ofAssociation for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="26356" citStr="Berant et al., 2012" startWordPosition="4141" endWordPosition="4144">ples. Our model uses the transitivity rule (entails(A, B) ∧ entails(B, C) ⇒ entails(A, C)). We also include the local entailment scores (score(A, B) ⇒ entails(A, B)) which were released by (Berant et al., 2011). Table 5 presents micro-averaged precision, recall and F1 scores for this comparison. PSL was much faster than the other exact methods used for this problem. To compare efficiency we measured the run-time of our method. Without any graph decomposition it took on average 232 seconds. The experiments were performed on a multi-core 2.67GHz server with 32GB of RAM. The methods reported in (Berant et al., 2012), which did not utilize graph decomposition method, had run-time above 5000 seconds. 4.5 Extrinsic Evaluation The ultimate goal of producing a high-quality hypernymy graph is to deepen our understanding of natural language and improve performance on the many NLP applications. One such application is document retrieval, where billions of queries are performed each day through search engines. In our extrinsic evaluation, we demonstrate how a hypernymy graph can improve performance on a document ranking and retrieval task. We consider a task where an input query document is compared to a corpus o</context>
</contexts>
<marker>Berant, Dagan, Adler, Goldberger, 2012</marker>
<rawString>Jonathan Berant, Ido Dagan, Meni Adler, and Jacob Goldberger. 2012. Efficient tree-based approximation for entailment graph learning. In Proceedings ofAssociation for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="31083" citStr="Bollacker et al., 2008" startWordPosition="4925" endWordPosition="4928">m model with relational phrases, as well as the combination of the bigram model with relational phrases + hypernyms. The results of the experiment are presented in Table 6. The best MRR score was obtained by relational phrases + hypernyms + bigrams model. The number of samples, 96, was large enough for statistical significance. We performed a paired ttest for MRR between each of these methods. The obtained p-values were below 0.05. 5 Related Work The biggest sources of hypernyms, subsumptions, and hierarchical structure can be found in existing knowledge bases. Examples of these are Freebase (Bollacker et al., 2008), YAGO, DBPedia (Lehmann et al., 2014), and Google Knowledge Vault (Dong et al., 2014). However, these knowledge bases are mainly concentrated on named entities and noun phrases, and the variety of relations between entities is much smaller. Relations and information about them are underrepresented. Open Information Extraction systems try to solve this problem by extracting new relations from natural text. These new relations do not necessarily follow the standard schema of knowledge bases. Additionally, these systems often organize the newly extracted relations by clustering or hierarchy cons</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of ACM SIGMOD International Conference on Management of Data, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Broecheler</author>
<author>Lise Getoor</author>
</authors>
<title>Probabilistic similarity logic.</title>
<date>2009</date>
<booktitle>In International Workshop on Statistical Relational Learning.</booktitle>
<contexts>
<context position="35102" citStr="Broecheler and Getoor, 2009" startWordPosition="5553" endWordPosition="5556">izing Textual Entailment (RTE) shared task (Dagan et al., 2005). Instead of short typed relational phrases, the input data are two texts – the entailing text T and the hypothesis text H. According to (Dagan et al., 2005)’s definition, “T entails H if, typically, a human reading T would infer that H is most probably true.” In RELLY, we use probabilistic soft logic (PSL) as the main ingredient of our approach. PSL was successfully used for numerous other applications including knowledge graph construction (Pujara et al., 2013), trust in social networks (Huang et al., 2012b), ontology alignment (Broecheler and Getoor, 2009), and social group modeling (Huang et al., 2012a). 6 Conclusion This paper presents RELLY, a scalable method for integrating statistical and semantic signals to produce a hypernymy graph of relational phrases. We used RELLY to create a hypernymy graph that has both high coverage and precision, as shown in our evaluation. RELLY is extensible and can easily incorporate additional information sources and features. The hypernymy graph of relational phrases could potentially be useful for many problems of natural language processing and information retrieval. For example, we applied the hypernymy g</context>
</contexts>
<marker>Broecheler, Getoor, 2009</marker>
<rawString>Matthias Broecheler and Lise Getoor. 2009. Probabilistic similarity logic. In International Workshop on Statistical Relational Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence D Brown</author>
<author>T Tony Cai</author>
<author>Anirban Dasgupta</author>
</authors>
<title>Interval estimation for a binomial proportion.</title>
<date>2001</date>
<journal>Statistical Science,</journal>
<pages>16--101</pages>
<contexts>
<context position="21537" citStr="Brown et al., 2001" startWordPosition="3390" endWordPosition="3393">se the relational phrase. Based on this information, annotators could make one of four judgments: (1) the phrases are unrelated; (2) the phrases are synonymous; (3) the first phrase is more specific than the second phrase; (4) the second phrase is more specific than the first phrase. This evaluation task had good interannotator agreement, with a Cohen’s Kappa of 0.624. Separately, the precision@100 dataset had Cohen’s Kappa of 0.708 and the randomly sampled dataset had Cohen’s Kappa of 0.521. We show the results of the intrinsic evaluation in Table 3 with 0.9-confidence Wilson score interval (Brown et al., 2001). In comparison to HARPY and PATTY, RELLY has higher precision for both precision@100 and random evaluations. Precision in RELLY is comparable to PATTY, but RELLY has more than four times as many hypernym links. HARPY has far more hypernymy links, but with a precision of 0.43, we find that many of these links are incorrect. Table 4 includes example hypernymy links from RELLY. There are examples where PATTY’s subsumption is a dominant signal (“&lt;person&gt; publicly accused &lt;person&gt;” ⇒ “&lt;person&gt; accused &lt;person&gt;”). We also observe YAGO type hierarchy influence (“&lt;athlete&gt; played for &lt;team&gt;” ⇒ “&lt;pers</context>
</contexts>
<marker>Brown, Cai, Dasgupta, 2001</marker>
<rawString>Lawrence D. Brown, T. Tony Cai, and Anirban Dasgupta. 2001. Interval estimation for a binomial proportion. Statistical Science, 16:101–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for never-ending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of Association for the Advancement of Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="31867" citStr="Carlson et al., 2010" startWordPosition="5046" endWordPosition="5049">d noun phrases, and the variety of relations between entities is much smaller. Relations and information about them are underrepresented. Open Information Extraction systems try to solve this problem by extracting new relations from natural text. These new relations do not necessarily follow the standard schema of knowledge bases. Additionally, these systems often organize the newly extracted relations by clustering or hierarchy construction. A first attempt to extract and cluster similar relations was presented in DIRT. This work was followed by projects such as ReVerb, PATTY, WiseNet, NELL (Carlson et al., 2010), and RESOLVER (Yates and Etzioni, 2009). PATTY and WiseNet also introduced semantic types to their concept of relational phrases. All of these systems rely on the co-occurrence of arguments of clustered relations. A different approach was presented in PPDB, where the authors 978 cluster phrases based on the similarity of translations to other languages. Of these systems, only PATTY attempted to create a hierarchy of relations and the result was very sparse. HARPY aimed to overcome this problem by disambiguating and aligning relational phrases with WordNet, and performing a simple reconstructi</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Hruschka, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka, and Tom M. Mitchell. 2010. Toward an architecture for never-ending language learning. In Proceedings of Association for the Advancement of Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>33--40</pages>
<contexts>
<context position="7938" citStr="Chklovski and Pantel, 2004" startWordPosition="1181" endWordPosition="1185">ype &lt;book&gt;. Several projects from the Open Information Extraction community have addressed the task of finding synonyms of relational phrases using clustering algorithms. The biggest collection of relational phrases and their synonyms is currently the PATTY project (Nakashole et al., 2012), with around 350,000 semantically typed relational phrases. Prominent alternatives are WiseNet (Moro and Navigli, 2012), which offers 40,000 synsets of relational phrases, PPDB (Ganitkevitch et al., 2013), which contains over 220 million paraphrase pairs, as well as DIRT and VerbOcean (Lin and Pantel, 2001; Chklovski and Pantel, 2004) which inspired the approach and results pursued here. Relational phrases can be further organized into a hierarchical structure according to their hypernymy (subsumption) relationships. For example, “&lt;person&gt; moves to &lt;country&gt;” is a hypernym of the relational phrase “&lt;musician&gt; emigrates to &lt;country&gt;.” Of the aforementioned collections, only PATTY attempts to automatically create a subsumption hierarchy for the extracted relational phrases. The authors of the HARPY system argue that the sparseness of PATTY’s graph comes from the lack of general phrases in the source corpus. As a solution, th</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 33– 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge. In</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1988" citStr="Dagan et al., 2005" startWordPosition="269" endWordPosition="272">nal phrases and 35K hypernymy links. Our evaluation indicates a hypernymy link precision of 78%, and demonstrates the value of this resource for a document-relevance ranking task. 1 Introduction One of the many challenges in natural language understanding is interpreting the multiword phrases that denote relationships between entities. Semantically organizing the complex relationships between diverse phrases is crucial to applications including question answering, open information extraction, paraphrasing, and entailment detection (Yahya et al., 2012; Fader et al., 2011; Madnani et al., 2012; Dagan et al., 2005). For example, a corpus containing the phrase “George Burns was married to Gracie Allen” allows us to answer the query “Who was the spouse of George Burns?” However, “Jay Z is in a relationship with Beyonc´e” provides insufficient information to determine whether the couple is married. To capture the knowledge found in text, relational phrases need to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the challenge of understanding relational phrases, but existing lingu</context>
<context position="34537" citStr="Dagan et al., 2005" startWordPosition="5461" endWordPosition="5464">e WordNet taxonomy was extended by 10,000 novel noun synsets with hypernym-hyponym links. In (Bansal et al., 2014), the authors reconstructed WordNet’s noun hypernymy/hyponymy hierarchy from scratch using a probabilistic graphical model formulation. Another method of organizing noun phrases was proposed in (Mehdad et al., 2013), where an entailment graph of noun phrases was constructed. Building a hypernymy graph for relational phrases is strongly related with the textual entailment task (Dagan et al., 2010). This concept was introduced in the Recognizing Textual Entailment (RTE) shared task (Dagan et al., 2005). Instead of short typed relational phrases, the input data are two texts – the entailing text T and the hypothesis text H. According to (Dagan et al., 2005)’s definition, “T entails H if, typically, a human reading T would infer that H is most probably true.” In RELLY, we use probabilistic soft logic (PSL) as the main ingredient of our approach. PSL was successfully used for numerous other applications including knowledge graph construction (Pujara et al., 2013), trust in social networks (Huang et al., 2012b), ontology alignment (Broecheler and Getoor, 2009), and social group modeling (Huang </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: Rational, evaluation and approaches erratum.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<pages>1</pages>
<contexts>
<context position="34431" citStr="Dagan et al., 2010" startWordPosition="5445" endWordPosition="5448">rases has received much more attention than organization of verbs or relations. In (Snow et al., 2006), the WordNet taxonomy was extended by 10,000 novel noun synsets with hypernym-hyponym links. In (Bansal et al., 2014), the authors reconstructed WordNet’s noun hypernymy/hyponymy hierarchy from scratch using a probabilistic graphical model formulation. Another method of organizing noun phrases was proposed in (Mehdad et al., 2013), where an entailment graph of noun phrases was constructed. Building a hypernymy graph for relational phrases is strongly related with the textual entailment task (Dagan et al., 2010). This concept was introduced in the Recognizing Textual Entailment (RTE) shared task (Dagan et al., 2005). Instead of short typed relational phrases, the input data are two texts – the entailing text T and the hypothesis text H. According to (Dagan et al., 2005)’s definition, “T entails H if, typically, a human reading T would infer that H is most probably true.” In RELLY, we use probabilistic soft logic (PSL) as the main ingredient of our approach. PSL was successfully used for numerous other applications including knowledge graph construction (Pujara et al., 2013), trust in social networks </context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2010</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2010. Recognizing textual entailment: Rational, evaluation and approaches erratum. Natural Language Engineering, 16:105–105, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Dong</author>
<author>Evgeniy Gabrilovich</author>
<author>Geremy Heitz</author>
<author>Wilko Horn</author>
<author>Ni Lao</author>
<author>Kevin Murphy</author>
<author>Thomas Strohmann</author>
<author>Shaohua Sun</author>
<author>Wei Zhang</author>
</authors>
<title>Knowledge Vault: A web-scale approach to probabilistic knowledge fusion.</title>
<date>2014</date>
<booktitle>In Proceedings ofACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>601--610</pages>
<contexts>
<context position="31169" citStr="Dong et al., 2014" startWordPosition="4939" endWordPosition="4942">onal phrases + hypernyms. The results of the experiment are presented in Table 6. The best MRR score was obtained by relational phrases + hypernyms + bigrams model. The number of samples, 96, was large enough for statistical significance. We performed a paired ttest for MRR between each of these methods. The obtained p-values were below 0.05. 5 Related Work The biggest sources of hypernyms, subsumptions, and hierarchical structure can be found in existing knowledge bases. Examples of these are Freebase (Bollacker et al., 2008), YAGO, DBPedia (Lehmann et al., 2014), and Google Knowledge Vault (Dong et al., 2014). However, these knowledge bases are mainly concentrated on named entities and noun phrases, and the variety of relations between entities is much smaller. Relations and information about them are underrepresented. Open Information Extraction systems try to solve this problem by extracting new relations from natural text. These new relations do not necessarily follow the standard schema of knowledge bases. Additionally, these systems often organize the newly extracted relations by clustering or hierarchy construction. A first attempt to extract and cluster similar relations was presented in DI</context>
</contexts>
<marker>Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, Zhang, 2014</marker>
<rawString>Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge Vault: A web-scale approach to probabilistic knowledge fusion. In Proceedings ofACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 601–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1535--1545</pages>
<contexts>
<context position="1945" citStr="Fader et al., 2011" startWordPosition="261" endWordPosition="264"> hypernymy graph consisting of 20K relational phrases and 35K hypernymy links. Our evaluation indicates a hypernymy link precision of 78%, and demonstrates the value of this resource for a document-relevance ranking task. 1 Introduction One of the many challenges in natural language understanding is interpreting the multiword phrases that denote relationships between entities. Semantically organizing the complex relationships between diverse phrases is crucial to applications including question answering, open information extraction, paraphrasing, and entailment detection (Yahya et al., 2012; Fader et al., 2011; Madnani et al., 2012; Dagan et al., 2005). For example, a corpus containing the phrase “George Burns was married to Gracie Allen” allows us to answer the query “Who was the spouse of George Burns?” However, “Jay Z is in a relationship with Beyonc´e” provides insufficient information to determine whether the couple is married. To capture the knowledge found in text, relational phrases need to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the challenge of understan</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 1535–1545.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT),</booktitle>
<pages>758--764</pages>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT), pages 758–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Grycner</author>
<author>Gerhard Weikum</author>
</authors>
<title>HARPY: Hypernyms and alignment of relational paraphrases.</title>
<date>2014</date>
<booktitle>In Proceedings of Conference on Computational Linguistics (COLING),</booktitle>
<pages>2195--2204</pages>
<contexts>
<context position="3127" citStr="Grycner and Weikum, 2014" startWordPosition="449" endWordPosition="453">projects address the challenge of understanding relational phrases, but existing linguistic resources are often limited to synonymy, suffer from low precision, or have low coverage. Systems such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), and WiseNet (Moro and Navigli, 2012) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the largest component consisted of 120 relations. A number of manuallycurated relational taxonomies such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998) also offer highprecision hierarchies with limited coverage. In this paper, we introduce RELLY, a method for producing a hypernymy graph that has both high coverage and p</context>
<context position="13281" citStr="Grycner and Weikum, 2014" startWordPosition="2073" endWordPosition="2076">pernymy, as well as rules to encode consistency in the graph. One statistical signal for phrase subsumption is argument overlap. If the arguments to a relational phrase R1 are also found as arguments to another relational phrase R2, R1 and R2 may be synonymous or 973 Table 1: PSL predicates; R1, R2 are relational phrases, V b1, V b2 WordNet verb senses and TL1, TR1, T1, T2 YAGO types PSL predicate Type Description 2003) patty5ubsumption(R1, R2) statistical PATTY subsumption (Nakashole et al., 2012) harpy(R1, V b1) statistical alignment links between relational phrases and WordNet verb senses (Grycner and Weikum, 2014) wordnetHyponym(V b1, V b2) semantic hyponymy link between WordNet verb senses lType(R1, TL1) semantic left (domain) type of arguments of a relational phrase rType(R1, TR1) semantic right (range) type of arguments of a relational phrase yagoHyponym(T 1, T2) semantic T1 is a subtype of T2 in YAGO hierarchy candidateHyponym(R1, R2) output relational phrase R1 is more specific than R2 (without enforcing consistent argument types) hyponym(R1, R2) output relational phrase R1 is more specific than R2 weedsInclusion(R1, R2) statistical degree of inclusion of sets of argument pairs of relations define</context>
</contexts>
<marker>Grycner, Weikum, 2014</marker>
<rawString>Adam Grycner and Gerhard Weikum. 2014. HARPY: Hypernyms and alignment of relational paraphrases. In Proceedings of Conference on Computational Linguistics (COLING), pages 2195–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bert Huang</author>
<author>Stephen H Bach</author>
<author>Eric Norris</author>
<author>Jay Pujara</author>
<author>Lise Getoor</author>
</authors>
<title>Social group modeling with probabilistic soft logic.</title>
<date>2012</date>
<booktitle>In NIPS Workshop on Social Network</booktitle>
<contexts>
<context position="35050" citStr="Huang et al., 2012" startWordPosition="5547" endWordPosition="5550"> This concept was introduced in the Recognizing Textual Entailment (RTE) shared task (Dagan et al., 2005). Instead of short typed relational phrases, the input data are two texts – the entailing text T and the hypothesis text H. According to (Dagan et al., 2005)’s definition, “T entails H if, typically, a human reading T would infer that H is most probably true.” In RELLY, we use probabilistic soft logic (PSL) as the main ingredient of our approach. PSL was successfully used for numerous other applications including knowledge graph construction (Pujara et al., 2013), trust in social networks (Huang et al., 2012b), ontology alignment (Broecheler and Getoor, 2009), and social group modeling (Huang et al., 2012a). 6 Conclusion This paper presents RELLY, a scalable method for integrating statistical and semantic signals to produce a hypernymy graph of relational phrases. We used RELLY to create a hypernymy graph that has both high coverage and precision, as shown in our evaluation. RELLY is extensible and can easily incorporate additional information sources and features. The hypernymy graph of relational phrases could potentially be useful for many problems of natural language processing and informatio</context>
</contexts>
<marker>Huang, Bach, Norris, Pujara, Getoor, 2012</marker>
<rawString>Bert Huang, Stephen H. Bach, Eric Norris, Jay Pujara, and Lise Getoor. 2012a. Social group modeling with probabilistic soft logic. In NIPS Workshop on Social Network and Social Media Analysis: Methods, Models, and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bert Huang</author>
<author>Angelika Kimmig</author>
<author>Lise Getoor</author>
<author>Jennifer Golbeck</author>
</authors>
<title>Probabilistic soft logic for trust analysis in social networks.</title>
<date>2012</date>
<booktitle>In International Workshop on Statistical Relational Artificial Intelligence (StaRAI2012).</booktitle>
<contexts>
<context position="35050" citStr="Huang et al., 2012" startWordPosition="5547" endWordPosition="5550"> This concept was introduced in the Recognizing Textual Entailment (RTE) shared task (Dagan et al., 2005). Instead of short typed relational phrases, the input data are two texts – the entailing text T and the hypothesis text H. According to (Dagan et al., 2005)’s definition, “T entails H if, typically, a human reading T would infer that H is most probably true.” In RELLY, we use probabilistic soft logic (PSL) as the main ingredient of our approach. PSL was successfully used for numerous other applications including knowledge graph construction (Pujara et al., 2013), trust in social networks (Huang et al., 2012b), ontology alignment (Broecheler and Getoor, 2009), and social group modeling (Huang et al., 2012a). 6 Conclusion This paper presents RELLY, a scalable method for integrating statistical and semantic signals to produce a hypernymy graph of relational phrases. We used RELLY to create a hypernymy graph that has both high coverage and precision, as shown in our evaluation. RELLY is extensible and can easily incorporate additional information sources and features. The hypernymy graph of relational phrases could potentially be useful for many problems of natural language processing and informatio</context>
</contexts>
<marker>Huang, Kimmig, Getoor, Golbeck, 2012</marker>
<rawString>Bert Huang, Angelika Kimmig, Lise Getoor, and Jennifer Golbeck. 2012b. Probabilistic soft logic for trust analysis in social networks. In International Workshop on Statistical Relational Artificial Intelligence (StaRAI2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>A large-scale classification of english verbs.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="3522" citStr="Kipper et al., 2008" startWordPosition="509" endWordPosition="512">bsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the largest component consisted of 120 relations. A number of manuallycurated relational taxonomies such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998) also offer highprecision hierarchies with limited coverage. In this paper, we introduce RELLY, a method for producing a hypernymy graph that has both high coverage and precision. We build on previous work, integrating the high-precision knowledge in resources such as YAGO (Suchanek et al., 971 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 971–981, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2007) and WordNet with noisy statistical information from OpenIE projects PA</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classification of english verbs. Language Resources and Evaluation, 42(1):21–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Lehmann</author>
<author>Robert Isele</author>
<author>Max Jakob</author>
<author>Anja Jentzsch</author>
</authors>
<title>Dimitris Kontokostas, Pablo Mendes,</title>
<date>2014</date>
<location>Sebastian Hellmann, Mohamed Morsey, Patrick</location>
<contexts>
<context position="31121" citStr="Lehmann et al., 2014" startWordPosition="4931" endWordPosition="4934"> as the combination of the bigram model with relational phrases + hypernyms. The results of the experiment are presented in Table 6. The best MRR score was obtained by relational phrases + hypernyms + bigrams model. The number of samples, 96, was large enough for statistical significance. We performed a paired ttest for MRR between each of these methods. The obtained p-values were below 0.05. 5 Related Work The biggest sources of hypernyms, subsumptions, and hierarchical structure can be found in existing knowledge bases. Examples of these are Freebase (Bollacker et al., 2008), YAGO, DBPedia (Lehmann et al., 2014), and Google Knowledge Vault (Dong et al., 2014). However, these knowledge bases are mainly concentrated on named entities and noun phrases, and the variety of relations between entities is much smaller. Relations and information about them are underrepresented. Open Information Extraction systems try to solve this problem by extracting new relations from natural text. These new relations do not necessarily follow the standard schema of knowledge bases. Additionally, these systems often organize the newly extracted relations by clustering or hierarchy construction. A first attempt to extract a</context>
</contexts>
<marker>Lehmann, Isele, Jakob, Jentzsch, 2014</marker>
<rawString>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, S¨oren Auer, and Chris Bizer. 2014. DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Focused entailment graphs for Open IE propositions.</title>
<date>2014</date>
<booktitle>In Proceedings of Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>87--97</pages>
<contexts>
<context position="32624" citStr="Levy et al., 2014" startWordPosition="5170" endWordPosition="5173">hese systems rely on the co-occurrence of arguments of clustered relations. A different approach was presented in PPDB, where the authors 978 cluster phrases based on the similarity of translations to other languages. Of these systems, only PATTY attempted to create a hierarchy of relations and the result was very sparse. HARPY aimed to overcome this problem by disambiguating and aligning relational phrases with WordNet, and performing a simple reconstruction of the WordNet hierarchy on top of relational phrases from PATTY. A very similar problem was addressed in the entailment graph project (Levy et al., 2014). The authors automatically created graphs of entailments between propositions, using Integer Linear Programing as one of the main components. Propositions can be encoded as triples of form (subject, relation, object). Edges in the entailment graph occur between these triples, whereas edges connect typed relations in PATTY and HARPY. Moreover, the relations in the propositions were mainly limited to single verbs, whereas in our case we also consider longer relational phrases. Relations with semantic types were also used in typed entailment graphs (Berant et al., 2011). However, the type hierar</context>
</contexts>
<marker>Levy, Dagan, Goldberger, 2014</marker>
<rawString>Omer Levy, Ido Dagan, and Jacob Goldberger. 2014. Focused entailment graphs for Open IE propositions. In Proceedings of Conference on Computational Natural Language Learning (CoNLL), pages 87–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT @SBT@discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="2727" citStr="Lin and Pantel, 2001" startWordPosition="389" endWordPosition="392"> “Who was the spouse of George Burns?” However, “Jay Z is in a relationship with Beyonc´e” provides insufficient information to determine whether the couple is married. To capture the knowledge found in text, relational phrases need to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the challenge of understanding relational phrases, but existing linguistic resources are often limited to synonymy, suffer from low precision, or have low coverage. Systems such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), and WiseNet (Moro and Navigli, 2012) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method requ</context>
<context position="7909" citStr="Lin and Pantel, 2001" startWordPosition="1177" endWordPosition="1180">he relation) has the type &lt;book&gt;. Several projects from the Open Information Extraction community have addressed the task of finding synonyms of relational phrases using clustering algorithms. The biggest collection of relational phrases and their synonyms is currently the PATTY project (Nakashole et al., 2012), with around 350,000 semantically typed relational phrases. Prominent alternatives are WiseNet (Moro and Navigli, 2012), which offers 40,000 synsets of relational phrases, PPDB (Ganitkevitch et al., 2013), which contains over 220 million paraphrase pairs, as well as DIRT and VerbOcean (Lin and Pantel, 2001; Chklovski and Pantel, 2004) which inspired the approach and results pursued here. Relational phrases can be further organized into a hierarchical structure according to their hypernymy (subsumption) relationships. For example, “&lt;person&gt; moves to &lt;country&gt;” is a hypernym of the relational phrase “&lt;musician&gt; emigrates to &lt;country&gt;.” Of the aforementioned collections, only PATTY attempts to automatically create a subsumption hierarchy for the extracted relational phrases. The authors of the HARPY system argue that the sparseness of PATTY’s graph comes from the lack of general phrases in the sou</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT @SBT@discovery of inference rules from text. In Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT),</booktitle>
<pages>182--190</pages>
<contexts>
<context position="1967" citStr="Madnani et al., 2012" startWordPosition="265" endWordPosition="268">sisting of 20K relational phrases and 35K hypernymy links. Our evaluation indicates a hypernymy link precision of 78%, and demonstrates the value of this resource for a document-relevance ranking task. 1 Introduction One of the many challenges in natural language understanding is interpreting the multiword phrases that denote relationships between entities. Semantically organizing the complex relationships between diverse phrases is crucial to applications including question answering, open information extraction, paraphrasing, and entailment detection (Yahya et al., 2012; Fader et al., 2011; Madnani et al., 2012; Dagan et al., 2005). For example, a corpus containing the phrase “George Burns was married to Gracie Allen” allows us to answer the query “Who was the spouse of George Burns?” However, “Jay Z is in a relationship with Beyonc´e” provides insufficient information to determine whether the couple is married. To capture the knowledge found in text, relational phrases need to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the challenge of understanding relational phrase</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT), pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Shafiq Joty</author>
</authors>
<title>Towards topic labeling with phrase entailment and aggregation.</title>
<date>2013</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT),</booktitle>
<pages>179--189</pages>
<contexts>
<context position="34247" citStr="Mehdad et al., 2013" startWordPosition="5415" endWordPosition="5418">erbNet and FrameNet. WordNet has 13,767 verb synsets, which are organized into a hierarchy with 13,239 hypernymy links. Automatic construction of taxonomies of named entities or noun phrases has received much more attention than organization of verbs or relations. In (Snow et al., 2006), the WordNet taxonomy was extended by 10,000 novel noun synsets with hypernym-hyponym links. In (Bansal et al., 2014), the authors reconstructed WordNet’s noun hypernymy/hyponymy hierarchy from scratch using a probabilistic graphical model formulation. Another method of organizing noun phrases was proposed in (Mehdad et al., 2013), where an entailment graph of noun phrases was constructed. Building a hypernymy graph for relational phrases is strongly related with the textual entailment task (Dagan et al., 2010). This concept was introduced in the Recognizing Textual Entailment (RTE) shared task (Dagan et al., 2005). Instead of short typed relational phrases, the input data are two texts – the entailing text T and the hypothesis text H. According to (Dagan et al., 2005)’s definition, “T entails H if, typically, a human reading T would infer that H is most probably true.” In RELLY, we use probabilistic soft logic (PSL) a</context>
</contexts>
<marker>Mehdad, Carenini, Ng, Joty, 2013</marker>
<rawString>Yashar Mehdad, Giuseppe Carenini, Raymond T. Ng, and Shafiq Joty. 2013. Towards topic labeling with phrase entailment and aggregation. In Proceedings of North American Chapter of the Association for Computational Linguistics Human Language Technologies (NAACL-HLT), pages 179–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="6348" citStr="Mikolov et al., 2013" startWordPosition="938" endWordPosition="941">LY is extensible and can easily incorporate additional information sources and features. Second, we generate a complete and precise hypernymy graph over 20K relational phrases and 35K hypernymy links. We have publicly released this hypernymy graph as a resource for the NLP community. Third, we present a thorough empirical evaluation to measure the precision of the hypernymy graph as well as demonstrate its usefulness in a real-world document ranking task. Our results show a high precision (0.78) and superior performance in document ranking compared to state-of-the-art models such as word2vec (Mikolov et al., 2013). 2 Background Before describing the details of RELLY, we begin with necessary background information on the task of semantically organizing relational phrases, as well as the probabilistic soft logic modeling language which we use to develop our hypernymy graph construction method. 2.1 Relational Phrases Relational phrases are textual representations of relations which occur between named entities (e.g., “Terry Pratchett”) or noun phrases (e.g., “the great writer”). Nakashole et al. (2012) identify relational phrases with the semantic type signature of the relation, i.e. the fine-grained lexi</context>
<context position="28647" citStr="Mikolov et al., 2013" startWordPosition="4522" endWordPosition="4525">DB with the goal of maximizing the ranking of the corresponding “Iron Man” plot summary. We evaluate the quality of these rankings using the mean reciprocal rank (MRR) score, MRR = Q � Q 1 1ranki. Here, Q is the number of documents in the collection (i.e. 2*48 = 96) and rankz is the position of the counterpart document in the ranking of document i. As baseline algorithms, we use a unigram word2vec model and a bigram model. In the unigram word2vec model documents are represented by the average of the 300-dimensional word vectors trained on part of Google News dataset (about 100 billion words) (Mikolov et al., 2013). We could not use the bigram word2vec model because of the frequent occurrence of the placeholder symbol. In the bigram model, documents are represented by vectors in the bag-of-bigrams model with bigram frequency weights. The similarity measure in both cases is the cosine similarity measure. As the first of our approaches we proposed a solution purely based on relational phrases. In the relational phrases model we extract relational phrases from a text and we map them to their synsets from PATTY (clusters of synonyms). A phrase is mapped to a synset if the Jaccard similarity between tokens o</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Roberto Navigli</author>
</authors>
<title>WiseNet: building a wikipedia-based semantic network with ontologized relations.</title>
<date>2012</date>
<booktitle>In Proceedings ofACM International Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>1672--1676</pages>
<contexts>
<context position="2801" citStr="Moro and Navigli, 2012" startWordPosition="401" endWordPosition="404">hip with Beyonc´e” provides insufficient information to determine whether the couple is married. To capture the knowledge found in text, relational phrases need to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the challenge of understanding relational phrases, but existing linguistic resources are often limited to synonymy, suffer from low precision, or have low coverage. Systems such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), and WiseNet (Moro and Navigli, 2012) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the largest component consisted of 120 rel</context>
<context position="7721" citStr="Moro and Navigli, 2012" startWordPosition="1146" endWordPosition="1149">ational phrase “&lt;person&gt; published his * ADJ novel &lt;book&gt;.” In this case, the left-hand argument (the domain of the relation) has the type &lt;person&gt; and the right-hand argument (the range of the relation) has the type &lt;book&gt;. Several projects from the Open Information Extraction community have addressed the task of finding synonyms of relational phrases using clustering algorithms. The biggest collection of relational phrases and their synonyms is currently the PATTY project (Nakashole et al., 2012), with around 350,000 semantically typed relational phrases. Prominent alternatives are WiseNet (Moro and Navigli, 2012), which offers 40,000 synsets of relational phrases, PPDB (Ganitkevitch et al., 2013), which contains over 220 million paraphrase pairs, as well as DIRT and VerbOcean (Lin and Pantel, 2001; Chklovski and Pantel, 2004) which inspired the approach and results pursued here. Relational phrases can be further organized into a hierarchical structure according to their hypernymy (subsumption) relationships. For example, “&lt;person&gt; moves to &lt;country&gt;” is a hypernym of the relational phrase “&lt;musician&gt; emigrates to &lt;country&gt;.” Of the aforementioned collections, only PATTY attempts to automatically creat</context>
</contexts>
<marker>Moro, Navigli, 2012</marker>
<rawString>Andrea Moro and Roberto Navigli. 2012. WiseNet: building a wikipedia-based semantic network with ontologized relations. In Proceedings ofACM International Conference on Information and Knowledge Management (CIKM), pages 1672–1676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>PATTY: A taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<booktitle>In Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLP-CoNLL,</booktitle>
<pages>1135--1145</pages>
<contexts>
<context position="2959" citStr="Nakashole et al., 2012" startWordPosition="423" endWordPosition="426">ed to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the challenge of understanding relational phrases, but existing linguistic resources are often limited to synonymy, suffer from low precision, or have low coverage. Systems such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), and WiseNet (Moro and Navigli, 2012) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the largest component consisted of 120 relations. A number of manuallycurated relational taxonomies such as WordNet (Fellbaum, 1998), VerbNet (Kipper et al., 2008), and FrameNet (Baker et al., 1998) a</context>
<context position="6843" citStr="Nakashole et al. (2012)" startWordPosition="1010" endWordPosition="1013"> (0.78) and superior performance in document ranking compared to state-of-the-art models such as word2vec (Mikolov et al., 2013). 2 Background Before describing the details of RELLY, we begin with necessary background information on the task of semantically organizing relational phrases, as well as the probabilistic soft logic modeling language which we use to develop our hypernymy graph construction method. 2.1 Relational Phrases Relational phrases are textual representations of relations which occur between named entities (e.g., “Terry Pratchett”) or noun phrases (e.g., “the great writer”). Nakashole et al. (2012) identify relational phrases with the semantic type signature of the relation, i.e. the fine-grained lexical types of left- and right-hand side arguments. For example, “Terry Pratchett published his new novel The Colour of Magic” is an instance of the relational phrase “&lt;person&gt; published his * ADJ novel &lt;book&gt;.” In this case, the left-hand argument (the domain of the relation) has the type &lt;person&gt; and the right-hand argument (the range of the relation) has the type &lt;book&gt;. Several projects from the Open Information Extraction community have addressed the task of finding synonyms of relationa</context>
<context position="13159" citStr="Nakashole et al., 2012" startWordPosition="2055" endWordPosition="2058">l and semantic signals into the hypernymy graph. Our model includes rules to encode signals that provide evidence for hypernymy, as well as rules to encode consistency in the graph. One statistical signal for phrase subsumption is argument overlap. If the arguments to a relational phrase R1 are also found as arguments to another relational phrase R2, R1 and R2 may be synonymous or 973 Table 1: PSL predicates; R1, R2 are relational phrases, V b1, V b2 WordNet verb senses and TL1, TR1, T1, T2 YAGO types PSL predicate Type Description 2003) patty5ubsumption(R1, R2) statistical PATTY subsumption (Nakashole et al., 2012) harpy(R1, V b1) statistical alignment links between relational phrases and WordNet verb senses (Grycner and Weikum, 2014) wordnetHyponym(V b1, V b2) semantic hyponymy link between WordNet verb senses lType(R1, TL1) semantic left (domain) type of arguments of a relational phrase rType(R1, TR1) semantic right (range) type of arguments of a relational phrase yagoHyponym(T 1, T2) semantic T1 is a subtype of T2 in YAGO hierarchy candidateHyponym(R1, R2) output relational phrase R1 is more specific than R2 (without enforcing consistent argument types) hyponym(R1, R2) output relational phrase R1 is </context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. PATTY: A taxonomy of relational patterns with semantic types. In Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLP-CoNLL, pages 1135– 1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Pujara</author>
<author>Hui Miao</author>
<author>Lise Getoor</author>
<author>William Cohen</author>
</authors>
<title>Knowledge graph identification.</title>
<date>2013</date>
<booktitle>In International Semantic Web Conference (ISWC).</booktitle>
<contexts>
<context position="35004" citStr="Pujara et al., 2013" startWordPosition="5539" endWordPosition="5542">he textual entailment task (Dagan et al., 2010). This concept was introduced in the Recognizing Textual Entailment (RTE) shared task (Dagan et al., 2005). Instead of short typed relational phrases, the input data are two texts – the entailing text T and the hypothesis text H. According to (Dagan et al., 2005)’s definition, “T entails H if, typically, a human reading T would infer that H is most probably true.” In RELLY, we use probabilistic soft logic (PSL) as the main ingredient of our approach. PSL was successfully used for numerous other applications including knowledge graph construction (Pujara et al., 2013), trust in social networks (Huang et al., 2012b), ontology alignment (Broecheler and Getoor, 2009), and social group modeling (Huang et al., 2012a). 6 Conclusion This paper presents RELLY, a scalable method for integrating statistical and semantic signals to produce a hypernymy graph of relational phrases. We used RELLY to create a hypernymy graph that has both high coverage and precision, as shown in our evaluation. RELLY is extensible and can easily incorporate additional information sources and features. The hypernymy graph of relational phrases could potentially be useful for many problems</context>
</contexts>
<marker>Pujara, Miao, Getoor, Cohen, 2013</marker>
<rawString>Jay Pujara, Hui Miao, Lise Getoor, and William Cohen. 2013. Knowledge graph identification. In International Semantic Web Conference (ISWC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Y Andrew Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Computational Linguistics and International Conference on Computational Linguistics (COLING/ACL),</booktitle>
<pages>801--808</pages>
<contexts>
<context position="33914" citStr="Snow et al., 2006" startWordPosition="5367" endWordPosition="5370">n two relations with different semantic types. The input dataset was also smaller – the biggest graph consisted of 118 relations. Although there is a scarcity of automatically created taxonomies of relations, there exist several manually curated taxonomies. Manually crafted verb or relation hierarchies are available in WordNet, VerbNet and FrameNet. WordNet has 13,767 verb synsets, which are organized into a hierarchy with 13,239 hypernymy links. Automatic construction of taxonomies of named entities or noun phrases has received much more attention than organization of verbs or relations. In (Snow et al., 2006), the WordNet taxonomy was extended by 10,000 novel noun synsets with hypernym-hyponym links. In (Bansal et al., 2014), the authors reconstructed WordNet’s noun hypernymy/hyponymy hierarchy from scratch using a probabilistic graphical model formulation. Another method of organizing noun phrases was proposed in (Mehdad et al., 2013), where an entailment graph of noun phrases was constructed. Building a hypernymy graph for relational phrases is strongly related with the textual entailment task (Dagan et al., 2010). This concept was introduced in the Recognizing Textual Entailment (RTE) shared ta</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Y. Andrew Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings ofAssociation for Computational Linguistics and International Conference on Computational Linguistics (COLING/ACL), pages 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on World Wide Web (WWW),</booktitle>
<pages>697--706</pages>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of International Conference on World Wide Web (WWW), pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>81--88</pages>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Maya Ramanath</author>
<author>Volker Tresp</author>
<author>Gerhard Weikum</author>
</authors>
<title>Natural language questions for the web of data.</title>
<date>2012</date>
<booktitle>In Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLP-CoNLL,</booktitle>
<pages>379--390</pages>
<contexts>
<context position="1925" citStr="Yahya et al., 2012" startWordPosition="257" endWordPosition="260">rage, high-precision hypernymy graph consisting of 20K relational phrases and 35K hypernymy links. Our evaluation indicates a hypernymy link precision of 78%, and demonstrates the value of this resource for a document-relevance ranking task. 1 Introduction One of the many challenges in natural language understanding is interpreting the multiword phrases that denote relationships between entities. Semantically organizing the complex relationships between diverse phrases is crucial to applications including question answering, open information extraction, paraphrasing, and entailment detection (Yahya et al., 2012; Fader et al., 2011; Madnani et al., 2012; Dagan et al., 2005). For example, a corpus containing the phrase “George Burns was married to Gracie Allen” allows us to answer the query “Who was the spouse of George Burns?” However, “Jay Z is in a relationship with Beyonc´e” provides insufficient information to determine whether the couple is married. To capture the knowledge found in text, relational phrases need to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the ch</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural language questions for the web of data. In Proceedings of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning EMNLP-CoNLL, pages 379–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2763" citStr="Yates and Etzioni, 2009" startWordPosition="395" endWordPosition="398">rns?” However, “Jay Z is in a relationship with Beyonc´e” provides insufficient information to determine whether the couple is married. To capture the knowledge found in text, relational phrases need to be systematically organized with lexical links like synonymy (“married to” and “spouse of”) and hypernymy (“in a relationship” generalizing “married to”). Many projects address the challenge of understanding relational phrases, but existing linguistic resources are often limited to synonymy, suffer from low precision, or have low coverage. Systems such as DIRT (Lin and Pantel, 2001), RESOLVER (Yates and Etzioni, 2009), and WiseNet (Moro and Navigli, 2012) have used sophisticated clustering techniques to determine synonymous phrases, but do not provide subsumption information. The PATTY (Nakashole et al., 2012) project goes beyond clustering and introduces a subsumption hierarchy, but suffers from sparsity and contains few hypernymy links. The HARPY (Grycner and Weikum, 2014) project extended PATTY, generating 600K hypernymy links, but with low precision. Berant et al. (2011) introduced entailment graphs that provided a high-quality subsumption hierarchy. This method required partitioning the graph and the </context>
<context position="31907" citStr="Yates and Etzioni, 2009" startWordPosition="5052" endWordPosition="5055">lations between entities is much smaller. Relations and information about them are underrepresented. Open Information Extraction systems try to solve this problem by extracting new relations from natural text. These new relations do not necessarily follow the standard schema of knowledge bases. Additionally, these systems often organize the newly extracted relations by clustering or hierarchy construction. A first attempt to extract and cluster similar relations was presented in DIRT. This work was followed by projects such as ReVerb, PATTY, WiseNet, NELL (Carlson et al., 2010), and RESOLVER (Yates and Etzioni, 2009). PATTY and WiseNet also introduced semantic types to their concept of relational phrases. All of these systems rely on the co-occurrence of arguments of clustered relations. A different approach was presented in PPDB, where the authors 978 cluster phrases based on the similarity of translations to other languages. Of these systems, only PATTY attempted to create a hierarchy of relations and the result was very sparse. HARPY aimed to overcome this problem by disambiguating and aligning relational phrases with WordNet, and performing a simple reconstruction of the WordNet hierarchy on top of re</context>
</contexts>
<marker>Yates, Etzioni, 2009</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Journal of Artificial Intelligence Research, 34(1):255–296.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
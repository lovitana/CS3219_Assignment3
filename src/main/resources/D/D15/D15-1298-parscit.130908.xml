<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998694">
PhraseRNN: Phrase Recursive Neural Network
for Aspect-based Sentiment Analysis
</title>
<author confidence="0.998279">
Thien Hai Nguyen Kiyoaki Shirai
</author>
<affiliation confidence="0.9975115">
School of Information Science
Japan Advanced Institute of Science and Technology
</affiliation>
<address confidence="0.936191">
1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan
</address>
<email confidence="0.998861">
{nhthien, kshirai}@jaist.ac.jp
</email>
<sectionHeader confidence="0.997378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880875">
This paper presents a new method to iden-
tify sentiment of an aspect of an entity. It
is an extension of RNN (Recursive Neu-
ral Network) that takes both dependency
and constituent trees of a sentence into ac-
count. Results of an experiment show that
our method significantly outperforms pre-
vious methods.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999548113636364">
Aspect-based sentiment analysis (ABSA) has been
found to play a significant role in many applica-
tions such as opinion mining on product or restau-
rant reviews. It is a task to determine an attitude,
opinion and emotions of people toward aspects in
a sentence. For example, given a sentence “Except
the design, the phone is bad for me”, the system
should classify positive and negative as the senti-
ments for the aspects ‘design’ and ‘phone’, respec-
tively.
The simple approach is to calculate a sentiment
score of a given aspect as the weighted sum of
opinion scores, which are defined by a sentiment
lexicon, of all words in the sentence (Liu and
Zhang, 2012; Pang and Lee, 2008). This method is
further improved by identifying the aspect-opinion
relations using tree kernel method (Nguyen and
Shirai, 2015a).
Other researches have attempted to use unsuper-
vised topic modeling methods. To identify the sen-
timent category of the aspect, topic models which
can simultaneously exploit aspect and sentiment
have been proposed, such as TSLDA (Nguyen and
Shirai, 2015b), ASUM (Jo and Oh, 2011), JST
(Lin and He, 2009) and FACTS model (Lakkaraju
et al., 2011).
Recursive Neural Network (RNN) is a kind of
deep neural network. Using distributed represen-
tations of words (aka word embedding) (Bengio et
al., 2003; Hinton, 1986), RNN merges word rep-
resentations to represent phrases or sentences. It
is one of the best methods to predict sentiment la-
bels for the phrases (Socher et al., 2011; Socher et
al., 2012; Socher et al., 2013). AdaRNN (Adap-
tive Recursive Neural Network) is an extension of
RNN for Twitter sentiment classification (Dong et
al., 2014a; Dong et al., 2014b).
This paper proposes a new method PhraseRNN
for ABSA. It is an extended model of RNN and
AdaRNN, which are briefly introduced in Section
2. The basic idea is to make the representation of
the target aspect richer by using syntactic infor-
mation from both the dependency and constituent
trees of the sentence.
</bodyText>
<sectionHeader confidence="0.990168" genericHeader="method">
2 Recursive Neural Networks for ABSA
</sectionHeader>
<bodyText confidence="0.992268727272727">
In RNN and AdaRNN, given a sentence contain-
ing a target aspect, “binary dependency tree” is
built from a dependency tree of the sentence. Intu-
itively, it represents syntactic relations associated
with the aspect. Each word (leaf) or phrase (inter-
nal node) in the binary dependency tree is repre-
sented as a d-dimensional vector. From bottom to
up, the representations of a parent node v is calcu-
lated by combination of left and right child vector
representations (vl and vr) using a global function
g in RNN:
</bodyText>
<equation confidence="0.9974735">
g(vl, vr) = W Ivr]
vl+ b (1)
</equation>
<bodyText confidence="0.949527875">
where W E Rd×2d is the composition matrix and
b E Rd is the bias vector. Then v = f(g(vl, vr))
where f is a nonlinear function such as tanh.
Instead of using only a global function g,
AdaRNN employed n compositional functions
G = {g1, · · · , gn} and selected them depending
on the linguistic tags and combined vectors as fol-
lows:
</bodyText>
<equation confidence="0.9813375">
�P(gi|vl, vr, e)gi(vl, vr) (2)
n
v = f
i=1
</equation>
<page confidence="0.932499">
2509
</page>
<note confidence="0.9301235">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2509–2514,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.999064">
Figure 1: Example of a Constituent Tree
</figureCaption>
<bodyText confidence="0.97808625">
where P(gi|vl, vr, e) is the probability of function
gi given the child vectors vl, vr and external fea-
ture vector e. The probabilities are estimated as
Equation (3).
</bodyText>
<equation confidence="0.9238095">
⎤ vl
= so f tmax (βR vr
e 1)
(3)
</equation>
<bodyText confidence="0.992119166666667">
where β ∈ &lt; is a hyper-parameter, and R ∈
&lt;nx(2d+|e|) is the parameter matrix.
The vector of the root node of the binary depen-
dency tree is regarded as a representation of the
target aspect. It is fed to a logistic regression to
predict the sentiment category of the aspect.
</bodyText>
<sectionHeader confidence="0.941608" genericHeader="method">
3 PhraseRNN: Phrase Recursive Neural
Network
</sectionHeader>
<bodyText confidence="0.999986857142857">
In this model, a representation of an aspect will be
obtained from a “target dependent binary phrase
dependency tree” constructed by combining the
constituent and dependency trees. In addition, in-
stead of using a list of global functions G as in
AdaRNN, two kinds of composition functions G
in inner-phrase and H in outer-phrase are used.
</bodyText>
<subsectionHeader confidence="0.999852">
3.1 Building Hierarchical Structure
</subsectionHeader>
<bodyText confidence="0.837928086956522">
First, the basic phrases (noun phrases, verb
phrases, preposition phrases and so on) are ex-
tracted from the constituent tree of the sentence.
For example, a list of phrases P = {PP[Except the
design], NP[the phone], VP[is bad for me]} is ex-
tracted from the constituent tree in Figure 1.
Given a dependency tree and a list of phrases,
a phrase dependency tree is created by Algorithm
1. The input is a dependency tree T = (V, E)
consisting of a set of vertices V = {v1, · · · , v|V |}
and a set of relation edges E = {(rji, vi, vj)}
between two vertices, and a list of phrases P =
{p1, · · · , pK} extracted from the constituent tree.
The output is a phrase dependency tree pT =
(pV, pE) where pV = {T1, · · · , TK} (Ti =
(Vi, Ei) is a subtree) and pE = {(rji, Ti, Tj)} (a
set of relations between two subtrees). With the
dependency tree and the phrase list in Figure 2(a),
the algorithm will output a phrase dependency tree
in Figure 2(b).
Algorithm 1: Convert to Phrase Dependency
Tree
Input: dependency tree T = (V, E), phrase
</bodyText>
<equation confidence="0.7505075">
list P = {p1, · · · , pK}
Output: phrase dependency tree:
pT = (pV, pE) where
pV = {T1, ··· , TK}, Ti = (Vi, Ei)
and pE = {(rji, Ti, Tj)}
1 for each phrase pi ∈ P do
2 Vi ← {vj|vj ∈ pi}
3 end
4 for each edge (rnm, vm, vn) ∈ E do
vm ∈ pk, vn ∈ pl
ifk=lthen
Ek ← Ek ∪ {(rnm, vm, vn)}
else
pE ← pE ∪ {(rnm, Tk, Tl)}
end
11 end
</equation>
<bodyText confidence="0.999927">
The phrase dependency tree is transformed into
a target dependent binary phrase dependency tree
bpT by Algorithm 2. The input of the algorithm
is a phrase dependency tree pT = (pV, pE) and a
target word vt (the aspect word we want to predict
the sentiment category). The output is the binary
tree bpT. Note that the leaves of the binary tree
bpT are binary subtrees bT1, · · · , bTK which are
the binary versions of subtrees T1, · · · , TK. On
the other hand, the leaves of binary subtree bTi are
the words in phrase pi. bpT and bTi are obtained
by convert function defined as Algorithm 3. It can
convert an arbitrary tree to a binary tree 1. Figure
2(c) and Figure 3 show the outputs for the aspect
‘design’ and ’phone’, respectively.
</bodyText>
<subsectionHeader confidence="0.999904">
3.2 Constructing the Aspect Representation
</subsectionHeader>
<bodyText confidence="0.9987105">
Each node in the binary tree is represented as a d-
dimensional vector. In this research, we use the
pre-trained Google News dataset 2 by word2vec
algorithms (Mikolov et al., 2013). Each word is
</bodyText>
<footnote confidence="0.998979333333333">
1Note that convert function returns a tree represented by
nested brackets such as [PP,[NP,VP]].
2https://code.google.com/p/word2vec/
</footnote>
<figure confidence="0.815808166666667">
P(g1|vl, vr, e)
· · ·
P(gn|vl, vr, e)
⎡
⎣
5
6
7
8
9
10
2510
</figure>
<figureCaption confidence="0.7753345">
Figure 2: Hierarchical Structures in PhraseRNN: (a) Dependency Tree, (b) Phrase Dependency Tree and
(c) Target Dependent Binary Phrase Dependency Tree
</figureCaption>
<figure confidence="0.939371833333333">
Input: phrase dependency tree:
pT = (pV, pE), target vt
Output: target dependent binary phrase
dependency tree: bpT
1 for Ti = (Vi, Ei) E pV do
2 if vt E Vi then
3 h + —vt
4 else
5 h +— vertex having no head in Ei
6 end
7 bTi +— convert(Ei, h)
8 end
9 Tvt +— Ti that contains vt
10 bpT +— convert(pE,Tvt)
11 Replace all Ti in bpT with bTi
Algorithm 3: Convert to a Binary Tree
1 Function convert(E, vt):
2 v +— vt
3 for vi —* vt, vt —* vi in E do
4 if vt —* vi then
5 E&apos;+—E\{vt—vi}
6 w +— [convert(E, vi), v]
7 else
8 E&apos; +— E \ {vi —* vt}
9 w +— [v, convert(E, vi)]
10 end
11 v +— w
12 end
13 return v
14 end
</figure>
<figureCaption confidence="0.9904845">
Figure 3: Another Target Dependent Binary
Phrase Dependency Tree (Target Aspect ‘phone’)
</figureCaption>
<bodyText confidence="0.933072888888889">
represented as a 300-dimensional vector in this
pre-trained dataset.
PhraseRNN uses two kinds of composition
function G = {g1, · · · , gn} for inner-phrase and
H = {h1, · · · , hm} for outer-phrase. n and m are
the number of functions in G and H, respectively.
The vector of the parent node vin in the binary
subtree bTi, where vl and vr are the vectors of the
left and right children, is computed as:
</bodyText>
<equation confidence="0.912256">
�P(gi|vl, vr, ein)gi(vl, vr) (4)
</equation>
<bodyText confidence="0.996307333333333">
where ein is the external feature vector.
P(gi|vl, vr, ein) is the probability of function
gi given the child vectors vl, vr and ein. It is
</bodyText>
<table confidence="0.345686">
Algorithm 2: Convert to Target Dependent Bi-
nary Phrase Dependency Tree
</table>
<equation confidence="0.897026818181818">
n
vin = f
i=1
2511
⎡
⎣
defined as Equation (5).
⎤ ⎛ ⎡⎤ ⎞
P (g1|vl, vr, ein) vl
· · · ⎦ = softmax ⎝βR ⎣ vr ⎦ ⎠
P(gn|vl, vr, ein) ein
</equation>
<bodyText confidence="0.943551714285714">
(5)
where β ∈ &lt; is a hyper-parameter, and R ∈
&lt;nx(2d+|ein|) is the parameter matrix.
In the target dependent binary phrase depen-
dency tree bpT, the vector of the parent node vout,
where the vectors of the left and right children are
vl and vr, is computed as:
</bodyText>
<equation confidence="0.9759135">
!P(hi|vl, vr, eout)hi(vl, vr) (6)
P (hi|vl, vr, eout) is the probability of function hi
</equation>
<bodyText confidence="0.8533685">
given the child vectors vl, vr and external feature
vector eout as shown in Equation (7).
</bodyText>
<equation confidence="0.9538384">
⎤ ⎛ ⎡ ⎤ ⎞
vl
⎦ = softmax ⎝βS ⎣ vr ⎦ ⎠
eout
(7)
</equation>
<bodyText confidence="0.993059305555556">
where S ∈ &lt;mx(2d+|eout|) is the parameter matrix.
The external features ei (ein and eout) of the
node vi consists of three types of features: Labell,
Labelr and DepTypei. Labell and Labelr are
the labels of the left and right child nodes, respec-
tively. If node vl is a leaf word, Labell is the POS
of the word vl. Otherwise, it is the non-terminal
symbol of the lowest common parent of descen-
dants of vl in the constituent tree. For example,
the Label of the node combined from ‘the’ and
‘design’ in Figure 2(c) is ‘NP’ which is the low-
est common parent of these two words in the con-
stituent tree in Figure 1. DepTypei is the depen-
dency relation for node vi. If the left and right
children of vi are leaf nodes, itis the direct relation
in the dependency tree between them. Otherwise,
DepTypei is the relation between head words of
the left and right nodes. For instance, in Figure
2(c), let a be the parent of ‘is’ and ‘bad’, b is the
parent of ‘for’ and ‘me’, c is the parent of a and b.
DepType of a and b are ‘COP’ and ‘POBJ’ that
are direct relations between two child nodes in the
dependency tree in Figure 2(a). While, DepType
of c is ‘PREP’ that is the dependency relation be-
tween two head words ‘bad’ and ‘for’. ei is a bi-
nary vector where the weight of the vector repre-
sents the presence of each feature.
We suppose a batch training data consist-
ing of B instances {(x(1), t(1)), · · · , (x(B), t(B))},
where x(b) and t(b) are the aspect and its sentiment
category of b-th instance. Let y(b) be the predicted
sentiment category for aspect x(b) by PhraseRNN.
The goal is to minimize the loss function which is
the sum of the mean of negative log likelihood and
L2 regularization penalty in a batch training set as
in Equation (8).
</bodyText>
<equation confidence="0.9962635">
L= −1B PB log(P(y(b) = t(b)|x(b), θ)) + λ P k θi k2
b=1 θiEθ
</equation>
<bodyText confidence="0.9418765">
(8)
where λ is a constant controlling the degree of
penalty, θ is all the parameters in the model.
Stochastic gradient descent is used to optimize
the loss function. Backpropagation is employed to
propagate the errors from the top node to the leaf
nodes. The derivatives of parameters are used to
update the parameters.
</bodyText>
<sectionHeader confidence="0.999569" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.988489894736842">
We use the restaurant reviews dataset in Se-
mEval2014 Task 4 consisting of over 3000 English
sentences. For each aspect, “positive”, “negative”
or “neutral” is annotated as its polarity. Dataset is
divided into three parts: 70% training, 10% devel-
opment and 20% test.
We compare the following methods:
ASA w/o RE: It defines a sentiment score of a
given aspect as the weighted sum of opinion scores
of all words in the sentence, where the weight is
defined by the distance from the aspect (Liu and
Zhang, 2012; Pang and Lee, 2008).
ASA with RE: It improves “ASA w/o RE” by
firstly identifying the aspect-opinion relations us-
ing tree kernel, then integrating them to the senti-
ment calculation (Nguyen and Shirai, 2015a).
RNN: It uses only one global function g1 over
the binary dependency tree.
AdaRNN: It uses multi-composition functions
</bodyText>
<equation confidence="0.945067">
G = {g1, · · · , gn} over a binary dependency tree
(Dong et al., 2014a).
PhraseRNN-1: our PhraseRNN with only one
global function: G = H = g1
</equation>
<bodyText confidence="0.948687625">
PhraseRNN-2: our PhraseRNN with two
global functions. One for inner-phrase, the other
for outer-phrase: G = g1 and H = h1
PhraseRNN-3: our PhraseRNN with multiple
global functions: G = H = {g1, · · · , gn}
PhraseRNN-4: our PhraseRNN with two lists
of global functions. One for inner-phrase, the
other for outer-phrase: G = {g1, · · · , gn} and
</bodyText>
<equation confidence="0.9945965">
H = {h1, · · · , hm}
vout = f Xm
i=1
P(h1|vl, vr, eout)
· · ·
P(hm|vl, vr, eout)
⎡
⎣
</equation>
<page confidence="0.968548">
2512
</page>
<bodyText confidence="0.999744142857143">
Stanford CoreNLP (Manning et al., 2014) is
used to parse the sentence and obtain constituent
and dependency trees. For RNN, AdaRNN and
PhraseRNN, the optimal parameters, which mini-
mize the error in the development set, are used for
the sentiment classification of the test set. We set
Q = 1 for AdaRNN and PhraseRNN since it is re-
ported that Q = 1 is the best parameter (Dong et
al., 2014a). The optimized number of composition
functions nand m = 2 are selected by grid search
with n = 12,4,6,8, 101 on the development set.
A = 0.0001 is employed. Accuracy (A), Preci-
sion (P), Recall (R) and F-measure (F) are used as
evaluation metrics 3.
Table 1 shows the results of the methods. Dif-
ferences of PhraseRNN and RNN are verified by
statistical significance tests. We use the paired
randomization test because it does not require
additional assumption about distribution of out-
puts (Smucker et al., 2007). The results indi-
cate that four variations of our PhraseRNN out-
perform “ASA w/o RE”, “ASA with RE”, RNN
and AdaRNN methods from 5.35% to 19.44% ac-
curacy and 8% to 16.48% F-measure. Among
four variations, PhraseRNN-2 and PhraseRNN-
3 achieved the best performance. By using dif-
ferent global functions in the inner and outer
phrases, PhraseRNN-2 improves PhraseRNN-1 by
2.54% F-measure while keeping the comparable
accuracy. Using multi-composition functions is
also effective since PhraseRNN-3 was better than
PhraseRNN-1 by 1.55% accuracy. PhraseRNN-4
improved PhraseRNN-3 by 6.38% precision while
keeping comparable in other metrics.
Since our PhraseRNN-1 and PhraseRNN-3 out-
perform RNN and AdaRNN (the models rely-
ing on the binary dependency tree) respectively,
we can conclude that our target dependent binary
phrase dependency tree is much effective than bi-
nary dependency tree for ABSA.
In the data used in (Dong et al., 2014a), one sen-
tence contains only one aspect. On the other hand,
two or more aspects can be appeared in one sen-
tence in SemEval 2014 data. It is common in the
real text. To examine in which cases our method is
better than the others, we conduct an additional ex-
periment by dividing the test set into three disjoint
subsets. The first subset (S1) contains sentences
having only one aspect. The second subset (S2)
</bodyText>
<tableCaption confidence="0.575765333333333">
3Precision, Recall and F-measure are the average for three
polarity categories weighted by the number of true instances.
Table 1: Results of ABSA
</tableCaption>
<table confidence="0.958385076923077">
Methods A P R F
ASA w/o RE 46.76 54.63 46.76 48.06
ASA with RE 52.39 53.91 52.39 52.54
RNN 60.85 53.59 60.85 54.21
AdaRNN 60.42 36.78 60.42 45.73
PhraseRNN-1 64.651 58.591 64.651 59.67*
PhraseRNN-2 63.941 62.40* 63.941 62.21*
PhraseRNN-3 66.20* 53.88 66.20* 59.32*
PhraseRNN-4 65.92* 60.261 65.92* 59.80*
Notes: Statistical significance test of PhraseRNN compar-
ing to RNN.
* Significant at the 1 percent level.
† Significant at the 5 percent level.
</table>
<tableCaption confidence="0.9134815">
Table 2: The Number of Correctly Identified As-
pects in Subsets S1, S2 and S3
</tableCaption>
<table confidence="0.998778">
Methods S1 S2 S3
ASA w/o RE 98 (49.00) 156 (48.30) 78 (41.71)
ASA with RE 111 (55.50) 176 (54.49) 85 (45.45)
RNN 123 (61.50) 226 (69.97) 83 (44.39)
AdaRNN 117 (58.50) 234 (72.45) 78 (41.71)
PhraseRNN-1 129 (64.50) 248 (76.78) 82 (43.85)
PhraseRNN-2 125 (62.50) 247 (76.47) 82 (43.85)
PhraseRNN-3 125 (62.50) 257 (79.57) 88 (47.06)
PhraseRNN-4 128 (64.00) 250 (77.40) 90 (48.13)
</table>
<bodyText confidence="0.999936714285714">
and third subset (S3) have two or more aspects in
each sentence. All aspects in a sentence in S2 have
the same sentiment category, while different sen-
timent categories in S3. The number of aspects in
S1, S2 and S3 are 200, 323 and 187, respectively.
Table 2 shows the number of aspects where their
sentiments are correctly identified by the methods
in the subsets S1, S2 and S3. The accuracies are
also shown in parentheses. Among three subsets,
S3 is the most difficult and ambiguous case. In all
methods, the performance in S3 is worse than S1
and S2. Comparing with other methods in each
subset, PhraseRNN improves the accuracy in S2
more than in S1 and S3.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999950222222222">
We proposed PhraseRNN to identify the sentiment
of the aspect in the sentence. Propagating the
semantics through the binary dependency tree in
RNN and AdaRNN could not be enough to rep-
resent the sentiment of the aspect. A new hierar-
chical structure was constructed by integrating the
dependency relations and phrases. The results in-
dicated that our PhraseRNN outperformed “ASA
w/o RE”, “ASA with RE”, RNN and AdaRNN.
</bodyText>
<page confidence="0.967615">
2513
</page>
<sectionHeader confidence="0.996225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999300212765958">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search (JMLR), 3:1137–1155.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014a. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 49–54.
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014b.
Adaptive multi-compositionality for recursive neu-
ral models with applications to sentiment analysis.
In Twenty-Eighth AAAI Conference on Artificial In-
telligence (AAAI), pages 1537–1543.
Geoffrey E Hinton. 1986. Learning distributed repre-
sentations of concepts. In Proceedings of the eighth
annual conference of the cognitive science society,
volume 1, page 12. Amherst, MA.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In Proceedings of the fourth ACM international con-
ference on Web search and data mining, pages 815–
824. ACM.
Himabindu Lakkaraju, Chiranjib Bhattacharyya, Indra-
jit Bhattacharya, and Srujana Merugu. 2011. Ex-
ploiting coherence for the simultaneous discovery
of latent facets and associated sentiments. In Pro-
ceedings of the Eleventh SIAM International Confer-
ence on Data Mining (SDM), pages 498–509. SIAM
/ Omnipress.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM conference on Infor-
mation and knowledge management (CIKM), pages
375–384. ACM.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text Data,
pages 415–463. Springer.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations (ACL),
pages 55–60.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems (NIPS), pages 3111–3119.
Thien Hai Nguyen and Kiyoaki Shirai. 2015a. Aspect-
based sentiment analysis using tree kernel based re-
lation extraction. In Alexander Gelbukh, editor,
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing), volume 9042 of Lecture Notes in
Computer Science, pages 114–125. Springer Inter-
national Publishing.
Thien Hai Nguyen and Kiyoaki Shirai. 2015b. Topic
modeling based sentiment analysis on social media
for stock market prediction. In Proceedings of the
53rd Annual Meeting of the Association for Com-
putational Linguistics (ACL), Volume 1: Long Pa-
pers, pages 1354–1364. The Association for Com-
puter Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Mark D Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings
of the sixteenth ACM conference on Conference on
information and knowledge management (CIKM),
pages 623–632. ACM.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML), pages 129–136.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1201–1211. Association for Compu-
tational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP), volume 1631, page 1642. Citeseer.
</reference>
<page confidence="0.991617">
2514
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616090">
<title confidence="0.9998105">PhraseRNN: Phrase Recursive Neural for Aspect-based Sentiment Analysis</title>
<author confidence="0.999799">Thien Hai Nguyen Kiyoaki Shirai</author>
<affiliation confidence="0.987773">School of Information Japan Advanced Institute of Science and</affiliation>
<address confidence="0.659781">1-1 Asahidai, Nomi, Ishikawa 923-1292,</address>
<abstract confidence="0.99279">This paper presents a new method to identify sentiment of an aspect of an entity. It is an extension of RNN (Recursive Neural Network) that takes both dependency and constituent trees of a sentence into account. Results of an experiment show that our method significantly outperforms previous methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research (JMLR),</journal>
<pages>3--1137</pages>
<contexts>
<context position="1895" citStr="Bengio et al., 2003" startWordPosition="300" endWordPosition="303">s method is further improved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseRNN for ABSA. It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2. The basic idea is to make the representation of the target aspect richer by using syntactic i</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research (JMLR), 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Dong</author>
<author>Furu Wei</author>
<author>Chuanqi Tan</author>
<author>Duyu Tang</author>
<author>Ming Zhou</author>
<author>Ke Xu</author>
</authors>
<title>Adaptive recursive neural network for target-dependent twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>49--54</pages>
<contexts>
<context position="2239" citStr="Dong et al., 2014" startWordPosition="358" endWordPosition="361"> TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseRNN for ABSA. It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2. The basic idea is to make the representation of the target aspect richer by using syntactic information from both the dependency and constituent trees of the sentence. 2 Recursive Neural Networks for ABSA In RNN and AdaRNN, given a sentence containing a target aspect, “binary dependency tree” is built from a dependency tree of the sentence. Intuitively, it represents syntactic relations associated with the aspect. Each word (leaf) or</context>
<context position="12322" citStr="Dong et al., 2014" startWordPosition="2264" endWordPosition="2267">ng methods: ASA w/o RE: It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence, where the weight is defined by the distance from the aspect (Liu and Zhang, 2012; Pang and Lee, 2008). ASA with RE: It improves “ASA w/o RE” by firstly identifying the aspect-opinion relations using tree kernel, then integrating them to the sentiment calculation (Nguyen and Shirai, 2015a). RNN: It uses only one global function g1 over the binary dependency tree. AdaRNN: It uses multi-composition functions G = {g1, · · · , gn} over a binary dependency tree (Dong et al., 2014a). PhraseRNN-1: our PhraseRNN with only one global function: G = H = g1 PhraseRNN-2: our PhraseRNN with two global functions. One for inner-phrase, the other for outer-phrase: G = g1 and H = h1 PhraseRNN-3: our PhraseRNN with multiple global functions: G = H = {g1, · · · , gn} PhraseRNN-4: our PhraseRNN with two lists of global functions. One for inner-phrase, the other for outer-phrase: G = {g1, · · · , gn} and H = {h1, · · · , hm} vout = f Xm i=1 P(h1|vl, vr, eout) · · · P(hm|vl, vr, eout) ⎡ ⎣ 2512 Stanford CoreNLP (Manning et al., 2014) is used to parse the sentence and obtain constituent </context>
<context position="14660" citStr="Dong et al., 2014" startWordPosition="2667" endWordPosition="2670">and outer phrases, PhraseRNN-2 improves PhraseRNN-1 by 2.54% F-measure while keeping the comparable accuracy. Using multi-composition functions is also effective since PhraseRNN-3 was better than PhraseRNN-1 by 1.55% accuracy. PhraseRNN-4 improved PhraseRNN-3 by 6.38% precision while keeping comparable in other metrics. Since our PhraseRNN-1 and PhraseRNN-3 outperform RNN and AdaRNN (the models relying on the binary dependency tree) respectively, we can conclude that our target dependent binary phrase dependency tree is much effective than binary dependency tree for ABSA. In the data used in (Dong et al., 2014a), one sentence contains only one aspect. On the other hand, two or more aspects can be appeared in one sentence in SemEval 2014 data. It is common in the real text. To examine in which cases our method is better than the others, we conduct an additional experiment by dividing the test set into three disjoint subsets. The first subset (S1) contains sentences having only one aspect. The second subset (S2) 3Precision, Recall and F-measure are the average for three polarity categories weighted by the number of true instances. Table 1: Results of ABSA Methods A P R F ASA w/o RE 46.76 54.63 46.76 </context>
</contexts>
<marker>Dong, Wei, Tan, Tang, Zhou, Xu, 2014</marker>
<rawString>Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014a. Adaptive recursive neural network for target-dependent twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 49–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Dong</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
<author>Ke Xu</author>
</authors>
<title>Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis.</title>
<date>2014</date>
<booktitle>In Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1537--1543</pages>
<contexts>
<context position="2239" citStr="Dong et al., 2014" startWordPosition="358" endWordPosition="361"> TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseRNN for ABSA. It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2. The basic idea is to make the representation of the target aspect richer by using syntactic information from both the dependency and constituent trees of the sentence. 2 Recursive Neural Networks for ABSA In RNN and AdaRNN, given a sentence containing a target aspect, “binary dependency tree” is built from a dependency tree of the sentence. Intuitively, it represents syntactic relations associated with the aspect. Each word (leaf) or</context>
<context position="12322" citStr="Dong et al., 2014" startWordPosition="2264" endWordPosition="2267">ng methods: ASA w/o RE: It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence, where the weight is defined by the distance from the aspect (Liu and Zhang, 2012; Pang and Lee, 2008). ASA with RE: It improves “ASA w/o RE” by firstly identifying the aspect-opinion relations using tree kernel, then integrating them to the sentiment calculation (Nguyen and Shirai, 2015a). RNN: It uses only one global function g1 over the binary dependency tree. AdaRNN: It uses multi-composition functions G = {g1, · · · , gn} over a binary dependency tree (Dong et al., 2014a). PhraseRNN-1: our PhraseRNN with only one global function: G = H = g1 PhraseRNN-2: our PhraseRNN with two global functions. One for inner-phrase, the other for outer-phrase: G = g1 and H = h1 PhraseRNN-3: our PhraseRNN with multiple global functions: G = H = {g1, · · · , gn} PhraseRNN-4: our PhraseRNN with two lists of global functions. One for inner-phrase, the other for outer-phrase: G = {g1, · · · , gn} and H = {h1, · · · , hm} vout = f Xm i=1 P(h1|vl, vr, eout) · · · P(hm|vl, vr, eout) ⎡ ⎣ 2512 Stanford CoreNLP (Manning et al., 2014) is used to parse the sentence and obtain constituent </context>
<context position="14660" citStr="Dong et al., 2014" startWordPosition="2667" endWordPosition="2670">and outer phrases, PhraseRNN-2 improves PhraseRNN-1 by 2.54% F-measure while keeping the comparable accuracy. Using multi-composition functions is also effective since PhraseRNN-3 was better than PhraseRNN-1 by 1.55% accuracy. PhraseRNN-4 improved PhraseRNN-3 by 6.38% precision while keeping comparable in other metrics. Since our PhraseRNN-1 and PhraseRNN-3 outperform RNN and AdaRNN (the models relying on the binary dependency tree) respectively, we can conclude that our target dependent binary phrase dependency tree is much effective than binary dependency tree for ABSA. In the data used in (Dong et al., 2014a), one sentence contains only one aspect. On the other hand, two or more aspects can be appeared in one sentence in SemEval 2014 data. It is common in the real text. To examine in which cases our method is better than the others, we conduct an additional experiment by dividing the test set into three disjoint subsets. The first subset (S1) contains sentences having only one aspect. The second subset (S2) 3Precision, Recall and F-measure are the average for three polarity categories weighted by the number of true instances. Table 1: Results of ABSA Methods A P R F ASA w/o RE 46.76 54.63 46.76 </context>
</contexts>
<marker>Dong, Wei, Zhou, Xu, 2014</marker>
<rawString>Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014b. Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis. In Twenty-Eighth AAAI Conference on Artificial Intelligence (AAAI), pages 1537–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Learning distributed representations of concepts.</title>
<date>1986</date>
<booktitle>In Proceedings of the eighth annual conference of the cognitive science society,</booktitle>
<volume>1</volume>
<pages>12</pages>
<location>Amherst, MA.</location>
<contexts>
<context position="1910" citStr="Hinton, 1986" startWordPosition="304" endWordPosition="305">mproved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseRNN for ABSA. It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2. The basic idea is to make the representation of the target aspect richer by using syntactic information from</context>
</contexts>
<marker>Hinton, 1986</marker>
<rawString>Geoffrey E Hinton. 1986. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, volume 1, page 12. Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohan Jo</author>
<author>Alice H Oh</author>
</authors>
<title>Aspect and sentiment unification model for online review analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining,</booktitle>
<pages>815--824</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1679" citStr="Jo and Oh, 2011" startWordPosition="264" endWordPosition="267">oach is to calculate a sentiment score of a given aspect as the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence (Liu and Zhang, 2012; Pang and Lee, 2008). This method is further improved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper propo</context>
</contexts>
<marker>Jo, Oh, 2011</marker>
<rawString>Yohan Jo and Alice H Oh. 2011. Aspect and sentiment unification model for online review analysis. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 815– 824. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Himabindu Lakkaraju</author>
<author>Chiranjib Bhattacharyya</author>
<author>Indrajit Bhattacharya</author>
<author>Srujana Merugu</author>
</authors>
<title>Exploiting coherence for the simultaneous discovery of latent facets and associated sentiments.</title>
<date>2011</date>
<journal>SIAM / Omnipress.</journal>
<booktitle>In Proceedings of the Eleventh SIAM International Conference on Data Mining (SDM),</booktitle>
<pages>498--509</pages>
<contexts>
<context position="1744" citStr="Lakkaraju et al., 2011" startWordPosition="276" endWordPosition="279">s the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence (Liu and Zhang, 2012; Pang and Lee, 2008). This method is further improved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseRNN for ABSA. It is an extended model of R</context>
</contexts>
<marker>Lakkaraju, Bhattacharyya, Bhattacharya, Merugu, 2011</marker>
<rawString>Himabindu Lakkaraju, Chiranjib Bhattacharyya, Indrajit Bhattacharya, and Srujana Merugu. 2011. Exploiting coherence for the simultaneous discovery of latent facets and associated sentiments. In Proceedings of the Eleventh SIAM International Conference on Data Mining (SDM), pages 498–509. SIAM / Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management (CIKM),</booktitle>
<pages>375--384</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1703" citStr="Lin and He, 2009" startWordPosition="269" endWordPosition="272">sentiment score of a given aspect as the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence (Liu and Zhang, 2012; Pang and Lee, 2008). This method is further improved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseR</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of the 18th ACM conference on Information and knowledge management (CIKM), pages 375–384. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Lei Zhang</author>
</authors>
<title>A survey of opinion mining and sentiment analysis.</title>
<date>2012</date>
<booktitle>In Mining Text Data,</booktitle>
<pages>415--463</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1250" citStr="Liu and Zhang, 2012" startWordPosition="198" endWordPosition="201">) has been found to play a significant role in many applications such as opinion mining on product or restaurant reviews. It is a task to determine an attitude, opinion and emotions of people toward aspects in a sentence. For example, given a sentence “Except the design, the phone is bad for me”, the system should classify positive and negative as the sentiments for the aspects ‘design’ and ‘phone’, respectively. The simple approach is to calculate a sentiment score of a given aspect as the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence (Liu and Zhang, 2012; Pang and Lee, 2008). This method is further improved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of wo</context>
<context position="11924" citStr="Liu and Zhang, 2012" startWordPosition="2195" endWordPosition="2198"> the top node to the leaf nodes. The derivatives of parameters are used to update the parameters. 4 Evaluation We use the restaurant reviews dataset in SemEval2014 Task 4 consisting of over 3000 English sentences. For each aspect, “positive”, “negative” or “neutral” is annotated as its polarity. Dataset is divided into three parts: 70% training, 10% development and 20% test. We compare the following methods: ASA w/o RE: It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence, where the weight is defined by the distance from the aspect (Liu and Zhang, 2012; Pang and Lee, 2008). ASA with RE: It improves “ASA w/o RE” by firstly identifying the aspect-opinion relations using tree kernel, then integrating them to the sentiment calculation (Nguyen and Shirai, 2015a). RNN: It uses only one global function g1 over the binary dependency tree. AdaRNN: It uses multi-composition functions G = {g1, · · · , gn} over a binary dependency tree (Dong et al., 2014a). PhraseRNN-1: our PhraseRNN with only one global function: G = H = g1 PhraseRNN-2: our PhraseRNN with two global functions. One for inner-phrase, the other for outer-phrase: G = g1 and H = h1 PhraseR</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis. In Mining Text Data, pages 415–463. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL),</booktitle>
<pages>55--60</pages>
<contexts>
<context position="12868" citStr="Manning et al., 2014" startWordPosition="2371" endWordPosition="2374">tions G = {g1, · · · , gn} over a binary dependency tree (Dong et al., 2014a). PhraseRNN-1: our PhraseRNN with only one global function: G = H = g1 PhraseRNN-2: our PhraseRNN with two global functions. One for inner-phrase, the other for outer-phrase: G = g1 and H = h1 PhraseRNN-3: our PhraseRNN with multiple global functions: G = H = {g1, · · · , gn} PhraseRNN-4: our PhraseRNN with two lists of global functions. One for inner-phrase, the other for outer-phrase: G = {g1, · · · , gn} and H = {h1, · · · , hm} vout = f Xm i=1 P(h1|vl, vr, eout) · · · P(hm|vl, vr, eout) ⎡ ⎣ 2512 Stanford CoreNLP (Manning et al., 2014) is used to parse the sentence and obtain constituent and dependency trees. For RNN, AdaRNN and PhraseRNN, the optimal parameters, which minimize the error in the development set, are used for the sentiment classification of the test set. We set Q = 1 for AdaRNN and PhraseRNN since it is reported that Q = 1 is the best parameter (Dong et al., 2014a). The optimized number of composition functions nand m = 2 are selected by grid search with n = 12,4,6,8, 101 on the development set. A = 0.0001 is employed. Accuracy (A), Precision (P), Recall (R) and F-measure (F) are used as evaluation metrics 3.</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL), pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="6963" citStr="Mikolov et al., 2013" startWordPosition="1242" endWordPosition="1245"> tree bpT are binary subtrees bT1, · · · , bTK which are the binary versions of subtrees T1, · · · , TK. On the other hand, the leaves of binary subtree bTi are the words in phrase pi. bpT and bTi are obtained by convert function defined as Algorithm 3. It can convert an arbitrary tree to a binary tree 1. Figure 2(c) and Figure 3 show the outputs for the aspect ‘design’ and ’phone’, respectively. 3.2 Constructing the Aspect Representation Each node in the binary tree is represented as a ddimensional vector. In this research, we use the pre-trained Google News dataset 2 by word2vec algorithms (Mikolov et al., 2013). Each word is 1Note that convert function returns a tree represented by nested brackets such as [PP,[NP,VP]]. 2https://code.google.com/p/word2vec/ P(g1|vl, vr, e) · · · P(gn|vl, vr, e) ⎡ ⎣ 5 6 7 8 9 10 2510 Figure 2: Hierarchical Structures in PhraseRNN: (a) Dependency Tree, (b) Phrase Dependency Tree and (c) Target Dependent Binary Phrase Dependency Tree Input: phrase dependency tree: pT = (pV, pE), target vt Output: target dependent binary phrase dependency tree: bpT 1 for Ti = (Vi, Ei) E pV do 2 if vt E Vi then 3 h + —vt 4 else 5 h +— vertex having no head in Ei 6 end 7 bTi +— convert(Ei, </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS), pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thien Hai Nguyen</author>
<author>Kiyoaki Shirai</author>
</authors>
<title>Aspectbased sentiment analysis using tree kernel based relation extraction.</title>
<date>2015</date>
<booktitle>Computational Linguistics and Intelligent Text Processing (CICLing),</booktitle>
<volume>9042</volume>
<pages>114--125</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer International Publishing.</publisher>
<contexts>
<context position="1398" citStr="Nguyen and Shirai, 2015" startWordPosition="220" endWordPosition="223">ine an attitude, opinion and emotions of people toward aspects in a sentence. For example, given a sentence “Except the design, the phone is bad for me”, the system should classify positive and negative as the sentiments for the aspects ‘design’ and ‘phone’, respectively. The simple approach is to calculate a sentiment score of a given aspect as the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence (Liu and Zhang, 2012; Pang and Lee, 2008). This method is further improved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the be</context>
<context position="12131" citStr="Nguyen and Shirai, 2015" startWordPosition="2229" endWordPosition="2232">sentences. For each aspect, “positive”, “negative” or “neutral” is annotated as its polarity. Dataset is divided into three parts: 70% training, 10% development and 20% test. We compare the following methods: ASA w/o RE: It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence, where the weight is defined by the distance from the aspect (Liu and Zhang, 2012; Pang and Lee, 2008). ASA with RE: It improves “ASA w/o RE” by firstly identifying the aspect-opinion relations using tree kernel, then integrating them to the sentiment calculation (Nguyen and Shirai, 2015a). RNN: It uses only one global function g1 over the binary dependency tree. AdaRNN: It uses multi-composition functions G = {g1, · · · , gn} over a binary dependency tree (Dong et al., 2014a). PhraseRNN-1: our PhraseRNN with only one global function: G = H = g1 PhraseRNN-2: our PhraseRNN with two global functions. One for inner-phrase, the other for outer-phrase: G = g1 and H = h1 PhraseRNN-3: our PhraseRNN with multiple global functions: G = H = {g1, · · · , gn} PhraseRNN-4: our PhraseRNN with two lists of global functions. One for inner-phrase, the other for outer-phrase: G = {g1, · · · , </context>
</contexts>
<marker>Nguyen, Shirai, 2015</marker>
<rawString>Thien Hai Nguyen and Kiyoaki Shirai. 2015a. Aspectbased sentiment analysis using tree kernel based relation extraction. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing (CICLing), volume 9042 of Lecture Notes in Computer Science, pages 114–125. Springer International Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thien Hai Nguyen</author>
<author>Kiyoaki Shirai</author>
</authors>
<title>Topic modeling based sentiment analysis on social media for stock market prediction.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), Volume 1: Long Papers,</booktitle>
<pages>1354--1364</pages>
<publisher>The Association</publisher>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="1398" citStr="Nguyen and Shirai, 2015" startWordPosition="220" endWordPosition="223">ine an attitude, opinion and emotions of people toward aspects in a sentence. For example, given a sentence “Except the design, the phone is bad for me”, the system should classify positive and negative as the sentiments for the aspects ‘design’ and ‘phone’, respectively. The simple approach is to calculate a sentiment score of a given aspect as the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence (Liu and Zhang, 2012; Pang and Lee, 2008). This method is further improved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the be</context>
<context position="12131" citStr="Nguyen and Shirai, 2015" startWordPosition="2229" endWordPosition="2232">sentences. For each aspect, “positive”, “negative” or “neutral” is annotated as its polarity. Dataset is divided into three parts: 70% training, 10% development and 20% test. We compare the following methods: ASA w/o RE: It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence, where the weight is defined by the distance from the aspect (Liu and Zhang, 2012; Pang and Lee, 2008). ASA with RE: It improves “ASA w/o RE” by firstly identifying the aspect-opinion relations using tree kernel, then integrating them to the sentiment calculation (Nguyen and Shirai, 2015a). RNN: It uses only one global function g1 over the binary dependency tree. AdaRNN: It uses multi-composition functions G = {g1, · · · , gn} over a binary dependency tree (Dong et al., 2014a). PhraseRNN-1: our PhraseRNN with only one global function: G = H = g1 PhraseRNN-2: our PhraseRNN with two global functions. One for inner-phrase, the other for outer-phrase: G = g1 and H = h1 PhraseRNN-3: our PhraseRNN with multiple global functions: G = H = {g1, · · · , gn} PhraseRNN-4: our PhraseRNN with two lists of global functions. One for inner-phrase, the other for outer-phrase: G = {g1, · · · , </context>
</contexts>
<marker>Nguyen, Shirai, 2015</marker>
<rawString>Thien Hai Nguyen and Kiyoaki Shirai. 2015b. Topic modeling based sentiment analysis on social media for stock market prediction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), Volume 1: Long Papers, pages 1354–1364. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="1271" citStr="Pang and Lee, 2008" startWordPosition="202" endWordPosition="205">lay a significant role in many applications such as opinion mining on product or restaurant reviews. It is a task to determine an attitude, opinion and emotions of people toward aspects in a sentence. For example, given a sentence “Except the design, the phone is bad for me”, the system should classify positive and negative as the sentiments for the aspects ‘design’ and ‘phone’, respectively. The simple approach is to calculate a sentiment score of a given aspect as the weighted sum of opinion scores, which are defined by a sentiment lexicon, of all words in the sentence (Liu and Zhang, 2012; Pang and Lee, 2008). This method is further improved by identifying the aspect-opinion relations using tree kernel method (Nguyen and Shirai, 2015a). Other researches have attempted to use unsupervised topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embeddi</context>
<context position="11945" citStr="Pang and Lee, 2008" startWordPosition="2199" endWordPosition="2202">leaf nodes. The derivatives of parameters are used to update the parameters. 4 Evaluation We use the restaurant reviews dataset in SemEval2014 Task 4 consisting of over 3000 English sentences. For each aspect, “positive”, “negative” or “neutral” is annotated as its polarity. Dataset is divided into three parts: 70% training, 10% development and 20% test. We compare the following methods: ASA w/o RE: It defines a sentiment score of a given aspect as the weighted sum of opinion scores of all words in the sentence, where the weight is defined by the distance from the aspect (Liu and Zhang, 2012; Pang and Lee, 2008). ASA with RE: It improves “ASA w/o RE” by firstly identifying the aspect-opinion relations using tree kernel, then integrating them to the sentiment calculation (Nguyen and Shirai, 2015a). RNN: It uses only one global function g1 over the binary dependency tree. AdaRNN: It uses multi-composition functions G = {g1, · · · , gn} over a binary dependency tree (Dong et al., 2014a). PhraseRNN-1: our PhraseRNN with only one global function: G = H = g1 PhraseRNN-2: our PhraseRNN with two global functions. One for inner-phrase, the other for outer-phrase: G = g1 and H = h1 PhraseRNN-3: our PhraseRNN w</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Smucker</author>
<author>James Allan</author>
<author>Ben Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management (CIKM),</booktitle>
<pages>623--632</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13731" citStr="Smucker et al., 2007" startWordPosition="2523" endWordPosition="2526">set Q = 1 for AdaRNN and PhraseRNN since it is reported that Q = 1 is the best parameter (Dong et al., 2014a). The optimized number of composition functions nand m = 2 are selected by grid search with n = 12,4,6,8, 101 on the development set. A = 0.0001 is employed. Accuracy (A), Precision (P), Recall (R) and F-measure (F) are used as evaluation metrics 3. Table 1 shows the results of the methods. Differences of PhraseRNN and RNN are verified by statistical significance tests. We use the paired randomization test because it does not require additional assumption about distribution of outputs (Smucker et al., 2007). The results indicate that four variations of our PhraseRNN outperform “ASA w/o RE”, “ASA with RE”, RNN and AdaRNN methods from 5.35% to 19.44% accuracy and 8% to 16.48% F-measure. Among four variations, PhraseRNN-2 and PhraseRNN3 achieved the best performance. By using different global functions in the inner and outer phrases, PhraseRNN-2 improves PhraseRNN-1 by 2.54% F-measure while keeping the comparable accuracy. Using multi-composition functions is also effective since PhraseRNN-3 was better than PhraseRNN-1 by 1.55% accuracy. PhraseRNN-4 improved PhraseRNN-3 by 6.38% precision while kee</context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>Mark D Smucker, James Allan, and Ben Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management (CIKM), pages 623–632. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th international conference on machine learning (ICML),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="2073" citStr="Socher et al., 2011" startWordPosition="331" endWordPosition="334">topic modeling methods. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseRNN for ABSA. It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2. The basic idea is to make the representation of the target aspect richer by using syntactic information from both the dependency and constituent trees of the sentence. 2 Recursive Neural Networks for ABSA In RNN and AdaRNN, given a sentence containing a target aspect, “b</context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2094" citStr="Socher et al., 2012" startWordPosition="335" endWordPosition="338">s. To identify the sentiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseRNN for ABSA. It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2. The basic idea is to make the representation of the target aspect richer by using syntactic information from both the dependency and constituent trees of the sentence. 2 Recursive Neural Networks for ABSA In RNN and AdaRNN, given a sentence containing a target aspect, “binary dependency tree</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing (EMNLP),</booktitle>
<volume>1631</volume>
<pages>1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2116" citStr="Socher et al., 2013" startWordPosition="339" endWordPosition="342">ntiment category of the aspect, topic models which can simultaneously exploit aspect and sentiment have been proposed, such as TSLDA (Nguyen and Shirai, 2015b), ASUM (Jo and Oh, 2011), JST (Lin and He, 2009) and FACTS model (Lakkaraju et al., 2011). Recursive Neural Network (RNN) is a kind of deep neural network. Using distributed representations of words (aka word embedding) (Bengio et al., 2003; Hinton, 1986), RNN merges word representations to represent phrases or sentences. It is one of the best methods to predict sentiment labels for the phrases (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013). AdaRNN (Adaptive Recursive Neural Network) is an extension of RNN for Twitter sentiment classification (Dong et al., 2014a; Dong et al., 2014b). This paper proposes a new method PhraseRNN for ABSA. It is an extended model of RNN and AdaRNN, which are briefly introduced in Section 2. The basic idea is to make the representation of the target aspect richer by using syntactic information from both the dependency and constituent trees of the sentence. 2 Recursive Neural Networks for ABSA In RNN and AdaRNN, given a sentence containing a target aspect, “binary dependency tree” is built from a depe</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642. Citeseer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
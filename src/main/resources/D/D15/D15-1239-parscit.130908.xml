<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022433">
<title confidence="0.951524">
Talking to the crowd: What do people react to in online discussions?
</title>
<author confidence="0.9989">
Aaron Jaech, Vicky Zayats, Hao Fang, Mari Ostendorf and Hannaneh Hajishirzi
</author>
<affiliation confidence="0.998305">
Dept. of Electrical Engineering
University of Washington
</affiliation>
<email confidence="0.997593">
{ajaech,vzayats,hfang,ostendor,hannaneh}@uw.edu
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999667846153846">
This paper addresses the question of how
language use affects community reaction
to comments in online discussion forums,
and the relative importance of the mes-
sage vs. the messenger. A new comment
ranking task is proposed based on com-
munity annotated karma in Reddit discus-
sions, which controls for topic and tim-
ing of comments. Experimental work
with discussion threads from six subred-
dits shows that the importance of different
types of language features varies with the
community of interest.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947069767442">
Online discussion forums are a popular platform
for people to share their views about current events
and learn about issues of concern to them. Discus-
sion forums tend to specialize on different topics,
and people participating in them form communi-
ties of interest. The reaction of people within a
community to comments posted provides an indi-
cation of community endorsement of opinions and
value of information. In most discussions, the vast
majority of comments spawn little reaction. In this
paper, we look at whether (and how) language use
affects the reaction, compared to the relative im-
portance of the author and timing of the post.
Early work on factors that appear to influence
crowd-based judgments of comments in the Slash-
dot forum (Lampe and Resnick, 2004) indicate
that timing, starting score, length of the comment,
and poster anonymity/reputation appear to play
a role (where anonymity has a negative effect).
Judging by differences in popularity of various
discussion forums, topic is clearly important. Ev-
idence that language use also matters is provided
by recent work (Danescu-Niculescu-Mizil et al.,
2012; Lakkaraju et al., 2013; Althoff et al., 2014;
Tan et al., 2014). Teasing these different factors
apart, however, is a challenge. The work presented
in this paper provides additional insight into this
question by controlling for these factors in a dif-
ferent way than previous work and by examining
multiple communities of interest. Specifically, us-
ing data from Reddit discussion forums, we look at
the role of author reputation as measured in terms
of a karma k-index, and control for topic and tim-
ing by ranking comments in a constrained window
within a discussion.
The primary contributions of this work include
findings about the role of author reputation and
variation across communities in terms of aspects
of language use that matter, as well as the problem
formulation, associated data collection, and devel-
opment of a variety of features for characterizing
informativeness, community response, relevance
and mood.
</bodyText>
<sectionHeader confidence="0.997898" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999945055555556">
Reddit1 is the largest public online discussion fo-
rum with a wide variety of subreddits, which
makes it a good data source for studying how tex-
tual content in a discussion impacts the response
of the crowd. On Reddit, people initiate a dis-
cussion thread with a post (a question, a link to
a news item, etc.), and others respond with com-
ments. Registered users vote on which posts and
comments are important. The total amount of up
votes minus the down votes (roughly) is called
karma; it provides an indication of community en-
dorsement and popularity of a comment, as used
in (Lakkaraju et al., 2013). Karma is valued as it
impacts the order in which the posts or comments
are displayed, with the high karma content rising
to the top. Karma points are also accumulated by
members of the discussion forum as a function of
the karma associated with their comments.
</bodyText>
<footnote confidence="0.998227">
1http://www.reddit.com
</footnote>
<page confidence="0.846271">
2026
</page>
<note confidence="0.8695055">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2026–2031,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<table confidence="0.969222571428571">
subreddit # Posts # Comments/Post
FITNESS 3K 16.3
ASKSCIENCE 4K 8.8
POLITICS 7K 23.7
ASKWOMEN 4K 50.5
ASKMEN 4K 58.3
WORLDNEWS 12K 26.1
</table>
<tableCaption confidence="0.999222">
Table 1: Data collection statistics.
</tableCaption>
<bodyText confidence="0.9999325625">
The Reddit data is highly skewed. Although
there are thousands of active communities, only
a handful of them are large. Similarly, out of
the more than a million comments made per day2,
most of them receive little to no attention; the dis-
tributions of positive comment karma and author
karma are Zipfian. Slightly more than half of all
comments have exactly one karma point (no votes
beyond the author), and only 5% of comments
have less than one karma point.
For this study, we downloaded all the posts and
associated comments made to six subreddits over
a few weeks, as summarized in Table 1, as well as
karma of participants in the discussion3. All avail-
able comments on each post were downloaded at
least 48 hours after the post was made.4
</bodyText>
<sectionHeader confidence="0.976042" genericHeader="method">
3 Uptake Factors
</sectionHeader>
<bodyText confidence="0.999919263157895">
Factors other than the language use that influence
whether a comment will have uptake from the
community include the topic, the timing of the
message, and the messenger. These factors are all
evident in the Reddit discussions. Some subred-
dits are more popular and thus have higher karma
comments than others, reflecting the influence of
topic. Comments that are posted early in the dis-
cussion are more likely to have high karma, since
they have more potential responses.
Previous studies on Twitter show that the rep-
utation of the author substantially increases the
chances of the retweet (Suh et al., 2010; Cha et
al., 2010), and reputation is also raised as a fac-
tor in Slashdot (Lampe and Resnick, 2004). On
Reddit most users are anonymous, but it is possi-
ble that members of a forum become familiar with
particular usernames associated with high karma
comments. In order to see how important per-
</bodyText>
<footnote confidence="0.964013666666667">
2http://www.redditblog.com/2014/12/reddit-in-2014.html
3Our data collection is available online at
https://ssli.ee.washington.edu/tial/data/reddit
4Based on our initial look at the data, we noticed that most
posts receive all of their comments within 48 hours. Some
comments are deleted before we are able to download them.
</footnote>
<tableCaption confidence="0.7870455">
Table 2: Percentage of discussions where the top
comment is made by the top k-index person (or top
</tableCaption>
<equation confidence="0.510792">
3 people) in the discussion.
</equation>
<bodyText confidence="0.999815185185185">
sonal reputation is, we looked at how often the
top karma comments are associated with the top
karma participants in the discussion. Since an in-
dividual’s karma can be skewed by a few very pop-
ular posts, we measure reputation instead using a
measure we call the k-index, defined to be equal
to the number of comments in each user’s history
that have karma ≥ k. The k-index is analgous to
the h-index (Hirsch, 2005) and arguably a better
indicator of extended impact than total karma.
The results in Table 2 address the question of
whether the top karma comments always come
from the top karma person. The Top1 column
shows the percentage of threads where the top
karma comment in a discussion happens to be
made by the highest k-index person participating
in the discussion; the next column shows the per-
centage of threads where the comment comes from
any one of the top 3 k-index people. We find
that, in fact, the highest karma comment in a dis-
cussion is rarely from the highest k-index people.
The highest percentage is in ASKSCIENCE, where
expertise is more highly valued. If we consider
whether any one of the multiple comments that the
top k-index person made is the top karma com-
ment in the discussion, then the frequency is even
lower.
</bodyText>
<sectionHeader confidence="0.996862" genericHeader="method">
4 Methods
</sectionHeader>
<subsectionHeader confidence="0.992714">
4.1 Tasks
</subsectionHeader>
<bodyText confidence="0.9998407">
Having shown that the reputation of the author
of a post is not a dominating factor in predicting
high karma comments, we propose to control for
topic and timing by ranking a set of 10 comments
that were made consecutively in a short window
of time within one discussion thread according to
the karma they finally received. The ranking has
access to the comment history about these posts.
This simulates the view of an early reader of these
posts, i.e., without influence of the ratings of oth-
</bodyText>
<table confidence="0.884312285714286">
Top1 Top3
ASKSCIENCE 9.3 25.9
FITNESS 1.4 12.3
POLITICS 0.3 7.4
ASKWOMEN 2.2 13.5
ASKMEN 3.9 11.9
WORLDNEWS 3.1 6.4
</table>
<page confidence="0.99175">
2027
</page>
<bodyText confidence="0.999939607142857">
ers, so that the language content of the post is more
likely to have an impact. Very long threads are
sampled, so that these do not dominate the set of
lists. Approximately 75% of the comment lists are
designated for training and the rest is for testing,
with splits at the discussion thread level. Here,
feature selection is based on mean precision of
the top-ranked comment (P@1), so as to empha-
size learning the rare high karma events. (Note
that P@1 is equivalent to accuracy but allows for
any top-ranking comment to count as correct in the
case of ties.) The system performance is evaluated
using both P@1 and normalized discounted cumu-
lative gain (NDCG) (Burges et al., 2005), which is
a standard criterion for ranking evaluation when
the samples to be ranked have meaningful differ-
ences in scores, as is the case for karma of the
comments.
In addition, for analysis purposes, we report re-
sults for three surrogate tasks that can be used in
the ranking problem: i) the binary ranker trained
on all comment pairs within each list, in which low
karma comments dominate, ii) a positive vs. neg-
ative karma classifier, and iii) a high vs. medium
karma classifier. All use class-balanced data; the
second two are trained and tested on a biased sam-
pling of the data, where the pairs need not be from
the same discussion thread.
</bodyText>
<subsectionHeader confidence="0.840322">
4.2 Classifier
</subsectionHeader>
<bodyText confidence="0.999989684210527">
We use the support vector machine (SVM) rank
algorithm (Joachims, 2002) to predict a rank or-
der for each list of comments. The SVM is trained
to predict which of a pair of comments has higher
karma. The error term penalty parameter is tuned
to maximize P@1 on a held-out validation set
(20% of the training samples).
Since much of the data includes low-karma
comments, there will be a tendancy for the learn-
ing to emphasize features that discriminate com-
ments at the lower end of the scale. In order to
learn features that improve P@1, and to under-
stand the relative importance of different features,
we use a greedy automatic feature selection pro-
cess that incrementally adds one feature whose re-
sulting feature set achives the highest P@1 on the
validation set. Once all features have been used,
we select the model with the subset of features that
obtains the best P@1 on the validation set.
</bodyText>
<subsectionHeader confidence="0.949097">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.9998348">
The features are designed to capture several key at-
tributes that we hypothesize are predictive of com-
ment karma motivated by related work. The fea-
tures are categorized in groups as summarized be-
low, with details in supplementary material.
</bodyText>
<listItem confidence="0.984801205882353">
• Graph and Timing (G&amp;T): A baseline that
captures discourse history (response structure)
and comment timing, but no text content.
• Authority and Reputation (A&amp;R): K-index,
whether the commenter was the original poster,
and in some subreddits “flair” (display next to a
comment author’s username that is subject to a
cursory verification by moderators).
• Informativeness (Info.): Different indicators
suggestive of informative content and novelty,
including various word counts, named entity
counts, urls, and unseen n-grams.
• Lexical Unigrams (Lex.): Miscellaneous word
class indicators, puncutation, and part-of-
speech counts
• Predicted Community Response (Resp.):
Probability scores from surrogate classification
tasks (reply vs. no reply, positive vs. negative
sentiment) to measure the community response
of a comment using bag-of-words predictors.
• Relevance (Rel.): Comment similarity to the
parent, post and title in terms of topic, computed
with three methods: i) a distributed vector rep-
resentation of topic using a non-negative matrix
factorization (NMF) model (Xu et al., 2003),
ii) the average of skip-gram word embeddings
(Mikolov et al., 2013), and iii) word set Jaccard
similarity (Strehl et al., 2000).
• Mood: Mean and std. deviation of sentence sen-
timent in the comment; word list indicators for
politeness, argumentativeness and profanity.
• Community Style (Comm.): Posterior proba-
bility of each subreddit given the comment us-
ing a bag-of-words model.
</listItem>
<bodyText confidence="0.999965909090909">
The various word lists are motivated by fea-
ture exploration studies in surrogate tasks. For
example, projecting words to a two dimensional
space of positive vs. negative and likelihood of
reply showed that self-oriented pronouns were
more likely to have no response and second-
person pronouns were more likely to have a neg-
ative response. The politeness and argumentative-
ness/profanity lists are generated by starting with
hand-specified seed lists used to train an SVM to
classify word embeddings (Mikolov et al., 2013)
</bodyText>
<page confidence="0.994578">
2028
</page>
<figureCaption confidence="0.897229">
Figure 1: Relative improvement in P@1 over
G&amp;T for individual feature groups.
</figureCaption>
<bodyText confidence="0.9993935">
into these categories, and expanding the lists with
500 words farthest from the decision boundary.
Both the NMF and the skip-gram topic models
use a cosine distance to determine topic similarity,
with 300 as the word embedding dimension. Both
are trained on approximately 2 million comments
in high karma posts taken across a wide variety of
subreddits. We use topic models in various mea-
sures of comment relevance to the discussion, but
we do not use topic of the comment on its own
since topic is controlled for by ranking within a
thread.
</bodyText>
<sectionHeader confidence="0.985815" genericHeader="method">
5 Ranking Experiments
</sectionHeader>
<bodyText confidence="0.999879818181818">
We present three sets of experiments on comment
karma ranking, all of which show very differ-
ent behavior for the different subreddits. Fig. 1
shows the relative gain in P@1 over the G&amp;T
baseline associated with using different feature
groups. The importance of the different features
reflect the nature of the different communities.
The authority/reputation features help most for
ASKSCIENCE, consistent with our k-index study.
Informativeness and relevance help all subred-
dits except ASKMEN and WORLDNEWS. Lexical,
mood and community style features are useful in
some cases, but hurt others. The predicted proba-
bility of a reply was least useful, possibly because
of the low-karma training bias.
Tables 3 and 4 summarize the results for the
P@1 and NDCG criteria using the greedy selec-
tion procedure (which optimizes P@1) compared
to a random baseline and the G&amp;T baseline. The
random baseline for P@1 is greater than 10% be-
cause of ties. The G&amp;T baseline results show that
the graph and timing features alone obtain 21-32%
</bodyText>
<table confidence="0.999314">
subreddit Random G&amp;T All
ASKSCIENCE 15.9 21.8 25.3
FITNESS 19.4 22.1 27.3
POLITICS 18.5 24.7 26.4
ASKWOMEN 17.6 24.9 28.0
ASKMEN 18.2 31.4 29.1
WORLDNEWS 15.4 24.5 23.3
Improvement - 42.9% 52.1%
</table>
<tableCaption confidence="0.9738435">
Table 3: Test set precision of top one prediction
(P@1) performance for specific subreddits.
</tableCaption>
<table confidence="0.990883">
subreddit Random G&amp;T All
ASKSCIENCE 0.53 0.60 0.60
FITNESS 0.57 0.61 0.62
POLITICS 0.55 0.61 0.62
ASKWOMEN 0.56 0.62 0.65
ASKMEN 0.56 0.66 0.66
WORLDNEWS 0.54 0.61 0.60
Improvement - 12.5% 13.2%
</table>
<tableCaption confidence="0.976988">
Table 4: Test set ranking NDCG performance for
specific subreddits.
</tableCaption>
<bodyText confidence="0.9998323125">
of top karma comments depending on subreddits.
Adding the textual features gives an improvement
in P@1 performance over the G&amp;T baseline for
all subreddits except ASKMEN and WORLDNEWS.
The trends for performance measured with NDCG
are similar, but the benefit from textual features is
smaller. The results in both tables show different
ways of reporting performance of the same sys-
tem, but the system has been optimized for P@1
in terms of feature selection. In initial exploratory
experiments, this seems to have a small impact:
when optimizing for NDCG in feature selection
we obtain 0.61 vs. 0.60 with the P@1-optimized
features.
A major challenge with identifying high karma
comments (and negative karma comments) is that
</bodyText>
<table confidence="0.99949975">
subreddit Pos/Neg High/Mid Ranking
ASKSCIENCE 44.5 63.7 61.3
FITNESS 74.7 43.9 57.5
POLITICS 95.5 59.1 58.0
ASKWOMEN 82.5 67.6 59.7
ASKMEN 87.0 66.2 60.6
WORLDNEWS 93.3 69.9 57.3
Average 79.6 61.7 59.1
</table>
<tableCaption confidence="0.8101434">
Table 5: Accuracy of binary classifiers trained on
balanced data to distinguish: positive vs. nega-
tive karma (Pos/Neg), high vs. mid-level karma
(High/Mid), and ranking between any pair (Rank-
ing).
</tableCaption>
<page confidence="0.995003">
2029
</page>
<bodyText confidence="0.999963862068965">
they are so rare. Although our feature selection
tunes for high rank precision, it is possible that
the low-karma data dominate the learning. Alter-
natively, it may be that language cues are mainly
useful for identifying distinguishing the negative
or mid-level karma comments, and that the very
high karma comments are a matter of timing. To
better understand the role of language for these
different types, we trained classifiers on balanced
data for positive vs. negative karma and high vs.
mid levels of karma. For these models, the training
pairs could come from different threads, but topic
is controlled for in that all topic features are rela-
tive (similarity to original post, parent, etc.). We
compared the results to the binary classifier used
in ranking, where all pairs are considered. In all
three cases, random chance accuracy is 50%.
Table 5 shows the pairwise accuracy of these
classifiers. We find that distinguishing positive
from negative classes is fairly easy, with the no-
table exception of the more information-oriented
subreddit ASKSCIENCE. Averaging across the dif-
ferent subreddits, the high vs. mid task is slightly
easier than the general ranking task, but the vari-
ation across subreddits is substantial. The high
vs. mid distinction for FITNESS falls below chance
(likely overtraining), whereas it seems to be an
easier task for the ASKWOMEN, ASKMEN, and
WORLDNEWS.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999983918918919">
Interest in social media is rapidly growing in re-
cent years, which includes work on predicting
the popularity of posts, comments and tweets.
Danescu-Niculescu-Mizil et al. (2012) investigate
phrase memorability in the movie quotes. Cheng
et al. (2014) explore prediction of information
cascades on Facebook. Weninger et al. (2013)
analyze the hierarchy of the Reddit discussions,
topic shifts, and popularity of the comment, us-
ing among the others very simple language anal-
ysis. Lampos et al. (2014) study the problem of
predicting a Twitter user impact score (determined
by combining the numbers of user’s followers, fol-
lowees, and listings) using text-based and non-
textual features, showing that performance im-
proves when user participation in particular topics
is included.
Most relevant to this paper are studies of the ef-
fect of language in popularity predictions. Tan et
al. (2014) study how word choice affects the pop-
ularity of Twitter messages. As in our work, they
control for topic, but they also control for the pop-
ularity of the message authors. On Reddit, we find
that celebrity status is less important than it is on
Twitter since on Reddit almost everyone is anony-
mous. Lakkaraju et al. (2013) study how timing
and language affect the popularity of posting im-
ages on Reddit. They control for content by only
making comparisons between reposts of the same
image. Our focus is on studying comments within
a discussion instead of standalone posts, and we
analyze a vast majority of language features. Al-
thoff et al. (2014) use deeper language analysis on
Reddit to predict the success of receiving a pizza
in the Random Acts of Pizza subreddit. To our
knowledge, this is the first work on ranking com-
ments in terms of community endorsement.
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999991733333333">
This paper addresses the problem of how language
affects the reaction of community in Reddit com-
ments. We collect a new dataset of six subredit dis-
cussion forums. We introduce a new task of rank-
ing comments based on karma in Reddit discus-
sions, which controls for topic and timing of com-
ments. Our results show that using language fea-
tures improve the comment ranking task in most
of the subreddits. Informativeness and relevance
are the most broadly useful feature categories; rep-
utation matters for ASKSCIENCE, and other cate-
gories could either help or hurt depending on the
community. Future work involves improving the
classification algorithm by using new approaches
to learning about rare events.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996985">
Tim Althoff, Cristian Danescu-Niculescu-Mizil, and
Dan Jurafsky. 2014. How to ask for a favor: A case
study on the success of altruistic requests. In Proc.
ICWSM.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of the International Conference on Ma-
chine Learning, pages 89–96.
Meeyoung Cha, Hamed Haddadi, Fabricio Ben-
evenuto, and P Krishna Gummadi. 2010. Measur-
ing user influence in twitter: The million follower
fallacy. ICWSM, 10(10-17):30.
Justin Cheng, Lada Adamic, P Alex Dow, Jon Michael
Kleinberg, and Jure Leskovec. 2014. Can cascades
be predicted? In Proc. WWW.
</reference>
<page confidence="0.763648">
2030
</page>
<reference confidence="0.999867255319149">
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Proc.
ACL.
Jorge E Hirsch. 2005. An index to quantify an individ-
ual’s scientific research output. Proceedings of the
National Academy of Sciences of the United States
of America, 102(46):16569–16572.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. SIGKDD.
Himabindu Lakkaraju, Julian J McAuley, and Jure
Leskovec. 2013. What’s in a name? understanding
the interplay between titles, content, and communi-
ties in social media. In Proc. ICWSM.
Cliff Lampe and Paul Resnick. 2004. Slash(dot) and
burn: distributed moderation in a large online con-
versation space. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems,
pages 543–550.
Vasileios Lampos, Nikolaos Aletras, Daniel Preotiuc-
Pietro, and Trevor Cohn. 2014. Predicting and char-
acterizing user impact on Twitter. In Proceedings of
the Conference of the European Chapter of the ACL,
pages 405–413.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. NIPS.
Alexander Strehl, Joydeep Ghosh, and Raymond
Mooney. 2000. Impact of similarity measures on
web-page clustering. In Workshop on Artificial In-
telligence for Web Search.
Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H
Chi. 2010. Want to be retweeted? Large scale an-
alytics on factors impacting retweet in Twitter net-
work. In Proc. SocialCom, pages 177–184. IEEE.
Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The ef-
fect of wording on message propagation: Topic-and
author-controlled natural experiments on Twitter. In
Proc. ACL.
Tim Weninger, Xihao Avi Zhu, and Jiawei Han. 2013.
An exploration of discussion threads in social news
sites: A case study of the reddit community. In Proc.
ASONAM.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factoriza-
tion. In Proc. SIGIR.
</reference>
<page confidence="0.993664">
2031
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953611">
<title confidence="0.991141">Talking to the crowd: What do people react to in online discussions?</title>
<author confidence="0.985927">Aaron Jaech</author>
<author confidence="0.985927">Vicky Zayats</author>
<author confidence="0.985927">Hao Fang</author>
<author confidence="0.985927">Mari Ostendorf</author>
<author confidence="0.985927">Hannaneh</author>
<affiliation confidence="0.997913">Dept. of Electrical University of</affiliation>
<abstract confidence="0.998231928571429">This paper addresses the question of how language use affects community reaction to comments in online discussion forums, and the relative importance of the message vs. the messenger. A new comment ranking task is proposed based on community annotated karma in Reddit discussions, which controls for topic and timing of comments. Experimental work with discussion threads from six subreddits shows that the importance of different types of language features varies with the community of interest.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tim Althoff</author>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Dan Jurafsky</author>
</authors>
<title>How to ask for a favor: A case study on the success of altruistic requests.</title>
<date>2014</date>
<booktitle>In Proc. ICWSM.</booktitle>
<contexts>
<context position="1940" citStr="Althoff et al., 2014" startWordPosition="296" endWordPosition="299">s the reaction, compared to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest. Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion. The primary contributions of this work include findings about the role of aut</context>
<context position="18856" citStr="Althoff et al. (2014)" startWordPosition="3075" endWordPosition="3079">cts the popularity of Twitter messages. As in our work, they control for topic, but they also control for the popularity of the message authors. On Reddit, we find that celebrity status is less important than it is on Twitter since on Reddit almost everyone is anonymous. Lakkaraju et al. (2013) study how timing and language affect the popularity of posting images on Reddit. They control for content by only making comparisons between reposts of the same image. Our focus is on studying comments within a discussion instead of standalone posts, and we analyze a vast majority of language features. Althoff et al. (2014) use deeper language analysis on Reddit to predict the success of receiving a pizza in the Random Acts of Pizza subreddit. To our knowledge, this is the first work on ranking comments in terms of community endorsement. 7 Conclusion This paper addresses the problem of how language affects the reaction of community in Reddit comments. We collect a new dataset of six subredit discussion forums. We introduce a new task of ranking comments based on karma in Reddit discussions, which controls for topic and timing of comments. Our results show that using language features improve the comment ranking </context>
</contexts>
<marker>Althoff, Danescu-Niculescu-Mizil, Jurafsky, 2014</marker>
<rawString>Tim Althoff, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2014. How to ask for a favor: A case study on the success of altruistic requests. In Proc. ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Burges</author>
<author>Tal Shaked</author>
<author>Erin Renshaw</author>
<author>Ari Lazier</author>
<author>Matt Deeds</author>
<author>Nicole Hamilton</author>
<author>Greg Hullender</author>
</authors>
<title>Learning to rank using gradient descent.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Machine Learning,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="8756" citStr="Burges et al., 2005" startWordPosition="1446" endWordPosition="1449">o have an impact. Very long threads are sampled, so that these do not dominate the set of lists. Approximately 75% of the comment lists are designated for training and the rest is for testing, with splits at the discussion thread level. Here, feature selection is based on mean precision of the top-ranked comment (P@1), so as to emphasize learning the rare high karma events. (Note that P@1 is equivalent to accuracy but allows for any top-ranking comment to count as correct in the case of ties.) The system performance is evaluated using both P@1 and normalized discounted cumulative gain (NDCG) (Burges et al., 2005), which is a standard criterion for ranking evaluation when the samples to be ranked have meaningful differences in scores, as is the case for karma of the comments. In addition, for analysis purposes, we report results for three surrogate tasks that can be used in the ranking problem: i) the binary ranker trained on all comment pairs within each list, in which low karma comments dominate, ii) a positive vs. negative karma classifier, and iii) a high vs. medium karma classifier. All use class-balanced data; the second two are trained and tested on a biased sampling of the data, where the pairs</context>
</contexts>
<marker>Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton, Hullender, 2005</marker>
<rawString>Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the International Conference on Machine Learning, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meeyoung Cha</author>
<author>Hamed Haddadi</author>
<author>Fabricio Benevenuto</author>
<author>P Krishna Gummadi</author>
</authors>
<title>Measuring user influence in twitter: The million follower fallacy.</title>
<date>2010</date>
<journal>ICWSM,</journal>
<pages>10--10</pages>
<contexts>
<context position="5470" citStr="Cha et al., 2010" startWordPosition="882" endWordPosition="885">other than the language use that influence whether a comment will have uptake from the community include the topic, the timing of the message, and the messenger. These factors are all evident in the Reddit discussions. Some subreddits are more popular and thus have higher karma comments than others, reflecting the influence of topic. Comments that are posted early in the discussion are more likely to have high karma, since they have more potential responses. Previous studies on Twitter show that the reputation of the author substantially increases the chances of the retweet (Suh et al., 2010; Cha et al., 2010), and reputation is also raised as a factor in Slashdot (Lampe and Resnick, 2004). On Reddit most users are anonymous, but it is possible that members of a forum become familiar with particular usernames associated with high karma comments. In order to see how important per2http://www.redditblog.com/2014/12/reddit-in-2014.html 3Our data collection is available online at https://ssli.ee.washington.edu/tial/data/reddit 4Based on our initial look at the data, we noticed that most posts receive all of their comments within 48 hours. Some comments are deleted before we are able to download them. Ta</context>
</contexts>
<marker>Cha, Haddadi, Benevenuto, Gummadi, 2010</marker>
<rawString>Meeyoung Cha, Hamed Haddadi, Fabricio Benevenuto, and P Krishna Gummadi. 2010. Measuring user influence in twitter: The million follower fallacy. ICWSM, 10(10-17):30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Cheng</author>
<author>Lada Adamic</author>
<author>P Alex Dow</author>
<author>Jon Michael Kleinberg</author>
<author>Jure Leskovec</author>
</authors>
<title>Can cascades be predicted? In</title>
<date>2014</date>
<booktitle>Proc. WWW.</booktitle>
<contexts>
<context position="17572" citStr="Cheng et al. (2014)" startWordPosition="2863" endWordPosition="2866">d subreddit ASKSCIENCE. Averaging across the different subreddits, the high vs. mid task is slightly easier than the general ranking task, but the variation across subreddits is substantial. The high vs. mid distinction for FITNESS falls below chance (likely overtraining), whereas it seems to be an easier task for the ASKWOMEN, ASKMEN, and WORLDNEWS. 6 Related Work Interest in social media is rapidly growing in recent years, which includes work on predicting the popularity of posts, comments and tweets. Danescu-Niculescu-Mizil et al. (2012) investigate phrase memorability in the movie quotes. Cheng et al. (2014) explore prediction of information cascades on Facebook. Weninger et al. (2013) analyze the hierarchy of the Reddit discussions, topic shifts, and popularity of the comment, using among the others very simple language analysis. Lampos et al. (2014) study the problem of predicting a Twitter user impact score (determined by combining the numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included. Most relevant to this paper are studies of the effect of language in popul</context>
</contexts>
<marker>Cheng, Adamic, Dow, Kleinberg, Leskovec, 2014</marker>
<rawString>Justin Cheng, Lada Adamic, P Alex Dow, Jon Michael Kleinberg, and Jure Leskovec. 2014. Can cascades be predicted? In Proc. WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Justin Cheng</author>
<author>Jon Kleinberg</author>
<author>Lillian Lee</author>
</authors>
<title>You had me at hello: How phrasing affects memorability.</title>
<date>2012</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1894" citStr="Danescu-Niculescu-Mizil et al., 2012" startWordPosition="288" endWordPosition="291">n this paper, we look at whether (and how) language use affects the reaction, compared to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest. Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion. The primary contributions of th</context>
<context position="17499" citStr="Danescu-Niculescu-Mizil et al. (2012)" startWordPosition="2852" endWordPosition="2855">negative classes is fairly easy, with the notable exception of the more information-oriented subreddit ASKSCIENCE. Averaging across the different subreddits, the high vs. mid task is slightly easier than the general ranking task, but the variation across subreddits is substantial. The high vs. mid distinction for FITNESS falls below chance (likely overtraining), whereas it seems to be an easier task for the ASKWOMEN, ASKMEN, and WORLDNEWS. 6 Related Work Interest in social media is rapidly growing in recent years, which includes work on predicting the popularity of posts, comments and tweets. Danescu-Niculescu-Mizil et al. (2012) investigate phrase memorability in the movie quotes. Cheng et al. (2014) explore prediction of information cascades on Facebook. Weninger et al. (2013) analyze the hierarchy of the Reddit discussions, topic shifts, and popularity of the comment, using among the others very simple language analysis. Lampos et al. (2014) study the problem of predicting a Twitter user impact score (determined by combining the numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included. M</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Cheng, Kleinberg, Lee, 2012</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon Kleinberg, and Lillian Lee. 2012. You had me at hello: How phrasing affects memorability. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge E Hirsch</author>
</authors>
<title>An index to quantify an individual’s scientific research output.</title>
<date>2005</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<pages>102--46</pages>
<contexts>
<context position="6610" citStr="Hirsch, 2005" startWordPosition="1071" endWordPosition="1072"> hours. Some comments are deleted before we are able to download them. Table 2: Percentage of discussions where the top comment is made by the top k-index person (or top 3 people) in the discussion. sonal reputation is, we looked at how often the top karma comments are associated with the top karma participants in the discussion. Since an individual’s karma can be skewed by a few very popular posts, we measure reputation instead using a measure we call the k-index, defined to be equal to the number of comments in each user’s history that have karma ≥ k. The k-index is analgous to the h-index (Hirsch, 2005) and arguably a better indicator of extended impact than total karma. The results in Table 2 address the question of whether the top karma comments always come from the top karma person. The Top1 column shows the percentage of threads where the top karma comment in a discussion happens to be made by the highest k-index person participating in the discussion; the next column shows the percentage of threads where the comment comes from any one of the top 3 k-index people. We find that, in fact, the highest karma comment in a discussion is rarely from the highest k-index people. The highest perce</context>
</contexts>
<marker>Hirsch, 2005</marker>
<rawString>Jorge E Hirsch. 2005. An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences of the United States of America, 102(46):16569–16572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>Proc. SIGKDD.</booktitle>
<contexts>
<context position="9488" citStr="Joachims, 2002" startWordPosition="1575" endWordPosition="1576">cores, as is the case for karma of the comments. In addition, for analysis purposes, we report results for three surrogate tasks that can be used in the ranking problem: i) the binary ranker trained on all comment pairs within each list, in which low karma comments dominate, ii) a positive vs. negative karma classifier, and iii) a high vs. medium karma classifier. All use class-balanced data; the second two are trained and tested on a biased sampling of the data, where the pairs need not be from the same discussion thread. 4.2 Classifier We use the support vector machine (SVM) rank algorithm (Joachims, 2002) to predict a rank order for each list of comments. The SVM is trained to predict which of a pair of comments has higher karma. The error term penalty parameter is tuned to maximize P@1 on a held-out validation set (20% of the training samples). Since much of the data includes low-karma comments, there will be a tendancy for the learning to emphasize features that discriminate comments at the lower end of the scale. In order to learn features that improve P@1, and to understand the relative importance of different features, we use a greedy automatic feature selection process that incrementally</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proc. SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Himabindu Lakkaraju</author>
<author>Julian J McAuley</author>
<author>Jure Leskovec</author>
</authors>
<title>What’s in a name? understanding the interplay between titles, content, and communities in social media.</title>
<date>2013</date>
<booktitle>In Proc. ICWSM.</booktitle>
<contexts>
<context position="1918" citStr="Lakkaraju et al., 2013" startWordPosition="292" endWordPosition="295">how) language use affects the reaction, compared to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest. Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion. The primary contributions of this work include findings</context>
<context position="3425" citStr="Lakkaraju et al., 2013" startWordPosition="543" endWordPosition="546">vance and mood. 2 Data Reddit1 is the largest public online discussion forum with a wide variety of subreddits, which makes it a good data source for studying how textual content in a discussion impacts the response of the crowd. On Reddit, people initiate a discussion thread with a post (a question, a link to a news item, etc.), and others respond with comments. Registered users vote on which posts and comments are important. The total amount of up votes minus the down votes (roughly) is called karma; it provides an indication of community endorsement and popularity of a comment, as used in (Lakkaraju et al., 2013). Karma is valued as it impacts the order in which the posts or comments are displayed, with the high karma content rising to the top. Karma points are also accumulated by members of the discussion forum as a function of the karma associated with their comments. 1http://www.reddit.com 2026 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2026–2031, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. subreddit # Posts # Comments/Post FITNESS 3K 16.3 ASKSCIENCE 4K 8.8 POLITICS 7K 23.7 ASKWOMEN 4K 50.5 ASKMEN 4K 58</context>
<context position="18530" citStr="Lakkaraju et al. (2013)" startWordPosition="3021" endWordPosition="3024">he numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included. Most relevant to this paper are studies of the effect of language in popularity predictions. Tan et al. (2014) study how word choice affects the popularity of Twitter messages. As in our work, they control for topic, but they also control for the popularity of the message authors. On Reddit, we find that celebrity status is less important than it is on Twitter since on Reddit almost everyone is anonymous. Lakkaraju et al. (2013) study how timing and language affect the popularity of posting images on Reddit. They control for content by only making comparisons between reposts of the same image. Our focus is on studying comments within a discussion instead of standalone posts, and we analyze a vast majority of language features. Althoff et al. (2014) use deeper language analysis on Reddit to predict the success of receiving a pizza in the Random Acts of Pizza subreddit. To our knowledge, this is the first work on ranking comments in terms of community endorsement. 7 Conclusion This paper addresses the problem of how la</context>
</contexts>
<marker>Lakkaraju, McAuley, Leskovec, 2013</marker>
<rawString>Himabindu Lakkaraju, Julian J McAuley, and Jure Leskovec. 2013. What’s in a name? understanding the interplay between titles, content, and communities in social media. In Proc. ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cliff Lampe</author>
<author>Paul Resnick</author>
</authors>
<title>Slash(dot) and burn: distributed moderation in a large online conversation space.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>543--550</pages>
<contexts>
<context position="1538" citStr="Lampe and Resnick, 2004" startWordPosition="236" endWordPosition="239"> Discussion forums tend to specialize on different topics, and people participating in them form communities of interest. The reaction of people within a community to comments posted provides an indication of community endorsement of opinions and value of information. In most discussions, the vast majority of comments spawn little reaction. In this paper, we look at whether (and how) language use affects the reaction, compared to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors </context>
<context position="5551" citStr="Lampe and Resnick, 2004" startWordPosition="897" endWordPosition="900">ake from the community include the topic, the timing of the message, and the messenger. These factors are all evident in the Reddit discussions. Some subreddits are more popular and thus have higher karma comments than others, reflecting the influence of topic. Comments that are posted early in the discussion are more likely to have high karma, since they have more potential responses. Previous studies on Twitter show that the reputation of the author substantially increases the chances of the retweet (Suh et al., 2010; Cha et al., 2010), and reputation is also raised as a factor in Slashdot (Lampe and Resnick, 2004). On Reddit most users are anonymous, but it is possible that members of a forum become familiar with particular usernames associated with high karma comments. In order to see how important per2http://www.redditblog.com/2014/12/reddit-in-2014.html 3Our data collection is available online at https://ssli.ee.washington.edu/tial/data/reddit 4Based on our initial look at the data, we noticed that most posts receive all of their comments within 48 hours. Some comments are deleted before we are able to download them. Table 2: Percentage of discussions where the top comment is made by the top k-index</context>
</contexts>
<marker>Lampe, Resnick, 2004</marker>
<rawString>Cliff Lampe and Paul Resnick. 2004. Slash(dot) and burn: distributed moderation in a large online conversation space. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 543–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nikolaos Aletras</author>
<author>Daniel PreotiucPietro</author>
<author>Trevor Cohn</author>
</authors>
<title>Predicting and characterizing user impact on Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the ACL,</booktitle>
<pages>405--413</pages>
<contexts>
<context position="17820" citStr="Lampos et al. (2014)" startWordPosition="2902" endWordPosition="2905">nce (likely overtraining), whereas it seems to be an easier task for the ASKWOMEN, ASKMEN, and WORLDNEWS. 6 Related Work Interest in social media is rapidly growing in recent years, which includes work on predicting the popularity of posts, comments and tweets. Danescu-Niculescu-Mizil et al. (2012) investigate phrase memorability in the movie quotes. Cheng et al. (2014) explore prediction of information cascades on Facebook. Weninger et al. (2013) analyze the hierarchy of the Reddit discussions, topic shifts, and popularity of the comment, using among the others very simple language analysis. Lampos et al. (2014) study the problem of predicting a Twitter user impact score (determined by combining the numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included. Most relevant to this paper are studies of the effect of language in popularity predictions. Tan et al. (2014) study how word choice affects the popularity of Twitter messages. As in our work, they control for topic, but they also control for the popularity of the message authors. On Reddit, we find that celebrity status</context>
</contexts>
<marker>Lampos, Aletras, PreotiucPietro, Cohn, 2014</marker>
<rawString>Vasileios Lampos, Nikolaos Aletras, Daniel PreotiucPietro, and Trevor Cohn. 2014. Predicting and characterizing user impact on Twitter. In Proceedings of the Conference of the European Chapter of the ACL, pages 405–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="11726" citStr="Mikolov et al., 2013" startWordPosition="1928" endWordPosition="1931">grams (Lex.): Miscellaneous word class indicators, puncutation, and part-ofspeech counts • Predicted Community Response (Resp.): Probability scores from surrogate classification tasks (reply vs. no reply, positive vs. negative sentiment) to measure the community response of a comment using bag-of-words predictors. • Relevance (Rel.): Comment similarity to the parent, post and title in terms of topic, computed with three methods: i) a distributed vector representation of topic using a non-negative matrix factorization (NMF) model (Xu et al., 2003), ii) the average of skip-gram word embeddings (Mikolov et al., 2013), and iii) word set Jaccard similarity (Strehl et al., 2000). • Mood: Mean and std. deviation of sentence sentiment in the comment; word list indicators for politeness, argumentativeness and profanity. • Community Style (Comm.): Posterior probability of each subreddit given the comment using a bag-of-words model. The various word lists are motivated by feature exploration studies in surrogate tasks. For example, projecting words to a two dimensional space of positive vs. negative and likelihood of reply showed that self-oriented pronouns were more likely to have no response and secondperson pr</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Strehl</author>
<author>Joydeep Ghosh</author>
<author>Raymond Mooney</author>
</authors>
<title>Impact of similarity measures on web-page clustering.</title>
<date>2000</date>
<booktitle>In Workshop on Artificial Intelligence for Web Search.</booktitle>
<contexts>
<context position="11786" citStr="Strehl et al., 2000" startWordPosition="1938" endWordPosition="1941">n, and part-ofspeech counts • Predicted Community Response (Resp.): Probability scores from surrogate classification tasks (reply vs. no reply, positive vs. negative sentiment) to measure the community response of a comment using bag-of-words predictors. • Relevance (Rel.): Comment similarity to the parent, post and title in terms of topic, computed with three methods: i) a distributed vector representation of topic using a non-negative matrix factorization (NMF) model (Xu et al., 2003), ii) the average of skip-gram word embeddings (Mikolov et al., 2013), and iii) word set Jaccard similarity (Strehl et al., 2000). • Mood: Mean and std. deviation of sentence sentiment in the comment; word list indicators for politeness, argumentativeness and profanity. • Community Style (Comm.): Posterior probability of each subreddit given the comment using a bag-of-words model. The various word lists are motivated by feature exploration studies in surrogate tasks. For example, projecting words to a two dimensional space of positive vs. negative and likelihood of reply showed that self-oriented pronouns were more likely to have no response and secondperson pronouns were more likely to have a negative response. The pol</context>
</contexts>
<marker>Strehl, Ghosh, Mooney, 2000</marker>
<rawString>Alexander Strehl, Joydeep Ghosh, and Raymond Mooney. 2000. Impact of similarity measures on web-page clustering. In Workshop on Artificial Intelligence for Web Search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bongwon Suh</author>
<author>Lichan Hong</author>
<author>Peter Pirolli</author>
<author>Ed H Chi</author>
</authors>
<title>Want to be retweeted? Large scale analytics on factors impacting retweet in Twitter network. In</title>
<date>2010</date>
<booktitle>Proc. SocialCom,</booktitle>
<pages>177--184</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5451" citStr="Suh et al., 2010" startWordPosition="878" endWordPosition="881">e Factors Factors other than the language use that influence whether a comment will have uptake from the community include the topic, the timing of the message, and the messenger. These factors are all evident in the Reddit discussions. Some subreddits are more popular and thus have higher karma comments than others, reflecting the influence of topic. Comments that are posted early in the discussion are more likely to have high karma, since they have more potential responses. Previous studies on Twitter show that the reputation of the author substantially increases the chances of the retweet (Suh et al., 2010; Cha et al., 2010), and reputation is also raised as a factor in Slashdot (Lampe and Resnick, 2004). On Reddit most users are anonymous, but it is possible that members of a forum become familiar with particular usernames associated with high karma comments. In order to see how important per2http://www.redditblog.com/2014/12/reddit-in-2014.html 3Our data collection is available online at https://ssli.ee.washington.edu/tial/data/reddit 4Based on our initial look at the data, we noticed that most posts receive all of their comments within 48 hours. Some comments are deleted before we are able t</context>
</contexts>
<marker>Suh, Hong, Pirolli, Chi, 2010</marker>
<rawString>Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H Chi. 2010. Want to be retweeted? Large scale analytics on factors impacting retweet in Twitter network. In Proc. SocialCom, pages 177–184. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhao Tan</author>
<author>Lillian Lee</author>
<author>Bo Pang</author>
</authors>
<title>The effect of wording on message propagation: Topic-and author-controlled natural experiments on Twitter. In</title>
<date>2014</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="1959" citStr="Tan et al., 2014" startWordPosition="300" endWordPosition="303">ed to the relative importance of the author and timing of the post. Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect). Judging by differences in popularity of various discussion forums, topic is clearly important. Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014; Tan et al., 2014). Teasing these different factors apart, however, is a challenge. The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest. Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion. The primary contributions of this work include findings about the role of author reputation and </context>
<context position="18208" citStr="Tan et al. (2014)" startWordPosition="2963" endWordPosition="2966"> of information cascades on Facebook. Weninger et al. (2013) analyze the hierarchy of the Reddit discussions, topic shifts, and popularity of the comment, using among the others very simple language analysis. Lampos et al. (2014) study the problem of predicting a Twitter user impact score (determined by combining the numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included. Most relevant to this paper are studies of the effect of language in popularity predictions. Tan et al. (2014) study how word choice affects the popularity of Twitter messages. As in our work, they control for topic, but they also control for the popularity of the message authors. On Reddit, we find that celebrity status is less important than it is on Twitter since on Reddit almost everyone is anonymous. Lakkaraju et al. (2013) study how timing and language affect the popularity of posting images on Reddit. They control for content by only making comparisons between reposts of the same image. Our focus is on studying comments within a discussion instead of standalone posts, and we analyze a vast majo</context>
</contexts>
<marker>Tan, Lee, Pang, 2014</marker>
<rawString>Chenhao Tan, Lillian Lee, and Bo Pang. 2014. The effect of wording on message propagation: Topic-and author-controlled natural experiments on Twitter. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Weninger</author>
<author>Xihao Avi Zhu</author>
<author>Jiawei Han</author>
</authors>
<title>An exploration of discussion threads in social news sites: A case study of the reddit community. In</title>
<date>2013</date>
<booktitle>Proc. ASONAM.</booktitle>
<contexts>
<context position="17651" citStr="Weninger et al. (2013)" startWordPosition="2874" endWordPosition="2877">vs. mid task is slightly easier than the general ranking task, but the variation across subreddits is substantial. The high vs. mid distinction for FITNESS falls below chance (likely overtraining), whereas it seems to be an easier task for the ASKWOMEN, ASKMEN, and WORLDNEWS. 6 Related Work Interest in social media is rapidly growing in recent years, which includes work on predicting the popularity of posts, comments and tweets. Danescu-Niculescu-Mizil et al. (2012) investigate phrase memorability in the movie quotes. Cheng et al. (2014) explore prediction of information cascades on Facebook. Weninger et al. (2013) analyze the hierarchy of the Reddit discussions, topic shifts, and popularity of the comment, using among the others very simple language analysis. Lampos et al. (2014) study the problem of predicting a Twitter user impact score (determined by combining the numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included. Most relevant to this paper are studies of the effect of language in popularity predictions. Tan et al. (2014) study how word choice affects the populari</context>
</contexts>
<marker>Weninger, Zhu, Han, 2013</marker>
<rawString>Tim Weninger, Xihao Avi Zhu, and Jiawei Han. 2013. An exploration of discussion threads in social news sites: A case study of the reddit community. In Proc. ASONAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Xin Liu</author>
<author>Yihong Gong</author>
</authors>
<title>Document clustering based on non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In Proc. SIGIR.</booktitle>
<contexts>
<context position="11657" citStr="Xu et al., 2003" startWordPosition="1917" endWordPosition="1920">ts, named entity counts, urls, and unseen n-grams. • Lexical Unigrams (Lex.): Miscellaneous word class indicators, puncutation, and part-ofspeech counts • Predicted Community Response (Resp.): Probability scores from surrogate classification tasks (reply vs. no reply, positive vs. negative sentiment) to measure the community response of a comment using bag-of-words predictors. • Relevance (Rel.): Comment similarity to the parent, post and title in terms of topic, computed with three methods: i) a distributed vector representation of topic using a non-negative matrix factorization (NMF) model (Xu et al., 2003), ii) the average of skip-gram word embeddings (Mikolov et al., 2013), and iii) word set Jaccard similarity (Strehl et al., 2000). • Mood: Mean and std. deviation of sentence sentiment in the comment; word list indicators for politeness, argumentativeness and profanity. • Community Style (Comm.): Posterior probability of each subreddit given the comment using a bag-of-words model. The various word lists are motivated by feature exploration studies in surrogate tasks. For example, projecting words to a two dimensional space of positive vs. negative and likelihood of reply showed that self-orien</context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on non-negative matrix factorization. In Proc. SIGIR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
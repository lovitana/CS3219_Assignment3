<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.995284">
Re-evaluating Automatic Summarization with BLEU and
192 Shades of ROUGE
</title>
<author confidence="0.976019">
Yvette Graham
</author>
<affiliation confidence="0.983108">
ADAPT Centre
School of Computer Science and Statistics
Trinity College Dublin
</affiliation>
<email confidence="0.995189">
graham.yvette@gmail.com
</email>
<sectionHeader confidence="0.997354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916035714286">
We provide an analysis of current evalua-
tion methodologies applied to summariza-
tion metrics and identify the following ar-
eas of concern: (1) movement away from
evaluation by correlation with human as-
sessment; (2) omission of important com-
ponents of human assessment from eval-
uations, in addition to large numbers of
metric variants; (3) absence of methods
of significance testing improvements over
a baseline. We outline an evaluation
methodology that overcomes all such chal-
lenges, providing the first method of sig-
nificance testing suitable for evaluation of
summarization metrics. Our evaluation re-
veals for the first time which metric vari-
ants significantly outperform others, op-
timal metric variants distinct from cur-
rent recommended best variants, as well
as machine translation metric BLEU to
have performance on-par with ROUGE for
the purpose of evaluation of summariza-
tion systems. We subsequently replicate
a recent large-scale evaluation that relied
on, what we now know to be, suboptimal
ROUGE variants revealing distinct conclu-
sions about the relative performance of
state-of-the-art summarization systems.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999841159090909">
Automatic metrics of summarization evaluation
have their origins in machine translation (MT),
with ROUGE (Lin and Hovy, 2003), the first and
still most widely used automatic summarization
metric, comprising an adaption of the BLEU score
(Papineni et al., 2002). Automatic evaluation in
MT and summarization have much in common, as
both involve the automatic comparison of system-
generated texts with one or more human-generated
reference texts, contrasting either system-output
translations or peer summaries with human ref-
erence translations or model summaries, depend-
ing on the task. In both MT and summarization
evaluation, any newly proposed automatic metric
must be assessed by the degree to which it pro-
vides a good substitute of human assessment, and
although there are obvious parallels between eval-
uation of systems in the two areas, when it comes
to evaluation of metrics, summarization has di-
verged considerably from methodologies applied
to evaluation of metrics in MT.
Since the inception of BLEU, evaluation of au-
tomatic metrics in MT has been by correlation
with human assessment. In contrast in summa-
rization, over the years since the introduction of
ROUGE, summarization evaluation has seen a va-
riety of different methodologies applied to evalu-
ation of its metrics. Evaluation of summarization
metrics has included, for example, the ability of a
metric/significance test combination to distinguish
between sets of human and system-generated sum-
maries (Rankel et al., 2011), or by accuracy of
conclusions drawn from metrics when combined
with a particular significance test, Wilcoxon rank-
sum (Owczarzak et al., 2012).
Besides moving away from well-established
methods such as correlation with human judg-
ment, previous summarization metric evaluations
have been additionally limited by inclusion of only
a small proportion of possible metrics and vari-
ants. For example, although the most commonly
used metric ROUGE has a very large number of
possible variants, it is common to include only a
small range of those in evaluations. This has the
</bodyText>
<page confidence="0.961422">
128
</page>
<note confidence="0.9851255">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128–137,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99993505882353">
obvious disadvantage that superior variants may
exist but remain unidentified due to their omission.
Despite such limitations, however, subsequent
evaluations of state-of-the-art summarization sys-
tems operate under the assumption that recom-
mended ROUGE variants are optimal and rely on
this assumption to draw conclusions about the rel-
ative performance of systems (Hong et al., 2014).
This forces us to raise some important questions.
Firstly, to what degree was the divergence away
from evaluation methodologies still applied to MT
metrics today well-founded? For example, were
the original methodology, by correlation with hu-
man assessment, to be applied, would a distinct
variant of ROUGE emerge as superior and subse-
quently lead to distinct system rankings? Sec-
ondly, were all variants of ROUGE to be included
in evaluations, would a variant originally omitted
from the evaluation emerge as superior and lead to
further differences in summarization system rank-
ings? Furthermore, although methods of statistical
significance testing are commonly applied to eval-
uation of summarization systems, attempts to iden-
tify significant differences in performance of met-
rics are extremely rare, and when they have been
applied unfortunately have not used an appropriate
test.
This motivates our review of past and current
methodologies applied to the evaluation of sum-
marization metrics. Since MT evaluation in gen-
eral has its own imperfections, we do not at-
tempt to indiscriminately impose all MT evalua-
tion methodologies on summarization, but specif-
ically revisit evaluation methodologies applied to
one particular area of summarization, evaluation
of metrics. Correlations with human assessment
reveal an extremely wide range in performance
among variants, highlighting the importance of an
optimal choice of ROUGE variant in system eval-
uations. Since distinct variants of ROUGE achieve
significantly stronger correlation with human as-
sessment than previous recommended best vari-
ants, we subsequently replicate a recent evaluation
of state-of-the-art summarization systems reveal-
ing distinct conclusions about the relative perfor-
mance of systems. In addition, we include in the
evaluation of metrics, an evaluation of BLEU for
the purpose of summarization evaluation, and con-
trary to common belief, precision-based BLEU is
on-par with recall-based ROUGE for evaluation of
summarization systems.
</bodyText>
<sectionHeader confidence="0.999769" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9995073">
When ROUGE (Lin and Hovy, 2003) was first pro-
posed, the methodology applied to its evaluation,
in one respect, was similar to that applied to met-
rics in MT, as ROUGE variants were evaluated
by correlation with a form of human assessment.
Where the evaluation methodology diverged from
MT, however, was with respect to the precise rep-
resentation of human assessment that was em-
ployed. In MT evaluation of metrics, although
experimentation has taken place with regards to
methods of elicitation of assessments from human
judges (Callison-Burch et al., 2008), human as-
sessment is always aimed to encapsulate the over-
all quality of translations. In contrast in summa-
rization, metrics are evaluated by the degree to
which metric scores correlate with human cover-
age scores for summaries, a recall-based formu-
lation of the number of peer summary units that
a human assessor believed had the same meaning
as model summaries. Substitution of overall qual-
ity assessments with a recall-based manual metric,
unfortunately has the potential to introduce bias
into the evaluation of metrics in favor of recall-
based formulations.
One dimension of summary quality omitted
from human coverage scores is, for example, the
order in which the units of a summary are ar-
ranged within the summary. Despite unit order
quite likely being something of importance to a
human assessor, assessment of metrics by correla-
tion with human coverage scores does not in any
respect take into account the order in which the
units of a summary appear, and evaluation by hu-
man coverage scores alone means that a summary
with its units scrambled or even reversed in the-
ory receives precisely the same metric score as the
original. Given current evaluation methodologies
for assessment of metrics, a metric that scores two
such summaries differently would be unfairly pe-
nalized for it. Furthermore, when the linguistic
quality of summaries has been assessed in parallel
with annotations used to compute human cover-
age scores, it has been shown that the two dimen-
sions of quality do not correlate with one another
(no significant correlation) (Pitler et al., 2010),
providing evidence that coverage scores alone do
not fully represent human judgment of the overall
quality of summaries.
Subsequent summarization metric evaluations
depart from correlation with human judgment fur-
</bodyText>
<page confidence="0.997877">
129
</page>
<bodyText confidence="0.9999364">
ther by evaluating metrics according to the abil-
ity of a metric/significance test combination to
identify a significant difference between the qual-
ity of human and system-generated summaries
(Rankel et al., 2011). Unfortunately, the evalua-
tion of metrics with respect to how well they dis-
tinguish between high-quality human summaries
and all system-generated summaries, does not pro-
vide insight into the task of metrics, to score better
quality system-generated summaries higher than
worse quality system-generated summaries, how-
ever. This is in contrast to evaluation of MT met-
rics by correlation with human judgment, where
metrics only receive credit for their ability to ap-
propriately score system-output documents rela-
tive to other system-output documents. Since dif-
ferences in quality levels between pairs of system-
generated summaries are likely to be far smaller
than differences in system and human-generated
summaries, the methodology unfortunately sets
too low a bar for summarization metrics to meet.
Furthermore, the approach to metric evaluation
unfortunately does not work in the long-term, as
the performance of summarization systems im-
proves and approaches or achieves the quality of
a human, a metric that accurately identifies this
achievement would be unfairly penalized for it.
Separate from the evaluation of metrics, Rankel et
al. (2011) make the highly important recommen-
dation of paired tests for identification of signifi-
cant differences in performance of summarization
systems. Since data used in the evaluation of sum-
marization systems is not independent, paired tests
are more appropriate and more powerful.
Owczarzak et al. (2012) diverge further from
correlation with human judgment for evaluation
of metrics by assessing the accuracy of metrics
to identify significant differences between pairs of
systems when combined with a significance test.
Although the approach to evaluation of metrics
provides insight into the accuracy of conclusions
drawn from metric/test combinations, the evalua-
tion is limited by inclusion of only six variants of
ROUGE, fewer than 4% of possible ROUGE vari-
ants. Despite such limitations, however, subse-
quent evaluations relied on recommended ROUGE
variants to rank state-of-the-art systems (Hong et
al., 2014).
Although methods of identifying significant dif-
ferences in performance are commonly applied to
the evaluation of systems in summarization, the
application of significance tests to the evaluation
of summarization metrics is extremely rare, and
when attempts have been made, unfortunately ap-
propriate tests have not been applied. Computa-
tion of confidence intervals for individual correla-
tion with human coverage scores, for example, un-
fortunately does not provide insight into whether
or not a difference in correlation with human cov-
erage scores is significant.
</bodyText>
<sectionHeader confidence="0.983065" genericHeader="method">
3 Summarization Metric Evaluation
</sectionHeader>
<bodyText confidence="0.999948133333333">
When large-scale human evaluation of summa-
rization systems takes place, human evaluation
commonly takes the form of annotation of whether
or not system-generated summary units express
the meaning of model summary units, annotations
subsequently used to compute human coverage
scores. In addition, an evaluation of the linguis-
tic quality of summaries is commonly carried out.
As described in Section 2, when used for the eval-
uation of metrics, linguistic quality is commonly
omitted, however, with metrics only assessed by
the degree to which they correlate with human
coverage scores. In contrast, we include all avail-
able human assessment data for evaluating met-
rics.
</bodyText>
<subsectionHeader confidence="0.999989">
3.1 Combining Quality and Coverage
</subsectionHeader>
<bodyText confidence="0.999847583333333">
In DUC-2004 (Over et al., 2007), human annota-
tions used to compute summary coverage are car-
ried out by identification of matching peer units
(PUs), the units in a peer summary that express
content of the corresponding model summary. In
addition, an overall coverage estimate (E) is pro-
vided by the human annotator, the proportion of
the corresponding model summary or collective
model units (MUs) expressed overall by a given
peer summary. Human coverage scores (CS) are
computed by combining Matching PUs with cov-
erage estimates as follows:
</bodyText>
<equation confidence="0.9946535">
CS = |Matching PUs |· E 1
|MUs |( )
</equation>
<bodyText confidence="0.972355625">
In addition to annotations used to compute human
coverage scores, human assessors were asked to
rate the linguistic quality of summaries under 7
different criteria, providing ratings from A to E,
with A denoting highest and E least quality rat-
ing.
Figure 1 is a scatter-plot of human coverage
scores and corresponding linguistic quality scores
</bodyText>
<page confidence="0.974951">
130
</page>
<figure confidence="0.9986555">
0 20 40 60 80 100
Coverage Score (%)
</figure>
<figureCaption confidence="0.985547666666667">
Figure 1: Scatter-plot of mean linguistic qual-
ity and coverage scores for human assessments of
summaries in DUC-2004
</figureCaption>
<figure confidence="0.855927">
Human Assessment
</figure>
<figureCaption confidence="0.746264">
Figure 2: Combining linguistic quality and cover-
age scores provided by human assessors in DUC-
2004
</figureCaption>
<figure confidence="0.999848">
0.0 0.2 0.4 0.6 0.8 1.0
Density
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Coverage (%)
Quality (%)
Mean
0 20 40 60 80 100
</figure>
<figureCaption confidence="0.545298">
Mean Ling. Quality (%)
</figureCaption>
<bodyText confidence="0.993770666666667">
for all human-assessed summaries from DUC-
2004, where, for the purpose of comparison, each
of the 7 linguistic quality ratings are converted to
a corresponding percentage quality (A= 100%;
B= 75%; C= 50%; D= 25%; E= 0%). The lo-
cation of all points almost exclusively within the
upper left corner of the plot in Figure 1 indicates
that the linguistic quality of almost all summaries
reaches at least as high a level as its corresponding
coverage score. This follows the intuition that a
summary is unlikely to obtain high coverage with-
out sufficient linguistic quality, while the same
cannot be said for the converse, that a high level
of linguistic quality necessarily leads to high cov-
erage. More importantly, however, linguistic qual-
ity scores provide an additional dimension of hu-
man assessment, allowing greater discriminatory
power between the quality of summaries than was
possible with coverage scores alone.
Figure 2 includes linguistic quality and cover-
age score distributions from DUC-2004 human
evaluation, where each distribution is skewed in
opposing directions, in addition to the distribution
of the average of the two scores for summaries.
For the purpose of metric evaluation, we com-
bine human coverage and linguistic quality scores
using the average of the two scores, and use this
as a gold standard human score for evaluation of
metrics:
Human Assessment Score = CS+MLQ
</bodyText>
<page confidence="0.926874">
2
</page>
<subsectionHeader confidence="0.984756">
3.2 ROUGE
</subsectionHeader>
<bodyText confidence="0.999966448275862">
ROUGE includes a large number of distinct vari-
ants, including eight choices of n-gram counting
method (ROUGE-1; 2; 3; 4; S4; SU4; W; L), binary
settings such as word-stemming of summaries and
an option to remove or retain stop-words. Addi-
tional configurations include the use of precision,
recall or f-score to compute individual summary
scores. Finally, options for computation of the
overall score for a system is by computation of the
mean or median of that system’s summary score
distribution. In total, therefore, when employing
ROUGE for the evaluation of summarization sys-
tems, there are 192 (8 x 2 x 2 x 3 x 2) possible
system-level variants to choose from.
The fact that final overall ROUGE scores for
systems are comprised of the mean or median
of ROUGE scores of individual summaries, is,
again, a divergence from MT evaluation, as n-
gram counts used to compute BLEU scores are
computed at the document as opposed to sentence-
level. However, in this respect, ROUGE has a dis-
tinct advantage over BLEU, as the fact that ROUGE
comprises the mean or median score of individ-
ual summary scores makes possible the applica-
tion of standard methods of significance testing
differences in system-level ROUGE scores, while
BLEU is restricted to the application of random-
ized methods (Koehn, 2004; Graham et al., 2014).
For this purpose, differences in median ROUGE
</bodyText>
<page confidence="0.992174">
131
</page>
<bodyText confidence="0.98552525">
scores can be tested for statistical significance us-
ing, for example, Wilcoxon signed-rank test, while
paired t-test can be applied to difference of mean
ROUGE scores for systems.
</bodyText>
<subsectionHeader confidence="0.996467">
3.3 Metric Evaluation by Pearson’s r
</subsectionHeader>
<bodyText confidence="0.999973277777778">
Moses (Koehn et al., 2007) multi-bleu1 was used
to compute BLEU (Papineni et al., 2002) scores
for summaries and prepare4rouge2 applied to sum-
maries before running ROUGE (Lin and Hovy,
2003). Table 1 shows the Pearson correlation of
each variant of ROUGE with human assessment, in
addition to BLEU’s correlation with the same hu-
man assessment of summaries from DUC-2004.
Somewhat surprisingly, BLEU MT evaluation met-
ric achieves strongest correlation with human as-
sessment overall, r = 0.797, with performance
of ROUGE variants ranging from r = 0.786, just
below that of BLEU, to as low as r = 0.293.
For many pairs of metrics, differences in correla-
tion with human judgment are small, however, and
prior to concluding superiority in performance of
one metric over another, significance tests should
be applied.
</bodyText>
<sectionHeader confidence="0.992911" genericHeader="method">
4 Metric Significance Testing
</sectionHeader>
<bodyText confidence="0.999800727272728">
In MT, recent work has identified the suitabil-
ity of Williams significance test (Williams, 1959)
for evaluation of automatic MT metrics (Graham
and Baldwin, 2014; Graham et al., 2015; Gra-
ham, 2015), and, for similar reasons, Williams
test is suited to significance testing differences
in performance of competing summarization met-
rics which we detail further below. Williams test
has additionally been used in evaluation of sys-
tems that automatically assess spoken and writ-
ten language quality (Yannakoudakis et al., 2011;
Yannakoudakis and Briscoe, 2012; Evanini et al.,
2013).
Evaluation of a given summarization metric,
Mnew, by Pearson correlation takes the form of
quantifying the correlation, r(Mnew, H), that ex-
ists between metric scores for systems and corre-
sponding human assessment scores, and contrast-
ing this correlation with the correlation for some
baseline metric, r(Mbase, H).
One approach to testing for significance that
may seem reasonable is to apply a significance test
</bodyText>
<footnote confidence="0.99352125">
1https://github.com/moses-smt/mosesdecoder/
commits/master/scripts/generic/multi-bleu.perl
2http://kavita-ganesan.com/content/
prepare4rouge-script-prepare-rouge-evaluation
</footnote>
<bodyText confidence="0.999898842105263">
separately to the correlation of each metric with
human assessment, with the hope that the new
metric will achieve a significant correlation where
the baseline metric does not. The reasoning here
is flawed however: the fact that one correlation is
significantly higher than zero (r(Mnew, H)) and
that of another is not, does not necessarily mean
that the difference between the two correlations is
significant. Instead, a specific test should be ap-
plied to the difference in correlations. For this
same reason, confidence intervals for individual
correlations with human assessment are also not
useful.
If samples that data are drawn from are inde-
pendent, and differences in correlations are com-
puted on independent data sets, the Fisher r to z
transformation is applied to test for significant dif-
ferences in correlations. Data used for the eval-
uation of summarization metrics are not indepen-
dent, as evaluations comprise three sets of scores
for precisely the same set of summaries (corre-
sponding to variables X1, X2 and X3 below),
and subsequently three correlations: r(Mbase, H),
r(Mnew, H) and r(Mnew, Mbase). If r(Mbase, H)
and r(Mnew, H) are both &gt; 0, then the third
correlation, between metric scores themselves,
r(Mbase, Mnew), must also be &gt; 0. The strength
of this correlation, directly between scores of
pairs of summarization metrics, should be taken
into account using a significance test of the dif-
ference in correlation between r(Mbase, H) and
r(Mnew, H).
Williams test 3 (Williams, 1959) evaluates the
significance of a difference in dependent correla-
tions (Steiger, 1980). It is formulated as follows
as a test of whether the population correlation be-
tween X1 and X3 equals the population correla-
tion between X2 and X3:
</bodyText>
<equation confidence="0.9990975">
/2K (n−1) + (r23+r13)2 (1 − r12)3
v (n−3) 4
</equation>
<bodyText confidence="0.9968805">
where rij is the correlation between Xi and Xj, n
is the size of the population, and:
</bodyText>
<equation confidence="0.996563">
K = 1 − r122 − r132 − r232 + 2r12r13r23
</equation>
<bodyText confidence="0.936936">
Since the power of Williams test increases when
the third correlation, r(Mbase, Mnew), between
metric scores is stronger, metrics should not be
ranked by the number of competing metrics they
</bodyText>
<equation confidence="0.879264">
3Also known as Hotelling-Williams.
t(n 3)
(r13 − r23) Y (n − 1)(1 + r12)
— =
,
</equation>
<page confidence="0.980242">
132
</page>
<figure confidence="0.860816214285714">
temming
Ste
SW
Ave./Med
Av
/R/F
temming
Ste
SW
Ave./Med
Av
/R/F
temming
SW
Ave./Med
Ste
Av
/R/F
P/R
Metric r
RS
P/R
Metric r
RS
P/R
Metric r
RS
BLEU
</figure>
<equation confidence="0.99766365625">
R-2 Y Y A
R-3 N N A
R-2 N Y A
R-3 N Y A
R-3 Y N A
R-3 N N A
R-4 N N A
R-3 N N A
R-3 Y N A
R-2 N Y A
R-4 N N A
R-2 Y Y A
R-3 Y N A
R-3 N N M
R-3 N Y A
R-3 Y Y A
R-4 Y N A
R-4 N N A
R-4 Y N A
R-3 N N M
R-4 Y Y A
R-2 Y N A
R-4 N Y A
R-2 N N A
R-3 N N M
R-4 Y N A
R-3 Y Y A
R-2 N N A
R-2 Y N A
R-3 N Y A
R-3 Y N M
R-2 N Y M
R-3 Y N M
R-2 N Y A
R-2 Y Y M
R-2 N Y M
R-3 Y N M
R-2 Y Y A
R-2 Y Y M
R-2 N N M
R-3 Y Y M
R-3 Y Y A
R-4 Y Y A
R-3 Y Y M
R-S4 Y N A
R-SU4 N N A
R-2 Y N M
R-S4 N Y A
R-SU4 Y N A
R-S4 N N A
R-2 N Y M
R-4 N Y A
R-1 N N A
R-2 N N M
R-SU4 N Y A
R-1 Y N A
R-2 Y Y M
R-3 Y Y M
R-4 Y Y A
R-S4 Y Y A
R-SU4 Y Y A
R-2 N N A
R-W N Y A
R-2 Y N A
</equation>
<table confidence="0.943359723076923">
0.797 •
P 0.786 •
F 0.785 •
P 0.783 •
P 0.781 •
F 0.779 •
R 0.777 •
F 0.771 •
P 0.771 •
R 0.770 •
F 0.769 •
R 0.768 •
F 0.768 •
P 0.767 •
F 0.766 •
F 0.764 •
P 0.764 •
F 0.763 •
P 0.762 •
R 0.761 •
P 0.760 •
P 0.759 •
P 0.759 •
P 0.758 •
P 0.757 •
R 0.753 •
P 0.752 •
F 0.748 •
F 0.747 •
F 0.747 •
R 0.746 •
P 0.744 •
P 0.743 •
F 0.743 •
R 0.742 •
P 0.741 •
F 0.740 •
R 0.739 •
R 0.737 •
F 0.735 •
P 0.734 •
P 0.733 •
R 0.730
F 0.729 •
F 0.726 •
P 0.725 •
P 0.724 •
P 0.724
P 0.724
P 0.723 •
P 0.723 •
R 0.722 •
F 0.721 •
P 0.720 •
F 0.719 •
P 0.719
P 0.714 •
R 0.714 •
R 0.713 •
R 0.712 •
P 0.711
P 0.710
R 0.710 •
P 0.709 •
R 0.707 •
</table>
<equation confidence="0.866337602409638">
R-2 Y N M F 0.706
R-3 N Y M P 0.704 •
R-1 N Y A P 0.704 •
R-4 N N M R 0.703 •
R-L N Y A P 0.700 •
R-W Y Y A P 0.700 •
R-4 N Y A R 0.700 •
R-1 Y N M P 0.699 •
R-S4 N Y M P 0.698
R-1 Y Y A P 0.698 •
R-3 N Y M F 0.697 •
R-W N N A P 0.696 •
R-W Y N A P 0.695 •
R-4 N N M F 0.695 •
R-S4 N Y M F 0.693
R-S4 N Y A F 0.691
R-SU4 N Y M P 0.690
R-1 N N M P 0.690 •
R-2 N N M R 0.689
R-L Y Y A P 0.688 •
R-3 N Y M R 0.687 •
R-S4 N N M P 0.687
R-S4 Y N A F 0.687
R-S4 N N A F 0.687
R-4 N N M P 0.687 •
R-L N N A P 0.686 •
R-SU4 N N M P 0.686
R-L Y N A P 0.683 •
R-W N N M P 0.682 •
R-W Y N M P 0.680 •
R-SU4 Y N M P 0.678
R-SU4 N Y A F 0.678
R-S4 Y Y A F 0.676
R-SU4 N Y M F 0.676
R-SU4 N N A F 0.673
R-1 N Y M P 0.673
R-2 Y N M R 0.672
R-SU4 Y N A F 0.671
R-S4 N Y M R 0.670
R-S4 Y N M P 0.670
R-SU4 Y Y A F 0.668
R-S4 N N M F 0.666
R-W N Y M P 0.664
R-S4 Y Y M P 0.664
R-SU4 Y Y M P 0.663
R-L N N M P 0.661 •
R-SU4 N N M F 0.658
R-1 N Y A F 0.656
R-W Y Y M P 0.656
R-S4 N Y A R 0.656
R-L Y N M P 0.656 •
R-W N Y A F 0.655
R-1 N Y M F 0.653
R-L N Y A F 0.652
R-1 Y Y M P 0.651
R-S4 Y Y M F 0.649
R-1 Y Y A F 0.649
R-SU4 Y Y M F 0.649
R-SU4 N Y M R 0.646
R-L N Y M P 0.645
R-W N Y M F 0.642
R-W Y Y A F 0.642
R-4 Y N M R 0.641
R-S4 Y Y A R 0.641
R-4 Y N M F 0.639
R-L Y Y A F 0.638
R-1 N N A F 0.637
R-S4 Y N M F 0.634
R-4 Y N M P 0.634
R-1 N N M F 0.634
R-SU4 N Y A R 0.633
R-L Y Y M P 0.633
R-SU4 Y Y M R 0.631
R-1 Y N A F 0.630
R-1 Y Y M F 0.629
R-S4 Y Y M R 0.626
R-S4 N N A R 0.626
R-SU4 Y N M F 0.625
R-S4 Y N A R 0.624
R-L N Y M F 0.623
R-SU4 Y Y A R 0.622
R-1 Y N M F 0.617
R-1 N Y M R 0.615
</equation>
<construct confidence="0.981529333333333">
R-W N Y A R 0.613
R-S4 N N M R 0.611
R-L N Y M R 0.609
R-1 N Y A R 0.604
R-L N Y A R 0.601
R-W N N M F 0.600
R-L N N M F 0.599
R-W Y Y A R 0.598
R-W N Y M R 0.597
R-1 Y Y A R 0.595
R-1 Y Y M R 0.591
R-L N N A F 0.586
R-W Y Y M F 0.586
R-W Y N M F 0.585
R-L Y Y A R 0.583
R-L Y Y M F 0.582
R-L Y Y M R 0.579
R-L Y N A F 0.579
R-W N N A F 0.579
R-SU4 N N M R 0.576
R-W Y N A F 0.576
R-SU4 N N A R 0.574
R-SU4 Y N A R 0.571
R-L Y N M F 0.569
R-W Y Y M R 0.567
R-S4 Y N M R 0.566
R-SU4 Y N M R 0.525
R-1 N N M R 0.488
R-1 Y N M R 0.477
R-W Y N M R 0.477
R-1 N N A R 0.470
R-W N N M R 0.470
R-L N N M R 0.470
R-1 Y N A R 0.459
R-W N N A R 0.456
R-W Y N A R 0.452
R-L Y N M R 0.423
R-L N N A R 0.416
R-L Y N A R 0.406
</construct>
<equation confidence="0.5488395">
R-4 Y Y M P 0.307
R-4 Y Y M F 0.302
R-4 N Y M P 0.301
R-4 Y Y M R 0.297
R-4 N Y M F 0.296
R-4 N Y M R 0.293
</equation>
<tableCaption confidence="0.963056">
Table 1: Pearson correlation (r) of BLEU and 192 variants of ROUGE (R-*) with human assessment in
</tableCaption>
<bodyText confidence="0.9465315">
DUC-2004, with (Y) and without (N) stemming, with (Y) and without (N) removal of stop words (RSW),
aggregated at the summary level using precision (P), recall (R) or f-score (F), aggregated at the system
level by average (A) or median (M) summary score, correlations marked with • signify a metric/variant
whose correlation with human assessment is not significantly weaker than that of any other metric/variant
(an optimal variant) according to pairwise Williams significance tests, variants employed in Hong et al.
(2014) are in bold.
</bodyText>
<page confidence="0.998022">
133
</page>
<bodyText confidence="0.998210538461539">
outperform, as a metric that happens to correlate
strongly with a higher number of competing met-
rics in a given competition would be at an un-
fair advantage. This increased power also means,
somewhat counter-intuitively, it can happen for a
pair of competing metrics for which the correla-
tion between metric scores is strong, that a small
difference in competing correlations with human
assessment is significant, while, for a different
pair of metrics with a larger difference in corre-
lation, the difference is not significant, because
r(Mbase, Mnew) is weak. For example, in Ta-
ble 1 the difference in correlation with human as-
sessment of BLEU and that of median ROUGE-L
precision with stemming and stop-words retained,
0.141 (0.797 − 0.656), is not significant, while
the smaller difference in correlation with human
assessment between correlations of BLEU and av-
erage ROUGE-3 recall with stemming and stop-
words removed, 0.067 (0.797 − 0.73) is signifi-
cant, since scores of the latter pair of metrics cor-
relate with one another with more strength.
As part of this research, we have made avail-
able an open-source implementation of statistical
tests for evaluation of summarization metrics, at
https://github.com/ygraham/nlp-williams.
</bodyText>
<subsectionHeader confidence="0.997139">
4.1 Significance Test Results
</subsectionHeader>
<bodyText confidence="0.980861833333333">
In Table 1, • identifies variants of ROUGE not sig-
nificantly outperformed by any other variant. Fig-
ure 3 shows pairwise Williams significance test
outcomes for BLEU, the top ten ROUGE variants,
as well as current recommended ROUGE variants
(Owczarzak et al. (2012)) used to compare sys-
tems in Hong et al. (2014). Current recommended
best variants of ROUGE are shown to be signifi-
cantly outperformed by several other ROUGE vari-
ants.
Although BLEU achieves strongest correlation
with human assessment overall, Figure 3 reveals
the difference between BLEU’s correlation with
human assessment and that of the best-performing
ROUGE variant as not statistically significant, and
since ROUGE holds the distinct advantage over
BLEU of facilitating standard methods of signif-
icance testing differences in scores for systems,
for this reason alone we recommend the use of the
best-performing ROUGE variant over BLEU, aver-
age ROUGE-2 precision with stemming and stop-
words removed.
Table 2 shows proportions of optimal ROUGE
Figure 3: Pairwise significance test outcomes for
BLEU, best-performing ROUGE (rows 2-9), and
ROUGE applied in Hong et al. (2014) (bottom 3
rows), with (ST1) and without (ST0) stemming,
with (RS1) and without (RS0) removal of stop
words, for average (A) or median (M) ROUGE pre-
cision (P), recall (R) or f-score (F), colored cells
denote significant win for row i metric over col-
umn j metric with Williams test.
variants that can be attributed to each of ROUGE’s
configuration options. Contrary to prior belief,
the vast majority of optimal ROUGE variants are
precision-based, showing that the assumption that
recall-based metrics are superior for evaluation
of summarization systems to be inaccurate, and
a likely presence of bias in favor of recall-based
metrics in evaluations by correlation with human
coverage scores alone. Furthermore, since there
exists a vast number of possible formulations that
could potentially be applied to evaluation of sum-
maries that are neither purely precision nor recall-
based, evaluation methodologies should avoid re-
liance on assumptions that either precision or re-
call is superior and instead base conclusions on
empirical evidence where possible.
</bodyText>
<sectionHeader confidence="0.929817" genericHeader="method">
5 Summarization System Evaluation
</sectionHeader>
<bodyText confidence="0.999840571428572">
Since we have established that the variants of
ROUGE used to rank state-of-the-art and baseline
summarization systems in Hong et al. (2014) have
significantly weaker correlations with human as-
sessment than several other ROUGE variants, this
motivates our replication of the evaluation. We
evaluate systems using the variant of ROUGE that
</bodyText>
<figure confidence="0.981940052631579">
BLEU
R2_ST1_RS1_A_P
R3_ST0_RS0_A_F
R2_ST0_RS1_A_P
R3_ST0_RS1_A_P
R3_ST1_RS0_A_F
R3_ST0_RS0_A_R
R4_ST0_RS0_A_F
R3_ST0_RS0_A_P
R4_ST1_RS0_M_R
R2_ST1_RS0_M_R
R1_ST1_RS0_M_R
BLEU
R2_ST1_RS1_A_P
R3_ST0_RSW0_A_F
R2_ST0_RS1_A_P
R3_ST0_RS1_A_P
R3_ST1_RS0_A_F
R3_ST0_RS0_A_R
R4_ST0_RS0_A_F
R3_ST0_RS0_A_P
R4_ST1_RS0_M_R
R2_ST1_RS0_M_R
R1_ST1_RS0_M_R
134
DPP
ICSISumm
RegSum
Submodular
CLASSY11
CLASSY04
OCCAMSV
Greedy_KL
FreqSum
TsSum
Centroid
LexRank
N-gram Count
</figure>
<table confidence="0.89454175">
Not Stemmed 53.8
Stemmed 46.2
Stop-words
Not Rem. 56.2
Removed 43.8
System-level Agg.
Average 63.7
Median 36.3
</table>
<tableCaption confidence="0.9967145">
Table 2: Proportions of optimal ROUGE variants
attributed to each ROUGE configuration option
</tableCaption>
<table confidence="0.999904666666667">
(%).
System ROUGE ROUGE
Best Original
DPP 8.498 9.62
ICSISumm 8.317 9.78
RegSum 8.187 9.75
Submodular 8.047 9.35
CLASSY11 7.717 9.20
CLASSY04 7.690 8.96
OCCAMS V 7.643 9.76
GreedyKL 6.918 8.53
FreqSum 6.838 8.11
TsSum 6.671 8.15
Centroid 6.660 7.97
LexRank 6.655 7.47
</table>
<tableCaption confidence="0.99919">
Table 3: Summarization systems originally in-
</tableCaption>
<bodyText confidence="0.9958884">
cluded in Hong et al. (2014) evaluated with the
best-performing ROUGE variant (Best): average
ROUGE-2 precision with stemming and stop words
removed; and evaluated with original suboptimal
variant (median ROUGE-2 recall with stemming
and without removal of stop-words)
achieves strongest correlation with human assess-
ment, average ROUGE-2 precision with stemming
and stop-words removed.
Table 3 shows ROUGE scores for summarization
systems originally presented in Hong et al. (2014).
System rankings diverge considerably from those
of the original evaluation. Notably, the system
now taking first place had originally ranked in
fourth position.
Since the best variant of ROUGE is based on av-
erage ROUGE scores as opposed to median ROUGE
scores, a difference of means significance test is
appropriate provided the normality assumption of
score distributions for systems is not violated. In
</bodyText>
<figure confidence="0.997407416666666">
DPP
ICSISumm
RegSum
Submodular
CLASSY11
CLASSY04
OCCAMS_V
GreedyKL
FreqSum
TsSum
Centroid
LexRank
</figure>
<figureCaption confidence="0.999676">
Figure 4: Summarization system pairwise signif-
</figureCaption>
<bodyText confidence="0.977607363636364">
icance test outcomes (paired t-test) for state-of-
the-art (top 7 rows) and baseline systems (bot-
tom 5 rows) of Hong et al. (2014) evaluated with
best-performing ROUGE variant: average ROUGE-
2 precision with stemming and stop words re-
moved, colored cells denote a significant greater
mean score for row i system over column j sys-
tem according to paired t-test.
addition, since data used to evaluate systems are
not independent, paired tests are also appropri-
ate (Rankel et al., 2011). ROUGE score distri-
butions for systems were tested for normality us-
ing the Shapiro-Wilk test (Royston, 1982) where
score distributions for none of the included sys-
tems were shown to be significantly non-normal.
Figure 4 shows outcomes of paired t-tests for
summary score distributions of each pair of sys-
tems, revealing three summarization systems not
significantly outperformed by any other as DPP,
ICSISUMM and REGSUM. In addition, as ex-
pected, all state-of-the-art systems significantly
outperform all baseline systems.
</bodyText>
<sectionHeader confidence="0.997752" genericHeader="method">
6 Human Assessment Combinations
</sectionHeader>
<bodyText confidence="0.999981888888889">
In order to evaluate metrics by correlation with hu-
man assessment, it is necessary to obtain a single
human evaluation score per system. For example,
in the evaluation of metrics in Section 3, we com-
bined linguistic quality and coverage into a sin-
gle score using the mean of the two scores. Other
combinations are of course possible, but without
any additional human evaluation data, it is chal-
lenging to identify the combination that best rep-
</bodyText>
<figure confidence="0.987602076923077">
Stemming
R-3 28.7
R-2 25.0
R-4 18.8
R-1 7.5
R-L 7.5
R-W 7.5
R-S4 2.5
R-SU4 2.5
Summary-level Agg.
Prec. 52.5
F-score 25.0
Recall 22.5
</figure>
<page confidence="0.988884">
135
</page>
<table confidence="0.999838703703704">
Metric Stem. RSW Ave/Med P/R/F Mean Geometric Harmonic Coverage Ling. Qual.
Mean Mean Only Only
BLEU 0.797• 0.901• 0.936• 0.944• 0.642•
ROUGE-2 Y Y A P 0.786• 0.870• 0.887• 0.878 0.660•
ROUGE-3 N N A F 0.785• 0.869• 0.893 0.894 0.650•
ROUGE-2 N Y A P 0.783• 0.868• 0.885• 0.876 0.658•
ROUGE-3 N Y A P 0.781• 0.836• 0.840 0.826 0.682•
ROUGE-3 Y N A F 0.779• 0.866• 0.891 0.893 0.643•
ROUGE-3 N N A R 0.777• 0.871• 0.901 0.907 0.632•
ROUGE-4 N N A F 0.771• 0.843• 0.863 0.866 0.645•
ROUGE-3 N N A P 0.771• 0.837• 0.849 0.843 0.658•
ROUGE-3 Y N A R 0.770• 0.867• 0.899 0.905 0.624•
ROUGE-2 N Y A F 0.769• 0.877• 0.909• 0.910• 0.619•
ROUGE-2 Y Y A F 0.768• 0.875• 0.908• 0.908• 0.618•
ROUGE-3 Y N A P 0.767• 0.835• 0.849 0.843 0.652•
ROUGE-3 Y Y A P 0.764• 0.825• 0.832 0.821 0.660•
ROUGE-4 N N A P 0.762• 0.815• 0.824 0.819 0.657•
ROUGE-4 Y Y A P 0.759• 0.794 0.790 0.774 0.678•
ROUGE-4 N Y A P 0.758• 0.793 0.789 0.772 0.678•
ROUGE-4 Y N A P 0.752• 0.809 0.819 0.815 0.646•
ROUGE-2 N N A F 0.747• 0.867• 0.907• 0.910• 0.587•
ROUGE-2 Y N A F 0.747• 0.868• 0.908• 0.912• 0.586•
ROUGE-2 N Y A R 0.742• 0.862• 0.904• 0.912• 0.578•
ROUGE-2 N Y M F 0.740• 0.855• 0.894• 0.898• 0.584•
ROUGE-2 Y Y A R 0.737• 0.858• 0.900• 0.908• 0.575•
ROUGE-2 N Y M R 0.722• 0.848• 0.895• 0.905• 0.553•
ROUGE-2 N N M R 0.689 0.828 0.884• 0.901• 0.508
</table>
<tableCaption confidence="0.993389">
Table 4: Correlation of top-ten metric variants for each alternate combination of linguistic quality and
</tableCaption>
<bodyText confidence="0.998004052631579">
coverage, • denotes a metric not significantly outperformed by any other under that particular human
evaluation combination, highest correlations highlighted in bold font.
resents an overall human assessment for a given
summary. One possibility would be to search for
optimal weights for combining quality and cover-
age, but there is a risk with this approach that we
will not find the most representative combination
but simply the combination that best describes the
metrics.
An additional variation of human assessment
scores is by combining coverage and quality with
a variant of the arithmetic mean, such as the har-
monic or geometric mean. Table 4 shows correla-
tions of BLEU and the top ten performing variants
of ROUGE when evaluated against the arithmetic
(mean), harmonic and geometric mean of quality
and coverage scores for summaries. In addition,
Table 4 includes correlations of metric scores with
coverage alone, as well as linguistic quality scores
alone to provide additional insight, although lin-
guistic quality scores alone do not provide a suffi-
cient evaluation of metrics – since it is possible to
generate summaries with perfect linguistic quality
without inclusion of any relevant content whatso-
ever.
BLEU MT metric achieves highest correlation
across all human evaluation combinations and
highest again when evaluated against human cov-
erage scores alone, and BLEU’s brevity penalty,
that like recall penalizes a system for too short out-
put, is a probable cause of the metric overcom-
ing the recall-based bias of an evaluation based
on coverage scores alone. In addition, our rec-
ommended variant, ave. ROUGE-2 prec. with
stemming and stop words removed is not signif-
icantly outperformed by BLEU or any other vari-
ant of ROUGE for any of the three combined mean
human assessment scores.
</bodyText>
<sectionHeader confidence="0.999701" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999955769230769">
An analysis of evaluation of summarization met-
rics was provided with an evaluation of BLEU and
192 variants of ROUGE. Detail of the first suitable
summarization metric significance test, Williams
test, was provided. Results reveal superior vari-
ants of metrics distinct from previously best rec-
ommendations. Replication of a recent evalua-
tion of state-of-the-art summarization systems also
revealed contrasting conclusions about the rela-
tive performance of systems. In addition, BLEU
achieves strongest correlation with human assess-
ment overall, but does not significantly outperform
the best-performing ROUGE variant.
</bodyText>
<sectionHeader confidence="0.996919" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9930256">
We wish to thank the anonymous reviewers.
This research is supported by Science Foun-
dation Ireland through the CNGL Programme
(Grant 12/CE/I2267) in the ADAPT Centre (www.
adaptcentre.ie) at Trinity College Dublin.
</bodyText>
<page confidence="0.998446">
136
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926234234234">
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proc. 3rd Wkshp. Statistical Machine Translation,
pages 70–106, Columbus, Ohio. Association for
Computational Linguistics.
Keelan Evanini, Shasha Xie, and Klaus Zechner. 2013.
Prompt-based content scoring for automated spoken
language assessment. In Proc. 2013 Conf. North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 157–162, Atlanta, GA. Association for Com-
putational Linguistics.
Yvette Graham and Timothy Baldwin. 2014. Testing
for significance of increased correlation with human
judgment. In Proc. 2014 Conf. Empirical Methods
in Natural Language Processing, pages 172–176,
Doha, Qatar. Association for Computational Lin-
guistics.
Yvette Graham, Nitika Mathur, and Timothy Baldwin.
2014. Randomized significance tests in machine
translation. In Proc. 9th Wkshp. Statistical Machine
Translation, pages 266–274, Baltimore, MD. Asso-
ciation for Computational Linguistics.
Yvette Graham, Nitika Mathur, and Timothy Baldwin.
2015. Accurate evaluation of segment-level ma-
chine translation metrics. In Proc. 2015 Conf. North
American Chapter of the Association for Compu-
tational Linguistics - Human Language Technolo-
gies, pages 1183–1191, Denver, CO. Association for
Computational Linguistics.
Yvette Graham. 2015. Improving evaluation of ma-
chine translation quality estimation. In Proc. Fifty-
Third Annual Meeting of the Association for Com-
putational Linguistics, pages 1804–1813, Beijing,
China. Association for Computational Linguistics.
Kai Hong, John M Conroy, Benoit Favre, Alex
Kulesza, Hui Lin, and Ani Nenkova. 2014. A repos-
itory of state of the art and competitive baseline sum-
maries for generic news summarization. In Proc. 9th
edition of the Language Resources and Evaluation
Conference, pages 1608–1616, Reykjavik, Iceland.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proc. 45th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 177–180,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. 2004 Conf.
Empirical Methods in Natural Language Process-
ing, pages 388–395, Barcelona, Spain. Association
for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proc. 2003 Conf. North
American Chapter of the Association for Compu-
tational Linguistics on Human Language Technol-
ogy, pages 71–78, Edmonton, Canada. Association
for Computational Linguistics.
Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing &amp; Management,
43(6):1506–1520.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment of
the accuracy of automatic evaluation in summariza-
tion. In Proc. Wkshp. on Evaluation Metrics and
System Comparison for Automatic Summarization,
pages 1–9, Quebec, Canada. Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proc. 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, PA. Asso-
ciation for Computational Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proc. 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 544–554, Uppsala, Sweden. Associ-
ation for Computational Linguistics.
Peter Rankel, John M. Conroy, Eric V. Slud, and Di-
anne P. O’Leary. 2011. Ranking human and ma-
chine summarization systems. In Proc. 2011 Conf.
Empirical Methods in Natural Language Process-
ing, pages 467–473, Edinburgh, Scotland. Associ-
ation for Computational Linguistics.
Patrick Royston. 1982. Algorithm as 181: The W test
for normality. Applied Statistics, 31:176–180.
James H. Steiger. 1980. Tests for comparing ele-
ments of a correlation matrix. Psychological Bul-
letin, 87(2):245–251.
Evan J. Williams. 1959. Regression analysis, vol-
ume 14. Wiley New York.
Helen Yannakoudakis and Ted Briscoe. 2012. Model-
ing coherence in ESOL learner texts. In Proc. Sev-
enth Wkshp. on Building Educational Applications
Using NLP, pages 33–43, Montreal, Canada. Asso-
ciation for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proc. 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 180–
189, Portland, OR. Association for Computational
Linguistics.
</reference>
<page confidence="0.997812">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.466499">
<title confidence="0.9915425">Automatic Summarization with Shades of</title>
<author confidence="0.578222">Yvette</author>
<affiliation confidence="0.849468333333333">ADAPT School of Computer Science and Trinity College Dublin</affiliation>
<email confidence="0.99952">graham.yvette@gmail.com</email>
<abstract confidence="0.999905965517241">We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern: (1) movement away from evaluation by correlation with human assessment; (2) omission of important components of human assessment from evaluations, in addition to large numbers of metric variants; (3) absence of methods of significance testing improvements over a baseline. We outline an evaluation methodology that overcomes all such challenges, providing the first method of significance testing suitable for evaluation of summarization metrics. Our evaluation reveals for the first time which metric variants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well machine translation metric performance on-par with the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal revealing distinct conclusions about the relative performance of state-of-the-art summarization systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proc. 3rd Wkshp. Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6546" citStr="Callison-Burch et al., 2008" startWordPosition="977" endWordPosition="980">call-based ROUGE for evaluation of summarization systems. 2 Related Work When ROUGE (Lin and Hovy, 2003) was first proposed, the methodology applied to its evaluation, in one respect, was similar to that applied to metrics in MT, as ROUGE variants were evaluated by correlation with a form of human assessment. Where the evaluation methodology diverged from MT, however, was with respect to the precise representation of human assessment that was employed. In MT evaluation of metrics, although experimentation has taken place with regards to methods of elicitation of assessments from human judges (Callison-Burch et al., 2008), human assessment is always aimed to encapsulate the overall quality of translations. In contrast in summarization, metrics are evaluated by the degree to which metric scores correlate with human coverage scores for summaries, a recall-based formulation of the number of peer summary units that a human assessor believed had the same meaning as model summaries. Substitution of overall quality assessments with a recall-based manual metric, unfortunately has the potential to introduce bias into the evaluation of metrics in favor of recallbased formulations. One dimension of summary quality omitte</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proc. 3rd Wkshp. Statistical Machine Translation, pages 70–106, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keelan Evanini</author>
<author>Shasha Xie</author>
<author>Klaus Zechner</author>
</authors>
<title>Prompt-based content scoring for automated spoken language assessment.</title>
<date>2013</date>
<booktitle>In Proc. 2013 Conf. North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>157--162</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, GA.</location>
<contexts>
<context position="17632" citStr="Evanini et al., 2013" startWordPosition="2739" endWordPosition="2742">d be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew, by Pearson correlation takes the form of quantifying the correlation, r(Mnew, H), that exists between metric scores for systems and corresponding human assessment scores, and contrasting this correlation with the correlation for some baseline metric, r(Mbase, H). One approach to testing for significance that may seem reasonable is to apply a significance test 1https://github.com/moses-smt/mosesdecoder/ commits/master/scripts/generic/multi-bleu.perl 2http://kavita-ganesan.com/content/ prepare4rouge-script-prepare-rouge-evaluation separately to</context>
</contexts>
<marker>Evanini, Xie, Zechner, 2013</marker>
<rawString>Keelan Evanini, Shasha Xie, and Klaus Zechner. 2013. Prompt-based content scoring for automated spoken language assessment. In Proc. 2013 Conf. North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 157–162, Atlanta, GA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Timothy Baldwin</author>
</authors>
<title>Testing for significance of increased correlation with human judgment.</title>
<date>2014</date>
<booktitle>In Proc. 2014 Conf. Empirical Methods in Natural Language Processing,</booktitle>
<pages>172--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar.</location>
<contexts>
<context position="17216" citStr="Graham and Baldwin, 2014" startWordPosition="2676" endWordPosition="2679"> surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew, by Pearson correlation takes the form of quantifying the correlation, r(Mnew, H), that exists between metric scores for systems and </context>
</contexts>
<marker>Graham, Baldwin, 2014</marker>
<rawString>Yvette Graham and Timothy Baldwin. 2014. Testing for significance of increased correlation with human judgment. In Proc. 2014 Conf. Empirical Methods in Natural Language Processing, pages 172–176, Doha, Qatar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Nitika Mathur</author>
<author>Timothy Baldwin</author>
</authors>
<title>Randomized significance tests in machine translation.</title>
<date>2014</date>
<booktitle>In Proc. 9th Wkshp. Statistical Machine Translation,</booktitle>
<pages>266--274</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, MD.</location>
<contexts>
<context position="15944" citStr="Graham et al., 2014" startWordPosition="2473" endWordPosition="2476">ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries, is, again, a divergence from MT evaluation, as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel. However, in this respect, ROUGE has a distinct advantage over BLEU, as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004; Graham et al., 2014). For this purpose, differences in median ROUGE 131 scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems. 3.3 Metric Evaluation by Pearson’s r Moses (Koehn et al., 2007) multi-bleu1 was used to compute BLEU (Papineni et al., 2002) scores for summaries and prepare4rouge2 applied to summaries before running ROUGE (Lin and Hovy, 2003). Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU’s correlation with the same human</context>
</contexts>
<marker>Graham, Mathur, Baldwin, 2014</marker>
<rawString>Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2014. Randomized significance tests in machine translation. In Proc. 9th Wkshp. Statistical Machine Translation, pages 266–274, Baltimore, MD. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Nitika Mathur</author>
<author>Timothy Baldwin</author>
</authors>
<title>Accurate evaluation of segment-level machine translation metrics.</title>
<date>2015</date>
<booktitle>In Proc. 2015 Conf. North American Chapter of the Association for Computational Linguistics - Human Language Technologies,</booktitle>
<pages>1183--1191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, CO.</location>
<contexts>
<context position="17237" citStr="Graham et al., 2015" startWordPosition="2680" endWordPosition="2683">luation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew, by Pearson correlation takes the form of quantifying the correlation, r(Mnew, H), that exists between metric scores for systems and corresponding human a</context>
</contexts>
<marker>Graham, Mathur, Baldwin, 2015</marker>
<rawString>Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2015. Accurate evaluation of segment-level machine translation metrics. In Proc. 2015 Conf. North American Chapter of the Association for Computational Linguistics - Human Language Technologies, pages 1183–1191, Denver, CO. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
</authors>
<title>Improving evaluation of machine translation quality estimation.</title>
<date>2015</date>
<booktitle>In Proc. FiftyThird Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1804--1813</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing,</location>
<contexts>
<context position="17252" citStr="Graham, 2015" startWordPosition="2684" endWordPosition="2686">es strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew, by Pearson correlation takes the form of quantifying the correlation, r(Mnew, H), that exists between metric scores for systems and corresponding human assessment score</context>
</contexts>
<marker>Graham, 2015</marker>
<rawString>Yvette Graham. 2015. Improving evaluation of machine translation quality estimation. In Proc. FiftyThird Annual Meeting of the Association for Computational Linguistics, pages 1804–1813, Beijing, China. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Hong</author>
<author>John M Conroy</author>
<author>Benoit Favre</author>
<author>Alex Kulesza</author>
<author>Hui Lin</author>
<author>Ani Nenkova</author>
</authors>
<title>A repository of state of the art and competitive baseline summaries for generic news summarization.</title>
<date>2014</date>
<booktitle>In Proc. 9th edition of the Language Resources and Evaluation Conference,</booktitle>
<pages>1608--1616</pages>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="3968" citStr="Hong et al., 2014" startWordPosition="588" endWordPosition="591">of those in evaluations. This has the 128 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128–137, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. obvious disadvantage that superior variants may exist but remain unidentified due to their omission. Despite such limitations, however, subsequent evaluations of state-of-the-art summarization systems operate under the assumption that recommended ROUGE variants are optimal and rely on this assumption to draw conclusions about the relative performance of systems (Hong et al., 2014). This forces us to raise some important questions. Firstly, to what degree was the divergence away from evaluation methodologies still applied to MT metrics today well-founded? For example, were the original methodology, by correlation with human assessment, to be applied, would a distinct variant of ROUGE emerge as superior and subsequently lead to distinct system rankings? Secondly, were all variants of ROUGE to be included in evaluations, would a variant originally omitted from the evaluation emerge as superior and lead to further differences in summarization system rankings? Furthermore, </context>
<context position="10601" citStr="Hong et al., 2014" startWordPosition="1603" endWordPosition="1606"> (2012) diverge further from correlation with human judgment for evaluation of metrics by assessing the accuracy of metrics to identify significant differences between pairs of systems when combined with a significance test. Although the approach to evaluation of metrics provides insight into the accuracy of conclusions drawn from metric/test combinations, the evaluation is limited by inclusion of only six variants of ROUGE, fewer than 4% of possible ROUGE variants. Despite such limitations, however, subsequent evaluations relied on recommended ROUGE variants to rank state-of-the-art systems (Hong et al., 2014). Although methods of identifying significant differences in performance are commonly applied to the evaluation of systems in summarization, the application of significance tests to the evaluation of summarization metrics is extremely rare, and when attempts have been made, unfortunately appropriate tests have not been applied. Computation of confidence intervals for individual correlation with human coverage scores, for example, unfortunately does not provide insight into whether or not a difference in correlation with human coverage scores is significant. 3 Summarization Metric Evaluation Wh</context>
<context position="24867" citStr="Hong et al. (2014)" startWordPosition="4537" endWordPosition="4540">le 1: Pearson correlation (r) of BLEU and 192 variants of ROUGE (R-*) with human assessment in DUC-2004, with (Y) and without (N) stemming, with (Y) and without (N) removal of stop words (RSW), aggregated at the summary level using precision (P), recall (R) or f-score (F), aggregated at the system level by average (A) or median (M) summary score, correlations marked with • signify a metric/variant whose correlation with human assessment is not significantly weaker than that of any other metric/variant (an optimal variant) according to pairwise Williams significance tests, variants employed in Hong et al. (2014) are in bold. 133 outperform, as a metric that happens to correlate strongly with a higher number of competing metrics in a given competition would be at an unfair advantage. This increased power also means, somewhat counter-intuitively, it can happen for a pair of competing metrics for which the correlation between metric scores is strong, that a small difference in competing correlations with human assessment is significant, while, for a different pair of metrics with a larger difference in correlation, the difference is not significant, because r(Mbase, Mnew) is weak. For example, in Table </context>
<context position="26460" citStr="Hong et al. (2014)" startWordPosition="4792" endWordPosition="4795">, since scores of the latter pair of metrics correlate with one another with more strength. As part of this research, we have made available an open-source implementation of statistical tests for evaluation of summarization metrics, at https://github.com/ygraham/nlp-williams. 4.1 Significance Test Results In Table 1, • identifies variants of ROUGE not significantly outperformed by any other variant. Figure 3 shows pairwise Williams significance test outcomes for BLEU, the top ten ROUGE variants, as well as current recommended ROUGE variants (Owczarzak et al. (2012)) used to compare systems in Hong et al. (2014). Current recommended best variants of ROUGE are shown to be significantly outperformed by several other ROUGE variants. Although BLEU achieves strongest correlation with human assessment overall, Figure 3 reveals the difference between BLEU’s correlation with human assessment and that of the best-performing ROUGE variant as not statistically significant, and since ROUGE holds the distinct advantage over BLEU of facilitating standard methods of significance testing differences in scores for systems, for this reason alone we recommend the use of the best-performing ROUGE variant over BLEU, aver</context>
<context position="28519" citStr="Hong et al. (2014)" startWordPosition="5106" endWordPosition="5109"> of recall-based metrics in evaluations by correlation with human coverage scores alone. Furthermore, since there exists a vast number of possible formulations that could potentially be applied to evaluation of summaries that are neither purely precision nor recallbased, evaluation methodologies should avoid reliance on assumptions that either precision or recall is superior and instead base conclusions on empirical evidence where possible. 5 Summarization System Evaluation Since we have established that the variants of ROUGE used to rank state-of-the-art and baseline summarization systems in Hong et al. (2014) have significantly weaker correlations with human assessment than several other ROUGE variants, this motivates our replication of the evaluation. We evaluate systems using the variant of ROUGE that BLEU R2_ST1_RS1_A_P R3_ST0_RS0_A_F R2_ST0_RS1_A_P R3_ST0_RS1_A_P R3_ST1_RS0_A_F R3_ST0_RS0_A_R R4_ST0_RS0_A_F R3_ST0_RS0_A_P R4_ST1_RS0_M_R R2_ST1_RS0_M_R R1_ST1_RS0_M_R BLEU R2_ST1_RS1_A_P R3_ST0_RSW0_A_F R2_ST0_RS1_A_P R3_ST0_RS1_A_P R3_ST1_RS0_A_F R3_ST0_RS0_A_R R4_ST0_RS0_A_F R3_ST0_RS0_A_P R4_ST1_RS0_M_R R2_ST1_RS0_M_R R1_ST1_RS0_M_R 134 DPP ICSISumm RegSum Submodular CLASSY11 CLASSY04 OCCAMSV</context>
<context position="30172" citStr="Hong et al. (2014)" startWordPosition="5321" endWordPosition="5324">edyKL 6.918 8.53 FreqSum 6.838 8.11 TsSum 6.671 8.15 Centroid 6.660 7.97 LexRank 6.655 7.47 Table 3: Summarization systems originally included in Hong et al. (2014) evaluated with the best-performing ROUGE variant (Best): average ROUGE-2 precision with stemming and stop words removed; and evaluated with original suboptimal variant (median ROUGE-2 recall with stemming and without removal of stop-words) achieves strongest correlation with human assessment, average ROUGE-2 precision with stemming and stop-words removed. Table 3 shows ROUGE scores for summarization systems originally presented in Hong et al. (2014). System rankings diverge considerably from those of the original evaluation. Notably, the system now taking first place had originally ranked in fourth position. Since the best variant of ROUGE is based on average ROUGE scores as opposed to median ROUGE scores, a difference of means significance test is appropriate provided the normality assumption of score distributions for systems is not violated. In DPP ICSISumm RegSum Submodular CLASSY11 CLASSY04 OCCAMS_V GreedyKL FreqSum TsSum Centroid LexRank Figure 4: Summarization system pairwise significance test outcomes (paired t-test) for state-of</context>
</contexts>
<marker>Hong, Conroy, Favre, Kulesza, Lin, Nenkova, 2014</marker>
<rawString>Kai Hong, John M Conroy, Benoit Favre, Alex Kulesza, Hui Lin, and Ani Nenkova. 2014. A repository of state of the art and competitive baseline summaries for generic news summarization. In Proc. 9th edition of the Language Resources and Evaluation Conference, pages 1608–1616, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proc. 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Alexandra</location>
<contexts>
<context position="16239" citStr="Koehn et al., 2007" startWordPosition="2520" endWordPosition="2523">dvantage over BLEU, as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004; Graham et al., 2014). For this purpose, differences in median ROUGE 131 scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems. 3.3 Metric Evaluation by Pearson’s r Moses (Koehn et al., 2007) multi-bleu1 was used to compute BLEU (Papineni et al., 2002) scores for summaries and prepare4rouge2 applied to summaries before running ROUGE (Lin and Hovy, 2003). Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU’s correlation with the same human assessment of summaries from DUC-2004. Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293. For many pairs of metrics</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. 45th Annual Meeting of the Association for Computational Linguistics, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. 2004 Conf. Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="15922" citStr="Koehn, 2004" startWordPosition="2471" endWordPosition="2472">inal overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries, is, again, a divergence from MT evaluation, as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel. However, in this respect, ROUGE has a distinct advantage over BLEU, as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004; Graham et al., 2014). For this purpose, differences in median ROUGE 131 scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems. 3.3 Metric Evaluation by Pearson’s r Moses (Koehn et al., 2007) multi-bleu1 was used to compute BLEU (Papineni et al., 2002) scores for summaries and prepare4rouge2 applied to summaries before running ROUGE (Lin and Hovy, 2003). Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU’s correlati</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. 2004 Conf. Empirical Methods in Natural Language Processing, pages 388–395, Barcelona, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proc. 2003 Conf. North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>71--78</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1457" citStr="Lin and Hovy, 2003" startWordPosition="209" endWordPosition="212">riants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems. 1 Introduction Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (Lin and Hovy, 2003), the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score (Papineni et al., 2002). Automatic evaluation in MT and summarization have much in common, as both involve the automatic comparison of systemgenerated texts with one or more human-generated reference texts, contrasting either system-output translations or peer summaries with human reference translations or model summaries, depending on the task. In both MT and summarization evaluation, any newly proposed automatic metric must be assessed by the degree to which it provides a good subs</context>
<context position="6022" citStr="Lin and Hovy, 2003" startWordPosition="893" endWordPosition="896"> in system evaluations. Since distinct variants of ROUGE achieve significantly stronger correlation with human assessment than previous recommended best variants, we subsequently replicate a recent evaluation of state-of-the-art summarization systems revealing distinct conclusions about the relative performance of systems. In addition, we include in the evaluation of metrics, an evaluation of BLEU for the purpose of summarization evaluation, and contrary to common belief, precision-based BLEU is on-par with recall-based ROUGE for evaluation of summarization systems. 2 Related Work When ROUGE (Lin and Hovy, 2003) was first proposed, the methodology applied to its evaluation, in one respect, was similar to that applied to metrics in MT, as ROUGE variants were evaluated by correlation with a form of human assessment. Where the evaluation methodology diverged from MT, however, was with respect to the precise representation of human assessment that was employed. In MT evaluation of metrics, although experimentation has taken place with regards to methods of elicitation of assessments from human judges (Callison-Burch et al., 2008), human assessment is always aimed to encapsulate the overall quality of tra</context>
<context position="16403" citStr="Lin and Hovy, 2003" startWordPosition="2546" endWordPosition="2549">nificance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004; Graham et al., 2014). For this purpose, differences in median ROUGE 131 scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems. 3.3 Metric Evaluation by Pearson’s r Moses (Koehn et al., 2007) multi-bleu1 was used to compute BLEU (Papineni et al., 2002) scores for summaries and prepare4rouge2 applied to summaries before running ROUGE (Lin and Hovy, 2003). Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU’s correlation with the same human assessment of summaries from DUC-2004. Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tes</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proc. 2003 Conf. North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 71–78, Edmonton, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
<author>Hoa Dang</author>
<author>Donna Harman</author>
</authors>
<date>2007</date>
<booktitle>Duc in context. Information Processing &amp; Management,</booktitle>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="11934" citStr="Over et al., 2007" startWordPosition="1804" endWordPosition="1807">nnotation of whether or not system-generated summary units express the meaning of model summary units, annotations subsequently used to compute human coverage scores. In addition, an evaluation of the linguistic quality of summaries is commonly carried out. As described in Section 2, when used for the evaluation of metrics, linguistic quality is commonly omitted, however, with metrics only assessed by the degree to which they correlate with human coverage scores. In contrast, we include all available human assessment data for evaluating metrics. 3.1 Combining Quality and Coverage In DUC-2004 (Over et al., 2007), human annotations used to compute summary coverage are carried out by identification of matching peer units (PUs), the units in a peer summary that express content of the corresponding model summary. In addition, an overall coverage estimate (E) is provided by the human annotator, the proportion of the corresponding model summary or collective model units (MUs) expressed overall by a given peer summary. Human coverage scores (CS) are computed by combining Matching PUs with coverage estimates as follows: CS = |Matching PUs |· E 1 |MUs |( ) In addition to annotations used to compute human cove</context>
</contexts>
<marker>Over, Dang, Harman, 2007</marker>
<rawString>Paul Over, Hoa Dang, and Donna Harman. 2007. Duc in context. Information Processing &amp; Management, 43(6):1506–1520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>John M Conroy</author>
<author>Hoa Trang Dang</author>
<author>Ani Nenkova</author>
</authors>
<title>An assessment of the accuracy of automatic evaluation in summarization.</title>
<date>2012</date>
<booktitle>In Proc. Wkshp. on Evaluation Metrics and System Comparison for Automatic Summarization,</booktitle>
<pages>1--9</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Quebec, Canada.</location>
<contexts>
<context position="2963" citStr="Owczarzak et al., 2012" startWordPosition="441" endWordPosition="444">, evaluation of automatic metrics in MT has been by correlation with human assessment. In contrast in summarization, over the years since the introduction of ROUGE, summarization evaluation has seen a variety of different methodologies applied to evaluation of its metrics. Evaluation of summarization metrics has included, for example, the ability of a metric/significance test combination to distinguish between sets of human and system-generated summaries (Rankel et al., 2011), or by accuracy of conclusions drawn from metrics when combined with a particular significance test, Wilcoxon ranksum (Owczarzak et al., 2012). Besides moving away from well-established methods such as correlation with human judgment, previous summarization metric evaluations have been additionally limited by inclusion of only a small proportion of possible metrics and variants. For example, although the most commonly used metric ROUGE has a very large number of possible variants, it is common to include only a small range of those in evaluations. This has the 128 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128–137, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Compu</context>
<context position="9990" citStr="Owczarzak et al. (2012)" startWordPosition="1513" endWordPosition="1516">h to metric evaluation unfortunately does not work in the long-term, as the performance of summarization systems improves and approaches or achieves the quality of a human, a metric that accurately identifies this achievement would be unfairly penalized for it. Separate from the evaluation of metrics, Rankel et al. (2011) make the highly important recommendation of paired tests for identification of significant differences in performance of summarization systems. Since data used in the evaluation of summarization systems is not independent, paired tests are more appropriate and more powerful. Owczarzak et al. (2012) diverge further from correlation with human judgment for evaluation of metrics by assessing the accuracy of metrics to identify significant differences between pairs of systems when combined with a significance test. Although the approach to evaluation of metrics provides insight into the accuracy of conclusions drawn from metric/test combinations, the evaluation is limited by inclusion of only six variants of ROUGE, fewer than 4% of possible ROUGE variants. Despite such limitations, however, subsequent evaluations relied on recommended ROUGE variants to rank state-of-the-art systems (Hong et</context>
<context position="26413" citStr="Owczarzak et al. (2012)" startWordPosition="4782" endWordPosition="4785">opwords removed, 0.067 (0.797 − 0.73) is significant, since scores of the latter pair of metrics correlate with one another with more strength. As part of this research, we have made available an open-source implementation of statistical tests for evaluation of summarization metrics, at https://github.com/ygraham/nlp-williams. 4.1 Significance Test Results In Table 1, • identifies variants of ROUGE not significantly outperformed by any other variant. Figure 3 shows pairwise Williams significance test outcomes for BLEU, the top ten ROUGE variants, as well as current recommended ROUGE variants (Owczarzak et al. (2012)) used to compare systems in Hong et al. (2014). Current recommended best variants of ROUGE are shown to be significantly outperformed by several other ROUGE variants. Although BLEU achieves strongest correlation with human assessment overall, Figure 3 reveals the difference between BLEU’s correlation with human assessment and that of the best-performing ROUGE variant as not statistically significant, and since ROUGE holds the distinct advantage over BLEU of facilitating standard methods of significance testing differences in scores for systems, for this reason alone we recommend the use of th</context>
</contexts>
<marker>Owczarzak, Conroy, Dang, Nenkova, 2012</marker>
<rawString>Karolina Owczarzak, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2012. An assessment of the accuracy of automatic evaluation in summarization. In Proc. Wkshp. on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1–9, Quebec, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1592" citStr="Papineni et al., 2002" startWordPosition="230" endWordPosition="233">translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems. 1 Introduction Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (Lin and Hovy, 2003), the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score (Papineni et al., 2002). Automatic evaluation in MT and summarization have much in common, as both involve the automatic comparison of systemgenerated texts with one or more human-generated reference texts, contrasting either system-output translations or peer summaries with human reference translations or model summaries, depending on the task. In both MT and summarization evaluation, any newly proposed automatic metric must be assessed by the degree to which it provides a good substitute of human assessment, and although there are obvious parallels between evaluation of systems in the two areas, when it comes to e</context>
<context position="16300" citStr="Papineni et al., 2002" startWordPosition="2530" endWordPosition="2533">ean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004; Graham et al., 2014). For this purpose, differences in median ROUGE 131 scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems. 3.3 Metric Evaluation by Pearson’s r Moses (Koehn et al., 2007) multi-bleu1 was used to compute BLEU (Papineni et al., 2002) scores for summaries and prepare4rouge2 applied to summaries before running ROUGE (Lin and Hovy, 2003). Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment, in addition to BLEU’s correlation with the same human assessment of summaries from DUC-2004. Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, h</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proc. 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic evaluation of linguistic quality in multidocument summarization.</title>
<date>2010</date>
<booktitle>In Proc. 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>544--554</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="8114" citStr="Pitler et al., 2010" startWordPosition="1234" endWordPosition="1237">ary appear, and evaluation by human coverage scores alone means that a summary with its units scrambled or even reversed in theory receives precisely the same metric score as the original. Given current evaluation methodologies for assessment of metrics, a metric that scores two such summaries differently would be unfairly penalized for it. Furthermore, when the linguistic quality of summaries has been assessed in parallel with annotations used to compute human coverage scores, it has been shown that the two dimensions of quality do not correlate with one another (no significant correlation) (Pitler et al., 2010), providing evidence that coverage scores alone do not fully represent human judgment of the overall quality of summaries. Subsequent summarization metric evaluations depart from correlation with human judgment fur129 ther by evaluating metrics according to the ability of a metric/significance test combination to identify a significant difference between the quality of human and system-generated summaries (Rankel et al., 2011). Unfortunately, the evaluation of metrics with respect to how well they distinguish between high-quality human summaries and all system-generated summaries, does not pro</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2010</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multidocument summarization. In Proc. 48th Annual Meeting of the Association for Computational Linguistics, pages 544–554, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Rankel</author>
<author>John M Conroy</author>
<author>Eric V Slud</author>
<author>Dianne P O’Leary</author>
</authors>
<title>Ranking human and machine summarization systems.</title>
<date>2011</date>
<booktitle>In Proc. 2011 Conf. Empirical Methods in Natural Language Processing,</booktitle>
<pages>467--473</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland.</location>
<marker>Rankel, Conroy, Slud, O’Leary, 2011</marker>
<rawString>Peter Rankel, John M. Conroy, Eric V. Slud, and Dianne P. O’Leary. 2011. Ranking human and machine summarization systems. In Proc. 2011 Conf. Empirical Methods in Natural Language Processing, pages 467–473, Edinburgh, Scotland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Royston</author>
</authors>
<title>Algorithm as 181: The W test for normality.</title>
<date>1982</date>
<journal>Applied Statistics,</journal>
<pages>31--176</pages>
<contexts>
<context position="31312" citStr="Royston, 1982" startWordPosition="5501" endWordPosition="5502">on system pairwise significance test outcomes (paired t-test) for state-ofthe-art (top 7 rows) and baseline systems (bottom 5 rows) of Hong et al. (2014) evaluated with best-performing ROUGE variant: average ROUGE2 precision with stemming and stop words removed, colored cells denote a significant greater mean score for row i system over column j system according to paired t-test. addition, since data used to evaluate systems are not independent, paired tests are also appropriate (Rankel et al., 2011). ROUGE score distributions for systems were tested for normality using the Shapiro-Wilk test (Royston, 1982) where score distributions for none of the included systems were shown to be significantly non-normal. Figure 4 shows outcomes of paired t-tests for summary score distributions of each pair of systems, revealing three summarization systems not significantly outperformed by any other as DPP, ICSISUMM and REGSUM. In addition, as expected, all state-of-the-art systems significantly outperform all baseline systems. 6 Human Assessment Combinations In order to evaluate metrics by correlation with human assessment, it is necessary to obtain a single human evaluation score per system. For example, in </context>
</contexts>
<marker>Royston, 1982</marker>
<rawString>Patrick Royston. 1982. Algorithm as 181: The W test for normality. Applied Statistics, 31:176–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James H Steiger</author>
</authors>
<title>Tests for comparing elements of a correlation matrix.</title>
<date>1980</date>
<journal>Psychological Bulletin,</journal>
<volume>87</volume>
<issue>2</issue>
<contexts>
<context position="19808" citStr="Steiger, 1980" startWordPosition="3062" endWordPosition="3063">ummaries (corresponding to variables X1, X2 and X3 below), and subsequently three correlations: r(Mbase, H), r(Mnew, H) and r(Mnew, Mbase). If r(Mbase, H) and r(Mnew, H) are both &gt; 0, then the third correlation, between metric scores themselves, r(Mbase, Mnew), must also be &gt; 0. The strength of this correlation, directly between scores of pairs of summarization metrics, should be taken into account using a significance test of the difference in correlation between r(Mbase, H) and r(Mnew, H). Williams test 3 (Williams, 1959) evaluates the significance of a difference in dependent correlations (Steiger, 1980). It is formulated as follows as a test of whether the population correlation between X1 and X3 equals the population correlation between X2 and X3: /2K (n−1) + (r23+r13)2 (1 − r12)3 v (n−3) 4 where rij is the correlation between Xi and Xj, n is the size of the population, and: K = 1 − r122 − r132 − r232 + 2r12r13r23 Since the power of Williams test increases when the third correlation, r(Mbase, Mnew), between metric scores is stronger, metrics should not be ranked by the number of competing metrics they 3Also known as Hotelling-Williams. t(n 3) (r13 − r23) Y (n − 1)(1 + r12) — = , 132 temming</context>
</contexts>
<marker>Steiger, 1980</marker>
<rawString>James H. Steiger. 1980. Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2):245–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan J Williams</author>
</authors>
<title>Regression analysis,</title>
<date>1959</date>
<volume>14</volume>
<publisher>Wiley</publisher>
<location>New York.</location>
<contexts>
<context position="17151" citStr="Williams, 1959" startWordPosition="2668" endWordPosition="2669">me human assessment of summaries from DUC-2004. Somewhat surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU, to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew, by Pearson correlation takes the form of quantifying the correlatio</context>
<context position="19723" citStr="Williams, 1959" startWordPosition="3050" endWordPosition="3051">ependent, as evaluations comprise three sets of scores for precisely the same set of summaries (corresponding to variables X1, X2 and X3 below), and subsequently three correlations: r(Mbase, H), r(Mnew, H) and r(Mnew, Mbase). If r(Mbase, H) and r(Mnew, H) are both &gt; 0, then the third correlation, between metric scores themselves, r(Mbase, Mnew), must also be &gt; 0. The strength of this correlation, directly between scores of pairs of summarization metrics, should be taken into account using a significance test of the difference in correlation between r(Mbase, H) and r(Mnew, H). Williams test 3 (Williams, 1959) evaluates the significance of a difference in dependent correlations (Steiger, 1980). It is formulated as follows as a test of whether the population correlation between X1 and X3 equals the population correlation between X2 and X3: /2K (n−1) + (r23+r13)2 (1 − r12)3 v (n−3) 4 where rij is the correlation between Xi and Xj, n is the size of the population, and: K = 1 − r122 − r132 − r232 + 2r12r13r23 Since the power of Williams test increases when the third correlation, r(Mbase, Mnew), between metric scores is stronger, metrics should not be ranked by the number of competing metrics they 3Also</context>
</contexts>
<marker>Williams, 1959</marker>
<rawString>Evan J. Williams. 1959. Regression analysis, volume 14. Wiley New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
</authors>
<title>Modeling coherence in ESOL learner texts.</title>
<date>2012</date>
<booktitle>In Proc. Seventh Wkshp. on Building Educational Applications Using NLP,</booktitle>
<pages>33--43</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Canada.</location>
<contexts>
<context position="17609" citStr="Yannakoudakis and Briscoe, 2012" startWordPosition="2735" endWordPosition="2738">another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew, by Pearson correlation takes the form of quantifying the correlation, r(Mnew, H), that exists between metric scores for systems and corresponding human assessment scores, and contrasting this correlation with the correlation for some baseline metric, r(Mbase, H). One approach to testing for significance that may seem reasonable is to apply a significance test 1https://github.com/moses-smt/mosesdecoder/ commits/master/scripts/generic/multi-bleu.perl 2http://kavita-ganesan.com/content/ prepare4rouge-script-prepare-rouge-e</context>
</contexts>
<marker>Yannakoudakis, Briscoe, 2012</marker>
<rawString>Helen Yannakoudakis and Ted Briscoe. 2012. Modeling coherence in ESOL learner texts. In Proc. Seventh Wkshp. on Building Educational Applications Using NLP, pages 33–43, Montreal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
</authors>
<title>A new dataset and method for automatically grading ESOL texts.</title>
<date>2011</date>
<booktitle>In Proc. 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>180--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, OR.</location>
<contexts>
<context position="17576" citStr="Yannakoudakis et al., 2011" startWordPosition="2731" endWordPosition="2734">formance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew, by Pearson correlation takes the form of quantifying the correlation, r(Mnew, H), that exists between metric scores for systems and corresponding human assessment scores, and contrasting this correlation with the correlation for some baseline metric, r(Mbase, H). One approach to testing for significance that may seem reasonable is to apply a significance test 1https://github.com/moses-smt/mosesdecoder/ commits/master/scripts/generic/multi-bleu.perl 2http://kavita-ganesan.com/content/ pre</context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically grading ESOL texts. In Proc. 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 180– 189, Portland, OR. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
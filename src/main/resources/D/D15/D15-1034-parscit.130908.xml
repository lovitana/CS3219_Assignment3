<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006010">
<title confidence="0.934514">
Composing Relationships with Translations
</title>
<author confidence="0.709313">
Alberto Garcia-Dur´an
</author>
<address confidence="0.672582333333333">
Sorbonne universit´es, UTC
CNRS, Heudiasyc 7253
60203 Compi`egne, France
</address>
<email confidence="0.995835">
agarciad@utc.fr
</email>
<author confidence="0.914988">
Antoine Bordes
</author>
<affiliation confidence="0.888558">
Facebook AI Research
</affiliation>
<address confidence="0.9342915">
770 Broadway
New York, NY. USA
</address>
<email confidence="0.998194">
abordes@fb.com
</email>
<author confidence="0.593335">
Nicolas Usunier
</author>
<affiliation confidence="0.528467">
Facebook AI Research
</affiliation>
<address confidence="0.6188265">
112, avenue de Wagram
75017 Paris, France
</address>
<email confidence="0.996788">
usunier@fb.com
</email>
<sectionHeader confidence="0.99736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837176470588">
Performing link prediction in Knowledge
Bases (KBs) with embedding-based mod-
els, like with the model TransE (Bordes et
al., 2013) which represents relationships
as translations in the embedding space,
have shown promising results in recent
years. Most of these works are focused on
modeling single relationships and hence
do not take full advantage of the graph
structure of KBs. In this paper, we pro-
pose an extension of TransE that learns to
explicitly model composition of relation-
ships via the addition of their correspond-
ing translation vectors. We show empir-
ically that this allows to improve perfor-
mance for predicting single relationships
as well as compositions of pairs of them.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998201">
Performing link prediction on multi-relational data
is becoming essential in order to complete the
huge amount of missing information of the knowl-
edge bases. These knowledge can be formalized
as directed multi-relation graphs, whose node cor-
respond to entities connected with edges encod-
ing various kind of relationships. We denote these
connections via triples (head, label, tail). Link
prediction consists in filling in incomplete triples
like (head, label, ?) or (?, label, tail).
In this context, embedding models (Wang et
al., 2014; Lin et al., 2015; Jenatton et al., 2012;
Socher et al., 2013) that attempt to learn low-
dimensional vector or matrix representations of
entities and relationships have shown promising
performance in recent years. In particular, the ba-
sic model TRANSE (Bordes et al., 2013) has been
proved to be very powerful. This model treats each
relationship as a translation vector operating on
the embedding representing the entities. Hence,
for a triple (head, label, tail), the vector embed-
dings of head and tail are learned so that they are
connected through a translation parameterized by
the vector associated with label. Many extensions
have been proposed to improve the representation
power of TRANSE while still keeping its simplic-
ity, by adding some projections steps before the
translation (Wang et al., 2014; Lin et al., 2015).
In this paper, we propose an extension of
TRANSE 1 that focuses on improving its represen-
tation of the underlying graph of multi-relational
data by trying to learn compositions of relation-
ships as sequences of translations in the embed-
ding space. The idea is to train the embeddings
by learning simple reasonings, such as the rela-
tionship people/nationality should give a similar
result as the composition people/city of birth and
city/country. In our approach, called RTRANSE,
the training set is augmented with relevant ex-
amples of such compositions by performing con-
strained walks in the knowledge graph, and train-
ing so that sequences of translations lead to the
desired result. The idea of compositionality to
model multi-relational data was previously intro-
duced in (Neelakantan et al., 2015). That work
composes relationships by means of recurrent neu-
ral networks (RNN) (one per relationship) with
non-linearities. However, we show that there is
a natural way to compose relationships by simply
adding translation vectors and not requiring ad-
ditional parameters, which makes it specially ap-
pealing because of its scalability.
We present experimental results that show the
superiority of RTRANSE over TRANSE in terms
of link prediction. A detailed evaluation, in which
test examples are classified as easy or hard de-
pending on their similarity with training data,
highlights the improvement of RTRANSE on both
categories. Our experiments include a new eval-
uation protocol, in which the model is directly
</bodyText>
<footnote confidence="0.991767">
1Code available in https://github.com/glorotxa/SME
</footnote>
<page confidence="0.941774">
286
</page>
<note confidence="0.669942">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 286–290,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9980075">
asked to answer questions related to compositions
of relations, such as (head, label1, label2, ?).
RTRANSE also achieves significantly better per-
formances than TRANSE on this new dataset.
We describe RTRANSE in the next section, and
present our experiments in Section 3.
</bodyText>
<sectionHeader confidence="0.981741" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.9999235">
The model we propose is inspired by TRANSE
(Bordes et al., 2013). In TRANSE, entities and
relationships of a KB are mapped to low dimen-
sional vectors, called embeddings. These embed-
dings are learnt so that for each fact (h, E, t) in the
KB, we have h + E ≈ t in the embedding space.
Using translations for relationships naturally
leads to embed the composition of two relation-
ships as the sum of their embeddings: on a path
(h, E, t), (t, E&apos;, t&apos;), we should have h+E+E&apos; ≈ t&apos; in
the embedding space. The original TRANSE does
not enforce that the embeddings accurately repro-
duce such compositions. The recurrent TRANSE
we propose here has a modified training stage to
include such compositions. This should allow to
model simple reasonings in the KB, such as peo-
ple/nationality is similar to the composition of
people/city of birth and city/country.
</bodyText>
<subsectionHeader confidence="0.994213">
2.1 Recurrent TransE
</subsectionHeader>
<bodyText confidence="0.99998025">
We describe in this section our model in its full
generality, which allows to deal with composi-
tions of an arbitrary number of relationships, even
though in this first work we experimented only
with compositions of two relationships.
Triples that are the result of a compositions are
denoted by (h, {Ei}p i=1, t), where p is the number
of relationships that are composed to go from h
to t. Such a path means that there exist entities
e1,..., ep+1, with e1 = h and ep+1 = t such that
for all k, (ek, Ek, ek+1) is a fact in the KB. Our
model, RTRANSE for recurrent TRANSE, repre-
sents each step sk(h, {Ei}p i=1, t) along the path in
the KB with the recurrence relationship (boldface
characters denote embedding vectors i.e. h is the
embedding vector of the entity h):
</bodyText>
<equation confidence="0.9743725">
s1(h, {Ei}p i=1,t) = h
sk+1(h, {Ei}pi=1, t) = sk(h, {Ei}p i=1,t) + fk .
</equation>
<bodyText confidence="0.972489">
Then, the energy of a triple is computed as
d(h, {Ei}p i=1, t) = ||sp(h, {Ei}p i=1, t) − t||2 .
</bodyText>
<subsectionHeader confidence="0.999601">
2.2 Path construction and filtering
</subsectionHeader>
<bodyText confidence="0.969175666666667">
The experience of the paper is motivated by learn-
ing simple reasonings in the KB through the com-
positions of relationships. Therefore, we restricted
our analysis to paths of length 2 created as follows.
First, for each fact (h, E, t), retrieve all paths
(h, {E1, E2}, t) such that there is e such that both
(h, E1, e) and (e, E2, t) are in the KB. Then, we
filter out paths where (h, E1, e) = (h, E, t) or
(e, E2, t) = (h, E, t), as well as the paths with
</bodyText>
<equation confidence="0.806196">
E1 = E2 and h = e = t.
</equation>
<bodyText confidence="0.999914333333333">
We focused on “unambiguous” paths, so that
the reasoning might actually make sense. In par-
ticular, we considered only paths where E1 is either
a 1-to-1 or a 1-to-many relationship, and where
E2 is either a 1-to-1 or a many-to-1 relationship.
In our experiments, the paths created for training
only consider the training subset of facts.
In the remainder of the paper, such paths of
length 2 are called quadruples.
</bodyText>
<subsectionHeader confidence="0.995938">
2.3 Training and regularization
</subsectionHeader>
<bodyText confidence="0.999834">
Our training objective is decomposed in two parts:
the first one is the ranking criterion on triples of
TRANSE, ignoring quadruples. Paths are then
taken into account through additional regulariza-
tion terms.
Denoting by S the set of facts in the KB, the
first part of the training objective is the following
ranking criterion that operates on triples
</bodyText>
<equation confidence="0.986776333333333">
E [γ + d(h, E, t) − d(h&apos;, E, t&apos;)]+ ,
(h,`,t)ES
(h&apos;,`,t&apos;)ES(h,e,t)
</equation>
<bodyText confidence="0.999962071428571">
where [x]+ = max(x, 0) is the positive part of
x, γ is a margin hyperparameter and S(h,`,t) is the
set of corrupted triples created from (h, E, t) by re-
placing either h or t with another KB entity.
This ranking loss effectively trains so that the
embedding of the tail is the nearest neighbor of the
translated head, but it does not guarantee that the
distance between the tail and the translated head is
small. The nearest neighbor criterion is sufficient
to make inference over simple triples, but making
sure that the distance is small is necessary for the
composition rule to be accurate. In order to ac-
count for the compositionality of relationships, we
add two additional regularization terms:
</bodyText>
<listItem confidence="0.90352875">
• λ E(h,`,t)ES d(h, E, t)2
2
• α E(h J`1,`2},t)ES N`IJ`1,`2Id(h, {E1, E2}, t)
.
</listItem>
<page confidence="0.952804">
287
</page>
<table confidence="0.999968625">
DATA SET FAMILY FB15K
ENTITIES 721 14,951
RELATIONSHIPS 7 1,345
TRAINING TRIPLES 8,461 483,142
TRAINING QUAD. – 30,252
VALIDATION TRIPLES 2,820 50,000
TEST TRIPLES 2,821 59,071
TEST QUAD. – 1,852
</table>
<tableCaption confidence="0.992686">
Table 1: Statistics of the datasets.
</tableCaption>
<table confidence="0.999906333333333">
MODEL TRANSE RTRANSE
MR H@10 MR H@10
EASY 17.7 76.8 12.5 82.2
HARD 191.0 48.9 205.7 51.0
EASY W. COMP. 16.4 78.8 11.6 83.0
EASY W/O COMP. 21.6 71.3 16.0 75.3
HARD W. COMP. 208.1 46.8 212.2 49.3
HARD W/O COMP. 122.9 57.0 123.8 57.5
OVERALL 50.7 71.5 49.5 76.2
</table>
<tableCaption confidence="0.993913">
Table 2: Detailed performances on FB15k of
</tableCaption>
<bodyText confidence="0.9564505">
TRANSE and RTRANSE. H@10 are in %. W.
COMP. indicates examples for which there exist
quadruplets in train matching their relationship.
The first criterion only applies to original facts
of the KB, while the second term applies to
quadruples. Nt1jt1,t2}, which involves both the
relationships of the quadruple and the relation-
ship E from which it was created, is the num-
ber of paths involving relationships {E1, E2} cre-
ated from a fact involving E, normalized by the
total number of quadruples created from facts
involving E. This criterion puts more weight
on paths that are reliable as an alternative for
a relationship, for instance {people/city of birth,
city/country} is likely a better alternative to peo-
ple/nationality than {people/writer of the film,
film/film release region}. Finally, a regularization
term p||e||2 2 is added for each entity embedding e.
</bodyText>
<sectionHeader confidence="0.999703" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9999076">
This section presents experiments on the bench-
mark FB15K introduced in (Bordes et al., 2013)
and on FAMILY, a slightly extended version of the
artificial database described in (Garcia-Dur´an et
al., 2014). Table 1 gives their statistics.
</bodyText>
<subsectionHeader confidence="0.982219">
3.1 Experimental Protocol
</subsectionHeader>
<bodyText confidence="0.9999145">
Data FB15K is a subset of Freebase, a very
large database of generic facts gathering more than
1.2 billion triples and 80 million entities. Inspired
by (Hinton, 1986), FAMILY is a database that
contains triples expressing family relationships
(cousin of, has ancestor, married to, parent of,
related to, sibling of, uncle of) among the mem-
bers of 5 families along 6 generations. This dataset
is artificial and each family is organized in a lay-
ered tree structure where each layer refers to a gen-
eration. Families are connected among them by
marriage links between two members, randomly
sampled from the same layer of different fami-
lies. Interestingly on this dataset, there are ob-
vious compositional relationships like uncle of ≈
sibling of + parent of or parent of ≈ married to
+ parent of, among others.
Setting Our main comparison is TRANSE so we
followed the same experimental setting as in (Bor-
des et al., 2013), using ranking metrics for eval-
uation. For each test triple we replaced the head
by each of the entities in turn, and then computed
the score of each of these candidates and sorted
them. Since other positive candidates (i.e. entities
forming true triples) can be ranked higher than the
target one, we filtered out all the positive candi-
dates existing in either the training, validation and
test set, except the target one, from the ranking
and then we kept the rank of the target entity. The
same procedure is repeated but removing the tail
instead of the head. The filtered mean rank (mean
rank in the rest) is the average of these ranks, and
the filtered Hits@10 (H@10 in the rest) is the pro-
portion of target entities in the top 10 predictions.
The embedding dimensions were set to 20 for
FAMILY and 100 for FB15K. Training was per-
formed by stochastic gradient descent, stopping
after for 500 epochs. On FB15K, we used the
embeddings of TRANSE to initialize RTRANSE,
and we set a learning rate of 0.001 to fine-tune
RTRANSE. On FAMILY, both algorithms were ini-
tialized randomly and used a learning rate of 0.01.
The mean rank was used as a validation criterion,
and the values of γ, A, α and p were chosen re-
spectively among {0.25,0.5, 1}, {1e−4, 1e−5, 0},
{0.1, 0.05, 0.1, 0.01, 0.005} and {1e−4, 1e−5, 0}.
</bodyText>
<subsectionHeader confidence="0.958627">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.9999241">
Overall performances Experiments on FAM-
ILY show a quantitative improvement of the per-
formance of RTRANSE : where TRANSE gets a
mean rank of 6.7 and a H@5 of 68.7, RTRANSE
get a performance of 6.3 and 72.3 respectively.
Similarly, on FB15K, Table 2 (last row) shows
that training on longer paths (length 2 here) actu-
ally consistently improves the performance while
predicting heads and tails of triples only: the over-
all H@10 improves by almost 5% from 71.5 for
</bodyText>
<page confidence="0.992689">
288
</page>
<table confidence="0.999679176470588">
3 Nearest entities to h + l1 + l2
RTRANSE TRANSE
h: madtv U.S.A. Ireland
l1: regular TV appearance Ireland U.S.A.
l2: nationality Japan U.K.
h: stargate atlantis Hawaii Scotland
l1: regular TV appearance Scotland Hawaii
l2: nationality U.S.A. U.K.
h: malay southeast asia taiwan
l1: language/main country malaysia southeast asia
l2: continent asia philippines
h: indiana state university the hoosier state maryland
l1: institution/campuses terre haute rhode island
l2: location/state province region rhode island the constitution state
h: university of victoria victoria kelowna
l1: institution/campuses kurnaby toronto
l2: location/citytown kelowna ottawa
</table>
<tableCaption confidence="0.996999">
Table 3: Examples of predictions on quadruples of TRANSE and RTRANSE. The relation paths {l1, l2}
</tableCaption>
<bodyText confidence="0.99797">
of the first two examples encode the single the relationship l tv program/country of origin; the third one
stands for /language/human language/region and the last two ones for /location/location/containedby.
The correct answer is in bold.
TRANSE to 76.2 for RTRANSE.
Detailed results In order to better understand
the gains of RTRANSE, we performed a detailed
evaluation on FB15K, by classifying the test
triples along two axes: easy vs hard and with
composition vs without composition. A test triple
(h, `, t) is easy if its head and tail are connected
by a triple in the training set, i.e. if either (h, `&apos;, t)
or (t, `&apos;, h) is seen in train for some relationship
`&apos;. Otherwise, the triple is hard. Orthogonally,
the test triple (h, `, t) is with composition if there
is at least one path {`1, `2} for the relationship `,
regardless of the existence of that specific path be-
tween the entities h and t. If no such path exists,
(h, `, t) is without composition.
The detailed results are shown in Table 2. We
can see that comparatively to TRANSE, RTANSE
particularly improves performances in terms of
H@10 on triples with composition, improving on
easy triples by 4.2% (from 78.8% to 83,0%) and
hard triples by 2.5% (from 46.8% to 49.3%). The
main gains are still on easy triples, and in fact
the H@10 on easy triples without composition in-
creases by 4%, from 71.3% to 75.3%. The mean
rank also considerably improves on easy triples,
and stays somehow still on hard ones. All in
all, the results show that considering paths during
training very significantly improves performances,
and the results on triples with composition suggest
that RTRANSE is indeed capable of capturing the
evidence of links that exist in longer paths.
</bodyText>
<subsectionHeader confidence="0.984129">
3.3 Results on quadruples
</subsectionHeader>
<bodyText confidence="0.999945863636364">
While usual evaluations for link prediction in KBs
focus on predicting a missing element of a test
triple, we propose here to extend the evaluation
to answering more complex questions, such as
(h, {`1, `2}, ?) or (?, {`1, `2}, t).
Examples Table 3 presents examples of predic-
tions of both TRANSE and our model RTRANSE
on such quadruples. The two first examples try
to predict the origin of two TV series from the na-
tionality of the actors that regularly appear in them
(regular tv appearance). In the first one, the amer-
ican actor phil lamarr is the only entity connected
to the american TV show madtv through the rela-
tionship regular tv appearance. RTRANSE is able
to correctly infer the country of origin from this
information since it forces country of origin ≈
regular tv appearance + nationality. On the other
side TRANSE is affected by the cascading error
since the ranking loss does not guarantee that the
distance between h + l1 and phil lamarr is small,
so when summing l2 it eventually ends up closer
to Ireland rather than USA. In contrast, the second
</bodyText>
<page confidence="0.992961">
289
</page>
<bodyText confidence="0.999979375">
example shows that answering that question by us-
ing that path is sometimes difficult: the members
of the cast of that TV show have different nation-
alities, so RTRANSE lists the nationalities of these
ones and the correct one is ranked third. TRANSE
is again more affected than RTRANSE by the cas-
cading error. In the third one, RTRANSE deducts
the main region where malay is spoken from the
continent of the country with the most number of
speakers of that language. In the last two exam-
ples, our model infers the location of those uni-
versities by forcing an equivalence between their
location and the location of their respective cam-
pus.
Prediction performance For a more quantita-
tive analysis, we have generated a new test dataset
of link prediction on quadruples on FB15K. This
test set was created by generating the paths from
the usual test set (the triple test set) and remov-
ing those quadruples that are used for training. We
obtain 1,852 quadruples. The overall experimen-
tal protocol is the same as before, trying to predict
the head or tail of these quadruple in turn.
On that evaluation protocol, RTRANSE has a
mean rank of 114.0 and a H@10 of 68.2%, while
TRANSE obtains a mean rank of 159.9 and a
H@10 of 65.2% (using the same models as in the
previous subsection). We can see that learning
on paths improves performances on both metrics,
with a gain of 3% in terms of H@10 and an im-
portant gain of about 46 in mean rank, which cor-
responds to a relative improvement of about 30%.
</bodyText>
<sectionHeader confidence="0.999771" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999993857142857">
We have proposed to learn embeddings of compo-
sitions of relationships in the translation model for
link prediction in KBs. Our experimental results
show that this approach is promising.
We considered in this work a restricted set of
small paths of length two. We leave the study of
more general paths to future work.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99974875">
This work was carried out in the framework of
the Labex MS2T (ANR-11-IDEX-0004-02), and
was funded by the French National Agency for Re-
search (EVEREST-12-JS02-005-01).
</bodyText>
<sectionHeader confidence="0.998468" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999712352941176">
Antoine Bordes, Nicolas Usunier, Alberto Garc´ıa-
Dur´an, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems, pages 2787–2795.
Alberto Garc´ıa-Dur´an, Antoine Bordes, and Nicolas
Usunier. 2014. Effective blending of two and three-
way interactions for modeling multi-relational data.
In ECML PKDD 2014. Springer Berlin Heidelberg.
Geoffrey E Hinton. 1986. Learning distributed repre-
sentations of concepts. In Proceedings of the eighth
annual conference of the cognitive science society,
volume 1, page 12. Amherst, MA.
Rodolphe Jenatton, Nicolas Le Roux, Antoine Bordes,
and Guillaume Obozinski. 2012. A latent factor
model for highly multi-relational data. In NIPS 25.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In Pro-
ceedings of AAAI’15.
Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space mod-
els for knowledge base completion. arXiv preprint
arXiv:1504.06662.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In Advances in Neural Information Processing Sys-
tems 26.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of the Twenty-
Eighth AAAI Conference on Artificial Intelligence,
pages 1112–1119.
</reference>
<page confidence="0.997105">
290
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.070417">
<title confidence="0.999965">Composing Relationships with Translations</title>
<author confidence="0.999918">Alberto Garcia-Dur´an</author>
<affiliation confidence="0.60784">Sorbonne universit´es, UTC</affiliation>
<address confidence="0.9883465">CNRS, Heudiasyc 7253 60203 Compi`egne, France</address>
<email confidence="0.990189">agarciad@utc.fr</email>
<author confidence="0.522239">Antoine</author>
<affiliation confidence="0.329988">Facebook AI</affiliation>
<address confidence="0.722672">770 New York, NY.</address>
<email confidence="0.995149">abordes@fb.com</email>
<author confidence="0.870511">Nicolas</author>
<affiliation confidence="0.626153">Facebook AI</affiliation>
<address confidence="0.955107">112, avenue de Wagram 75017 Paris, France</address>
<email confidence="0.99994">usunier@fb.com</email>
<abstract confidence="0.999410944444444">Performing link prediction in Knowledge Bases (KBs) with embedding-based models, like with the model TransE (Bordes et al., 2013) which represents relationships as translations in the embedding space, have shown promising results in recent years. Most of these works are focused on modeling single relationships and hence do not take full advantage of the graph structure of KBs. In this paper, we propose an extension of TransE that learns to explicitly model composition of relationships via the addition of their corresponding translation vectors. We show empirically that this allows to improve performance for predicting single relationships as well as compositions of pairs of them.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Alberto Garc´ıaDur´an</author>
<author>Jason Weston</author>
<author>Oksana Yakhnenko</author>
</authors>
<title>Translating embeddings for modeling multirelational data.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2787--2795</pages>
<marker>Bordes, Usunier, Garc´ıaDur´an, Weston, Yakhnenko, 2013</marker>
<rawString>Antoine Bordes, Nicolas Usunier, Alberto Garc´ıaDur´an, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems, pages 2787–2795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Garc´ıa-Dur´an</author>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
</authors>
<title>Effective blending of two and threeway interactions for modeling multi-relational data.</title>
<date>2014</date>
<booktitle>In ECML PKDD</booktitle>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Garc´ıa-Dur´an, Bordes, Usunier, 2014</marker>
<rawString>Alberto Garc´ıa-Dur´an, Antoine Bordes, and Nicolas Usunier. 2014. Effective blending of two and threeway interactions for modeling multi-relational data. In ECML PKDD 2014. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Learning distributed representations of concepts.</title>
<date>1986</date>
<booktitle>In Proceedings of the eighth annual conference of the cognitive science society,</booktitle>
<volume>1</volume>
<pages>12</pages>
<location>Amherst, MA.</location>
<contexts>
<context position="10251" citStr="Hinton, 1986" startWordPosition="1698" endWordPosition="1699">rnative to people/nationality than {people/writer of the film, film/film release region}. Finally, a regularization term p||e||2 2 is added for each entity embedding e. 3 Experiments This section presents experiments on the benchmark FB15K introduced in (Bordes et al., 2013) and on FAMILY, a slightly extended version of the artificial database described in (Garcia-Dur´an et al., 2014). Table 1 gives their statistics. 3.1 Experimental Protocol Data FB15K is a subset of Freebase, a very large database of generic facts gathering more than 1.2 billion triples and 80 million entities. Inspired by (Hinton, 1986), FAMILY is a database that contains triples expressing family relationships (cousin of, has ancestor, married to, parent of, related to, sibling of, uncle of) among the members of 5 families along 6 generations. This dataset is artificial and each family is organized in a layered tree structure where each layer refers to a generation. Families are connected among them by marriage links between two members, randomly sampled from the same layer of different families. Interestingly on this dataset, there are obvious compositional relationships like uncle of ≈ sibling of + parent of or parent of </context>
</contexts>
<marker>Hinton, 1986</marker>
<rawString>Geoffrey E Hinton. 1986. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, volume 1, page 12. Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodolphe Jenatton</author>
<author>Nicolas Le Roux</author>
<author>Antoine Bordes</author>
<author>Guillaume Obozinski</author>
</authors>
<title>A latent factor model for highly multi-relational data. In</title>
<date>2012</date>
<journal>NIPS</journal>
<volume>25</volume>
<marker>Jenatton, Le Roux, Bordes, Obozinski, 2012</marker>
<rawString>Rodolphe Jenatton, Nicolas Le Roux, Antoine Bordes, and Guillaume Obozinski. 2012. A latent factor model for highly multi-relational data. In NIPS 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yankai Lin</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Yang Liu</author>
<author>Xuan Zhu</author>
</authors>
<title>Learning entity and relation embeddings for knowledge graph completion.</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI’15.</booktitle>
<contexts>
<context position="1596" citStr="Lin et al., 2015" startWordPosition="236" endWordPosition="239">hips as well as compositions of pairs of them. 1 Introduction Performing link prediction on multi-relational data is becoming essential in order to complete the huge amount of missing information of the knowledge bases. These knowledge can be formalized as directed multi-relation graphs, whose node correspond to entities connected with edges encoding various kind of relationships. We denote these connections via triples (head, label, tail). Link prediction consists in filling in incomplete triples like (head, label, ?) or (?, label, tail). In this context, embedding models (Wang et al., 2014; Lin et al., 2015; Jenatton et al., 2012; Socher et al., 2013) that attempt to learn lowdimensional vector or matrix representations of entities and relationships have shown promising performance in recent years. In particular, the basic model TRANSE (Bordes et al., 2013) has been proved to be very powerful. This model treats each relationship as a translation vector operating on the embedding representing the entities. Hence, for a triple (head, label, tail), the vector embeddings of head and tail are learned so that they are connected through a translation parameterized by the vector associated with label. M</context>
</contexts>
<marker>Lin, Liu, Sun, Liu, Zhu, 2015</marker>
<rawString>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of AAAI’15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Benjamin Roth</author>
<author>Andrew McCallum</author>
</authors>
<title>Compositional vector space models for knowledge base completion. arXiv preprint arXiv:1504.06662.</title>
<date>2015</date>
<contexts>
<context position="3205" citStr="Neelakantan et al., 2015" startWordPosition="492" endWordPosition="495">ons of relationships as sequences of translations in the embedding space. The idea is to train the embeddings by learning simple reasonings, such as the relationship people/nationality should give a similar result as the composition people/city of birth and city/country. In our approach, called RTRANSE, the training set is augmented with relevant examples of such compositions by performing constrained walks in the knowledge graph, and training so that sequences of translations lead to the desired result. The idea of compositionality to model multi-relational data was previously introduced in (Neelakantan et al., 2015). That work composes relationships by means of recurrent neural networks (RNN) (one per relationship) with non-linearities. However, we show that there is a natural way to compose relationships by simply adding translation vectors and not requiring additional parameters, which makes it specially appealing because of its scalability. We present experimental results that show the superiority of RTRANSE over TRANSE in terms of link prediction. A detailed evaluation, in which test examples are classified as easy or hard depending on their similarity with training data, highlights the improvement o</context>
</contexts>
<marker>Neelakantan, Roth, McCallum, 2015</marker>
<rawString>Arvind Neelakantan, Benjamin Roth, and Andrew McCallum. 2015. Compositional vector space models for knowledge base completion. arXiv preprint arXiv:1504.06662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Reasoning With Neural Tensor Networks For Knowledge Base Completion.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26.</booktitle>
<contexts>
<context position="1641" citStr="Socher et al., 2013" startWordPosition="244" endWordPosition="247">them. 1 Introduction Performing link prediction on multi-relational data is becoming essential in order to complete the huge amount of missing information of the knowledge bases. These knowledge can be formalized as directed multi-relation graphs, whose node correspond to entities connected with edges encoding various kind of relationships. We denote these connections via triples (head, label, tail). Link prediction consists in filling in incomplete triples like (head, label, ?) or (?, label, tail). In this context, embedding models (Wang et al., 2014; Lin et al., 2015; Jenatton et al., 2012; Socher et al., 2013) that attempt to learn lowdimensional vector or matrix representations of entities and relationships have shown promising performance in recent years. In particular, the basic model TRANSE (Bordes et al., 2013) has been proved to be very powerful. This model treats each relationship as a translation vector operating on the embedding representing the entities. Hence, for a triple (head, label, tail), the vector embeddings of head and tail are learned so that they are connected through a translation parameterized by the vector associated with label. Many extensions have been proposed to improve </context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks For Knowledge Base Completion. In Advances in Neural Information Processing Systems 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Wang</author>
<author>Jianwen Zhang</author>
<author>Jianlin Feng</author>
<author>Zheng Chen</author>
</authors>
<title>Knowledge graph embedding by translating on hyperplanes.</title>
<date>2014</date>
<booktitle>In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1112--1119</pages>
<contexts>
<context position="1578" citStr="Wang et al., 2014" startWordPosition="232" endWordPosition="235">ng single relationships as well as compositions of pairs of them. 1 Introduction Performing link prediction on multi-relational data is becoming essential in order to complete the huge amount of missing information of the knowledge bases. These knowledge can be formalized as directed multi-relation graphs, whose node correspond to entities connected with edges encoding various kind of relationships. We denote these connections via triples (head, label, tail). Link prediction consists in filling in incomplete triples like (head, label, ?) or (?, label, tail). In this context, embedding models (Wang et al., 2014; Lin et al., 2015; Jenatton et al., 2012; Socher et al., 2013) that attempt to learn lowdimensional vector or matrix representations of entities and relationships have shown promising performance in recent years. In particular, the basic model TRANSE (Bordes et al., 2013) has been proved to be very powerful. This model treats each relationship as a translation vector operating on the embedding representing the entities. Hence, for a triple (head, label, tail), the vector embeddings of head and tail are learned so that they are connected through a translation parameterized by the vector associ</context>
</contexts>
<marker>Wang, Zhang, Feng, Chen, 2014</marker>
<rawString>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence, pages 1112–1119.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
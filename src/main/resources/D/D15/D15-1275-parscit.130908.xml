<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008368">
<title confidence="0.99617">
Automatic Diacritics Restoration for Hungarian
</title>
<author confidence="0.972125">
Attila Nov´ak&apos;,2 and Borb´ala Sikl´osi2
</author>
<affiliation confidence="0.7983045">
&apos;MTA-PPKE Hungarian Language Technology Research Group,
2P´azm´any P´eter Catholic University, Faculty of Information Technology and Bionics
</affiliation>
<address confidence="0.779322">
50/a Pr´ater street, 1083 Budapest, Hungary
</address>
<email confidence="0.998279">
{novak.attila, siklosi.borbala}@itk.ppke.hu
</email>
<sectionHeader confidence="0.993865" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938076923077">
In this paper, we describe a method based
on statistical machine translation (SMT)
that is able to restore accents in Hungarian
texts with high accuracy. Due to the ag-
glutination in Hungarian, there are always
plenty of word forms unknown to a sys-
tem trained on a fixed vocabulary. In or-
der to be able to handle such words, we
integrated a morphological analyzer into
the system that can suggest accented word
candidates for unknown words. We evalu-
ated the system in different setups, achiev-
ing an accuracy above 99% at the highest.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999841875">
Due to clumsy mobile device interfaces and re-
luctance of users to spend too much time entering
their message, a great amount of text is generated
in a format that lacks the diacritic marks normally
used in the orthography of the language the text
is written in. Whatever the causes for the missing
accents are, NLP applications should be able to re-
store or generate the accented version of such texts
prior to any further syntactic or semantic process-
ing to avoid upstream errors.
In this paper, we aim at solving the problem of
restoring accents in Hungarian texts with the com-
bined application of a statistical machine transla-
tion system and a morphological analyzer. Our
method can be applied to any other languages that
have an accurate morphological analyzer.
</bodyText>
<sectionHeader confidence="0.999718" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999928354166667">
For Hungarian, there have been some attempts
at creating accent restoration systems. Zaink´o et
al. (2000) and Mihalcea and Nastase (2002) are
examples for ML approaches, where the correct
places of diacritics are predicted from the immedi-
ate grapheme-level context of the unaccented let-
ter with an accuracy of 95%. Thus, unseen words
can also be accented, but incorrect forms may also
be introduced into the text. Dictionary-based ap-
proaches rely on large text corpora and the distri-
bution of the different accented forms. Zaink´o et
al. (2000) report to have achieved a performance
of 98% of accuracy with their dictionary-based
method. Nevertheless, their system cannot rec-
ognize unseen wordforms quite common in Hun-
garian. N´emeth et al. (2000) have implemented a
complex text processing system for TTS applica-
tions, applying morphological and syntactic anal-
ysis. The authors report that the performance of
accent restoration depends very much on the per-
formance of the analyzers (achieving 95% accu-
racy at best). Neither the implementations nor the
resources used in these systems have been made
publicly available.
A language-independent tool, Charlifter (Scan-
nell, 2011), is based on statistical methods relying
on a lexicon, a bigram contextual model and char-
acter distributions built from a training corpus. Its
performance on Hungarian with its pre-built mod-
els is compared to our results in Section 5.
For other languages, similar methods are used.
Yarowsky (1994) presents a comprehensive report
on corpus-based techniques used for French and
Spanish texts. The role of the context is empha-
sized in this report, however, both word form and
accent variations are relatively moderate in the
investigated languages compared to Hungarian.
The study of Zweigenbaum and Grabar (2002) is
also aiming at French, but in the medical domain,
which contains a higher ratio of unknown words
than general language. In their work, a tagging
method is applied in combination with transduc-
ers, resulting in a tag sequence corresponding to
each letter. The method is successfully (92% pre-
cision) applied to single headwords of a medical
thesaurus (without exploiting any context). The
most similar method to ours is that of Pham et
al. (2013), who also applied SMT in order to au-
</bodyText>
<page confidence="0.893874">
2286
</page>
<note confidence="0.6396675">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2286–2291,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999564333333333">
tomatically restore accents in Vietnamese texts.
In their case, the best results produced an accu-
racy of 93%. However, their system is augmented
with a dictionary, and the distribution of accents
and grammatical behaviour are also quite different
from Hungarian.
</bodyText>
<sectionHeader confidence="0.99608" genericHeader="method">
3 Hungarian
</sectionHeader>
<bodyText confidence="0.999179615384615">
Hungarian is an agglutinating language with an
orthography that represents compounds as single
word forms. These may result in rather complex
word forms and words are often composed of long
sequences of morphemes. Thus, agglutination and
compounding yield a huge number of different
word forms.
In Hungarian, umlauts and acute accents are
used as diacritics for vowels. Acute accents mark
long vowels, while umlauts are used to indicate the
frontness of rounded vowels o→¨o [o→ø] and u→¨u
[u→y], like in German. A combination of acutes
and umlauts is the double acute diacritic to mark
long front rounded vowels o˝ [ø:] and u˝ [y:]. Long
vowels generally have essentially the same quality
as their short counterpart (i-i, ¨u-˝u, u-´u, ¨o-˝o, o-´o).
The long pairs of the low vowels a [O] and e [E], on
the other hand, also differ in quality: a´ [a:] and e´
[e:]. There are a few lexicalized cases where there
is a free variation of vowel length without distin-
guishing meaning, e.g. hova∼hov´a ‘where to’. In
most cases, however, the meaning of differently
accented variants of a word is quite different. Ta-
ble 1 shows all the possible unaccented-accented
pairs of vowels in Hungarian together with their
distribution in a corpus of 1804 252 tokens.
</bodyText>
<table confidence="0.9159444">
a a: 70.33%; ´a: 29.66%
e e: 73.40% ´e: 26.59%
i i: 86.04% i: 13.95%
o o: 55.41% ´o: 14.65% ¨o: 15.82% ˝o: 14.10%
u u: 46.96% ´u: 12.72% ¨u: 29.98% ˝u: 10.32%
</table>
<tableCaption confidence="0.99979">
Table 1: Possible accent variations in Hungarian
</tableCaption>
<sectionHeader confidence="0.98522" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.9999298">
In this research, we considered the problem of ac-
cent restoration as a translation task, where the
source language is the unaccented version, and the
target language is accented Hungarian. Since it is
easy to come up with a parallel training corpus for
this task, methods of SMT can be applied.
In our experiments, we used Moses (Koehn et
al., 2007), a widely used SMT toolkit for build-
ing the translation models and performing decod-
ing, and SRILM (Stolcke et al., 2011) to build the
necessary language models. Moses was used with
its default configuration settings and monotone de-
coding (i.e. reordering was not allowed), and with-
out the alignment step, which was not needed in
our case.
</bodyText>
<subsectionHeader confidence="0.999035">
4.1 The baseline setup
</subsectionHeader>
<bodyText confidence="0.999988">
In the baseline setup, only the translation and lan-
guage models built from the training corpus were
used. The input for the decoder was Hungar-
ian raw texts with all the accents removed. The
translation model contained only unigram phrases
(larger n-grams were also tried, but did not change
the results) and the language model contained
phrases up to 5 grams. Thus, the translation model
was responsible for predicting the distribution of
accented forms and the language model exploited
contextual information.
Another baseline was also created in order to
monitor the effect of the SMT system. In this sec-
ond baseline, each unaccented word form was re-
placed by its most frequent accented form in the
training set.
</bodyText>
<subsectionHeader confidence="0.997317">
4.2 Incorporating a morphological analyzer
</subsectionHeader>
<bodyText confidence="0.999985391304348">
In order to be able to restore accents in unseen
words as well, a Hungarian morphological an-
alyzer (Pr´osz´eky and Kis, 1999; Nov´ak, 2003)
was integrated. A special version of the analyzer
was created that directly maps unaccented word
forms to their possible accented variants while
also marking morpheme boundaries and adding
morphosyntactic category tags. The segmenta-
tion marks (e.g. compound and derivational suf-
fix boundaries) and the tags are used when we as-
sign a score to the accented candidates. We also
reanalyze accented forms to retrieve lemmas not
directly returned by the accenting analyzer. In our
test set of 1804 252 tokens, about 1% of the words
were not found in the translation model even in the
case of the largest, 440 million words, training set.
Table 2 shows the ratio of unknown words (OOV)
as a function of the size of the training set used for
building the phrase table.
For these unknown words, all possible correct
accented candidates were generated by the mor-
phological analyzer. These candidates were then
fed to the Moses decoder using its -xml-input
</bodyText>
<page confidence="0.977392">
2287
</page>
<table confidence="0.999440666666667">
train sentences M words OOV in test
100K 100000 1.738 9.63%
1000K 1000 000 18.078 3.44%
5000K 5 000 000 89.907 1.23%
10M 10 000 000 180.644 1.68%
ALL 24 048 302 437.559 0.81%
</table>
<tableCaption confidence="0.874458">
Table 2: Ratio of OOV after building a translation
model from a training set of a certain size
</tableCaption>
<bodyText confidence="0.999176333333333">
parameter. In order to be able to use this feature
of the decoder, a probability for each candidate
form had to be estimated. First, we assumed uni-
form distribution among the candidates. However,
this approach assigned the same probability to the
most common and the most nonsensical (although
grammatical) candidates as well. Thus, in some
cases these forms showed up in the results. In
order to avoid the system to make such errors, a
more sophisticated distribution was estimated for
the candidate set. For this, we applied a linear
regression model based on corpus frequency data
determined for the lemma and other features of the
candidate word (since the actual wordform was
not present in the corpus). Thus, for each candi-
date, its lemma frequency (LEM), the number of
productively applied compounding (CMP), the
number of productively applied derivational af-
fixes (DER), and the frequency of the inflectional
suffix sequence returned by the analysis were de-
termined. Compounding and derivation were pe-
nalized (i.e. they were given a negative sign), be-
cause the morphological analyzer could suggest
some nonsensical, though grammatical compound
or derived forms. Sometimes such forms could
be the correct ones, but the more productive com-
pounding and derivation there is in a word, the
lower score it should get. On the other hand, the
frequencies of the lemma and the inflectional pat-
tern should increase the score of a candidate, thus
these components were given positive weights.
Based on these components, a score was assigned
to each candidate based on Formula (1).
</bodyText>
<equation confidence="0.988295833333333">
score = −Ac#CMP − Ad#DER (1)
+ log10LEM + Ailog10INF + MS
, where
�|minscore |+ 1 if minscore &lt; 0
MS = (2)
0 otherwise
</equation>
<bodyText confidence="0.997242068965517">
The MS component was used to scale up the
scores by adding |minscore |+ 1, i.e. the low-
est score received for any candidate in the actual
candidate set in order to evade negative scores.
The A weights were set by the mert tuning of
the Moses system. We used a separate develop-
ment set for this, on which we observed the distri-
bution of compounds, derivational and inflectional
suffixes in OOV words analyzed by the morphol-
ogy and from which we sampled 1000 words ap-
proximating the observed distributions. The target
of the optimization in the mert tuning was the ac-
curacy of the system on these words, resulting in
the optimal values for each A. Even though, in lin-
ear regression, it is standard to use an additional
bias weight, we did not find it necessary, because
we did not need to bring our estimates in sync
with estimates from other sources. And assuming
one factor to have a fixed unit value was just an-
other simplification that would not affect the over-
all ranking, just its scaling.
Even though, following an appropriate scaling
of the scores, the ranked candidates could be used
the same way as the entries in the translation table,
the system would never select any accented form
other than the most probable one, since the lan-
guage model does not include any of these forms.
Thus, only the candidate with the highest relative
score was made available to the system.
</bodyText>
<sectionHeader confidence="0.981414" genericHeader="method">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.999800857142857">
In our experiments, the Hungarian webcorpus
(Hal´acsy et al., 2004) was used for training and
testing purposes. A set of 100 000 sentences were
separated from the corpus as the test set, and an-
other 100 000 sentences were used as a develop-
ment set. The rest were used for training in differ-
ent settings. The size of each training set is shown
in Table 2.
We evaluated the performance on all the
1804 252 tokens of the test set (56.84% correct
without accent) and on a subset of 1472 200 words
that included any vowels (47.09% correct without
accent). The experiments were then performed for
the baseline system using the most frequent form
(BL-FREQ), for the baseline SMT system (BL-
SMT) and for the one augmented by the morphol-
ogy with the first-ranked candidate (RANK). Ta-
ble 3 shows the detailed results for the smallest
and largest training sets for all words (ALL) and
for words that include vowels (VOWEL). It can
be seen that the precision of the system is only
</bodyText>
<page confidence="0.979084">
2288
</page>
<table confidence="0.626604625">
100K ALL
system prec rec acc prec rec acc
BL-FREQ-ALL 98.25 82.82 92.34 98.37 96.26 98.13
BL-FREQ-VOW 98.25 82.82 90.62 98.37 96.26 97.71
BL-SMT-ALL 99.03 83.88 92.91 99.09 97.36 98.72
BL-SMT-VOW 99.03 83.88 91.31 99.09 97.36 98.44
RANK-ALL 98.81 98.08 98.99 99.01 98.56 99.23
RANK-VOW 98.82 98.08 98.77 99.02 98.56 99.06
</table>
<tableCaption confidence="0.594082">
Table 3: Performance results for each experimen-
tal settings and training size
</tableCaption>
<figure confidence="0.993490181818182">
accuracy
100
98
RANK-VOWEL
BL-SMT-VOWEL
BL-FREQ-VOWEL
96
94
92
90
5 6 7 8 9
</figure>
<bodyText confidence="0.99962103125">
slightly improved when increasing the size of the
training corpus, but the values of recall and accu-
racy do dramatically improve in the case of the
baseline system. However, the integration of the
suggestions of the morphology can make up for
the lack of information due to the small training set
improving recall a great deal while only slightly
affecting precision. Even for the biggest 437.6M-
word training corpus, incorporating the morpho-
logical analyzer with ranking yielded a relative er-
ror rate reduction of 39.74%, reducing the word
error rate from 1.56% to 0.94%. For the small-
est 1.74M-word training corpus tested, the relative
error rate reduction was 85.85%. The system in-
cluding the morphological analyzer performs bet-
ter even with the smallest training corpus in terms
of word accuracy than the baseline Moses sys-
tem with the biggest corpus. Figure 1 shows the
learning curves for each system with accuracy as
a function of training set size.
Comparing our results to those we obtained
using Charlifter (89.75% with most frequent ac-
cented form baseline, 90.00% with the lexicon-
lookup+bigram contextual model and 93.31%
with lookup+bigram context+character-n-gram-
based model), the results reveal that both the con-
textual model in the SMT system improves accu-
racy better than the bigram context model of Char-
lifter, and the performance boost we get by incor-
porating morphology vastly exceeds the accuracy
improvement yielded by the incorporation of the
character-n-gram-based model used in Charlifter.
</bodyText>
<sectionHeader confidence="0.990835" genericHeader="method">
6 Error analysis
</sectionHeader>
<bodyText confidence="0.7150346">
We performed a detailed error analysis on a 5000-
sentence (87786-token) fragment of the test set.
The results of the error analysis are presented in
Table 4.
training set 101 words
</bodyText>
<figureCaption confidence="0.842259">
Figure 1: Accuracy as a function of the size of the
training set for each system, measured on words
containing vowels.
</figureCaption>
<bodyText confidence="0.999873129032258">
The detailed analysis showed that 14.7% of mis-
matches between the original and the system out-
put is in fact not due to the latter being erroneous.
3.55% are equivalent forms , while the rest is cor-
rect in the output and erroneous in the reference,
i.e. the system corrected errors in the original.
Another part of the reference (17.91%) is like-
wise erroneous, however, since the error in these
cases was not in the accents, the system was not
able to correct it. Missing or substituted letters are
the most common mistakes (10.81%), and further
6.42% of the errors is due to punctuation errors in
the original.
About 2/3 of the mismatches are real errors.
5.57% of these could be attributed to the stem of
the word missing from the database of the morpho-
logical analyzer. In 3.55% of the cases, the sys-
tem transforms a name to a more frequent word:
sometimes to another name, but more often to
some common frequent word. A similar case
is when some common noun is transformed to a
more frequent name (another 1.35%). The num-
ber of these errors could be reduced to some ex-
tent by making the system rely on case informa-
tion (in the case of some proper name-common
noun ambiguities), however this could make the
system perform worse elsewhere due to increased
data sparseness. 2.20% of the errors is due to er-
rors in the training corpus. Since rare word forms
are quite frequent in Hungarian, the chances are
high that a specific form is more often mistyped
</bodyText>
<page confidence="0.982024">
2289
</page>
<table confidence="0.999619818181818">
Mismatch type Ratio Examples
Output correct 14.70%
Equivalent forms 3.55% l´ev˝o--+lev˝o fele--+fel´e ´ah´a--+aha perif´erikus--+periferikus
Corrected erroneous name 1.01% US ´A-ban--+USA-ban Sz´oladon--+Sz´ol´adon
Other corrected erroneous 10.14% un.--+´un. kolleg´ank--+koll´eg´ank lejto--+lejt˝o lathato--+l´athat´o
Real errors 67.40%
Missing from MA 5.57% hemokromat´ozis-g´en--+hemokromatozis-gen
Correct name to erroneous output 3.55% MIG--+m´ıg B¨osz--+B˝osz Ladd--+L´add M´arton--+Marton
Other correct original to some erroneous form 2.20% meg˝orz´est--+megorz´est router´ehez--+routerehez
Other correct original to contextually inade- 1.35% log´o--+logo eperjeskein--+eperjesk´ein
quate name
Other correct original to some contextually in- 51.01% m´eg--+meg termek--+term´ek g´ep´et--+g´epet c´ım´et--+c´ımet
adequate form v´agy´ok--+vagyok ´erm´eket--+´ermeket k´ep´e--+k´epe
Original is a filename or a url containing accents 3.72% latok--+l´atok viz--+v´ız szantok--+sz´ant´ok telepok--+telep´ok
felhaszn´al´o@profinter.hu--+felhasznalo@profinter.hu
www.valamic´eg.hu--+www.valamiceg.hu
Uncorrected error in original 17.91%
Punctuation error in original 6.42% k¨ozalk.tan--+kozalk.tan 1922.´evi--+1922.evi
Hyphenation error in original 0.68% bemuta-t´asra--+bemuta-tasra
Other error in original 10.81% v´eri--+veri ra--+r´a gonolkoz´as´aban--+gonolkozasaban
im´atkoztok--+imatkoztok hir´ujs´asghoz--+hirujsasghoz
v´altozaban--+v´alt´ozab´an k¨ornyezetk´ım´eli--+kornyezetkimeli
</table>
<tableCaption confidence="0.999565">
Table 4: Analysis of mismatches between the system output and the input on a 5000-sentence test sample
</tableCaption>
<bodyText confidence="0.999977323529412">
than not (this is especially true for word forms that
occur only once in the training data). 3.72% of the
errors in the analyzed test data was due to either
transforming arbitrary unaccented letter sequences
used as file names in the text being transformed to
some meaningful words or to accented words be-
ing used in an url in the original text.
The most common error (51.01% of all mis-
matches) is the case where the system is sim-
ply unable to correctly disambiguate the word
in context, and this is not due to some other
error or information loss. Interestingly, more
than half (51%) of these errors belong to a sin-
gle type where the system is unable to distin-
guish a possessive and a non-possessive form of
the same nominal lemma: gyereket—gyerek´et ‘the
child (accusative)’ vs.‘his/her child (accusative)’,
gyereken—gyerek´en ‘on/about the child’ vs.
‘on/about his/her child’, and gyereke—gyerek´e
‘his/her child’ vs.‘(belongs to a) child’ (anaphoric
possessive).
Another 26% of the mismatches is due to a sim-
ilar problem concerning verbs. In Hungarian, tran-
sitive verbs agree with their object in definiteness.
Certain past, present and conditional verb forms
differing in definiteness are only distinguished by
an accent: hajtottak—hajtott´ak ‘they drove’ vs.
‘they drove it’; hajtanak—hajtan´ak ‘they drive’
vs. ‘they would drive it’; hajtana—hajtan´a ‘he/she
would drive’ vs. ‘he/she would drive it’.
A factored model could in theory improve the
recognition of these structures. It is questionable
however, whether the improvement would justify
the costs.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999884">
We have described a method to restore accents in
Hungarian texts. The baseline method using only
a fixed training corpus to build translation and lan-
guage models for a statistical machine translation
system, which is limited to handling word forms
present in the training corpus achieved an accu-
racy of 98.44% at best. In order to process un-
known words, a morphological analyzer was in-
tegrated to produce accented candidates for these
unknown words as well, resulting in an improved
accuracy of 99.06%. This performance could only
be achieved by a system that is able to produce
correct word forms and takes context into account.
Our method can be applied to any other languages
for which a training corpus and a morphological
analyzer are available.
</bodyText>
<page confidence="0.98198">
2290
</page>
<sectionHeader confidence="0.989785" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999864938461538">
P´eter Hal´acsy, Andr´as Kornai, L´aszl´o N´emeth, Andr´as
Rung, Istv´an Szakad´at, and Viktor Tr´on. 2004. Cre-
ating open language resources for hungarian. In
LREC. European Language Resources Association.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceed-
ings of the ACL 2007 Demo and Poster Sessions,
pages 177–180, Prague. Association for Computa-
tional Linguistics.
Rada Mihalcea and Vivi Nastase. 2002. Letter level
learning for language independent diacritics restora-
tion. In Proceedings of the 6th Conference on Nat-
ural Language Learning - Volume 20, COLING-02,
pages 1–7, Stroudsburg, PA, USA. Association for
Computational Linguistics.
G´eza N´emeth, Csaba Zaink´o, L´aszl´o Fekete, G´abor
Olaszy, G´abor Endr´edi, P´eter Olaszi, G´eza Kiss, and
P´eter Kis. 2000. The design, implementation, and
operation of a Hungarian e-mail reader. Interna-
tional Journal of Speech Technology, 3(3-4):217–
236.
Attila Nov´ak. 2003. What is good Humor like? [Mi-
lyen a j´o Humor?]. In I. Magyar Sz´amit´og´epes
Nyelv´eszeti Konferencia, pages 138–144, Szeged.
SZTE.
Luan-Nghia Pham, Viet-Hong Tran, and Vinh-Van
Nguyen. 2013. Vietnamese text accent restora-
tion with statistical machine translation. In Proceed-
ings of the 27th Pacific Asia Conference on Lan-
guage, Information, and Computation (PACLIC 27),
pages 423–429. Department of English, National
Chengchi University.
G´abor Pr´osz´eky and Bal´azs Kis. 1999. A unification-
based approach to morpho-syntactic parsing of ag-
glutinative and other (highly) inflectional languages.
In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, ACL ’99, pages 261–268,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Kevin P. Scannell. 2011. Statistical unicodification of
african languages. Language Resources and Evalu-
ation, 45(3):375–386.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and out-
look. In Proc. IEEE Automatic Speech Recognition
and Understanding Workshop, Waikoloa, Hawaii,
December.
D. Yarowsky. 1994. A comparison of corpus-
based techniques for restoring accents in spanish and
french text. In Proceedings of the 2nd Annual Work-
shop on Very Large Text Corpora, pages 19—-32,
Las Cruces.
Cs. Zaink´o, G. N´emeth, G. Olaszy, and G. Gor-
dos. 2000. Elj´ar´as adott nyelven ´ekezetes
bet˝uk haszn´alata n´elk¨ul k´esz´ıtett sz¨ovegek ´ekezetes
bet˝uinek vissza´all´ıt´as´ara.
Pierre Zweigenbaum and Natalia Grabar. 2002. Ac-
centing unknown words in a specialized language.
In Stephen Johnson, editor, ACL Workshop on Natu-
ral Language Processing in the Biomedical Domain,
pages 21–28. ACL.
</reference>
<page confidence="0.991339">
2291
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.597909">
<title confidence="0.994441333333333">Automatic Diacritics Restoration for Hungarian and Borb´ala Hungarian Language Technology Research</title>
<author confidence="0.979244">P´eter Catholic University</author>
<author confidence="0.979244">Faculty of Information Technology</author>
<note confidence="0.714399">50/a Pr´ater street, 1083 Budapest,</note>
<abstract confidence="0.989724928571429">In this paper, we describe a method based on statistical machine translation (SMT) that is able to restore accents in Hungarian texts with high accuracy. Due to the agglutination in Hungarian, there are always plenty of word forms unknown to a system trained on a fixed vocabulary. In order to be able to handle such words, we integrated a morphological analyzer into the system that can suggest accented word candidates for unknown words. We evaluated the system in different setups, achieving an accuracy above 99% at the highest.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P´eter Hal´acsy</author>
<author>Andr´as Kornai</author>
<author>L´aszl´o N´emeth</author>
<author>Andr´as Rung</author>
<author>Istv´an Szakad´at</author>
<author>Viktor Tr´on</author>
</authors>
<title>Creating open language resources for hungarian. In LREC. European Language Resources Association.</title>
<date>2004</date>
<marker>Hal´acsy, Kornai, N´emeth, Rung, Szakad´at, Tr´on, 2004</marker>
<rawString>P´eter Hal´acsy, Andr´as Kornai, L´aszl´o N´emeth, Andr´as Rung, Istv´an Szakad´at, and Viktor Tr´on. 2004. Creating open language resources for hungarian. In LREC. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<institution>Prague. Association for Computational Linguistics.</institution>
<contexts>
<context position="6216" citStr="Koehn et al., 2007" startWordPosition="999" endWordPosition="1002">h their distribution in a corpus of 1804 252 tokens. a a: 70.33%; ´a: 29.66% e e: 73.40% ´e: 26.59% i i: 86.04% i: 13.95% o o: 55.41% ´o: 14.65% ¨o: 15.82% ˝o: 14.10% u u: 46.96% ´u: 12.72% ¨u: 29.98% ˝u: 10.32% Table 1: Possible accent variations in Hungarian 4 Method In this research, we considered the problem of accent restoration as a translation task, where the source language is the unaccented version, and the target language is accented Hungarian. Since it is easy to come up with a parallel training corpus for this task, methods of SMT can be applied. In our experiments, we used Moses (Koehn et al., 2007), a widely used SMT toolkit for building the translation models and performing decoding, and SRILM (Stolcke et al., 2011) to build the necessary language models. Moses was used with its default configuration settings and monotone decoding (i.e. reordering was not allowed), and without the alignment step, which was not needed in our case. 4.1 The baseline setup In the baseline setup, only the translation and language models built from the training corpus were used. The input for the decoder was Hungarian raw texts with all the accents removed. The translation model contained only unigram phrase</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180, Prague. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Vivi Nastase</author>
</authors>
<title>Letter level learning for language independent diacritics restoration.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning - Volume 20, COLING-02,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1791" citStr="Mihalcea and Nastase (2002)" startWordPosition="280" endWordPosition="283">issing accents are, NLP applications should be able to restore or generate the accented version of such texts prior to any further syntactic or semantic processing to avoid upstream errors. In this paper, we aim at solving the problem of restoring accents in Hungarian texts with the combined application of a statistical machine translation system and a morphological analyzer. Our method can be applied to any other languages that have an accurate morphological analyzer. 2 Related work For Hungarian, there have been some attempts at creating accent restoration systems. Zaink´o et al. (2000) and Mihalcea and Nastase (2002) are examples for ML approaches, where the correct places of diacritics are predicted from the immediate grapheme-level context of the unaccented letter with an accuracy of 95%. Thus, unseen words can also be accented, but incorrect forms may also be introduced into the text. Dictionary-based approaches rely on large text corpora and the distribution of the different accented forms. Zaink´o et al. (2000) report to have achieved a performance of 98% of accuracy with their dictionary-based method. Nevertheless, their system cannot recognize unseen wordforms quite common in Hungarian. N´emeth et </context>
</contexts>
<marker>Mihalcea, Nastase, 2002</marker>
<rawString>Rada Mihalcea and Vivi Nastase. 2002. Letter level learning for language independent diacritics restoration. In Proceedings of the 6th Conference on Natural Language Learning - Volume 20, COLING-02, pages 1–7, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G´eza N´emeth</author>
<author>Csaba Zaink´o</author>
<author>L´aszl´o Fekete</author>
<author>G´abor Olaszy</author>
<author>G´abor Endr´edi</author>
<author>P´eter Olaszi</author>
<author>G´eza Kiss</author>
<author>P´eter Kis</author>
</authors>
<title>The design, implementation, and operation of a Hungarian e-mail reader.</title>
<date>2000</date>
<journal>International Journal of Speech Technology,</journal>
<pages>3--3</pages>
<marker>N´emeth, Zaink´o, Fekete, Olaszy, Endr´edi, Olaszi, Kiss, Kis, 2000</marker>
<rawString>G´eza N´emeth, Csaba Zaink´o, L´aszl´o Fekete, G´abor Olaszy, G´abor Endr´edi, P´eter Olaszi, G´eza Kiss, and P´eter Kis. 2000. The design, implementation, and operation of a Hungarian e-mail reader. International Journal of Speech Technology, 3(3-4):217– 236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attila Nov´ak</author>
</authors>
<title>What is good Humor like? [Milyen a j´o Humor?]. In I. Magyar Sz´amit´og´epes Nyelv´eszeti Konferencia,</title>
<date>2003</date>
<pages>138--144</pages>
<location>Szeged. SZTE.</location>
<marker>Nov´ak, 2003</marker>
<rawString>Attila Nov´ak. 2003. What is good Humor like? [Milyen a j´o Humor?]. In I. Magyar Sz´amit´og´epes Nyelv´eszeti Konferencia, pages 138–144, Szeged. SZTE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luan-Nghia Pham</author>
<author>Viet-Hong Tran</author>
<author>Vinh-Van Nguyen</author>
</authors>
<title>Vietnamese text accent restoration with statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation (PACLIC 27),</booktitle>
<pages>423--429</pages>
<institution>Department of English, National Chengchi University.</institution>
<marker>Pham, Tran, Vinh-Van Nguyen, 2013</marker>
<rawString>Luan-Nghia Pham, Viet-Hong Tran, and Vinh-Van Nguyen. 2013. Vietnamese text accent restoration with statistical machine translation. In Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation (PACLIC 27), pages 423–429. Department of English, National Chengchi University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G´abor Pr´osz´eky</author>
<author>Bal´azs Kis</author>
</authors>
<title>A unificationbased approach to morpho-syntactic parsing of agglutinative and other (highly) inflectional languages.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>261--268</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Pr´osz´eky, Kis, 1999</marker>
<rawString>G´abor Pr´osz´eky and Bal´azs Kis. 1999. A unificationbased approach to morpho-syntactic parsing of agglutinative and other (highly) inflectional languages. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 261–268, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Scannell</author>
</authors>
<title>Statistical unicodification of african languages.</title>
<date>2011</date>
<journal>Language Resources and Evaluation,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="2830" citStr="Scannell, 2011" startWordPosition="443" endWordPosition="445">chieved a performance of 98% of accuracy with their dictionary-based method. Nevertheless, their system cannot recognize unseen wordforms quite common in Hungarian. N´emeth et al. (2000) have implemented a complex text processing system for TTS applications, applying morphological and syntactic analysis. The authors report that the performance of accent restoration depends very much on the performance of the analyzers (achieving 95% accuracy at best). Neither the implementations nor the resources used in these systems have been made publicly available. A language-independent tool, Charlifter (Scannell, 2011), is based on statistical methods relying on a lexicon, a bigram contextual model and character distributions built from a training corpus. Its performance on Hungarian with its pre-built models is compared to our results in Section 5. For other languages, similar methods are used. Yarowsky (1994) presents a comprehensive report on corpus-based techniques used for French and Spanish texts. The role of the context is emphasized in this report, however, both word form and accent variations are relatively moderate in the investigated languages compared to Hungarian. The study of Zweigenbaum and G</context>
</contexts>
<marker>Scannell, 2011</marker>
<rawString>Kevin P. Scannell. 2011. Statistical unicodification of african languages. Language Resources and Evaluation, 45(3):375–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>SRILM at sixteen: Update and outlook.</title>
<date>2011</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding Workshop,</booktitle>
<location>Waikoloa, Hawaii,</location>
<contexts>
<context position="6337" citStr="Stolcke et al., 2011" startWordPosition="1020" endWordPosition="1023">5% o o: 55.41% ´o: 14.65% ¨o: 15.82% ˝o: 14.10% u u: 46.96% ´u: 12.72% ¨u: 29.98% ˝u: 10.32% Table 1: Possible accent variations in Hungarian 4 Method In this research, we considered the problem of accent restoration as a translation task, where the source language is the unaccented version, and the target language is accented Hungarian. Since it is easy to come up with a parallel training corpus for this task, methods of SMT can be applied. In our experiments, we used Moses (Koehn et al., 2007), a widely used SMT toolkit for building the translation models and performing decoding, and SRILM (Stolcke et al., 2011) to build the necessary language models. Moses was used with its default configuration settings and monotone decoding (i.e. reordering was not allowed), and without the alignment step, which was not needed in our case. 4.1 The baseline setup In the baseline setup, only the translation and language models built from the training corpus were used. The input for the decoder was Hungarian raw texts with all the accents removed. The translation model contained only unigram phrases (larger n-grams were also tried, but did not change the results) and the language model contained phrases up to 5 grams</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at sixteen: Update and outlook. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop, Waikoloa, Hawaii, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>A comparison of corpusbased techniques for restoring accents in spanish and french text.</title>
<date>1994</date>
<booktitle>In Proceedings of the 2nd Annual Workshop on Very Large Text Corpora,</booktitle>
<pages>pages</pages>
<location>Las Cruces.</location>
<contexts>
<context position="3128" citStr="Yarowsky (1994)" startWordPosition="492" endWordPosition="493">alysis. The authors report that the performance of accent restoration depends very much on the performance of the analyzers (achieving 95% accuracy at best). Neither the implementations nor the resources used in these systems have been made publicly available. A language-independent tool, Charlifter (Scannell, 2011), is based on statistical methods relying on a lexicon, a bigram contextual model and character distributions built from a training corpus. Its performance on Hungarian with its pre-built models is compared to our results in Section 5. For other languages, similar methods are used. Yarowsky (1994) presents a comprehensive report on corpus-based techniques used for French and Spanish texts. The role of the context is emphasized in this report, however, both word form and accent variations are relatively moderate in the investigated languages compared to Hungarian. The study of Zweigenbaum and Grabar (2002) is also aiming at French, but in the medical domain, which contains a higher ratio of unknown words than general language. In their work, a tagging method is applied in combination with transducers, resulting in a tag sequence corresponding to each letter. The method is successfully (</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. A comparison of corpusbased techniques for restoring accents in spanish and french text. In Proceedings of the 2nd Annual Workshop on Very Large Text Corpora, pages 19—-32, Las Cruces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G N´emeth Zaink´o</author>
<author>G Olaszy</author>
<author>G Gordos</author>
</authors>
<title>Elj´ar´as adott nyelven ´ekezetes bet˝uk haszn´alata n´elk¨ul k´esz´ıtett sz¨ovegek ´ekezetes bet˝uinek vissza´all´ıt´as´ara.</title>
<date>2000</date>
<marker>Zaink´o, Olaszy, Gordos, 2000</marker>
<rawString>Cs. Zaink´o, G. N´emeth, G. Olaszy, and G. Gordos. 2000. Elj´ar´as adott nyelven ´ekezetes bet˝uk haszn´alata n´elk¨ul k´esz´ıtett sz¨ovegek ´ekezetes bet˝uinek vissza´all´ıt´as´ara.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Zweigenbaum</author>
<author>Natalia Grabar</author>
</authors>
<title>Accenting unknown words in a specialized language. In</title>
<date>2002</date>
<booktitle>ACL Workshop on Natural Language Processing in the Biomedical Domain,</booktitle>
<pages>21--28</pages>
<editor>Stephen Johnson, editor,</editor>
<publisher>ACL.</publisher>
<contexts>
<context position="3442" citStr="Zweigenbaum and Grabar (2002)" startWordPosition="538" endWordPosition="541"> (Scannell, 2011), is based on statistical methods relying on a lexicon, a bigram contextual model and character distributions built from a training corpus. Its performance on Hungarian with its pre-built models is compared to our results in Section 5. For other languages, similar methods are used. Yarowsky (1994) presents a comprehensive report on corpus-based techniques used for French and Spanish texts. The role of the context is emphasized in this report, however, both word form and accent variations are relatively moderate in the investigated languages compared to Hungarian. The study of Zweigenbaum and Grabar (2002) is also aiming at French, but in the medical domain, which contains a higher ratio of unknown words than general language. In their work, a tagging method is applied in combination with transducers, resulting in a tag sequence corresponding to each letter. The method is successfully (92% precision) applied to single headwords of a medical thesaurus (without exploiting any context). The most similar method to ours is that of Pham et al. (2013), who also applied SMT in order to au2286 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2286–2291, Lisbon</context>
</contexts>
<marker>Zweigenbaum, Grabar, 2002</marker>
<rawString>Pierre Zweigenbaum and Natalia Grabar. 2002. Accenting unknown words in a specialized language. In Stephen Johnson, editor, ACL Workshop on Natural Language Processing in the Biomedical Domain, pages 21–28. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
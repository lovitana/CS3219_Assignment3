<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997646">
Generalized Agreement for Bidirectional Word Alignment
</title>
<author confidence="0.998112">
Chunyang Liu†, Yang Liu†∗, Huanbo Luan†, Maosong Sun†, and Heng Yu‡† State Key Laboratory of Intelligent Technology and Systems
</author>
<affiliation confidence="0.9860835">
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
</affiliation>
<address confidence="0.632247">
‡ Samsung R&amp;D Institute of China, Beijing 100028, China
</address>
<email confidence="0.9724855">
{liuchunyang2012,liuyang.china,luanhuanbo}@gmail.com, sms@tsinghua.edu.cn
h0517.yu@samsung.com
</email>
<sectionHeader confidence="0.997163" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99944565">
While agreement-based joint training has
proven to deliver state-of-the-art align-
ment accuracy, the produced word align-
ments are usually restricted to one-to-
one mappings because of the hard cons-
traint on agreement. We propose a ge-
neral framework to allow for arbitrary loss
functions that measure the disagreement
between asymmetric alignments. The
loss functions can not only be defined
between asymmetric alignments but al-
so between alignments and other latent
structures such as phrase segmentations.
We use a Viterbi EM algorithm to train
the joint model since the inference is
intractable. Experiments on Chinese-
English translation show that joint training
with generalized agreement achieves sig-
nificant improvements over two state-of-
the-art alignment methods.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999897058823529">
Word alignment is a natural language process-
ing task that aims to specify the correspondence
between words in two languages (Brown et al.,
1993). It plays an important role in statistical
machine translation (SMT) as word-aligned bi-
lingual corpora serve as the input of translation
rule extraction (Koehn et al., 2003; Chiang, 2007;
Galley et al., 2006; Liu et al., 2006).
Although state-of-the-art generative alignment
models (Brown et al., 1993; Vogel et al., 1996)
have been widely used in practical SMT systems,
they fail to model the symmetry of word align-
ment. While word alignments in real-world bi-
lingual data usually exhibit complicated mappings
(i.e., mixed with one-to-one, one-to-many, many-
to-one, and many-to-many links), these models as-
sume that each target word is aligned to exactly
</bodyText>
<note confidence="0.702185">
∗Corresponding author: Yang Liu.
</note>
<bodyText confidence="0.99889825">
one source word. To alleviate this problem, heuris-
tic methods (e.g., grow-diag-final) have been pro-
posed to combine two asymmetric alignments
(source-to-target and target-to-source) to generate
symmetric bidirectional alignments (Och and Ney,
2003; Koehn and Hoang, 2007).
Instead of using heuristic symmetrization,
Liang et al. (2006) introduce a principled
approach that encourages the agreement between
asymmetric alignments in two directions. The
basic idea is to favor links on which both uni-
directional models agree. They associate two
models via the agreement constraint and show that
agreement-based joint training improves align-
ment accuracy significantly.
However, enforcing agreement in joint training
faces a major problem: the two models are restrict-
ed to one-to-one alignments (Liang et al., 2006).
This significantly limits the translation accuracy,
especially for distantly-related language pairs such
as Chinese-English (see Section 5). Although pos-
terior decoding can potentially address this prob-
lem, Liang et al. (2006) find that many-to-many
alignments occur infrequently because posteriors
are sharply peaked around the Viterbi alignments.
We believe that this happens because their model
imposes a hard constraint on agreement: the two
models must share the same alignment when esti-
mating the parameters by calculating the products
of alignment posteriors (see Section 2).
In this work, we propose a general framework
for imposing agreement constraints in joint train-
ing of unidirectional models. The central idea is to
use the expectation of a loss function, which mea-
sures the disagreement between two models, to
replace the original probability of agreement. This
allows for many possible ways to quantify agree-
ment. Experiments on Chinese-English translation
show that our approach outperforms two state-of-
the-art baselines significantly.
</bodyText>
<page confidence="0.94543">
1828
</page>
<note confidence="0.979553285714286">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1828–1836,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
C4E China China China
‘s ‘s ‘s
head head head
of of of
state state state
will will will
attend attend attend
the the the
unofficial unofficial unofficial
2002 2002 2002
APEC APEC APEC
summit summit summit
</note>
<figure confidence="0.973047">
. . .
2002
峰。
会
组
织
将
加
峰。
会
经
合
中
国
2002
将
加
亚
太
组
织
经
合
元
首
中
国
非
正
式
亚
太
元
首
非
正
式
2002
峰
会
组
织
经
合
亚
太
非
正
式
中
国
元
首
将
加
E4C China China China
‘s ‘s ‘s
</figure>
<figureCaption confidence="0.801105181818182">
head head head
of of of
state state state
will will will
attend attend attend
the the the
unofficial unofficial unofficial
2002 2002 2002
APEC APEC APEC
summit summit summit
. . .
</figureCaption>
<figure confidence="0.951079666666667">
independent training joint training joint training
w/o agreement w/ agreement w/ generalized agreement
(a) (b) (c)
</figure>
<figureCaption confidence="0.8415675">
Figure 1: Comparison of (a) independent training without agreement, (b) joint training with agreement,
and (c) joint training with generalized agreement. Bold squares are gold-standard links and solid squares
are model predictions. The Chinese and English sentences are segmented into phrases in (c). Joint
training with agreement achieves a high precision but generally only produces one-to-one alignments. We
propose generalized agreement to account for not only the consensus between asymmetric alignments,
but also the conformity of alignments to other latent structures such as phrase segmentations.
</figureCaption>
<equation confidence="0.98361462962963">
2002
峰
会
组
织
经
合
亚
太
非
正
式
中
国
元
首
将
加
2002
峰。
会
组
织
将
加
峰。
会
经
合
中
国
中
国
将
加
亚
太
组
织
经
合
元
首
2002
元
首
亚
太
非
正
式
非
正
式
</equation>
<sectionHeader confidence="0.995638" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.997077">
2.1 Asymmetric Alignment Models
</subsectionHeader>
<bodyText confidence="0.9949192">
Given a source-language sentence e ≡ eI1 =
e1, ... , eI and a target-language sentence f ≡
fJ1 = f1, ... , fJ, a source-to-target translation
model (Brown et al., 1993; Vogel et al., 1996) can
be defined as
</bodyText>
<equation confidence="0.9875485">
P(f|e; θ1) = � P(f, a1|e; θ1) (1)
a1
</equation>
<bodyText confidence="0.9998326">
where a1 denotes the source-to-target alignment
and θ1 is the set of source-to-target translation
model parameters.
Likewise, the target-to-source translation model
is given by
</bodyText>
<equation confidence="0.9892165">
P(e|f; θ2) = � P(e,a2|f; θ2) (2)
a2
</equation>
<bodyText confidence="0.999183714285714">
where a2 denotes the target-to-source alignment
and θ2 is the set of target-to-source translation
model parameters.
Given a training set D = {(f(s), e(s))}Ss=1, the
two models are trained independently to maximize
the log-likelihood of the training data for each
direction, respectively:
</bodyText>
<equation confidence="0.992331666666667">
S
L(θ1) = log P(f(s)|e(s); θ1) (3)
s=1
S
L(θ2) = log P(e(s)|f(s); θ2) (4)
s=1
</equation>
<bodyText confidence="0.99995475">
One key limitation of these generative models
is that they are asymmetric: each target word
is restricted to be aligned to exactly one source
word (including the empty cept) in the source-
to-target direction and vice versa. This is un-
desirable because most real-world word align-
ments are symmetric, in which one-to-one, one-
to-many, many-to-one, and many-to-many links
are usually mixed. See Figure 1(a) for example.
Therefore, a number of heuristic symmetrization
methods such as intersection, union, and grow-
diag-final have been proposed to combine asym-
</bodyText>
<page confidence="0.978802">
1829
</page>
<bodyText confidence="0.9726515">
metric alignments (Och and Ney, 2003; Koehn and
Hoang, 2007).
</bodyText>
<subsectionHeader confidence="0.998275">
2.2 Alignment by Agreement
</subsectionHeader>
<bodyText confidence="0.99137375">
Rather than using heuristic symmetrization meth-
ods, Liang et al. (2006) propose a principled
approach to jointly training of the two models via
enforcing agreement:
</bodyText>
<equation confidence="0.998614">
�= � P(a1|f(s), e(s); 01) x
a1 a2
P(a2|e(s), f(s); 02) x
6(a1,a2) (6)
</equation>
<bodyText confidence="0.9994905">
Note that the last term in Eq. (6) is actually the
expected value of agreement:
</bodyText>
<equation confidence="0.999138285714286">
J(01, 02)
S
log P(f(s)|e(s); 01) +
s=1
log P(e(s)|f(s); 02) +
P(a|f(s), e(s); 01) x
P(a|e(s), f(s); 02) (5)
</equation>
<bodyText confidence="0.999886933333333">
Note that the last term in Eq. (5) encourages the
two models to agree on asymmetric alignments.
While this strategy significantly improves align-
ment accuracy, the joint model is prone to generate
one-to-one alignments because it imposes a hard
constraint on agreement: the two models must
share the same alignment when estimating the
parameters by calculating the products of align-
ment posteriors. In Figure 1(b), the two one-
to-one alignments are almost identical except for
one link. This makes the posteriors to be sharply
peaked around the Viterbi alignments (Liang et
al., 2006). As a result, the lack of many-to-many
alignments limits the benefits of joint training to
end-to-end machine translation.
</bodyText>
<sectionHeader confidence="0.99251" genericHeader="method">
3 Generalized Agreement for
</sectionHeader>
<subsectionHeader confidence="0.719672">
Bidirectional Alignment
</subsectionHeader>
<bodyText confidence="0.999851571428571">
Our intuition is that the agreement between two
alignments can be defined as a loss function,
which enables us to consider various ways of
quantification (Section 3.1) and even to incorpo-
rate the dependency between alignments and oth-
er latent structures such as phrase segmentations
(Section 3.2).
</bodyText>
<subsectionHeader confidence="0.999744">
3.1 Agreement between Word Alignments
</subsectionHeader>
<bodyText confidence="0.9999565">
The key idea of generalizing agreement is to lever-
age loss functions that measure the difference be-
tween two unidirectional alignments. For exam-
ple, the last term in Eq. (5) can be re-written as
</bodyText>
<equation confidence="0.979748">
� � J�
Ea1|f(3),e(3);θ1 Ea2|e(3),f(3);θ2 6(a1, a2)
(7)
</equation>
<bodyText confidence="0.9982078">
Our idea is to replace 6(a1, a2) in Eq. (6) with
an arbitrary loss function A(a1, a2) that measures
the difference between a1 and a2. This gives
the new joint training objective with generalized
agreement:
</bodyText>
<equation confidence="0.999484875">
J(01, 02)
S
log P(f(s)|e(s); 01) +
s=1
log P(e(s)|f(s); 02) −
P(a1|f(s), e(s); 01) x
P(a2|e(s), f(s); 02) x
A(a1, a2) (8)
</equation>
<bodyText confidence="0.966505">
Obviously, Liang et al. (2006)’s training objec-
tive is a special case of our framework. We refer
to its loss function as hard matching:
</bodyText>
<equation confidence="0.870097">
AHM(a1, a2) = 1 − 6(a1, a2) (9)
</equation>
<bodyText confidence="0.998704785714286">
We are interested in developing a soft version of
the hard matching loss function because this will
help to produce many-to-many symmetric align-
ments. For example, in Figure 1(c), the two align-
ments share most links but still allow for dis-
agreed links to capture one-to-many and many-to-
one links. Note that the union of the two asymmet-
ric alignments is almost the same with the gold-
standard alignment in this example.
While there are many possible ways to define
a soft matching loss function, we choose the dif-
ference between disagreed and agreed link counts
because it is easy and efficient to calculate during
search:
</bodyText>
<figure confidence="0.987690888888889">
�
log
a
�
log
�
a1 a2
� P(a|f(s), e(s); 01)P(a|e(s), f(s); 02) ASM(a1, a2) = |a1 U a2 |− 2|a1 n a2 |(10)
a
</figure>
<page confidence="0.685259">
1830
</page>
<table confidence="0.9711026">
C4E E4C
China B + China B
‘s I + ‘s I
head I + head I
of I + of I
state E + state E
will B - will B
attend E + attend E
the B + B
unofficial I - I
2002 I + I
APEC I + I
summit E + E
. 5 5
the
unofficial
2002
APEC
summit
.
</table>
<figure confidence="0.992778638888889">
B E B E B I I I I E 5
B E B E B I I I I E 5
+ - - + - - + + + +
2002
峰
会
组
织
经
合
亚
太
非
正
式
将 参
加
中
国
元
首
2002
峰
会
织组
亚
太
非
正
式
将 参
加
中
国
元
首
</figure>
<figureCaption confidence="0.783559833333333">
Figure 2: Generalized agreement between word alignments and phrase segmentations. The Chinese and
English sentences are segmented into phrases using B (beginning), I (internal), E (ending), S (single)
labels. We expect that word alignment does not violate the phrase segmentation. The word “unofficial”
in the C —* E alignment is labeled with “-” because “unofficial” and “2002” belong to the same English
phrase but their counterparts are separated in two Chinese phrases. Words that do not violate the phrase
alignment are labeled with “+”. See Section 3.2 for details.
</figureCaption>
<subsectionHeader confidence="0.997493">
3.2 Agreement between Word Alignments
and Phrase Segmentations
</subsectionHeader>
<bodyText confidence="0.999859714285714">
Our framework is very general and can be
extended to include the agreement between word
alignment and other latent structures such as
phrase segmentations.
The words in a Chinese sentence often con-
stitute phrases that are translated as units in
English and vice versa. Inspired by the alignment
consistency constraint widely used in translation
rule extraction (Koehn et al., 2003), we make
the following assumption to impose a structural
agreement constraint between word alignment and
phrase segmentation: source words in one source
phrase should be aligned to target words belong-
ing to the same target phrase and vice versa.
</bodyText>
<figureCaption confidence="0.762834833333333">
For example, consider the C —* E alignment in
Figure 2. We segment Chinese and English sen-
tences into phrases, which are sequences of con-
secutive words. Since “2002” and “APEC” belong
to the same English phrase, their counterparts on
the Chinese side should also belong to one phrase.
</figureCaption>
<bodyText confidence="0.998297444444445">
While this assumption can potentially improve
the correlation between word alignment and
phrase-based translation, a question naturally a-
rises: how to segment sentences into phrases?
Instead of leveraging chunking, we treat phrase
segmentation as a latent variable and train the
joint alignment and segmentation model from
unlabeled data in an unsupervised way.
Formally, given a target-language sentence f =
fJ1 = f1, ... , fJ, we introduce a latent variable
b = bJ1 = b1, ... , bJ to denote a phrase segmen-
tation. Each label bj E {B, I, E, 5}, where B
denotes the beginning word of a phrase, I denotes
the internal word, E denotes the ending word, and
5 denotes the one-word phrase. Figure 2 shows
the label sequences for the sentence pair.
We use a first-order HMM to model phrase seg-
mentation of a target sentence:
</bodyText>
<equation confidence="0.991071">
P(f; A1) = � P(f, b1; A1) (11)
b1
</equation>
<bodyText confidence="0.999627666666667">
Similarly, the hidden Markov model for the
phrase segmentation of the source sentence can be
defined as
</bodyText>
<equation confidence="0.9856695">
P(e; A2) = � P(e, b2; A2) (12)
b2
</equation>
<bodyText confidence="0.983995333333333">
Then, we can combine word alignment and
phrase segmentation and define the joint training
objective as
</bodyText>
<equation confidence="0.961390666666667">
J(01, 02, A1, A2)
经
合
S
= log P(f(s)|e(s); 01) +
s=1
</equation>
<page confidence="0.804207">
1831
</page>
<listItem confidence="0.796672333333333">
1: procedure VITERBIEM(D)
2: Initialize Θ(0)
3: for all k = 1,...,K do
</listItem>
<equation confidence="0.978743">
ˆH(k) ← SEARCH(D, Θ(k−1))
Θ(k) ← UPDATE(D, ˆH(k))
</equation>
<listItem confidence="0.9294665">
6: end for
7: return ˆH(K), Θ(K)
8: end procedure
Algorithm 1: A Viterbi EM algorithm for learning
</listItem>
<bodyText confidence="0.836003">
the joint word alignment and phrase segmentation
model from bilingual corpus. D is a bilingual cor-
pus, Θ(k) is the set of model parameters at the k-th
iteration, H(k) is the set of Viterbi latent variables
at the k-th iteration.
</bodyText>
<listItem confidence="0.902048">
1: procedure SEARCH(D, Θ)
2: Hˆ← ∅
3: for all s ∈ {1, ... , S} do
</listItem>
<equation confidence="0.923263857142857">
4: ˆa1 ← ALIGN(f(3), e(3), θ1)
5: ˆa2 ← ALIGN(e(3), f(3), θ2)
6: ˆb1 ← SEGMENT(f(3), λ1)
7: ˆb2 ← SEGMENT(e(3), λ2)
8: h0 ← hˆa1, ˆa2, ˆb1, ˆb2i
9: hˆ ←HILLCLIMB(f(3), e(3), h0, Θ)
10: Hˆ← Hˆ∪ {ˆh}
</equation>
<listItem confidence="0.747068666666667">
11: end for
12: return Hˆ
13: end procedure
</listItem>
<equation confidence="0.967026833333333">
ˆa1 an d ˆa2 denote Viter-
gnments, ˆb1 an d ˆb2 denote Viterbi seg-
log P(e(3)|f(3); θ2) +
log P(f(3); λ1) +
log P(e(3); λ2) −
log E(f(3), e(3), θ1, θ2, λ1, λ2) (13)
</equation>
<bodyText confidence="0.976672">
where the expected loss is given by
</bodyText>
<equation confidence="0.999408833333333">
E(f(3), e(3), θ1, θ2, λ1, λ2)
P(a1|f(3), e(3); θ1) ×
P(a2|e(3), f(3); θ2) ×
P(b1|f(3); λ1) ×
P(b2|e(3); λ2) ×
Δ(a1, a2, b1, b2) (14)
</equation>
<bodyText confidence="0.999792333333333">
We define a new loss function segmentation
violation to measure the degree that an alignment
violates phrase segmentations.
</bodyText>
<equation confidence="0.999708333333333">
ΔSV(a1, a2, b1, b2) Q(a2, i, b2, b1)
J−1� Q(a1, j, b1, b2) + �z− 1 (15)
j=1 i=1
</equation>
<bodyText confidence="0.710717666666667">
where Q(a1, j, b1, b2) evaluates whether two links
l1 = (j, aj) and l2 = (j + 1, aj+1) violate the
phrase segmentation:
</bodyText>
<listItem confidence="0.9953295">
1. fj and fj+1 belong to one phrase but ear and
ear+1 belong to two phrases, or
2. fj and fj+1 belong to two phrases but ear and
ear+1 belong to one phrase.
</listItem>
<bodyText confidence="0.8759377">
The Q function returns 1 if there is violation and
0 otherwise.
In Figure 2, we use
to label words that do
not violate the phrase segmentations and
to
label violations.
In practice, we combine the two loss functions
to enable word alignment and phrase segmentation
to benefit each other in a joint search space:
</bodyText>
<equation confidence="0.968966">
a2,
b2)
=
a2) +
a2,
b2) (16)
</equation>
<sectionHeader confidence="0.505057" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.993372058823529">
Liang et al. (2006) indicate that it is intractable to
train the joint model. For simplicity and efficien-
cy, they exploit a simple heuristic procedure that
leverages the product of posterior marginal prob-
abilities. The intuition behind the heuristic is that
links on which two models disagree should be dis-
counted because the products of the marginals are
small (Liang et al., 2006).
Unfortunately, it is hard to develop a similar
heuristic for our model that allows for arbitrary
loss functions. Alternatively, we resort to a
Viterbi EM algorithm, as shown in Algorithm
1. The algorithm takes the training data D =
as input (line 1). We use
to denote the set of model
parameters at the k-th iteration. After initializing
the model parameters (line 2), the algori
</bodyText>
<equation confidence="0.905265428571428">
“+”
“-”
ΔSM+SV(a1,
b1,
ΔSM(a1,
ΔSV(a1,
b1,
{hf(3),e(3)i}S3=1
Θ(k)=hθ(k)
1 ,θ(k)
2 ,λ(k)
1 ,λ(k)
2 i
thm alter-
</equation>
<bodyText confidence="0.685507">
nates between searching for the Viterbi alignments
</bodyText>
<figure confidence="0.895833791666666">
�=
a1
�
b1
�
a2
�
b2
Algorithm 2: A search algorithm for finding the
Viterbi latent variables.
bi ali
mentations. They form a starting point
for
the hill climbing algorithm, which keeps chang-
ing alignments and segmentations until the model
score does not increase.
is the final set of Viterbi
latent vari
h0
hˆ
ables for one sentence.
1832
(c) MERGE
(d) SPLIT
</figure>
<figureCaption confidence="0.9788905">
Figure 3: Operators used in the HILLCLIMB pro-
cedure.
</figureCaption>
<bodyText confidence="0.99617605">
and segmentations ˆH(k) using the SEARCH proce-
dure (line 4) and updating model parameters using
the UPDATE procedure (line 5). The algorithm ter-
minates after running for K iterations.
It is challenging to search for the Viterbi align-
ments and segmentations because of complicat-
ed structural dependencies. As shown in Al-
gorithm 2, our strategy is first to find Viter-
bi alignments and segmentations independently
using the ALIGN and SEGMENT procedures (lines
4-7), which then serve as a starting point for the
HILLCLIMB procedure (lines 8-9).
Figure 3 shows three operators we use in the
HILLCLIMB procedure. The MOVE operator
moves a link in an alignment, the MERGE oper-
ator merges two phrases into one phrase, and the
SPLIT operator splits one phrase into two small-
er phrases. Note that each operator can be further
divided into two variants: one for the source side
and another for the target side.
</bodyText>
<sectionHeader confidence="0.998143" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.983581">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.99992262962963">
We evaluate our approach on Chinese-English
alignment and translation tasks.
The training corpus consists of 1.2M sentence
pairs with 32M Chinese words and 35.4M English
words. We used the SRILM toolkit (Stolcke,
2002) to train a 4-gram language model on the
Xinhua portion of the English GIGAWORD cor-
pus, which contains 398.6M words. For alignment
evaluation, we used the Tsinghua Chinese-English
word alignment evaluation data set.1 The evalu-
ation metric is alignment error rate (AER) (Och
and Ney, 2003). For translation evaluation, we
used the NIST 2006 dataset as the development set
and the NIST 2002, 2003, 2004, 2005, and 2008
datasets as the test sets. The evaluation metric is
case-insensitive BLEU (Papineni et al., 2002).
We used both phrase-based (Koehn et al.,
2003) and hierarchical phrase-based (Chiang,
2007) translation systems to evaluate whether our
approach improves translation performance. For
the phrase-based model, we used the open-source
toolkit Moses (Koehn and Hoang, 2007). For the
hierarchical phrase-based model, we used an in-
house re-implementation on par with state-of-the-
art open-source decoders.
We compared our approach with two state-of-
the-art generative alignment models:
</bodyText>
<listItem confidence="0.992848333333333">
1. GIZA++ (Och and Ney, 2003): unsupervised
training of IBM models (Brown et al., 1993)
and the HMM model (Vogel et al., 1996) us-
ing EM,
2. BERKELEY (Liang et al., 2006): unsuper-
vised training of joint HMMs using EM.
</listItem>
<bodyText confidence="0.9998896875">
For GIZA++, we trained IBM Model 4 in two
directions with the default setting and used the
grow-diag-final heuristic to generate symmetric
alignments. For BERKELEY, we trained joint
HMMs using the default setting. The hyper-
parameter of posterior decoding was optimized on
the development set.
We used first-order HMMs for both word
alignment and phrase segmentation. Our joint
alignment and segmentation model were trained
using the Viterbi EM algorithm for five iterations.
Note that the Chinese-to-English and English-to-
Chinese alignments are generally non-identical
but share many links (see Figure 1(c)). Then,
we used the grow-diag-final heuristic to generate
symmetric alignments.
</bodyText>
<subsectionHeader confidence="0.8790455">
5.2 Comparison with GIZA++ and
BERKELEY
</subsectionHeader>
<bodyText confidence="0.9988876">
Table 1 shows the comparison of our approach
with GIZA++ and BERKELEY in terms of AER
and BLEU. GIZA++ trains two asymmetric
models independently and uses the grow-diag-
final (i.e., GDF) for symmetrization. BERKELEY
</bodyText>
<footnote confidence="0.925565">
1http://nlp.csai.tsinghua.edu.cn/˜ly/systems/TsinghuaAlig
ner/TsinghuaAligner.html
</footnote>
<table confidence="0.841817857142857">
(a) MOVE
1833
system training agreement loss sym. AER BLEU
GIZA++ indep. N/A N/A GDF 21.35 24.46
BERKELEY joint word-word HM PD 20.52 24.54
this work joint word-word SM GDF 22.19 25.11
word-word, word-phrase SM+SV 22.01 25.78
</table>
<tableCaption confidence="0.9418328">
Table 1: Comparison with GIZA++ and BERKELEY. “word-word” denotes the agreement between
Chinese-to-English and English-to-Chinese word alignments. “word-phrase” denotes the agreement be-
tween word alignments and phrase segmentations. “HM” denotes the hard matching loss function, “SM”
denotes soft matching, and “SV” denotes segmentation violation. “GDF” denotes grow-diag-final. “PD”
denotes posterior decoding. The BLEU scores are evaluated on NIST08 test set.
</tableCaption>
<table confidence="0.999166222222222">
alignment loss translation NIST06 NIST02 NIST03 NIST04 NIST05 NIST08
GIZA++ N/A phrase 29.57 31.82 31.67 32.20 30.48 24.46
hier. 30.72 33.90 33.12 33.54 32.28 24.72
BERKELEY HM phrase 29.87 32.21 32.48 32.06 30.59 24.54
hier. 29.52 33.59 32.70 32.95 29.52 24.29
this work SM phrase 30.04* 32.75**++ 32.35** 32.47*+ 30.86*+ 25.11**++
hier. 30.71++ 34.50**++ 33.89**++ 34.02*++ 32.83**++ 24.32
SM+SV phrase 30.60**++ 33.37**++ 33.24**++ 33.15**++ 31.57**++ 25.78**++
hier. 30.88++ 34.53**++ 34.04**++ 33.66++ 32.93**++ 25.17*++
</table>
<tableCaption confidence="0.996244">
Table 2: Results on (hierarchical) phrase-based translation. The evaluation metric is case-insensitive
</tableCaption>
<bodyText confidence="0.993206558823529">
BLEU. “HM” denotes the hard matching loss function, “SM” denotes soft matching, and “SV” denotes
segmentation violation. “*”: significantly better than GIZA++ (p &lt; 0.05). “**”: significantly better than
GIZA++ (p &lt; 0.01). “+”: significantly better than BERKELEY (p &lt; 0.05). “++”: significantly better
than BERKELEY (p &lt; 0.01).
trains two models jointly with the hard-matching
(i.e., HM) loss function and uses posterior decod-
ing for symmetrization.
For our approach, we distinguish between two
variants:
1. Imposing agreement between word align-
ments (i.e., word-word) that uses the soft
matching loss function (i.e., SM) (see Section
3.1);
2. Imposing agreement between word align-
ments and phrase segmentations (i.e., word-
word, word-phrase) that uses both the soft
matching and segmentation violation loss
functions (i.e., SM+SV) (see Section 3.2).
We used the grow-diag-final heuristic for
symmetrization.
For the alignment evaluation, we find that our
approach achieves higher AER scores than the two
baseline systems. One possible reason is that links
in the intersection of two symmetric alignments or
two symmetric models agree usually correspond
to sure links in the gold-standard annotation. Our
approach loosens the hard constraint on agreement
and makes the posteriors less peaked around the
Viterbi alignments.
For the translation evaluation, we used the
phrase-based system Moses to report BLEU s-
cores on the NIST 2008 test set. We find that both
the two variants of our approach significantly out-
performs the two baselines (p &lt; 0.01).
</bodyText>
<sectionHeader confidence="0.858772" genericHeader="method">
5.3 Results on (Hierarchical) Phrase-based
Translation
</sectionHeader>
<bodyText confidence="0.999879933333333">
Table 2 shows the results on phrase-based and
hierarchical phrase-based translation systems. We
find that our approach systematically outperforms
GIZA++ and BERKELEY on all NIST datasets.
In particular, generalizing the agreement to
model the discrepancy between word alignment
and phrase segmentation is consistently beneficial
for improving translation quality, suggesting that
it is important to introduce structural constraints
into word alignment to increase the correlation
between alignment and translation.
While “SM+SV” improves over “SM” signifi-
cantly on phrase-based translation, the margins on
the hierarchical phrase-based system are relative-
ly smaller. One possible reason is that the “SV”
</bodyText>
<page confidence="0.969948">
1834
</page>
<table confidence="0.9994312">
system loss |AC,E ||AE,C ||AC,E n AE,C |F1
GIZA++ N/A 29.39M 27.64M 17.07M 59.86
BERKELEY HM 29.12M 28.09M 21.30M 74.46
this work SM 29.84M 29.31M 20.24M 68.42
SM+SV 30.04M 29.50M 20.54M 69.00
</table>
<tableCaption confidence="0.962635">
Table 3: Agreement evaluation of GIZA++, BERKELEY and our approach. The F1 score reflects how
well two asymmetric alignments agree with each other.
</tableCaption>
<bodyText confidence="0.999555545454546">
loss function can better account for phrase-based
rather than hierarchical phrase-based translation.
It is possible to design new loss functions tailored
to hierarchical phrase-based translation.
We also find that the BLEU scores of BERKE-
LEY on hierarchical phrase-based translation are
much lower than those on phrase-based transla-
tion. This might result from the fact that BERKE-
LEY is prone to produce one-to-one alignments,
which are not optimal for hierarchical phrase-
based translation.
</bodyText>
<subsectionHeader confidence="0.992354">
5.4 Agreement Evaluation
</subsectionHeader>
<bodyText confidence="0.99987325">
Table 3 compares how well two asymmetric
models agree with each other among GIZA++,
BERKELEY and our approach. We use F1 score
to measure the degree of agreement:
</bodyText>
<equation confidence="0.7468275">
2|AC,E n AE,C |(17)
|AC,E |+ |AE,C|
</equation>
<bodyText confidence="0.999975428571428">
where AC,E is the set of Chinese-to-English
alignments on the training data and AE,C is the
set of English-to-Chinese alignments.
It is clear that independent training leads to low
agreement and joint training results in high agree-
ment. BERKELEY achieves the highest value of
agreement because of the hard constraint.
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999345666666667">
This work is inspired by two lines of research: (1)
agreement-based learning and (2) joint modeling
of multiple NLP tasks.
</bodyText>
<subsectionHeader confidence="0.978886">
6.1 Agreement-based Learning
</subsectionHeader>
<bodyText confidence="0.99990155">
The key idea of agreement-based learning is to
train a set of models jointly by encouraging them
to agree on the hidden variables (Liang et al.,
2006; Liang et al., 2008). This can also be seen as
a particular form of posterior constraint or poste-
rior regularization (Grac¸a et al., 2007; Ganchev et
al., 2010). The agreement is prior knowledge and
indirect supervision, which helps to train a more
reasonable model with biased guidance.
While agreement-based learning provides a
principled approach to training a generative mod-
el, it constrains that the sub-models must share the
same output space. Our work extends (Liang et
al., 2006) to introduce arbitrary loss functions that
can encode prior knowledge. As a result, Liang et
al. (2006)’s model is a special case of our frame-
work. Another difference is that our framework
allows for including the agreement between word
alignment and other structures such as phrase seg-
mentations and parse trees.
</bodyText>
<subsectionHeader confidence="0.99763">
6.2 Joint Modeling of Multiple NLP Tasks
</subsectionHeader>
<bodyText confidence="0.999988470588235">
It is well accepted that different NLP tasks can
help each other by providing additional informa-
tion for resolving ambiguities. As a result, joint
modeling of multiple NLP tasks has received in-
tensive attention in recent years, including phrase
segmentation and alignment (Zhang et al., 2003),
alignment and parsing (Burkett et al., 2010), tok-
enization and translation (Xiao et al., 2010), pars-
ing and translation (Liu and Liu, 2010), alignment
and named entity recognition (Chen et al., 2010;
Wang et al., 2013).
Among them, Zhang et al. (2003)’s integrat-
ed search algorithm for phrase segmentation and
alignment is most close to our work. They use
Point-wise Mutual Information to identify possi-
ble phrase pairs. The major difference is we train
models jointly instead of integrated decoding.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99995525">
We have presented generalized agreement for bidi-
rectional word alignment. The loss functions can
be defined both between asymmetric alignments
and between alignments and other latent structures
such as phrase segmentations. We develop a Viter-
bi EM algorithm to train the joint model. Exper-
iments on Chinese-English translation show that
joint training with generalized agreement achieves
</bodyText>
<page confidence="0.967112">
1835
</page>
<bodyText confidence="0.9998935">
significant improvements over two baselines for
(hierarchical) phrase-based MT systems. In the fu-
ture, we plan to investigate more loss functions to
account for syntactic constraints.
</bodyText>
<sectionHeader confidence="0.999293" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.5342065">
Yang Liu and Maosong Sun are supported by the
863 Program (2015AA011808), the National Nat-
ural Science Foundation of China (No. 61331013
and No. 61432013), and Samsung R&amp;D Institute
of China. Huanbo Luan is supported by the Na-
tional Natural Science Foundation of China (No.
61303075). This research is also supported by
the Singapore National Research Foundation un-
der its International Research Centre@Singapore
Funding Initiative and administered by the IDM
Programme. We sincerely thank the reviewers for
their valuable suggestions. We also thank Yue
Zhang, Meng Zhang and Shiqi Shen for their in-
sightful discussions.
</reference>
<sectionHeader confidence="0.979229" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999854528571429">
Peter F. Brown, Stephen A. Della Pietra, Vincen-
t J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchro-
nized grammars. In Proceedings of NAACL-HLT
2010.
Yufeng Chen, Chengqing Zong, and Keh-Yih Su.
2010. On jointly recognizing and aligning bilingual
named entities. In Proceedings of ACL 2010.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL 2006, pages 961–968,
Sydney, Australia, July.
Kuzmann Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. Journal of Ma-
chine Learning Research.
Joao V Grac¸a, Kuzman Ganchev, and Ben Taskar.
2007. Expectation maximization and posterior con-
straints. In Proceedings of NIPS 2007.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of EMNLP-
CoNLL 2007, pages 868–876, Prague, Czech Re-
public, June.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127–133, Edmonton,
Canada, May.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL
2006, pages 104–111, New York City, USA, June.
Percy Liang, Dan Klein, and Michael I. Jordan. 2008.
Agreement-based learning. In Advances in Neural
Information Processing Systems (NIPS).
Yang Liu and Qun Liu. 2010. Joint parsing and trans-
lation. In Proceedings of COLING 2010.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL 2006.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a methof for automatic e-
valuation of machine translation. In Proceedings of
ACL 2002.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of ICSLP 2002.
Stephan Vogel, Hermann Ney, and Christoph Tillman-
n. 1996. Hmm-based word alignment in statistical
translation. In Proceedings of COLING 1996.
Mengqiu Wang, Wanxiang Che, and Christopher D.
Manning. 2013. Joint word alignment and bilingual
named entity recognition using dual decomposition.
In Proceedings ofACL 2013.
Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Li-
u, and Shouxun Lin. 2010. Joint tokenization and
translation. In Proceedings of COLING 2010.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2003.
Integrated phrase segmentation and alignment algo-
rithm for statistical machine translation. In Proceed-
ings of Natural Language Processing and Knowl-
edge Engineering, 2003. IEEE.
</reference>
<page confidence="0.993294">
1836
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.541890">
<title confidence="0.998335">Generalized Agreement for Bidirectional Word Alignment</title>
<author confidence="0.808298">Yang Huanbo Maosong Key Laboratory of Intelligent Technology</author>
<affiliation confidence="0.842989333333333">Tsinghua National Laboratory for Information Science and Department of Computer Science and Technology, Tsinghua University, Beijing 100084, R&amp;D Institute of China, Beijing 100028, China</affiliation>
<email confidence="0.998187">h0517.yu@samsung.com</email>
<abstract confidence="0.998927">While agreement-based joint training has proven to deliver state-of-the-art alignment accuracy, the produced word alignments are usually restricted to one-toone mappings because of the hard constraint on agreement. We propose a general framework to allow for arbitrary loss functions that measure the disagreement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on Chinese- English translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Yang Liu</author>
</authors>
<title>and Maosong Sun are supported by the 863 Program (2015AA011808), the National Natural Science Foundation of China (No.</title>
<volume>61331013</volume>
<marker>Liu, </marker>
<rawString>Yang Liu and Maosong Sun are supported by the 863 Program (2015AA011808), the National Natural Science Foundation of China (No. 61331013 and No. 61432013), and Samsung R&amp;D Institute of China. Huanbo Luan is supported by the National Natural Science Foundation of China (No. 61303075). This research is also supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme. We sincerely thank the reviewers for their valuable suggestions. We also thank Yue Zhang, Meng Zhang and Shiqi Shen for their insightful discussions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1427" citStr="Brown et al., 1993" startWordPosition="192" endWordPosition="195">eement between asymmetric alignments. The loss functions can not only be defined between asymmetric alignments but also between alignments and other latent structures such as phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods. 1 Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these mod</context>
<context position="5773" citStr="Brown et al., 1993" startWordPosition="919" endWordPosition="922">th agreement achieves a high precision but generally only produces one-to-one alignments. We propose generalized agreement to account for not only the consensus between asymmetric alignments, but also the conformity of alignments to other latent structures such as phrase segmentations. 2002 峰 会 组 织 经 合 亚 太 非 正 式 中 国 元 首 将 加 2002 峰。 会 组 织 将 加 峰。 会 经 合 中 国 中 国 将 加 亚 太 组 织 经 合 元 首 2002 元 首 亚 太 非 正 式 非 正 式 2 Background 2.1 Asymmetric Alignment Models Given a source-language sentence e ≡ eI1 = e1, ... , eI and a target-language sentence f ≡ fJ1 = f1, ... , fJ, a source-to-target translation model (Brown et al., 1993; Vogel et al., 1996) can be defined as P(f|e; θ1) = � P(f, a1|e; θ1) (1) a1 where a1 denotes the source-to-target alignment and θ1 is the set of source-to-target translation model parameters. Likewise, the target-to-source translation model is given by P(e|f; θ2) = � P(e,a2|f; θ2) (2) a2 where a2 denotes the target-to-source alignment and θ2 is the set of target-to-source translation model parameters. Given a training set D = {(f(s), e(s))}Ss=1, the two models are trained independently to maximize the log-likelihood of the training data for each direction, respectively: S L(θ1) = log P(f(s)|e</context>
<context position="18616" citStr="Brown et al., 1993" startWordPosition="3167" endWordPosition="3170">tric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) using EM, 2. BERKELEY (Liang et al., 2006): unsupervised training of joint HMMs using EM. For GIZA++, we trained IBM Model 4 in two directions with the default setting and used the grow-diag-final heuristic to generate symmetric alignments. For BERKELEY, we trained joint HMMs using the default setting. The hyperparameter of posterior decoding was optimized on the development set. We used first-order HMMs for both word alignment and phrase segmentation. Our joint alignment and segmentation model were trained using the Viterbi EM algorithm for five iterati</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="26271" citStr="Burkett et al., 2010" startWordPosition="4312" endWordPosition="4315">s a result, Liang et al. (2006)’s model is a special case of our framework. Another difference is that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetri</context>
</contexts>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>David Burkett, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In Proceedings of NAACL-HLT 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yufeng Chen</author>
<author>Chengqing Zong</author>
<author>Keh-Yih Su</author>
</authors>
<title>On jointly recognizing and aligning bilingual named entities.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="26425" citStr="Chen et al., 2010" startWordPosition="4337" endWordPosition="4340">n word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. We develop a Viterbi EM algorithm to train the joint model. </context>
</contexts>
<marker>Chen, Zong, Su, 2010</marker>
<rawString>Yufeng Chen, Chengqing Zong, and Keh-Yih Su. 2010. On jointly recognizing and aligning bilingual named entities. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1614" citStr="Chiang, 2007" startWordPosition="223" endWordPosition="224">s. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods. 1 Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗Corresponding author: Yang Liu. one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been pro</context>
<context position="18143" citStr="Chiang, 2007" startWordPosition="3101" endWordPosition="3102"> 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) using EM, 2. BERKELEY (Liang et al., 2006): unsupervised training of joint HMMs using E</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1635" citStr="Galley et al., 2006" startWordPosition="225" endWordPosition="228">terbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods. 1 Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗Corresponding author: Yang Liu. one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL 2006, pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzmann Ganchev</author>
<author>Jo˜ao Grac¸a</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research.</journal>
<marker>Ganchev, Grac¸a, Gillenwater, Taskar, 2010</marker>
<rawString>Kuzmann Ganchev, Jo˜ao Grac¸a, Jennifer Gillenwater, and Ben Taskar. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao V Grac¸a</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
</authors>
<title>Expectation maximization and posterior constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of NIPS</booktitle>
<marker>Grac¸a, Ganchev, Taskar, 2007</marker>
<rawString>Joao V Grac¸a, Kuzman Ganchev, and Ben Taskar. 2007. Expectation maximization and posterior constraints. In Proceedings of NIPS 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL 2007,</booktitle>
<pages>868--876</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2386" citStr="Koehn and Hoang, 2007" startWordPosition="335" endWordPosition="338">ely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗Corresponding author: Yang Liu. one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007). Instead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al., 2006). This significantly limits the translation accuracy, especi</context>
<context position="7047" citStr="Koehn and Hoang, 2007" startWordPosition="1123" endWordPosition="1126">s=1 One key limitation of these generative models is that they are asymmetric: each target word is restricted to be aligned to exactly one source word (including the empty cept) in the sourceto-target direction and vice versa. This is undesirable because most real-world word alignments are symmetric, in which one-to-one, oneto-many, many-to-one, and many-to-many links are usually mixed. See Figure 1(a) for example. Therefore, a number of heuristic symmetrization methods such as intersection, union, and growdiag-final have been proposed to combine asym1829 metric alignments (Och and Ney, 2003; Koehn and Hoang, 2007). 2.2 Alignment by Agreement Rather than using heuristic symmetrization methods, Liang et al. (2006) propose a principled approach to jointly training of the two models via enforcing agreement: �= � P(a1|f(s), e(s); 01) x a1 a2 P(a2|e(s), f(s); 02) x 6(a1,a2) (6) Note that the last term in Eq. (6) is actually the expected value of agreement: J(01, 02) S log P(f(s)|e(s); 01) + s=1 log P(e(s)|f(s); 02) + P(a|f(s), e(s); 01) x P(a|e(s), f(s); 02) (5) Note that the last term in Eq. (5) encourages the two models to agree on asymmetric alignments. While this strategy significantly improves alignment</context>
<context position="18320" citStr="Koehn and Hoang, 2007" startWordPosition="3123" endWordPosition="3126">a Chinese-English word alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) using EM, 2. BERKELEY (Liang et al., 2006): unsupervised training of joint HMMs using EM. For GIZA++, we trained IBM Model 4 in two directions with the default setting and used the grow-diag-final heuristic to generate symmetric alignments. For BERKELEY, we traine</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of EMNLPCoNLL 2007, pages 868–876, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1600" citStr="Koehn et al., 2003" startWordPosition="219" endWordPosition="222"> phrase segmentations. We use a Viterbi EM algorithm to train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods. 1 Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗Corresponding author: Yang Liu. one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final)</context>
<context position="11451" citStr="Koehn et al., 2003" startWordPosition="1925" endWordPosition="1928"> English phrase but their counterparts are separated in two Chinese phrases. Words that do not violate the phrase alignment are labeled with “+”. See Section 3.2 for details. 3.2 Agreement between Word Alignments and Phrase Segmentations Our framework is very general and can be extended to include the agreement between word alignment and other latent structures such as phrase segmentations. The words in a Chinese sentence often constitute phrases that are translated as units in English and vice versa. Inspired by the alignment consistency constraint widely used in translation rule extraction (Koehn et al., 2003), we make the following assumption to impose a structural agreement constraint between word alignment and phrase segmentation: source words in one source phrase should be aligned to target words belonging to the same target phrase and vice versa. For example, consider the C —* E alignment in Figure 2. We segment Chinese and English sentences into phrases, which are sequences of consecutive words. Since “2002” and “APEC” belong to the same English phrase, their counterparts on the Chinese side should also belong to one phrase. While this assumption can potentially improve the correlation betwee</context>
<context position="18098" citStr="Koehn et al., 2003" startWordPosition="3094" endWordPosition="3097"> English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) using EM, 2. BERKELEY (Liang et al., 2006)</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127–133, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>104--111</pages>
<location>New York City, USA,</location>
<contexts>
<context position="2450" citStr="Liang et al. (2006)" startWordPosition="344" endWordPosition="347">of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗Corresponding author: Yang Liu. one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007). Instead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al., 2006). This significantly limits the translation accuracy, especially for distantly-related language pairs such as Chinese-Englis</context>
<context position="7147" citStr="Liang et al. (2006)" startWordPosition="1138" endWordPosition="1141">ricted to be aligned to exactly one source word (including the empty cept) in the sourceto-target direction and vice versa. This is undesirable because most real-world word alignments are symmetric, in which one-to-one, oneto-many, many-to-one, and many-to-many links are usually mixed. See Figure 1(a) for example. Therefore, a number of heuristic symmetrization methods such as intersection, union, and growdiag-final have been proposed to combine asym1829 metric alignments (Och and Ney, 2003; Koehn and Hoang, 2007). 2.2 Alignment by Agreement Rather than using heuristic symmetrization methods, Liang et al. (2006) propose a principled approach to jointly training of the two models via enforcing agreement: �= � P(a1|f(s), e(s); 01) x a1 a2 P(a2|e(s), f(s); 02) x 6(a1,a2) (6) Note that the last term in Eq. (6) is actually the expected value of agreement: J(01, 02) S log P(f(s)|e(s); 01) + s=1 log P(e(s)|f(s); 02) + P(a|f(s), e(s); 01) x P(a|e(s), f(s); 02) (5) Note that the last term in Eq. (5) encourages the two models to agree on asymmetric alignments. While this strategy significantly improves alignment accuracy, the joint model is prone to generate one-to-one alignments because it imposes a hard cons</context>
<context position="9201" citStr="Liang et al. (2006)" startWordPosition="1475" endWordPosition="1478">Alignments The key idea of generalizing agreement is to leverage loss functions that measure the difference between two unidirectional alignments. For example, the last term in Eq. (5) can be re-written as � � J� Ea1|f(3),e(3);θ1 Ea2|e(3),f(3);θ2 6(a1, a2) (7) Our idea is to replace 6(a1, a2) in Eq. (6) with an arbitrary loss function A(a1, a2) that measures the difference between a1 and a2. This gives the new joint training objective with generalized agreement: J(01, 02) S log P(f(s)|e(s); 01) + s=1 log P(e(s)|f(s); 02) − P(a1|f(s), e(s); 01) x P(a2|e(s), f(s); 02) x A(a1, a2) (8) Obviously, Liang et al. (2006)’s training objective is a special case of our framework. We refer to its loss function as hard matching: AHM(a1, a2) = 1 − 6(a1, a2) (9) We are interested in developing a soft version of the hard matching loss function because this will help to produce many-to-many symmetric alignments. For example, in Figure 1(c), the two alignments share most links but still allow for disagreed links to capture one-to-many and many-toone links. Note that the union of the two asymmetric alignments is almost the same with the goldstandard alignment in this example. While there are many possible ways to define</context>
<context position="15076" citStr="Liang et al. (2006)" startWordPosition="2592" endWordPosition="2595">tes whether two links l1 = (j, aj) and l2 = (j + 1, aj+1) violate the phrase segmentation: 1. fj and fj+1 belong to one phrase but ear and ear+1 belong to two phrases, or 2. fj and fj+1 belong to two phrases but ear and ear+1 belong to one phrase. The Q function returns 1 if there is violation and 0 otherwise. In Figure 2, we use to label words that do not violate the phrase segmentations and to label violations. In practice, we combine the two loss functions to enable word alignment and phrase segmentation to benefit each other in a joint search space: a2, b2) = a2) + a2, b2) (16) 4 Training Liang et al. (2006) indicate that it is intractable to train the joint model. For simplicity and efficiency, they exploit a simple heuristic procedure that leverages the product of posterior marginal probabilities. The intuition behind the heuristic is that links on which two models disagree should be discounted because the products of the marginals are small (Liang et al., 2006). Unfortunately, it is hard to develop a similar heuristic for our model that allows for arbitrary loss functions. Alternatively, we resort to a Viterbi EM algorithm, as shown in Algorithm 1. The algorithm takes the training data D = as </context>
<context position="18698" citStr="Liang et al., 2006" startWordPosition="3184" endWordPosition="3187">Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) using EM, 2. BERKELEY (Liang et al., 2006): unsupervised training of joint HMMs using EM. For GIZA++, we trained IBM Model 4 in two directions with the default setting and used the grow-diag-final heuristic to generate symmetric alignments. For BERKELEY, we trained joint HMMs using the default setting. The hyperparameter of posterior decoding was optimized on the development set. We used first-order HMMs for both word alignment and phrase segmentation. Our joint alignment and segmentation model were trained using the Viterbi EM algorithm for five iterations. Note that the Chinese-to-English and English-toChinese alignments are general</context>
<context position="25089" citStr="Liang et al., 2006" startWordPosition="4123" endWordPosition="4126">e set of Chinese-to-English alignments on the training data and AE,C is the set of English-to-Chinese alignments. It is clear that independent training leads to low agreement and joint training results in high agreement. BERKELEY achieves the highest value of agreement because of the hard constraint. 6 Related Work This work is inspired by two lines of research: (1) agreement-based learning and (2) joint modeling of multiple NLP tasks. 6.1 Agreement-based Learning The key idea of agreement-based learning is to train a set of models jointly by encouraging them to agree on the hidden variables (Liang et al., 2006; Liang et al., 2008). This can also be seen as a particular form of posterior constraint or posterior regularization (Grac¸a et al., 2007; Ganchev et al., 2010). The agreement is prior knowledge and indirect supervision, which helps to train a more reasonable model with biased guidance. While agreement-based learning provides a principled approach to training a generative model, it constrains that the sub-models must share the same output space. Our work extends (Liang et al., 2006) to introduce arbitrary loss functions that can encode prior knowledge. As a result, Liang et al. (2006)’s model</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL 2006, pages 104–111, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Dan Klein</author>
<author>Michael I Jordan</author>
</authors>
<title>Agreement-based learning.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="25110" citStr="Liang et al., 2008" startWordPosition="4127" endWordPosition="4130">English alignments on the training data and AE,C is the set of English-to-Chinese alignments. It is clear that independent training leads to low agreement and joint training results in high agreement. BERKELEY achieves the highest value of agreement because of the hard constraint. 6 Related Work This work is inspired by two lines of research: (1) agreement-based learning and (2) joint modeling of multiple NLP tasks. 6.1 Agreement-based Learning The key idea of agreement-based learning is to train a set of models jointly by encouraging them to agree on the hidden variables (Liang et al., 2006; Liang et al., 2008). This can also be seen as a particular form of posterior constraint or posterior regularization (Grac¸a et al., 2007; Ganchev et al., 2010). The agreement is prior knowledge and indirect supervision, which helps to train a more reasonable model with biased guidance. While agreement-based learning provides a principled approach to training a generative model, it constrains that the sub-models must share the same output space. Our work extends (Liang et al., 2006) to introduce arbitrary loss functions that can encode prior knowledge. As a result, Liang et al. (2006)’s model is a special case of</context>
</contexts>
<marker>Liang, Klein, Jordan, 2008</marker>
<rawString>Percy Liang, Dan Klein, and Michael I. Jordan. 2008. Agreement-based learning. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>Joint parsing and translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="26366" citStr="Liu and Liu, 2010" startWordPosition="4328" endWordPosition="4331">that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. W</context>
</contexts>
<marker>Liu, Liu, 2010</marker>
<rawString>Yang Liu and Qun Liu. 2010. Joint parsing and translation. In Proceedings of COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<contexts>
<context position="1654" citStr="Liu et al., 2006" startWordPosition="229" endWordPosition="232"> train the joint model since the inference is intractable. Experiments on ChineseEnglish translation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods. 1 Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗Corresponding author: Yang Liu. one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignmen</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of COLING/ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2362" citStr="Och and Ney, 2003" startWordPosition="331" endWordPosition="334">1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗Corresponding author: Yang Liu. one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och and Ney, 2003; Koehn and Hoang, 2007). Instead of using heuristic symmetrization, Liang et al. (2006) introduce a principled approach that encourages the agreement between asymmetric alignments in two directions. The basic idea is to favor links on which both unidirectional models agree. They associate two models via the agreement constraint and show that agreement-based joint training improves alignment accuracy significantly. However, enforcing agreement in joint training faces a major problem: the two models are restricted to one-to-one alignments (Liang et al., 2006). This significantly limits the tran</context>
<context position="7023" citStr="Och and Ney, 2003" startWordPosition="1119" endWordPosition="1122">e(s)|f(s); θ2) (4) s=1 One key limitation of these generative models is that they are asymmetric: each target word is restricted to be aligned to exactly one source word (including the empty cept) in the sourceto-target direction and vice versa. This is undesirable because most real-world word alignments are symmetric, in which one-to-one, oneto-many, many-to-one, and many-to-many links are usually mixed. See Figure 1(a) for example. Therefore, a number of heuristic symmetrization methods such as intersection, union, and growdiag-final have been proposed to combine asym1829 metric alignments (Och and Ney, 2003; Koehn and Hoang, 2007). 2.2 Alignment by Agreement Rather than using heuristic symmetrization methods, Liang et al. (2006) propose a principled approach to jointly training of the two models via enforcing agreement: �= � P(a1|f(s), e(s); 01) x a1 a2 P(a2|e(s), f(s); 02) x 6(a1,a2) (6) Note that the last term in Eq. (6) is actually the expected value of agreement: J(01, 02) S log P(f(s)|e(s); 01) + s=1 log P(e(s)|f(s); 02) + P(a|f(s), e(s); 01) x P(a|e(s), f(s); 02) (5) Note that the last term in Eq. (5) encourages the two models to agree on asymmetric alignments. While this strategy signific</context>
<context position="17824" citStr="Och and Ney, 2003" startWordPosition="3049" endWordPosition="3052">her divided into two variants: one for the source side and another for the target side. 5 Experiments 5.1 Setup We evaluate our approach on Chinese-English alignment and translation tasks. The training corpus consists of 1.2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-the</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a methof for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="18050" citStr="Papineni et al., 2002" startWordPosition="3086" endWordPosition="3089">.2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a methof for automatic evaluation of machine translation. In Proceedings of ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP</booktitle>
<contexts>
<context position="17536" citStr="Stolcke, 2002" startWordPosition="3005" endWordPosition="3006">(lines 8-9). Figure 3 shows three operators we use in the HILLCLIMB procedure. The MOVE operator moves a link in an alignment, the MERGE operator merges two phrases into one phrase, and the SPLIT operator splits one phrase into two smaller phrases. Note that each operator can be further divided into two variants: one for the source side and another for the target side. 5 Experiments 5.1 Setup We evaluate our approach on Chinese-English alignment and translation tasks. The training corpus consists of 1.2M sentence pairs with 32M Chinese words and 35.4M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the English GIGAWORD corpus, which contains 398.6M words. For alignment evaluation, we used the Tsinghua Chinese-English word alignment evaluation data set.1 The evaluation metric is alignment error rate (AER) (Och and Ney, 2003). For translation evaluation, we used the NIST 2006 dataset as the development set and the NIST 2002, 2003, 2004, 2005, and 2008 datasets as the test sets. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1750" citStr="Vogel et al., 1996" startWordPosition="242" endWordPosition="245">lation show that joint training with generalized agreement achieves significant improvements over two state-ofthe-art alignment methods. 1 Introduction Word alignment is a natural language processing task that aims to specify the correspondence between words in two languages (Brown et al., 1993). It plays an important role in statistical machine translation (SMT) as word-aligned bilingual corpora serve as the input of translation rule extraction (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006; Liu et al., 2006). Although state-of-the-art generative alignment models (Brown et al., 1993; Vogel et al., 1996) have been widely used in practical SMT systems, they fail to model the symmetry of word alignment. While word alignments in real-world bilingual data usually exhibit complicated mappings (i.e., mixed with one-to-one, one-to-many, manyto-one, and many-to-many links), these models assume that each target word is aligned to exactly ∗Corresponding author: Yang Liu. one source word. To alleviate this problem, heuristic methods (e.g., grow-diag-final) have been proposed to combine two asymmetric alignments (source-to-target and target-to-source) to generate symmetric bidirectional alignments (Och a</context>
<context position="5794" citStr="Vogel et al., 1996" startWordPosition="923" endWordPosition="926">s a high precision but generally only produces one-to-one alignments. We propose generalized agreement to account for not only the consensus between asymmetric alignments, but also the conformity of alignments to other latent structures such as phrase segmentations. 2002 峰 会 组 织 经 合 亚 太 非 正 式 中 国 元 首 将 加 2002 峰。 会 组 织 将 加 峰。 会 经 合 中 国 中 国 将 加 亚 太 组 织 经 合 元 首 2002 元 首 亚 太 非 正 式 非 正 式 2 Background 2.1 Asymmetric Alignment Models Given a source-language sentence e ≡ eI1 = e1, ... , eI and a target-language sentence f ≡ fJ1 = f1, ... , fJ, a source-to-target translation model (Brown et al., 1993; Vogel et al., 1996) can be defined as P(f|e; θ1) = � P(f, a1|e; θ1) (1) a1 where a1 denotes the source-to-target alignment and θ1 is the set of source-to-target translation model parameters. Likewise, the target-to-source translation model is given by P(e|f; θ2) = � P(e,a2|f; θ2) (2) a2 where a2 denotes the target-to-source alignment and θ2 is the set of target-to-source translation model parameters. Given a training set D = {(f(s), e(s))}Ss=1, the two models are trained independently to maximize the log-likelihood of the training data for each direction, respectively: S L(θ1) = log P(f(s)|e(s); θ1) (3) s=1 S L(</context>
<context position="18655" citStr="Vogel et al., 1996" startWordPosition="3175" endWordPosition="3178"> et al., 2002). We used both phrase-based (Koehn et al., 2003) and hierarchical phrase-based (Chiang, 2007) translation systems to evaluate whether our approach improves translation performance. For the phrase-based model, we used the open-source toolkit Moses (Koehn and Hoang, 2007). For the hierarchical phrase-based model, we used an inhouse re-implementation on par with state-of-theart open-source decoders. We compared our approach with two state-ofthe-art generative alignment models: 1. GIZA++ (Och and Ney, 2003): unsupervised training of IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) using EM, 2. BERKELEY (Liang et al., 2006): unsupervised training of joint HMMs using EM. For GIZA++, we trained IBM Model 4 in two directions with the default setting and used the grow-diag-final heuristic to generate symmetric alignments. For BERKELEY, we trained joint HMMs using the default setting. The hyperparameter of posterior decoding was optimized on the development set. We used first-order HMMs for both word alignment and phrase segmentation. Our joint alignment and segmentation model were trained using the Viterbi EM algorithm for five iterations. Note that the Chinese-to-English a</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of COLING 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Wanxiang Che</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint word alignment and bilingual named entity recognition using dual decomposition.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="26445" citStr="Wang et al., 2013" startWordPosition="4341" endWordPosition="4344">d other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other latent structures such as phrase segmentations. We develop a Viterbi EM algorithm to train the joint model. Experiments on Chine</context>
</contexts>
<marker>Wang, Che, Manning, 2013</marker>
<rawString>Mengqiu Wang, Wanxiang Che, and Christopher D. Manning. 2013. Joint word alignment and bilingual named entity recognition using dual decomposition. In Proceedings ofACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Yang Liu</author>
<author>Young-Sook Hwang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Joint tokenization and translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="26321" citStr="Xiao et al., 2010" startWordPosition="4320" endWordPosition="4323">case of our framework. Another difference is that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss functions can be defined both between asymmetric alignments and between alignments and other late</context>
</contexts>
<marker>Xiao, Liu, Hwang, Liu, Lin, 2010</marker>
<rawString>Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu, and Shouxun Lin. 2010. Joint tokenization and translation. In Proceedings of COLING 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Integrated phrase segmentation and alignment algorithm for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of Natural Language Processing and Knowledge Engineering,</booktitle>
<publisher>IEEE.</publisher>
<contexts>
<context position="26225" citStr="Zhang et al., 2003" startWordPosition="4305" endWordPosition="4308">functions that can encode prior knowledge. As a result, Liang et al. (2006)’s model is a special case of our framework. Another difference is that our framework allows for including the agreement between word alignment and other structures such as phrase segmentations and parse trees. 6.2 Joint Modeling of Multiple NLP Tasks It is well accepted that different NLP tasks can help each other by providing additional information for resolving ambiguities. As a result, joint modeling of multiple NLP tasks has received intensive attention in recent years, including phrase segmentation and alignment (Zhang et al., 2003), alignment and parsing (Burkett et al., 2010), tokenization and translation (Xiao et al., 2010), parsing and translation (Liu and Liu, 2010), alignment and named entity recognition (Chen et al., 2010; Wang et al., 2013). Among them, Zhang et al. (2003)’s integrated search algorithm for phrase segmentation and alignment is most close to our work. They use Point-wise Mutual Information to identify possible phrase pairs. The major difference is we train models jointly instead of integrated decoding. 7 Conclusion We have presented generalized agreement for bidirectional word alignment. The loss f</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2003</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2003. Integrated phrase segmentation and alignment algorithm for statistical machine translation. In Proceedings of Natural Language Processing and Knowledge Engineering, 2003. IEEE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
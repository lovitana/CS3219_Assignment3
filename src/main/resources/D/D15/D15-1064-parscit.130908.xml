<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000620">
<title confidence="0.9993235">
Named Entity Recognition for Chinese Social Media
with Jointly Trained Embeddings
</title>
<author confidence="0.995049">
Nanyun Peng and Mark Dredze
</author>
<affiliation confidence="0.909589666666667">
Human Language Technology Center of Excellence
Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD, 21218
</affiliation>
<email confidence="0.997773">
npeng1@jhu.edu, mdredze@cs.jhu.edu
</email>
<sectionHeader confidence="0.993858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841">
We consider the task of named entity
recognition for Chinese social media. The
long line of work in Chinese NER has fo-
cused on formal domains, and NER for
social media has been largely restricted
to English. We present a new corpus of
Weibo messages annotated for both name
and nominal mentions. Additionally, we
evaluate three types of neural embeddings
for representing Chinese text. Finally, we
propose a joint training objective for the
embeddings that makes use of both (NER)
labeled and unlabeled raw text. Our meth-
ods yield a 9% improvement over a state-
of-the-art baseline.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999738055555556">
Named entity recognition (NER), and more gen-
erally the task of mention detection1, is an essen-
tial component of information extraction technolo-
gies: the first step before tasks such as relation
extraction (Bunescu and Mooney, 2005) and en-
tity linking (Dredze et al., 2010; Ratinov et al.,
2011). A long line of work has focused on NER
in both formal and informal domains (Collins and
Singer, 1999; McCallum and Li, 2003; Nadeau
and Sekine, 2007; Jin and Chen, 2008; He et al.,
2012a), with recent efforts turning towards social
media (Finin et al., 2010; Liu et al., 2011; Ritter et
al., 2011; Fromreide et al., 2014; Li et al., 2012;
Liu et al., 2012). While NER has included work
on several languages, work on social media NER
has largely focused on English language data.2
We consider NER on Chinese social media from
the popular Sina Weibo service, both because of
</bodyText>
<footnote confidence="0.9721846">
1Since we consider name and nominals, our work is closer
to mention detection. For simplicity, we use the term NER.
2Etter et al. (2013) considered Spanish Twitter, which is
quite similar to English from the standpoint of building mod-
els and features.
</footnote>
<bodyText confidence="0.998708222222222">
the popularity of the service (comparable in size
to Twitter and previously used in NLP research
(Ling et al., 2013)) and the challenges faced in
processing Chinese language data. One approach
is to utilize lexical embeddings to improve NER
systems (Collobert and Weston, 2008; Turian et
al., 2010; Passos et al., 2014), including for Twit-
ter (Cherry and Guo, 2015). However, the use
of embeddings for Chinese remains a challenge.
Unlike most languages, we cannot easily assign
an embedding to each Chinese word without au-
tomated segmentation, which may be unreliable,
especially when we want to model informal text.3
For this reason, state-of-the-art NER systems for
Chinese do not tag words; they instead tag charac-
ters directly (Mao et al., 2008). While work has
explored different embeddings for Chinese (Liu et
al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen
et al., 2015), their inclusion in downstream tasks,
such as NER, remains untested.
We explore several types of embeddings for
Chinese text and their effect on Chinese social
media NER. Specifically, we make the following
contributions. 1) We present the first system for
NER on Chinese social media using a new cor-
pus based on Weibo messages. We consider both
name and nominal mentions, with the goal of sup-
porting downstream systems, such as coreference
resolution. Notably, our results reveal that the gap
between social media and traditional text for Chi-
nese is much larger than similar corpora for En-
glish, suggesting this task as an interesting area
of future work.4 2) We evaluate three types of
embeddings for Chinese text based on their inclu-
sion in a downstream task. We include results with
and without fine-tuning. 3) We present a joint ob-
</bodyText>
<footnote confidence="0.815134166666667">
3Word segmentation performance is much worse on social
media compared to formal text (Duan et al., 2012).
4Consider the overall F1 scores from Ritter et al. (2011),
Cherry and Guo (2015) and Fromreide et al. (2014) compared
to our best results in Table 2. This is despite the fact that Chi-
nese NER performance on formal texts is similar to English.
</footnote>
<page confidence="0.81318">
548
</page>
<note confidence="0.6919485">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999489">
jective that trains embeddings simultaneously for
both NER and language modeling. Joint training
yields better results than post-hoc fine-tuning.
</bodyText>
<sectionHeader confidence="0.663946" genericHeader="method">
2 NER for Chinese Social Media
</sectionHeader>
<bodyText confidence="0.999463818181818">
Several SIGHAN shared tasks have focused on
Chinese NER (Zhang et al., 2006; Jin and Chen,
2008; He et al., 2012b; Zhu et al., 2003; Fang et
al., 2004; Zhang et al., 2006), though they have
been restricted to formal text, e.g. news. NER for
Chinese social media remains unexplored.5
As is the case for other languages, social me-
dia informality introduces numerous problems
for NLP systems, such as spelling errors, novel
words, and ungrammatical constructions. Chinese
presents additional challenges, since it uses lo-
gograms instead of alphabets, and lacks many of
the clues that a word is a name, e.g. capitaliza-
tion and punctuation marks. The lack of explicit
word boundaries further confuses NER systems.
These problems are worse in social media, which
has worse word segmentation. Additionally, typi-
cal Chinese corpora use exclusively traditional or
simplified characters, whereas social media mixes
them. Figure 1 demonstrates some challenges.
The baseline system for our task is our own
implementation of Mao et al. (2008), which is
the current state-of-the-art on the SIGHAN 2008
shared task (Jin and Chen, 2008). They use a CRF
tagger with a BIOSE (begin, inside, outside, sin-
gleton, end) encoding that tags individual charac-
ters, not words, since word segmentation errors
are especially problematic for NER (Zhang et al.,
2006). Features include many common English
NER features, e.g. character unigrams and bi-
grams, with context windows of size 5. See Mao
et al. (2008) for complete details on their system.
Mao et al. (2008) use a two pass approach, train-
ing a CRF first for mention detection and using
the resulting predictions as a feature for an NER
system. Furthermore, they make extensive use
of gazetteer features. For simplicity, we exclude
the first pass mention detection and the gazetteer
features, which make only small improvements to
their overall performance. We note that other im-
plementations of this system (Zhang et al., 2013)
have been unable to match the performance re-
ported in Mao et al. (2008). Similarly, our imple-
mentation yields results on SIGHAN 2008 similar
</bodyText>
<footnote confidence="0.896906">
5Yang et al. (2014) consider a related problem of identi-
fying product mentions in Weibo messages.
</footnote>
<equation confidence="0.677590777777778">
有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云
m的心
Have many many words to say to you Jinfan Li wanna thin thin thin to
Fan Li I am a heart that want to cut the cloud
美得呀~顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊
Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao
hahahahahaha beautiful
看见前女友和她的新欢走在一起的时候,已经无处可躲了,只好
硬着 头皮上去打招呼哎呀,好久不见,你儿子都这么高了。
</equation>
<bodyText confidence="0.6651435">
When saw ex-girl friend and her new partner coming across, nowhere
to hide, have to say hello, long time no see, your son grown up.
</bodyText>
<figureCaption confidence="0.9910785">
Figure 1: Examples of Weibos messages and translations
with named (red) and nominal (blue) mentions.
</figureCaption>
<bodyText confidence="0.992732666666667">
to those reported in Zhang et al. (2013).6 Overall,
we take this tagger as representative of state-of-
the-art for Chinese NER.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="method">
3 Embeddings for Chinese Text
</sectionHeader>
<bodyText confidence="0.99950815625">
Lexical embeddings represent words in a continu-
ous low dimensional space, which can capture se-
mantic or syntactic properties of the lexicon: sim-
ilar words would have similar low dimensional
vector representations. Embeddings have been
used to gain improvements in a variety of NLP
tasks. In NER specifically, several papers have
shown improvements by using pre-trained neu-
ral embeddings as features in standard NER sys-
tems (Collobert and Weston, 2008; Turian et al.,
2010; Passos et al., 2014). More recently, these
improvements have been demonstrated on Twitter
data (Cherry and Guo, 2015). Embeddings are es-
pecially helpful when there is little training data,
since they can be trained on a large amount of un-
labeled data. This is the case for new languages
and domains, the task we face in this paper.
However, training embeddings for Chinese is
not straightforward: Chinese is not word seg-
mented, so embeddings for each word cannot be
trained on a raw corpus. Additionally, the state-
of-the-art systems for downstream Chinese tasks,
such as NER, may not use words.
We present three types of Chinese embeddings
that will be trained on a large corpus of Weibo
messages. These embeddings will be used as fea-
tures in the NER system by adding a (real valued)
feature for each dimension of the embedding for
the current word/character.
Word Embeddings We train an embedding for
each word type, the standard approach in other
languages. We run a Chinese word segmentation
</bodyText>
<footnote confidence="0.952058">
6Our implementation obtains an F1 of 88.63%.
</footnote>
<page confidence="0.996811">
549
</page>
<bodyText confidence="0.997933925">
system7 over the raw corpus of Weibo messages.
To create features, we first segment the NER data,
and then lookup the embedding that matches the
segmented word. Since the NER system tags char-
acters, we add the same word embedding features
to each character in the word.
Character Embeddings We learn an embed-
ding for each character in the training corpus (Sun
et al., 2014; Liu et al., 2014).This removes the de-
pendency on pre-processing the text, and better fits
our intended use case: NER tagging over charac-
ters. Since there are many fewer characters than
words, we learn many fewer embeddings. On the
one hand, this means fewer parameters and less
over-fitting. However, the reduction in parameters
comes with a loss of specificity, where we may
be unable to learn different behaviors of a charac-
ter in different settings. We explore a compromise
approach in the next section. These embeddings
are directly incorporated into the NER system by
adding embedding features for each character.
Character and Position Embeddings Charac-
ter embeddings cannot distinguish between uses of
the same character in different contexts, whereas
word embeddings fail to make use of characters
or character n-grams that are part of many words.
A compromise is to use character embeddings that
are sensitive to the character’s position in the word
(Chen et al., 2015). We first word segment the
corpus. For each character in each word, we add
a positional tag, e.g. the first/second/etc. charac-
ter in the word, yielding multiple embeddings per
character. We learn separate embeddings for each
positionally tagged character. To use these embed-
dings as features, we segment the NER text, obtain
position tags for each character, and add features
for the corresponding embedding.
These three methods lead to 179,809 word
embeddings, 10,912 character embeddings, and
24,818 character with position embeddings.
</bodyText>
<subsectionHeader confidence="0.997369">
3.1 Fine Tuning
</subsectionHeader>
<bodyText confidence="0.999978666666667">
For each of the embeddings, we fine-tune pre-
trained embeddings in the context of the NER task.
This corresponds to initializing the embeddings
parameters using a pre-trained model, and then
modifying the parameters during gradient updates
of the NER model by back-propogating gradients.
</bodyText>
<footnote confidence="0.537346">
7We use Jieba for segmentation: https://github.
com/fxsjy/jieba
</footnote>
<bodyText confidence="0.9994915">
This is a standard method that has been previously
explored in sequential and structured prediction
problem (Collobert et al., 2011; Zheng et al., 2013;
Yao et al., 2014; Pei et al., 2014).
</bodyText>
<subsectionHeader confidence="0.999812">
3.2 Joint Training Objectives
</subsectionHeader>
<bodyText confidence="0.999460210526316">
Fine-tuning has a disadvantage: it can arbitrar-
ily deviate from the settings obtained from train-
ing on large amounts of raw text. Recent work
has instead tuned embeddings for a specific task,
while maintaining information learned from raw
text. Yu and Dredze (2014) use multi-part objec-
tives that include both standard unlabeled objec-
tives, such as skip-gram models in word2vec, and
task specific objectives. Jointly training the em-
beddings with the multi-part objectives allows the
fine-tuned embeddings to further influence other
embeddings, even those that do not appear in the
labeled training data. This type of training can
help improve OOVs (Yu and Dredze, 2015), an
important aspect of improving social media NER.
We propose to jointly learn embeddings for both
language models and the NER task. The modified
objective function (log-likelihood) for the CRF is
given by:
</bodyText>
<equation confidence="0.6076425">
Ls(A, ew)
⎡ ⎤
1log Z(x)k +XλjFj(yk, xk, ew)
j
</equation>
<bodyText confidence="0.986624238095238">
where K is the number of instances, A is the
weight vector, xk and yk are the words and la-
bels sequence for each instance, ew is the embed-
ding for a word/character/character-position rep-
resentation w, Z(x)k is the normalization fac-
tor for each instance, and Fj(yk, xk, ew) =
Pni=1 fj(yki−1, yki , xk, ew, i) represents the fea-
ture function in which j denotes different feature
templates and i denotes the position index in a
sentence. This differs from a traditional CRF in
that the feature function depends on the additional
variables ew, which are the embeddings (as de-
fined above). As a result, the objective is no longer
log-linear, but log-bilinear 8.
8It is log-bilinear because the log-likelihood takes the
form f(x, y) = axy + bx + cy, where x, y are variables and
a, b, c are coefficients. In this case, x is the feature weight
and y is the embedding; both of them are vectors. Taking the
partial derivative with respect to any one of the variables, one
gets a constant (wrt that variable). This satisfies the definition
of log-bilinear functions.
</bodyText>
<equation confidence="0.981053">
1 X
K
k
=
</equation>
<page confidence="0.953451">
550
</page>
<bodyText confidence="0.9998865">
The second term is the standard skip-gram lan-
guage model objective (Mikolov et al., 2013):
</bodyText>
<equation confidence="0.957385375">
T
1
Lu(ew) = Tt=1
where
T
p( wi|wj) = exp (ewi ewj ~
Fi, exp eT
wi, ewj
</equation>
<bodyText confidence="0.9998574">
The first objective is notated Ls for “super-
vised” (trained on labeled NER data), and the sec-
ond is Lu, “unsupervised” (trained on raw text.)
Both objectives share the same variables ew. The
overall goal is to maximize their weighted sum:
</bodyText>
<equation confidence="0.617257">
arg max = Ls(λ, ew) + CLu(ew) (2)
ew
</equation>
<bodyText confidence="0.863941">
where C is a tradeoff parameter.
</bodyText>
<subsectionHeader confidence="0.976313">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999365739130435">
We pre-trained embeddings using word2vec
(Mikolov et al., 2013) with the skip-gram train-
ing objective and NEC negative sampling. Un-
less otherwise stated, we used word2vec’s default
parameter settings. All embeddings were 100-
dimensional, and we used the same embeddings
for the input and output parameters in the skip-
gram objective. We optimized the joint objective
(2) using an alternative optimization strategy: we
alternated 30 iterations of CRF training on the NE
labeled data and 5 multi-threaded passes through
both the labeled and unlabeled data for the skip-
gram objective. We avoided over-fitting using
early-stopping. For simplicity, we set C = 1 for
(2). The CRF was trained using stochastic gra-
dient descent with an L2 regularizer. All model
hyper-parameters were tuned on dev data.
We use the off-the-shelf tool word2vec
(Mikolov et al., 2013) to do skip-gram training
for language model, and implement our own CRF
model to modify the embeddings. We optimize
(2) by alternating the optimzation of each of the
two objectives.
</bodyText>
<sectionHeader confidence="0.997808" genericHeader="method">
4 Weibo NER Corpus
</sectionHeader>
<bodyText confidence="0.924920666666667">
We constructed a corpus of Weibo messages an-
notated for NER. We followed the DEFT ERE
(Linguistics Data Consortium, 2014) 9 annotation
</bodyText>
<footnote confidence="0.583204">
9See Aguilar et al. (2014) for a comparison of DEFT ERE
with other common standards.
</footnote>
<table confidence="0.9997965">
Entity Type Name Mentions Total
Nominal
Geo-political 243 0 243
Location 88 38 126
Organization 224 31 255
Person 721 636 1,357
</table>
<tableCaption confidence="0.999975">
Table 1: Mention statistics for the Weibo NER corpus.
</tableCaption>
<bodyText confidence="0.998834714285714">
guidelines for entities, which includes four ma-
jor semantic types: person, organization, location
and geo-political entity. We annotated both name
and nominal mentions. Chinese pronoun mentions
can be easily recognized with a regular expression.
We used Amazon Mechanical Turk, using stan-
dard methods of multiple annotators and including
gold examples to ensure high quality annotations
(Callison-Burch and Dredze, 2010).
Our corpus includes 1,890 messages sampled
from Weibo between November 2013 and De-
cember 2014. Rather than selecting messages at
random, which would yield a small number of
messages with entities, we selected messages that
contained three or more (segmented) words that
were not in a fixed vocabulary of common Chi-
nese words. Initial experiments showed this gave
messages more likely to contain entities.
Table 1 shows statistics of the final corpus. We
divided the corpus into 7 folds, each with 127 mes-
sages, where each message corresponds to a single
instance. We use the first 5 folds for train, the 6th
for development, and the 7th for test. We make
our code and the annotated corpus available.10
We constructed an additional corpus of unla-
beled messages for training the embeddings. We
randomly selected 2,259,434 messages from the
same time period as above.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999742">
We evaluate our methods under two settings: train-
ing on only name mentions, and training on both
name and nominal mentions. We re-train the Stan-
ford NER system (Finkel et al., 2005) as a base-
line; besides, we also evaluate our implementa-
tion of the CRF from Mao et al. (2008) as de-
scribed in §2 as Baseline Features. To this base-
line, we add each of our three embedding mod-
els: word, character, character+position (as de-
scribed in §3), and report results on the modified
</bodyText>
<equation confidence="0.9091085">
10https://github.com/hltcoe/golden-horse
X log p(wt+j|wt), (1)
−c&lt;j&lt;c,j�0
T
</equation>
<page confidence="0.995612">
551
</page>
<table confidence="0.99964">
Method Without Fine Tuning Dev Without Fine Tuning Test
Precision Recall F1 With Fine Tuning Precision Recall F1 With Fine Tuning
Precision Recall F1 Precision Recall F1
Stanford 63.51 23.27 34.06 55.70 22.86 33.06
Baseline Features 63.51 27.17 38.06 56.98 25.26 35.00
+ word 65.71 26.59 37.86 70.97 25.43 37.45 56.82 25.77 35.46 64.94 25.77 36.90
+ character 53.54 30.64 38.97 58.76 32.95 42.22 56.48 31.44 40.40 57.89 34.02 42.86
+ character+position 60.87 32.37 42.26 61.76 36.42 45.82 61.90 33.51 43.48 57.26 34.53 43.09
Joint(cp) 57.41 35.84 44.13 57.98 35.57 44.09
Stanford 72.39 31.80 44.19 63.96 22.19 32.95
Baseline Features 71.94 33.22 45.45 60.16 23.87 34.18
+ word 69.66 33.55 45.29 70.67 35.22 47.01 59.40 25.48 35.67 60.68 22.90 33.26
+ character 58.76 32.95 42.22 66.88 35.55 46.42 58.28 28.39 38.18 55.15 29.35 38.32
+ character+position 73.43 34.88 47.30 69.38 36.88 48.16 65.91 28.06 39.37 62.33 29.35 39.91
Joint(cp) 72.55 36.88 48.90 63.84 29.45 40.38
</table>
<tableCaption confidence="0.998777">
Table 2: NER results for name mentions (top) and name + nominal mentions (bottom).
</tableCaption>
<bodyText confidence="0.999743638888889">
CRF model with and without fine-tuning. We also
report results for the joint method trained with the
character+position model (cp), which performed
the best on dev data for joint training.
General Results Table 2 shows results for both
dev (tuned) and test (held out) splits. First, we
observe that the results for the baseline are signif-
icantly below those for SIGHAN shared tasks as
well as the reported results on Twitter NER, show-
ing the difficulty of this task. In particular, recall
is especially challenging. Second, all embeddings
improve the baseline on test data, but the character
+ position model gets the best results. Fine-tuning
improves embedding results, but seems to over-
fit on dev data. Finally, our joint model does the
best in both conditions (name and name+nominal)
on test data, improving over fine-tuning, yielding
up to a 9% (absolute) improvement over a strong
baseline.
Effect of Embeddings We expect improve-
ments from embeddings to be larger when there
is less training data. Figure 2 shows F1 on dev
data for different amounts of training data, from
200 instances up to 1400, for the character + po-
sition embeddings versus the baseline model. We
see that for both settings, we see larger improve-
ments from embeddings for smaller training sets.
Error Analysis Since the results are relatively
low, we conducted an error analysis by randomly
sampling 150 error items and manually looking
through them. Among the 150 examples, 65 are
annotation errors, majorly cause by annotators ne-
glecting some mentions, this contributes 43% of
the errors. The second largest error source are
the person names: Chinese person names are very
flexible and nearly every character can be used
</bodyText>
<figure confidence="0.997894333333333">
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1 NAM+NOM w/ embedding
NAM+NOM w/o embedding
0.05 NAM w/ embedding
NAM w/o embedding
0
200 400 600 800 1000 1200 1400
Training instances
</figure>
<figureCaption confidence="0.999808">
Figure 2: Dev F1 for varying number of training instances.
</figureCaption>
<bodyText confidence="0.999934">
in given names, this makes recognizing person
names challenging and contributes to 9% of our
errors. The following largest source of error are
transliterated foreign names, which contributes to
7% of the errors. Other sources including bound-
ary error, type error, name abbreviation, nick-
names, etc.
</bodyText>
<sectionHeader confidence="0.995038" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.989735636363636">
Our results show that NER for Chinese social me-
dia remains a challenging task, results lag be-
hind both formal Chinese text and English Twitter.
Nevertheless, our embeddings, combined with our
joint training objective, provide a large improve-
ment over a state-of-the-art model.
Acknowledgement We would like to thank the
reviewers for their helpful comments and perspec-
tives. We thank Mo Yu, Kevin Duh, Jiang Guo
and Wenzhe Pei for the insightful discussions and
Xuezhe Ma for help running the Stanford baseline.
</bodyText>
<page confidence="0.8436665">
F1
552
</page>
<sectionHeader confidence="0.874592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985844758928571">
Jacqueline Aguilar, Charley Beller, Paul McNamee,
Benjamin Van Durme, Stephanie Strassel, Zhiyi
Song, and Joe Ellis. 2014. A comparison of the
events and relations across ACE, ERE, TAC-KBP,
and FrameNet annotation standards. In ACL Work-
shop: EVENTS.
Razvan C Bunescu and Raymond J Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Empirical Methods in Natural Language
Processing (EMNLP), pages 724–731. Association
for Computational Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with Amazon’s Me-
chanical Turk. In NAACL Workshop on Creating
Speech and Language Data With Mechanical Turk.
Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,
and Huanbo Luan. 2015. Joint learning of character
and word embeddings. In International Joint Con-
ference on Artificial Intelligence (IJCAI’15).
Colin Cherry and Hongyu Guo. 2015. The unreason-
able effectiveness of word representations for twit-
ter named entity recognition. In North America
Chapter of Association for Computational Linguis-
tics (NAACL). Association for Computational Lin-
guistics.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 100–110. Citeseer.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Inter-
national Conference on Machine Learning (ICML),
pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li.
2012. The cips-sighan clp 2012 chineseword seg-
mentation onmicroblog corpora bakeoff. In Second
CIPS-SIGHAN Joint Conference on Chinese Lan-
guage Processing, pages 35–40, Tianjin, China, De-
cember. Association for Computational Linguistics.
David Etter, Francis Ferraro, Ryan Cotterell, Olivia
Buzek, and Benjamin Van Durme. 2013. Nerit:
Named entity recognition for informal text. Tech-
nical report, Technical Report 11, Human Language
Technology Center of Excellence, Johns Hopkins
University, July.
Xiaoshan Fang, Jianfeng Gao, and Huanye Sheng.
2004. A semi-supervised approach to build anno-
tated corpus for chinese named entity recognition.
In Oliver Streiter and Qin Lu, editors, ACL SIGHAN
Workshop 2004, pages 129–133, Barcelona, Spain,
July. Association for Computational Linguistics.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark
Dredze. 2010. Annotating named entities in twit-
ter data with crowdsourcing. In NAACL Workshop
on Creating Speech and Language Data With Me-
chanical Turk.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher Manning. 2005. Incorporating non-local in-
formation into information extraction systems by
gibbs sampling. In Association for Computational
Linguistics (ACL), pages 363–370. Association for
Computational Linguistics.
Hege Fromreide, Dirk Hovy, and Anders Søgaard.
2014. Crowdsourcing and annotating NER for Twit-
ter# drift. In LREC.
Zhengyan He, Houfeng Wang, and Sujian Li. 2012a.
The task 2 of cips-sighan 2012 named entity recog-
nition and disambiguation in chinese bakeoff. In
Second CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pages 108–114, Tianjin,
China, December. Association for Computational
Linguistics.
Zhengyan He, Houfeng Wang, and Sujian Li. 2012b.
The task 2 of cips-sighan 2012 named entity recog-
nition and disambiguation in chinese bakeoff. In
Second CIPS-SIGHAN Joint Conference on Chi-
nese Language Processing, pages 108–114, Tianjin,
China, December. Association for Computational
Linguistics.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and
chinese pos tagging. In Sixth SIGHAN Workshop on
Chinese Language Processing, page 69. Citeseer.
Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, An-
witaman Datta, Aixin Sun, and Bu-Sung Lee. 2012.
Twiner: Named entity recognition in targeted twit-
ter stream. In SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’12,
pages 721–730, New York, NY, USA. ACM.
Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and
Isabel Trancoso. 2013. Microblogs as parallel cor-
pora. In Association for Computational Linguistics
(ACL). Association for Computational Linguistics.
Linguistics Data Consortium. 2014. DEFT ERE An-
notation Guidelines: Entities.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In Association for Computational Linguistics (ACL),
pages 359–367. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.994908">
553
</page>
<reference confidence="0.99932571">
Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu,
and Xiangyang Zhou. 2012. Joint inference
of named entity recognition and normalization for
tweets. In Association for Computational Linguis-
tics (ACL), ACL ’12, pages 526–535, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Xiaodong Liu, Kevin Duh, Yuji Matsumoto, and To-
moya Iwakura. 2014. Learning character repre-
sentations for chinese word segmentation. In NIPS
2014 Workshop on Modern Machine Learning and
Natural Language Processing.
Xinnian Mao, Yuan Dong, Saike He, Sencheng Bao,
and Haila Wang. 2008. Chinese word segmentation
and named entity recognition based on conditional
random fields. In IJCNLP, pages 90–93.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In North America Chapter of Association for Com-
putational Linguistics (NAACL).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In Neural Information Processing Systems
(NIPS), pages 3111–3119.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3–26.
Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. CoRR, abs/1404.5367.
Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Association for Computational Lin-
guistics (ACL).
Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan
Liu. 2014. Co-learning of word representations and
morpheme representations. In Conference on Com-
putational Linguistics (Coling).
Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Association for Com-
putational Linguistics (ACL), pages 1375–1384. As-
sociation for Computational Linguistics.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Empirical Methods in Natural Language
Processing (EMNLP), pages 1524–1534. Associa-
tion for Computational Linguistics.
Yaming Sun, Lei Lin, Nan Yang, Zhenzhou Ji, and
Xiaolong Wang. 2014. Radical-enhanced chinese
character embedding. In Neural Information Pro-
cessing Systems (NIPS), pages 279–286. Springer.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Association for
Computational Linguistics (ACL), pages 384–394.
Association for Computational Linguistics.
Xianxiang Yang, Heyan Huang, Xin Xin, Quanchao
Liu, and Xiaochi Wei. 2014. Domain-specific
product named entity recognition from chinese mi-
croblog. In Computational Intelligence and Secu-
rity (CIS), 2014 Tenth International Conference on,
pages 218–222. IEEE.
Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Dong
Yu, Xiaolong Li, and Feng Gao. 2014. Recurrent
conditional random field for language understand-
ing. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 4077–4081. IEEE.
Mo Yu and Mark Dredze. 2014. Improving lexical
embeddings with semantic knowledge. In Associ-
ation for Computational Linguistics (ACL), pages
545–550.
Mo Yu and Mark Dredze. 2015. Learning composition
models for phrase embeddings. Transactions of the
Association for Computational Linguistics, 3:227–
242.
Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie
Wang. 2006. Word segmentation and named entity
recognition for sighan bakeoff3. In Fifth SIGHAN
Workshop on Chinese Language Processing, pages
158–161, Sydney, Australia, July. Association for
Computational Linguistics.
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for chinese word seg-
mentation. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmentation
and pos tagging. In Empirical Methods in Natural
Language Processing (EMNLP), pages 647–657.
Xiaodan Zhu, Mu Li, Jianfeng Gao, and Chang-Ning
Huang. 2003. Single character chinese named en-
tity recognition. In Second SIGHAN Workshop on
Chinese Language Processing, pages 125–132, Sap-
poro, Japan, July. Association for Computational
Linguistics.
</reference>
<page confidence="0.998709">
554
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.370131">
<title confidence="0.6869188">Named Entity Recognition for Chinese Social with Jointly Trained Embeddings Peng Human Language Technology Center of Center for Language and Speech</title>
<author confidence="0.44538">Johns Hopkins University</author>
<author confidence="0.44538">Baltimore</author>
<email confidence="0.99806">npeng1@jhu.edu,mdredze@cs.jhu.edu</email>
<abstract confidence="0.9958299375">We consider the task of named entity recognition for Chinese social media. The long line of work in Chinese NER has focused on formal domains, and NER for social media has been largely restricted to English. We present a new corpus of Weibo messages annotated for both name and nominal mentions. Additionally, we evaluate three types of neural embeddings for representing Chinese text. Finally, we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacqueline Aguilar</author>
<author>Charley Beller</author>
<author>Paul McNamee</author>
<author>Benjamin Van Durme</author>
<author>Stephanie Strassel</author>
<author>Zhiyi Song</author>
<author>Joe Ellis</author>
</authors>
<title>A comparison of the events and relations across ACE, ERE, TAC-KBP, and FrameNet annotation standards.</title>
<date>2014</date>
<booktitle>In ACL Workshop: EVENTS.</booktitle>
<marker>Aguilar, Beller, McNamee, Van Durme, Strassel, Song, Ellis, 2014</marker>
<rawString>Jacqueline Aguilar, Charley Beller, Paul McNamee, Benjamin Van Durme, Stephanie Strassel, Zhiyi Song, and Joe Ellis. 2014. A comparison of the events and relations across ACE, ERE, TAC-KBP, and FrameNet annotation standards. In ACL Workshop: EVENTS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>724--731</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1117" citStr="Bunescu and Mooney, 2005" startWordPosition="168" endWordPosition="171">lish. We present a new corpus of Weibo messages annotated for both name and nominal mentions. Additionally, we evaluate three types of neural embeddings for representing Chinese text. Finally, we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popula</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C Bunescu and Raymond J Mooney. 2005. A shortest path dependency kernel for relation extraction. In Empirical Methods in Natural Language Processing (EMNLP), pages 724–731. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Mechanical Turk.</booktitle>
<contexts>
<context position="15721" citStr="Callison-Burch and Dredze, 2010" startWordPosition="2560" endWordPosition="2563"> with other common standards. Entity Type Name Mentions Total Nominal Geo-political 243 0 243 Location 88 38 126 Organization 224 31 255 Person 721 636 1,357 Table 1: Mention statistics for the Weibo NER corpus. guidelines for entities, which includes four major semantic types: person, organization, location and geo-political entity. We annotated both name and nominal mentions. Chinese pronoun mentions can be easily recognized with a regular expression. We used Amazon Mechanical Turk, using standard methods of multiple annotators and including gold examples to ensure high quality annotations (Callison-Burch and Dredze, 2010). Our corpus includes 1,890 messages sampled from Weibo between November 2013 and December 2014. Rather than selecting messages at random, which would yield a small number of messages with entities, we selected messages that contained three or more (segmented) words that were not in a fixed vocabulary of common Chinese words. Initial experiments showed this gave messages more likely to contain entities. Table 1 shows statistics of the final corpus. We divided the corpus into 7 folds, each with 127 messages, where each message corresponds to a single instance. We use the first 5 folds for train</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In NAACL Workshop on Creating Speech and Language Data With Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxiong Chen</author>
<author>Lei Xu</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Huanbo Luan</author>
</authors>
<title>Joint learning of character and word embeddings.</title>
<date>2015</date>
<booktitle>In International Joint Conference on Artificial Intelligence (IJCAI’15).</booktitle>
<contexts>
<context position="2888" citStr="Chen et al., 2015" startWordPosition="469" endWordPosition="472"> Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks, such as NER, remains untested. We explore several types of embeddings for Chinese text and their effect on Chinese social media NER. Specifically, we make the following contributions. 1) We present the first system for NER on Chinese social media using a new corpus based on Weibo messages. We consider both name and nominal mentions, with the goal of supporting downstream systems, such as coreference resolution. Notably, our results reveal that the gap between social media and traditional text for Chinese is much larger than similar corpora for English, su</context>
<context position="10224" citStr="Chen et al., 2015" startWordPosition="1660" endWordPosition="1663">y, where we may be unable to learn different behaviors of a character in different settings. We explore a compromise approach in the next section. These embeddings are directly incorporated into the NER system by adding embedding features for each character. Character and Position Embeddings Character embeddings cannot distinguish between uses of the same character in different contexts, whereas word embeddings fail to make use of characters or character n-grams that are part of many words. A compromise is to use character embeddings that are sensitive to the character’s position in the word (Chen et al., 2015). We first word segment the corpus. For each character in each word, we add a positional tag, e.g. the first/second/etc. character in the word, yielding multiple embeddings per character. We learn separate embeddings for each positionally tagged character. To use these embeddings as features, we segment the NER text, obtain position tags for each character, and add features for the corresponding embedding. These three methods lead to 179,809 word embeddings, 10,912 character embeddings, and 24,818 character with position embeddings. 3.1 Fine Tuning For each of the embeddings, we fine-tune pret</context>
</contexts>
<marker>Chen, Xu, Liu, Sun, Luan, 2015</marker>
<rawString>Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. 2015. Joint learning of character and word embeddings. In International Joint Conference on Artificial Intelligence (IJCAI’15).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Hongyu Guo</author>
</authors>
<title>The unreasonable effectiveness of word representations for twitter named entity recognition.</title>
<date>2015</date>
<booktitle>In North America Chapter of Association for Computational Linguistics (NAACL). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2372" citStr="Cherry and Guo, 2015" startWordPosition="384" endWordPosition="387"> of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2013)) and the challenges faced in processing Chinese language data. One approach is to utilize lexical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks, such as NER, remains untested. We explore sev</context>
<context position="3911" citStr="Cherry and Guo (2015)" startWordPosition="640" endWordPosition="643">downstream systems, such as coreference resolution. Notably, our results reveal that the gap between social media and traditional text for Chinese is much larger than similar corpora for English, suggesting this task as an interesting area of future work.4 2) We evaluate three types of embeddings for Chinese text based on their inclusion in a downstream task. We include results with and without fine-tuning. 3) We present a joint ob3Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang </context>
<context position="7945" citStr="Cherry and Guo, 2015" startWordPosition="1280" endWordPosition="1283"> 3 Embeddings for Chinese Text Lexical embeddings represent words in a continuous low dimensional space, which can capture semantic or syntactic properties of the lexicon: similar words would have similar low dimensional vector representations. Embeddings have been used to gain improvements in a variety of NLP tasks. In NER specifically, several papers have shown improvements by using pre-trained neural embeddings as features in standard NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014). More recently, these improvements have been demonstrated on Twitter data (Cherry and Guo, 2015). Embeddings are especially helpful when there is little training data, since they can be trained on a large amount of unlabeled data. This is the case for new languages and domains, the task we face in this paper. However, training embeddings for Chinese is not straightforward: Chinese is not word segmented, so embeddings for each word cannot be trained on a raw corpus. Additionally, the stateof-the-art systems for downstream Chinese tasks, such as NER, may not use words. We present three types of Chinese embeddings that will be trained on a large corpus of Weibo messages. These embeddings wi</context>
</contexts>
<marker>Cherry, Guo, 2015</marker>
<rawString>Colin Cherry and Hongyu Guo. 2015. The unreasonable effectiveness of word representations for twitter named entity recognition. In North America Chapter of Association for Computational Linguistics (NAACL). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>100--110</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1282" citStr="Collins and Singer, 1999" startWordPosition="199" endWordPosition="202">ing Chinese text. Finally, we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et a</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Empirical Methods in Natural Language Processing (EMNLP), pages 100–110. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning (ICML),</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2283" citStr="Collobert and Weston, 2008" startWordPosition="368" endWordPosition="371">ta.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2013)) and the challenges faced in processing Chinese language data. One approach is to utilize lexical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., </context>
<context position="7805" citStr="Collobert and Weston, 2008" startWordPosition="1258" endWordPosition="1261">al (blue) mentions. to those reported in Zhang et al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embeddings represent words in a continuous low dimensional space, which can capture semantic or syntactic properties of the lexicon: similar words would have similar low dimensional vector representations. Embeddings have been used to gain improvements in a variety of NLP tasks. In NER specifically, several papers have shown improvements by using pre-trained neural embeddings as features in standard NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014). More recently, these improvements have been demonstrated on Twitter data (Cherry and Guo, 2015). Embeddings are especially helpful when there is little training data, since they can be trained on a large amount of unlabeled data. This is the case for new languages and domains, the task we face in this paper. However, training embeddings for Chinese is not straightforward: Chinese is not word segmented, so embeddings for each word cannot be trained on a raw corpus. Additionally, the stateof-the-art systems for downstream Chinese tasks, such as NER, m</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning (ICML), pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="11261" citStr="Collobert et al., 2011" startWordPosition="1814" endWordPosition="1817"> three methods lead to 179,809 word embeddings, 10,912 character embeddings, and 24,818 character with position embeddings. 3.1 Fine Tuning For each of the embeddings, we fine-tune pretrained embeddings in the context of the NER task. This corresponds to initializing the embeddings parameters using a pre-trained model, and then modifying the parameters during gradient updates of the NER model by back-propogating gradients. 7We use Jieba for segmentation: https://github. com/fxsjy/jieba This is a standard method that has been previously explored in sequential and structured prediction problem (Collobert et al., 2011; Zheng et al., 2013; Yao et al., 2014; Pei et al., 2014). 3.2 Joint Training Objectives Fine-tuning has a disadvantage: it can arbitrarily deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to fur</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Paul McNamee</author>
<author>Delip Rao</author>
<author>Adam Gerber</author>
<author>Tim Finin</author>
</authors>
<title>Entity disambiguation for knowledge base population.</title>
<date>2010</date>
<booktitle>In Conference on Computational Linguistics (Coling).</booktitle>
<contexts>
<context position="1157" citStr="Dredze et al., 2010" startWordPosition="176" endWordPosition="179">s annotated for both name and nominal mentions. Additionally, we evaluate three types of neural embeddings for representing Chinese text. Finally, we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1S</context>
</contexts>
<marker>Dredze, McNamee, Rao, Gerber, Finin, 2010</marker>
<rawString>Mark Dredze, Paul McNamee, Delip Rao, Adam Gerber, and Tim Finin. 2010. Entity disambiguation for knowledge base population. In Conference on Computational Linguistics (Coling).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huiming Duan</author>
<author>Zhifang Sui</author>
<author>Ye Tian</author>
<author>Wenjie Li</author>
</authors>
<title>The cips-sighan clp 2012 chineseword segmentation onmicroblog corpora bakeoff.</title>
<date>2012</date>
<booktitle>In Second CIPS-SIGHAN Joint Conference on Chinese Language Processing,</booktitle>
<pages>35--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Tianjin, China,</location>
<contexts>
<context position="3829" citStr="Duan et al., 2012" startWordPosition="626" endWordPosition="629">sages. We consider both name and nominal mentions, with the goal of supporting downstream systems, such as coreference resolution. Notably, our results reveal that the gap between social media and traditional text for Chinese is much larger than similar corpora for English, suggesting this task as an interesting area of future work.4 2) We evaluate three types of embeddings for Chinese text based on their inclusion in a downstream task. We include results with and without fine-tuning. 3) We present a joint ob3Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Ch</context>
</contexts>
<marker>Duan, Sui, Tian, Li, 2012</marker>
<rawString>Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li. 2012. The cips-sighan clp 2012 chineseword segmentation onmicroblog corpora bakeoff. In Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 35–40, Tianjin, China, December. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Etter</author>
<author>Francis Ferraro</author>
<author>Ryan Cotterell</author>
<author>Olivia Buzek</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Nerit: Named entity recognition for informal text.</title>
<date>2013</date>
<tech>Technical report, Technical Report 11,</tech>
<institution>Human Language Technology Center of Excellence, Johns Hopkins University,</institution>
<marker>Etter, Ferraro, Cotterell, Buzek, Van Durme, 2013</marker>
<rawString>David Etter, Francis Ferraro, Ryan Cotterell, Olivia Buzek, and Benjamin Van Durme. 2013. Nerit: Named entity recognition for informal text. Technical report, Technical Report 11, Human Language Technology Center of Excellence, Johns Hopkins University, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoshan Fang</author>
<author>Jianfeng Gao</author>
<author>Huanye Sheng</author>
</authors>
<title>A semi-supervised approach to build annotated corpus for chinese named entity recognition.</title>
<date>2004</date>
<booktitle>In Oliver Streiter</booktitle>
<pages>129--133</pages>
<editor>and Qin Lu, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="4598" citStr="Fang et al., 2004" startWordPosition="749" endWordPosition="752">This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are worse in social media, </context>
</contexts>
<marker>Fang, Gao, Sheng, 2004</marker>
<rawString>Xiaoshan Fang, Jianfeng Gao, and Huanye Sheng. 2004. A semi-supervised approach to build annotated corpus for chinese named entity recognition. In Oliver Streiter and Qin Lu, editors, ACL SIGHAN Workshop 2004, pages 129–133, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>William Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In NAACL Workshop on Creating Speech and Language Data With Mechanical Turk.</booktitle>
<contexts>
<context position="1439" citStr="Finin et al., 2010" startWordPosition="226" endWordPosition="229">a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (c</context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, William Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in twitter data with crowdsourcing. In NAACL Workshop on Creating Speech and Language Data With Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16783" citStr="Finkel et al., 2005" startWordPosition="2737" endWordPosition="2740">al corpus. We divided the corpus into 7 folds, each with 127 messages, where each message corresponds to a single instance. We use the first 5 folds for train, the 6th for development, and the 7th for test. We make our code and the annotated corpus available.10 We constructed an additional corpus of unlabeled messages for training the embeddings. We randomly selected 2,259,434 messages from the same time period as above. 5 Experiments We evaluate our methods under two settings: training on only name mentions, and training on both name and nominal mentions. We re-train the Stanford NER system (Finkel et al., 2005) as a baseline; besides, we also evaluate our implementation of the CRF from Mao et al. (2008) as described in §2 as Baseline Features. To this baseline, we add each of our three embedding models: word, character, character+position (as described in §3), and report results on the modified 10https://github.com/hltcoe/golden-horse X log p(wt+j|wt), (1) −c&lt;j&lt;c,j�0 T 551 Method Without Fine Tuning Dev Without Fine Tuning Test Precision Recall F1 With Fine Tuning Precision Recall F1 With Fine Tuning Precision Recall F1 Precision Recall F1 Stanford 63.51 23.27 34.06 55.70 22.86 33.06 Baseline Featur</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Association for Computational Linguistics (ACL), pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hege Fromreide</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Crowdsourcing and annotating NER for Twitter# drift.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="1502" citStr="Fromreide et al., 2014" startWordPosition="238" endWordPosition="241">ction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previously used in NLP researc</context>
<context position="3939" citStr="Fromreide et al. (2014)" startWordPosition="645" endWordPosition="648">s coreference resolution. Notably, our results reveal that the gap between social media and traditional text for Chinese is much larger than similar corpora for English, suggesting this task as an interesting area of future work.4 2) We evaluate three types of embeddings for Chinese text based on their inclusion in a downstream task. We include results with and without fine-tuning. 3) We present a joint ob3Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, </context>
</contexts>
<marker>Fromreide, Hovy, Søgaard, 2014</marker>
<rawString>Hege Fromreide, Dirk Hovy, and Anders Søgaard. 2014. Crowdsourcing and annotating NER for Twitter# drift. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengyan He</author>
<author>Houfeng Wang</author>
<author>Sujian Li</author>
</authors>
<title>The task 2 of cips-sighan 2012 named entity recognition and disambiguation in chinese bakeoff.</title>
<date>2012</date>
<booktitle>In Second CIPS-SIGHAN Joint Conference on Chinese Language Processing,</booktitle>
<pages>108--114</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Tianjin, China,</location>
<contexts>
<context position="1367" citStr="He et al., 2012" startWordPosition="215" endWordPosition="218"> use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the stan</context>
<context position="4560" citStr="He et al., 2012" startWordPosition="741" endWordPosition="744">red to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. Thes</context>
</contexts>
<marker>He, Wang, Li, 2012</marker>
<rawString>Zhengyan He, Houfeng Wang, and Sujian Li. 2012a. The task 2 of cips-sighan 2012 named entity recognition and disambiguation in chinese bakeoff. In Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 108–114, Tianjin, China, December. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengyan He</author>
<author>Houfeng Wang</author>
<author>Sujian Li</author>
</authors>
<title>The task 2 of cips-sighan 2012 named entity recognition and disambiguation in chinese bakeoff.</title>
<date>2012</date>
<booktitle>In Second CIPS-SIGHAN Joint Conference on Chinese Language Processing,</booktitle>
<pages>108--114</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Tianjin, China,</location>
<contexts>
<context position="1367" citStr="He et al., 2012" startWordPosition="215" endWordPosition="218"> use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the stan</context>
<context position="4560" citStr="He et al., 2012" startWordPosition="741" endWordPosition="744">red to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. Thes</context>
</contexts>
<marker>He, Wang, Li, 2012</marker>
<rawString>Zhengyan He, Houfeng Wang, and Sujian Li. 2012b. The task 2 of cips-sighan 2012 named entity recognition and disambiguation in chinese bakeoff. In Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 108–114, Tianjin, China, December. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangjin Jin</author>
<author>Xiao Chen</author>
</authors>
<title>The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging.</title>
<date>2008</date>
<booktitle>In Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>69</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1350" citStr="Jin and Chen, 2008" startWordPosition="211" endWordPosition="214">mbeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to Engl</context>
<context position="4543" citStr="Jin and Chen, 2008" startWordPosition="737" endWordPosition="740"> et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses </context>
</contexts>
<marker>Jin, Chen, 2008</marker>
<rawString>Guangjin Jin and Xiao Chen. 2008. The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging. In Sixth SIGHAN Workshop on Chinese Language Processing, page 69. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenliang Li</author>
<author>Jianshu Weng</author>
<author>Qi He</author>
<author>Yuxia Yao</author>
<author>Anwitaman Datta</author>
<author>Aixin Sun</author>
<author>Bu-Sung Lee</author>
</authors>
<title>Twiner: Named entity recognition in targeted twitter stream.</title>
<date>2012</date>
<booktitle>In SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12,</booktitle>
<pages>721--730</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1519" citStr="Li et al., 2012" startWordPosition="242" endWordPosition="245">nition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2</context>
</contexts>
<marker>Li, Weng, He, Yao, Datta, Sun, Lee, 2012</marker>
<rawString>Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, Anwitaman Datta, Aixin Sun, and Bu-Sung Lee. 2012. Twiner: Named entity recognition in targeted twitter stream. In SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12, pages 721–730, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Guang Xiang</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Microblogs as parallel corpora.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2123" citStr="Ling et al., 2013" startWordPosition="344" endWordPosition="347">i et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2013)) and the challenges faced in processing Chinese language data. One approach is to utilize lexical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag chara</context>
</contexts>
<marker>Ling, Xiang, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang Ling, Guang Xiang, Chris Dyer, Alan Black, and Isabel Trancoso. 2013. Microblogs as parallel corpora. In Association for Computational Linguistics (ACL). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistics Data Consortium</author>
</authors>
<date>2014</date>
<journal>DEFT ERE Annotation Guidelines: Entities.</journal>
<contexts>
<context position="15020" citStr="Consortium, 2014" startWordPosition="2455" endWordPosition="2456"> skipgram objective. We avoided over-fitting using early-stopping. For simplicity, we set C = 1 for (2). The CRF was trained using stochastic gradient descent with an L2 regularizer. All model hyper-parameters were tuned on dev data. We use the off-the-shelf tool word2vec (Mikolov et al., 2013) to do skip-gram training for language model, and implement our own CRF model to modify the embeddings. We optimize (2) by alternating the optimzation of each of the two objectives. 4 Weibo NER Corpus We constructed a corpus of Weibo messages annotated for NER. We followed the DEFT ERE (Linguistics Data Consortium, 2014) 9 annotation 9See Aguilar et al. (2014) for a comparison of DEFT ERE with other common standards. Entity Type Name Mentions Total Nominal Geo-political 243 0 243 Location 88 38 126 Organization 224 31 255 Person 721 636 1,357 Table 1: Mention statistics for the Weibo NER corpus. guidelines for entities, which includes four major semantic types: person, organization, location and geo-political entity. We annotated both name and nominal mentions. Chinese pronoun mentions can be easily recognized with a regular expression. We used Amazon Mechanical Turk, using standard methods of multiple annota</context>
</contexts>
<marker>Consortium, 2014</marker>
<rawString>Linguistics Data Consortium. 2014. DEFT ERE Annotation Guidelines: Entities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Shaodian Zhang</author>
<author>Furu Wei</author>
<author>Ming Zhou</author>
</authors>
<title>Recognizing named entities in tweets.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>359--367</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1457" citStr="Liu et al., 2011" startWordPosition="230" endWordPosition="233">r a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size </context>
</contexts>
<marker>Liu, Zhang, Wei, Zhou, 2011</marker>
<rawString>Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming Zhou. 2011. Recognizing named entities in tweets. In Association for Computational Linguistics (ACL), pages 359–367. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Furu Wei</author>
<author>Zhongyang Fu</author>
<author>Xiangyang Zhou</author>
</authors>
<title>Joint inference of named entity recognition and normalization for tweets.</title>
<date>2012</date>
<booktitle>In Association for Computational Linguistics (ACL), ACL ’12,</booktitle>
<pages>526--535</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1538" citStr="Liu et al., 2012" startWordPosition="246" endWordPosition="249"> more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2013)) and the chall</context>
</contexts>
<marker>Liu, Zhou, Wei, Fu, Zhou, 2012</marker>
<rawString>Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu, and Xiangyang Zhou. 2012. Joint inference of named entity recognition and normalization for tweets. In Association for Computational Linguistics (ACL), ACL ’12, pages 526–535, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Liu</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
<author>Tomoya Iwakura</author>
</authors>
<title>Learning character representations for chinese word segmentation.</title>
<date>2014</date>
<booktitle>In NIPS 2014 Workshop on Modern Machine Learning and Natural Language Processing.</booktitle>
<contexts>
<context position="2832" citStr="Liu et al., 2014" startWordPosition="457" endWordPosition="460">xical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks, such as NER, remains untested. We explore several types of embeddings for Chinese text and their effect on Chinese social media NER. Specifically, we make the following contributions. 1) We present the first system for NER on Chinese social media using a new corpus based on Weibo messages. We consider both name and nominal mentions, with the goal of supporting downstream systems, such as coreference resolution. Notably, our results reveal that the gap between social media and traditional text for Chi</context>
<context position="9263" citStr="Liu et al., 2014" startWordPosition="1507" endWordPosition="1510">embedding for the current word/character. Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation 6Our implementation obtains an F1 of 88.63%. 549 system7 over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word. Since the NER system tags characters, we add the same word embedding features to each character in the word. Character Embeddings We learn an embedding for each character in the training corpus (Sun et al., 2014; Liu et al., 2014).This removes the dependency on pre-processing the text, and better fits our intended use case: NER tagging over characters. Since there are many fewer characters than words, we learn many fewer embeddings. On the one hand, this means fewer parameters and less over-fitting. However, the reduction in parameters comes with a loss of specificity, where we may be unable to learn different behaviors of a character in different settings. We explore a compromise approach in the next section. These embeddings are directly incorporated into the NER system by adding embedding features for each character</context>
</contexts>
<marker>Liu, Duh, Matsumoto, Iwakura, 2014</marker>
<rawString>Xiaodong Liu, Kevin Duh, Yuji Matsumoto, and Tomoya Iwakura. 2014. Learning character representations for chinese word segmentation. In NIPS 2014 Workshop on Modern Machine Learning and Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinnian Mao</author>
<author>Yuan Dong</author>
<author>Saike He</author>
<author>Sencheng Bao</author>
<author>Haila Wang</author>
</authors>
<title>Chinese word segmentation and named entity recognition based on conditional random fields.</title>
<date>2008</date>
<booktitle>In IJCNLP,</booktitle>
<pages>90--93</pages>
<contexts>
<context position="2756" citStr="Mao et al., 2008" startWordPosition="445" endWordPosition="448">nges faced in processing Chinese language data. One approach is to utilize lexical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks, such as NER, remains untested. We explore several types of embeddings for Chinese text and their effect on Chinese social media NER. Specifically, we make the following contributions. 1) We present the first system for NER on Chinese social media using a new corpus based on Weibo messages. We consider both name and nominal mentions, with the goal of supporting downstream systems, such as coreference resolution. Notably, our r</context>
<context position="5476" citStr="Mao et al. (2008)" startWordPosition="884" endWordPosition="887">rs, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are worse in social media, which has worse word segmentation. Additionally, typical Chinese corpora use exclusively traditional or simplified characters, whereas social media mixes them. Figure 1 demonstrates some challenges. The baseline system for our task is our own implementation of Mao et al. (2008), which is the current state-of-the-art on the SIGHAN 2008 shared task (Jin and Chen, 2008). They use a CRF tagger with a BIOSE (begin, inside, outside, singleton, end) encoding that tags individual characters, not words, since word segmentation errors are especially problematic for NER (Zhang et al., 2006). Features include many common English NER features, e.g. character unigrams and bigrams, with context windows of size 5. See Mao et al. (2008) for complete details on their system. Mao et al. (2008) use a two pass approach, training a CRF first for mention detection and using the resulting </context>
<context position="16877" citStr="Mao et al. (2008)" startWordPosition="2756" endWordPosition="2759">onds to a single instance. We use the first 5 folds for train, the 6th for development, and the 7th for test. We make our code and the annotated corpus available.10 We constructed an additional corpus of unlabeled messages for training the embeddings. We randomly selected 2,259,434 messages from the same time period as above. 5 Experiments We evaluate our methods under two settings: training on only name mentions, and training on both name and nominal mentions. We re-train the Stanford NER system (Finkel et al., 2005) as a baseline; besides, we also evaluate our implementation of the CRF from Mao et al. (2008) as described in §2 as Baseline Features. To this baseline, we add each of our three embedding models: word, character, character+position (as described in §3), and report results on the modified 10https://github.com/hltcoe/golden-horse X log p(wt+j|wt), (1) −c&lt;j&lt;c,j�0 T 551 Method Without Fine Tuning Dev Without Fine Tuning Test Precision Recall F1 With Fine Tuning Precision Recall F1 With Fine Tuning Precision Recall F1 Precision Recall F1 Stanford 63.51 23.27 34.06 55.70 22.86 33.06 Baseline Features 63.51 27.17 38.06 56.98 25.26 35.00 + word 65.71 26.59 37.86 70.97 25.43 37.45 56.82 25.77 </context>
</contexts>
<marker>Mao, Dong, He, Bao, Wang, 2008</marker>
<rawString>Xinnian Mao, Yuan Dong, Saike He, Sencheng Bao, and Haila Wang. 2008. Chinese word segmentation and named entity recognition based on conditional random fields. In IJCNLP, pages 90–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In North America Chapter of Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="1305" citStr="McCallum and Li, 2003" startWordPosition="203" endWordPosition="206"> we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Sp</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In North America Chapter of Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Neural Information Processing Systems (NIPS),</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="13434" citStr="Mikolov et al., 2013" startWordPosition="2186" endWordPosition="2189"> which are the embeddings (as defined above). As a result, the objective is no longer log-linear, but log-bilinear 8. 8It is log-bilinear because the log-likelihood takes the form f(x, y) = axy + bx + cy, where x, y are variables and a, b, c are coefficients. In this case, x is the feature weight and y is the embedding; both of them are vectors. Taking the partial derivative with respect to any one of the variables, one gets a constant (wrt that variable). This satisfies the definition of log-bilinear functions. 1 X K k = 550 The second term is the standard skip-gram language model objective (Mikolov et al., 2013): T 1 Lu(ew) = Tt=1 where T p( wi|wj) = exp (ewi ewj ~ Fi, exp eT wi, ewj The first objective is notated Ls for “supervised” (trained on labeled NER data), and the second is Lu, “unsupervised” (trained on raw text.) Both objectives share the same variables ew. The overall goal is to maximize their weighted sum: arg max = Ls(λ, ew) + CLu(ew) (2) ew where C is a tradeoff parameter. 3.3 Parameter Estimation We pre-trained embeddings using word2vec (Mikolov et al., 2013) with the skip-gram training objective and NEC negative sampling. Unless otherwise stated, we used word2vec’s default parameter s</context>
<context position="14698" citStr="Mikolov et al., 2013" startWordPosition="2399" endWordPosition="2402">al, and we used the same embeddings for the input and output parameters in the skipgram objective. We optimized the joint objective (2) using an alternative optimization strategy: we alternated 30 iterations of CRF training on the NE labeled data and 5 multi-threaded passes through both the labeled and unlabeled data for the skipgram objective. We avoided over-fitting using early-stopping. For simplicity, we set C = 1 for (2). The CRF was trained using stochastic gradient descent with an L2 regularizer. All model hyper-parameters were tuned on dev data. We use the off-the-shelf tool word2vec (Mikolov et al., 2013) to do skip-gram training for language model, and implement our own CRF model to modify the embeddings. We optimize (2) by alternating the optimzation of each of the two objectives. 4 Weibo NER Corpus We constructed a corpus of Weibo messages annotated for NER. We followed the DEFT ERE (Linguistics Data Consortium, 2014) 9 annotation 9See Aguilar et al. (2014) for a comparison of DEFT ERE with other common standards. Entity Type Name Mentions Total Nominal Geo-political 243 0 243 Location 88 38 126 Organization 224 31 255 Person 721 636 1,357 Table 1: Mention statistics for the Weibo NER corpu</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Neural Information Processing Systems (NIPS), pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Lingvisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1330" citStr="Nadeau and Sekine, 2007" startWordPosition="207" endWordPosition="210">ining objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is q</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Passos</author>
<author>Vineet Kumar</author>
<author>Andrew McCallum</author>
</authors>
<title>Lexicon infused phrase embeddings for named entity resolution.</title>
<date>2014</date>
<location>CoRR, abs/1404.5367.</location>
<contexts>
<context position="2326" citStr="Passos et al., 2014" startWordPosition="376" endWordPosition="379"> the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2013)) and the challenges faced in processing Chinese language data. One approach is to utilize lexical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks,</context>
<context position="7848" citStr="Passos et al., 2014" startWordPosition="1266" endWordPosition="1269"> al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embeddings represent words in a continuous low dimensional space, which can capture semantic or syntactic properties of the lexicon: similar words would have similar low dimensional vector representations. Embeddings have been used to gain improvements in a variety of NLP tasks. In NER specifically, several papers have shown improvements by using pre-trained neural embeddings as features in standard NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014). More recently, these improvements have been demonstrated on Twitter data (Cherry and Guo, 2015). Embeddings are especially helpful when there is little training data, since they can be trained on a large amount of unlabeled data. This is the case for new languages and domains, the task we face in this paper. However, training embeddings for Chinese is not straightforward: Chinese is not word segmented, so embeddings for each word cannot be trained on a raw corpus. Additionally, the stateof-the-art systems for downstream Chinese tasks, such as NER, may not use words. We present three types of</context>
</contexts>
<marker>Passos, Kumar, McCallum, 2014</marker>
<rawString>Alexandre Passos, Vineet Kumar, and Andrew McCallum. 2014. Lexicon infused phrase embeddings for named entity resolution. CoRR, abs/1404.5367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Chang Baobao</author>
</authors>
<title>Maxmargin tensor neural network for chinese word segmentation.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="11318" citStr="Pei et al., 2014" startWordPosition="1826" endWordPosition="1829">er embeddings, and 24,818 character with position embeddings. 3.1 Fine Tuning For each of the embeddings, we fine-tune pretrained embeddings in the context of the NER task. This corresponds to initializing the embeddings parameters using a pre-trained model, and then modifying the parameters during gradient updates of the NER model by back-propogating gradients. 7We use Jieba for segmentation: https://github. com/fxsjy/jieba This is a standard method that has been previously explored in sequential and structured prediction problem (Collobert et al., 2011; Zheng et al., 2013; Yao et al., 2014; Pei et al., 2014). 3.2 Joint Training Objectives Fine-tuning has a disadvantage: it can arbitrarily deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other embeddings, even those that do not a</context>
</contexts>
<marker>Pei, Ge, Baobao, 2014</marker>
<rawString>Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Maxmargin tensor neural network for chinese word segmentation. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siyu Qiu</author>
<author>Qing Cui</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Co-learning of word representations and morpheme representations.</title>
<date>2014</date>
<booktitle>In Conference on Computational Linguistics (Coling).</booktitle>
<contexts>
<context position="2868" citStr="Qiu et al., 2014" startWordPosition="465" endWordPosition="468">ems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks, such as NER, remains untested. We explore several types of embeddings for Chinese text and their effect on Chinese social media NER. Specifically, we make the following contributions. 1) We present the first system for NER on Chinese social media using a new corpus based on Weibo messages. We consider both name and nominal mentions, with the goal of supporting downstream systems, such as coreference resolution. Notably, our results reveal that the gap between social media and traditional text for Chinese is much larger than similar cor</context>
</contexts>
<marker>Qiu, Cui, Bian, Gao, Liu, 2014</marker>
<rawString>Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Co-learning of word representations and morpheme representations. In Conference on Computational Linguistics (Coling).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
<author>Doug Downey</author>
<author>Mike Anderson</author>
</authors>
<title>Local and global algorithms for disambiguation to wikipedia.</title>
<date>2011</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>1375--1384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1180" citStr="Ratinov et al., 2011" startWordPosition="180" endWordPosition="183">name and nominal mentions. Additionally, we evaluate three types of neural embeddings for representing Chinese text. Finally, we propose a joint training objective for the embeddings that makes use of both (NER) labeled and unlabeled raw text. Our methods yield a 9% improvement over a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name a</context>
</contexts>
<marker>Ratinov, Roth, Downey, Anderson, 2011</marker>
<rawString>Lev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to wikipedia. In Association for Computational Linguistics (ACL), pages 1375–1384. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1524--1534</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1478" citStr="Ritter et al., 2011" startWordPosition="234" endWordPosition="237">t baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1, is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previo</context>
<context position="3888" citStr="Ritter et al. (2011)" startWordPosition="636" endWordPosition="639">he goal of supporting downstream systems, such as coreference resolution. Notably, our results reveal that the gap between social media and traditional text for Chinese is much larger than similar corpora for English, suggesting this task as an interesting area of future work.4 2) We evaluate three types of embeddings for Chinese text based on their inclusion in a downstream task. We include results with and without fine-tuning. 3) We present a joint ob3Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused</context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Empirical Methods in Natural Language Processing (EMNLP), pages 1524–1534. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaming Sun</author>
<author>Lei Lin</author>
<author>Nan Yang</author>
<author>Zhenzhou Ji</author>
<author>Xiaolong Wang</author>
</authors>
<title>Radical-enhanced chinese character embedding.</title>
<date>2014</date>
<booktitle>In Neural Information Processing Systems (NIPS),</booktitle>
<pages>279--286</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2850" citStr="Sun et al., 2014" startWordPosition="461" endWordPosition="464">o improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks, such as NER, remains untested. We explore several types of embeddings for Chinese text and their effect on Chinese social media NER. Specifically, we make the following contributions. 1) We present the first system for NER on Chinese social media using a new corpus based on Weibo messages. We consider both name and nominal mentions, with the goal of supporting downstream systems, such as coreference resolution. Notably, our results reveal that the gap between social media and traditional text for Chinese is much large</context>
<context position="9244" citStr="Sun et al., 2014" startWordPosition="1503" endWordPosition="1506"> dimension of the embedding for the current word/character. Word Embeddings We train an embedding for each word type, the standard approach in other languages. We run a Chinese word segmentation 6Our implementation obtains an F1 of 88.63%. 549 system7 over the raw corpus of Weibo messages. To create features, we first segment the NER data, and then lookup the embedding that matches the segmented word. Since the NER system tags characters, we add the same word embedding features to each character in the word. Character Embeddings We learn an embedding for each character in the training corpus (Sun et al., 2014; Liu et al., 2014).This removes the dependency on pre-processing the text, and better fits our intended use case: NER tagging over characters. Since there are many fewer characters than words, we learn many fewer embeddings. On the one hand, this means fewer parameters and less over-fitting. However, the reduction in parameters comes with a loss of specificity, where we may be unable to learn different behaviors of a character in different settings. We explore a compromise approach in the next section. These embeddings are directly incorporated into the NER system by adding embedding features</context>
</contexts>
<marker>Sun, Lin, Yang, Ji, Wang, 2014</marker>
<rawString>Yaming Sun, Lei Lin, Nan Yang, Zhenzhou Ji, and Xiaolong Wang. 2014. Radical-enhanced chinese character embedding. In Neural Information Processing Systems (NIPS), pages 279–286. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2304" citStr="Turian et al., 2010" startWordPosition="372" endWordPosition="375">ese social media from the popular Sina Weibo service, both because of 1Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2013)) and the challenges faced in processing Chinese language data. One approach is to utilize lexical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusio</context>
<context position="7826" citStr="Turian et al., 2010" startWordPosition="1262" endWordPosition="1265"> reported in Zhang et al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embeddings represent words in a continuous low dimensional space, which can capture semantic or syntactic properties of the lexicon: similar words would have similar low dimensional vector representations. Embeddings have been used to gain improvements in a variety of NLP tasks. In NER specifically, several papers have shown improvements by using pre-trained neural embeddings as features in standard NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014). More recently, these improvements have been demonstrated on Twitter data (Cherry and Guo, 2015). Embeddings are especially helpful when there is little training data, since they can be trained on a large amount of unlabeled data. This is the case for new languages and domains, the task we face in this paper. However, training embeddings for Chinese is not straightforward: Chinese is not word segmented, so embeddings for each word cannot be trained on a raw corpus. Additionally, the stateof-the-art systems for downstream Chinese tasks, such as NER, may not use words. We </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Association for Computational Linguistics (ACL), pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianxiang Yang</author>
<author>Heyan Huang</author>
<author>Xin Xin</author>
<author>Quanchao Liu</author>
<author>Xiaochi Wei</author>
</authors>
<title>Domain-specific product named entity recognition from chinese microblog.</title>
<date>2014</date>
<booktitle>In Computational Intelligence and Security (CIS), 2014 Tenth International Conference on,</booktitle>
<pages>218--222</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6564" citStr="Yang et al. (2014)" startWordPosition="1064" endWordPosition="1067">ls on their system. Mao et al. (2008) use a two pass approach, training a CRF first for mention detection and using the resulting predictions as a feature for an NER system. Furthermore, they make extensive use of gazetteer features. For simplicity, we exclude the first pass mention detection and the gazetteer features, which make only small improvements to their overall performance. We note that other implementations of this system (Zhang et al., 2013) have been unable to match the performance reported in Mao et al. (2008). Similarly, our implementation yields results on SIGHAN 2008 similar 5Yang et al. (2014) consider a related problem of identifying product mentions in Weibo messages. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 m的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 美得呀~顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful 看见前女友和她的新欢走在一起的时候,已经无处可躲了,只好 硬着 头皮上去打招呼哎呀,好久不见,你儿子都这么高了。 When saw ex-girl friend and her new partner coming across, nowhere to hide, have to say hello, long time no see, your son grown up. Figure 1: Examples of Weibos messages and translations with named </context>
</contexts>
<marker>Yang, Huang, Xin, Liu, Wei, 2014</marker>
<rawString>Xianxiang Yang, Heyan Huang, Xin Xin, Quanchao Liu, and Xiaochi Wei. 2014. Domain-specific product named entity recognition from chinese microblog. In Computational Intelligence and Security (CIS), 2014 Tenth International Conference on, pages 218–222. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaisheng Yao</author>
<author>Baolin Peng</author>
<author>Geoffrey Zweig</author>
<author>Dong Yu</author>
<author>Xiaolong Li</author>
<author>Feng Gao</author>
</authors>
<title>Recurrent conditional random field for language understanding.</title>
<date>2014</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on,</booktitle>
<pages>4077--4081</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="11299" citStr="Yao et al., 2014" startWordPosition="1822" endWordPosition="1825">gs, 10,912 character embeddings, and 24,818 character with position embeddings. 3.1 Fine Tuning For each of the embeddings, we fine-tune pretrained embeddings in the context of the NER task. This corresponds to initializing the embeddings parameters using a pre-trained model, and then modifying the parameters during gradient updates of the NER model by back-propogating gradients. 7We use Jieba for segmentation: https://github. com/fxsjy/jieba This is a standard method that has been previously explored in sequential and structured prediction problem (Collobert et al., 2011; Zheng et al., 2013; Yao et al., 2014; Pei et al., 2014). 3.2 Joint Training Objectives Fine-tuning has a disadvantage: it can arbitrarily deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other embeddings, even </context>
</contexts>
<marker>Yao, Peng, Zweig, Yu, Li, Gao, 2014</marker>
<rawString>Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Dong Yu, Xiaolong Li, and Feng Gao. 2014. Recurrent conditional random field for language understanding. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 4077–4081. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Improving lexical embeddings with semantic knowledge.</title>
<date>2014</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>545--550</pages>
<contexts>
<context position="11615" citStr="Yu and Dredze (2014)" startWordPosition="1873" endWordPosition="1876">uring gradient updates of the NER model by back-propogating gradients. 7We use Jieba for segmentation: https://github. com/fxsjy/jieba This is a standard method that has been previously explored in sequential and structured prediction problem (Collobert et al., 2011; Zheng et al., 2013; Yao et al., 2014; Pei et al., 2014). 3.2 Joint Training Objectives Fine-tuning has a disadvantage: it can arbitrarily deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other embeddings, even those that do not appear in the labeled training data. This type of training can help improve OOVs (Yu and Dredze, 2015), an important aspect of improving social media NER. We propose to jointly learn embeddings for both language models and the NER task. The modified objective function (log-likelihood) for the CRF </context>
</contexts>
<marker>Yu, Dredze, 2014</marker>
<rawString>Mo Yu and Mark Dredze. 2014. Improving lexical embeddings with semantic knowledge. In Association for Computational Linguistics (ACL), pages 545–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Learning composition models for phrase embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>3</volume>
<pages>242</pages>
<contexts>
<context position="12019" citStr="Yu and Dredze, 2015" startWordPosition="1935" endWordPosition="1938">y deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other embeddings, even those that do not appear in the labeled training data. This type of training can help improve OOVs (Yu and Dredze, 2015), an important aspect of improving social media NER. We propose to jointly learn embeddings for both language models and the NER task. The modified objective function (log-likelihood) for the CRF is given by: Ls(A, ew) ⎡ ⎤ 1log Z(x)k +XλjFj(yk, xk, ew) j where K is the number of instances, A is the weight vector, xk and yk are the words and labels sequence for each instance, ew is the embedding for a word/character/character-position representation w, Z(x)k is the normalization factor for each instance, and Fj(yk, xk, ew) = Pni=1 fj(yki−1, yki , xk, ew, i) represents the feature function in wh</context>
</contexts>
<marker>Yu, Dredze, 2015</marker>
<rawString>Mo Yu and Mark Dredze. 2015. Learning composition models for phrase embeddings. Transactions of the Association for Computational Linguistics, 3:227– 242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suxiang Zhang</author>
<author>Ying Qin</author>
<author>Juan Wen</author>
<author>Xiaojie Wang</author>
</authors>
<title>Word segmentation and named entity recognition for sighan bakeoff3.</title>
<date>2006</date>
<booktitle>In Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>158--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4523" citStr="Zhang et al., 2006" startWordPosition="733" endWordPosition="736">(2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundari</context>
<context position="5784" citStr="Zhang et al., 2006" startWordPosition="934" endWordPosition="937">blems are worse in social media, which has worse word segmentation. Additionally, typical Chinese corpora use exclusively traditional or simplified characters, whereas social media mixes them. Figure 1 demonstrates some challenges. The baseline system for our task is our own implementation of Mao et al. (2008), which is the current state-of-the-art on the SIGHAN 2008 shared task (Jin and Chen, 2008). They use a CRF tagger with a BIOSE (begin, inside, outside, singleton, end) encoding that tags individual characters, not words, since word segmentation errors are especially problematic for NER (Zhang et al., 2006). Features include many common English NER features, e.g. character unigrams and bigrams, with context windows of size 5. See Mao et al. (2008) for complete details on their system. Mao et al. (2008) use a two pass approach, training a CRF first for mention detection and using the resulting predictions as a feature for an NER system. Furthermore, they make extensive use of gazetteer features. For simplicity, we exclude the first pass mention detection and the gazetteer features, which make only small improvements to their overall performance. We note that other implementations of this system (</context>
</contexts>
<marker>Zhang, Qin, Wen, Wang, 2006</marker>
<rawString>Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie Wang. 2006. Word segmentation and named entity recognition for sighan bakeoff3. In Fifth SIGHAN Workshop on Chinese Language Processing, pages 158–161, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longkai Zhang</author>
<author>Houfeng Wang</author>
<author>Xu Sun</author>
<author>Mairgup Mansur</author>
</authors>
<title>Exploring representations from unlabeled data with co-training for chinese word segmentation.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6403" citStr="Zhang et al., 2013" startWordPosition="1036" endWordPosition="1039">. Features include many common English NER features, e.g. character unigrams and bigrams, with context windows of size 5. See Mao et al. (2008) for complete details on their system. Mao et al. (2008) use a two pass approach, training a CRF first for mention detection and using the resulting predictions as a feature for an NER system. Furthermore, they make extensive use of gazetteer features. For simplicity, we exclude the first pass mention detection and the gazetteer features, which make only small improvements to their overall performance. We note that other implementations of this system (Zhang et al., 2013) have been unable to match the performance reported in Mao et al. (2008). Similarly, our implementation yields results on SIGHAN 2008 similar 5Yang et al. (2014) consider a related problem of identifying product mentions in Weibo messages. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 m的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 美得呀~顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful 看见前女友和她的新欢走在一起的时候,已经无处可躲了,只好 硬着 头皮上去打招呼哎呀,好久不见,你儿子都这么高了。 When saw ex-girl friend and her new p</context>
</contexts>
<marker>Zhang, Wang, Sun, Mansur, 2013</marker>
<rawString>Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup Mansur. 2013. Exploring representations from unlabeled data with co-training for chinese word segmentation. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqing Zheng</author>
<author>Hanyang Chen</author>
<author>Tianyu Xu</author>
</authors>
<title>Deep learning for chinese word segmentation and pos tagging.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>647--657</pages>
<contexts>
<context position="11281" citStr="Zheng et al., 2013" startWordPosition="1818" endWordPosition="1821">79,809 word embeddings, 10,912 character embeddings, and 24,818 character with position embeddings. 3.1 Fine Tuning For each of the embeddings, we fine-tune pretrained embeddings in the context of the NER task. This corresponds to initializing the embeddings parameters using a pre-trained model, and then modifying the parameters during gradient updates of the NER model by back-propogating gradients. 7We use Jieba for segmentation: https://github. com/fxsjy/jieba This is a standard method that has been previously explored in sequential and structured prediction problem (Collobert et al., 2011; Zheng et al., 2013; Yao et al., 2014; Pei et al., 2014). 3.2 Joint Training Objectives Fine-tuning has a disadvantage: it can arbitrarily deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other</context>
</contexts>
<marker>Zheng, Chen, Xu, 2013</marker>
<rawString>Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmentation and pos tagging. In Empirical Methods in Natural Language Processing (EMNLP), pages 647–657.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Mu Li</author>
<author>Jianfeng Gao</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Single character chinese named entity recognition.</title>
<date>2003</date>
<booktitle>In Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>125--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="4579" citStr="Zhu et al., 2003" startWordPosition="745" endWordPosition="748">sults in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548–554, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 2 NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are wors</context>
</contexts>
<marker>Zhu, Li, Gao, Huang, 2003</marker>
<rawString>Xiaodan Zhu, Mu Li, Jianfeng Gao, and Chang-Ning Huang. 2003. Single character chinese named entity recognition. In Second SIGHAN Workshop on Chinese Language Processing, pages 125–132, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030488">
<title confidence="0.9982335">
Chinese Semantic Role Labeling with Bidirectional Recurrent Neural
Networks
</title>
<author confidence="0.999538">
Zhen Wang, Tingsong Jiang, Baobao Chang, Zhifang Sui
</author>
<affiliation confidence="0.937911666666667">
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
Collaborative Innovation Center for Language Ability, Xuzhou 221009 China
</affiliation>
<email confidence="0.972215">
wzpkuer@gmail.com, {tingsong, chbb, szf}@pku.edu.cn
</email>
<sectionHeader confidence="0.997228" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999881555555556">
Traditional approaches to Chinese Seman-
tic Role Labeling (SRL) almost heavily re-
ly on feature engineering. Even worse,
the long-range dependencies in a sentence
can hardly be modeled by these method-
s. In this paper, we introduce bidirection-
al recurrent neural network (RNN) with
long-short-term memory (LSTM) to cap-
ture bidirectional and long-range depen-
dencies in a sentence with minimal fea-
ture engineering. Experimental results on
Chinese Proposition Bank (CPB) show a
significant improvement over the state-of-
the-art methods. Moreover, our model
makes it convenient to introduce hetero-
geneous resource, which makes a further
improvement on our experimental perfor-
mance.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999025368421053">
Semantic Role Labeling (SRL) is defined as the
task to recognize arguments for a given predicate
and assign semantic role labels to them. Because
of its ability to encode semantic information, there
has been an increasing interest in SRL on many
languages (Gildea and Jurafsky, 2002; Sun and Ju-
rafsky, 2004). Figure 1 shows an example in Chi-
nese Proposition Bank (CPB) (Xue and Palmer,
2003), which is a Chinese corpus annotated with
semantic role labels.
Traditional approaches to Chinese SRL often
extract a large number of handcrafted features
from the sentence, even its parse tree, and feed
these features to statistical classifiers such as CRF,
MaxEnt and SVM (Sun and Jurafsky, 2004; Xue,
2008; Ding and Chang, 2008; Ding and Chang,
2009; Sun, 2010). However, these methods suf-
fer from three major problems. Firstly, their per-
formances are heavily dependent on feature engi-
</bodyText>
<figureCaption confidence="0.993376">
Figure 1: A sentence with semantic roles labeled
from CPB.
</figureCaption>
<bodyText confidence="0.999860470588235">
neering, which needs domain knowledge and la-
borious work of feature extraction and selection.
Secondly, although sophisticated features are de-
signed, the long-range dependencies in a sentence
can hardly be modeled. Thirdly, a specific anno-
tated dataset is often limited in its scalability, but
the existence of heterogenous resource, which has
very different semantic role labels and annotation
schema but related latent semantic meaning, can
alleviate this problem. However, traditional meth-
ods cannot relate distinct annotation schemas and
introduce heterogeneous resource with ease.
Concerning these problems, in this paper, we
propose bidirectional recurrent neural network
(RNN) with long-short-term memory (LSTM) to
solve the problem of Chinese SRL. Our approach
makes the following contributions:
</bodyText>
<listItem confidence="0.998634928571429">
• We formulate Chinese SRL with bidirection-
al LSTM RNN model. With bidirectional
RNN, the dependencies in a sentence from
both directions can be captured, and with L-
STM architecture, long-range dependencies
can be well modeled. The test results on the
bechmark dataset CPB show a significant im-
provement over the state-of-the-art methods.
• Compared with previous work that relied on
a huge number of handcrafted features, our
model can achieve much better performance
only with minimal feature engineering.
• The framework of our model makes the intro-
duction of heterogeneous resource efficient
</listItem>
<page confidence="0.928368">
1626
</page>
<note confidence="0.932846">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1626–1631,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.999273">
Figure 2: The model architecture.
</figureCaption>
<bodyText confidence="0.9513525">
and convenient, and this can further improve
our experimental performance.
</bodyText>
<sectionHeader confidence="0.960012" genericHeader="introduction">
2 Chinese SRL with RNN
</sectionHeader>
<bodyText confidence="0.999957647058824">
Following previous work, we regard Chinese S-
RL as a task of sequence labeling, which assigns
a label for each word in the sequence. To iden-
tify the boundary information of semantic roles,
we adopt the IOBES tagging schema for the la-
bels as shown in Figure 1. For sequence labeling,
it is important to capture dependencies in the se-
quence, especially for the problem of SRL, where
the semantic role label for a word not only relies
on its local information, but also is determined by
long-range dependencies from other words. The
advantage of RNN is the ability to better capture
the contextual information, which is beneficial to
capture dependencies in SRL. Moreover, we en-
rich the basic RNN model with bidirectional LST-
M RNN, which can model bidirectional and long-
range dependencies simultaneously.
</bodyText>
<subsectionHeader confidence="0.998196">
2.1 Model Architecture
</subsectionHeader>
<bodyText confidence="0.999906">
The architecture of our approach is illustrated in
Figure 2. Given a sentence, we first get repre-
sentation for each word to be labeled. Then af-
ter a nonlinear transformation, bidirectional LST-
M RNN layer is designed to combine the local in-
formation of a word and its contextual information
from both directions. With a nonlinear layer to
form more complex features, a linear output lay-
er follows. For each word to be labeled, there is
an output vector, whose each dimension is a score
corresponding to a kind of semantic role label.
</bodyText>
<subsectionHeader confidence="0.999493">
2.2 Word Representation
</subsectionHeader>
<bodyText confidence="0.988089833333333">
Word representation captures the features locally
embedded around the word. The features used in
our work are: the current word, the current POS
tag, the predicate, left and right words, left and
right POS tags, distance to the predicate. Note that
these features are all basic information about the
word, hence we alleviate the heavy job of feature
engineering. All these features are introduced by
embeddings. After concatenation, we get the word
representation feature vector.
To get more complex features, we adopt a non-
linear transformation:
</bodyText>
<equation confidence="0.7304">
zt = f(W1xt) 1 &lt; t &lt; N
</equation>
<bodyText confidence="0.9999202">
where xt is the word representation of the t-th
word, W1 E Rn1×n0, n0 is the length of word rep-
resentation, f is an activation function and we use
tanh in our experiments, N is the number of words
to be labeled in the sequence.
</bodyText>
<subsectionHeader confidence="0.997716">
2.3 Bidirectional LSTM RNN
</subsectionHeader>
<bodyText confidence="0.999969727272727">
Representation zt only captures the local informa-
tion. Here we adopt RNN to capture contextual
information. Traditional RNN has the problem of
vanishing or exploding gradients, which means the
long-term dependencies can hardly be modeled.
LSTM is designed to mitigate this problem.
At each word position t, the LSTM RNN com-
putes six internal vectors: C, gZ, gf, go, Ct and ht
for the memory cell, which is _a structure used in
LSTM to store information. C computes the can-
didate value for the state of the memory cell:
</bodyText>
<equation confidence="0.990656">
C = f(Wczt + Ucht−1 + bc)
</equation>
<bodyText confidence="0.999898">
The activations of the memory cell’s input gate,
forget gate and output gate are defined as:
</bodyText>
<equation confidence="0.968007">
gj = Q(WjzZ + Ujht−1 + bj)
</equation>
<bodyText confidence="0.999985">
where j stands for i, f or o. Q is taken sigmoid in
experiments. Then we can compute Ct, the mem-
ory cell’s new state at position t:
</bodyText>
<equation confidence="0.892992">
Ct=gZO C_+ gf 0 Ct−1
</equation>
<page confidence="0.847585">
1627
</page>
<bodyText confidence="0.966334428571429">
where θ is an ensemble of all the parameters in the
network.
The log likelihood with a single sample is then:
where O indicates elementwise vector multiplica-
tion. With the new state of the memory cell, we
can compute the value of output state ht:
ht=goOf(Ct)
ht contains the information not only from local
representation zt, but also from previous output
state ht−1, hence can capture dependencies in
a sentence. Because the dependencies forward
and backward are both important to label seman-
tic roles, we extend LSTM with bidirectional ap-
proach, resulting in:
</bodyText>
<equation confidence="0.968952333333333">
at = [−� htT,�− htT]T 1 &lt; t &lt; N
Further, a nonlinear transformation follows:
vt = f(W2at) 1 &lt; t &lt; N
</equation>
<bodyText confidence="0.915579">
where W2 E Rn3xn2, n2 is the dimension of at.
</bodyText>
<subsectionHeader confidence="0.973864">
2.4 Output Representation
</subsectionHeader>
<bodyText confidence="0.9998235">
For each word to be labeled, we adopt linear trans-
formation to get the output vector ot:
</bodyText>
<equation confidence="0.720007">
ot = W3vt 1 &lt; t &lt; N
</equation>
<bodyText confidence="0.977168">
W3 E Rn4xn3, where n4 is the number of seman-
tic role labels in IOBES tagging schema. There-
fore, the resulting vector ot for the t-th word is of
length n4, and each dimension corresponds to the
score of a certain semantic role label.
</bodyText>
<subsectionHeader confidence="0.994837">
2.5 Training Criteria
</subsectionHeader>
<bodyText confidence="0.999843428571428">
Because there are dependencies among word la-
bels in a sentence, isolated training approach
which independently considers each word will be
inappropriate. Therefore, we adopt sentence tag
approach, in which we encourage the correct path
of tags, while discouraging all other valid paths.
Given all our training examples:
</bodyText>
<equation confidence="0.997326">
T = (x(i), y(i))
</equation>
<bodyText confidence="0.999977166666667">
where x(i) denotes the i-th training sentence, y(i)
is the corresponding Ni (the number of words to
be labeled) dimension vector, which indicates the
correct path of tags, andy(i) t= k means the t-th
word has the k-th semantic role label. The score
of x(i) along the path y(i) is defined as follows:
</bodyText>
<equation confidence="0.995345">
s(x(i), y(i), θ) =
logp(y(i)|x(i), θ) = log exp (s(x(i), y(i), θ))
Eye exp (s(x(i), yi, θ))
� �
exp s(x(i), yi, θ)
y1
</equation>
<bodyText confidence="0.999555666666667">
where yi ranges from all the valid paths of tags.
The full log likelihood of the whole training cor-
pus is as follows:
</bodyText>
<equation confidence="0.999732">
J(θ) = LT logp(y(i)|x(i), θ)
i=1
</equation>
<bodyText confidence="0.999888">
To compute the network parameter θ, we maxi-
mize the log likelihood J(θ) using stochastic gra-
dient ascent in the experiments.
</bodyText>
<subsectionHeader confidence="0.931523">
2.6 Introducing Heterogeneous Resource
</subsectionHeader>
<bodyText confidence="0.99998464">
A single annotated corpus with semantic role la-
bels is often limited in its scalability. Heteroge-
neous resource in our work is defined as another
dataset annotated with semantic roles, which also
provides predicate-argument structure annotation,
but uses very different semantic role labels and an-
notation schema. However, in spite of these differ-
ences, the latent semantic meaning may be highly
correlated. Therefore, the introduction of hetero-
geneous data can alleviate the problem of scalabil-
ity with a single annotated corpus.
Traditional approaches hardly concern the ex-
istence of heterogeneous resource and are diffi-
cult to relate different annotation schemas, but in
the framework of our model, heterogeneous data
can be introduced in a relatively convenient way.
Specifically, we learn a bidirectional LSTM RN-
N model based on heterogeneous data, then with
the fine-tuned word embeddings we initialize the
model on our experimental dataset. The princi-
ple behind is that the words almost convey the
same semantic meaning albeit in distinct annota-
tion schemas. The introduction of heterogenous
resource in this way is efficient and can lead to
performance improvement on our experiment.
</bodyText>
<sectionHeader confidence="0.999862" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999801333333333">
We conduct experiments to compare our model
with previous landmark methods on the bench-
mark dataset CPB for Chinese SRL. The result
</bodyText>
<equation confidence="0.974602428571429">
Ni
L
t=1
oty(i)
t
L
= s(x(i), y(i), θ) − log
</equation>
<page confidence="0.927942">
1628
</page>
<table confidence="0.999847375">
Remark Choice
Word embedding dimension nword = 50
POS tag dimension npos = 20
Distance dimension ndis = 20
Nonlinear layer n1 = 200
RNN layer nh = 100
Nonlinear layer n3 = 100
Learning rate α = 10−3
</table>
<tableCaption confidence="0.997344">
Table 1: Hyper parameters of our model.
</tableCaption>
<table confidence="0.999389625">
Method F1(%)
Xue (2008) 71.90
Collobert and Weston (2008) 74.05
Sun et al. (2009) 74.12
Yang and Zong (2014) 75.31
Ours (Random Initialization) 77.09
+ Standard Pre-training 77.21
+ Heterogenous Resource 77.59
</table>
<tableCaption confidence="0.999749">
Table 2: Results comparison on CPB dataset.
</tableCaption>
<bodyText confidence="0.999934555555556">
reveals that even with our basic model, which
does not resort to other resources, our approach
can significantly outperform all of the competitors.
Moreover, we enrich our work with introducing
heterogenous resource to make a further improve-
ment on performance. And the result also shows
the influence of heterogeneous resource is more
evident than the standard method of pre-training
for word embeddings.
</bodyText>
<subsectionHeader confidence="0.995986">
3.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999996952380952">
To facilitate comparison with previous work, we
conduct experiments on the standard benchmark
dataset CPB 1.0.1 We follow the same data setting
as previous work (Xue, 2008; Sun et al., 2009),
which divided the dataset into three parts: 648
files (from chtb 081.fid to chtb 899.fid) are used
as the training set. The development set includes
40 files, from chtb 041.fid to chtb 080.fid. The
test set includes 72 files, which are chtb 001.fid
to chtb 040.fid, and chtb 900.fid to chtb 931.fid.
We use another annotated corpus2 with distinct se-
mantic role labels and annotation schema, which is
designed by ourselves for other projects, as hetero-
geneous resource. This labeled dataset has 17,308
annotated sentences, and the semantic roles con-
cerned are like “agent” and “patient”, resulting in
21 kinds of types, which are all distinct from the
semantic roles defined in CPB. We use the devel-
opment set of CPB for model selection, and the
hyper parameter setting of our model is reported
in Table 1.
</bodyText>
<subsectionHeader confidence="0.998654">
3.2 Chinese SRL Performance
</subsectionHeader>
<bodyText confidence="0.9992385">
Table 2 summarizes our SRL performance com-
pared to previous landmark results. The work of
Collobert and Weston (2008) was conducted on
English SRL, we implement their approach on CP-
</bodyText>
<footnote confidence="0.9988545">
1https://catalog.ldc.upenn.edu/LDC2005T23
2This Chinese dataset is available on request.
</footnote>
<bodyText confidence="0.99974044">
B for comparison. As indicated by this table, our
approach significantly outperforms previous state-
of-the-art methods even with all parameters ran-
domly initialized, that is without introducing other
resources. This result can prove the ability of our
model to capture useful dependencies for Chinese
SRL with minimal feature engineering.
Further, we conduct experiments with the intro-
duction of heterogenous resource. Previous work
found that the performance can be improved by
pre-training the word embeddings on large unla-
beled data and using the obtained embeddings to
make initialization. With the result in Table 2,
it is true that these pre-trained word embeddings
have a good effect on our performance (we use
word2vec3 on Chinese Gigaword Corpus for word
pre-training). However, as shown in Table 2, com-
pared to standard pre-training, the influence of het-
erogenous data is more evident. We can explain
this difference via the distinction between these
two kinds of methods for performance improve-
ment. The information provided by standard pre-
training with unlabeled data is more general, while
that of heterogenous resource is more relevant to
our task, hence is more informative and evident.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.998907636363636">
Semantic Role Labeling (SRL) was first defined
by Gildea and Jurafsky (2002), who presented a
a system based on statistical classifiers trained on
hand-annotated corpus FrameNet. Sun and Ju-
rafsky (2004) did the preliminary work on Chi-
nese SRL without any large semantically annotat-
ed corpus and produced promising results. Af-
ter CPB (Xue and Palmer, 2003) was built, X-
ue and Palmer (2005) and Xue (2008) produced
more complete and systematic research on Chi-
nese SRL. Ding and Chang (2009) established a
</bodyText>
<footnote confidence="0.965798">
3https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.996258">
1629
</page>
<bodyText confidence="0.99993597826087">
word based Chinese SRL system, which is quite
different from the previous parsing based ones.
Sun et al. (2009) extended the work of Chen et
al. (2006), performed Chinese SRL with shallow
parsing, which took partial parses as inputs. Yang
and Zong (2014) proposed multi-predicate SRL,
which showed improvements both on English and
Chinese Proposition Bank. Different from most
work relying on a large number of handcrafted
features, Collobert and Weston (2008) proposed a
convolutional neural network for SRL. Their ap-
proach achieved competitive performance on En-
glish SRL without requiring task specific feature
engineering. However, by max-pooling operation,
the convolution approach only preserved the most
evident features in a sentence, thus can only weak-
ly model the dependencies. With our bidirectional
LSTM RNN model, this problem can be well alle-
viated.
Our model is based on recurrent neural network
(RNN), which uses iterative function loops to store
contextual information. To remedy the problem
of vanishing and exploding gradients when train-
ing the standard RNN, Hochreiter and Schmidhu-
ber (1997) proposed long-short-term memory (L-
STM), which has been shown capable of storing
and accessing information over very long time s-
pans. Bidirectional RNN (Schuster and Paliwal,
1997) and bidirectional LSTM RNN (Graves et
al., 2005) are the extensions of RNN and LSTM
RNN with the capability of capturing contextual
information from both directions in the sequence.
In recent years, RNN has shown the state-of-the-
art results in many NLP problems such as lan-
guage modeling (Mikolov et al., 2010) and ma-
chine translation (Sutskever et al., 2014; Bah-
danau et al., 2014). Sundermeyer et al. (2014)
also used bidirectional LSTM RNN model to im-
prove strong baselines when modeling translation.
More recently, Zhou and Xu (2015) proposed L-
STM RNN approach for English Semantic Role
Labeling, which shared similar idea with our mod-
el. However, the features used and the network ar-
chitecture were different from ours. Moreover, it is
delightful that our work can achieve a rather good
result with a relatively simpler model architecture.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999951066666667">
In this paper, we formulate Chinese SRL problem
with the framework of bidirectional LSTM RN-
N model. In our approach, the bidirectional and
long-range dependencies in a sentence, which are
important for Chinese SRL, can be well modeled.
And with the framework of deep neural network,
the heavy job of feature engineering is much alle-
viated. Moreover, our model makes the introduc-
tion of heterogenous data, which can alleviate the
problem of scalability with a single annotated cor-
pus, more convenient. Experiments show that our
approach achieves much better results than previ-
ous work, and the introduction of heterogenous re-
source can make further improvement on perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998094" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9969398">
This research is supported by National Key Basic
Research Program of China (No.2014CB340504)
and National Natural Science Foundation of China
(No.61375074,61273318). The contact authors of
this paper are Baobao Chang and Zhifang Sui.
</bodyText>
<sectionHeader confidence="0.998414" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985162677419355">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint arX-
iv:1409.0473.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of chinese chunking. In
Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 97–104. Association for
Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Weiwei Ding and Baobao Chang. 2008. Improving
chinese semantic role classification with hierarchi-
cal feature selection strategy. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 324–333. Association for
Computational Linguistics.
Weiwei Ding and Baobao Chang. 2009. Word based
chinese semantic role labeling with semantic chunk-
ing. International Journal of Computer Processing
Of Languages, 22(02n03):133–154.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245–288.
Alex Graves, Santiago Fern´andez, and J¨urgen Schmid-
huber. 2005. Bidirectional lstm networks for im-
proved phoneme classification and recognition. In
Artificial Neural Networks: Formal Models and
</reference>
<page confidence="0.756115">
1630
</page>
<reference confidence="0.999751016393442">
Their Applications–ICANN 2005, pages 799–804.
Springer.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantic parsing of chinese. In Proceedings of NAA-
CL 2004, pages 249–256.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.
2009. Chinese semantic role labeling with shallow
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3-Volume 3, pages 1475–1483. Asso-
ciation for Computational Linguistics.
Weiwei Sun. 2010. Improving chinese semantic role
labeling with rich syntactic features. In Proceedings
of the ACL 2010 Conference Short Papers, pages
168–172. Association for Computational Linguistic-
s.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling
with bidirectional recurrent neural networks. In Pro-
ceedings of the Conference on Empirical Methods
on Natural Language Processing, October.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural network-
s. In Advances in neural information processing sys-
tems, pages 3104–3112.
Nianwen Xue and Martha Palmer. 2003. Annotat-
ing the propositions in the penn chinese treebank.
In Proceedings of the second SIGHAN workshop on
Chinese language processing-Volume 17, pages 47–
54. Association for Computational Linguistics.
Nianwen Xue and Martha Palmer. 2005. Automatic
semantic role labeling for chinese verbs. In IJCAI,
volume 5, pages 1160–1165. Citeseer.
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational linguistics,
34(2):225–255.
Haitong Yang and Chengqing Zong. 2014. Multi-
predicate semantic role labeling. In Proceedings of
the 2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 363–373.
Jie Zhou and Wei Xu. 2015. End-to-end learning of
semantic role labeling using recurrent neural net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1127–1137, Beijing, China, July. Association
for Computational Linguistics.
</reference>
<page confidence="0.992655">
1631
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931681">
<title confidence="0.999669">Chinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks</title>
<author confidence="0.999501">Zhen Wang</author>
<author confidence="0.999501">Tingsong Jiang</author>
<author confidence="0.999501">Baobao Chang</author>
<author confidence="0.999501">Zhifang Sui</author>
<affiliation confidence="0.998869">Key Laboratory of Computational Linguistics, Ministry of Education School of Electronics Engineering and Computer Science, Peking University</affiliation>
<address confidence="0.991895">Collaborative Innovation Center for Language Ability, Xuzhou 221009 China</address>
<email confidence="0.992483">chbb,</email>
<abstract confidence="0.997310210526316">Traditional approaches to Chinese Semantic Role Labeling (SRL) almost heavily rely on feature engineering. Even worse, the long-range dependencies in a sentence can hardly be modeled by these methods. In this paper, we introduce bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to capture bidirectional and long-range dependencies in a sentence with minimal feature engineering. Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</title>
<date>2014</date>
<contexts>
<context position="16004" citStr="Bahdanau et al., 2014" startWordPosition="2580" endWordPosition="2584">raining the standard RNN, Hochreiter and Schmidhuber (1997) proposed long-short-term memory (LSTM), which has been shown capable of storing and accessing information over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling, which shared similar idea with our model. However, the features used and the network architecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. 5 Conclusion In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RNN model. In our approach, the bid</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An empirical study of chinese chunking.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>97--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14482" citStr="Chen et al. (2006)" startWordPosition="2346" endWordPosition="2349">who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can </context>
</contexts>
<marker>Chen, Zhang, Isahara, 2006</marker>
<rawString>Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006. An empirical study of chinese chunking. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 97–104. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10646" citStr="Collobert and Weston (2008)" startWordPosition="1740" endWordPosition="1743">schemas. The introduction of heterogenous resource in this way is efficient and can lead to performance improvement on our experiment. 3 Experiments We conduct experiments to compare our model with previous landmark methods on the benchmark dataset CPB for Chinese SRL. The result Ni L t=1 oty(i) t L = s(x(i), y(i), θ) − log 1628 Remark Choice Word embedding dimension nword = 50 POS tag dimension npos = 20 Distance dimension ndis = 20 Nonlinear layer n1 = 200 RNN layer nh = 100 Nonlinear layer n3 = 100 Learning rate α = 10−3 Table 1: Hyper parameters of our model. Method F1(%) Xue (2008) 71.90 Collobert and Weston (2008) 74.05 Sun et al. (2009) 74.12 Yang and Zong (2014) 75.31 Ours (Random Initialization) 77.09 + Standard Pre-training 77.21 + Heterogenous Resource 77.59 Table 2: Results comparison on CPB dataset. reveals that even with our basic model, which does not resort to other resources, our approach can significantly outperform all of the competitors. Moreover, we enrich our work with introducing heterogenous resource to make a further improvement on performance. And the result also shows the influence of heterogeneous resource is more evident than the standard method of pre-training for word embedding</context>
<context position="12417" citStr="Collobert and Weston (2008)" startWordPosition="2026" endWordPosition="2029"> corpus2 with distinct semantic role labels and annotation schema, which is designed by ourselves for other projects, as heterogeneous resource. This labeled dataset has 17,308 annotated sentences, and the semantic roles concerned are like “agent” and “patient”, resulting in 21 kinds of types, which are all distinct from the semantic roles defined in CPB. We use the development set of CPB for model selection, and the hyper parameter setting of our model is reported in Table 1. 3.2 Chinese SRL Performance Table 2 summarizes our SRL performance compared to previous landmark results. The work of Collobert and Weston (2008) was conducted on English SRL, we implement their approach on CP1https://catalog.ldc.upenn.edu/LDC2005T23 2This Chinese dataset is available on request. B for comparison. As indicated by this table, our approach significantly outperforms previous stateof-the-art methods even with all parameters randomly initialized, that is without introducing other resources. This result can prove the ability of our model to capture useful dependencies for Chinese SRL with minimal feature engineering. Further, we conduct experiments with the introduction of heterogenous resource. Previous work found that the </context>
<context position="14791" citStr="Collobert and Weston (2008)" startWordPosition="2391" endWordPosition="2394">mer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can only weakly model the dependencies. With our bidirectional LSTM RNN model, this problem can be well alleviated. Our model is based on recurrent neural network (RNN), which uses iterative function loops to store contextual information. To remedy the problem of vanishing and exploding gradients when training t</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Ding</author>
<author>Baobao Chang</author>
</authors>
<title>Improving chinese semantic role classification with hierarchical feature selection strategy.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>324--333</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1816" citStr="Ding and Chang, 2008" startWordPosition="265" endWordPosition="268">ssign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engiFigure 1: A sentence with semantic roles labeled from CPB. neering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and</context>
</contexts>
<marker>Ding, Chang, 2008</marker>
<rawString>Weiwei Ding and Baobao Chang. 2008. Improving chinese semantic role classification with hierarchical feature selection strategy. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 324–333. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Ding</author>
<author>Baobao Chang</author>
</authors>
<title>Word based chinese semantic role labeling with semantic chunking.</title>
<date>2009</date>
<journal>International Journal of Computer Processing Of Languages,</journal>
<pages>22--02</pages>
<contexts>
<context position="1838" citStr="Ding and Chang, 2009" startWordPosition="269" endWordPosition="272">bels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engiFigure 1: A sentence with semantic roles labeled from CPB. neering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but</context>
<context position="14274" citStr="Ding and Chang (2009)" startWordPosition="2315" endWordPosition="2318">ore general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 4 Related Work Semantic Role Labeling (SRL) was first defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competiti</context>
</contexts>
<marker>Ding, Chang, 2009</marker>
<rawString>Weiwei Ding and Baobao Chang. 2009. Word based chinese semantic role labeling with semantic chunking. International Journal of Computer Processing Of Languages, 22(02n03):133–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational linguistics,</journal>
<pages>28--3</pages>
<contexts>
<context position="1376" citStr="Gildea and Jurafsky, 2002" startWordPosition="192" endWordPosition="195">ge dependencies in a sentence with minimal feature engineering. Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engiFi</context>
<context position="13862" citStr="Gildea and Jurafsky (2002)" startWordPosition="2246" endWordPosition="2249">ings have a good effect on our performance (we use word2vec3 on Chinese Gigaword Corpus for word pre-training). However, as shown in Table 2, compared to standard pre-training, the influence of heterogenous data is more evident. We can explain this difference via the distinction between these two kinds of methods for performance improvement. The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 4 Related Work Semantic Role Labeling (SRL) was first defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work o</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Santiago Fern´andez</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Bidirectional lstm networks for improved phoneme classification and recognition.</title>
<date>2005</date>
<booktitle>In Artificial Neural Networks: Formal Models and</booktitle>
<marker>Graves, Fern´andez, Schmidhuber, 2005</marker>
<rawString>Alex Graves, Santiago Fern´andez, and J¨urgen Schmidhuber. 2005. Bidirectional lstm networks for improved phoneme classification and recognition. In Artificial Neural Networks: Formal Models and</rawString>
</citation>
<citation valid="true">
<title>Their Applications–ICANN</title>
<date>2005</date>
<pages>799--804</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="14174" citStr="(2005)" startWordPosition="2301" endWordPosition="2301">mprovement. The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 4 Related Work Semantic Role Labeling (SRL) was first defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert </context>
</contexts>
<marker>2005</marker>
<rawString>Their Applications–ICANN 2005, pages 799–804. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="15441" citStr="Hochreiter and Schmidhuber (1997)" startWordPosition="2488" endWordPosition="2492">lutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can only weakly model the dependencies. With our bidirectional LSTM RNN model, this problem can be well alleviated. Our model is based on recurrent neural network (RNN), which uses iterative function loops to store contextual information. To remedy the problem of vanishing and exploding gradients when training the standard RNN, Hochreiter and Schmidhuber (1997) proposed long-short-term memory (LSTM), which has been shown capable of storing and accessing information over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Schuster</author>
<author>Kuldip K Paliwal</author>
</authors>
<title>Bidirectional recurrent neural networks.</title>
<date>1997</date>
<journal>Signal Processing, IEEE Transactions on,</journal>
<volume>45</volume>
<issue>11</issue>
<contexts>
<context position="15621" citStr="Schuster and Paliwal, 1997" startWordPosition="2516" endWordPosition="2519">e convolution approach only preserved the most evident features in a sentence, thus can only weakly model the dependencies. With our bidirectional LSTM RNN model, this problem can be well alleviated. Our model is based on recurrent neural network (RNN), which uses iterative function loops to store contextual information. To remedy the problem of vanishing and exploding gradients when training the standard RNN, Hochreiter and Schmidhuber (1997) proposed long-short-term memory (LSTM), which has been shown capable of storing and accessing information over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling</context>
</contexts>
<marker>Schuster, Paliwal, 1997</marker>
<rawString>Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. Signal Processing, IEEE Transactions on, 45(11):2673–2681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglin Sun</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow semantic parsing of chinese.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>249--256</pages>
<contexts>
<context position="1401" citStr="Sun and Jurafsky, 2004" startWordPosition="196" endWordPosition="200">ce with minimal feature engineering. Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engiFigure 1: A sentence with s</context>
<context position="13988" citStr="Sun and Jurafsky (2004)" startWordPosition="2264" endWordPosition="2268"> in Table 2, compared to standard pre-training, the influence of heterogenous data is more evident. We can explain this difference via the distinction between these two kinds of methods for performance improvement. The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 4 Related Work Semantic Role Labeling (SRL) was first defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) pr</context>
</contexts>
<marker>Sun, Jurafsky, 2004</marker>
<rawString>Honglin Sun and Daniel Jurafsky. 2004. Shallow semantic parsing of chinese. In Proceedings of NAACL 2004, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Zhifang Sui</author>
<author>Meng Wang</author>
<author>Xin Wang</author>
</authors>
<title>Chinese semantic role labeling with shallow parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1475--1483</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10670" citStr="Sun et al. (2009)" startWordPosition="1745" endWordPosition="1748">ogenous resource in this way is efficient and can lead to performance improvement on our experiment. 3 Experiments We conduct experiments to compare our model with previous landmark methods on the benchmark dataset CPB for Chinese SRL. The result Ni L t=1 oty(i) t L = s(x(i), y(i), θ) − log 1628 Remark Choice Word embedding dimension nword = 50 POS tag dimension npos = 20 Distance dimension ndis = 20 Nonlinear layer n1 = 200 RNN layer nh = 100 Nonlinear layer n3 = 100 Learning rate α = 10−3 Table 1: Hyper parameters of our model. Method F1(%) Xue (2008) 71.90 Collobert and Weston (2008) 74.05 Sun et al. (2009) 74.12 Yang and Zong (2014) 75.31 Ours (Random Initialization) 77.09 + Standard Pre-training 77.21 + Heterogenous Resource 77.59 Table 2: Results comparison on CPB dataset. reveals that even with our basic model, which does not resort to other resources, our approach can significantly outperform all of the competitors. Moreover, we enrich our work with introducing heterogenous resource to make a further improvement on performance. And the result also shows the influence of heterogeneous resource is more evident than the standard method of pre-training for word embeddings. 3.1 Experimental Sett</context>
<context position="14442" citStr="Sun et al. (2009)" startWordPosition="2338" endWordPosition="2341">defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most e</context>
</contexts>
<marker>Sun, Sui, Wang, Wang, 2009</marker>
<rawString>Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang. 2009. Chinese semantic role labeling with shallow parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1475–1483. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Improving chinese semantic role labeling with rich syntactic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>168--172</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1850" citStr="Sun, 2010" startWordPosition="273" endWordPosition="274">of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engiFigure 1: A sentence with semantic roles labeled from CPB. neering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related lat</context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Improving chinese semantic role labeling with rich syntactic features. In Proceedings of the ACL 2010 Conference Short Papers, pages 168–172. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation modeling with bidirectional recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<contexts>
<context position="16031" citStr="Sundermeyer et al. (2014)" startWordPosition="2585" endWordPosition="2588">, Hochreiter and Schmidhuber (1997) proposed long-short-term memory (LSTM), which has been shown capable of storing and accessing information over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling, which shared similar idea with our model. However, the features used and the network architecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. 5 Conclusion In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RNN model. In our approach, the bidirectional and long-range d</context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation modeling with bidirectional recurrent neural networks. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="15980" citStr="Sutskever et al., 2014" startWordPosition="2576" endWordPosition="2579">ploding gradients when training the standard RNN, Hochreiter and Schmidhuber (1997) proposed long-short-term memory (LSTM), which has been shown capable of storing and accessing information over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling, which shared similar idea with our model. However, the features used and the network architecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. 5 Conclusion In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RNN model. </context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Annotating the propositions in the penn chinese treebank.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1485" citStr="Xue and Palmer, 2003" startWordPosition="212" endWordPosition="215">(CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engiFigure 1: A sentence with semantic roles labeled from CPB. neering, which needs domain knowledge and laborious </context>
<context position="14141" citStr="Xue and Palmer, 2003" startWordPosition="2291" endWordPosition="2294">een these two kinds of methods for performance improvement. The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 4 Related Work Semantic Role Labeling (SRL) was first defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of</context>
</contexts>
<marker>Xue, Palmer, 2003</marker>
<rawString>Nianwen Xue and Martha Palmer. 2003. Annotating the propositions in the penn chinese treebank. In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17, pages 47– 54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Automatic semantic role labeling for chinese verbs.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<volume>5</volume>
<pages>1160--1165</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="14174" citStr="Xue and Palmer (2005)" startWordPosition="2297" endWordPosition="2301">r performance improvement. The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 4 Related Work Semantic Role Labeling (SRL) was first defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert </context>
</contexts>
<marker>Xue, Palmer, 2005</marker>
<rawString>Nianwen Xue and Martha Palmer. 2005. Automatic semantic role labeling for chinese verbs. In IJCAI, volume 5, pages 1160–1165. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Labeling chinese predicates with semantic roles.</title>
<date>2008</date>
<booktitle>Computational linguistics,</booktitle>
<pages>34--2</pages>
<contexts>
<context position="1794" citStr="Xue, 2008" startWordPosition="263" endWordPosition="264">icate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engiFigure 1: A sentence with semantic roles labeled from CPB. neering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different se</context>
<context position="10612" citStr="Xue (2008)" startWordPosition="1737" endWordPosition="1738">tinct annotation schemas. The introduction of heterogenous resource in this way is efficient and can lead to performance improvement on our experiment. 3 Experiments We conduct experiments to compare our model with previous landmark methods on the benchmark dataset CPB for Chinese SRL. The result Ni L t=1 oty(i) t L = s(x(i), y(i), θ) − log 1628 Remark Choice Word embedding dimension nword = 50 POS tag dimension npos = 20 Distance dimension ndis = 20 Nonlinear layer n1 = 200 RNN layer nh = 100 Nonlinear layer n3 = 100 Learning rate α = 10−3 Table 1: Hyper parameters of our model. Method F1(%) Xue (2008) 71.90 Collobert and Weston (2008) 74.05 Sun et al. (2009) 74.12 Yang and Zong (2014) 75.31 Ours (Random Initialization) 77.09 + Standard Pre-training 77.21 + Heterogenous Resource 77.59 Table 2: Results comparison on CPB dataset. reveals that even with our basic model, which does not resort to other resources, our approach can significantly outperform all of the competitors. Moreover, we enrich our work with introducing heterogenous resource to make a further improvement on performance. And the result also shows the influence of heterogeneous resource is more evident than the standard method </context>
<context position="14189" citStr="Xue (2008)" startWordPosition="2303" endWordPosition="2304"> The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 4 Related Work Semantic Role Labeling (SRL) was first defined by Gildea and Jurafsky (2002), who presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (200</context>
</contexts>
<marker>Xue, 2008</marker>
<rawString>Nianwen Xue. 2008. Labeling chinese predicates with semantic roles. Computational linguistics, 34(2):225–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitong Yang</author>
<author>Chengqing Zong</author>
</authors>
<title>Multipredicate semantic role labeling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>363--373</pages>
<contexts>
<context position="10697" citStr="Yang and Zong (2014)" startWordPosition="1750" endWordPosition="1753"> way is efficient and can lead to performance improvement on our experiment. 3 Experiments We conduct experiments to compare our model with previous landmark methods on the benchmark dataset CPB for Chinese SRL. The result Ni L t=1 oty(i) t L = s(x(i), y(i), θ) − log 1628 Remark Choice Word embedding dimension nword = 50 POS tag dimension npos = 20 Distance dimension ndis = 20 Nonlinear layer n1 = 200 RNN layer nh = 100 Nonlinear layer n3 = 100 Learning rate α = 10−3 Table 1: Hyper parameters of our model. Method F1(%) Xue (2008) 71.90 Collobert and Weston (2008) 74.05 Sun et al. (2009) 74.12 Yang and Zong (2014) 75.31 Ours (Random Initialization) 77.09 + Standard Pre-training 77.21 + Heterogenous Resource 77.59 Table 2: Results comparison on CPB dataset. reveals that even with our basic model, which does not resort to other resources, our approach can significantly outperform all of the competitors. Moreover, we enrich our work with introducing heterogenous resource to make a further improvement on performance. And the result also shows the influence of heterogeneous resource is more evident than the standard method of pre-training for word embeddings. 3.1 Experimental Setting To facilitate compariso</context>
<context position="14585" citStr="Yang and Zong (2014)" startWordPosition="2362" endWordPosition="2365">Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3https://code.google.com/p/word2vec/ 1629 word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can only weakly model the dependencies. With our bidirectional LSTM RNN model, this problem can be well all</context>
</contexts>
<marker>Yang, Zong, 2014</marker>
<rawString>Haitong Yang and Chengqing Zong. 2014. Multipredicate semantic role labeling. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 363–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Zhou</author>
<author>Wei Xu</author>
</authors>
<title>End-to-end learning of semantic role labeling using recurrent neural networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),</booktitle>
<pages>1127--1137</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="16159" citStr="Zhou and Xu (2015)" startWordPosition="2605" endWordPosition="2608">mation over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling, which shared similar idea with our model. However, the features used and the network architecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. 5 Conclusion In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RNN model. In our approach, the bidirectional and long-range dependencies in a sentence, which are important for Chinese SRL, can be well modeled. And with the framework of deep neural netwo</context>
</contexts>
<marker>Zhou, Xu, 2015</marker>
<rawString>Jie Zhou and Wei Xu. 2015. End-to-end learning of semantic role labeling using recurrent neural networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1127–1137, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
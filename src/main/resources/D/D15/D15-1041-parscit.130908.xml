<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.9996475">
Improved Transition-Based Parsing
by Modeling Characters instead of Words with LSTMs
</title>
<author confidence="0.996965">
Miguel Ballesteros♦* Chris Dyer4� Noah A. Smith°
</author>
<affiliation confidence="0.9583805">
♦NLP Group, Pompeu Fabra University, Barcelona, Spain
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
4Marianas Labs, Pittsburgh, PA, USA
°Computer Science &amp; Engineering, University of Washington, Seattle, WA, USA
</affiliation>
<email confidence="0.997866">
miguel.ballesteros@upf.edu, chris@marianaslabs.com, nasmith@cs.washington.edu
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984941176471">
We present extensions to a continuous-
state dependency parsing method that
makes it applicable to morphologically
rich languages. Starting with a high-
performance transition-based parser that
uses long short-term memory (LSTM) re-
current neural networks to learn repre-
sentations of the parser state, we replace
lookup-based word representations with
representations constructed from the or-
thographic representations of the words,
also using LSTMs. This allows statistical
sharing across word forms that are simi-
lar on the surface. Experiments for mor-
phologically rich languages show that the
parsing model benefits from incorporating
the character-based encodings of words.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963355932204">
At the heart of natural language parsing is the chal-
lenge of representing the “state” of an algorithm—
what parts of a parse have been built and what
parts of the input string are not yet accounted for—
as it incrementally constructs a parse. Traditional
approaches rely on independence assumptions, de-
composition of scoring functions, and/or greedy
approximations to keep this space manageable.
Continuous-state parsers have been proposed, in
which the state is embedded as a vector (Titov
and Henderson, 2007; Stenetorp, 2013; Chen and
Manning, 2014; Dyer et al., 2015; Zhou et al.,
2015; Weiss et al., 2015). Dyer et al. reported
state-of-the-art performance on English and Chi-
nese benchmarks using a transition-based parser
whose continuous-state embeddings were con-
structed using LSTM recurrent neural networks
(RNNs) whose parameters were estimated to max-
imize the probability of a gold-standard sequence
of parse actions.
The primary contribution made in this work is to
take the idea of continuous-state parsing a step fur-
ther by making the word embeddings that are used
to construct the parse state sensitive to the mor-
phology of the words.1 Since it it is well known
that a word’s form often provides strong evidence
regarding its grammatical role in morphologically
rich languages (Ballesteros, 2013, inter alia), this
has promise to improve accuracy and statistical ef-
ficiency relative to traditional approaches that treat
each word type as opaque and independently mod-
eled. In the traditional parameterization, words
with similar grammatical roles will only be em-
bedded near each other if they are observed in
similar contexts with sufficient frequency. Our
approach reparameterizes word embeddings using
the same RNN machinery used in the parser: a
word’s vector is calculated based on the sequence
of orthographic symbols representing it (§3).
Although our model is provided no supervision
in the form of explicit morphological annotation,
we find that it gives a large performance increase
when parsing morphologically rich languages in
the SPMRL datasets (Seddah et al., 2013; Seddah
and Tsarfaty, 2014), especially in agglutinative
languages and the ones that present extensive case
systems (§4). In languages that show little mor-
phology, performance remains good, showing that
the RNN composition strategy is capable of cap-
turing both morphological regularities and arbi-
trariness in the sense of Saussure (1916). Finally,
a particularly noteworthy result is that we find that
character-based word embeddings in some cases
obviate explicit POS information, which is usually
found to be indispensable for accurate parsing.
A secondary contribution of this work is to
show that the continuous-state parser of Dyer et al.
(2015) can learn to generate nonprojective trees.
We do this by augmenting its transition operations
</bodyText>
<footnote confidence="0.9992405">
1Software for replicating the experiments is available
from https://github.com/clab/lstm-parser.
</footnote>
<page confidence="0.961079">
349
</page>
<note confidence="0.985238">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9991525">
with a SWAP operation (Nivre, 2009) (§2.4), en-
abling the parser to produce nonprojective depen-
dencies which are often found in morphologically
rich languages.
</bodyText>
<sectionHeader confidence="0.989433" genericHeader="method">
2 An LSTM Dependency Parser
</sectionHeader>
<bodyText confidence="0.999937473684211">
We begin by reviewing the parsing approach of
Dyer et al. (2015) on which our work is based.
Like most transition-based parsers, Dyer et al.’s
parser can be understood as the sequential manip-
ulation of three data structures: a buffer B initial-
ized with the sequence of words to be parsed, a
stack 5 containing partially-built parses, and a list
A of actions previously taken by the parser. In
particular, the parser implements the arc-standard
parsing algorithm (Nivre, 2004).
At each time step t, a transition action is ap-
plied that alters these data structures by pushing
or popping words from the stack and the buffer;
the operations are listed in Figure 1.
Along with the discrete transitions above, the
parser calculates a vector representation of the
states of B, 5, and A; at time step t these are de-
noted by bt, st, and at, respectively. The total
parser state at t is given by
</bodyText>
<equation confidence="0.813922">
pt = max {0, W[st; bt; at] + d} (1)
</equation>
<bodyText confidence="0.999840555555556">
where the matrix W and the vector d are learned
parameters. This continuous-state representation
pt is used to decide which operation to apply next,
updating B, 5, and A (Figure 1).
We elaborate on the design of bt, st, and at us-
ing RNNs in §2.1, on the representation of partial
parses in 5 in §2.2, and on the parser’s decision
mechanism in §2.3. We discuss the inclusion of
SWAP in §2.4.
</bodyText>
<subsectionHeader confidence="0.998312">
2.1 Stack LSTMs
</subsectionHeader>
<bodyText confidence="0.99997295">
RNNs are functions that read a sequence of vectors
incrementally; at time step t the vector xt is read in
and the hidden state ht computed using xt and the
previous hidden state ht−1. In principle, this al-
lows retaining information from time steps in the
distant past, but the nonlinear “squashing” func-
tions applied in the calcluation of each ht result
in a decay of the error signal used in training with
backpropagation. LSTMs are a variant of RNNs
designed to cope with this “vanishing gradient”
problem using an extra memory “cell” (Hochreiter
and Schmidhuber, 1997; Graves, 2013).
Past work explains the computation within an
LSTM through the metaphors of deciding how
much of the current input to pass into memory
(it) or forget (ft). We refer interested readers to
the original papers and present only the recursive
equations updating the memory cell ct and hidden
state ht given xt, the previous hidden state ht−1,
and the memory cell ct−1:
</bodyText>
<equation confidence="0.999922333333333">
it = Q(WZxxt + WZhht−1 + WZcct−1 + bZ)
ft = 1 − it
ct = ft O ct−1+
it O tanh(Wcxxt + Wchht−1 + bc)
ot = Q(Woxxt + Wohht−1 + Wocct + bo)
ht = ot O tanh(ct),
</equation>
<bodyText confidence="0.999962576923077">
where Q is the component-wise logistic sig-
moid function and O is the component-wise
(Hadamard) product. Parameters are all repre-
sented using W and b. This formulation differs
slightly from the classic LSTM formulation in that
it makes use of “peephole connections” (Gers et
al., 2002) and defines the forget gate so that it sums
with the input gate to 1(Greff et al., 2015). To im-
prove the representational capacity of LSTMs (and
RNNs generally), they can be stacked in “layers.”
In these architectures, the input LSTM at higher
layers at time t is the value of ht computed by the
lower layer (and xt is the input at the lowest layer).
The stack LSTM augments the left-to-right se-
quential model of the conventional LSTM with a
stack pointer. As in the LSTM, new inputs are
added in the right-most position, but the stack
pointer indicates which LSTM cell provides ct−1
and ht−1 for the computation of the next iterate.
Further, the stack LSTM provides a pop opera-
tion that moves the stack pointer to the previous
element. Hence each of the parser data structures
(B, 5, and A) is implemented with its own stack
LSTM, each with its own parameters. The values
of bt, st, and at are the ht vectors from their re-
spective stack LSTMs.
</bodyText>
<subsectionHeader confidence="0.99948">
2.2 Composition Functions
</subsectionHeader>
<bodyText confidence="0.998980428571429">
Whenever a REDUCE operation is selected, two
tree fragments are popped off of 5 and combined
to form a new tree fragment, which is then popped
back onto 5 (see Figure 1). This tree must be em-
bedded as an input vector xt.
To do this, Dyer et al. (2015) use a recursive
neural network gr (for relation r) that composes
</bodyText>
<page confidence="0.994839">
350
</page>
<table confidence="0.970920428571429">
Stackt Buffert Action Stackt+1 Buffert+1 Dependency
(u, u), (v, v), S B REDUCE-RIGHT(r) (gr(u, v), u), S B r
(u, u), (v, v), S B REDUCE-LEFT(r) (gr(v, u), v), S B u → v
S (u, u), B SHIFT (u, u), S B r
u ← v
—
(u, u), (v, v), S B SWAP (u, u), S (v, v), B —
</table>
<figureCaption confidence="0.94550325">
Figure 1: Parser transitions indicating the action applied to the stack and buffer and the resulting stack and
buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate
the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their
continuous-state parser; we add SWAP.
</figureCaption>
<bodyText confidence="0.9975106">
the representations of the two subtrees popped
from S (we denote these by u and v), resulting in
a new vector gr(u, v) or gr(v, u), depending on
the direction of attachment. The resulting vector
embeds the tree fragment in the same space as the
words and other tree fragments. This kind of com-
position was thoroughly explored in prior work
(Socher et al., 2011; Socher et al., 2013b; Her-
mann and Blunsom, 2013; Socher et al., 2013a);
for details, see Dyer et al. (2015).
</bodyText>
<subsectionHeader confidence="0.999731">
2.3 Predicting Parser Decisions
</subsectionHeader>
<bodyText confidence="0.997992142857143">
The parser uses a probabilistic model of parser de-
cisions at each time step t. Letting A(S, B) de-
note the set of allowed transitions given the stack
S and buffer S (i.e., those where preconditions
are met; see Figure 1), the probability of action
z E A(S, B) defined using a log-linear distribu-
tion:
</bodyText>
<equation confidence="0.9943875">
p(z  |pt) = exp (gz pt + Q (2)
Ez&apos;EA(S,B) exp (gz pt + qz&apos;)
</equation>
<bodyText confidence="0.999900333333333">
(where gz and qz are parameters associated with
each action type z).
Parsing proceeds by always choosing the most
probable action from A(S, B). The probabilistic
definition allows parameter estimation for all of
the parameters (W*, b* in all three stack LSTMs,
as well as W, d, g*, and q*) by maximizing the
conditional likelihood of each correct parser deci-
sions given the state.
</bodyText>
<subsectionHeader confidence="0.999263">
2.4 Adding the SWAP Operation
</subsectionHeader>
<bodyText confidence="0.99997">
Dyer et al. (2015)’s parser implemented the most
basic version of the arc-standard algorithm, which
is capable of producing only projective parse trees.
In order to deal with nonprojective trees, we also
add the SWAP operation which allows nonprojec-
tive trees to be produced.
The SWAP operation, first introduced by Nivre
(2009), allows a transition-based parser to produce
nonprojective trees. Here, the inclusion of the
SWAP operation requires breaking the linearity of
the stack by removing tokens that are not at the top
of the stack. This is easily handled with the stack
LSTM. Figure 1 shows how the parser is capable
of moving words from the stack (S) to the buffer
(B), breaking the linear order of words. Since a
node that is swapped may have already been as-
signed as the head of a dependent, the buffer (B)
can now also contain tree fragments.
</bodyText>
<sectionHeader confidence="0.975892" genericHeader="method">
3 Word Representations
</sectionHeader>
<bodyText confidence="0.999972666666667">
The main contribution of this paper is to change
the word representations. In this section, we
present the standard word embeddings as in Dyer
et al. (2015), and the improvements we made gen-
erating word embeddings designed to capture mor-
phology based on orthographic strings.
</bodyText>
<subsectionHeader confidence="0.999306">
3.1 Baseline: Standard Word Embeddings
</subsectionHeader>
<bodyText confidence="0.998215">
Dyer et al.’s parser generates a word representation
for each input token by concatenating two vectors:
a vector representation for each word type (w)
and a representation (t) of the POS tag of the to-
ken (if it is used), provided as auxiliary input to the
parser.2 A linear map (V) is applied to the result-
ing vector and passed through a component-wise
ReLU:
</bodyText>
<equation confidence="0.514028">
x = max {0, V[w; t] + b}
</equation>
<bodyText confidence="0.999607">
For out-of-vocabulary words, the parser uses an
“UNK” token that is handled as a separate word
during parsing time. This mapping can be shown
schematically as in Figure 2.
</bodyText>
<footnote confidence="0.7531505">
2Dyer et al. (2015), included a third input representation
learned from a neural language model ( ˜wLM). We do not in-
clude these pretrained representations in our experiments, fo-
cusing instead on character-based representations.
</footnote>
<page confidence="0.996162">
351
</page>
<figureCaption confidence="0.8414675">
Figure 2: Baseline model word embeddings for an
in-vocabulary word that is tagged with POS tag
NN (right) and an out-of-vocabulary word with
POS tag JJ (left).
</figureCaption>
<subsectionHeader confidence="0.999715">
3.2 Character-Based Embeddings of Words
</subsectionHeader>
<bodyText confidence="0.9932433">
Following Ling et al. (2015), we compute
character-based continuous-space vector embed-
dings of words using bidirectional LSTMs (Graves
and Schmidhuber, 2005). When the parser initi-
ates the learning process and populates the buffer
with all the words from the sentence, it reads the
words character by character from left to right and
computes a continuous-space vector embedding
the character sequence, which is the h vector of
→
the LSTM; we denote it by w. The same process
is also applied in reverse (albeit with different pa-
rameters), computing a similar continuous-space
vector embedding starting from the last character
and finishing at the first ( ← w); again each character
is represented with an LSTM cell. After that, we
concatenate these vectors and a (learned) represen-
tation of their tag to produce the representation w.
As in §3.1, a linear map (V) is applied and passed
through a component-wise ReLU.
</bodyText>
<equation confidence="0.9397875">
x = max �
0, V [→w; w; t] + b
</equation>
<bodyText confidence="0.998399833333333">
This process is shown schematically in Figure 3.
Note that under this representation, out-of-
vocabulary words are treated as bidirectional
LSTM encodings and thus they will be “close” to
other words that the parser has seen during train-
ing, ideally close to their more frequent, syntacti-
cally similar morphological relatives. We conjec-
ture that this will give a clear advantage over a sin-
gle “UNK” token for all the words that the parser
does not see during training, as done by Dyer et
al. (2015) and other parsers without additional re-
sources. In §4 we confirm this hypothesis.
</bodyText>
<sectionHeader confidence="0.999661" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999778">
We applied our parsing model and several varia-
tions of it to several parsing tasks and report re-
</bodyText>
<figureCaption confidence="0.990293">
Figure 3: Character-based word embedding of the
word party. This representation is used for both
in-vocabulary and out-of-vocabulary words.
</figureCaption>
<bodyText confidence="0.761288">
sults below.
</bodyText>
<subsectionHeader confidence="0.97599">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99996276">
In order to find out whether the character-based
representations are capable of learning the mor-
phology of words, we applied the parser to mor-
phologically rich languages specifically the tree-
banks of the SPMRL shared task (Seddah et
al., 2013; Seddah and Tsarfaty, 2014): Arabic
(Maamouri et al., 2004), Basque (Aduriz et al.,
2003), French (Abeill´e et al., 2003), German
(Seeker and Kuhn, 2012), Hebrew (Sima’an et al.,
2001), Hungarian (Vincze et al., 2010), Korean
(Choi, 2013), Polish (´Swidzi´nski and Woli´nski,
2010) and Swedish (Nivre et al., 2006b). For all
the corpora of the SPMRL Shared Task we used
predicted POS tags as provided by the shared task
organizers.3 For these datasets, evaluation is cal-
culated using eval07.pl, which includes punc-
tuation.
We also experimented with the Turkish de-
pendency treebank4 (Oflazer et al., 2003) of the
CoNLL-X Shared Task (Buchholz and Marsi,
2006). We used gold POS tags, as is common with
the CoNLL-X data sets.
To put our results in context with the most re-
cent neural network transition-based parsers, we
run the parser in the same Chinese and English
</bodyText>
<listItem confidence="0.745310833333333">
3The POS tags were calculated with the MarMot tag-
ger (M¨uller et al., 2013) by the best performing system of
the SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic:
97.38. Basque: 97.02. French: 97.61. German: 98.10. He-
brew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish:
98.12. Swedish: 97.27.
</listItem>
<footnote confidence="0.925237666666667">
4Since the Turkish dependency treebank does not have a
development set, we extracted the last 150 sentences from the
4996 sentences of the training set as a development set.
</footnote>
<figure confidence="0.995363153846154">
UNK
w
JJ
t
party
w
NN
t
w
w
t
&lt;w&gt;
P
a
r
t
Y
&lt;/w&gt;
&lt;/w&gt;
&lt;w&gt;
a
r
t
Y
P
NN
</figure>
<page confidence="0.988511">
352
</page>
<bodyText confidence="0.9992289">
setups as Chen and Manning (2014) and Dyer et
al. (2015). For Chinese, we use the Penn Chi-
nese Treebank 5.1 (CTB5) following Zhang and
Clark (2008b),5 with gold POS tags. For En-
glish, we used the Stanford Dependency (SD) rep-
resentation of the Penn Treebank6 (Marcus et al.,
1993; Marneffe et al., 2006).7. Results for Turk-
ish, Chinese, and English are calculated using the
CoNLL-X eval.pl script, which ignores punc-
tuation symbols.
</bodyText>
<subsectionHeader confidence="0.914896">
4.2 Experimental Configurations
</subsectionHeader>
<bodyText confidence="0.9960105">
In order to isolate the improvements provided by
the LSTM encodings of characters, we run the
stack LSTM parser in the following configura-
tions:
</bodyText>
<listItem confidence="0.993265222222222">
• Words: words only, as in §3.1 (but without
POS tags)
• Chars: character-based representations of
words with bidirectional LSTMs, as in §3.2
(but without POS tags)
• Words + POS: words and POS tags (§3.1)
• Chars + POS: character-based representa-
tions of words with bidirectional LSTMs plus
POS tags (§3.2)
</listItem>
<bodyText confidence="0.9997884">
None of the experimental configurations in-
clude pretrained word-embeddings or any addi-
tional data resources. All experiments include the
SWAP transition, meaning that nonprojective trees
can be produced in any language.
Dimensionality. The full version of our parsing
model sets dimensionalities as follows. LSTM
hidden states are of size 100, and we use two
layers of LSTMs for each stack. Embeddings of
the parser actions used in the composition func-
tions have 20 dimensions, and the output embed-
ding size is 20 dimensions. The learned word
representations embeddings have 32 dimensions
when used, while the character-based representa-
tions have 100 dimensions, when used. Part of
speech embeddings have 12 dimensions. These di-
mensionalities were chosen after running several
tests with different values, but a more careful se-
lection of these values would probably further im-
prove results.
</bodyText>
<footnote confidence="0.7149838">
5Training: 001–815, 1001–1136. Development: 886–
931, 1148–1151. Test: 816–885, 1137–1147.
6Training: 02–21. Development: 22. Test: 23.
7The POS tags are predicted by using the Stanford Tagger
(Toutanova et al., 2003) with an accuracy of 97.3%.
</footnote>
<subsectionHeader confidence="0.988407">
4.3 Training Procedure
</subsectionHeader>
<bodyText confidence="0.9996754">
Parameters are initialized randomly—refer to
Dyer et al. (2015) for specifics—and optimized
using stochastic gradient descent (without mini-
batches) using derivatives of the negative log like-
lihood of the sequence of parsing actions com-
puted using backpropagation. Training is stopped
when the learned model’s UAS stops improving
on the development set, and this model is used to
parse the test set. No pretraining of any parameters
is done.
</bodyText>
<subsectionHeader confidence="0.849478">
4.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.960054621621621">
Tables 1 and 2 show the results of the parsers for
the development sets and the final test sets, respec-
tively. Most notable are improvements for agglu-
tinative languages—Basque, Hungarian, Korean,
and Turkish—both when POS tags are included
and when they are not. Consistently, across all
languages, Chars outperforms Words, suggest-
ing that the character-level LSTMs are learning
representations that capture similar information to
parts of speech. On average, Chars is on par with
Words + POS, and the best average of labeled at-
tachment scores is achieved with Chars + POS.
It is common practice to encode morphological
information in treebank POS tags; for instance, the
Penn Treebank includes English number and tense
(e.g., NNS is plural noun and VBD is verb in past
tense). Even if our character-based representations
are capable of encoding the same kind of informa-
tion, existing POS tags suffice for high accuracy.
However, the POS tags in treebanks for morpho-
logically rich languages do not seem to be enough.
Swedish, English, and French use suffixes for
the verb tenses and number,8 while Hebrew uses
prepositional particles rather than grammatical
case. Tsarfaty (2006) and Cohen and Smith (2007)
argued that, for Hebrew, determining the correct
morphological segmentation is dependent on syn-
tactic context. Our approach sidesteps this step,
capturing the same kind of information in the vec-
tors, and learning it from syntactic context. Even
for Chinese, which is not morphologically rich,
Chars shows a benefit over Words, perhaps by
capturing regularities in syllable structure within
words.
8Tense and number features provide little improvement in
a transition-based parser, compared with other features such
as case, when the POS tags are included (Ballesteros, 2013).
</bodyText>
<page confidence="0.993373">
353
</page>
<table confidence="0.99748390625">
UAS
Language Words Chars Words Chars
+ POS + POS
Arabic 86.14 87.20 87.44 87.07
Basque 78.42 84.97 83.49 85.58
French 84.84 86.21 87.00 86.33
German 88.14 90.94 91.16 91.23
Hebrew 79.73 79.92 81.99 80.76
Hungarian 72.38 80.16 78.47 80.85
Korean 78.98 88.98 87.36 89.14
Polish 73.29 85.69 89.32 88.54
Swedish 73.44 75.03 80.02 78.85
Turkish 71.10 74.91 77.13 77.96
Chinese 79.43 80.36 85.98 85.81
English 91.64 91.98 92.94 92.49
Average 79.79 83.86 85.19 85.38
LAS
Language Words Chars Words Chars
+ POS + POS
Arabic 82.73 84.34 84.81 84.36
Basque 67.08 78.22 74.31 79.52
French 80.32 81.70 82.71 81.51
German 85.36 88.68 89.04 88.83
Hebrew 69.42 70.58 74.11 72.18
Hungarian 62.14 75.61 69.50 76.16
Korean 67.48 86.80 83.80 86.88
Polish 65.13 78.23 81.84 80.97
Swedish 64.77 66.74 72.09 69.88
Turkish 53.98 62.91 62.30 62.87
Chinese 75.64 77.06 84.36 84.10
English 88.60 89.58 90.63 90.08
Average 71.89 78.37 79.13 79.78
</table>
<tableCaption confidence="0.7093508">
Table 1: Unlabeled attachment scores (left) and labeled attachment scores (right) on the development
sets (not a standard development set for Turkish). In each table, the first two columns show the results of
the parser with word lookup (Words) vs. character-based (Chars) representations. The last two columns
add POS tags. Boldface shows the better result comparing Words vs. Chars and comparing Words +
POS vs. Chars + POS.
</tableCaption>
<table confidence="0.99896096875">
UAS
Language Words Chars Words Chars
+ POS + POS
Arabic 85.21 86.08 86.05 86.07
Basque 77.06 85.19 82.92 85.22
French 83.74 85.34 86.15 85.78
German 82.75 86.80 87.33 87.26
Hebrew 77.62 79.93 80.68 80.17
Hungarian 72.78 80.35 78.64 80.92
Korean 78.70 88.39 86.85 88.30
Polish 72.01 83.44 87.06 85.97
Swedish 76.39 79.18 83.43 83.24
Turkish 71.70 76.32 75.32 76.34
Chinese 79.01 79.94 85.96 85.30
English 91.16 91.47 92.57 91.63
Average 79.01 85.36 84.41 84.68
LAS
Language Words Chars Words Chars
+ POS + POS
Arabic 82.05 83.41 83.46 83.40
Basque 66.61 79.09 73.56 78.61
French 79.22 80.92 82.03 81.08
German 79.15 84.04 84.62 84.49
Hebrew 68.71 71.26 72.70 72.26
Hungarian 61.93 75.19 69.31 76.34
Korean 67.50 86.27 83.37 86.21
Polish 63.96 76.84 79.83 78.24
Swedish 67.69 71.19 76.40 74.47
Turkish 54.55 64.34 61.22 62.28
Chinese 74.79 76.29 84.40 83.72
English 88.42 88.94 90.31 89.44
Average 71.22 78.15 78.43 79.21
</table>
<tableCaption confidence="0.995715">
Table 2: Unlabeled attachment scores (left) and labeled attachment scores (right) on the test sets. In
</tableCaption>
<bodyText confidence="0.920664333333333">
each table, the first two columns show the results of the parser with word lookup (Words) vs. character-
based (Chars) representations. The last two columns add POS tags. Boldface shows the better result
comparing Words vs. Chars and comparing Words + POS vs. Chars + POS.
</bodyText>
<subsectionHeader confidence="0.82794">
4.4.1 Learned Word Representations
</subsectionHeader>
<bodyText confidence="0.999767666666667">
Figure 4 visualizes a sample of the character-
based bidirectional LSTMs’s learned representa-
tions (Chars). Clear clusters of past tense verbs,
gerunds, and other syntactic classes are visible.
The colors in the figure represent the most com-
mon POS tag for each word.
</bodyText>
<sectionHeader confidence="0.5264" genericHeader="method">
4.4.2 Out-of-Vocabulary Words
</sectionHeader>
<bodyText confidence="0.999986411764706">
The character-based representation for words is
notably beneficial for out-of-vocabulary (OOV)
words. We tested this specifically by comparing
Chars to a model in which all OOVs are replaced
by the string “UNK” during parsing. This always
has a negative effect on LAS (average −4.5 points,
−2.8 UAS). Figure 5 shows how this drop varies
with the development OOV rate across treebanks;
most extreme is Korean, which drops 15.5 LAS. A
similar, but less pronounced pattern, was observed
for models that include POS.
Interestingly, this artificially impoverished
model is still consistently better than Words for
all languages (e.g., for Korean, by 4 LAS). This
implies that not all of the improvement is due to
OOV words; statistical sharing across orthograph-
ically close words is beneficial, as well.
</bodyText>
<subsectionHeader confidence="0.697396">
4.4.3 Computational Requirements
</subsectionHeader>
<bodyText confidence="0.996054">
The character-based representations make the
parser slower, since they require composing the
character-based bidirectional LSTMs for each
</bodyText>
<page confidence="0.995288">
354
</page>
<figure confidence="0.996543777777778">
LAS difference
−15 −10 −5 0
eu hu pl ko tr
enar
fr zh he
de
sv
0.05 0.10 0.15 0.20 0.25 0.30
OOV rate
</figure>
<figureCaption confidence="0.860853666666667">
Figure 5: On the x-axis is the OOV rate in development data, by treebank; on the y-axis is the difference
in development-set LAS between Chars model as described in §3.2 and one in which all OOV words are
given a single representation.
</figureCaption>
<figure confidence="0.761364">
answer
</figure>
<figureCaption confidence="0.992101">
Figure 4: Character-based word representations
</figureCaption>
<bodyText confidence="0.919835615384615">
of 30 random words from the English develop-
ment set (Chars). Dots in red represent past tense
verbs; dots in orange represent gerund verbs; dots
in black represent present tense verbs; dots in blue
represent adjectives; dots in green represent ad-
verbs; dots in yellow represent singular nouns;
dots in brown represent plural nouns. The visu-
alization was produced using t-SNE; see http:
//lvdmaaten.github.io/tsne/.
word of the input sentence; however, at test time
these results could be cached. On average, Words
parses a sentence in 44 ms, whileChars needs 130
ms.9 Training time is affected by the same cons-
</bodyText>
<footnote confidence="0.7944625">
9We are using a machine with 32 Intel Xeon CPU E5-
2650 at 2.00GHz; the parser runs on a single core.
</footnote>
<bodyText confidence="0.99956">
tant, needing some hours to have a competitive
model. In terms of memory, Words requires on
average 300 MB of main memory for both train-
ing and parsing, while Chars requires 450 MB.
</bodyText>
<subsectionHeader confidence="0.909876">
4.4.4 Comparison with State-of-the-Art
</subsectionHeader>
<bodyText confidence="0.99996032">
Table 3 shows a comparison with state-of-the-
art parsers. We include greedy transition-based
parsers that, like ours, do not apply a beam
search (Zhang and Clark, 2008b) or a dynamic
oracle (Goldberg and Nivre, 2013). For all the
SPMRL languages we show the results of Balles-
teros (2013), who reported results after carrying
out a careful automatic morphological feature se-
lection experiment. For Turkish, we show the re-
sults of Nivre et al. (2006a) which also carried
out a careful manual morphological feature se-
lection. Our parser outperforms these in most
cases. Since those systems rely on morphological
features, we believe that this comparison shows
even more that the character-based representations
are capturing morphological information, though
without explicit morphological features. For En-
glish and Chinese, we report (Dyer et al., 2015)
which is Words + POS but with pretrained word
embeddings.
We also show the best reported results on
these datasets. For the SPMRL data sets, the
best performing system of the shared task is ei-
ther Bj¨orkelund et al. (2013) or Bj¨orkelund et al.
(2014), which are consistently better than our sys-
</bodyText>
<figure confidence="0.998578964285714">
days
cranes
gasoline
event
steady grandiose
profession
consumption
consultant
overtly
possibly
pitcher relief
drop
constructive
perfect
meet
computer-driven
median
plummeting
approving
restating
suspended
washing
leveraging
retiring
declared
outnumbered
achieved
advanced
</figure>
<page confidence="0.994074">
355
</page>
<table confidence="0.999266428571429">
This Work Best Greedy Result Best Published Result
Language UAS LAS System UAS LAS System UAS LAS System
Arabic 86.08 83.41 Chars 84.57 81.90 B’13 88.32 86.21 B+’13
Basque 85.22 78.61 Chars + POS 84.33 78.58 B’13 89.96 85.70 B+’14
French 86.15 82.03 Words + POS 83.35 77.98 B’13 89.02 85.66 B+’14
German 87.33 84.62 Words + POS 85.38 82.75 B’13 91.64 89.65 B+’13
Hebrew 80.68 72.70 Words + POS 79.89 73.01 B’13 87.41 81.65 B+’14
Hungarian 80.92 76.34 Chars + POS 83.71 79.63 B’13 89.81 86.13 B+’13
Korean 88.39 86.27 Chars 85.72 82.06 B’13 89.10 87.27 B+’14
Polish 87.06 79.83 Words + POS 85.80 79.89 B’13 91.75 87.07 B+’13
Swedish 83.43 76.40 Words + POS 83.20 75.82 B’13 88.48 82.75 B+’14
Turkish 76.32 64.34 Chars 75.82 65.68 N+’06a 77.55 n/a K+’10
Chinese 85.96 84.40 Words + POS 87.20 85.70 D+’15 87.20 85.70 D+’15
English 92.57 90.31 Words + POS 93.10 90.90 D+’15 94.08 92.19 W+’15
</table>
<tableCaption confidence="0.999676">
Table 3: Test-set performance of our best results (according to UAS or LAS, whichever has the larger
</tableCaption>
<bodyText confidence="0.942466266666667">
difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best
results reported (“Best Published Result”). All of the systems we compare against use explicit mor-
phological features and/or one of the following: pretrained word embeddings, unlabeled data and a
combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a);
D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10
is Koo et al. (2010); W+’15 is Weiss et al. (2015).
tem for all languages. Note that the comparison
is harsh to our system, which does not use unla-
beled data or explicit morphological features nor
any combination of different parsers. For Turkish,
we report the results of Koo et al. (2010), which
only reported unlabeled attachment scores. For
English, we report (Weiss et al., 2015) and for Chi-
nese, we report (Dyer et al., 2015) which is Words
+ POS but with pretrained word embeddings.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999648756097561">
Character-based representations have been ex-
plored in other NLP tasks; for instance, dos San-
tos and Zadrozny (2014) and dos Santos and
Guimar˜aes (2015) learned character-level neural
representations for POS tagging and named entity
recognition, getting a large error reduction in both
tasks. Our approach is similar to theirs. Others
have used character-based models as features to
improve existing models. For instance, Chrupała
(2014) used character-based recurrent neural net-
works to normalize tweets.
Botha and Blunsom (2014) show that stems,
prefixes and suffixes can be used to learn useful
word representations but relying on an external
morphological analyzer. That is, they learn the
morpheme-meaning relationship with an additive
model, whereas we do not need a morphological
analyzer. Similarly, Chen et al. (2015) proposed
joint learning of character and word embeddings
for Chinese, claiming that characters contain rich
information.
Methods for joint morphological disambigua-
tion and parsing have been widely explored Tsar-
faty (2006; Cohen and Smith (2007; Goldberg
and Tsarfaty (2008; Goldberg and Elhadad (2011).
More recently, Bohnet et al. (2013) presented an
arc-standard transition-based parser that performs
competitively for joint morphological tagging and
dependency parsing for richly inflected languages,
such as Czech, Finnish, German, Hungarian, and
Russian. Our model seeks to achieve a simi-
lar benefit to parsing without explicitly reasoning
about the internal structure of words.
Zhang et al. (2013) presented efforts on Chinese
parsing with characters showing that Chinese can
be parsed at the character level, and that Chinese
word segmentation is useful for predicting the cor-
rect POS tags (Zhang and Clark, 2008a).
To the best of our knowledge, previous work has
not used character-based embeddings to improve
dependency parsers, as done in this paper.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999233">
We have presented several interesting findings.
First, we add new evidence that character-based
representations are useful for NLP tasks. In this
paper, we demonstrate that they are useful for
transition-based dependency parsing, since they
are capable of capturing morphological informa-
tion crucial for analyzing syntax.
The improvements provided by the character-
based representations using bidirectional LSTMs
are strong for agglutinative languages, such as
</bodyText>
<page confidence="0.996455">
356
</page>
<bodyText confidence="0.999722176470588">
Basque, Hungarian, Korean, and Turkish, compar-
ing favorably to POS tags as encoded in those lan-
guages’ currently available treebanks. This out-
come is important, since annotating morphologi-
cal information for a treebank is expensive. Our
finding suggests that the best investment of anno-
tation effort may be in dependencies, leaving mor-
phological features to be learned implicitly from
strings.
The character-based representations are also a
way of overcoming the out-of-vocabulary prob-
lem; without any additional resources, they en-
able the parser to substantially improve the per-
formance when OOV rates are high. We expect
that, in conjunction with a pretraing regime, or in
conjunction with distributional word embeddings,
further improvements could be realized.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999882636363636">
MB was supported by the European Com-
mission under the contract numbers FP7-ICT-
610411 (project MULTISENSOR) and H2020-
RIA-645012 (project KRISTINA). This research
was supported by the U.S. Army Research Labo-
ratory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533 and
NSF IIS-1054319. This work was completed
while NAS was at CMU. Thanks to Joakim Nivre,
Bernd Bohnet, Fei Liu and Swabha Swayamdipta
for useful comments.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.967873690140846">
Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel.
2003. Building a treebank for French. In Treebanks.
Springer.
Itziar Aduriz, Mar´ıa Jes´us Aranzabe, Jose Mari Arriola,
Aitziber Atutxa, Arantza D´ıaz de Ilarraza, Aitzpea
Garmendia, and Maite Oronoz. 2003. Construction
of a Basque dependency treebank. In Proc of TLT.
Miguel Ballesteros. 2013. Effective morphological
feature selection with maltoptimizer at the SPMRL
2013 shared task. In Proc. of SPMRL-EMNLP.
Anders Bj¨orkelund, Ozlem Cetinoglu, Rich´ard Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking Meets Morphosyntax: State-of-the-art
Results from the SPMRL 2013 Shared Task. In
SPMRL-EMNLP.
Anders Bj¨orkelund, ¨Ozlem C¸etino˘glu, Agnieszka
Fale´nska, Rich´ard Farkas, Thomas Mueller, Wolf-
gang Seeker, and Zsolt Sz´ant´o. 2014. Introducing
the IMS-Wrocław-Szeged-CIS entry at the SPMRL
2014 Shared Task: Reranking and Morpho-syntax
meet Unlabeled Data. In SPMRL-SANCL.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky,
Richard Farkas, Filip Ginter, and Jan Hajiˇc. 2013.
Joint morphological and syntactic analysis for richly
inflected languages. TACL, 1.
Jan A. Botha and Phil Blunsom. 2014. Composi-
tional Morphology for Word Representations and
Language Modelling. In ICML.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X.
In Proc of CoNLL.
Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. EMNLP.
Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun,
and Huanbo Luan. 2015. Joint learning of character
and word embeddings. In Proc. IJCAI.
Jinho D. Choi. 2013. Preparing Korean Data for the
Shared Task on Parsing Morphologically Rich Lan-
guages. ArXiv e-prints, September.
Grzegorz Chrupała. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proc of ACL.
Shay B. Cohen and Noah A. Smith. 2007. Joint mor-
phological and syntactic disambiguation. In Proc.
EMNLP-CoNLL.
Cicero Nogueira dos Santos and Victor Guimar˜aes.
2015. Boosting named entity recognition with neu-
ral character embeddings. Arxiv.
Cicero dos Santos and Bianca Zadrozny. 2014.
Learning character-level representations for part-of-
speech tagging. In Proc of ICML-14.
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proc of ACL.
Felix A. Gers, Nicol N. Schraudolph, and J¨urgen
Schmidhuber. 2002. Learning precise timing with
LSTM recurrent networks. JMLR.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew segmentation and parsing using a PCFG-LA
lattice parser. In Proc of ACL.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
TACL.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proc of ACL.
Alex Graves and J¨urgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5-6).
</reference>
<page confidence="0.989537">
357
</page>
<table confidence="0.3516436">
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-T¨ur,
and G¨okhan T¨ur. 2003. Building a Turkish tree-
bank. In Treebanks, pages 261–277. Springer.
Alex Graves. 2013. Generating sequences with recur-
rent neural networks. CoRR, abs/1308.0850.
</table>
<reference confidence="0.99906751">
Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnfk,
Bas R. Steunebrink, and J¨urgen Schmidhuber.
2015. LSTM: A search space odyssey. CoRR,
abs/1503.04069.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. In Proc. ACL.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proc of EMNLP.
Wang Ling, Tiago Lufs, Lufs Marujo, Ram´on Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W
Black, and Isabel Trancoso. 2015. Finding function
in form: Compositional character models for open
vocabulary word representation. In Proc. EMNLP.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics, 19(2):313–330.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc of LREC.
Thomas M¨uller, Helmut Schmid, and Hinrich Sch¨utze.
2013. Efficient higher-order CRFs for morphologi-
cal tagging. In Proc of EMNLP.
Joakim Nivre, Johan Hall, Jens Nilsson, G¨ulsen
Eryi˘git, and Svetoslav Marinov. 2006a. Labeled
pseudo-projective dependency parsing with support
vector machines. In Proc of CoNLL.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006b.
Talbanken05: A Swedish treebank with phrase
structure and dependency annotation. In Proc of
LREC, Genoa, Italy.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proc of the Workshop on In-
cremental Parsing: Bringing Engineering and Cog-
nition Together.
Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proc of ACL.
Ferdinand Saussure. 1916. Nature of the linguistic
sign. In Course in General Linguistics.
Djam´e Seddah and Reut Tsarfaty. 2014. Intro-
ducing the SPMRL 2014 shared task on parsing
morphologically-rich languages. SPMRL-SANCL
2014.
Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie
Candito, Jinho D. Choi, Rich´ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang
Seeker, Yannick Versley, Veronika Vincze, Marcin
Woli´nski, Alina Wr´oblewska, and Eric Villemonte
de la Clergerie. 2013. Overview of the SPMRL
2013 shared task: cross-framework evaluation of
parsing morphologically rich languages. In SPMRL-
EMNLP 2013.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proc of LREC.
Khalil Sima’an, Alon Itai, Yoad Winter, Alon Altman,
and Noa Nativ. 2001. Building a Tree-Bank for
Modern Hebrew Text. In Traitement Automatique
des Langues.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Proc of NIPS.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2013a.
Grounded compositional semantics for finding and
describing images with sentences. TACL.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc of EMNLP.
Pontus Stenetorp. 2013. Transition-based dependency
parsing using recursive neural networks. In Proc of
NIPS Deep Learning Workshop.
Marek ´Swidzi´nski and Marcin Woli´nski. 2010. To-
wards a bank of constituent parse trees for Polish.
In Proc of TSD.
Ivan. Titov and James. Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proc of IWPT.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proc of NAACL.
</reference>
<page confidence="0.98069">
358
</page>
<reference confidence="0.999283304347826">
Reut Tsarfaty. 2006. Integrated morphological and
syntactic disambiguation for Modern Hebrew. In
Proc of ACL Student Research Workshop.
Veronika Vincze, D´ora Szauter, Attila Alm´asi, Gy¨orgy
M´ora, Zolt´an Alexin, and J´anos Csirik. 2010. Hun-
garian dependency treebank. In Proc of LREC.
David Weiss, Christopher Alberti, Michael Collins, and
Slav Petrov. 2015. Structured training for neural
network transition-based parsing. In Proc of ACL.
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proc ofACL.
Yue Zhang and Stephen Clark. 2008b. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Proc
of EMNLP.
Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting
Liu. 2013. Chinese parsing exploiting characters.
In Proc of ACL.
Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun
Chen. 2015. A Neural Probabilistic Structured-
Prediction Model for Transition-Based Dependency
Parsing. In Proc of ACL.
</reference>
<page confidence="0.999076">
359
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.109447">
<title confidence="0.8800725">Improved Transition-Based by Modeling Characters instead of Words with LSTMs</title>
<author confidence="0.228477">A</author>
<affiliation confidence="0.751235">NLP Group, Pompeu Fabra University, Barcelona, Spain</affiliation>
<address confidence="0.819159666666667">of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA Labs, Pittsburgh, PA, USA Science &amp; Engineering, University of Washington, Seattle, WA, USA</address>
<email confidence="0.999472">miguel.ballesteros@upf.edu,chris@marianaslabs.com,nasmith@cs.washington.edu</email>
<abstract confidence="0.999193166666667">We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages. Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are similar on the surface. Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Lionel Cl´ement</author>
<author>Franc¸ois Toussenel</author>
</authors>
<title>Building a treebank for French. In Treebanks.</title>
<date>2003</date>
<publisher>Springer.</publisher>
<marker>Abeill´e, Cl´ement, Toussenel, 2003</marker>
<rawString>Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a treebank for French. In Treebanks. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Itziar Aduriz</author>
<author>Mar´ıa Jes´us Aranzabe</author>
<author>Jose Mari Arriola</author>
</authors>
<title>Aitziber Atutxa, Arantza D´ıaz de Ilarraza, Aitzpea Garmendia, and Maite Oronoz.</title>
<date>2003</date>
<booktitle>In Proc of TLT.</booktitle>
<contexts>
<context position="14679" citStr="Aduriz et al., 2003" startWordPosition="2439" endWordPosition="2442">his hypothesis. 4 Experiments We applied our parsing model and several variations of it to several parsing tasks and report reFigure 3: Character-based word embedding of the word party. This representation is used for both in-vocabulary and out-of-vocabulary words. sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (´Swidzi´nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is </context>
</contexts>
<marker>Aduriz, Aranzabe, Arriola, 2003</marker>
<rawString>Itziar Aduriz, Mar´ıa Jes´us Aranzabe, Jose Mari Arriola, Aitziber Atutxa, Arantza D´ıaz de Ilarraza, Aitzpea Garmendia, and Maite Oronoz. 2003. Construction of a Basque dependency treebank. In Proc of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
</authors>
<title>Effective morphological feature selection with maltoptimizer at the SPMRL 2013 shared task.</title>
<date>2013</date>
<booktitle>In Proc. of SPMRL-EMNLP.</booktitle>
<contexts>
<context position="2460" citStr="Ballesteros, 2013" startWordPosition="356" endWordPosition="357">ks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well known that a word’s form often provides strong evidence regarding its grammatical role in morphologically rich languages (Ballesteros, 2013, inter alia), this has promise to improve accuracy and statistical efficiency relative to traditional approaches that treat each word type as opaque and independently modeled. In the traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency. Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3). Although our model is provided no supervision in th</context>
<context position="20328" citStr="Ballesteros, 2013" startWordPosition="3349" endWordPosition="3350"> Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. 8Tense and number features provide little improvement in a transition-based parser, compared with other features such as case, when the POS tags are included (Ballesteros, 2013). 353 UAS Language Words Chars Words Chars + POS + POS Arabic 86.14 87.20 87.44 87.07 Basque 78.42 84.97 83.49 85.58 French 84.84 86.21 87.00 86.33 German 88.14 90.94 91.16 91.23 Hebrew 79.73 79.92 81.99 80.76 Hungarian 72.38 80.16 78.47 80.85 Korean 78.98 88.98 87.36 89.14 Polish 73.29 85.69 89.32 88.54 Swedish 73.44 75.03 80.02 78.85 Turkish 71.10 74.91 77.13 77.96 Chinese 79.43 80.36 85.98 85.81 English 91.64 91.98 92.94 92.49 Average 79.79 83.86 85.19 85.38 LAS Language Words Chars Words Chars + POS + POS Arabic 82.73 84.34 84.81 84.36 Basque 67.08 78.22 74.31 79.52 French 80.32 81.70 82.7</context>
<context position="25891" citStr="Ballesteros (2013)" startWordPosition="4254" endWordPosition="4256">same cons9We are using a machine with 32 Intel Xeon CPU E5- 2650 at 2.00GHz; the parser runs on a single core. tant, needing some hours to have a competitive model. In terms of memory, Words requires on average 300 MB of main memory for both training and parsing, while Chars requires 450 MB. 4.4.4 Comparison with State-of-the-Art Table 3 shows a comparison with state-of-theart parsers. We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013). For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features. For English and Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretr</context>
<context position="28387" citStr="Ballesteros (2013)" startWordPosition="4646" endWordPosition="4647">n/a K+’10 Chinese 85.96 84.40 Words + POS 87.20 85.70 D+’15 87.20 85.70 D+’15 English 92.57 90.31 Words + POS 93.10 90.90 D+’15 94.08 92.19 W+’15 Table 3: Test-set performance of our best results (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but wi</context>
</contexts>
<marker>Ballesteros, 2013</marker>
<rawString>Miguel Ballesteros. 2013. Effective morphological feature selection with maltoptimizer at the SPMRL 2013 shared task. In Proc. of SPMRL-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Ozlem Cetinoglu</author>
<author>Rich´ard Farkas</author>
<author>Thomas Mueller</author>
<author>Wolfgang Seeker</author>
</authors>
<title>(Re)ranking Meets Morphosyntax: State-of-the-art Results from the SPMRL 2013 Shared Task.</title>
<date>2013</date>
<booktitle>In SPMRL-EMNLP.</booktitle>
<marker>Bj¨orkelund, Cetinoglu, Farkas, Mueller, Seeker, 2013</marker>
<rawString>Anders Bj¨orkelund, Ozlem Cetinoglu, Rich´ard Farkas, Thomas Mueller, and Wolfgang Seeker. 2013. (Re)ranking Meets Morphosyntax: State-of-the-art Results from the SPMRL 2013 Shared Task. In SPMRL-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>¨Ozlem C¸etino˘glu</author>
<author>Agnieszka Fale´nska</author>
<author>Rich´ard Farkas</author>
<author>Thomas Mueller</author>
<author>Wolfgang Seeker</author>
<author>Zsolt Sz´ant´o</author>
</authors>
<title>Introducing the IMS-Wrocław-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and Morpho-syntax meet Unlabeled Data.</title>
<date>2014</date>
<booktitle>In SPMRL-SANCL.</booktitle>
<marker>Bj¨orkelund, C¸etino˘glu, Fale´nska, Farkas, Mueller, Seeker, Sz´ant´o, 2014</marker>
<rawString>Anders Bj¨orkelund, ¨Ozlem C¸etino˘glu, Agnieszka Fale´nska, Rich´ard Farkas, Thomas Mueller, Wolfgang Seeker, and Zsolt Sz´ant´o. 2014. Introducing the IMS-Wrocław-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and Morpho-syntax meet Unlabeled Data. In SPMRL-SANCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
<author>Igor Boguslavsky</author>
<author>Richard Farkas</author>
<author>Filip Ginter</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Joint morphological and syntactic analysis for richly inflected languages.</title>
<date>2013</date>
<journal>TACL,</journal>
<volume>1</volume>
<marker>Bohnet, Nivre, Boguslavsky, Farkas, Ginter, Hajiˇc, 2013</marker>
<rawString>Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richard Farkas, Filip Ginter, and Jan Hajiˇc. 2013. Joint morphological and syntactic analysis for richly inflected languages. TACL, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan A Botha</author>
<author>Phil Blunsom</author>
</authors>
<title>Compositional Morphology for Word Representations and Language Modelling.</title>
<date>2014</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="29563" citStr="Botha and Blunsom (2014)" startWordPosition="4835" endWordPosition="4838"> (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar˜aes (2015) learned character-level neural representations for POS tagging and named entity recognition, getting a large error reduction in both tasks. Our approach is similar to theirs. Others have used character-based models as features to improve existing models. For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). </context>
</contexts>
<marker>Botha, Blunsom, 2014</marker>
<rawString>Jan A. Botha and Phil Blunsom. 2014. Compositional Morphology for Word Representations and Language Modelling. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<date>2006</date>
<booktitle>CoNLL-X. In Proc of CoNLL.</booktitle>
<contexts>
<context position="15248" citStr="Buchholz and Marsi, 2006" startWordPosition="2529" endWordPosition="2532">ic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (´Swidzi´nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same Chinese and English 3The POS tags were calculated with the MarMot tagger (M¨uller et al., 2013) by the best performing system of the SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4Since the Turkish dependency treebank does not have a development set, we extracted the last </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X. In Proc of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="1701" citStr="Chen and Manning, 2014" startWordPosition="234" endWordPosition="237">m incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 </context>
<context position="16036" citStr="Chen and Manning (2014)" startWordPosition="2676" endWordPosition="2679">the parser in the same Chinese and English 3The POS tags were calculated with the MarMot tagger (M¨uller et al., 2013) by the best performing system of the SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. UNK w JJ t party w NN t w w t &lt;w&gt; P a r t Y &lt;/w&gt; &lt;/w&gt; &lt;w&gt; a r t Y P NN 352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7. Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 Experimental Configurations In order to isolate the improvements provided by the LSTM encodings of characters, we run the stack LSTM parser in the following configurations: • Words: words only, as </context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxiong Chen</author>
<author>Lei Xu</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Huanbo Luan</author>
</authors>
<title>Joint learning of character and word embeddings.</title>
<date>2015</date>
<booktitle>In Proc. IJCAI.</booktitle>
<contexts>
<context position="29859" citStr="Chen et al. (2015)" startWordPosition="4880" endWordPosition="4883">tagging and named entity recognition, getting a large error reduction in both tasks. Our approach is similar to theirs. Others have used character-based models as features to improve existing models. For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar be</context>
</contexts>
<marker>Chen, Xu, Liu, Sun, Luan, 2015</marker>
<rawString>Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. 2015. Joint learning of character and word embeddings. In Proc. IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
</authors>
<title>Preparing Korean Data for the Shared Task on Parsing Morphologically Rich Languages. ArXiv e-prints,</title>
<date>2013</date>
<contexts>
<context position="14828" citStr="Choi, 2013" startWordPosition="2464" endWordPosition="2465">mbedding of the word party. This representation is used for both in-vocabulary and out-of-vocabulary words. sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (´Swidzi´nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in context with the most recent neural network transition-based parsers, we run the parser in t</context>
</contexts>
<marker>Choi, 2013</marker>
<rawString>Jinho D. Choi. 2013. Preparing Korean Data for the Shared Task on Parsing Morphologically Rich Languages. ArXiv e-prints, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
</authors>
<title>Normalizing tweets with edit scripts and recurrent neural embeddings.</title>
<date>2014</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="29470" citStr="Chrupała (2014)" startWordPosition="4824" endWordPosition="4825">hment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar˜aes (2015) learned character-level neural representations for POS tagging and named entity recognition, getting a large error reduction in both tasks. Our approach is similar to theirs. Others have used character-based models as features to improve existing models. For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. Methods for joint morphological disambiguation and parsing have been widely explored Tsar</context>
</contexts>
<marker>Chrupała, 2014</marker>
<rawString>Grzegorz Chrupała. 2014. Normalizing tweets with edit scripts and recurrent neural embeddings. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Joint morphological and syntactic disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL.</booktitle>
<contexts>
<context position="19753" citStr="Cohen and Smith (2007)" startWordPosition="3262" endWordPosition="3265">on practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense). Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy. However, the POS tags in treebanks for morphologically rich languages do not seem to be enough. Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case. Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. 8Tense and number features provide little improvement in a transition-based parser, compared with other features such as case, when the POS tags are included (Ballesteros, 2013). 353 UAS Language Words </context>
<context position="30103" citStr="Cohen and Smith (2007" startWordPosition="4915" endWordPosition="4918">ter-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segme</context>
</contexts>
<marker>Cohen, Smith, 2007</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2007. Joint morphological and syntactic disambiguation. In Proc. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero Nogueira dos Santos</author>
<author>Victor Guimar˜aes</author>
</authors>
<title>Boosting named entity recognition with neural character embeddings.</title>
<date>2015</date>
<publisher>Arxiv.</publisher>
<marker>Santos, Guimar˜aes, 2015</marker>
<rawString>Cicero Nogueira dos Santos and Victor Guimar˜aes. 2015. Boosting named entity recognition with neural character embeddings. Arxiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero dos Santos</author>
<author>Bianca Zadrozny</author>
</authors>
<title>Learning character-level representations for part-ofspeech tagging.</title>
<date>2014</date>
<booktitle>In Proc of ICML-14.</booktitle>
<contexts>
<context position="29148" citStr="Santos and Zadrozny (2014)" startWordPosition="4775" endWordPosition="4779">K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar˜aes (2015) learned character-level neural representations for POS tagging and named entity recognition, getting a large error reduction in both tasks. Our approach is similar to theirs. Others have used character-based models as features to improve existing models. For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning rela</context>
</contexts>
<marker>Santos, Zadrozny, 2014</marker>
<rawString>Cicero dos Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-ofspeech tagging. In Proc of ICML-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Miguel Ballesteros</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Noah A Smith</author>
</authors>
<title>Transitionbased dependency parsing with stack long shortterm memory.</title>
<date>2015</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="1720" citStr="Dyer et al., 2015" startWordPosition="238" endWordPosition="241">acter-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well</context>
<context position="3893" citStr="Dyer et al. (2015)" startWordPosition="571" endWordPosition="574">ecially in agglutinative languages and the ones that present extensive case systems (§4). In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916). Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parser of Dyer et al. (2015) can learn to generate nonprojective trees. We do this by augmenting its transition operations 1Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. with a SWAP operation (Nivre, 2009) (§2.4), enabling the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing th</context>
<context position="8411" citStr="Dyer et al. (2015)" startWordPosition="1361" endWordPosition="1364">putation of the next iterate. Further, the stack LSTM provides a pop operation that moves the stack pointer to the previous element. Hence each of the parser data structures (B, 5, and A) is implemented with its own stack LSTM, each with its own parameters. The values of bt, st, and at are the ht vectors from their respective stack LSTMs. 2.2 Composition Functions Whenever a REDUCE operation is selected, two tree fragments are popped off of 5 and combined to form a new tree fragment, which is then popped back onto 5 (see Figure 1). This tree must be embedded as an input vector xt. To do this, Dyer et al. (2015) use a recursive neural network gr (for relation r) that composes 350 Stackt Buffert Action Stackt+1 Buffert+1 Dependency (u, u), (v, v), S B REDUCE-RIGHT(r) (gr(u, v), u), S B r (u, u), (v, v), S B REDUCE-LEFT(r) (gr(v, u), v), S B u → v S (u, u), B SHIFT (u, u), S B r u ← v — (u, u), (v, v), S B SWAP (u, u), S (v, v), B — Figure 1: Parser transitions indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015)</context>
<context position="10387" citStr="Dyer et al. (2015)" startWordPosition="1721" endWordPosition="1724"> those where preconditions are met; see Figure 1), the probability of action z E A(S, B) defined using a log-linear distribution: p(z |pt) = exp (gz pt + Q (2) Ez&apos;EA(S,B) exp (gz pt + qz&apos;) (where gz and qz are parameters associated with each action type z). Parsing proceeds by always choosing the most probable action from A(S, B). The probabilistic definition allows parameter estimation for all of the parameters (W*, b* in all three stack LSTMs, as well as W, d, g*, and q*) by maximizing the conditional likelihood of each correct parser decisions given the state. 2.4 Adding the SWAP Operation Dyer et al. (2015)’s parser implemented the most basic version of the arc-standard algorithm, which is capable of producing only projective parse trees. In order to deal with nonprojective trees, we also add the SWAP operation which allows nonprojective trees to be produced. The SWAP operation, first introduced by Nivre (2009), allows a transition-based parser to produce nonprojective trees. Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack. This is easily handled with the stack LSTM. Figure 1 shows how the parser is ca</context>
<context position="12136" citStr="Dyer et al. (2015)" startWordPosition="2021" endWordPosition="2024">gs. 3.1 Baseline: Standard Word Embeddings Dyer et al.’s parser generates a word representation for each input token by concatenating two vectors: a vector representation for each word type (w) and a representation (t) of the POS tag of the token (if it is used), provided as auxiliary input to the parser.2 A linear map (V) is applied to the resulting vector and passed through a component-wise ReLU: x = max {0, V[w; t] + b} For out-of-vocabulary words, the parser uses an “UNK” token that is handled as a separate word during parsing time. This mapping can be shown schematically as in Figure 2. 2Dyer et al. (2015), included a third input representation learned from a neural language model ( ˜wLM). We do not include these pretrained representations in our experiments, focusing instead on character-based representations. 351 Figure 2: Baseline model word embeddings for an in-vocabulary word that is tagged with POS tag NN (right) and an out-of-vocabulary word with POS tag JJ (left). 3.2 Character-Based Embeddings of Words Following Ling et al. (2015), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber, 2005). When the parser initiates t</context>
<context position="13992" citStr="Dyer et al. (2015)" startWordPosition="2329" endWordPosition="2332">tation w. As in §3.1, a linear map (V) is applied and passed through a component-wise ReLU. x = max � 0, V [→w; w; t] + b This process is shown schematically in Figure 3. Note that under this representation, out-ofvocabulary words are treated as bidirectional LSTM encodings and thus they will be “close” to other words that the parser has seen during training, ideally close to their more frequent, syntactically similar morphological relatives. We conjecture that this will give a clear advantage over a single “UNK” token for all the words that the parser does not see during training, as done by Dyer et al. (2015) and other parsers without additional resources. In §4 we confirm this hypothesis. 4 Experiments We applied our parsing model and several variations of it to several parsing tasks and report reFigure 3: Character-based word embedding of the word party. This representation is used for both in-vocabulary and out-of-vocabulary words. sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; S</context>
<context position="16059" citStr="Dyer et al. (2015)" startWordPosition="2681" endWordPosition="2684">se and English 3The POS tags were calculated with the MarMot tagger (M¨uller et al., 2013) by the best performing system of the SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. UNK w JJ t party w NN t w w t &lt;w&gt; P a r t Y &lt;/w&gt; &lt;/w&gt; &lt;w&gt; a r t Y P NN 352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7. Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 Experimental Configurations In order to isolate the improvements provided by the LSTM encodings of characters, we run the stack LSTM parser in the following configurations: • Words: words only, as in §3.1 (but without PO</context>
<context position="18142" citStr="Dyer et al. (2015)" startWordPosition="3006" endWordPosition="3009">e the character-based representations have 100 dimensions, when used. Part of speech embeddings have 12 dimensions. These dimensionalities were chosen after running several tests with different values, but a more careful selection of these values would probably further improve results. 5Training: 001–815, 1001–1136. Development: 886– 931, 1148–1151. Test: 816–885, 1137–1147. 6Training: 02–21. Development: 22. Test: 23. 7The POS tags are predicted by using the Stanford Tagger (Toutanova et al., 2003) with an accuracy of 97.3%. 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped when the learned model’s UAS stops improving on the development set, and this model is used to parse the test set. No pretraining of any parameters is done. 4.4 Results and Discussion Tables 1 and 2 show the results of the parsers for the development sets and the final test sets, respectively. Most notable are improvements for agglutinative languages—Basque, Hungarian, Korean,</context>
<context position="26455" citStr="Dyer et al., 2015" startWordPosition="4336" endWordPosition="4339">RL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features. For English and Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. We also show the best reported results on these datasets. For the SPMRL data sets, the best performing system of the shared task is either Bj¨orkelund et al. (2013) or Bj¨orkelund et al. (2014), which are consistently better than our sysdays cranes gasoline event steady grandiose profession consumption consultant overtly possibly pitcher relief drop constructive perfect meet computer-driven median plummeting approving restating suspended washing leveraging retiring declared outnumbered achieved advanced 355 This Work Best Greedy Result</context>
<context position="28448" citStr="Dyer et al. (2015)" startWordPosition="4656" endWordPosition="4659">7.20 85.70 D+’15 English 92.57 90.31 Words + POS 93.10 90.90 D+’15 94.08 92.19 W+’15 Table 3: Test-set performance of our best results (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based</context>
</contexts>
<marker>Dyer, Ballesteros, Ling, Matthews, Smith, 2015</marker>
<rawString>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transitionbased dependency parsing with stack long shortterm memory. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix A Gers</author>
<author>Nicol N Schraudolph</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Learning precise timing with LSTM recurrent networks.</title>
<date>2002</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="7188" citStr="Gers et al., 2002" startWordPosition="1136" endWordPosition="1139"> the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt, the previous hidden state ht−1, and the memory cell ct−1: it = Q(WZxxt + WZhht−1 + WZcct−1 + bZ) ft = 1 − it ct = ft O ct−1+ it O tanh(Wcxxt + Wchht−1 + bc) ot = Q(Woxxt + Wohht−1 + Wocct + bo) ht = ot O tanh(ct), where Q is the component-wise logistic sigmoid function and O is the component-wise (Hadamard) product. Parameters are all represented using W and b. This formulation differs slightly from the classic LSTM formulation in that it makes use of “peephole connections” (Gers et al., 2002) and defines the forget gate so that it sums with the input gate to 1(Greff et al., 2015). To improve the representational capacity of LSTMs (and RNNs generally), they can be stacked in “layers.” In these architectures, the input LSTM at higher layers at time t is the value of ht computed by the lower layer (and xt is the input at the lowest layer). The stack LSTM augments the left-to-right sequential model of the conventional LSTM with a stack pointer. As in the LSTM, new inputs are added in the right-most position, but the stack pointer indicates which LSTM cell provides ct−1 and ht−1 for th</context>
</contexts>
<marker>Gers, Schraudolph, Schmidhuber, 2002</marker>
<rawString>Felix A. Gers, Nicol N. Schraudolph, and J¨urgen Schmidhuber. 2002. Learning precise timing with LSTM recurrent networks. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Joint Hebrew segmentation and parsing using a PCFG-LA lattice parser.</title>
<date>2011</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="30161" citStr="Goldberg and Elhadad (2011)" startWordPosition="4923" endWordPosition="4926">ets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zha</context>
</contexts>
<marker>Goldberg, Elhadad, 2011</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2011. Joint Hebrew segmentation and parsing using a PCFG-LA lattice parser. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parsers with non-deterministic oracles.</title>
<date>2013</date>
<publisher>TACL.</publisher>
<contexts>
<context position="25820" citStr="Goldberg and Nivre, 2013" startWordPosition="4240" endWordPosition="4243">sentence in 44 ms, whileChars needs 130 ms.9 Training time is affected by the same cons9We are using a machine with 32 Intel Xeon CPU E5- 2650 at 2.00GHz; the parser runs on a single core. tant, needing some hours to have a competitive model. In terms of memory, Words requires on average 300 MB of main memory for both training and parsing, while Chars requires 450 MB. 4.4.4 Comparison with State-of-the-Art Table 3 shows a comparison with state-of-theart parsers. We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013). For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features. For English and Chi</context>
</contexts>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. TACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="30132" citStr="Goldberg and Tsarfaty (2008" startWordPosition="4919" endWordPosition="4922">ral networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predict</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Framewise phoneme classification with bidirectional lstm and other neural network architectures.</title>
<date>2005</date>
<journal>Neural Networks,</journal>
<pages>18--5</pages>
<contexts>
<context position="12707" citStr="Graves and Schmidhuber, 2005" startWordPosition="2105" endWordPosition="2108">be shown schematically as in Figure 2. 2Dyer et al. (2015), included a third input representation learned from a neural language model ( ˜wLM). We do not include these pretrained representations in our experiments, focusing instead on character-based representations. 351 Figure 2: Baseline model word embeddings for an in-vocabulary word that is tagged with POS tag NN (right) and an out-of-vocabulary word with POS tag JJ (left). 3.2 Character-Based Embeddings of Words Following Ling et al. (2015), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber, 2005). When the parser initiates the learning process and populates the buffer with all the words from the sentence, it reads the words character by character from left to right and computes a continuous-space vector embedding the character sequence, which is the h vector of → the LSTM; we denote it by w. The same process is also applied in reverse (albeit with different parameters), computing a similar continuous-space vector embedding starting from the last character and finishing at the first ( ← w); again each character is represented with an LSTM cell. After that, we concatenate these vectors </context>
</contexts>
<marker>Graves, Schmidhuber, 2005</marker>
<rawString>Alex Graves and J¨urgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Greff</author>
<author>Rupesh Kumar Srivastava</author>
<author>Jan Koutnfk</author>
<author>Bas R Steunebrink</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>LSTM: A search space odyssey.</title>
<date>2015</date>
<location>CoRR, abs/1503.04069.</location>
<contexts>
<context position="7277" citStr="Greff et al., 2015" startWordPosition="1154" endWordPosition="1157">t and hidden state ht given xt, the previous hidden state ht−1, and the memory cell ct−1: it = Q(WZxxt + WZhht−1 + WZcct−1 + bZ) ft = 1 − it ct = ft O ct−1+ it O tanh(Wcxxt + Wchht−1 + bc) ot = Q(Woxxt + Wohht−1 + Wocct + bo) ht = ot O tanh(ct), where Q is the component-wise logistic sigmoid function and O is the component-wise (Hadamard) product. Parameters are all represented using W and b. This formulation differs slightly from the classic LSTM formulation in that it makes use of “peephole connections” (Gers et al., 2002) and defines the forget gate so that it sums with the input gate to 1(Greff et al., 2015). To improve the representational capacity of LSTMs (and RNNs generally), they can be stacked in “layers.” In these architectures, the input LSTM at higher layers at time t is the value of ht computed by the lower layer (and xt is the input at the lowest layer). The stack LSTM augments the left-to-right sequential model of the conventional LSTM with a stack pointer. As in the LSTM, new inputs are added in the right-most position, but the stack pointer indicates which LSTM cell provides ct−1 and ht−1 for the computation of the next iterate. Further, the stack LSTM provides a pop operation that </context>
</contexts>
<marker>Greff, Srivastava, Koutnfk, Steunebrink, Schmidhuber, 2015</marker>
<rawString>Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnfk, Bas R. Steunebrink, and J¨urgen Schmidhuber. 2015. LSTM: A search space odyssey. CoRR, abs/1503.04069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="9505" citStr="Hermann and Blunsom, 2013" startWordPosition="1562" endWordPosition="1566">cate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr(u, v) or gr(v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 Predicting Parser Decisions The parser uses a probabilistic model of parser decisions at each time step t. Letting A(S, B) denote the set of allowed transitions given the stack S and buffer S (i.e., those where preconditions are met; see Figure 1), the probability of action z E A(S, B) defined using a log-linear distribution: p(z |pt) = exp (gz pt + Q (2) Ez&apos;EA(S,B) exp (gz pt + qz&apos;) (where gz and qz are parameters associated with each action type z). Parsing proceeds by always choosing the most probable action from A(S, B). The</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of compositional semantics. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<journal>Neural Computation,</journal>
<volume>9</volume>
<issue>8</issue>
<contexts>
<context position="6368" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="986" endWordPosition="989">.3. We discuss the inclusion of SWAP in §2.4. 2.1 Stack LSTMs RNNs are functions that read a sequence of vectors incrementally; at time step t the vector xt is read in and the hidden state ht computed using xt and the previous hidden state ht−1. In principle, this allows retaining information from time steps in the distant past, but the nonlinear “squashing” functions applied in the calcluation of each ht result in a decay of the error signal used in training with backpropagation. LSTMs are a variant of RNNs designed to cope with this “vanishing gradient” problem using an extra memory “cell” (Hochreiter and Schmidhuber, 1997; Graves, 2013). Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory (it) or forget (ft). We refer interested readers to the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt, the previous hidden state ht−1, and the memory cell ct−1: it = Q(WZxxt + WZhht−1 + WZcct−1 + bZ) ft = 1 − it ct = ft O ct−1+ it O tanh(Wcxxt + Wchht−1 + bc) ot = Q(Woxxt + Wohht−1 + Wocct + bo) ht = ot O tanh(ct), where Q is the component-wise logistic sigmoid function and O is</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proc of EMNLP.</booktitle>
<contexts>
<context position="28548" citStr="Koo et al. (2010)" startWordPosition="4674" endWordPosition="4677">t performance of our best results (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014)</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
</authors>
<title>Tiago Lufs, Lufs Marujo,</title>
<date>2015</date>
<booktitle>In Proc. EMNLP.</booktitle>
<location>Ram´on</location>
<marker>Ling, 2015</marker>
<rawString>Wang Ling, Tiago Lufs, Lufs Marujo, Ram´on Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. 2015. Finding function in form: Compositional character models for open vocabulary word representation. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Wigdan Mekki</author>
</authors>
<title>The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus.</title>
<date>2004</date>
<booktitle>In NEMLAR Conference on Arabic Language Resources and Tools.</booktitle>
<contexts>
<context position="14649" citStr="Maamouri et al., 2004" startWordPosition="2434" endWordPosition="2437">al resources. In §4 we confirm this hypothesis. 4 Experiments We applied our parsing model and several variations of it to several parsing tasks and report reFigure 3: Character-based word embedding of the word party. This representation is used for both in-vocabulary and out-of-vocabulary words. sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (´Swidzi´nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006).</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus. In NEMLAR Conference on Arabic Language Resources and Tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="16281" citStr="Marcus et al., 1993" startWordPosition="2720" endWordPosition="2723">rman: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. UNK w JJ t party w NN t w w t &lt;w&gt; P a r t Y &lt;/w&gt; &lt;/w&gt; &lt;w&gt; a r t Y P NN 352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7. Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 Experimental Configurations In order to isolate the improvements provided by the LSTM encodings of characters, we run the stack LSTM parser in the following configurations: • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags) • Words + POS: words and POS tags (§3.1) • Chars + POS: character-based representations of words with bi</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proc of LREC.</booktitle>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proc of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M¨uller</author>
<author>Helmut Schmid</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Efficient higher-order CRFs for morphological tagging.</title>
<date>2013</date>
<booktitle>In Proc of EMNLP.</booktitle>
<marker>M¨uller, Schmid, Sch¨utze, 2013</marker>
<rawString>Thomas M¨uller, Helmut Schmid, and Hinrich Sch¨utze. 2013. Efficient higher-order CRFs for morphological tagging. In Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>G¨ulsen Eryi˘git</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proc of CoNLL.</booktitle>
<marker>Nivre, Hall, Nilsson, Eryi˘git, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, G¨ulsen Eryi˘git, and Svetoslav Marinov. 2006a. Labeled pseudo-projective dependency parsing with support vector machines. In Proc of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
</authors>
<title>Talbanken05: A Swedish treebank with phrase structure and dependency annotation.</title>
<date>2006</date>
<booktitle>In Proc of LREC,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="14903" citStr="Nivre et al., 2006" startWordPosition="2473" endWordPosition="2476">vocabulary and out-of-vocabulary words. sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (´Swidzi´nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same Chinese and English 3The POS tags were calculated with the MarMot t</context>
<context position="26051" citStr="Nivre et al. (2006" startWordPosition="4279" endWordPosition="4282">del. In terms of memory, Words requires on average 300 MB of main memory for both training and parsing, while Chars requires 450 MB. 4.4.4 Comparison with State-of-the-Art Table 3 shows a comparison with state-of-theart parsers. We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013). For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features. For English and Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. We also show the best reported results on these datasets. For the SPMRL data sets, the best performing system of the shared task is eithe</context>
<context position="28417" citStr="Nivre et al. (2006" startWordPosition="4650" endWordPosition="4653">Words + POS 87.20 85.70 D+’15 87.20 85.70 D+’15 English 92.57 90.31 Words + POS 93.10 90.90 D+’15 94.08 92.19 W+’15 Table 3: Test-set performance of our best results (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings.</context>
</contexts>
<marker>Nivre, Nilsson, Hall, 2006</marker>
<rawString>Joakim Nivre, Jens Nilsson, and Johan Hall. 2006b. Talbanken05: A Swedish treebank with phrase structure and dependency annotation. In Proc of LREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proc of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together.</booktitle>
<contexts>
<context position="4944" citStr="Nivre, 2004" startWordPosition="729" endWordPosition="730">ng the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based. Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack 5 containing partially-built parses, and a list A of actions previously taken by the parser. In particular, the parser implements the arc-standard parsing algorithm (Nivre, 2004). At each time step t, a transition action is applied that alters these data structures by pushing or popping words from the stack and the buffer; the operations are listed in Figure 1. Along with the discrete transitions above, the parser calculates a vector representation of the states of B, 5, and A; at time step t these are denoted by bt, st, and at, respectively. The total parser state at t is given by pt = max {0, W[st; bt; at] + d} (1) where the matrix W and the vector d are learned parameters. This continuous-state representation pt is used to decide which operation to apply next, upda</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proc of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-projective dependency parsing in expected linear time.</title>
<date>2009</date>
<booktitle>In Proc of ACL. Ferdinand Saussure.</booktitle>
<contexts>
<context position="4317" citStr="Nivre, 2009" startWordPosition="627" endWordPosition="628">it POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parser of Dyer et al. (2015) can learn to generate nonprojective trees. We do this by augmenting its transition operations 1Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. with a SWAP operation (Nivre, 2009) (§2.4), enabling the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based. Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack 5 containing partially-built parses, and a list A of actions previously taken by the parser. In particular, the parser implements the arc-standard pars</context>
<context position="10697" citStr="Nivre (2009)" startWordPosition="1771" endWordPosition="1772">on from A(S, B). The probabilistic definition allows parameter estimation for all of the parameters (W*, b* in all three stack LSTMs, as well as W, d, g*, and q*) by maximizing the conditional likelihood of each correct parser decisions given the state. 2.4 Adding the SWAP Operation Dyer et al. (2015)’s parser implemented the most basic version of the arc-standard algorithm, which is capable of producing only projective parse trees. In order to deal with nonprojective trees, we also add the SWAP operation which allows nonprojective trees to be produced. The SWAP operation, first introduced by Nivre (2009), allows a transition-based parser to produce nonprojective trees. Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack. This is easily handled with the stack LSTM. Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. 3 Word Representations The main contribution of this paper is to change th</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proc of ACL. Ferdinand Saussure. 1916. Nature of the linguistic sign. In Course in General Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Reut Tsarfaty</author>
</authors>
<title>Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages. SPMRL-SANCL</title>
<date>2014</date>
<contexts>
<context position="3270" citStr="Seddah and Tsarfaty, 2014" startWordPosition="477" endWordPosition="480">traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency. Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3). Although our model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets (Seddah et al., 2013; Seddah and Tsarfaty, 2014), especially in agglutinative languages and the ones that present extensive case systems (§4). In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916). Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parse</context>
<context position="14617" citStr="Seddah and Tsarfaty, 2014" startWordPosition="2429" endWordPosition="2432">) and other parsers without additional resources. In §4 we confirm this hypothesis. 4 Experiments We applied our parsing model and several variations of it to several parsing tasks and report reFigure 3: Character-based word embedding of the word party. This representation is used for both in-vocabulary and out-of-vocabulary words. sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (´Swidzi´nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared </context>
</contexts>
<marker>Seddah, Tsarfaty, 2014</marker>
<rawString>Djam´e Seddah and Reut Tsarfaty. 2014. Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages. SPMRL-SANCL 2014.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Djam´e Seddah</author>
<author>Reut Tsarfaty</author>
<author>Sandra K¨ubler</author>
<author>Marie Candito</author>
<author>Jinho D Choi</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
</authors>
<title>Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green,</title>
<date>2013</date>
<booktitle>Wr´oblewska, and Eric Villemonte de la Clergerie. 2013. Overview of the SPMRL</booktitle>
<institution>Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, Alina</institution>
<location>Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang</location>
<marker>Seddah, Tsarfaty, K¨ubler, Candito, Choi, Farkas, Foster, 2013</marker>
<rawString>Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie Candito, Jinho D. Choi, Rich´ard Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, Alina Wr´oblewska, and Eric Villemonte de la Clergerie. 2013. Overview of the SPMRL 2013 shared task: cross-framework evaluation of parsing morphologically rich languages. In SPMRLEMNLP 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Making Ellipses Explicit in Dependency Conversion for a German Treebank. In</title>
<date>2012</date>
<booktitle>Proc of LREC.</booktitle>
<contexts>
<context position="14743" citStr="Seeker and Kuhn, 2012" startWordPosition="2449" endWordPosition="2452"> several variations of it to several parsing tasks and report reFigure 3: Character-based word embedding of the word party. This representation is used for both in-vocabulary and out-of-vocabulary words. sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (´Swidzi´nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in context</context>
</contexts>
<marker>Seeker, Kuhn, 2012</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German Treebank. In Proc of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
<author>Alon Itai</author>
<author>Yoad Winter</author>
<author>Alon Altman</author>
<author>Noa Nativ</author>
</authors>
<title>Building a Tree-Bank for Modern Hebrew Text. In Traitement Automatique des Langues.</title>
<date>2001</date>
<marker>Sima’an, Itai, Winter, Altman, Nativ, 2001</marker>
<rawString>Khalil Sima’an, Alon Itai, Yoad Winter, Alon Altman, and Noa Nativ. 2001. Building a Tree-Bank for Modern Hebrew Text. In Traitement Automatique des Langues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Proc of NIPS.</booktitle>
<contexts>
<context position="9456" citStr="Socher et al., 2011" startWordPosition="1554" endWordPosition="1557"> stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr(u, v) or gr(v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 Predicting Parser Decisions The parser uses a probabilistic model of parser decisions at each time step t. Letting A(S, B) denote the set of allowed transitions given the stack S and buffer S (i.e., those where preconditions are met; see Figure 1), the probability of action z E A(S, B) defined using a log-linear distribution: p(z |pt) = exp (gz pt + Q (2) Ez&apos;EA(S,B) exp (gz pt + qz&apos;) (where gz and qz are parameters associated with each action type z). Parsing proceeds by always ch</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Proc of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2013</date>
<publisher>TACL.</publisher>
<contexts>
<context position="9477" citStr="Socher et al., 2013" startWordPosition="1558" endWordPosition="1561">tes. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr(u, v) or gr(v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 Predicting Parser Decisions The parser uses a probabilistic model of parser decisions at each time step t. Letting A(S, B) denote the set of allowed transitions given the stack S and buffer S (i.e., those where preconditions are met; see Figure 1), the probability of action z E A(S, B) defined using a log-linear distribution: p(z |pt) = exp (gz pt + Q (2) Ez&apos;EA(S,B) exp (gz pt + qz&apos;) (where gz and qz are parameters associated with each action type z). Parsing proceeds by always choosing the most proba</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2013a. Grounded compositional semantics for finding and describing images with sentences. TACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc of EMNLP.</booktitle>
<contexts>
<context position="9477" citStr="Socher et al., 2013" startWordPosition="1558" endWordPosition="1561">tes. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr(u, v) or gr(v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 Predicting Parser Decisions The parser uses a probabilistic model of parser decisions at each time step t. Letting A(S, B) denote the set of allowed transitions given the stack S and buffer S (i.e., those where preconditions are met; see Figure 1), the probability of action z E A(S, B) defined using a log-linear distribution: p(z |pt) = exp (gz pt + Q (2) Ez&apos;EA(S,B) exp (gz pt + qz&apos;) (where gz and qz are parameters associated with each action type z). Parsing proceeds by always choosing the most proba</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
</authors>
<title>Transition-based dependency parsing using recursive neural networks.</title>
<date>2013</date>
<booktitle>In Proc of NIPS Deep Learning Workshop.</booktitle>
<contexts>
<context position="1677" citStr="Stenetorp, 2013" startWordPosition="232" endWordPosition="233">odel benefits from incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the mo</context>
</contexts>
<marker>Stenetorp, 2013</marker>
<rawString>Pontus Stenetorp. 2013. Transition-based dependency parsing using recursive neural networks. In Proc of NIPS Deep Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marek ´Swidzi´nski</author>
<author>Marcin Woli´nski</author>
</authors>
<title>Towards a bank of constituent parse trees for Polish.</title>
<date>2010</date>
<booktitle>In Proc of TSD.</booktitle>
<marker>´Swidzi´nski, Woli´nski, 2010</marker>
<rawString>Marek ´Swidzi´nski and Marcin Woli´nski. 2010. Towards a bank of constituent parse trees for Polish. In Proc of TSD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Titov</author>
<author>James Henderson</author>
</authors>
<title>A latent variable model for generative dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc of IWPT.</booktitle>
<contexts>
<context position="1660" citStr="Titov and Henderson, 2007" startWordPosition="228" endWordPosition="231">ges show that the parsing model benefits from incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state se</context>
</contexts>
<marker>Titov, Henderson, 2007</marker>
<rawString>Ivan. Titov and James. Henderson. 2007. A latent variable model for generative dependency parsing. In Proc of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc of NAACL.</booktitle>
<contexts>
<context position="18028" citStr="Toutanova et al., 2003" startWordPosition="2989" endWordPosition="2992"> output embedding size is 20 dimensions. The learned word representations embeddings have 32 dimensions when used, while the character-based representations have 100 dimensions, when used. Part of speech embeddings have 12 dimensions. These dimensionalities were chosen after running several tests with different values, but a more careful selection of these values would probably further improve results. 5Training: 001–815, 1001–1136. Development: 886– 931, 1148–1151. Test: 816–885, 1137–1147. 6Training: 02–21. Development: 22. Test: 23. 7The POS tags are predicted by using the Stanford Tagger (Toutanova et al., 2003) with an accuracy of 97.3%. 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped when the learned model’s UAS stops improving on the development set, and this model is used to parse the test set. No pretraining of any parameters is done. 4.4 Results and Discussion Tables 1 and 2 show the results of the parsers for the development sets and the f</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proc of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
</authors>
<title>Integrated morphological and syntactic disambiguation for Modern Hebrew.</title>
<date>2006</date>
<booktitle>In Proc of ACL Student Research Workshop.</booktitle>
<contexts>
<context position="19726" citStr="Tsarfaty (2006)" startWordPosition="3259" endWordPosition="3260">rs + POS. It is common practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense). Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy. However, the POS tags in treebanks for morphologically rich languages do not seem to be enough. Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case. Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. 8Tense and number features provide little improvement in a transition-based parser, compared with other features such as case, when the POS tags are included (Ballesteros, 201</context>
<context position="30080" citStr="Tsarfaty (2006" startWordPosition="4912" endWordPosition="4914">014) used character-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and </context>
</contexts>
<marker>Tsarfaty, 2006</marker>
<rawString>Reut Tsarfaty. 2006. Integrated morphological and syntactic disambiguation for Modern Hebrew. In Proc of ACL Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
</authors>
<title>D´ora Szauter, Attila Alm´asi, Gy¨orgy M´ora, Zolt´an Alexin, and J´anos Csirik.</title>
<date>2010</date>
<booktitle>In Proc of LREC.</booktitle>
<marker>Vincze, 2010</marker>
<rawString>Veronika Vincze, D´ora Szauter, Attila Alm´asi, Gy¨orgy M´ora, Zolt´an Alexin, and J´anos Csirik. 2010. Hungarian dependency treebank. In Proc of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Weiss</author>
<author>Christopher Alberti</author>
<author>Michael Collins</author>
<author>Slav Petrov</author>
</authors>
<title>Structured training for neural network transition-based parsing.</title>
<date>2015</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="1760" citStr="Weiss et al., 2015" startWordPosition="246" endWordPosition="249">oduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well known that a word’s form often provides</context>
<context position="28578" citStr="Weiss et al. (2015)" startWordPosition="4680" endWordPosition="4683">sults (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar˜aes</context>
</contexts>
<marker>Weiss, Alberti, Collins, Petrov, 2015</marker>
<rawString>David Weiss, Christopher Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural network transition-based parsing. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron. In</title>
<date>2008</date>
<booktitle>Proc ofACL.</booktitle>
<contexts>
<context position="16149" citStr="Zhang and Clark (2008" startWordPosition="2697" endWordPosition="2700">13) by the best performing system of the SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. UNK w JJ t party w NN t w w t &lt;w&gt; P a r t Y &lt;/w&gt; &lt;/w&gt; &lt;w&gt; a r t Y P NN 352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7. Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 Experimental Configurations In order to isolate the improvements provided by the LSTM encodings of characters, we run the stack LSTM parser in the following configurations: • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in </context>
<context position="25771" citStr="Zhang and Clark, 2008" startWordPosition="4232" endWordPosition="4235"> could be cached. On average, Words parses a sentence in 44 ms, whileChars needs 130 ms.9 Training time is affected by the same cons9We are using a machine with 32 Intel Xeon CPU E5- 2650 at 2.00GHz; the parser runs on a single core. tant, needing some hours to have a competitive model. In terms of memory, Words requires on average 300 MB of main memory for both training and parsing, while Chars requires 450 MB. 4.4.4 Comparison with State-of-the-Art Table 3 shows a comparison with state-of-theart parsers. We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013). For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without exp</context>
<context position="30779" citStr="Zhang and Clark, 2008" startWordPosition="5015" endWordPosition="5018">11). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a). To the best of our knowledge, previous work has not used character-based embeddings to improve dependency parsers, as done in this paper. 6 Conclusion We have presented several interesting findings. First, we add new evidence that character-based representations are useful for NLP tasks. In this paper, we demonstrate that they are useful for transition-based dependency parsing, since they are capable of capturing morphological information crucial for analyzing syntax. The improvements provided by the characterbased representations using bidirectional LSTMs are strong for agglutinative lang</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008a. Joint word segmentation and POS tagging using a single perceptron. In Proc ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc of EMNLP.</booktitle>
<contexts>
<context position="16149" citStr="Zhang and Clark (2008" startWordPosition="2697" endWordPosition="2700">13) by the best performing system of the SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. UNK w JJ t party w NN t w w t &lt;w&gt; P a r t Y &lt;/w&gt; &lt;/w&gt; &lt;w&gt; a r t Y P NN 352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7. Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 Experimental Configurations In order to isolate the improvements provided by the LSTM encodings of characters, we run the stack LSTM parser in the following configurations: • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in </context>
<context position="25771" citStr="Zhang and Clark, 2008" startWordPosition="4232" endWordPosition="4235"> could be cached. On average, Words parses a sentence in 44 ms, whileChars needs 130 ms.9 Training time is affected by the same cons9We are using a machine with 32 Intel Xeon CPU E5- 2650 at 2.00GHz; the parser runs on a single core. tant, needing some hours to have a competitive model. In terms of memory, Words requires on average 300 MB of main memory for both training and parsing, while Chars requires 450 MB. 4.4.4 Comparison with State-of-the-Art Table 3 shows a comparison with state-of-theart parsers. We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013). For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without exp</context>
<context position="30779" citStr="Zhang and Clark, 2008" startWordPosition="5015" endWordPosition="5018">11). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a). To the best of our knowledge, previous work has not used character-based embeddings to improve dependency parsers, as done in this paper. 6 Conclusion We have presented several interesting findings. First, we add new evidence that character-based representations are useful for NLP tasks. In this paper, we demonstrate that they are useful for transition-based dependency parsing, since they are capable of capturing morphological information crucial for analyzing syntax. The improvements provided by the characterbased representations using bidirectional LSTMs are strong for agglutinative lang</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008b. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proc of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Chinese parsing exploiting characters.</title>
<date>2013</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="30563" citStr="Zhang et al. (2013)" startWordPosition="4980" endWordPosition="4983">characters contain rich information. Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a). To the best of our knowledge, previous work has not used character-based embeddings to improve dependency parsers, as done in this paper. 6 Conclusion We have presented several interesting findings. First, we add new evidence that character-based representations are useful for NLP tasks. In this paper, we demonstrate that they are useful for transition-based dependency parsing,</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2013</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2013. Chinese parsing exploiting characters. In Proc of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhou</author>
<author>Yue Zhang</author>
<author>Shujian Huang</author>
<author>Jiajun Chen</author>
</authors>
<title>A Neural Probabilistic StructuredPrediction Model for Transition-Based Dependency Parsing.</title>
<date>2015</date>
<booktitle>In Proc of ACL.</booktitle>
<contexts>
<context position="1739" citStr="Zhou et al., 2015" startWordPosition="242" endWordPosition="245">gs of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well known that a word’</context>
</contexts>
<marker>Zhou, Zhang, Huang, Chen, 2015</marker>
<rawString>Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun Chen. 2015. A Neural Probabilistic StructuredPrediction Model for Transition-Based Dependency Parsing. In Proc of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
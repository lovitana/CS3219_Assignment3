<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009986">
<title confidence="0.983624">
Syntactic Parse Fusion
</title>
<author confidence="0.979231">
Do Kook Choe David McClosky Eugene Charniak
</author>
<affiliation confidence="0.963439">
Brown University IBM Research Brown University
</affiliation>
<address confidence="0.698219">
Providence, RI Yorktown Heights, NY Providence, RI
</address>
<email confidence="0.977469">
dc65@cs.brown.edu dmcclosky@us.ibm.com ec@cs.brown.edu
</email>
<sectionHeader confidence="0.99347" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919565217391">
Model combination techniques have con-
sistently shown state-of-the-art perfor-
mance across multiple tasks, including
syntactic parsing. However, they dramat-
ically increase runtime and can be diffi-
cult to employ in practice. We demon-
strate that applying constituency model
combination techniques to n-best lists in-
stead of n different parsers results in sig-
nificant parsing accuracy improvements.
Parses are weighted by their probabilities
and combined using an adapted version
of Sagae and Lavie (2006). These accu-
racy gains come with marginal computa-
tional costs and are obtained on top of ex-
isting parsing techniques such as discrim-
inative reranking and self-training, result-
ing in state-of-the-art accuracy: 92.6% on
WSJ section 23. On out-of-domain cor-
pora, accuracy is improved by 0.4% on
average. We empirically confirm that six
well-known n-best parsers benefit from
the proposed methods across six domains.
</bodyText>
<sectionHeader confidence="0.998944" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995173369565217">
Researchers have proposed many algorithms to
combine parses from multiple parsers into one fi-
nal parse (Henderson and Brill, 1999; Zeman and
ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Now-
son and Dale, 2007; Fossum and Knight, 2009;
Petrov, 2010; Johnson and Ural, 2010; Huang et
al., 2010; McDonald and Nivre, 2011; Shindo et
al., 2012; Narayan and Cohen, 2015). These new
parses are substantially better than the originals:
Zhang et al. (2009) combine outputs from mul-
tiple n-best parsers and achieve an F1 of 92.6%
on the WSJ test set, a 0.5% improvement over
their best n-best parser. Model combination ap-
proaches tend to fall into the following categories:
hybridization, where multiple parses are combined
into a single parse; switching, which picks a sin-
gle parse according to some criteria (usually a
form of voting); grammar merging where gram-
mars are combined before or during parsing; and
stacking, where one parser sends its prediction
to another at runtime. All of these have at least
one of the caveats that (1) overall computation
is increased and runtime is determined by the
slowest parser and (2) using multiple parsers in-
creases the system complexity, making it more
difficult to deploy in practice. In this paper, we
describe a simple hybridization extension (“fu-
sion”) which obtains much of hybridization’s ben-
efits while using only a single n-best parser and
minimal extra computation. Our method treats
each parse in a single parser’s n-best list as a
parse from n separate parsers. We then adapt
parse combination methods by Henderson and
Brill (1999), Sagae and Lavie (2006), and Fos-
sum and Knight (2009) to fuse the constituents
from the n parses into a single tree. We empir-
ically show that six n-best parsers benefit from
parse fusion across six domains, obtaining state-
of-the-art results. These improvements are com-
plementary to other techniques such as rerank-
ing and self-training. Our best system obtains
an F1 of 92.6% on WSJ section 23, a score pre-
viously obtained only by combining the outputs
from multiple parsers. A reference implementa-
tion is available as part of BLLIP Parser at http:
//github.com/BLLIP/bllip-parser/
</bodyText>
<sectionHeader confidence="0.99678" genericHeader="introduction">
2 Fusion
</sectionHeader>
<bodyText confidence="0.999899142857143">
Henderson and Brill (1999) propose a method to
combine trees from m parsers in three steps: pop-
ulate a chart with constituents along with the num-
ber of times they appear in the trees; remove any
constituent with count less than m/2 from the
chart; and finally create a final tree with all the
remaining constituents. Intuitively their method
</bodyText>
<page confidence="0.899999">
1360
</page>
<note confidence="0.647363">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1360–1366,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999985692307692">
constructs a tree with constituents from the ma-
jority of the trees, which boosts precision signif-
icantly. Henderson and Brill (1999) show that
this process is guaranteed to produce a valid tree.
Sagae and Lavie (2006) generalize this work by
reparsing the chart populated with constituents
whose counts are above a certain threshold. By
adjusting the threshold on development data, their
generalized method balances precision and recall.
Fossum and Knight (2009) further extend this line
of work by using n-best lists from multiple parsers
and combining productions in addition to con-
stituents. Their model assigns sums of joint proba-
bilities of constituents and parsers to constituents.
Surprisingly, exploiting n-best trees does not lead
to large improvement over combining 1-best trees
in their experiments.
Our extension takes the n-best trees from a
parser as if they are 1-best parses from n parsers,
then follows Sagae and Lavie (2006). Parses
are weighted by the estimated probabilities from
the parser. Given n trees and their weights, the
model computes a constituent’s weight by sum-
ming weights of all trees containing that con-
stituent. Concretely, the weight of a constituent
spanning from ith word to jth word with label E is
</bodyText>
<equation confidence="0.992909666666667">
n
c`(i → j) = W(k)Ck` (i → j) (1)
k=1
</equation>
<bodyText confidence="0.999982785714286">
where W (k) is the weight of kth tree and Ck ` (i →
j) is one if a constituent with label E spanning from
i to j is in kth tree, zero otherwise. After populat-
ing the chart with constituents and their weights,
it throws out constituents with weights below a set
threshold t. Using the threshold t = 0.5 emulates
the method of Henderson and Brill (1999) in that
it constructs the tree with the constituents in the
majority of the trees. The CYK parsing algorithm
is applied to the chart to produce the final tree.
Note that populating the chart is linear in the
number of words and the chart contains substan-
tially fewer constituents than charts in well-known
parsers, making this a fast procedure.
</bodyText>
<subsectionHeader confidence="0.997076">
2.1 Score distribution over trees
</subsectionHeader>
<bodyText confidence="0.999882333333333">
We assume that n-best parsers provide trees along
with some kind of scores (often probabilities or
log probabilities). Given these scores, a natural
way to obtain weights is to normalize the prob-
abilities. However, parsers do not always provide
accurate estimates of parse quality. We may obtain
better performance from parse fusion by altering
this distribution and passing scores through a non-
linear function, f(·). The kth parse is weighted:
</bodyText>
<equation confidence="0.999272">
W k = f (SCORE(k)) (2)
( )
En
i=1 f(SCORE(i))
</equation>
<bodyText confidence="0.9997735">
where SCORE(i) is the score of ith tree.1 We ex-
plore the family of functions f(x) = xβ which can
smooth or sharpen the score distributions. This in-
cludes a tunable parameter, Q E R+0 :
</bodyText>
<equation confidence="0.916619">
SCORE(k)β
W(k) = (3)
i=1 SCORE(i)β
</equation>
<bodyText confidence="0.995870222222222">
Employing Q &lt; 1 flattens the score distribution
over n-best trees and helps over-confident parsers.
On the other hand, having Q &gt; 1 skews the distri-
bution toward parses with higher scores and helps
under-confident parsers. Note that setting Q = 0
weights all parses equally and results in majority
voting at the constituent level. We leave develop-
ing other nonlinear functions for fusion as future
work.
</bodyText>
<sectionHeader confidence="0.999235" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999229666666667">
Corpora: Parse fusion is evaluated on British
National Corpus (BNC), Brown, GENIA, Ques-
tion Bank (QB), Switchboard (SB) and Wall Street
Journal (WSJ) (Foster and van Genabith, 2008;
Francis and Kuˇcera, 1989; Kim et al., 2003;
Judge et al., 2006; Godfrey et al., 1992; Mar-
cus et al., 1993). WSJ is used to evaluate in-
domain parsing, the remaining five are used for
out-of-domain. For divisions, we use tune and test
splits from Bacchiani et al. (2006) for Brown, Mc-
Closky’s test PMIDs2 for GENIA, Stanford’s test
splits3 for QuestionBank, and articles 4000–4153
for Switchboard.
Parsers: The methods are applied to six widely
used n-best parsers: Charniak (2000), Stanford
(Klein and Manning, 2003), BLLIP (Charniak and
Johnson, 2005), Self-trained BLLIP (McClosky et
al., 2006)4, Berkeley (Petrov et al., 2006), and
Stanford RNN (Socher et al., 2013). The list of
parsers and their accuracies on the WSJ test set
is reported in Table 1. We convert to Stanford
</bodyText>
<footnote confidence="0.981197714285714">
1For parsers that return log probabilities, we turn these
into probabilities first.
2http://nlp.stanford.edu/˜mcclosky/
biomedical.html
3http://nlp.stanford.edu/data/
QuestionBank-Stanford.shtml
4Using the ‘WSJ+Gigaword-v2’ BLLIP model.
</footnote>
<page confidence="0.952931">
1361
</page>
<table confidence="0.503106">
Parser F1 UAS LAS
</table>
<tableCaption confidence="0.897033">
Table 1: Six parsers along with their 1-best F1
scores, unlabeled attachment scores (UAS) and la-
beled attachment scores (LAS) on WSJ section 23.
</tableCaption>
<bodyText confidence="0.999097694444445">
Dependencies (basic dependencies, version 3.3.0)
and provide dependency metrics (UAS, LAS) as
well.
Supervised parsers are trained on the WSJ train-
ing set (sections 2–21) and use section 22 or
24 for development. Self-trained BLLIP is self-
trained using two million sentences from Giga-
word and Stanford RNN uses word embeddings
trained from larger corpora.
Parameter tuning: There are three parameters for
our fusion process: the size of the n-best list (2 &lt;
n ≤ 50), the smoothing exponent from Section 2.1
(Q ∈ [0.5,1.5] with 0.1 increments), and the mini-
mum threshold for constituents (t ∈ [0.2, 0.7] with
0.01 increments). We use grid search to tune these
parameters for two separate scenarios. When pars-
ing WSJ (in-domain), we tune parameters on WSJ
section 24. For the remaining corpora (out-of-
domain), we use the tuning section from Brown.
Each parser is tuned separately, resulting in 12
different tuning scenarios. In practice, though,
in-domain and out-of-domain tuning regimes tend
to pick similar settings within a parser. Across
parsers, settings are also fairly similar (n is usu-
ally 30 or 40, t is usually between 0.45 and 0.5).
While the smoothing exponent varies from 0.5 to
1.3, setting Q = 1 does not significantly hurt ac-
curacy for most parsers.
To study the effects of these parameters, Fig-
ure 1 shows three slices of the tuning surface for
BLLIP parser on WSJ section 24 around the op-
timal settings (n = 30,Q = 1.1,t = 0.47). In
each graph, one of the parameters is varied while
the other is held constant. Increasing n-best size
improves accuracy until about n = 30 where there
seems to be sufficient diversity. For BLLIP, the
</bodyText>
<footnote confidence="0.976753666666667">
5Socher et al. (2013) report an Fl of 90.4%, but this is
the result of using an ensemble of two RNNs (p.c.). We use a
single RNN in this work.
</footnote>
<table confidence="0.9995918">
Parser WSJ Brown
BLLIP 90.6 85.7
+ Fusion 91.0 86.0
+ Majority voting (Q = 0) 89.1 83.8
+ Rank-based weighting 89.3 84.1
</table>
<tableCaption confidence="0.894766">
Table 2: F1 of a baseline parser, fusion, and base-
lines on development sections of corpora (WSJ
section 24 and Brown tune).
</tableCaption>
<bodyText confidence="0.999808756097561">
smoothing exponent (Q) is best set around 1.0,
with accuracy falling off if the value deviates too
much. Finally, the threshold parameter is empiri-
cally optimized a little below t = 0.5 (the value
suggested by Henderson and Brill (1999)). Since
score values are normalized, this means that con-
stituents need roughly half the “score mass” in or-
der to be included in the chart. Varying the thresh-
old changes the precision/recall balance since a
high threshold adds only the most confident con-
stituents to the chart (Sagae and Lavie, 2006).
Baselines: Table 2 gives the accuracy of fusion
and baselines for BLLIP on the development cor-
pora. Majority voting sets n = 50, Q = 0, t =
0.5 giving all parses equal weight and results in
constituent-level majority voting. We explore a
rank-based weighting which ignores parse prob-
abilities and weight parses only using the rank:
Wrank(k) = 1/(2k). These show that accu-
rate parse-level scores are critical for good perfor-
mance.
Final evaluation: Table 3 gives our final re-
sults for all parsers across all domains. Results
in blue are significant at p &lt; 0.01 using a ran-
domized permutation test. Fusion generally im-
proves F1 for in-domain and out-of-domain pars-
ing by a significant margin. For the self-trained
BLLIP parser, in-domain F1 increases by 0.4%
and out-of-domain F1 increases by 0.4% on av-
erage. Berkeley parser obtains the smallest gains
from fusion since Berkeley’s n-best lists are or-
dered by factors other than probabilities. As a re-
sult, the probabilities from Berkeley can mislead
the fusion process.
We also compare against model combination
using our reimplementation of Sagae and Lavie
(2006). For these results, all six parsers were given
equal weight. The threshold was set to 0.42 to
optimize model combination F1 on development
data (similar to Setting 2 for constituency parsing
in Sagae and Lavie (2006)). Model combination
</bodyText>
<table confidence="0.998293333333333">
Stanford 85.4 90.0 87.3
Stanford RNN5 89.6 92.9 90.4
Berkeley 90.0 93.5 91.2
Charniak 89.7 93.2 90.8
BLLIP 91.5 94.4 92.0
Self-trained BLLIP 92.2 94.7 92.2
</table>
<page confidence="0.965444">
1362
</page>
<figureCaption confidence="0.852266333333333">
Figure 1: Tuning parameters independently for BLLIP and their impact on F1 for WSJ section 24 (solid
purple line). For each graph, non-tuned parameters were set at the optimal configuration for BLLIP
(n = 30, Q = 1.1, t = 0.47). The dashed grey line represents the 1-best baseline at 90.6% F1.
</figureCaption>
<table confidence="0.999839125">
Parser BNC Brown GENIA SB QB WSJ
Stanford 78.4 / 79.6 80.7 / 81.6 73.1 / 73.9 67.0 / 67.9 78.6 / 80.0 85.4 / 86.2
Stanford RNN 82.0 / 82.3 84.0 / 84.3 76.0 / 76.2 70.7 / 71.2 82.9 / 83.6 89.6 / 89.7
Berkeley 82.3 / 82.9 84.6 / 84.6 76.4 / 76.6 74.5 / 75.1 86.5 / 85.9 90.0 / 90.3
Charniak 82.5 / 83.0 83.9 / 84.6 74.8 / 75.7 76.8 / 77.6 85.6 / 86.3 89.7 / 90.1
BLLIP 84.1 / 84.7 85.8 / 86.0 76.7 / 77.1 79.2 / 79.5 88.1 / 88.9 91.5 / 91.7
Self-trained BLLIP 85.2 / 85.8 87.4 / 87.7 77.8 / 78.2 80.9 / 81.7 89.5 / 89.5 92.2 / 92.6
Model combination 86.6 87.7 79.4 80.9 89.3 92.5
</table>
<tableCaption confidence="0.998804">
Table 3: Evaluation of the constituency fusion method on six parsers across six domains. x/y indicates
</tableCaption>
<bodyText confidence="0.997908">
the F1 from the baseline parser (x) and the baseline parser with fusion (y) respectively. Blue indicates a
statistically significant difference between fusion and its baseline parser (p &lt; 0.01).
performs better than fusion on BNC and GENIA,
but surprisingly fusion outperforms model com-
bination on three of the six domains (not usually
not by a significant margin). With further tuning
(e.g., specific weights for each constituent-parser
pair), the benefits from model combination should
increase.
Multilingual evaluation: We evaluate fusion with
the Berkeley parser on Arabic (Maamouri et al.,
2004; Green and Manning, 2010), French (Abeill´e
et al., 2003), and German (Brants et al., 2002)
from the SPMRL 2014 shared task (Seddah et al.,
2014) but did not observe any improvement. We
suspect this has to do with the same ranking issues
seen in the Berkeley Parser’s English results. On
the other hand, fusion helps the parser of Narayan
and Cohen (2015) on the German NEGRA tree-
bank (Skut et al., 1997) to improve from 80.9% to
82.4%.
Runtime: As discussed in Section 2, fusion’s run-
time overhead is minimal. Reranking parsers (e.g.,
BLLIP and Stanford RNN) already need to per-
form n-best decoding as input for the reranker.
Using a somewhat optimized implementation fu-
sion in C++, the overhead over BLLIP parser is
less than 1%.
Discussion: Why does fusion help? It is possible
that a parser’s n-list and its scores act as a weak
approximation to the full parse forest. As a result,
fusion seems to provide part of the benefits seen in
forest reranking (Huang, 2008).
Results from Fossum and Knight (2009) imply
that fusion and model combination might not be
complementary. Both n-best lists and additional
parsers provide syntactic diversity. While addi-
tional parsers provide greater diversity, n-best lists
from common parsers are varied enough to pro-
vide improvements for parse hybridization.
We analyzed how often fusion produces com-
pletely novel trees. For BLLIP on WSJ section
24, this only happens about 11% of the time.
Fusion picks the 1-best tree 72% of the time.
This means that for the remaining 17%, fusion
picks an existing parse from the rest of the n-
list, acting similar to a reranker. When fusion
creates unique trees, they are significantly better
than the original 1-best trees (for the 11% sub-
set of WSJ 24, F1 scores are 85.5% with fusion
and 84.1% without, p &lt; 0.003). This contrasts
with McClosky et al. (2012) where novel predic-
</bodyText>
<page confidence="0.939443">
1363
</page>
<bodyText confidence="0.999890925">
tions from model combination (stacking) were
worse than baseline performance. The difference
is that novel predictions with fusion better incor-
porate model confidence whereas when stacking,
a novel prediction is less trusted than those pro-
duced by one or both of the base parsers.
Preliminary extensions: Here, we summarize
two extensions to fusion which have yet to show
benefits. The first extension explores applying
fusion to dependency parsing. We explored two
ways to apply fusion when starting from con-
stituency parses: (1) fuse constituents and then
convert them to dependencies and (2) convert to
dependencies then fuse the dependencies as in
Sagae and Lavie (2006). Approach (1) does not
provide any benefit (LAS drops between 0.5% and
2.4%). This may result from fusion’s artifacts in-
cluding unusual unary chains or nodes with a large
number of children — it is possible that adjusting
unary handling and the precision/recall tradeoff
may reduce these issues. Approach (2) provided
only modest benefits compared to those from con-
stituency parsing fusion. The largest LAS increase
for (2) is 0.6% for the Stanford Parser, though for
Berkeley and Self-trained BLLIP, dependency fu-
sion results in small losses (-0.1% LAS). Two pos-
sible reasons are that the dependency baseline is
higher than its constituency counterpart and some
dependency graphs from the n-best list are dupli-
cates which lowers diversity and may need special
handling, but this remains an open question.
While fusion helps on top of a self-trained
parser, we also explored whether a fused parser
can self-train (McClosky et al., 2006). To test this,
we (1) parsed two million sentences with BLLIP
(trained on WSJ), (2) fused those parses, (3) added
the fused parses to the gold training set, and (4)
retrained the parser on the expanded training. The
resulting model did not perform better than a self-
trained parsing model that didn’t use fusion.
</bodyText>
<sectionHeader confidence="0.997111" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999985">
We presented a simple extension to parse hy-
bridization which adapts model combination tech-
niques to operate over a single parser’s n-best list
instead of across multiple parsers. By weight-
ing each parse by its probability from the n-best
parser, we are able to better capture the confidence
at the constituent level. Our best configuration ob-
tains state-of-the-art accuracy on WSJ with an F1
of 92.6%. This is similar to the accuracy obtained
from actual model combination techniques but at
a fraction of the computational cost. Additionally,
improvements are not limited to a single parser or
domain. Fusion improves parser accuracy for six
n-best parsers both in-domain and out-of-domain.
Future work includes applying fusion to n-best
dependency parsers and additional (parser, lan-
guage) pairs. We also intend to explore how to bet-
ter apply fusion to converted dependencies from
constituency parsers. Lastly, it would be interest-
ing to adapt fusion to other structured prediction
tasks where n-best lists are available.
</bodyText>
<sectionHeader confidence="0.969306" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999958285714286">
We are grateful to Shay Cohen and Shashi
Narayan who gave us early access to their parser’s
German results. We would also like to thank Mo-
hit Bansal, Yoav Goldberg, Siddharth Patward-
han, and Kapil Thadani for helpful discussions and
our anonymous reviewers for their insightful com-
ments and suggestions.
</bodyText>
<sectionHeader confidence="0.990021" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.858270533333333">
Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel.
2003. Building a treebank for french. In Treebanks,
pages 165–187. Springer.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. Map adaptation of stochastic
grammars. Computer speech &amp; language, 20(1):41–
68.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In Proceedings of the workshop on tree-
banks and linguistic theories, volume 168.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL’05), pages 173–180, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132–139. Asso-
ciation for Computational Linguistics.
Victoria Fossum and Kevin Knight. 2009. Combining
constituent parsers. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 253–256, Boulder, Colorado,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.986337">
1364
</page>
<reference confidence="0.998361696428572">
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the BNC: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation (LREC’08). European Language Re-
sources Association (ELRA).
Winthrop Nelson Francis and Henry Kuˇcera. 1989.
Manual of information to accompany a standard
corpus of present-day edited American English, for
use with digital computers. Brown University, De-
partment of Linguistics.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517–520. IEEE.
Spence Green and Christopher D Manning. 2010. Bet-
ter arabic parsing: Baselines, evaluations, and anal-
ysis. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 394–
402. Association for Computational Linguistics.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combin-
ing parsers. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages
187–194, College Park, MD, June. Association for
Computational Linguistics.
Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent vari-
able grammars. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 12–22. Association for Computa-
tional Linguistics.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586–594, Columbus, Ohio,
June. Association for Computational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the berkeley and brown parsers. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 665–668. As-
sociation for Computational Linguistics.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. Questionbank: Creating a corpus of parse-
annotated questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 497–504, Sydney,
Australia, July. Association for Computational Lin-
guistics.
J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-
jii. 2003. Genia corpus—a semantically annotated
corpus for bio-textmining. Bioinformatics, 19(suppl
1):i180–i182.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423–430, Sapporo,
Japan, July. Association for Computational Linguis-
tics.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The penn arabic treebank:
Building a large-scale annotated arabic corpus. In
NEMLAR conference on Arabic language resources
and tools, pages 102–109.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152–159, New York City, USA, June. Association
for Computational Linguistics.
David McClosky, Sebastian Riedel, Mihai Surdeanu,
Andrew McCallum, and Christopher D. Manning.
2012. Combining joint models for biomedical event
extraction. BMCBioinformatics, 13(Suppl 11):S9.
Ryan McDonald and Joakim Nivre. 2011. Analyzing
and integrating dependency parsers. Computational
Linguistics, 37(1):197–230.
Shashi Narayan and Shay B. Cohen. 2015. Diversity
in spectral learning for natural language parsing. In
Proceedings of EMNLP.
Scott Nowson and Robert Dale. 2007. Chart-
ing democracy across parsers. In Proceedings of
the Australasian Language Technology Workshop,
pages 75–82.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Slav Petrov. 2010. Products of random latent vari-
able grammars. In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 19–27. Association for Computa-
tional Linguistics.
Kenji Sagae and Alon Lavie. 2006. Parser combi-
nation by reparsing. In Proceedings of the Human
Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 129–132,
New York City, USA, June. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.815956">
1365
</page>
<reference confidence="0.999820538461539">
Djam´e Seddah, Sandra K¨ubler, and Reut Tsarfaty.
2014. Introducing the spmrl 2014 shared task on
parsing morphologically-rich languages. In Pro-
ceedings of the First Joint Workshop on Statisti-
cal Parsing of Morphologically Rich Languages and
Syntactic Analysis of Non-Canonical Languages,
pages 103–109, Dublin, Ireland, August. Dublin
City University.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 440–448. Association for
Computational Linguistics.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings of the
fifth conference on Applied natural language pro-
cessing, pages 88–95. Association for Computa-
tional Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
455–465, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Daniel Zeman and Zdenˇek ˇZabokrtsk`y. 2005. Improv-
ing parsing accuracy by combining diverse depen-
dency parsers. In Proceedings of the Ninth Interna-
tional Workshop on Parsing Technology, pages 171–
178. Association for Computational Linguistics.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1552–1560, Singapore, August. Association
for Computational Linguistics.
</reference>
<page confidence="0.992826">
1366
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.834297">
<title confidence="0.999931">Syntactic Parse Fusion</title>
<author confidence="0.999204">Do Kook Choe David McClosky Eugene Charniak</author>
<affiliation confidence="0.9256285">Brown University IBM Research Brown University Providence, RI Yorktown Heights, NY Providence,</affiliation>
<email confidence="0.996069">dc65@cs.brown.edudmcclosky@us.ibm.comec@cs.brown.edu</email>
<abstract confidence="0.999356208333333">Model combination techniques have consistently shown state-of-the-art performance across multiple tasks, including syntactic parsing. However, they dramatically increase runtime and can be difficult to employ in practice. We demonstrate that applying constituency model techniques to lists inof parsers results in significant parsing accuracy improvements. Parses are weighted by their probabilities and combined using an adapted version of Sagae and Lavie (2006). These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six parsers benefit from the proposed methods across six domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Lionel Cl´ement</author>
<author>Franc¸ois Toussenel</author>
</authors>
<title>Building a treebank for french. In Treebanks,</title>
<date>2003</date>
<pages>165--187</pages>
<publisher>Springer.</publisher>
<marker>Abeill´e, Cl´ement, Toussenel, 2003</marker>
<rawString>Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a treebank for french. In Treebanks, pages 165–187. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>Map adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer speech &amp; language,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>68</pages>
<contexts>
<context position="7452" citStr="Bacchiani et al. (2006)" startWordPosition="1210" endWordPosition="1213"> weights all parses equally and results in majority voting at the constituent level. We leave developing other nonlinear functions for fusion as future work. 3 Experiments Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1For parsers that return log probabilities, we turn these into probabilities first. 2http:</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. Map adaptation of stochastic grammars. Computer speech &amp; language, 20(1):41– 68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The tiger treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the workshop on treebanks and linguistic theories,</booktitle>
<volume>168</volume>
<contexts>
<context position="14120" citStr="Brants et al., 2002" startWordPosition="2339" endWordPosition="2342"> (y) respectively. Blue indicates a statistically significant difference between fusion and its baseline parser (p &lt; 0.01). performs better than fusion on BNC and GENIA, but surprisingly fusion outperforms model combination on three of the six domains (not usually not by a significant margin). With further tuning (e.g., specific weights for each constituent-parser pair), the benefits from model combination should increase. Multilingual evaluation: We evaluate fusion with the Berkeley parser on Arabic (Maamouri et al., 2004; Green and Manning, 2010), French (Abeill´e et al., 2003), and German (Brants et al., 2002) from the SPMRL 2014 shared task (Seddah et al., 2014) but did not observe any improvement. We suspect this has to do with the same ranking issues seen in the Berkeley Parser’s English results. On the other hand, fusion helps the parser of Narayan and Cohen (2015) on the German NEGRA treebank (Skut et al., 1997) to improve from 80.9% to 82.4%. Runtime: As discussed in Section 2, fusion’s runtime overhead is minimal. Reranking parsers (e.g., BLLIP and Stanford RNN) already need to perform n-best decoding as input for the reranker. Using a somewhat optimized implementation fusion in C++, the ove</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The tiger treebank. In Proceedings of the workshop on treebanks and linguistic theories, volume 168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="7735" citStr="Charniak and Johnson, 2005" startWordPosition="1251" endWordPosition="1254">chboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1For parsers that return log probabilities, we turn these into probabilities first. 2http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser F1 UAS LAS Table 1: Six parsers along with their 1-best F1 scores, unlabeled attachment scores (UAS) and labeled attachment </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 173–180, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7663" citStr="Charniak (2000)" startWordPosition="1243" endWordPosition="1244">ational Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1For parsers that return log probabilities, we turn these into probabilities first. 2http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser F1 UAS LAS Table 1: Six parsers along with their 1-b</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
</authors>
<title>Combining constituent parsers.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>253--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1387" citStr="Fossum and Knight, 2009" startWordPosition="199" endWordPosition="202">rginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); gramm</context>
<context position="2785" citStr="Fossum and Knight (2009)" startWordPosition="429" endWordPosition="433">the caveats that (1) overall computation is increased and runtime is determined by the slowest parser and (2) using multiple parsers increases the system complexity, making it more difficult to deploy in practice. In this paper, we describe a simple hybridization extension (“fusion”) which obtains much of hybridization’s benefits while using only a single n-best parser and minimal extra computation. Our method treats each parse in a single parser’s n-best list as a parse from n separate parsers. We then adapt parse combination methods by Henderson and Brill (1999), Sagae and Lavie (2006), and Fossum and Knight (2009) to fuse the constituents from the n parses into a single tree. We empirically show that six n-best parsers benefit from parse fusion across six domains, obtaining stateof-the-art results. These improvements are complementary to other techniques such as reranking and self-training. Our best system obtains an F1 of 92.6% on WSJ section 23, a score previously obtained only by combining the outputs from multiple parsers. A reference implementation is available as part of BLLIP Parser at http: //github.com/BLLIP/bllip-parser/ 2 Fusion Henderson and Brill (1999) propose a method to combine trees fr</context>
<context position="4326" citStr="Fossum and Knight (2009)" startWordPosition="671" endWordPosition="674">e on Empirical Methods in Natural Language Processing, pages 1360–1366, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. constructs a tree with constituents from the majority of the trees, which boosts precision significantly. Henderson and Brill (1999) show that this process is guaranteed to produce a valid tree. Sagae and Lavie (2006) generalize this work by reparsing the chart populated with constituents whose counts are above a certain threshold. By adjusting the threshold on development data, their generalized method balances precision and recall. Fossum and Knight (2009) further extend this line of work by using n-best lists from multiple parsers and combining productions in addition to constituents. Their model assigns sums of joint probabilities of constituents and parsers to constituents. Surprisingly, exploiting n-best trees does not lead to large improvement over combining 1-best trees in their experiments. Our extension takes the n-best trees from a parser as if they are 1-best parses from n parsers, then follows Sagae and Lavie (2006). Parses are weighted by the estimated probabilities from the parser. Given n trees and their weights, the model compute</context>
<context position="15037" citStr="Fossum and Knight (2009)" startWordPosition="2499" endWordPosition="2502"> (Skut et al., 1997) to improve from 80.9% to 82.4%. Runtime: As discussed in Section 2, fusion’s runtime overhead is minimal. Reranking parsers (e.g., BLLIP and Stanford RNN) already need to perform n-best decoding as input for the reranker. Using a somewhat optimized implementation fusion in C++, the overhead over BLLIP parser is less than 1%. Discussion: Why does fusion help? It is possible that a parser’s n-list and its scores act as a weak approximation to the full parse forest. As a result, fusion seems to provide part of the benefits seen in forest reranking (Huang, 2008). Results from Fossum and Knight (2009) imply that fusion and model combination might not be complementary. Both n-best lists and additional parsers provide syntactic diversity. While additional parsers provide greater diversity, n-best lists from common parsers are varied enough to provide improvements for parse hybridization. We analyzed how often fusion produces completely novel trees. For BLLIP on WSJ section 24, this only happens about 11% of the time. Fusion picks the 1-best tree 72% of the time. This means that for the remaining 17%, fusion picks an existing parse from the rest of the nlist, acting similar to a reranker. Whe</context>
</contexts>
<marker>Fossum, Knight, 2009</marker>
<rawString>Victoria Fossum and Kevin Knight. 2009. Combining constituent parsers. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 253–256, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Parser evaluation and the BNC: Evaluating 4 constituency parsers with 3 metrics.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08). European Language Resources Association (ELRA).</booktitle>
<marker>Foster, van Genabith, 2008</marker>
<rawString>Jennifer Foster and Josef van Genabith. 2008. Parser evaluation and the BNC: Evaluating 4 constituency parsers with 3 metrics. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08). European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winthrop Nelson Francis</author>
<author>Henry Kuˇcera</author>
</authors>
<title>Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers.</title>
<date>1989</date>
<institution>Brown University, Department of Linguistics.</institution>
<marker>Francis, Kuˇcera, 1989</marker>
<rawString>Winthrop Nelson Francis and Henry Kuˇcera. 1989. Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers. Brown University, Department of Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>517--520</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7268" citStr="Godfrey et al., 1992" startWordPosition="1177" endWordPosition="1180"> and helps over-confident parsers. On the other hand, having Q &gt; 1 skews the distribution toward parses with higher scores and helps under-confident parsers. Note that setting Q = 0 weights all parses equally and results in majority voting at the constituent level. We leave developing other nonlinear functions for fusion as future work. 3 Experiments Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of pa</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on, volume 1, pages 517–520. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Christopher D Manning</author>
</authors>
<title>Better arabic parsing: Baselines, evaluations, and analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>394--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14054" citStr="Green and Manning, 2010" startWordPosition="2328" endWordPosition="2331">he F1 from the baseline parser (x) and the baseline parser with fusion (y) respectively. Blue indicates a statistically significant difference between fusion and its baseline parser (p &lt; 0.01). performs better than fusion on BNC and GENIA, but surprisingly fusion outperforms model combination on three of the six domains (not usually not by a significant margin). With further tuning (e.g., specific weights for each constituent-parser pair), the benefits from model combination should increase. Multilingual evaluation: We evaluate fusion with the Berkeley parser on Arabic (Maamouri et al., 2004; Green and Manning, 2010), French (Abeill´e et al., 2003), and German (Brants et al., 2002) from the SPMRL 2014 shared task (Seddah et al., 2014) but did not observe any improvement. We suspect this has to do with the same ranking issues seen in the Berkeley Parser’s English results. On the other hand, fusion helps the parser of Narayan and Cohen (2015) on the German NEGRA treebank (Skut et al., 1997) to improve from 80.9% to 82.4%. Runtime: As discussed in Section 2, fusion’s runtime overhead is minimal. Reranking parsers (e.g., BLLIP and Stanford RNN) already need to perform n-best decoding as input for the reranker</context>
</contexts>
<marker>Green, Manning, 2010</marker>
<rawString>Spence Green and Christopher D Manning. 2010. Better arabic parsing: Baselines, evaluations, and analysis. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 394– 402. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Henderson</author>
<author>Eric Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>187--194</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>College Park, MD,</location>
<contexts>
<context position="1286" citStr="Henderson and Brill, 1999" startWordPosition="182" endWordPosition="185">ties and combined using an adapted version of Sagae and Lavie (2006). These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single pars</context>
<context position="2731" citStr="Henderson and Brill (1999)" startWordPosition="420" endWordPosition="423">o another at runtime. All of these have at least one of the caveats that (1) overall computation is increased and runtime is determined by the slowest parser and (2) using multiple parsers increases the system complexity, making it more difficult to deploy in practice. In this paper, we describe a simple hybridization extension (“fusion”) which obtains much of hybridization’s benefits while using only a single n-best parser and minimal extra computation. Our method treats each parse in a single parser’s n-best list as a parse from n separate parsers. We then adapt parse combination methods by Henderson and Brill (1999), Sagae and Lavie (2006), and Fossum and Knight (2009) to fuse the constituents from the n parses into a single tree. We empirically show that six n-best parsers benefit from parse fusion across six domains, obtaining stateof-the-art results. These improvements are complementary to other techniques such as reranking and self-training. Our best system obtains an F1 of 92.6% on WSJ section 23, a score previously obtained only by combining the outputs from multiple parsers. A reference implementation is available as part of BLLIP Parser at http: //github.com/BLLIP/bllip-parser/ 2 Fusion Henderson</context>
<context position="3996" citStr="Henderson and Brill (1999)" startWordPosition="621" endWordPosition="624">ombine trees from m parsers in three steps: populate a chart with constituents along with the number of times they appear in the trees; remove any constituent with count less than m/2 from the chart; and finally create a final tree with all the remaining constituents. Intuitively their method 1360 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1360–1366, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. constructs a tree with constituents from the majority of the trees, which boosts precision significantly. Henderson and Brill (1999) show that this process is guaranteed to produce a valid tree. Sagae and Lavie (2006) generalize this work by reparsing the chart populated with constituents whose counts are above a certain threshold. By adjusting the threshold on development data, their generalized method balances precision and recall. Fossum and Knight (2009) further extend this line of work by using n-best lists from multiple parsers and combining productions in addition to constituents. Their model assigns sums of joint probabilities of constituents and parsers to constituents. Surprisingly, exploiting n-best trees does n</context>
<context position="5492" citStr="Henderson and Brill (1999)" startWordPosition="877" endWordPosition="880">e parser. Given n trees and their weights, the model computes a constituent’s weight by summing weights of all trees containing that constituent. Concretely, the weight of a constituent spanning from ith word to jth word with label E is n c`(i → j) = W(k)Ck` (i → j) (1) k=1 where W (k) is the weight of kth tree and Ck ` (i → j) is one if a constituent with label E spanning from i to j is in kth tree, zero otherwise. After populating the chart with constituents and their weights, it throws out constituents with weights below a set threshold t. Using the threshold t = 0.5 emulates the method of Henderson and Brill (1999) in that it constructs the tree with the constituents in the majority of the trees. The CYK parsing algorithm is applied to the chart to produce the final tree. Note that populating the chart is linear in the number of words and the chart contains substantially fewer constituents than charts in well-known parsers, making this a fast procedure. 2.1 Score distribution over trees We assume that n-best parsers provide trees along with some kind of scores (often probabilities or log probabilities). Given these scores, a natural way to obtain weights is to normalize the probabilities. However, parse</context>
<context position="10640" citStr="Henderson and Brill (1999)" startWordPosition="1735" endWordPosition="1738">e 5Socher et al. (2013) report an Fl of 90.4%, but this is the result of using an ensemble of two RNNs (p.c.). We use a single RNN in this work. Parser WSJ Brown BLLIP 90.6 85.7 + Fusion 91.0 86.0 + Majority voting (Q = 0) 89.1 83.8 + Rank-based weighting 89.3 84.1 Table 2: F1 of a baseline parser, fusion, and baselines on development sections of corpora (WSJ section 24 and Brown tune). smoothing exponent (Q) is best set around 1.0, with accuracy falling off if the value deviates too much. Finally, the threshold parameter is empirically optimized a little below t = 0.5 (the value suggested by Henderson and Brill (1999)). Since score values are normalized, this means that constituents need roughly half the “score mass” in order to be included in the chart. Varying the threshold changes the precision/recall balance since a high threshold adds only the most confident constituents to the chart (Sagae and Lavie, 2006). Baselines: Table 2 gives the accuracy of fusion and baselines for BLLIP on the development corpora. Majority voting sets n = 50, Q = 0, t = 0.5 giving all parses equal weight and results in constituent-level majority voting. We explore a rank-based weighting which ignores parse probabilities and w</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>John C. Henderson and Eric Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 187–194, College Park, MD, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Slav Petrov</author>
</authors>
<title>Self-training with products of latent variable grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>12--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1445" citStr="Huang et al., 2010" startWordPosition="209" endWordPosition="212">parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging where grammars are combined before or during pa</context>
</contexts>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with products of latent variable grammars. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 12–22. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>586--594</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="14998" citStr="Huang, 2008" startWordPosition="2495" endWordPosition="2496">n the German NEGRA treebank (Skut et al., 1997) to improve from 80.9% to 82.4%. Runtime: As discussed in Section 2, fusion’s runtime overhead is minimal. Reranking parsers (e.g., BLLIP and Stanford RNN) already need to perform n-best decoding as input for the reranker. Using a somewhat optimized implementation fusion in C++, the overhead over BLLIP parser is less than 1%. Discussion: Why does fusion help? It is possible that a parser’s n-list and its scores act as a weak approximation to the full parse forest. As a result, fusion seems to provide part of the benefits seen in forest reranking (Huang, 2008). Results from Fossum and Knight (2009) imply that fusion and model combination might not be complementary. Both n-best lists and additional parsers provide syntactic diversity. While additional parsers provide greater diversity, n-best lists from common parsers are varied enough to provide improvements for parse hybridization. We analyzed how often fusion produces completely novel trees. For BLLIP on WSJ section 24, this only happens about 11% of the time. Fusion picks the 1-best tree 72% of the time. This means that for the remaining 17%, fusion picks an existing parse from the rest of the n</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL-08: HLT, pages 586–594, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Ahmet Engin Ural</author>
</authors>
<title>Reranking the berkeley and brown parsers.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>665--668</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1425" citStr="Johnson and Ural, 2010" startWordPosition="205" endWordPosition="208">ined on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging where grammars are combined</context>
</contexts>
<marker>Johnson, Ural, 2010</marker>
<rawString>Mark Johnson and Ahmet Engin Ural. 2010. Reranking the berkeley and brown parsers. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 665–668. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Judge</author>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Questionbank: Creating a corpus of parseannotated questions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>497--504</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<marker>Judge, Cahill, van Genabith, 2006</marker>
<rawString>John Judge, Aoife Cahill, and Josef van Genabith. 2006. Questionbank: Creating a corpus of parseannotated questions. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 497–504, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Junichi Tsujii</author>
</authors>
<title>Genia corpus—a semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>1--180</pages>
<contexts>
<context position="7226" citStr="Kim et al., 2003" startWordPosition="1169" endWordPosition="1172">e score distribution over n-best trees and helps over-confident parsers. On the other hand, having Q &gt; 1 skews the distribution toward parses with higher scores and helps under-confident parsers. Note that setting Q = 0 weights all parses equally and results in majority voting at the constituent level. We leave developing other nonlinear functions for fusion as future work. 3 Experiments Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsujii. 2003. Genia corpus—a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="7699" citStr="Klein and Manning, 2003" startWordPosition="1246" endWordPosition="1249">, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1For parsers that return log probabilities, we turn these into probabilities first. 2http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser F1 UAS LAS Table 1: Six parsers along with their 1-best F1 scores, unlabeled attachment </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Wigdan Mekki</author>
</authors>
<title>The penn arabic treebank: Building a large-scale annotated arabic corpus.</title>
<date>2004</date>
<booktitle>In NEMLAR conference on Arabic language resources and tools,</booktitle>
<pages>102--109</pages>
<contexts>
<context position="14028" citStr="Maamouri et al., 2004" startWordPosition="2324" endWordPosition="2327">omains. x/y indicates the F1 from the baseline parser (x) and the baseline parser with fusion (y) respectively. Blue indicates a statistically significant difference between fusion and its baseline parser (p &lt; 0.01). performs better than fusion on BNC and GENIA, but surprisingly fusion outperforms model combination on three of the six domains (not usually not by a significant margin). With further tuning (e.g., specific weights for each constituent-parser pair), the benefits from model combination should increase. Multilingual evaluation: We evaluate fusion with the Berkeley parser on Arabic (Maamouri et al., 2004; Green and Manning, 2010), French (Abeill´e et al., 2003), and German (Brants et al., 2002) from the SPMRL 2014 shared task (Seddah et al., 2014) but did not observe any improvement. We suspect this has to do with the same ranking issues seen in the Berkeley Parser’s English results. On the other hand, fusion helps the parser of Narayan and Cohen (2015) on the German NEGRA treebank (Skut et al., 1997) to improve from 80.9% to 82.4%. Runtime: As discussed in Section 2, fusion’s runtime overhead is minimal. Reranking parsers (e.g., BLLIP and Stanford RNN) already need to perform n-best decoding</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The penn arabic treebank: Building a large-scale annotated arabic corpus. In NEMLAR conference on Arabic language resources and tools, pages 102–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="7290" citStr="Marcus et al., 1993" startWordPosition="1181" endWordPosition="1185">ent parsers. On the other hand, having Q &gt; 1 skews the distribution toward parses with higher scores and helps under-confident parsers. Note that setting Q = 0 weights all parses equally and results in majority voting at the constituent level. We leave developing other nonlinear functions for fusion as future work. 3 Experiments Corpora: Parse fusion is evaluated on British National Corpus (BNC), Brown, GENIA, Question Bank (QB), Switchboard (SB) and Wall Street Journal (WSJ) (Foster and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accura</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="7779" citStr="McClosky et al., 2006" startWordPosition="1257" endWordPosition="1260">r and van Genabith, 2008; Francis and Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1For parsers that return log probabilities, we turn these into probabilities first. 2http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser F1 UAS LAS Table 1: Six parsers along with their 1-best F1 scores, unlabeled attachment scores (UAS) and labeled attachment scores (LAS) on WSJ section 23. Dependencies</context>
<context position="17500" citStr="McClosky et al., 2006" startWordPosition="2898" endWordPosition="2901">ded only modest benefits compared to those from constituency parsing fusion. The largest LAS increase for (2) is 0.6% for the Stanford Parser, though for Berkeley and Self-trained BLLIP, dependency fusion results in small losses (-0.1% LAS). Two possible reasons are that the dependency baseline is higher than its constituency counterpart and some dependency graphs from the n-best list are duplicates which lowers diversity and may need special handling, but this remains an open question. While fusion helps on top of a self-trained parser, we also explored whether a fused parser can self-train (McClosky et al., 2006). To test this, we (1) parsed two million sentences with BLLIP (trained on WSJ), (2) fused those parses, (3) added the fused parses to the gold training set, and (4) retrained the parser on the expanded training. The resulting model did not perform better than a selftrained parsing model that didn’t use fusion. 4 Conclusions We presented a simple extension to parse hybridization which adapts model combination techniques to operate over a single parser’s n-best list instead of across multiple parsers. By weighting each parse by its probability from the n-best parser, we are able to better captu</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Sebastian Riedel</author>
<author>Mihai Surdeanu</author>
<author>Andrew McCallum</author>
<author>Christopher D Manning</author>
</authors>
<title>Combining joint models for biomedical event extraction. BMCBioinformatics, 13(Suppl 11):S9.</title>
<date>2012</date>
<contexts>
<context position="15865" citStr="McClosky et al. (2012)" startWordPosition="2638" endWordPosition="2641">om common parsers are varied enough to provide improvements for parse hybridization. We analyzed how often fusion produces completely novel trees. For BLLIP on WSJ section 24, this only happens about 11% of the time. Fusion picks the 1-best tree 72% of the time. This means that for the remaining 17%, fusion picks an existing parse from the rest of the nlist, acting similar to a reranker. When fusion creates unique trees, they are significantly better than the original 1-best trees (for the 11% subset of WSJ 24, F1 scores are 85.5% with fusion and 84.1% without, p &lt; 0.003). This contrasts with McClosky et al. (2012) where novel predic1363 tions from model combination (stacking) were worse than baseline performance. The difference is that novel predictions with fusion better incorporate model confidence whereas when stacking, a novel prediction is less trusted than those produced by one or both of the base parsers. Preliminary extensions: Here, we summarize two extensions to fusion which have yet to show benefits. The first extension explores applying fusion to dependency parsing. We explored two ways to apply fusion when starting from constituency parses: (1) fuse constituents and then convert them to de</context>
</contexts>
<marker>McClosky, Riedel, Surdeanu, McCallum, Manning, 2012</marker>
<rawString>David McClosky, Sebastian Riedel, Mihai Surdeanu, Andrew McCallum, and Christopher D. Manning. 2012. Combining joint models for biomedical event extraction. BMCBioinformatics, 13(Suppl 11):S9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Analyzing and integrating dependency parsers.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1471" citStr="McDonald and Nivre, 2011" startWordPosition="213" endWordPosition="216">uch as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging where grammars are combined before or during parsing; and stacking, where</context>
</contexts>
<marker>McDonald, Nivre, 2011</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2011. Analyzing and integrating dependency parsers. Computational Linguistics, 37(1):197–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shashi Narayan</author>
<author>Shay B Cohen</author>
</authors>
<title>Diversity in spectral learning for natural language parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1518" citStr="Narayan and Cohen, 2015" startWordPosition="221" endWordPosition="224">ng, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging where grammars are combined before or during parsing; and stacking, where one parser sends its prediction to another at </context>
<context position="14384" citStr="Narayan and Cohen (2015)" startWordPosition="2386" endWordPosition="2389">ally not by a significant margin). With further tuning (e.g., specific weights for each constituent-parser pair), the benefits from model combination should increase. Multilingual evaluation: We evaluate fusion with the Berkeley parser on Arabic (Maamouri et al., 2004; Green and Manning, 2010), French (Abeill´e et al., 2003), and German (Brants et al., 2002) from the SPMRL 2014 shared task (Seddah et al., 2014) but did not observe any improvement. We suspect this has to do with the same ranking issues seen in the Berkeley Parser’s English results. On the other hand, fusion helps the parser of Narayan and Cohen (2015) on the German NEGRA treebank (Skut et al., 1997) to improve from 80.9% to 82.4%. Runtime: As discussed in Section 2, fusion’s runtime overhead is minimal. Reranking parsers (e.g., BLLIP and Stanford RNN) already need to perform n-best decoding as input for the reranker. Using a somewhat optimized implementation fusion in C++, the overhead over BLLIP parser is less than 1%. Discussion: Why does fusion help? It is possible that a parser’s n-list and its scores act as a weak approximation to the full parse forest. As a result, fusion seems to provide part of the benefits seen in forest reranking</context>
</contexts>
<marker>Narayan, Cohen, 2015</marker>
<rawString>Shashi Narayan and Shay B. Cohen. 2015. Diversity in spectral learning for natural language parsing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Nowson</author>
<author>Robert Dale</author>
</authors>
<title>Charting democracy across parsers.</title>
<date>2007</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="1362" citStr="Nowson and Dale, 2007" startWordPosition="194" endWordPosition="198">racy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually</context>
</contexts>
<marker>Nowson, Dale, 2007</marker>
<rawString>Scott Nowson and Robert Dale. 2007. Charting democracy across parsers. In Proceedings of the Australasian Language Technology Workshop, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="7812" citStr="Petrov et al., 2006" startWordPosition="1262" endWordPosition="1265">nd Kuˇcera, 1989; Kim et al., 2003; Judge et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1For parsers that return log probabilities, we turn these into probabilities first. 2http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser F1 UAS LAS Table 1: Six parsers along with their 1-best F1 scores, unlabeled attachment scores (UAS) and labeled attachment scores (LAS) on WSJ section 23. Dependencies (basic dependencies, version 3.3</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of random latent variable grammars.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1401" citStr="Petrov, 2010" startWordPosition="203" endWordPosition="204">s and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging whe</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of random latent variable grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>129--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="729" citStr="Sagae and Lavie (2006)" startWordPosition="96" endWordPosition="99">ersity Providence, RI Yorktown Heights, NY Providence, RI dc65@cs.brown.edu dmcclosky@us.ibm.com ec@cs.brown.edu Abstract Model combination techniques have consistently shown state-of-the-art performance across multiple tasks, including syntactic parsing. However, they dramatically increase runtime and can be difficult to employ in practice. We demonstrate that applying constituency model combination techniques to n-best lists instead of n different parsers results in significant parsing accuracy improvements. Parses are weighted by their probabilities and combined using an adapted version of Sagae and Lavie (2006). These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and L</context>
<context position="2755" citStr="Sagae and Lavie (2006)" startWordPosition="424" endWordPosition="427"> these have at least one of the caveats that (1) overall computation is increased and runtime is determined by the slowest parser and (2) using multiple parsers increases the system complexity, making it more difficult to deploy in practice. In this paper, we describe a simple hybridization extension (“fusion”) which obtains much of hybridization’s benefits while using only a single n-best parser and minimal extra computation. Our method treats each parse in a single parser’s n-best list as a parse from n separate parsers. We then adapt parse combination methods by Henderson and Brill (1999), Sagae and Lavie (2006), and Fossum and Knight (2009) to fuse the constituents from the n parses into a single tree. We empirically show that six n-best parsers benefit from parse fusion across six domains, obtaining stateof-the-art results. These improvements are complementary to other techniques such as reranking and self-training. Our best system obtains an F1 of 92.6% on WSJ section 23, a score previously obtained only by combining the outputs from multiple parsers. A reference implementation is available as part of BLLIP Parser at http: //github.com/BLLIP/bllip-parser/ 2 Fusion Henderson and Brill (1999) propos</context>
<context position="4081" citStr="Sagae and Lavie (2006)" startWordPosition="636" endWordPosition="639"> the number of times they appear in the trees; remove any constituent with count less than m/2 from the chart; and finally create a final tree with all the remaining constituents. Intuitively their method 1360 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1360–1366, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. constructs a tree with constituents from the majority of the trees, which boosts precision significantly. Henderson and Brill (1999) show that this process is guaranteed to produce a valid tree. Sagae and Lavie (2006) generalize this work by reparsing the chart populated with constituents whose counts are above a certain threshold. By adjusting the threshold on development data, their generalized method balances precision and recall. Fossum and Knight (2009) further extend this line of work by using n-best lists from multiple parsers and combining productions in addition to constituents. Their model assigns sums of joint probabilities of constituents and parsers to constituents. Surprisingly, exploiting n-best trees does not lead to large improvement over combining 1-best trees in their experiments. Our ex</context>
<context position="10940" citStr="Sagae and Lavie, 2006" startWordPosition="1786" endWordPosition="1789">on, and baselines on development sections of corpora (WSJ section 24 and Brown tune). smoothing exponent (Q) is best set around 1.0, with accuracy falling off if the value deviates too much. Finally, the threshold parameter is empirically optimized a little below t = 0.5 (the value suggested by Henderson and Brill (1999)). Since score values are normalized, this means that constituents need roughly half the “score mass” in order to be included in the chart. Varying the threshold changes the precision/recall balance since a high threshold adds only the most confident constituents to the chart (Sagae and Lavie, 2006). Baselines: Table 2 gives the accuracy of fusion and baselines for BLLIP on the development corpora. Majority voting sets n = 50, Q = 0, t = 0.5 giving all parses equal weight and results in constituent-level majority voting. We explore a rank-based weighting which ignores parse probabilities and weight parses only using the rank: Wrank(k) = 1/(2k). These show that accurate parse-level scores are critical for good performance. Final evaluation: Table 3 gives our final results for all parsers across all domains. Results in blue are significant at p &lt; 0.01 using a randomized permutation test. F</context>
<context position="12272" citStr="Sagae and Lavie (2006)" startWordPosition="2006" endWordPosition="2009">ned BLLIP parser, in-domain F1 increases by 0.4% and out-of-domain F1 increases by 0.4% on average. Berkeley parser obtains the smallest gains from fusion since Berkeley’s n-best lists are ordered by factors other than probabilities. As a result, the probabilities from Berkeley can mislead the fusion process. We also compare against model combination using our reimplementation of Sagae and Lavie (2006). For these results, all six parsers were given equal weight. The threshold was set to 0.42 to optimize model combination F1 on development data (similar to Setting 2 for constituency parsing in Sagae and Lavie (2006)). Model combination Stanford 85.4 90.0 87.3 Stanford RNN5 89.6 92.9 90.4 Berkeley 90.0 93.5 91.2 Charniak 89.7 93.2 90.8 BLLIP 91.5 94.4 92.0 Self-trained BLLIP 92.2 94.7 92.2 1362 Figure 1: Tuning parameters independently for BLLIP and their impact on F1 for WSJ section 24 (solid purple line). For each graph, non-tuned parameters were set at the optimal configuration for BLLIP (n = 30, Q = 1.1, t = 0.47). The dashed grey line represents the 1-best baseline at 90.6% F1. Parser BNC Brown GENIA SB QB WSJ Stanford 78.4 / 79.6 80.7 / 81.6 73.1 / 73.9 67.0 / 67.9 78.6 / 80.0 85.4 / 86.2 Stanford R</context>
<context position="16563" citStr="Sagae and Lavie (2006)" startWordPosition="2747" endWordPosition="2750">han baseline performance. The difference is that novel predictions with fusion better incorporate model confidence whereas when stacking, a novel prediction is less trusted than those produced by one or both of the base parsers. Preliminary extensions: Here, we summarize two extensions to fusion which have yet to show benefits. The first extension explores applying fusion to dependency parsing. We explored two ways to apply fusion when starting from constituency parses: (1) fuse constituents and then convert them to dependencies and (2) convert to dependencies then fuse the dependencies as in Sagae and Lavie (2006). Approach (1) does not provide any benefit (LAS drops between 0.5% and 2.4%). This may result from fusion’s artifacts including unusual unary chains or nodes with a large number of children — it is possible that adjusting unary handling and the precision/recall tradeoff may reduce these issues. Approach (2) provided only modest benefits compared to those from constituency parsing fusion. The largest LAS increase for (2) is 0.6% for the Stanford Parser, though for Berkeley and Self-trained BLLIP, dependency fusion results in small losses (-0.1% LAS). Two possible reasons are that the dependenc</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 129–132, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Sandra K¨ubler</author>
<author>Reut Tsarfaty</author>
</authors>
<title>Introducing the spmrl 2014 shared task on parsing morphologically-rich languages.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages,</booktitle>
<pages>103--109</pages>
<institution>City University.</institution>
<location>Dublin, Ireland, August. Dublin</location>
<marker>Seddah, K¨ubler, Tsarfaty, 2014</marker>
<rawString>Djam´e Seddah, Sandra K¨ubler, and Reut Tsarfaty. 2014. Introducing the spmrl 2014 shared task on parsing morphologically-rich languages. In Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 103–109, Dublin, Ireland, August. Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Yusuke Miyao</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Bayesian symbol-refined tree substitution grammars for syntactic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>440--448</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1492" citStr="Shindo et al., 2012" startWordPosition="217" endWordPosition="220">nking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging where grammars are combined before or during parsing; and stacking, where one parser sends its</context>
</contexts>
<marker>Shindo, Miyao, Fujino, Nagata, 2012</marker>
<rawString>Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 440–448. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorsten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<pages>88--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14433" citStr="Skut et al., 1997" startWordPosition="2396" endWordPosition="2399">(e.g., specific weights for each constituent-parser pair), the benefits from model combination should increase. Multilingual evaluation: We evaluate fusion with the Berkeley parser on Arabic (Maamouri et al., 2004; Green and Manning, 2010), French (Abeill´e et al., 2003), and German (Brants et al., 2002) from the SPMRL 2014 shared task (Seddah et al., 2014) but did not observe any improvement. We suspect this has to do with the same ranking issues seen in the Berkeley Parser’s English results. On the other hand, fusion helps the parser of Narayan and Cohen (2015) on the German NEGRA treebank (Skut et al., 1997) to improve from 80.9% to 82.4%. Runtime: As discussed in Section 2, fusion’s runtime overhead is minimal. Reranking parsers (e.g., BLLIP and Stanford RNN) already need to perform n-best decoding as input for the reranker. Using a somewhat optimized implementation fusion in C++, the overhead over BLLIP parser is less than 1%. Discussion: Why does fusion help? It is possible that a parser’s n-list and its scores act as a weak approximation to the full parse forest. As a result, fusion seems to provide part of the benefits seen in forest reranking (Huang, 2008). Results from Fossum and Knight (2</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order languages. In Proceedings of the fifth conference on Applied natural language processing, pages 88–95. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>455--465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7852" citStr="Socher et al., 2013" startWordPosition="1269" endWordPosition="1272">e et al., 2006; Godfrey et al., 1992; Marcus et al., 1993). WSJ is used to evaluate indomain parsing, the remaining five are used for out-of-domain. For divisions, we use tune and test splits from Bacchiani et al. (2006) for Brown, McClosky’s test PMIDs2 for GENIA, Stanford’s test splits3 for QuestionBank, and articles 4000–4153 for Switchboard. Parsers: The methods are applied to six widely used n-best parsers: Charniak (2000), Stanford (Klein and Manning, 2003), BLLIP (Charniak and Johnson, 2005), Self-trained BLLIP (McClosky et al., 2006)4, Berkeley (Petrov et al., 2006), and Stanford RNN (Socher et al., 2013). The list of parsers and their accuracies on the WSJ test set is reported in Table 1. We convert to Stanford 1For parsers that return log probabilities, we turn these into probabilities first. 2http://nlp.stanford.edu/˜mcclosky/ biomedical.html 3http://nlp.stanford.edu/data/ QuestionBank-Stanford.shtml 4Using the ‘WSJ+Gigaword-v2’ BLLIP model. 1361 Parser F1 UAS LAS Table 1: Six parsers along with their 1-best F1 scores, unlabeled attachment scores (UAS) and labeled attachment scores (LAS) on WSJ section 23. Dependencies (basic dependencies, version 3.3.0) and provide dependency metrics (UAS,</context>
<context position="10037" citStr="Socher et al. (2013)" startWordPosition="1624" endWordPosition="1627">settings are also fairly similar (n is usually 30 or 40, t is usually between 0.45 and 0.5). While the smoothing exponent varies from 0.5 to 1.3, setting Q = 1 does not significantly hurt accuracy for most parsers. To study the effects of these parameters, Figure 1 shows three slices of the tuning surface for BLLIP parser on WSJ section 24 around the optimal settings (n = 30,Q = 1.1,t = 0.47). In each graph, one of the parameters is varied while the other is held constant. Increasing n-best size improves accuracy until about n = 30 where there seems to be sufficient diversity. For BLLIP, the 5Socher et al. (2013) report an Fl of 90.4%, but this is the result of using an ensemble of two RNNs (p.c.). We use a single RNN in this work. Parser WSJ Brown BLLIP 90.6 85.7 + Fusion 91.0 86.0 + Majority voting (Q = 0) 89.1 83.8 + Rank-based weighting 89.3 84.1 Table 2: F1 of a baseline parser, fusion, and baselines on development sections of corpora (WSJ section 24 and Brown tune). smoothing exponent (Q) is best set around 1.0, with accuracy falling off if the value deviates too much. Finally, the threshold parameter is empirically optimized a little below t = 0.5 (the value suggested by Henderson and Brill (19</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455–465, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Zdenˇek ˇZabokrtsk`y</author>
</authors>
<title>Improving parsing accuracy by combining diverse dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>171--178</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Zeman, ˇZabokrtsk`y, 2005</marker>
<rawString>Daniel Zeman and Zdenˇek ˇZabokrtsk`y. 2005. Improving parsing accuracy by combining diverse dependency parsers. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 171– 178. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
<author>Haizhou Li</author>
</authors>
<title>K-best combination of syntactic parsers.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1552--1560</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1601" citStr="Zhang et al. (2009)" startWordPosition="234" endWordPosition="237">rpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains. 1 Introduction Researchers have proposed many algorithms to combine parses from multiple parsers into one final parse (Henderson and Brill, 1999; Zeman and ˇZabokrtsk`y, 2005; Sagae and Lavie, 2006; Nowson and Dale, 2007; Fossum and Knight, 2009; Petrov, 2010; Johnson and Ural, 2010; Huang et al., 2010; McDonald and Nivre, 2011; Shindo et al., 2012; Narayan and Cohen, 2015). These new parses are substantially better than the originals: Zhang et al. (2009) combine outputs from multiple n-best parsers and achieve an F1 of 92.6% on the WSJ test set, a 0.5% improvement over their best n-best parser. Model combination approaches tend to fall into the following categories: hybridization, where multiple parses are combined into a single parse; switching, which picks a single parse according to some criteria (usually a form of voting); grammar merging where grammars are combined before or during parsing; and stacking, where one parser sends its prediction to another at runtime. All of these have at least one of the caveats that (1) overall computation</context>
</contexts>
<marker>Zhang, Zhang, Tan, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou Li. 2009. K-best combination of syntactic parsers. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1552–1560, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
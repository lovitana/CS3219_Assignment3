<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000986">
<title confidence="0.9982105">
Learning Word Meanings and Grammar
for Describing Everyday Activities in Smart Environments
</title>
<author confidence="0.9843005">
Muhammad Attamimi&apos; Yuji Ando&apos; Tomoaki Nakamura&apos; Takayuki Nagai&apos;
Daichi Mochihashi2 Ichiro Kobayashi3 Hideki Asoh4
</author>
<affiliation confidence="0.9709635">
&apos; The University of Electro-Communications, 1-5-1 Chofugaoka, Chofu-shi, Tokyo, Japan
2 Institute of Statistical Mathematics, 10-3 Midori-cho, Tachikawa, Tokyo, Japan
3 Ochanomizu University, 2-1-1 Otsuka Bunkyo-ku Tokyo, Japan
4 National Institute of Advanced Industrial Science and Technology,
</affiliation>
<address confidence="0.735569">
1-1-1 Umezono, Tsukuba, Ibaraki, Japan
</address>
<email confidence="0.780803">
{m att, ando, naka t}@apple.ee.uec.ac.jp, tnagai@ee.uec.ac.jp,
daichi@ism.ac.jp, koba@is.ocha.ac.jp, h.asoh@aist.go.jp
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899555555556">
If intelligent systems are to interact with
humans in a natural manner, the ability
to describe daily life activities is impor-
tant. To achieve this, sensing human ac-
tivities by capturing multimodal informa-
tion is necessary. In this study, we con-
sider a smart environment for sensing ac-
tivities with respect to realistic scenarios.
We next propose a sentence generation
system from observed multimodal infor-
mation in a bottom up manner using mul-
tilayered multimodal latent Dirichlet allo-
cation and Bayesian hidden Markov mod-
els. We evaluate the grammar learning and
sentence generation as a complete process
within a realistic setting. The experimen-
tal result reveals the effectiveness of the
proposed method.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978038461539">
Describing daily life activities is an important abil-
ity of intelligent systems. In fact, we can use
this ability to achieve a monitoring system that
is able to report on an observed situation, or cre-
ate an automatic diary of a user. Recently, sev-
eral studies have been performed to generate sen-
tences that describe images using Deep Learning
(Vinyals et al., 2014; Fang et al., 2014; Donahue
et al., 2014; Kiros et al., 2015). Although these
results were good, we are interested in unsuper-
vised frameworks. This is necessary to achieve
a system that can adapt to the user, that is, one
that can learn a user-unique language and gener-
ate it automatically. Moreover, the use of crowd-
sourcing should be avoided to respect the privacy
of the user. Regarding this, studies on sentence
generation from RGB videos have been discussed
in (Yu and Siskind, 2013; Regneri et al., 2013).
A promising result for language learning has been
shown in (Yu and Siskind, 2013) and a quite chal-
lenging effort to describe cooking activities was
made in (Regneri et al., 2013). However, these
studies rely only on visual information, while we
aim to build a system that is able to describe every-
day activities using multimodal information. To
realize such systems, we need to consider two
problems. The first problem is the sensing of daily
life activities. In this paper, we utilize a smart
house (Motooka et al., 2010) for sensing human
activities. Thanks to the smart house, multimodal
information such as visual, motion, and audio data
can be captured. The second problem to be tack-
led is verbalization of the observed scenes. To
solve this problem, a multilayered multimodal la-
tent Dirichlet allocation (mMLDA) was proposed
in (Attamimi et al., 2014).
In this paper, we propose a sentence generation
system from observed scenes in a bottom up man-
ner using mMLDA and a Bayesian hidden Markov
model (BHMM) (Goldwater and Griffiths, 2007).
To generate sentences from scenes, we need to
consider the words that represent the scenes and
their order. Here, mMLDA is used to infer words
for given scenes. To determine the order of words,
inspired by (Kawai et al., 2014), a probabilis-
tic grammar that considers syntactic information
is learned using BHMM. In this study, the order
of concepts is generated by sampling the learned
grammar. The word selection for each generated
concept is then performed using the observed data.
Moreover, a language model that represents the re-
lationship between words is also used to calculate
</bodyText>
<page confidence="0.953323">
2249
</page>
<note confidence="0.9336425">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2249–2254,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.993412">
Figure 1: Language learning and sentence genera-
tion system.
</figureCaption>
<figure confidence="0.999518447368421">
Sensing
Smart
Environment
Grammar
Sentence
Generation
Bayesian
HMM
Connection updates
Seq. of concepts
Seq. of concepts
Multimodal data
Seq. of teaching
sentences
Bigram model
Generated sentences
Mutual info.
for init. value
Morphological
analysis
Signal processing
Seq. of words
Functional
Connection between
words and concepts
BoW
Word
BoF
Concept
mMLDA
Place Concept
Word Position Word
Motion Concept
Angle
Integrated Concept
Word
Object Concept
Object
</figure>
<figureCaption confidence="0.999911">
Figure 3: Graphical model of mMLDA.
Figure 2: Multimodal information acquisition.
</figureCaption>
<bodyText confidence="0.999946545454545">
the transition probability between them. Consid-
ering the transition probability at word level, a lat-
tice of word candidates corresponding to the con-
cept sequence can be generated. Therefore, sen-
tence generation can be thought of as a problem
of finding the word sequence that has the high-
est probability from the lattice of word candidates,
which can be solved by the Viterbi algorithm. Fi-
nally, sampling from grammar is performed mul-
tiple times to generate sentence candidates and se-
lect the most probable one.
</bodyText>
<sectionHeader confidence="0.981808" genericHeader="method">
2 Proposed method
</sectionHeader>
<subsectionHeader confidence="0.863222">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999530837837838">
Figure 1 illustrates the overall system of proposed
language learning and sentence generation. In
this study, we use a smart environment for sens-
ing multimodal information. The system shown in
Figure 2 is part of a smart house (Motooka et al.,
2010) that is used to capture multimodal informa-
tion. Here, an RFID tag is attached to an object
to enable the object information to be read using a
wearable tag reader. To capture motion, five sen-
sors that consist of 3-axis acceleration with 3-axis
gyroscope sensors are attached to the upper body,
as shown in Figure 2. Moreover, a particle filter-
based human tracker (Glas et al., 2007) applied
to four laser range finders is used to estimate the
location of a person while performing an action.
This is a setup designed to demonstrate that lan-
guage can be learned and generated from real hu-
man actions. Ultimately, our goal is sensing based
on image recognition.
The acquired multimodal data is then processed,
which results in a bag-of-words model (BoW) and
bag-of-features model (BoF) (Csurka et al., 2004).
Using mMLDA (see section 2.2), various concepts
can be formed from the multimodal data. Given
teaching sentences, the connection between words
and concepts can be learned based on mMLDA
and BHMM which is learned with mutual infor-
mation (MI) as the initial value. On the other hand,
the bigram model of words is calculated and used
as the score when reordering words inferred from
multimodal information using grammar. A mor-
phological analyzer for parsing words in a sen-
tence is also necessary in the proposed system. We
use publicly available parser MeCab (Kudo et al.,
2004). In the future, we plan to use the unsuper-
vised morphological analysis technique proposed
in (Mochihashi et al., 2009).
</bodyText>
<subsectionHeader confidence="0.983487">
2.2 mMLDA
</subsectionHeader>
<bodyText confidence="0.99848275">
Figure 3 shows the graphical model of
mMLDA used in this paper. Here, z repre-
sents the integrated category (concept), whereas
zO, zM, and zP represent the object, mo-
</bodyText>
<figure confidence="0.999390357142857">
Object
with an
RFID tag
RFID tag
to detect
the end of
actions
Wearable
camera
Laser range
finder
Tag reader
Gyro-acceleration
sensor
</figure>
<page confidence="0.967194">
2250
</page>
<bodyText confidence="0.999882970588235">
tion, and place concepts, respectively. In
the bottom layer (lower panel of Figure 3),
wm ∈ {wo, wwO, wa, wwM, wl, wwP} represents
the multimodal information obtained from each
object, motion, and place. Here, wo, wa, and wl
denote multimodal information obtained respec-
tively from the object used in an action, motion of
a person while using the object, and location of
the action. Further, wwC ∈ {wwO, wwM, wwP}
denotes word information obtained from teaching
sentences. Observation information is acquired
by using the system shown in Figure 2. A brief
explanation of each observation is as follows.
For object information, an No-dimensional vec-
tor wo = (o1, o2, · · · , oNo) is used, where No de-
notes the number of objects. In this vector, o*
takes a value of 0 or 1, where oi is set to 1 if an
object with index i is observed. Moreover, all of
the teaching sentences are segmented into words
and represented by a BoW as word information.
Here, motion is segmented according to the ob-
ject used. A sequence of 15-dimensional feature
vectors for each motion is acquired. Using BoF,
the acquired feature vectors are vector quantized,
resulting in a 70-dimensional vector. The acquired
two dimensional of human positions are processed
using BoF to construct a 10-dimensional vector as
place information.
In mMLDA, latent variables that represent up-
per and lower concepts z and zC ∈ {zO, zM, zP}
are learned simultaneously. Gibbs sampling is ap-
plied to the marginalized posterior probability of
latent variables to learn the model from observed
data wm (Attamimi et al., 2014).
</bodyText>
<subsectionHeader confidence="0.998214">
2.3 Language learning and generation
2.3.1 Word inference
</subsectionHeader>
<bodyText confidence="0.999805777777778">
In this study, word information is obtained from
teaching sentences and employed for all concepts,
as shown in Figure 3. Considering that appropriate
words to express each concept exist, a criterion to
measure the correlation between words and con-
cepts is needed. At the start of grammar learn-
ing, MI, which can measure the mutual depen-
dence of two stochastic variables, is used. There-
fore, a word is considered to express a category
when the MI between the word and category is
large. On the other hand, a word with small MI
is identified as a functional word. This determi-
nation is used as an initial value in the syntac-
tic learning and needs not be strictly determined.
Once the grammar is learned, we can utilize
BHMM’s parameters P(ww|c) to infer a word ww
from observed data wmobs as P�(wwC|wmobs,c) ∝
maxk P(wwC|c)P(wwC|k)P(k|wmobs, c), where
P(wwC|k) and P(k|wmobs,c) can be estimated
from mMLDA (Attamimi et al., 2014) and k is
category of concept c′ ∈ {object, motion, place}
and c ∈ {c′, functional}. It should be note
that P(wwC|k) and P(k|wmobs, c) are considered
as uniform distribution for “functional” since
they cannot be inferred from observed data using
mMLDA. In this case, we can rely on syntactic in-
formation which is learned by BHMM.
</bodyText>
<subsectionHeader confidence="0.547205">
2.3.2 Grammar learning using BHMM
</subsectionHeader>
<bodyText confidence="0.999151714285714">
Thanks to mMLDA and BHMM, appropriate
words to represent the observed information can
be inferred. Given an input consisting of a teach-
ing sentence of a sequence of words, a BHMM
can infer a sequence of concepts. In the learning
phase, the MI results of concept selection for each
word are used as the initial values of the BHMM.
Here, grammar is defined as the concept transi-
tion probability P(Ct|Ct_1), which is estimated
using Gibbs sampling, where Ct ∈ c represents
the corresponding concepts of the t-th word in the
sentence. In addition, a language model that rep-
resent the bigram model of words in the teaching
sentences is also used for generating sentences.
</bodyText>
<table confidence="0.999843285714286">
Motion Object Place Motion Object Place Motion Object Place
Drink (1) Juice (1) Sofa (1) Wipe (7) Dustcloth (9) Kitchen (4) Write on (12) Notebook (16) Bedroom (5)
Tea (4) Dining room (2) Tissue (10) Dining room (2) Textbook (17) Sofa (1)
Eat (2) Cookies (2) Dining room (2) Turn on (8) Remote control Living room (3) Open (13) Refrigerator (18) Kitchen (4)
(air conditioner) (11)
Chocolate (3) Living room (3) Bedroom (5) Microwave (19) Kitchen (4)
Shake (3) Tea (4) Sofa (1) Open (turn) (9) Tea (4) Living room (3) Closet (20) Bedroom (5)
Dressing (5) Kitchen (4) Honey (6) Dining room (2) Read (14) Textbook (17) Bedroom (5)
Pour (4) Tea (4) Kitchen (4) Wrap (10) Plastic wrap (12) Dining room (2) Magazine (21) Sofa (1)
Juice (1) Living room (3) Aluminum foil (13) Kitchen (4) Spray (15) Deodorizer (22) Living room (3)
Put on (5) Dressing (5) Dining room (2) Hang (11) Shirt (14) Bedroom (5) Bedroom (5)
Honey (6) Kitchen (4) Parka (15) Living room (3) Scrub (16) Scourer (23) Kitchen (4)
Throw (6) Ball (7) Sofa (1) Sponge (24) Kitchen (4)
Plushie (8) Bedroom (5)
</table>
<tableCaption confidence="0.979063">
Table 1: Object, motion, and place correspondences (numbers in parentheses represent the category
index).
</tableCaption>
<page confidence="0.978404">
2251
</page>
<figure confidence="0.996214263157895">
Actual scenes Acquired multimodal info. Generated sentences
Place info.
Throwing
Object info. Motion info.
-100
Cookies
Ball
Parka
Sofa
Dining
Eating Throwing Hanging
Hanging
10000
Living
Eating
100
0
5000
0
</figure>
<bodyText confidence="0.60165925">
B: {dining room, with, cookies, the, eat, eat}
P: {dining room, in, cookies, the, eat}
G: {dining room, in, cookies, the, eat}
Eating the cookies in the dining room.
B: {sofa, with, ball, to, throw, throw}
P: {sofa, on, ball, the, throw}
G: {sofa, on, ball, the, throw}
Throwing the ball on the sofa.
B: {living room, in, hang}
P: {living room, in, parka, the, hang}
G: {living room, in, parka, the, hang}
Hanging the parka in the living room.
</bodyText>
<figure confidence="0.994708">
(a) (b) (c)
</figure>
<figureCaption confidence="0.8411735">
Figure 4: Examples of: (a) actual images, (b) captured multimodal information, and (c) generated sen-
tences. In each image, B, P, and G indicate the sentence structure in Japanese grammar generated by the
baseline method, proposed method, and correct sentence, respectively; whereas the bottom line gives the
meaning of the generated sentence. Words marked in red have been incorrectly generated.
</figureCaption>
<subsectionHeader confidence="0.672358">
2.3.3 Sentence generation of observed scenes
</subsectionHeader>
<bodyText confidence="0.990518428571429">
First, concepts are sampled from the begin of sen-
tence “BOS” until the end of sentence “EOS” ac-
cording to the learned grammar N times. Let
the n-th (n ∈ {1, 2, · · · , N}) sequence of con-
cepts that excludes “BOS” and “EOS” be Cn =
{Cn1 , · · · , Cnt , · · · , CnTn}, where, Tn denotes the
number of sampled concepts, which corresponds
to the length of a sampled sentence.
Next, the word that corresponds to concept Cnt
is estimated. Here, for a given observed infor-
mation wmobs, the top-K words that correspond to
concept Cnt and have high probabilities wnt =
{wnt1, wnt�, · · · , wntK} are used. Hence, the set of
all words for a sequence of concepts Cn can be
written as Wn = {wn1, wn�, · · · , wn }. There-
Tn
fore, KTn number of patterns for a candidate of
the sentence can be considered for Cn and the cor-
responding words Wn. Each candidate for sen-
tence Sn is selected from these patterns and has
the following probability:
</bodyText>
<equation confidence="0.998208333333333">
P(Sn|Cn, Wn, wmobs) ∝
∏ P(Cnt |Cnt−1)P(wnt |wmobs, Cnt )P(wnt |wnt−1). (1)
t
</equation>
<bodyText confidence="0.972115">
For observed information, the most probable sen-
tence is selected from N sequences of concepts
with sets of words. Here, the sentence §n that
maximizes Eq. (1) is determined for each se-
quence of concepts. Because many patterns of
Sn exist, the Viterbi algorithm is applied to cut
the computational cost and determine the most
probable sentence. Thus, a set of sentences that
consists of sentences with the highest probability
for each sequence of concepts can be written as
S� = {�S1, ··· , sn, ··· , §N}.
We can select the final sentence from S� by con-
sidering the most probable candidate. In fact,
long sentences tend to have low probability and
are less likely to be selected. To cope with
this problem, adjustment coefficient ℓ(sn) =
</bodyText>
<equation confidence="0.97267">
(L—&apos;−Lg
n)
N ∑n log P(�Sn|Cn, Wn, wm
obs) is in-
∑n L�Sn
</equation>
<bodyText confidence="0.934014363636363">
troduced, where, Lsn denotes the length of sen-
tence Sn and Lma&amp;quot; represents the maximum
value of the sentence length in k Using ℓ(�Sn),
the logarithmic probability of the sentence can
be calculated as log P�(Sn|Cn, Wn, wmobs) =
log P( �Sn|Cn,Wn,wm obs) + ωℓ(�Sn), where ω
is a weight that controls the length of sen-
tences. A large weight leads to longer sen-
tences. The final sentence S is determined as
S=argmax.- log P�(Sn|Cn,Wn,wmobs).
SnES
</bodyText>
<sectionHeader confidence="0.995942" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999937857142857">
The acquisition system shown in Figure 2 was
used to capture multimodal information from hu-
man actions. Table 1 shows the actions that were
performed by three subjects twice, resulting in a
total of 195 multimodal data with 1170 sentences.
We then divided the data into training data (99
multimodal data with 594 sentences) and test data
(96 multimodal data with 576 sentences). Some
examples of acquired multimodal data are shown
in Figure 4(b). Using training data, various con-
cepts were formed by mMLDA, and the catego-
rization accuracies for object, motion, and place
were respectively 100.00%, 52.53%, and 95.96%.
Motion similarity was responsible for the false cat-
</bodyText>
<page confidence="0.974937">
2252
</page>
<table confidence="0.999729">
♯ of words Baseline Proposed
w/o functional words 78 65.38% 73.08%
w functional words 98 – 68.37%
</table>
<tableCaption confidence="0.996843">
Table 2: Concepts selection results.
</tableCaption>
<bodyText confidence="0.999813108695652">
egorization of motion concepts. Since our goal is
to generate sentences from observed scenes, these
results are used as reference instead of comparing
with the baseline.
To evaluate the concept selection of words, 98
words in teaching sentences were used. We com-
pared the results of concept selection with hand-
labeled ones. Table 2 shows the accuracy rate of
concept selection. Here, we excluded the func-
tional words (resulting in 78 words) for fair com-
parison with the baseline method (Attamimi et
al., 2014). One can see that, better results can
be achieved by the proposed method. It is clear
that concept selection is improved by using the
BHMM, indicating that a better grammar can be
learned using this model.
Next, the learned grammar was used and sen-
tences were generated. To reduce randomness of
the results, sentence generation was conducted 10
times for each data. To verify sentence gener-
ation quantitatively, we evaluated the sentences
automatically using BLEU score (Papineni et al.,
2002). Figure 5 depicts the results of 2- to 4-gram
of BLEU scores. Since functional words are not
considered in (Attamimi et al., 2014), we used our
grammar and performed sentence generation pro-
posed in (Attamimi et al., 2014) as the baseline
method. One can see from the figure that in all
cases the BLEU scores of proposed method out
performs the baseline method. It can be said that
the sentences generated by the proposed method
are of better quality than those generated by the
baseline method.
Moreover, we also manually evaluated gener-
ated sentences by asking four subjects (i.e., col-
lege students who understand Japanese) whether
the sentences were: correct both in grammar and
meaning (E1), grammatically correct but incorrect
in meaning (E2), grammatically incorrect but cor-
rect in meaning (E3), or incorrect both in grammar
and meaning (E4). The average rates of E1, E2,
E3, and E4 were shown in Table 3. We can see
that the proposed method out performs the base-
line method by providing high rates of E1 and E2;
and low rates of E4. Because we want to generate
sentences that explain actions, incorrect motion in-
</bodyText>
<table confidence="0.910026166666667">
Grammar Meaning Baseline Proposed
E1 correct correct (23.21 f 5.28)% (45.39 f 3.02)%
E2 correct incorrect (35.07 f 9.32)% (49.79 f 3.77)%
E3 incorrect correct (11.34 f 5.59)% (2.79 f 2.39)%
E4 incorrect incorrect (30.38 f 10.54)% (2.03 f 2.10)%
2-gram 3-gram 4-gram
</table>
<figureCaption confidence="0.994764">
Figure 5: BLEU scores of generated sentences.
</figureCaption>
<bodyText confidence="0.999991642857143">
ference would lead to incorrect sentence genera-
tion. Examples of E2 are “Eating the plastic wrap
in the dining room” and “Opening the dressing in
the kitchen.” One can see that these sentences
are grammatically correct but do not express the
scenes correctly because the words that represent
the motion are incorrect. Hence, the misclassi-
fication that occurred in the motion concept for-
mation was responsible for the incorrect meaning
of the generated sentences. Figure 4(c) shows the
sentences generated from the given scenes (Fig-
ure 4(a)). We can see that meaningful yet natural
sentences that explain the observed scenes can be
generated using the proposed method.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999981058823529">
In this paper, we proposed an unsupervised
method to generate natural sentences from ob-
served scenes in a smart environment using
mMLDA and BHMM. In the smart environment,
multimodal information can be acquired for real-
istic scenarios. Thanks to mMLDA, various con-
cepts can be formed and an initial determination of
functional words can be made by assuming a weak
connection of concepts and words calculated by
MI. The possibility that grammar can be learned
from BHMM by considering the syntactic infor-
mation has also been shown. We conducted exper-
iments to verify the proposed sentence generation,
and promising preliminary results were obtained.
In future work, we aim to implement a nonpara-
metric Bayes model that will be able to estimate
the number of concepts automatically.
</bodyText>
<sectionHeader confidence="0.998629" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.964513">
This work is partly supported by JSPS KAKENHI
26280096.
</bodyText>
<tableCaption confidence="0.950988">
Table 3: Evaluation results of generated sentences.
</tableCaption>
<figure confidence="0.9943225">
BLEU score
0.5
0
1
Baseline method
Proposed method
</figure>
<page confidence="0.979306">
2253
</page>
<sectionHeader confidence="0.987817" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998969625">
Muhammad Attamimi, Muhammad Fadlil, Kasumi
Abe, Tomoaki Nakamura, Kotaro Funakoshi, and
Takayuki Nagai. 2014. Integration of Various
Concepts and Grounding of Word Meanings Using
Multi-layered Multimodal LDA for Sentence Gener-
ation. In Proc. of IEEE/RSJ International Confer-
ence on Intelligent Robots, pp.2194–2201.
Gabriella Csurka, Christopher R. Dance, Lixin Fan,
Jutta Willamowski, C´edric Bray. 2004. Visual Cat-
egorization with Bags of Keypoints. In Proc. of
ECCV International Workshop on Statistical Learn-
ing in Computer Vision.
Jeffrey Donahue, Lisa Hendricks, Sergio Guadarrama,
Marcus Rohrbach, Subhashini Venugopalan, Kate
Saenko, and Trevor Darrell. 2014. Long-term Re-
current Convolutional Networks for Visual Recog-
nition and Description. Technical Report No.
UCB/EECS-2014-180.
Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh
Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao,
Xiaodong He, Margaret Mitchell, John Platt, C.
Lawrence Zitnick, and Geoffrey Zweig. 2014. From
Captions to Visual Concepts and Back. In Proc. of
IEEE International Conference on Computer Vision
and Pattern Recognition.
Dylan F. Glas, Takahiro Miyashita, Hiroshi Ishiguro,
and Norihiro Hagita. 2007. Laser Tracking of Hu-
man Body Motion Using Adaptive Shape Modeling.
In Proc. of IEEE/RSJ International Conference on
Intelligent Robots, pp.602–608.
Sharon Goldwater and Thomas L. Griffiths. 2007. A
Fully Bayesian Approach to Unsupervised Part-of-
Speech Tagging. In Proc. of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pp.744–751.
Yuji Kawai, Yuji Oshima, Yuki Sasamoto, Yukie Nagai,
and Minoru Asada. 2014. Computational Model
for Syntactic Development: Identifying How Chil-
dren Learn to Generalize Nouns and Verbs for Dif-
ferent Languages In Proc. of Joint IEEE Interna-
tional Conferences on Development and Learning
and Epigenetic Robotics, pp.78–84.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S.
Zemel. 2015. Unifying Visual-Semantic Embed-
dings with Multimodal Neural Language Models. In
Trans. of the Association for Computational Lin-
guistics.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying Conditional Random Fields to
Japanese Morphological Analysis. In Proc. of Con-
ference on Empirical Methods in Natural Language
Processing, pp.230–237.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian Unsupervised Word Segmen-
tation with Nested Pitman-Yor Language Modeling
In Proc. of the Association for Computational Lin-
guistics, pp.100–108.
Nobuhisa Motooka, Ichiro Shiio, Yuji Ohta, Koji
Tsukada, Keisuke Kambara, and Masato Iguchi.
2010. Ubiquitous Computing House Project: De-
sign for Everyday Life. Journal of Asian Architec-
ture and Building Engineering, 8:77–82.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
Association for Computational Linguistics, pp.311–
318.
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding Action Descriptions in
Videos. Trans. of the Association for Computational
Linguistics, 1:25–36.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2014. Show and Tell: A Neu-
ral Image Caption Generator. In arXiv:1411.4555
[cs.CV].
Haonan Yu and Jeffrey M. Siskind. 2013. Grounded
Language Learning from Video Described with Sen-
tences. In Proc. of the Association for Computa-
tional Linguistics, pp.53–63.
</reference>
<page confidence="0.994069">
2254
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573537">
<title confidence="0.9997335">Learning Word Meanings and for Describing Everyday Activities in Smart Environments</title>
<author confidence="0.997371">Ichiro Hideki</author>
<affiliation confidence="0.995681">University of Electro-Communications, 1-5-1 Chofugaoka, Chofu-shi, Tokyo,</affiliation>
<address confidence="0.811179">of Statistical Mathematics, 10-3 Midori-cho, Tachikawa, Tokyo, University, 2-1-1 Otsuka Bunkyo-ku Tokyo,</address>
<affiliation confidence="0.999305">Institute of Advanced Industrial Science and</affiliation>
<address confidence="0.9330735">1-1-1 Umezono, Tsukuba, Ibaraki, att, ando, naka</address>
<email confidence="0.990622">daichi@ism.ac.jp,koba@is.ocha.ac.jp,h.asoh@aist.go.jp</email>
<abstract confidence="0.998344263157895">If intelligent systems are to interact with humans in a natural manner, the ability to describe daily life activities is important. To achieve this, sensing human activities by capturing multimodal information is necessary. In this study, we consider a smart environment for sensing activities with respect to realistic scenarios. We next propose a sentence generation system from observed multimodal information in a bottom up manner using multilayered multimodal latent Dirichlet allocation and Bayesian hidden Markov models. We evaluate the grammar learning and sentence generation as a complete process within a realistic setting. The experimental result reveals the effectiveness of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Muhammad Attamimi</author>
<author>Muhammad Fadlil</author>
<author>Kasumi Abe</author>
<author>Tomoaki Nakamura</author>
<author>Kotaro Funakoshi</author>
<author>Takayuki Nagai</author>
</authors>
<title>Integration of Various Concepts and Grounding of Word Meanings Using Multi-layered Multimodal LDA for Sentence Generation.</title>
<date>2014</date>
<booktitle>In Proc. of IEEE/RSJ International Conference on Intelligent Robots,</booktitle>
<pages>2194--2201</pages>
<contexts>
<context position="3127" citStr="Attamimi et al., 2014" startWordPosition="481" endWordPosition="484"> while we aim to build a system that is able to describe everyday activities using multimodal information. To realize such systems, we need to consider two problems. The first problem is the sensing of daily life activities. In this paper, we utilize a smart house (Motooka et al., 2010) for sensing human activities. Thanks to the smart house, multimodal information such as visual, motion, and audio data can be captured. The second problem to be tackled is verbalization of the observed scenes. To solve this problem, a multilayered multimodal latent Dirichlet allocation (mMLDA) was proposed in (Attamimi et al., 2014). In this paper, we propose a sentence generation system from observed scenes in a bottom up manner using mMLDA and a Bayesian hidden Markov model (BHMM) (Goldwater and Griffiths, 2007). To generate sentences from scenes, we need to consider the words that represent the scenes and their order. Here, mMLDA is used to infer words for given scenes. To determine the order of words, inspired by (Kawai et al., 2014), a probabilistic grammar that considers syntactic information is learned using BHMM. In this study, the order of concepts is generated by sampling the learned grammar. The word selection</context>
<context position="8880" citStr="Attamimi et al., 2014" startWordPosition="1421" endWordPosition="1424">on is segmented according to the object used. A sequence of 15-dimensional feature vectors for each motion is acquired. Using BoF, the acquired feature vectors are vector quantized, resulting in a 70-dimensional vector. The acquired two dimensional of human positions are processed using BoF to construct a 10-dimensional vector as place information. In mMLDA, latent variables that represent upper and lower concepts z and zC ∈ {zO, zM, zP} are learned simultaneously. Gibbs sampling is applied to the marginalized posterior probability of latent variables to learn the model from observed data wm (Attamimi et al., 2014). 2.3 Language learning and generation 2.3.1 Word inference In this study, word information is obtained from teaching sentences and employed for all concepts, as shown in Figure 3. Considering that appropriate words to express each concept exist, a criterion to measure the correlation between words and concepts is needed. At the start of grammar learning, MI, which can measure the mutual dependence of two stochastic variables, is used. Therefore, a word is considered to express a category when the MI between the word and category is large. On the other hand, a word with small MI is identified </context>
<context position="16754" citStr="Attamimi et al., 2014" startWordPosition="2780" endWordPosition="2783">oposed w/o functional words 78 65.38% 73.08% w functional words 98 – 68.37% Table 2: Concepts selection results. egorization of motion concepts. Since our goal is to generate sentences from observed scenes, these results are used as reference instead of comparing with the baseline. To evaluate the concept selection of words, 98 words in teaching sentences were used. We compared the results of concept selection with handlabeled ones. Table 2 shows the accuracy rate of concept selection. Here, we excluded the functional words (resulting in 78 words) for fair comparison with the baseline method (Attamimi et al., 2014). One can see that, better results can be achieved by the proposed method. It is clear that concept selection is improved by using the BHMM, indicating that a better grammar can be learned using this model. Next, the learned grammar was used and sentences were generated. To reduce randomness of the results, sentence generation was conducted 10 times for each data. To verify sentence generation quantitatively, we evaluated the sentences automatically using BLEU score (Papineni et al., 2002). Figure 5 depicts the results of 2- to 4-gram of BLEU scores. Since functional words are not considered i</context>
</contexts>
<marker>Attamimi, Fadlil, Abe, Nakamura, Funakoshi, Nagai, 2014</marker>
<rawString>Muhammad Attamimi, Muhammad Fadlil, Kasumi Abe, Tomoaki Nakamura, Kotaro Funakoshi, and Takayuki Nagai. 2014. Integration of Various Concepts and Grounding of Word Meanings Using Multi-layered Multimodal LDA for Sentence Generation. In Proc. of IEEE/RSJ International Conference on Intelligent Robots, pp.2194–2201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriella Csurka</author>
<author>Christopher R Dance</author>
<author>Lixin Fan</author>
<author>Jutta Willamowski</author>
<author>C´edric Bray</author>
</authors>
<title>Visual Categorization with Bags of Keypoints.</title>
<date>2004</date>
<booktitle>In Proc. of ECCV International Workshop on Statistical Learning in Computer Vision.</booktitle>
<contexts>
<context position="6294" citStr="Csurka et al., 2004" startWordPosition="985" endWordPosition="988">ve sensors that consist of 3-axis acceleration with 3-axis gyroscope sensors are attached to the upper body, as shown in Figure 2. Moreover, a particle filterbased human tracker (Glas et al., 2007) applied to four laser range finders is used to estimate the location of a person while performing an action. This is a setup designed to demonstrate that language can be learned and generated from real human actions. Ultimately, our goal is sensing based on image recognition. The acquired multimodal data is then processed, which results in a bag-of-words model (BoW) and bag-of-features model (BoF) (Csurka et al., 2004). Using mMLDA (see section 2.2), various concepts can be formed from the multimodal data. Given teaching sentences, the connection between words and concepts can be learned based on mMLDA and BHMM which is learned with mutual information (MI) as the initial value. On the other hand, the bigram model of words is calculated and used as the score when reordering words inferred from multimodal information using grammar. A morphological analyzer for parsing words in a sentence is also necessary in the proposed system. We use publicly available parser MeCab (Kudo et al., 2004). In the future, we pla</context>
</contexts>
<marker>Csurka, Dance, Fan, Willamowski, Bray, 2004</marker>
<rawString>Gabriella Csurka, Christopher R. Dance, Lixin Fan, Jutta Willamowski, C´edric Bray. 2004. Visual Categorization with Bags of Keypoints. In Proc. of ECCV International Workshop on Statistical Learning in Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Donahue</author>
<author>Lisa Hendricks</author>
<author>Sergio Guadarrama</author>
<author>Marcus Rohrbach</author>
<author>Subhashini Venugopalan</author>
<author>Kate Saenko</author>
<author>Trevor Darrell</author>
</authors>
<title>Long-term Recurrent Convolutional Networks for Visual Recognition and Description.</title>
<date>2014</date>
<tech>Technical Report No. UCB/EECS-2014-180.</tech>
<contexts>
<context position="1798" citStr="Donahue et al., 2014" startWordPosition="259" endWordPosition="262">den Markov models. We evaluate the grammar learning and sentence generation as a complete process within a realistic setting. The experimental result reveals the effectiveness of the proposed method. 1 Introduction Describing daily life activities is an important ability of intelligent systems. In fact, we can use this ability to achieve a monitoring system that is able to report on an observed situation, or create an automatic diary of a user. Recently, several studies have been performed to generate sentences that describe images using Deep Learning (Vinyals et al., 2014; Fang et al., 2014; Donahue et al., 2014; Kiros et al., 2015). Although these results were good, we are interested in unsupervised frameworks. This is necessary to achieve a system that can adapt to the user, that is, one that can learn a user-unique language and generate it automatically. Moreover, the use of crowdsourcing should be avoided to respect the privacy of the user. Regarding this, studies on sentence generation from RGB videos have been discussed in (Yu and Siskind, 2013; Regneri et al., 2013). A promising result for language learning has been shown in (Yu and Siskind, 2013) and a quite challenging effort to describe coo</context>
</contexts>
<marker>Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, Darrell, 2014</marker>
<rawString>Jeffrey Donahue, Lisa Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2014. Long-term Recurrent Convolutional Networks for Visual Recognition and Description. Technical Report No. UCB/EECS-2014-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Fang</author>
<author>Saurabh Gupta</author>
<author>Forrest Iandola</author>
<author>Rupesh Srivastava</author>
<author>Li Deng</author>
<author>Piotr Dollar</author>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Margaret Mitchell</author>
<author>John Platt</author>
<author>C Lawrence Zitnick</author>
<author>Geoffrey Zweig</author>
</authors>
<title>From Captions to Visual Concepts and Back.</title>
<date>2014</date>
<booktitle>In Proc. of IEEE International Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="1776" citStr="Fang et al., 2014" startWordPosition="255" endWordPosition="258">on and Bayesian hidden Markov models. We evaluate the grammar learning and sentence generation as a complete process within a realistic setting. The experimental result reveals the effectiveness of the proposed method. 1 Introduction Describing daily life activities is an important ability of intelligent systems. In fact, we can use this ability to achieve a monitoring system that is able to report on an observed situation, or create an automatic diary of a user. Recently, several studies have been performed to generate sentences that describe images using Deep Learning (Vinyals et al., 2014; Fang et al., 2014; Donahue et al., 2014; Kiros et al., 2015). Although these results were good, we are interested in unsupervised frameworks. This is necessary to achieve a system that can adapt to the user, that is, one that can learn a user-unique language and generate it automatically. Moreover, the use of crowdsourcing should be avoided to respect the privacy of the user. Regarding this, studies on sentence generation from RGB videos have been discussed in (Yu and Siskind, 2013; Regneri et al., 2013). A promising result for language learning has been shown in (Yu and Siskind, 2013) and a quite challenging </context>
</contexts>
<marker>Fang, Gupta, Iandola, Srivastava, Deng, Dollar, Gao, He, Mitchell, Platt, Zitnick, Zweig, 2014</marker>
<rawString>Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John Platt, C. Lawrence Zitnick, and Geoffrey Zweig. 2014. From Captions to Visual Concepts and Back. In Proc. of IEEE International Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dylan F Glas</author>
<author>Takahiro Miyashita</author>
<author>Hiroshi Ishiguro</author>
<author>Norihiro Hagita</author>
</authors>
<title>Laser Tracking of Human Body Motion Using Adaptive Shape Modeling.</title>
<date>2007</date>
<booktitle>In Proc. of IEEE/RSJ International Conference on Intelligent Robots,</booktitle>
<pages>602--608</pages>
<contexts>
<context position="5871" citStr="Glas et al., 2007" startWordPosition="915" endWordPosition="918">trates the overall system of proposed language learning and sentence generation. In this study, we use a smart environment for sensing multimodal information. The system shown in Figure 2 is part of a smart house (Motooka et al., 2010) that is used to capture multimodal information. Here, an RFID tag is attached to an object to enable the object information to be read using a wearable tag reader. To capture motion, five sensors that consist of 3-axis acceleration with 3-axis gyroscope sensors are attached to the upper body, as shown in Figure 2. Moreover, a particle filterbased human tracker (Glas et al., 2007) applied to four laser range finders is used to estimate the location of a person while performing an action. This is a setup designed to demonstrate that language can be learned and generated from real human actions. Ultimately, our goal is sensing based on image recognition. The acquired multimodal data is then processed, which results in a bag-of-words model (BoW) and bag-of-features model (BoF) (Csurka et al., 2004). Using mMLDA (see section 2.2), various concepts can be formed from the multimodal data. Given teaching sentences, the connection between words and concepts can be learned base</context>
</contexts>
<marker>Glas, Miyashita, Ishiguro, Hagita, 2007</marker>
<rawString>Dylan F. Glas, Takahiro Miyashita, Hiroshi Ishiguro, and Norihiro Hagita. 2007. Laser Tracking of Human Body Motion Using Adaptive Shape Modeling. In Proc. of IEEE/RSJ International Conference on Intelligent Robots, pp.602–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A Fully Bayesian Approach to Unsupervised Part-ofSpeech Tagging.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="3312" citStr="Goldwater and Griffiths, 2007" startWordPosition="512" endWordPosition="515">roblem is the sensing of daily life activities. In this paper, we utilize a smart house (Motooka et al., 2010) for sensing human activities. Thanks to the smart house, multimodal information such as visual, motion, and audio data can be captured. The second problem to be tackled is verbalization of the observed scenes. To solve this problem, a multilayered multimodal latent Dirichlet allocation (mMLDA) was proposed in (Attamimi et al., 2014). In this paper, we propose a sentence generation system from observed scenes in a bottom up manner using mMLDA and a Bayesian hidden Markov model (BHMM) (Goldwater and Griffiths, 2007). To generate sentences from scenes, we need to consider the words that represent the scenes and their order. Here, mMLDA is used to infer words for given scenes. To determine the order of words, inspired by (Kawai et al., 2014), a probabilistic grammar that considers syntactic information is learned using BHMM. In this study, the order of concepts is generated by sampling the learned grammar. The word selection for each generated concept is then performed using the observed data. Moreover, a language model that represents the relationship between words is also used to calculate 2249 Proceedin</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A Fully Bayesian Approach to Unsupervised Part-ofSpeech Tagging. In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics, pp.744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Kawai</author>
<author>Yuji Oshima</author>
<author>Yuki Sasamoto</author>
<author>Yukie Nagai</author>
<author>Minoru Asada</author>
</authors>
<title>Computational Model for Syntactic Development: Identifying How Children Learn to Generalize Nouns and Verbs for Different Languages In</title>
<date>2014</date>
<booktitle>Proc. of Joint IEEE International Conferences on Development and Learning and Epigenetic Robotics,</booktitle>
<pages>78--84</pages>
<contexts>
<context position="3540" citStr="Kawai et al., 2014" startWordPosition="552" endWordPosition="555">ptured. The second problem to be tackled is verbalization of the observed scenes. To solve this problem, a multilayered multimodal latent Dirichlet allocation (mMLDA) was proposed in (Attamimi et al., 2014). In this paper, we propose a sentence generation system from observed scenes in a bottom up manner using mMLDA and a Bayesian hidden Markov model (BHMM) (Goldwater and Griffiths, 2007). To generate sentences from scenes, we need to consider the words that represent the scenes and their order. Here, mMLDA is used to infer words for given scenes. To determine the order of words, inspired by (Kawai et al., 2014), a probabilistic grammar that considers syntactic information is learned using BHMM. In this study, the order of concepts is generated by sampling the learned grammar. The word selection for each generated concept is then performed using the observed data. Moreover, a language model that represents the relationship between words is also used to calculate 2249 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2249–2254, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Language learning and sentence g</context>
</contexts>
<marker>Kawai, Oshima, Sasamoto, Nagai, Asada, 2014</marker>
<rawString>Yuji Kawai, Yuji Oshima, Yuki Sasamoto, Yukie Nagai, and Minoru Asada. 2014. Computational Model for Syntactic Development: Identifying How Children Learn to Generalize Nouns and Verbs for Different Languages In Proc. of Joint IEEE International Conferences on Development and Learning and Epigenetic Robotics, pp.78–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard S Zemel</author>
</authors>
<title>Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models. In Trans. of the Association for Computational Linguistics.</title>
<date>2015</date>
<contexts>
<context position="1819" citStr="Kiros et al., 2015" startWordPosition="263" endWordPosition="266">evaluate the grammar learning and sentence generation as a complete process within a realistic setting. The experimental result reveals the effectiveness of the proposed method. 1 Introduction Describing daily life activities is an important ability of intelligent systems. In fact, we can use this ability to achieve a monitoring system that is able to report on an observed situation, or create an automatic diary of a user. Recently, several studies have been performed to generate sentences that describe images using Deep Learning (Vinyals et al., 2014; Fang et al., 2014; Donahue et al., 2014; Kiros et al., 2015). Although these results were good, we are interested in unsupervised frameworks. This is necessary to achieve a system that can adapt to the user, that is, one that can learn a user-unique language and generate it automatically. Moreover, the use of crowdsourcing should be avoided to respect the privacy of the user. Regarding this, studies on sentence generation from RGB videos have been discussed in (Yu and Siskind, 2013; Regneri et al., 2013). A promising result for language learning has been shown in (Yu and Siskind, 2013) and a quite challenging effort to describe cooking activities was m</context>
</contexts>
<marker>Kiros, Salakhutdinov, Zemel, 2015</marker>
<rawString>Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. 2015. Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models. In Trans. of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Appliying Conditional Random Fields to Japanese Morphological Analysis.</title>
<date>2004</date>
<booktitle>In Proc. of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>230--237</pages>
<contexts>
<context position="6871" citStr="Kudo et al., 2004" startWordPosition="1081" endWordPosition="1084">eatures model (BoF) (Csurka et al., 2004). Using mMLDA (see section 2.2), various concepts can be formed from the multimodal data. Given teaching sentences, the connection between words and concepts can be learned based on mMLDA and BHMM which is learned with mutual information (MI) as the initial value. On the other hand, the bigram model of words is calculated and used as the score when reordering words inferred from multimodal information using grammar. A morphological analyzer for parsing words in a sentence is also necessary in the proposed system. We use publicly available parser MeCab (Kudo et al., 2004). In the future, we plan to use the unsupervised morphological analysis technique proposed in (Mochihashi et al., 2009). 2.2 mMLDA Figure 3 shows the graphical model of mMLDA used in this paper. Here, z represents the integrated category (concept), whereas zO, zM, and zP represent the object, moObject with an RFID tag RFID tag to detect the end of actions Wearable camera Laser range finder Tag reader Gyro-acceleration sensor 2250 tion, and place concepts, respectively. In the bottom layer (lower panel of Figure 3), wm ∈ {wo, wwO, wa, wwM, wl, wwP} represents the multimodal information obtained</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Appliying Conditional Random Fields to Japanese Morphological Analysis. In Proc. of Conference on Empirical Methods in Natural Language Processing, pp.230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling</title>
<date>2009</date>
<booktitle>In Proc. of the Association for Computational Linguistics,</booktitle>
<pages>100--108</pages>
<contexts>
<context position="6990" citStr="Mochihashi et al., 2009" startWordPosition="1100" endWordPosition="1103"> multimodal data. Given teaching sentences, the connection between words and concepts can be learned based on mMLDA and BHMM which is learned with mutual information (MI) as the initial value. On the other hand, the bigram model of words is calculated and used as the score when reordering words inferred from multimodal information using grammar. A morphological analyzer for parsing words in a sentence is also necessary in the proposed system. We use publicly available parser MeCab (Kudo et al., 2004). In the future, we plan to use the unsupervised morphological analysis technique proposed in (Mochihashi et al., 2009). 2.2 mMLDA Figure 3 shows the graphical model of mMLDA used in this paper. Here, z represents the integrated category (concept), whereas zO, zM, and zP represent the object, moObject with an RFID tag RFID tag to detect the end of actions Wearable camera Laser range finder Tag reader Gyro-acceleration sensor 2250 tion, and place concepts, respectively. In the bottom layer (lower panel of Figure 3), wm ∈ {wo, wwO, wa, wwM, wl, wwP} represents the multimodal information obtained from each object, motion, and place. Here, wo, wa, and wl denote multimodal information obtained respectively from the</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling In Proc. of the Association for Computational Linguistics, pp.100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhisa Motooka</author>
</authors>
<title>Ichiro Shiio, Yuji Ohta, Koji Tsukada, Keisuke Kambara, and Masato Iguchi.</title>
<date>2010</date>
<journal>Journal of Asian Architecture and Building Engineering,</journal>
<pages>8--77</pages>
<marker>Motooka, 2010</marker>
<rawString>Nobuhisa Motooka, Ichiro Shiio, Yuji Ohta, Koji Tsukada, Keisuke Kambara, and Masato Iguchi. 2010. Ubiquitous Computing House Project: Design for Everyday Life. Journal of Asian Architecture and Building Engineering, 8:77–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Association for Computational Linguistics, pp.311–</booktitle>
<pages>318</pages>
<contexts>
<context position="17248" citStr="Papineni et al., 2002" startWordPosition="2860" endWordPosition="2863">e, we excluded the functional words (resulting in 78 words) for fair comparison with the baseline method (Attamimi et al., 2014). One can see that, better results can be achieved by the proposed method. It is clear that concept selection is improved by using the BHMM, indicating that a better grammar can be learned using this model. Next, the learned grammar was used and sentences were generated. To reduce randomness of the results, sentence generation was conducted 10 times for each data. To verify sentence generation quantitatively, we evaluated the sentences automatically using BLEU score (Papineni et al., 2002). Figure 5 depicts the results of 2- to 4-gram of BLEU scores. Since functional words are not considered in (Attamimi et al., 2014), we used our grammar and performed sentence generation proposed in (Attamimi et al., 2014) as the baseline method. One can see from the figure that in all cases the BLEU scores of proposed method out performs the baseline method. It can be said that the sentences generated by the proposed method are of better quality than those generated by the baseline method. Moreover, we also manually evaluated generated sentences by asking four subjects (i.e., college students</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proc. of the Association for Computational Linguistics, pp.311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Marcus Rohrbach</author>
<author>Dominikus Wetzel</author>
<author>Stefan Thater</author>
<author>Bernt Schiele</author>
<author>Manfred Pinkal</author>
</authors>
<date>2013</date>
<booktitle>Grounding Action Descriptions in Videos. Trans. of the Association for Computational Linguistics,</booktitle>
<pages>1--25</pages>
<contexts>
<context position="2268" citStr="Regneri et al., 2013" startWordPosition="339" endWordPosition="342">udies have been performed to generate sentences that describe images using Deep Learning (Vinyals et al., 2014; Fang et al., 2014; Donahue et al., 2014; Kiros et al., 2015). Although these results were good, we are interested in unsupervised frameworks. This is necessary to achieve a system that can adapt to the user, that is, one that can learn a user-unique language and generate it automatically. Moreover, the use of crowdsourcing should be avoided to respect the privacy of the user. Regarding this, studies on sentence generation from RGB videos have been discussed in (Yu and Siskind, 2013; Regneri et al., 2013). A promising result for language learning has been shown in (Yu and Siskind, 2013) and a quite challenging effort to describe cooking activities was made in (Regneri et al., 2013). However, these studies rely only on visual information, while we aim to build a system that is able to describe everyday activities using multimodal information. To realize such systems, we need to consider two problems. The first problem is the sensing of daily life activities. In this paper, we utilize a smart house (Motooka et al., 2010) for sensing human activities. Thanks to the smart house, multimodal informa</context>
</contexts>
<marker>Regneri, Rohrbach, Wetzel, Thater, Schiele, Pinkal, 2013</marker>
<rawString>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding Action Descriptions in Videos. Trans. of the Association for Computational Linguistics, 1:25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Alexander Toshev</author>
<author>Samy Bengio</author>
<author>Dumitru Erhan</author>
</authors>
<title>Show and Tell: A Neural Image Caption Generator. In</title>
<date>2014</date>
<note>arXiv:1411.4555 [cs.CV].</note>
<contexts>
<context position="1757" citStr="Vinyals et al., 2014" startWordPosition="251" endWordPosition="254">ent Dirichlet allocation and Bayesian hidden Markov models. We evaluate the grammar learning and sentence generation as a complete process within a realistic setting. The experimental result reveals the effectiveness of the proposed method. 1 Introduction Describing daily life activities is an important ability of intelligent systems. In fact, we can use this ability to achieve a monitoring system that is able to report on an observed situation, or create an automatic diary of a user. Recently, several studies have been performed to generate sentences that describe images using Deep Learning (Vinyals et al., 2014; Fang et al., 2014; Donahue et al., 2014; Kiros et al., 2015). Although these results were good, we are interested in unsupervised frameworks. This is necessary to achieve a system that can adapt to the user, that is, one that can learn a user-unique language and generate it automatically. Moreover, the use of crowdsourcing should be avoided to respect the privacy of the user. Regarding this, studies on sentence generation from RGB videos have been discussed in (Yu and Siskind, 2013; Regneri et al., 2013). A promising result for language learning has been shown in (Yu and Siskind, 2013) and a</context>
</contexts>
<marker>Vinyals, Toshev, Bengio, Erhan, 2014</marker>
<rawString>Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2014. Show and Tell: A Neural Image Caption Generator. In arXiv:1411.4555 [cs.CV].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey M Siskind</author>
</authors>
<title>Grounded Language Learning from Video Described with Sentences.</title>
<date>2013</date>
<booktitle>In Proc. of the Association for Computational Linguistics,</booktitle>
<pages>53--63</pages>
<contexts>
<context position="2245" citStr="Yu and Siskind, 2013" startWordPosition="335" endWordPosition="338">. Recently, several studies have been performed to generate sentences that describe images using Deep Learning (Vinyals et al., 2014; Fang et al., 2014; Donahue et al., 2014; Kiros et al., 2015). Although these results were good, we are interested in unsupervised frameworks. This is necessary to achieve a system that can adapt to the user, that is, one that can learn a user-unique language and generate it automatically. Moreover, the use of crowdsourcing should be avoided to respect the privacy of the user. Regarding this, studies on sentence generation from RGB videos have been discussed in (Yu and Siskind, 2013; Regneri et al., 2013). A promising result for language learning has been shown in (Yu and Siskind, 2013) and a quite challenging effort to describe cooking activities was made in (Regneri et al., 2013). However, these studies rely only on visual information, while we aim to build a system that is able to describe everyday activities using multimodal information. To realize such systems, we need to consider two problems. The first problem is the sensing of daily life activities. In this paper, we utilize a smart house (Motooka et al., 2010) for sensing human activities. Thanks to the smart ho</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and Jeffrey M. Siskind. 2013. Grounded Language Learning from Video Described with Sentences. In Proc. of the Association for Computational Linguistics, pp.53–63.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
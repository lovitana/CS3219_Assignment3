<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.726509">
Mise en Place: Unsupervised Interpretation of Instructional Recipes
</title>
<author confidence="0.52042">
Chlo´e Kiddon†, Ganesa Thandavam Ponnuraj‡, Luke Zettlemoyer†, and Yejin Choi†
</author>
<affiliation confidence="0.343369">
† Computer Science &amp; Engineering, University of Washington, Seattle, WA
</affiliation>
<email confidence="0.773932">
{chloe, lsz, yejin}@cs.washington.edu
</email>
<affiliation confidence="0.886812">
‡ Department of Computer Science, Stony Brook University, Stony Brook, NY
</affiliation>
<email confidence="0.991457">
gthandavam@gmail.com
</email>
<sectionHeader confidence="0.997308" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958380952381">
We present an unsupervised hard EM ap-
proach to automatically mapping instruc-
tional recipes to action graphs, which de-
fine what actions should be performed on
which objects and in what order. Recov-
ering such structures can be challenging,
due to unique properties of procedural lan-
guage where, for example, verbal argu-
ments are commonly elided when they can
be inferred from context and disambigua-
tion often requires world knowledge. Our
probabilistic model incorporates aspects
of procedural semantics and world knowl-
edge, such as likely locations and selec-
tional preferences for different actions.
Experiments with cooking recipes demon-
strate the ability to recover high quality
action graphs, outperforming a strong se-
quential baseline by 8 points in F1, while
also discovering general-purpose knowl-
edge about cooking.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999992905660377">
Instructional language describes how to achieve a
wide variety of goals, from traveling successfully
to a desired location to cooking a particular dish
for dinner. Despite the fact that such language is
important to our everyday lives, there has been rel-
atively little effort to design algorithms that can
automatically convert it into an actionable form.
Existing methods typically assume labeled train-
ing data (Lau et al., 2009; Maeta et al., 2015) or
access to a physical simulator that can be used to
test understanding of the instructions (Branavan et
al., 2009; Chen and Mooney, 2011; Bollini et al.,
2013). In this paper, we present the first approach
for unsupervised learning to interpret instructional
recipes using text alone, with application to cook-
ing recipes.
Given a recipe, our task is to segment it into
text spans that describe individual actions and con-
struct an action graph whose nodes represent ac-
tions and edges represent the flow of arguments
across actions, for example as seen in Fig. 1. This
task poses unique challenges for semantic anal-
ysis. First, null arguments and ellipses are ex-
tremely common (Zwicky, 1988). For example,
sentences such as “Bake for 50 minutes” do not
explicitly mention what to bake or where. Second,
we must reason about how properties of the phys-
ical objects are changed by the described actions,
for example to correctly resolve what the phrase
“the wet mixture” refers to in a baking recipe. Al-
though linguistic context is important to resolving
both of these challenges, more crucial is common
sense knowledge about how the world works, in-
cluding what types of things are typically baked or
what ingredients could be referred to as “wet.”1
These challenges seemingly present a chicken
and egg problem — if we had a high quality se-
mantic analyzer for instructions we could learn
common sense knowledge simply by reading large
bodies of text. However, correctly understand-
ing instructions requires reasoning with exactly
this desired knowledge. We show that this con-
flict can be resolved with an unsupervised learn-
ing approach, where we design models to learn
various aspects of procedural knowledge and then
fit them to unannotated instructional text. Cook-
ing recipes are an ideal domain to study these
two challenges simultaneously, as vast amounts of
recipes are available online today, with significant
redundancy in their coverage that can help boot-
strap the overall learning process. For example,
there are over 400 variations on “macaroni and
cheese” recipes on allrecipes.com, from “chipotle
</bodyText>
<footnote confidence="0.889013">
1The goal of representing common sense world knowl-
edge about actions and objects also drives theories of frame
semantics (Fillmore, 1982) and script knowledge (Schank
and Abelson, 1977). However, our focus is on inducing this
style of knowledge automatically from procedural texts.
</footnote>
<page confidence="0.831975">
982
</page>
<note confidence="0.9925675">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 982–992,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.930868957446808">
e1
e2
e3
e4
e5
v1:preheat a11 s oven
11
v3: press
a51
51 object a52
52 prepositi
1 implicit 1 implicit
v5: bake
s
s
on
v2: mix
v4: lay
a21
1 implicit
a31 s31 object a32
1
a41 41
sbacon a42
1 É 6
s21 ground beef s21
1
1
s32
into loaf pan
s42
over the top
raw
ingre
r
nrn
gd t
dients
Amish Meatloaf (http://allrecipes.com/recipe/amish-meatloaf/)
Ingredients
2 pounds ground beef
2 1/2 cups crushed butter-ßavored crackers
1 small onion, chopped
2 eggs
3/4 cup ketchup
1/4 cup brown sugar
2 slices bacon
</figure>
<bodyText confidence="0.833954055555556">
Preheat the oven to 350 degrees F (175 degrees C).
In a medium bowl, mix together ground beef, crushed
crackers, onion, eggs, ketchup, and brown sugar until
well blended.
Press into a 9x5 inch loaf pan.
Lay the two slices of bacon over the top.
Bake for 1 hour, or until cooked through.
(recipe condensed)
represents an action. The leftmost oval (vi) in each action is the
verb and the following
ovals
represents the
arguments. The yellow ovals represent foods; the grey ovals represent
locations. Argument ovals with dotted boundaries are implicit, i.e., not present in text. The inner white
ovals
are string spans. The red dashed lines represent connections to string spans from their origi-
nating verb or raw ingredient. The string spans also connect to their associated verb in the action diagram
to model the flow of ingredients. For example, there is a directed path from each raw ingredient to the
</bodyText>
<equation confidence="0.9207071">
impli
(ei)
action’s
(aij)
verb’s
(skij)
cit object of bake, representing that the object being baked is composed of all of the raw ingredients.
macaroni and
to
salsa
</equation>
<bodyText confidence="0.999972777777778">
We present two models that are learned with
hard EM algorithms: (1) a segmentation model to
extract the actions from the recipe text, and (2) a
graph model that defines a distribution over the
connections between the extracted actions. The
common sense knowledge is encoded in the sec-
ond model which can, for example, prefer graphs
that model implicit verb arguments when they
better match the learned selectional preferences.
The final action graph is constructed with a local
search algorithm, that allows for global reasoning
about ingredients as they flow through the recipe.
Experiments demonstrate the ability to recover
high quality action graphs, gaining up to 8 points
in F1 over a strong baseline where the ingredients
flow sequentially through the verbs. The learned
models are also highly interpretable, specifying
for example that
</bodyText>
<equation confidence="0.652159666666667">
likely contains
an
cheese,”
“cheesy
mac.”
“dough”
</equation>
<bodyText confidence="0.9627834">
“flour”
d that “add” generally requires two food argu-
ments, even if only one is mentioned in the sen-
tence.
983 arguments flow through a chain of actions; the re-
</bodyText>
<sectionHeader confidence="0.985712" genericHeader="introduction">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.908053">
ding is to recover how different
</bodyText>
<figure confidence="0.964093">
1
brown sugar
</figure>
<figureCaption confidence="0.999862">
Figure 1: An input recipe (left) and a partial corresponding output action graph (right). Each rectangle
</figureCaption>
<bodyText confidence="0.934405444444444">
Procedural text such as a recipe defines a set of
actions, i.e. predicates, applied to a set of objects,
i.e. arguments. A unique challenge in procedu-
ral text understan
the pasta
provide the inputs for future ac-
We represent these corre-
a recipe and a set of connections we can construct
an
</bodyText>
<equation confidence="0.779701">
“Boil
dente.”)
“Drain.”).
</equation>
<bodyText confidence="0.957634333333333">
action graph that models the flow of ingredi-
ents through the recipe. Fig. 1 provides a detailed
running example for the section.
</bodyText>
<subsectionHeader confidence="0.996493">
2.1 Recipe R
</subsectionHeader>
<bodyText confidence="0.9774644">
A recipe R is a piece of text that describes a list
of instructions and a (possibly-empty) set of raw
ingredients that are required to perform the in-
structions. The first step is to segment the text
into a list of verb-argument tuples, called actions,
</bodyText>
<equation confidence="0.959892666666667">
ER =
= (vi, ai), ... ,
Sec. 6
</equation>
<bodyText confidence="0.978977">
will describe an unsupervised approach for learn-
ing to segment recipes. Each action
</bodyText>
<subsectionHeader confidence="0.798281">
pairs a verb
</subsectionHeader>
<bodyText confidence="0.971195888888889">
vi with a list of arguments
where
is the
argument of verb vi. In Fig. 1, each row contains
an action with a verb in the white oval and its ar-
guments in the yellow and gray ovals.
Each argument is a tuple
with a syntactic type tsyn(a) E
{DOBJ, PP}, a semantic type tsem(
</bodyText>
<equation confidence="0.936690428571429">
{e1
en=(vn,an)}.
ei
ai,
aij
jth
aij=(tsyn
ij ,tsem
ij ,Sij)
Tsyn=
a) E
sults of intermediate actions (e.g.,
until al
tions (e.g.,
</equation>
<bodyText confidence="0.999622769230769">
spondences with an action graph. In this section,
we first describe our structured representation of
recipe text, then we define how components of the
recipe connect. Finally, we will show how given
Tsem = {food, location, other}, and a list of
text string spans Sij = {s1ij, ... , s|Si&apos; |ij}, where
skij is the kth span in the jth argument of verb vi.
In Fig. 1, the spans of each argument are repre-
sented by the white ovals inside of the argument
ovals. For example, a21 contains a span for each
raw ingredient being mixed in the second action
(e.g., s121 =“ground beef,” s621 =“brown sugar”).
The syntactic type determines whether the argu-
ment is the direct object or a prepositional phrase
argument of the verb in the recipe text. All other
syntactic constructs are ignored and left for future
work. The semantic types include food, location,
and other. In Fig. 1, yellow ovals represent foods
and gray ovals represent locations. Arguments of
other semantic types are marked as other (e.g.,
“Mash using a fork”).
We also augment the set of arguments for each
verb to include implicit arguments with empty
string spans. This allows making connections to
arguments that the author does not mention explic-
itly (e.g., the elided direct object of “bake” in e5).
Every verb is assigned one implicit PP argument,
and, if a verb has no argument with syntactic type
DOBJ, an implicit direct object. These argu-
ments have indeterminate semantic types, which
are to be determined based on how they connect
to other actions. For example, in Fig. 1, when the
implicit object of “bake” is connected to the out-
put of the “lay” action, it is inferred to be of type
food since that is what is created by the “lay” ac-
tion. However, when the implicit PP argument
of “bake” is connected to the output of the “pre-
heat” action, it is inferred to be a location since
“preheat” does not generate a food.
</bodyText>
<subsectionHeader confidence="0.998549">
2.2 Connections C
</subsectionHeader>
<bodyText confidence="0.999411565217391">
Given a segmented recipe, we can build graph con-
nections. A connection identifies the origin of a
given string span as either the output of a previ-
ous action or as a new ingredient or entity being
introduced into the recipe. A connection is a six-
tuple (o, i, j, k, tsyn, tsem) indicating that there is
a connection from the output of vo to the argu-
ment span skij with syntactic type tsyn E Tsyn
and semantic type tsem E Tsem. We call o the
origin index and i the destination index. For ex-
ample, in Fig. 1, the connection from the output of
the “press” verb (e3) to “over the top” (s142) would
be (3, 4, 2, 1, PP, food). If a span introduces raw
ingredient or new location into the recipe, then
o = 0; in Fig. 1, this occurs for each of the spans
that represent raw ingredients as well as “oven”
and “into loaf pan.”
Given a recipe R, a set of connections C is valid
for R if there is a one-to-one correspondence be-
tween spans in R and connections in C, and if
the origin indexes of connections in C are 0 or
valid verb indexes in R, b(o, i, j, k, tsyn, tsem) E
C, o E {Z 1 0 &lt; o &lt; JERJ}.
</bodyText>
<subsectionHeader confidence="0.998294">
2.3 Action graph G
</subsectionHeader>
<bodyText confidence="0.999944941176471">
A recipe R and a set of connections C define
an action graph, which is a directed graph G =
(V, E). Each raw ingredient, verb, and argu-
ment span is represented by a vertex in V . Each
argument span vertex is connected to its asso-
ciated verb vertex, and each connection c =
(o, i, j, k, tsyn, tsem) adds a corresponding edge
to E. Edges from connections with seman-
tic type food propagate ingredients through the
recipe; edges from connections with semantic type
location propagate a location. Fig. 1 shows an ac-
tion graph. By following the edges, we can tell
that the implicit food entity that is being baked
in the final action has been formed from the set
of ingredients in the mixing action and the bacon
from e4 and that the baking action occurs inside
the oven preheated in e1.
</bodyText>
<sectionHeader confidence="0.992697" genericHeader="method">
3 Probabilistic connection model
</sectionHeader>
<bodyText confidence="0.999982">
Our goal is, given a segmented recipe R, to deter-
mine the most likely set of connections, and thus
the most likely action graph. We model (1) a prior
probability over C, P(C) (Sec. 3.1), and (2) the
probability of seeing a segmented recipe R given
a set of connections C, P(RIC) (Sec. 3.2). The
most likely set of connections will maximize the
joint probability: P(RIC)P(C). A summary of
this model is presented in Fig. 2, and the details
are described in the this section.
</bodyText>
<subsectionHeader confidence="0.999219">
3.1 Connections prior model
</subsectionHeader>
<bodyText confidence="0.999848333333333">
The probability of a set of connections C depends
on features of the incoming set of connections for
each action. Let a destination subset di C C be
the subset of C that contains all connections that
have i as the destination index. In Fig. 1, d3 con-
tains the connection from v2 to the implicit object
as well as a connection to “into loaf pan” with an
origin index of 0. Using the chain rule, the proba-
bility of C is equal to the product of the probability
</bodyText>
<page confidence="0.988367">
984
</page>
<listItem confidence="0.952159666666667">
• Input: A set of connections C and a recipe R segmented (Sec. 6) into its actions {e1 = (v1, a1), ... , en = (vn, an)}
• The joint probability of C and R is P(C, R) = P(C)P(R|C), each defined below:
1. Connections Prior (Sec. 3.1): P(C) = jji P(di|d1, ... , di−1)
Define di as the list of connections with destination index i. Let cp = (o, i, j, k, tsyn, tsem) ∈ di. Then,
• P(di|d1, ... , di−1) = P(vs(di)) flcp∈d, P(1(o → skij)|vs(di), d1, ... , di−1, c1, ... , cp−1)
(a) P(vs(di)): multinomial verb signature model (Sec. 3.1.1)
(b) P(1(o → skij)|vs(di), d1, ... , di−1, c1, ... , cp−1): multinomial connection origin model, conditioned on
the verb signature of di and all previous connections (Sec. 3.1.2)
2. Recipe Model (Sec. 3.2): P(R|C) = Hi P(ei|C, e1, ... , ei−1)
For brevity, define hi = (e1, ... , ei−1).
• P(ei|C, hi) = P(vi|C, hi) flj P(aij|C, hi) (Sec. 3.2)
t3yn t3m Sz
</listItem>
<figure confidence="0.5895280625">
�Define argument aij by its types and s spans, aij = zj U
(a) P(vi|C, hi) = P(vi|gi): multinomial verb distribution conditioned on verb signature (Sec. 3.2)
(b) P(aij|C, hi) = P(tsyn
ij ,tem  |C, hi) fl k P(si � l tsyn tsem C h&apos;)
j 3*j ∈3i7 7 ij , ij , , %
i. P(tsyn
ij , tsem
ij |C, hi): deterministic argument types model given connections (Sec. 3.2.1)
ii. P(sk ij|tsyn
ij , tsem
ij , C, hi): string span model computed by case (Sec. 3.2.2):
A. tsem
ij =food and origin(skij)6=0: IBM Model 1 generating composites (Part-composite model)
B. tsem
ij =food and origin(skij)=0: naive Bayes model generating raw food references (Raw food model)
C. tsem
</figure>
<figureCaption confidence="0.677167">
ij =location: model for generating location referring expressions (Location model)
Figure 2: Summary of the joint probabilistic model P(C, R) over connection set C and recipe R.
of each of the destination subsets:
</figureCaption>
<equation confidence="0.996071">
P (C) = � P(di|d1, ... , di−1).
i
</equation>
<bodyText confidence="0.999963666666667">
The probability of each destination subset de-
composes into two distributions, a verb signature
model and a connection origin model:
</bodyText>
<equation confidence="0.9835172">
P(di|d1, ... , di−1) = P(vs(di))
�× P(1(o → skij)|vs(di), di−1
1 , cp−1
1 ).
cp∈di
</equation>
<bodyText confidence="0.966655666666667">
We define each of these distributions below.
3.1.1 Verb signature model
A destination subset di deterministically defines a
verb signature gi for verb vi based on the syntac-
tic and semantic types of the connections in di as
well as whether or not each connection has a non-
zero origin index. If the origin index is 0 for all
connections in di, we call vi a leaf. (In Fig, 1,
v1 (preheat) and v2 (mix) are leafs.) The formal
definition of a verb signature is as follows:
Definition 1 The verb signature gi for a verb vi
given a destination set di consists of two parts:
</bodyText>
<listItem confidence="0.926275666666667">
1. type: {tsyn  |∃(o, i, j, k, tsyn, food) E di}
2. leaf: true iff (o, i, j, k, tsyn, tsem) E di ⇒
o = 0
</listItem>
<bodyText confidence="0.969545454545455">
For example, in Fig. 1, the signature for the
“mix” action is g2 = ({DOBJ}, true) and
the signature for the “lay” action is g4 =
({DOBJ, PP}, false). Given that there are two
syntactic types (i.e., DOBJ and PP) and each
verb signature can either be labeled as a leaf or
not, there are eight possible verb signatures.
We define a deterministic function that re-
turns the verb signature of a destination subset:
vs(di) = gi. P(vs(di)) is a multinomial distri-
bution over the possible verb signatures.
</bodyText>
<subsectionHeader confidence="0.83682">
3.1.2 Connection origin model
</subsectionHeader>
<bodyText confidence="0.995177315789474">
We define 1(o → skij) as an indicator function that
is 1 if there is a connection from the action with in-
dex o to the span skij. The probability that a string
span has a particular origin depends on (1) the verb
signature of the span’s corresponding verb, and (2)
the previous connections. If, for example, gi has
leaf= true, then the origin of sk ij must be 0. If an
origin has been used in a previous connection, it is
much less likely to be used again.2
We assume that a destination subset is a list of
connections: if cp E di, we define cp−1
1 as the con-
nections that are prior to cp in the list. Similarly,
di−1
1 is the set of destination sets (d1, ... , di−1).
The connection origin model is a multinomial dis-
tribution that defines the probability of an origin
for a span conditioned on the verb signature and
all previous connections:
</bodyText>
<equation confidence="0.942292">
P(1(o → skij)|vs(di), di−1
1 , cp−1
1 ),
</equation>
<footnote confidence="0.985424666666667">
2A counterexample in the cooking domain is separating
egg yolks from egg whites to be used in separate components,
only to be incorporated again in a later action.
</footnote>
<page confidence="0.995296">
985
</page>
<bodyText confidence="0.991552">
where cp = (o, i, j, k, tsyn, tsem).
</bodyText>
<subsectionHeader confidence="0.992585">
3.2 Recipe model
</subsectionHeader>
<bodyText confidence="0.999791571428571">
Given a set of connections C for a recipe R, we
can determine how the actions of the recipe inter-
act and we can calculate the probability of gen-
erating a set of recipe actions ER = {e1 =
(v1, a1), ... , en = (vn,an)J. Intuitively, R is
more likely given C if the destinations of the con-
nections are good text representations of the ori-
gins. For example, a string span “oven” is much
more likely to refer to the output of the action
“Preheat the oven” than “Mix flour and sugar.”
We define the history hi of an action to be the
set of all previous actions: hi = (e1, ... , ei−1).
The probability of a recipe R given a set of con-
nections C can be factored by the chain rule:
</bodyText>
<equation confidence="0.997609">
P(R|C) = � P(ei|C, hi).
i
</equation>
<bodyText confidence="0.9574665">
Given C and a history hi, we assume the verb and
arguments of an action are independent:
</bodyText>
<equation confidence="0.9478745">
P(ei|C, hi) = P(vi|C, hi) � P(aij|C, hi).
j
</equation>
<bodyText confidence="0.983713583333333">
Since the set of connections deterministically de-
fines a verb signature gi for a verb vi, we can sim-
plify P(vi|C, hi) to the multinomial distribution
P(vi|gi). For example, if gi defines the verb to
have an ingredient direct object, then the probabil-
ity of “preheat” given that signature will be lower
than the probability of “mix.”
The probability of an argument aij =
(tsyn
ij , tsem
ij , Sij) given the connections and history
decomposes as follows:
</bodyText>
<equation confidence="0.9494072">
P(aij|C, hi) = P(tsyn
ij , tsem
ij |C, hi)
X P(Sij|tsyn
ij , tsem
</equation>
<bodyText confidence="0.685349">
ij , C, hi).
</bodyText>
<subsectionHeader confidence="0.861429">
3.2.1 Argument types model
</subsectionHeader>
<bodyText confidence="0.992531454545454">
The first distribution, P(tsyn
ij , tsem
ij |C, hi), ensures
that the syntactic and semantic types of the argu-
ment match the syntactic and semantic type of the
incoming connections to spans of that argument.
The probability is 1 if all the types match, 0 oth-
erwise. For example, in Fig. 1, this distribution
would prevent a connection from the “preheat” ac-
tion to the food argument a42, i.e., “over the top,”
since the semantic types would not match.
</bodyText>
<subsectionHeader confidence="0.83421">
3.2.2 String span models
</subsectionHeader>
<bodyText confidence="0.991377276595745">
The second distribution, P(Sij|tsyn
ij , tsem
ij , C, hi),
models how likely it is to generate a particular
string span given the types of its encompassing ar-
gument, the connections, and history. We assume
the probability of each span is independent:
P(Sij |tijn t j C hi) = T7 P(sk syn sem
ij |tij , tij , C, hi).
skij∈Sij
We break this distribution into three cases. To
help describe the separate cases we define the
function origin(s, C) to determine the origin in-
dex of the connection in C to the span s. That is,
origin(skij, C)=o�? ](o, i, j, k, tsyn, tsem) E C.
Part-composite model When the encompassing
argument is a food and the origin is a previous verb
(i.e., P(sk  |tsjn, Z� m = food, origin (s kij) �
0, C, hi)), tijhen the probability of the span depends
on the ingredients that the span represents given
the connections in C. For example, “dressing” is
more likely given ingredients “oil” and “vinegar”
than given “chicken” and “noodles”. We use IBM
Model 1 (Brown et al., 1993) to model the prob-
ability of a composite destination phrase given a
set of origin food tokens. Let food(skij, C) be the
set of spans in food arguments such that there is
a directed path from those arguments to skij. IBM
Model 1 defines the probability of a span given the
propagated food spans, P(sk ij|food(skij, C)).3
Raw food model When the encompassing ar-
gument is a food but the origin index is 0
(i.e., P(sk ij|tsyn
ij , tsem
ij = food, origin(skij) =
0, C, hi)), then there is no flow of ingredients into
the span. A span that represents a newly intro-
duced raw ingredient (e.g., “bacon” in e4 of Fig. 1)
should have a high probability. However, spans
that denote the output of actions (e.g, ‘batter,” “ba-
nana mixture”) should have low probability. We
use a naive Bayes model over the tokens in the
span P(s|is raw) = H` P(w`|is raw) where w` is
the ith token in s (e.g., “mixture” would have a
very low probability but “flour” would be likely).
Location model When the encompassing ar-
gument is a location (i.e., tsem
</bodyText>
<equation confidence="0.714927">
ij = location),
</equation>
<bodyText confidence="0.935259833333333">
3IBM Model 1 cannot handle implicit arguments. In this
case, we model the probability of having an implicit food ar-
gument given the length of the connection (i.e., implicit food
arguments nearly deterministically connect to the previous
action). The probability of non-empty string spans is scaled
accordingly to ensure a valid probability distribution.
</bodyText>
<page confidence="0.992904">
986
</page>
<figureCaption confidence="0.9990085">
Figure 3: The three types of search operators. For
swaps, one of the origins can be 0.
</figureCaption>
<bodyText confidence="0.954575777777778">
P(Sij|tsyn
ij ,tsem
ij ,C, h) models the appropriate-
ness of the origin action’s location for the destina-
tion. If the string span is not implicit, the model
deterministically relies on string match between
the span and the location argument of the verb
at the origin index. For example, the probability
of “the preheated oven” conditioned on an origin
with location “oven” is 1, but 0 for an origin with
location “bowl.” If the span skij is empty, we use
a multinomial model P(loc(origin(sk ij, C))|vi)
that determines how likely it is that an action vi
occurs in the location of the origin verb. For ex-
ample, baking generally happens in an oven and
grilling on a grill, but not vice versa. For example,
in Fig. 1, the probability of the location span of
“bake” is determined by P(“oven”  |“bake”).
</bodyText>
<sectionHeader confidence="0.997772" genericHeader="method">
4 Local Search
</sectionHeader>
<bodyText confidence="0.999347">
Connections among actions and arguments iden-
tify which ingredients are being used by which
action. For example, in Fig. 1, we know that we
are baking something that contains all the ingre-
dients introduced in e2 and e4 because there is a
path of connections from the introduction of the
raw ingredients to the implicit object of “bake”. We
cannot make decisions about the origins of argu-
ments independently; the likelihood of each edge
depends on the other edges. Identifying the most
likely set of connections is, therefore, intractable.
We adopt a local search approach to infer the
best set of connections.4 We initialize the set of
</bodyText>
<footnote confidence="0.9717">
4Similar local search methods have been shown to work
well for other NLP tasks, including recent work on depen-
</footnote>
<bodyText confidence="0.893529285714286">
Algorithm 1 Pseudocode for learning P(C, R)
Input: Initialized P(C, R), recipe dataset R
Repeat until convergence:
E-step: Update C ( arg maxC P(C, R)
for each R E R using local search (Sec. 4)
M-step: Update parameters of P(C, R)
using action graphs generated in E-step
</bodyText>
<equation confidence="0.323764">
Return P(C, R)
</equation>
<bodyText confidence="0.9988617">
connections using a sequential algorithm that con-
nects the output of each event to an argument of
the following event, which is a strong baseline as
shown in Sec. 8. We score potential local search
operators that can be applied to the current set of
connections C and make a greedy selection that
improves P(C, R) the most until no search opera-
tor can improve the probability. We constrain the
search so all verbs have a direct object (i.e., im-
plicit direct objects connect to a previous action).
We employ three types of search operators (see
Fig. 3 for details). OP ADD changes the origin in-
dex of a connection in C from 0 to the index of
an event. OP 2SWAP swaps the origin indexes of
two connections. This works even if one of the
origin indexes is 0. OP 3SWAP rotates the origin
indexes of three connections. This works even if
one of the origin indexes is 0. For efficiency rea-
sons, we only allow 3-way swaps with destination
indexes within 4 events of each other.
</bodyText>
<sectionHeader confidence="0.994893" genericHeader="method">
5 Learning
</sectionHeader>
<bodyText confidence="0.999971181818182">
We use hard EM to learn the probabilistic mod-
els. Pseudocode is given in Alg. 1. At each itera-
tion, we use our local search algorithm and the cur-
rent probabilistic models to annotate each recipe
in the data set with its most likely set of connec-
tions C (Sec. 4). Then, we re-estimate the param-
eters of the probabilistic models using the recipe-
connections pairs as training data. A small (33
recipes) development set was used to determine
when to stop the iterations. Experimental details
and model initialization are described in Sec. 7.
</bodyText>
<sectionHeader confidence="0.997219" genericHeader="method">
6 Segmentation
</sectionHeader>
<bodyText confidence="0.9460945">
Our inference and learning algorithms assume as
input a recipe segmented into a set of events ER =
{(v1, a1), ... , (vn, an)1. We designed a segmen-
tation system that could be trained on our un-
annotated data set of mostly imperative sentences.
dency parsing (Zhang et al., 2014).
</bodyText>
<figure confidence="0.998284076923077">
v v’ v’’ v v’ v’’
S S’
v
v v’ v v’
S S’
S S’
2-way swap
3-way swap
S’’ S S’ S’’
Add
v
S S’
S S’
</figure>
<page confidence="0.991708">
987
</page>
<bodyText confidence="0.999825">
Our system achieves an F1 score of 95.6% on the
task of identifying the correct verbs in the test set.5
Segmentation model We define a generative
model for recipes as:
</bodyText>
<equation confidence="0.994272333333333">
n
P(R) = P(n) ri P(vi)P(m  |vi)
i
</equation>
<bodyText confidence="0.99896796875">
We first select a number of verbs n in the recipe
from a geometric distribution. Given the number
of verbs, we select a set of verbs V = {v1, ... , vn}
using a multinomial distribution. For each verb vi,
we select a number of arguments m from a sep-
arate multinomial distribution that has the prob-
ability of 0, 1, 2, or 3+ arguments given the
verb, P(m  |vi). For each argument, we gen-
erate a string using a bigram model, P(aij) =
Ht P(wt|wt−1), where wt is the SCh word of aij.
Inference Given tokenized sentence T =
(w1, ... , wk), we enumerate all possible segmen-
tations and choose the one with the highest prob-
ability. To keep this efficient, we use a closed set
of possible verbs and assume a closed set of words
(e.g., prepositions, adverbs) can only follow the
start token in the argument bigram model. Thus,
annotating the verbs in a sentence determines a
unique set of argument strings. Despite scoring
the segmentations for all possible sets of verbs, we
found the process to be very efficient in practice.
Learning For unsupervised learning, we again
employ a hard EM approach. We initialize our
models, segment all of the training data, re-
estimate the parameters, and iterate these steps un-
til performance on a development set converges.
We estimate the initial verb multinomial model
using counts from the first word of each sentence
in the dataset, which are normally verbs in imper-
ative sentences, and filter out any words that have
no verb synsets in WordNet (Miller, 1995). All
other models are initialized to be uniform.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9919878">
Data Set We collected 2456 recipes (with over
23,000 sentences) from allrecipes.com by search-
ing for 20 dish names (e.g., including “banana
muffins”, and “deviled eggs”). We randomly sam-
pled, removed, and hand labeled 33 recipes for a
</bodyText>
<footnote confidence="0.701627">
5Early efforts using a state-of-the-art parser could only
achieve an F1 score of 73.6% for identifying verbs, likely due
to a lack of imperative sentences in the training data. This
result motivated us to develop our segmentation system.
</footnote>
<bodyText confidence="0.999968816326531">
development set and 100 recipes for test. All mod-
els were trained on the unannotated recipes; the
dev set was used to determine the stopping point
for training. Each recipe in the test set has 13 ac-
tions on average.
Recipe pre-processing To pre-process each
recipe, we first use the segmentation system de-
scribed in Sec. 6. Then, we use a string classifi-
cation model to determine the semantic type (e.g.,
food, location, or other) of an argument based
on its spans. We identify spans as raw ingredients
based on string match heuristics (e.g., in Fig. 1, the
span “crushed crackers” represents the ingredients
“crushed butter-flavored crackers”). We stem all
words and ignore function words.
Sequential Baseline Because most connections
are sequential – i.e., argument spans are most of-
ten connected to the output of the previous verb
– sequential connections make a strong baseline;
we connect the output of each predicate to the first
available argument span of the following predi-
cate. If no argument exists, an implicit argument is
created. We run this baseline with and without first
identifying raw ingredients in the recipe; if raw in-
gredient spans are identified, the baseline will not
connect the previous event to those spans. Perfor-
mance suffers significantly if the raw ingredients
are not identified beforehand.
Evaluation metrics We report F-measure by
comparing the predicted connections from actions
to spans (i.e., where the origin index &gt; 0) against
gold standard annotations. We don’t evaluate con-
nections to raw ingredients as we create those con-
nections during pre-processing (see Sec. 7).
Model initialization The verb signature model
(Sec. 3.2) is initialized by first identifying food
arguments using string overlap with the ingredi-
ent list. All other arguments’ types are considered
unknown, and partial counts were awarded to all
verb signatures consistent with the partial infor-
mation. The first verb in each recipe was assumed
to be the only leaf. The string classification model
for the pre-processing step was initialized by us-
ing the initialized verb signature model to identify
the types of DOBJ arguments. The string classi-
fication model was estimated using the argument
tokens given the types. We initialized the part-
composite model (Sec. 3.2.2) so that exact string
matches between ingredients and spans are given
</bodyText>
<equation confidence="0.86373175">
m
H
j=1
P(aij).
</equation>
<page confidence="0.985128">
988
</page>
<table confidence="0.987342454545454">
Algorithm Prec Rec F1
Automatic segmentations
Sequential baseline 55.7 52.7 54.1
Sequential baseline w/ ingredients 60.4 57.2 58.8
Our model before EM 65.8 62.7 64.2
Our model after EM 68.7 65.0 66.8
Oracle segmentations
Sequential baseline 67.8 65.2 66.5
Sequential baseline w/ ingredients 73.5 70.7 72.0
Our model before EM 77.1 74.8 75.9
Our model after EM 81.6 78.5 80.0
</table>
<tableCaption confidence="0.9930555">
Table 1: Performance of our algorithm against the
sequential baselines.
</tableCaption>
<table confidence="0.99707875">
Verb Top location tokens
bake oven - 55.4% min - 0.7%
mix bowl - 32.6% hand - 0.9%
press pan - 24.7% dish - 6.5%
stir bowl - 5.5% skillet - 2.0%
fry heat - 11.9% skillet - 10.2%
cool rack - 10.5% pan - 3.8%
boil water - 15.5% saucepan - 5.2%
</table>
<tableCaption confidence="0.986866">
Table 2: The top scoring location token for exam-
</tableCaption>
<bodyText confidence="0.928304555555555">
ple verbs. The percentage is the percent of times
the verb has that as a visible location token.
high probabilities and those without are given low
probabilities. Given the initialized string classifi-
cation model, the raw food model (Sec. 3.2.2) is
initialized counting whether or not tokens in food
arguments occur in the ingredient list. The proba-
bility of an implicit location (Sec. 3.2.2) is initial-
ized to a hand-tuned value using the dev set.
</bodyText>
<sectionHeader confidence="0.999784" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.998973277777778">
Quantitative Results We trained our model for
four iterations of hard EM until performance con-
verged on the development set. Table 1 presents
our results on the test set. We compare our model
to the sequential baselines using both the output
of our segmentation system and oracle segmen-
tations. We perform significantly better than the
sequential baselines, with an increase in F1 of
8 points over the more competitive baseline us-
ing our segmentation system and an increase of 8
points using the oracle segmentations.
Qualitative Results We find that the learned
models demonstrate interpretable cooking knowl-
edge. Table 3 shows the top composite tokens
for different ingredients as learned by the part-
composite model (Sec. 3.2.2). The composite
tokens show parts of the ingredient (e.g., after
“eggs” can be split into “whites” or “yolks”) or
</bodyText>
<table confidence="0.572052090909091">
Verb Top verb signature (%)
add {DOBJ, PP} 58%
{DOBJ} 27%
combine {DOBJ}:leaf 68%
{DOBJ} 17%
bake {DOBJ} 95%
grease {}:leaf 75%
pour {DOBJ, PP} 68%
{DOBJ} 27%
reduce {PP} 90%
{DOBJ} 8%
</table>
<tableCaption confidence="0.935901">
Table 4: The top verb signatures for example
</tableCaption>
<bodyText confidence="0.998317815789474">
verbs. The syntactic types identify which argu-
ments of the verb are foods and “leaf” means no
arguments of the verb connect to previous actions.
composites that are likely to contain an ingredi-
ent (e.g., “flour” is generally found in “batter”
and “dough”). Unsurprisingly, the word “mixture”
is one of the top words to describe a combina-
tion of ingredients, regardless of the ingredient.
The model also learns modifiers that describe key
properties of ingredients (e.g., flour is “dry” but
bananas are “wet”) which is important when eval-
uating connections for sentences such as “Fold the
wet mixture into the dry ingredients.”
Table 2 shows the location preferences of verbs
learned by the location model (Sec. 3.2.2). Some
verbs show strong preferences on locations (e.g.,
“bake” occurs in an oven, “mix” in a bowl). The
top location for a “boil” action is in “water,” but in
other recipes “water” is an ingredient.
Finally, Table 4 shows learned verb signatures.
For example, “add” tends to be a non-leaf action,
and can take one or two food arguments (e.g.,
one food argument: “Heat the pan. Add onions.”
vs. two food arguments: “Add the wet mixture
to the dry mixture.”) We learn that the most likely
verb signature for “add” has two food arguments;
since over 74% of the occurrences of “add” in the
dataset only have one visible argument, the seg-
mentation alone is not enough to determine the
signature.
Errors Finally, we performed an error analysis
on the development set. 24% of the errors were
due to missing or incorrect actions caused by seg-
mentation errors. Among the actions that were
segmented correctly, 82% of the outgoing connec-
tions were sequential. Of those, our system missed
17.6% of the sequential connections and 18.3% of
the non-sequential connections.
</bodyText>
<page confidence="0.995159">
989
</page>
<table confidence="0.670851125">
Ingredient Top composite tokens
eggs egg, yolk, mixture, noodles, whites, cook, top, potato, cold, fill
beef beef, mixture, grease, meat, excess, cook, top, loaf, sauce, ground
flour flour, mixture, dough, batter, top, crust, ingredients, sauce, dry, pie
noodles noodles, cook, mixture, egg, sauce, top, meat, drain, pasta, layer
chicken chicken, mixture, salad, cook, dressing, pasta, soup, breast, vegetables, noodles
pumpkin pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough
bananas banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice
</table>
<tableCaption confidence="0.999391">
Table 3: Examples of ingredients with their top inferred composite words.
</tableCaption>
<sectionHeader confidence="0.999666" genericHeader="evaluation">
9 Related work
</sectionHeader>
<bodyText confidence="0.999977578947368">
Our work relates to a substantial body of research
that transforms natural language instructions into
actionable plans (Artzi and Zettlemoyer, 2013,
Chen and Mooney, 2011, Branavan et al., 2011,
Branavan et al., 2009, McMahon et al., 2006).
Most of these approaches do interactive learning
in virtual environments or simulations, while we
learn from the redundancy seen in the text of dif-
ferent instances of similar recipes.
There is also significant related work on su-
pervised learning for instructions. A recent se-
ries of studies have explored parsing of cook-
ing recipes (Mori et al., 2012; Mori et al., 2014;
Maeta et al., 2015). However, they assume anno-
tated data, study Japanese recipes, and make edge
connections independently without taking into ac-
count the flow of ingredients. Tasse and Smith
(2008) develops annotation for English recipes,
but do not mark connections from implicit roles,
and only studied segmentation models. Lau et
al. (2009) develop models to interpret how-to in-
structions, but also assume supervision, and do not
make connections between different actions.
Data-driven extraction of cooking knowledge
has been explored in the context of building a
cooking ontology (Gaillard et al., 2012; Nanba et
al., 2014). In contrast, our work induces prob-
abilistic cooking knowledge as part of unsuper-
vised learning process for understanding recipes.
Cooking knowledge is also closely related to
script knowledge, but most prior work focus on
newswire and children’s books rather than proce-
dural language (Fujiki et al., 2003; Chambers and
Jurafsky, 2009; Pichotta and Mooney, 2014; Bala-
subramanian et al., 2013) or rely on crowdsourced
descriptions to learn procedural knowledge (Reg-
neri et al., 2010; Regneri et al., 2011; Frermann
et al., 2014). There is work on related, but dis-
tinct, tasks that use recipes, including identifying
actionable refinements from online recipe reviews
(Druck and Pang, 2012) and extracting structured
information from ingredient lists (Greene, 2015)
Cooking recipes have also been studied in the
context of grounded language learning, e.g., to
build robots that can cook (e.g., Bollini et al.,
2013, Beetz et al., 2011), or to align cooking
videos to natural language descriptions of actions
(Regneri et al., 2013) or recipe texts (Malmaud et
al., 2014; Malmaud et al., 2015). Our work com-
plements these efforts by recovering fine-grained
procedural semantics from text alone.
Finally, detection and resolution of implicit ar-
guments is an instance of zero anaphora detec-
tion and resolution (Silberer and Anette, 2012,
Tetreault 2002, Whittemore et al., 1991, Palmer et
al., 1986). We present an empirical approach for
understanding these phenomena in instructions.
</bodyText>
<sectionHeader confidence="0.994787" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999982571428571">
We presented unsupervised methods for segment-
ing and identifying latent connections among ac-
tions in recipe text. Our model outperformed a
strong linear baseline, while learning a variety of
domain knowledge, such as verb signatures and
probable ingredient components for different com-
posites. Future work includes learning a more
comprehensive model of locations (e.g., identify-
ing nested locations such as an oven and a pan in
the oven), enriching action graphs with greater se-
mantic coverage (e.g., durations, tools, amounts),
and training and evaluating on larger datasets. We
also plan to use our techniques to support related
tasks, such as instructional recipe generation.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995825">
We thank the anonymous reviewers, Mike Lewis,
Dan Weld, Yoav Artzi, Antoine Bosselut, Kenton
Lee, Luheng He, Mark Yatskar, and Gagan Bansal
for helpful comments, and Polina Kuznetsova for
the preliminary work. This research was sup-
ported in part by the Intel Science and Technol-
ogy Center for Pervasive Computing (ISTC-PC)
and the NSF (IIS-1252835 and IIS-1524371).
</bodyText>
<page confidence="0.994662">
990
</page>
<sectionHeader confidence="0.995736" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998692138888889">
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49–62.
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In Proceedings
of the 2013 Conference on Empirical Methods on
Natural Language Processing, pages 1721–1731.
Michael Beetz, Ulrich Klank, Ingo Kresse, Alexis Mal-
donado, Lorenz Mosenlechner, Dejan Pangercic,
Thomas Ruhr, and Moritz Tenorth. 2011. Robotic
roommates making pancakes. In Proceedings of
the 11th IEEE-RAS International Conference on Hu-
manoid Robots (Humanoids), pages 529–536.
Mario Bollini, Stefanie Tellex, Tyler Thompson,
Nicholas Roy, and Daniela Rus. 2013. Interpreting
and executing recipes with a cooking robot. Experi-
mental Robotics, 88:481–495.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 1 - Volume 1, pages 82–90.
S.R.K. Branavan, David Silver, and Regina Barzilay.
2011. Non-linear monte-carlo search in civiliza-
tion ii. In Proceedings of the Twenty-Second Inter-
national Joint Conference on Artificial Intelligence,
pages 2404–2410.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263–311.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing
of the Asian Federation of Natural Language Pro-
cessing, pages 602–610.
David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011), pages 859–865.
Gregory Druck and Bo Pang. 2012. Spice it up? Min-
ing refinements to online instructions from user gen-
erated content. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, pages 545–553.
Charles J. Fillmore, 1982. Frame semantics, pages
111–137. Hanshin Publishing Co., Seoul, South Ko-
rea.
Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014.
A hierarchical bayesian model for unsupervised in-
duction of script knowledge. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 49–
57.
Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Automatic acquisition of script knowl-
edge from a text collection. In Proceedings of the
Tenth Conference on European Chapter of the As-
sociation for Computational Linguistics - Volume 2,
pages 91–94.
Emmanuelle Gaillard, Emmanuel Nauer, Marie
Lefevre, and Am´elie Cordier. 2012. Extracting
generic cooking adaptation knowledge for the
TAAABLE case-based reasoning system. In
Proceedings of the 1st Workshop on Cooking with
Computers (CwC).
Erica Greene. 2015. Extracting structured data from
recipes using conditional random fields. The New
York Times Open Blog.
TA Lau, Clemens Drews, and Jeffrey Nichols. 2009.
Interpreting written how-to instructions. In Pro-
ceedings of the Twenty-First International Joint
Conference on Artificial Intelligence, pages 1433–
1438.
Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: Connecting language,
knowledge, and action in route instructions. In Pro-
ceedings of the 21st National Conference on Artifi-
cial Intelligence - Volume 2, AAAI’06, pages 1475–
1482. AAAI Press.
Hirokuni Maeta, Tetsuro Sasada, and Shinsuke Mori.
2015. A framework for procedural text understand-
ing. In Proceedings of the 14th International Con-
ference on Parsing Technologies, pages 50–60.
Jon Malmaud, Earl J. Wagner, Nancy Chang, and
Kevin Murphy. 2014. Cooking with semantics. In
Proceedings of the ACL 2014 Workshop on Semantic
Parsing, pages 33–38.
Jonathan Malmaud, Jonathan Huang, Vivek Rathod,
Nick Johnston, Andrew Rabinovich, and Kevin
Murphy. 2015. What’s cookin’? Interpreting cook-
ing videos using text, speech and vision. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
143–152.
George A. Miller. 1995. WordNet: A lexical
database for english. Communications of the ACM,
38(11):39–41.
</reference>
<page confidence="0.976106">
991
</page>
<reference confidence="0.999837974683544">
Shinsuke Mori, Tetsuro Sasada, Yoko Yamakata, and
Koichiro Yoshino. 2012. A machine learning ap-
proach to recipe text processing. In Proceedings
of the 1st Workshop on Cooking with Computers
(CwC).
Shinsuke Mori, Hirokuni Maeta, Yoko Yamakata, and
Tetsuro Sasada. 2014. Flow graph corpus from
recipe texts. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), pages 26–31.
Hidetsugu Nanba, Yoko Doi, Miho Tsujita, Toshiyuki
Takezawa, and Kazutoshi Sumiya. 2014. Construc-
tion of a cooking ontology from cooking recipes and
patents. In Proceedings the 2014 ACMInternational
Joint Conference on Pervasive and Ubiquitous Com-
puting: Adjunct Publication, pages 507–516.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and
John Dowding. 1986. Recovering implicit infor-
mation. In Proceedings of the 24th Annual Meeting
on Association for Computational Linguistics, pages
10–19.
Karl Pichotta and Raymond Mooney. 2014. Statisti-
cal script learning with multi-argument events. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 220–229.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 979–988.
Michaela Regneri, Alexander Koller, Josef Ruppen-
hofer, and Manfred Pinkal. 2011. Learning script
participants from unlabeled data. In Proceedings of
the Conference on Recent Advances in Natural Lan-
guage Processing, pages 463–470.
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Com-
putational Linguistics (TACL), Volume 1., 1:25–36.
Roger Carl Schank and Robert P. Abelson. 1977.
Scripts, plans, goals and understanding : an inquiry
into human knowledge structures. The Artificial in-
telligence series. L. Erlbaum, Hillsdale, N.J.
Carina Silberer and Anette Frank. 2012. Casting im-
plicit role linking as an anaphora resolution task. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics - Volume 1: Proceed-
ings of the Main Conference and the Shared Task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation, pages 1–
10.
Dan Tasse and Noah A. Smith. 2008. SOUR CREAM:
Toward semantic processing of recipes. Technical
Report CMU-LTI-08-005, Carnegie Mellon Univer-
sity, Pittsburgh, PA.
Joel R. Tetreault. 2002. Implicit role reference. In
Proceedings of the International Symposium on Ref-
erence Resolution for Natural Language Processing,
pages 109–115.
Greg Whittemore, Melissa Macpherson, and Greg
Carlson. 1991. Event-building through role-filling
and anaphora resolution. In Proceedings of the 29th
Annual Meeting on Association for Computational
Linguistics, pages 17–24.
Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2014. Greed is good if randomized: New
inference for dependency parsing. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1013–
1024.
Arnold M. Zwicky. 1988. On the subject of bare im-
peratives in english. In C. Duncan-Rose and T. Ven-
nemann, editors, On Language: Rhetorica, Phono-
logica, Syntactica - A Festschriftfor Robert P. Stock-
well from His Friends and Colleagues, pages 437–
450. Routledge, London.
</reference>
<page confidence="0.99734">
992
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.532472">
<title confidence="0.999151">en Unsupervised Interpretation of Instructional Recipes</title>
<author confidence="0.862404">Ganesa Thandavam Luke</author>
<author confidence="0.862404">Yejin</author>
<affiliation confidence="0.718587">Science &amp; Engineering, University of Washington, Seattle,</affiliation>
<email confidence="0.923097">lsz,</email>
<affiliation confidence="0.878412">of Computer Science, Stony Brook University, Stony Brook,</affiliation>
<email confidence="0.999698">gthandavam@gmail.com</email>
<abstract confidence="0.998266181818182">We present an unsupervised hard EM approach to automatically mapping instructional recipes to action graphs, which define what actions should be performed on which objects and in what order. Recovering such structures can be challenging, due to unique properties of procedural language where, for example, verbal arguments are commonly elided when they can be inferred from context and disambiguation often requires world knowledge. Our probabilistic model incorporates aspects of procedural semantics and world knowledge, such as likely locations and selectional preferences for different actions. Experiments with cooking recipes demonstrate the ability to recover high quality action graphs, outperforming a strong sequential baseline by 8 points in F1, while also discovering general-purpose knowledge about cooking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="34928" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="6092" endWordPosition="6095">e, dough, batter, top, crust, ingredients, sauce, dry, pie noodles noodles, cook, mixture, egg, sauce, top, meat, drain, pasta, layer chicken chicken, mixture, salad, cook, dressing, pasta, soup, breast, vegetables, noodles pumpkin pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough bananas banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without takin</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niranjan Balasubramanian</author>
<author>Stephen Soderland</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Generating coherent event schemas at scale.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>1721--1731</pages>
<contexts>
<context position="36414" citStr="Balasubramanian et al., 2013" startWordPosition="6325" endWordPosition="6329"> but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language description</context>
</contexts>
<marker>Balasubramanian, Soderland, Mausam, Etzioni, 2013</marker>
<rawString>Niranjan Balasubramanian, Stephen Soderland, Mausam, and Oren Etzioni. 2013. Generating coherent event schemas at scale. In Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing, pages 1721–1731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Beetz</author>
<author>Ulrich Klank</author>
<author>Ingo Kresse</author>
<author>Alexis Maldonado</author>
<author>Lorenz Mosenlechner</author>
<author>Dejan Pangercic</author>
<author>Thomas Ruhr</author>
<author>Moritz Tenorth</author>
</authors>
<title>Robotic roommates making pancakes.</title>
<date>2011</date>
<booktitle>In Proceedings of the 11th IEEE-RAS International Conference on Humanoid Robots (Humanoids),</booktitle>
<pages>529--536</pages>
<contexts>
<context position="36954" citStr="Beetz et al., 2011" startWordPosition="6410" endWordPosition="6413">rs and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. 10 Conclusion We presented unsupervised methods for se</context>
</contexts>
<marker>Beetz, Klank, Kresse, Maldonado, Mosenlechner, Pangercic, Ruhr, Tenorth, 2011</marker>
<rawString>Michael Beetz, Ulrich Klank, Ingo Kresse, Alexis Maldonado, Lorenz Mosenlechner, Dejan Pangercic, Thomas Ruhr, and Moritz Tenorth. 2011. Robotic roommates making pancakes. In Proceedings of the 11th IEEE-RAS International Conference on Humanoid Robots (Humanoids), pages 529–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Bollini</author>
<author>Stefanie Tellex</author>
<author>Tyler Thompson</author>
<author>Nicholas Roy</author>
<author>Daniela Rus</author>
</authors>
<title>Interpreting and executing recipes with a cooking robot. Experimental Robotics,</title>
<date>2013</date>
<pages>88--481</pages>
<contexts>
<context position="1809" citStr="Bollini et al., 2013" startWordPosition="268" endWordPosition="271">oduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. Given a recipe, our task is to segment it into text spans that describe individual actions and construct an action graph whose nodes represent actions and edges represent the flow of arguments across actions, for example as seen in Fig. 1. This task poses unique challenges for semantic analysis. First, null arguments and ellipses are extremely common (Zwicky, 1988). For example, sentences such as “Bake for 50 minutes” do not explicit</context>
<context position="36933" citStr="Bollini et al., 2013" startWordPosition="6406" endWordPosition="6409">i et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. 10 Conclusion We presented unsupe</context>
</contexts>
<marker>Bollini, Tellex, Thompson, Roy, Rus, 2013</marker>
<rawString>Mario Bollini, Stefanie Tellex, Tyler Thompson, Nicholas Roy, and Daniela Rus. 2013. Interpreting and executing recipes with a cooking robot. Experimental Robotics, 88:481–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>82--90</pages>
<contexts>
<context position="1763" citStr="Branavan et al., 2009" startWordPosition="260" endWordPosition="263">eneral-purpose knowledge about cooking. 1 Introduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. Given a recipe, our task is to segment it into text spans that describe individual actions and construct an action graph whose nodes represent actions and edges represent the flow of arguments across actions, for example as seen in Fig. 1. This task poses unique challenges for semantic analysis. First, null arguments and ellipses are extremely common (Zwicky, 1988). For example, sentences</context>
<context position="34997" citStr="Branavan et al., 2009" startWordPosition="6104" endWordPosition="6107"> cook, mixture, egg, sauce, top, meat, drain, pasta, layer chicken chicken, mixture, salad, cook, dressing, pasta, soup, breast, vegetables, noodles pumpkin pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough bananas banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develo</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>David Silver</author>
<author>Regina Barzilay</author>
</authors>
<title>Non-linear monte-carlo search in civilization ii.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2404--2410</pages>
<contexts>
<context position="34974" citStr="Branavan et al., 2011" startWordPosition="6100" endWordPosition="6103">y, pie noodles noodles, cook, mixture, egg, sauce, top, meat, drain, pasta, layer chicken chicken, mixture, salad, cook, dressing, pasta, soup, breast, vegetables, noodles pumpkin pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough bananas banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse </context>
</contexts>
<marker>Branavan, Silver, Barzilay, 2011</marker>
<rawString>S.R.K. Branavan, David Silver, and Regina Barzilay. 2011. Non-linear monte-carlo search in civilization ii. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, pages 2404–2410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="20212" citStr="Brown et al., 1993" startWordPosition="3584" endWordPosition="3587">lp describe the separate cases we define the function origin(s, C) to determine the origin index of the connection in C to the span s. That is, origin(skij, C)=o�? ](o, i, j, k, tsyn, tsem) E C. Part-composite model When the encompassing argument is a food and the origin is a previous verb (i.e., P(sk |tsjn, Z� m = food, origin (s kij) � 0, C, hi)), tijhen the probability of the span depends on the ingredients that the span represents given the connections in C. For example, “dressing” is more likely given ingredients “oil” and “vinegar” than given “chicken” and “noodles”. We use IBM Model 1 (Brown et al., 1993) to model the probability of a composite destination phrase given a set of origin food tokens. Let food(skij, C) be the set of spans in food arguments such that there is a directed path from those arguments to skij. IBM Model 1 defines the probability of a span given the propagated food spans, P(sk ij|food(skij, C)).3 Raw food model When the encompassing argument is a food but the origin index is 0 (i.e., P(sk ij|tsyn ij , tsem ij = food, origin(skij) = 0, C, hi)), then there is no flow of ingredients into the span. A span that represents a newly introduced raw ingredient (e.g., “bacon” in e4 </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,</booktitle>
<pages>602--610</pages>
<contexts>
<context position="36356" citStr="Chambers and Jurafsky, 2009" startWordPosition="6317" endWordPosition="6320"> (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), </context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, pages 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI2011),</booktitle>
<pages>859--865</pages>
<contexts>
<context position="1786" citStr="Chen and Mooney, 2011" startWordPosition="264" endWordPosition="267">e about cooking. 1 Introduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. Given a recipe, our task is to segment it into text spans that describe individual actions and construct an action graph whose nodes represent actions and edges represent the flow of arguments across actions, for example as seen in Fig. 1. This task poses unique challenges for semantic analysis. First, null arguments and ellipses are extremely common (Zwicky, 1988). For example, sentences such as “Bake for 50 m</context>
<context position="34951" citStr="Chen and Mooney, 2011" startWordPosition="6096" endWordPosition="6099"> ingredients, sauce, dry, pie noodles noodles, cook, mixture, egg, sauce, top, meat, drain, pasta, layer chicken chicken, mixture, salad, cook, dressing, pasta, soup, breast, vegetables, noodles pumpkin pumpkin, mixture, pie, filling, temperature, seeds, mash, oven, crust, dough bananas banana, mixture, batter, muffin, bread, egg, wet, cup, ingredients, slice Table 3: Examples of ingredients with their top inferred composite words. 9 Related work Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>David L. Chen and Raymond J. Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI2011), pages 859–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Bo Pang</author>
</authors>
<title>Spice it up? Mining refinements to online instructions from user generated content.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>545--553</pages>
<contexts>
<context position="36709" citStr="Druck and Pang, 2012" startWordPosition="6372" endWordPosition="6375">part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection</context>
</contexts>
<marker>Druck, Pang, 2012</marker>
<rawString>Gregory Druck and Bo Pang. 2012. Spice it up? Mining refinements to online instructions from user generated content. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 545–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics,</title>
<date>1982</date>
<pages>111--137</pages>
<publisher>Hanshin Publishing Co.,</publisher>
<location>Seoul, South</location>
<contexts>
<context position="3890" citStr="Fillmore, 1982" startWordPosition="605" endWordPosition="606">oach, where we design models to learn various aspects of procedural knowledge and then fit them to unannotated instructional text. Cooking recipes are an ideal domain to study these two challenges simultaneously, as vast amounts of recipes are available online today, with significant redundancy in their coverage that can help bootstrap the overall learning process. For example, there are over 400 variations on “macaroni and cheese” recipes on allrecipes.com, from “chipotle 1The goal of representing common sense world knowledge about actions and objects also drives theories of frame semantics (Fillmore, 1982) and script knowledge (Schank and Abelson, 1977). However, our focus is on inducing this style of knowledge automatically from procedural texts. 982 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 982–992, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. e1 e2 e3 e4 e5 v1:preheat a11 s oven 11 v3: press a51 51 object a52 52 prepositi 1 implicit 1 implicit v5: bake s s on v2: mix v4: lay a21 1 implicit a31 s31 object a32 1 a41 41 sbacon a42 1 É 6 s21 ground beef s21 1 1 s32 into loaf pan s42 over the top raw </context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>Charles J. Fillmore, 1982. Frame semantics, pages 111–137. Hanshin Publishing Co., Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lea Frermann</author>
<author>Ivan Titov</author>
<author>Manfred Pinkal</author>
</authors>
<title>A hierarchical bayesian model for unsupervised induction of script knowledge.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="36549" citStr="Frermann et al., 2014" startWordPosition="6348" endWordPosition="6351">ored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by</context>
</contexts>
<marker>Frermann, Titov, Pinkal, 2014</marker>
<rawString>Lea Frermann, Ivan Titov, and Manfred Pinkal. 2014. A hierarchical bayesian model for unsupervised induction of script knowledge. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 49– 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Fujiki</author>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Automatic acquisition of script knowledge from a text collection.</title>
<date>2003</date>
<booktitle>In Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics -</booktitle>
<volume>2</volume>
<pages>91--94</pages>
<contexts>
<context position="36327" citStr="Fujiki et al., 2003" startWordPosition="6313" endWordPosition="6316">on models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al.</context>
</contexts>
<marker>Fujiki, Nanba, Okumura, 2003</marker>
<rawString>Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Okumura. 2003. Automatic acquisition of script knowledge from a text collection. In Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics - Volume 2, pages 91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuelle Gaillard</author>
<author>Emmanuel Nauer</author>
<author>Marie Lefevre</author>
<author>Am´elie Cordier</author>
</authors>
<title>Extracting generic cooking adaptation knowledge for the TAAABLE case-based reasoning system.</title>
<date>2012</date>
<booktitle>In Proceedings of the 1st Workshop on Cooking with Computers (CwC).</booktitle>
<contexts>
<context position="36000" citStr="Gaillard et al., 2012" startWordPosition="6262" endWordPosition="6265">et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks tha</context>
</contexts>
<marker>Gaillard, Nauer, Lefevre, Cordier, 2012</marker>
<rawString>Emmanuelle Gaillard, Emmanuel Nauer, Marie Lefevre, and Am´elie Cordier. 2012. Extracting generic cooking adaptation knowledge for the TAAABLE case-based reasoning system. In Proceedings of the 1st Workshop on Cooking with Computers (CwC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erica Greene</author>
</authors>
<title>Extracting structured data from recipes using conditional random fields. The New York Times Open Blog.</title>
<date>2015</date>
<contexts>
<context position="36784" citStr="Greene, 2015" startWordPosition="6383" endWordPosition="6384">is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et a</context>
</contexts>
<marker>Greene, 2015</marker>
<rawString>Erica Greene. 2015. Extracting structured data from recipes using conditional random fields. The New York Times Open Blog.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TA Lau</author>
<author>Clemens Drews</author>
<author>Jeffrey Nichols</author>
</authors>
<title>Interpreting written how-to instructions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1433--1438</pages>
<contexts>
<context position="1626" citStr="Lau et al., 2009" startWordPosition="236" endWordPosition="239">bility to recover high quality action graphs, outperforming a strong sequential baseline by 8 points in F1, while also discovering general-purpose knowledge about cooking. 1 Introduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. Given a recipe, our task is to segment it into text spans that describe individual actions and construct an action graph whose nodes represent actions and edges represent the flow of arguments across actions, for example as seen in Fig. 1. This task pose</context>
<context position="35736" citStr="Lau et al. (2009)" startWordPosition="6223" endWordPosition="6226"> learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chamber</context>
</contexts>
<marker>Lau, Drews, Nichols, 2009</marker>
<rawString>TA Lau, Clemens Drews, and Jeffrey Nichols. 2009. Interpreting written how-to instructions. In Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence, pages 1433– 1438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt MacMahon</author>
<author>Brian Stankiewicz</author>
<author>Benjamin Kuipers</author>
</authors>
<title>Walk the talk: Connecting language, knowledge, and action in route instructions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 2, AAAI’06,</booktitle>
<pages>1475--1482</pages>
<publisher>AAAI Press.</publisher>
<marker>MacMahon, Stankiewicz, Kuipers, 2006</marker>
<rawString>Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. 2006. Walk the talk: Connecting language, knowledge, and action in route instructions. In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 2, AAAI’06, pages 1475– 1482. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirokuni Maeta</author>
<author>Tetsuro Sasada</author>
<author>Shinsuke Mori</author>
</authors>
<title>A framework for procedural text understanding.</title>
<date>2015</date>
<booktitle>In Proceedings of the 14th International Conference on Parsing Technologies,</booktitle>
<pages>50--60</pages>
<contexts>
<context position="1647" citStr="Maeta et al., 2015" startWordPosition="240" endWordPosition="243">high quality action graphs, outperforming a strong sequential baseline by 8 points in F1, while also discovering general-purpose knowledge about cooking. 1 Introduction Instructional language describes how to achieve a wide variety of goals, from traveling successfully to a desired location to cooking a particular dish for dinner. Despite the fact that such language is important to our everyday lives, there has been relatively little effort to design algorithms that can automatically convert it into an actionable form. Existing methods typically assume labeled training data (Lau et al., 2009; Maeta et al., 2015) or access to a physical simulator that can be used to test understanding of the instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. Given a recipe, our task is to segment it into text spans that describe individual actions and construct an action graph whose nodes represent actions and edges represent the flow of arguments across actions, for example as seen in Fig. 1. This task poses unique challenges f</context>
<context position="35412" citStr="Maeta et al., 2015" startWordPosition="6174" endWordPosition="6177">ates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et a</context>
</contexts>
<marker>Maeta, Sasada, Mori, 2015</marker>
<rawString>Hirokuni Maeta, Tetsuro Sasada, and Shinsuke Mori. 2015. A framework for procedural text understanding. In Proceedings of the 14th International Conference on Parsing Technologies, pages 50–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Malmaud</author>
<author>Earl J Wagner</author>
<author>Nancy Chang</author>
<author>Kevin Murphy</author>
</authors>
<title>Cooking with semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Workshop on Semantic Parsing,</booktitle>
<pages>33--38</pages>
<contexts>
<context position="37087" citStr="Malmaud et al., 2014" startWordPosition="6432" endWordPosition="6435">ural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while lear</context>
</contexts>
<marker>Malmaud, Wagner, Chang, Murphy, 2014</marker>
<rawString>Jon Malmaud, Earl J. Wagner, Nancy Chang, and Kevin Murphy. 2014. Cooking with semantics. In Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Malmaud</author>
<author>Jonathan Huang</author>
<author>Vivek Rathod</author>
<author>Nick Johnston</author>
<author>Andrew Rabinovich</author>
<author>Kevin Murphy</author>
</authors>
<title>What’s cookin’? Interpreting cooking videos using text, speech and vision.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>143--152</pages>
<contexts>
<context position="37110" citStr="Malmaud et al., 2015" startWordPosition="6436" endWordPosition="6439">i et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domai</context>
</contexts>
<marker>Malmaud, Huang, Rathod, Johnston, Rabinovich, Murphy, 2015</marker>
<rawString>Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, and Kevin Murphy. 2015. What’s cookin’? Interpreting cooking videos using text, speech and vision. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 143–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="27142" citStr="Miller, 1995" startWordPosition="4827" endWordPosition="4828">f argument strings. Despite scoring the segmentations for all possible sets of verbs, we found the process to be very efficient in practice. Learning For unsupervised learning, we again employ a hard EM approach. We initialize our models, segment all of the training data, reestimate the parameters, and iterate these steps until performance on a development set converges. We estimate the initial verb multinomial model using counts from the first word of each sentence in the dataset, which are normally verbs in imperative sentences, and filter out any words that have no verb synsets in WordNet (Miller, 1995). All other models are initialized to be uniform. 7 Experimental Setup Data Set We collected 2456 recipes (with over 23,000 sentences) from allrecipes.com by searching for 20 dish names (e.g., including “banana muffins”, and “deviled eggs”). We randomly sampled, removed, and hand labeled 33 recipes for a 5Early efforts using a state-of-the-art parser could only achieve an F1 score of 73.6% for identifying verbs, likely due to a lack of imperative sentences in the training data. This result motivated us to develop our segmentation system. development set and 100 recipes for test. All models wer</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Tetsuro Sasada</author>
<author>Yoko Yamakata</author>
<author>Koichiro Yoshino</author>
</authors>
<title>A machine learning approach to recipe text processing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 1st Workshop on Cooking with Computers (CwC).</booktitle>
<contexts>
<context position="35372" citStr="Mori et al., 2012" startWordPosition="6166" endWordPosition="6169">ite words. 9 Related work Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ont</context>
</contexts>
<marker>Mori, Sasada, Yamakata, Yoshino, 2012</marker>
<rawString>Shinsuke Mori, Tetsuro Sasada, Yoko Yamakata, and Koichiro Yoshino. 2012. A machine learning approach to recipe text processing. In Proceedings of the 1st Workshop on Cooking with Computers (CwC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Hirokuni Maeta</author>
<author>Yoko Yamakata</author>
<author>Tetsuro Sasada</author>
</authors>
<title>Flow graph corpus from recipe texts.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<pages>26--31</pages>
<contexts>
<context position="35391" citStr="Mori et al., 2014" startWordPosition="6170" endWordPosition="6173">d work Our work relates to a substantial body of research that transforms natural language instructions into actionable plans (Artzi and Zettlemoyer, 2013, Chen and Mooney, 2011, Branavan et al., 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et </context>
</contexts>
<marker>Mori, Maeta, Yamakata, Sasada, 2014</marker>
<rawString>Shinsuke Mori, Hirokuni Maeta, Yoko Yamakata, and Tetsuro Sasada. 2014. Flow graph corpus from recipe texts. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 26–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Yoko Doi</author>
<author>Miho Tsujita</author>
<author>Toshiyuki Takezawa</author>
<author>Kazutoshi Sumiya</author>
</authors>
<title>Construction of a cooking ontology from cooking recipes and patents.</title>
<date>2014</date>
<booktitle>In Proceedings the 2014 ACMInternational Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication,</booktitle>
<pages>507--516</pages>
<contexts>
<context position="36021" citStr="Nanba et al., 2014" startWordPosition="6266" endWordPosition="6269">al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, includ</context>
</contexts>
<marker>Nanba, Doi, Tsujita, Takezawa, Sumiya, 2014</marker>
<rawString>Hidetsugu Nanba, Yoko Doi, Miho Tsujita, Toshiyuki Takezawa, and Kazutoshi Sumiya. 2014. Construction of a cooking ontology from cooking recipes and patents. In Proceedings the 2014 ACMInternational Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication, pages 507–516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha S Palmer</author>
<author>Deborah A Dahl</author>
<author>Rebecca J Schiffman</author>
<author>Lynette Hirschman</author>
<author>Marcia Linebarger</author>
<author>John Dowding</author>
</authors>
<title>Recovering implicit information.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>10--19</pages>
<contexts>
<context position="37414" citStr="Palmer et al., 1986" startWordPosition="6482" endWordPosition="6485">ecipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., identifying nested locations such as an oven and a pan in the oven), enriching action graphs with greater semantic coverage (e.g.</context>
</contexts>
<marker>Palmer, Dahl, Schiffman, Hirschman, Linebarger, Dowding, 1986</marker>
<rawString>Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiffman, Lynette Hirschman, Marcia Linebarger, and John Dowding. 1986. Recovering implicit information. In Proceedings of the 24th Annual Meeting on Association for Computational Linguistics, pages 10–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Pichotta</author>
<author>Raymond Mooney</author>
</authors>
<title>Statistical script learning with multi-argument events.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>220--229</pages>
<contexts>
<context position="36383" citStr="Pichotta and Mooney, 2014" startWordPosition="6321" endWordPosition="6324">erpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos </context>
</contexts>
<marker>Pichotta, Mooney, 2014</marker>
<rawString>Karl Pichotta and Raymond Mooney. 2014. Statistical script learning with multi-argument events. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Alexander Koller</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning script knowledge with web experiments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>979--988</pages>
<contexts>
<context position="36503" citStr="Regneri et al., 2010" startWordPosition="6339" endWordPosition="6343">xtraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al.</context>
</contexts>
<marker>Regneri, Koller, Pinkal, 2010</marker>
<rawString>Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning script knowledge with web experiments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 979–988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Alexander Koller</author>
<author>Josef Ruppenhofer</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning script participants from unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>463--470</pages>
<contexts>
<context position="36525" citStr="Regneri et al., 2011" startWordPosition="6344" endWordPosition="6347">nowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely related to script knowledge, but most prior work focus on newswire and children’s books rather than procedural language (Fujiki et al., 2003; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Balasubramanian et al., 2013) or rely on crowdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work comp</context>
</contexts>
<marker>Regneri, Koller, Ruppenhofer, Pinkal, 2011</marker>
<rawString>Michaela Regneri, Alexander Koller, Josef Ruppenhofer, and Manfred Pinkal. 2011. Learning script participants from unlabeled data. In Proceedings of the Conference on Recent Advances in Natural Language Processing, pages 463–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Marcus Rohrbach</author>
<author>Dominikus Wetzel</author>
<author>Stefan Thater</author>
<author>Bernt Schiele</author>
<author>Manfred Pinkal</author>
</authors>
<title>Grounding action descriptions in videos.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>1</volume>
<pages>1--25</pages>
<contexts>
<context position="37049" citStr="Regneri et al., 2013" startWordPosition="6425" endWordPosition="6428">owdsourced descriptions to learn procedural knowledge (Regneri et al., 2010; Regneri et al., 2011; Frermann et al., 2014). There is work on related, but distinct, tasks that use recipes, including identifying actionable refinements from online recipe reviews (Druck and Pang, 2012) and extracting structured information from ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperforme</context>
</contexts>
<marker>Regneri, Rohrbach, Wetzel, Thater, Schiele, Pinkal, 2013</marker>
<rawString>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics (TACL), Volume 1., 1:25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Carl Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, plans, goals and understanding : an inquiry into human knowledge structures. The Artificial intelligence series.</title>
<date>1977</date>
<journal>L. Erlbaum,</journal>
<location>Hillsdale, N.J.</location>
<contexts>
<context position="3938" citStr="Schank and Abelson, 1977" startWordPosition="610" endWordPosition="613">various aspects of procedural knowledge and then fit them to unannotated instructional text. Cooking recipes are an ideal domain to study these two challenges simultaneously, as vast amounts of recipes are available online today, with significant redundancy in their coverage that can help bootstrap the overall learning process. For example, there are over 400 variations on “macaroni and cheese” recipes on allrecipes.com, from “chipotle 1The goal of representing common sense world knowledge about actions and objects also drives theories of frame semantics (Fillmore, 1982) and script knowledge (Schank and Abelson, 1977). However, our focus is on inducing this style of knowledge automatically from procedural texts. 982 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 982–992, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. e1 e2 e3 e4 e5 v1:preheat a11 s oven 11 v3: press a51 51 object a52 52 prepositi 1 implicit 1 implicit v5: bake s s on v2: mix v4: lay a21 1 implicit a31 s31 object a32 1 a41 41 sbacon a42 1 É 6 s21 ground beef s21 1 1 s32 into loaf pan s42 over the top raw ingre r nrn gd t dients Amish Meatloaf (http://a</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger Carl Schank and Robert P. Abelson. 1977. Scripts, plans, goals and understanding : an inquiry into human knowledge structures. The Artificial intelligence series. L. Erlbaum, Hillsdale, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Anette Frank</author>
</authors>
<title>Casting implicit role linking as an anaphora resolution task.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>1--10</pages>
<marker>Silberer, Frank, 2012</marker>
<rawString>Carina Silberer and Anette Frank. 2012. Casting implicit role linking as an anaphora resolution task. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 1– 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tasse</author>
<author>Noah A Smith</author>
</authors>
<title>SOUR CREAM: Toward semantic processing of recipes.</title>
<date>2008</date>
<tech>Technical Report CMU-LTI-08-005,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="35590" citStr="Tasse and Smith (2008)" startWordPosition="6201" endWordPosition="6204">, 2011, Branavan et al., 2009, McMahon et al., 2006). Most of these approaches do interactive learning in virtual environments or simulations, while we learn from the redundancy seen in the text of different instances of similar recipes. There is also significant related work on supervised learning for instructions. A recent series of studies have explored parsing of cooking recipes (Mori et al., 2012; Mori et al., 2014; Maeta et al., 2015). However, they assume annotated data, study Japanese recipes, and make edge connections independently without taking into account the flow of ingredients. Tasse and Smith (2008) develops annotation for English recipes, but do not mark connections from implicit roles, and only studied segmentation models. Lau et al. (2009) develop models to interpret how-to instructions, but also assume supervision, and do not make connections between different actions. Data-driven extraction of cooking knowledge has been explored in the context of building a cooking ontology (Gaillard et al., 2012; Nanba et al., 2014). In contrast, our work induces probabilistic cooking knowledge as part of unsupervised learning process for understanding recipes. Cooking knowledge is also closely rel</context>
</contexts>
<marker>Tasse, Smith, 2008</marker>
<rawString>Dan Tasse and Noah A. Smith. 2008. SOUR CREAM: Toward semantic processing of recipes. Technical Report CMU-LTI-08-005, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
</authors>
<title>Implicit role reference.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Symposium on Reference Resolution for Natural Language Processing,</booktitle>
<pages>109--115</pages>
<contexts>
<context position="37367" citStr="Tetreault 2002" startWordPosition="6476" endWordPosition="6477">ingredient lists (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., identifying nested locations such as an oven and a pan in the oven), enriching act</context>
</contexts>
<marker>Tetreault, 2002</marker>
<rawString>Joel R. Tetreault. 2002. Implicit role reference. In Proceedings of the International Symposium on Reference Resolution for Natural Language Processing, pages 109–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Whittemore</author>
<author>Melissa Macpherson</author>
<author>Greg Carlson</author>
</authors>
<title>Event-building through role-filling and anaphora resolution.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="37392" citStr="Whittemore et al., 1991" startWordPosition="6478" endWordPosition="6481"> (Greene, 2015) Cooking recipes have also been studied in the context of grounded language learning, e.g., to build robots that can cook (e.g., Bollini et al., 2013, Beetz et al., 2011), or to align cooking videos to natural language descriptions of actions (Regneri et al., 2013) or recipe texts (Malmaud et al., 2014; Malmaud et al., 2015). Our work complements these efforts by recovering fine-grained procedural semantics from text alone. Finally, detection and resolution of implicit arguments is an instance of zero anaphora detection and resolution (Silberer and Anette, 2012, Tetreault 2002, Whittemore et al., 1991, Palmer et al., 1986). We present an empirical approach for understanding these phenomena in instructions. 10 Conclusion We presented unsupervised methods for segmenting and identifying latent connections among actions in recipe text. Our model outperformed a strong linear baseline, while learning a variety of domain knowledge, such as verb signatures and probable ingredient components for different composites. Future work includes learning a more comprehensive model of locations (e.g., identifying nested locations such as an oven and a pan in the oven), enriching action graphs with greater s</context>
</contexts>
<marker>Whittemore, Macpherson, Carlson, 1991</marker>
<rawString>Greg Whittemore, Melissa Macpherson, and Greg Carlson. 1991. Event-building through role-filling and anaphora resolution. In Proceedings of the 29th Annual Meeting on Association for Computational Linguistics, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Zhang</author>
<author>Tao Lei</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Greed is good if randomized: New inference for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1013--1024</pages>
<contexts>
<context position="25352" citStr="Zhang et al., 2014" startWordPosition="4491" endWordPosition="4494">with its most likely set of connections C (Sec. 4). Then, we re-estimate the parameters of the probabilistic models using the recipeconnections pairs as training data. A small (33 recipes) development set was used to determine when to stop the iterations. Experimental details and model initialization are described in Sec. 7. 6 Segmentation Our inference and learning algorithms assume as input a recipe segmented into a set of events ER = {(v1, a1), ... , (vn, an)1. We designed a segmentation system that could be trained on our unannotated data set of mostly imperative sentences. dency parsing (Zhang et al., 2014). v v’ v’’ v v’ v’’ S S’ v v v’ v v’ S S’ S S’ 2-way swap 3-way swap S’’ S S’ S’’ Add v S S’ S S’ 987 Our system achieves an F1 score of 95.6% on the task of identifying the correct verbs in the test set.5 Segmentation model We define a generative model for recipes as: n P(R) = P(n) ri P(vi)P(m |vi) i We first select a number of verbs n in the recipe from a geometric distribution. Given the number of verbs, we select a set of verbs V = {v1, ... , vn} using a multinomial distribution. For each verb vi, we select a number of arguments m from a separate multinomial distribution that has the proba</context>
</contexts>
<marker>Zhang, Lei, Barzilay, Jaakkola, 2014</marker>
<rawString>Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2014. Greed is good if randomized: New inference for dependency parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1013– 1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnold M Zwicky</author>
</authors>
<title>On the subject of bare imperatives in english.</title>
<date>1988</date>
<booktitle>On Language: Rhetorica, Phonologica, Syntactica - A Festschriftfor Robert P. Stockwell from His Friends and Colleagues,</booktitle>
<pages>437--450</pages>
<editor>In C. Duncan-Rose and T. Vennemann, editors,</editor>
<publisher>Routledge,</publisher>
<location>London.</location>
<contexts>
<context position="2339" citStr="Zwicky, 1988" startWordPosition="357" endWordPosition="358">e instructions (Branavan et al., 2009; Chen and Mooney, 2011; Bollini et al., 2013). In this paper, we present the first approach for unsupervised learning to interpret instructional recipes using text alone, with application to cooking recipes. Given a recipe, our task is to segment it into text spans that describe individual actions and construct an action graph whose nodes represent actions and edges represent the flow of arguments across actions, for example as seen in Fig. 1. This task poses unique challenges for semantic analysis. First, null arguments and ellipses are extremely common (Zwicky, 1988). For example, sentences such as “Bake for 50 minutes” do not explicitly mention what to bake or where. Second, we must reason about how properties of the physical objects are changed by the described actions, for example to correctly resolve what the phrase “the wet mixture” refers to in a baking recipe. Although linguistic context is important to resolving both of these challenges, more crucial is common sense knowledge about how the world works, including what types of things are typically baked or what ingredients could be referred to as “wet.”1 These challenges seemingly present a chicken</context>
</contexts>
<marker>Zwicky, 1988</marker>
<rawString>Arnold M. Zwicky. 1988. On the subject of bare imperatives in english. In C. Duncan-Rose and T. Vennemann, editors, On Language: Rhetorica, Phonologica, Syntactica - A Festschriftfor Robert P. Stockwell from His Friends and Colleagues, pages 437– 450. Routledge, London.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002243">
<title confidence="0.985986">
Factorization of Latent Variables in Distributional Semantic Models
</title>
<author confidence="0.96747">
Arvid ¨Osterlund and David ¨Odling Magnus Sahlgren
</author>
<affiliation confidence="0.967672">
KTH Royal Institute of Technology, Sweden Gavagai, Sweden
</affiliation>
<email confidence="0.969732">
arvidos|dodling@kth.se mange@gavagai.se
</email>
<sectionHeader confidence="0.997017" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971166666667">
This paper discusses the use of factoriza-
tion techniques in distributional semantic
models. We focus on a method for re-
distributing the weight of latent variables,
which has previously been shown to im-
prove the performance of distributional se-
mantic models. However, this result has
not been replicated and remains poorly un-
derstood. We refine the method, and pro-
vide additional theoretical justification, as
well as empirical results that demonstrate
the viability of the proposed approach.
</bodyText>
<sectionHeader confidence="0.999382" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99962634375">
Distributional Semantic Models (DSMs) have be-
come standard paraphernalia in the natural lan-
guage processing toolbox, and even though there
is a wide variety of models available, the basic
parameters of DSMs (context type and size, fre-
quency weighting, and dimension reduction) are
now well understood. This is demonstrated by the
recent convergence of state-of-the-art results (Ba-
roni et al., 2014; Levy and Goldberg, 2014).
However, there are a few notable exceptions.
One is the performance improvements demon-
strated in two different papers using a method for
redistributing the weight of principal components
(PCs) in factorized DSMs (Caron, 2001; Bulli-
naria and Levy, 2012). In the latter of these pa-
pers, the factorization of latent variables in DSMs
is used to reach a perfect score of 100% correct
answers on the TOEFL synonym test. This re-
sult is somewhat surprising, since the factorization
method is the inverse of what is normally used.
Neither the result nor the method has been repli-
cated, and therefore remains poorly understood.
The goal of this paper is to replicate and explain
the result. In the following sections, we first pro-
vide a brief review of DSMs and factorization, and
review the method for redistributing the weight of
latent variables. We then replicate the 100% score
on the TOEFL test and provide additional state-of-
the-art scores for the BLESS test. We also provide
a more principled reformulation of the factoriza-
tion method that is better suited for practical ap-
plications.
</bodyText>
<sectionHeader confidence="0.99193" genericHeader="method">
2 Distributional Semantics
</sectionHeader>
<bodyText confidence="0.999653">
Consider a set of words W = {w1, ... , w,,,} and a
set of context words C = {c1, ... , cm}. The DSM
representation is created by registering an occur-
rence of a word wi with a set of context words
cj, ... , ck with a corresponding increment of the
projection of wi on the cj, ... , ck bases. In other
words, each cell fij in the matrix representation F
represents a co-occurrence count between a word
wi and a context cj. In the following, we use
W = C, making the co-occurrence matrix sym-
metric F,,,×,,,. We also adhere to standard practice
of weighting the co-occurrence counts with Posi-
tive Pointwise Mutual Information (PPMI) (Niwa
and Nitta, 1994), which is a variation of the stan-
dard PMI weighting,1 which simply discards non-
positive PMI values.
</bodyText>
<sectionHeader confidence="0.996016" genericHeader="method">
3 Singular Value Decomposition
</sectionHeader>
<bodyText confidence="0.999979142857143">
The high dimensionality of the co-occurrence ma-
trix makes it necessary in most practical applica-
tions to apply some form of dimensionality re-
duction to F, with the goal of finding a ba-
sis { ˆxj, ..., ˆxk} that restates the original basis
{xk, ...} in a lower-dimensional space Fˆ, where
Fˆ denotes the rank-k approximation of F:
</bodyText>
<equation confidence="0.9499536">
min |F − Fˆ |(1)
Fˆ∈Rn×Rk
1 fij(Eij fij)2
PMI(fij) = log Ei fij E Eij fij .
j fij
</equation>
<page confidence="0.956222">
227
</page>
<note confidence="0.65352">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 227–231,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999097636363636">
Assuming Gaussian-like distributions,2 a canoni-
cal way of achieving this is to maximize the vari-
ance of the data in the new basis. This enables or-
dering of the new basis according to how much of
the variance in the original data each component
describes.
A standard co-occurrence matrix is positive and
symmetric and thus has, by the spectral theorem,
a spectral decomposition of an ordered set of posi-
tive eigenvalues and an orthogonal set of eigenval-
ues:
</bodyText>
<equation confidence="0.99782">
F = UEVT (2)
</equation>
<bodyText confidence="0.999944">
where U holds the eigenvectors of F, E holds the
eigenvalues, and V E U(w) is a unitary matrix
mapping the original basis of F into its eigenba-
sis. Hence, by simply choosing the first k eigen-
values and their respective eigenvectors we have
the central result:
</bodyText>
<equation confidence="0.9116545">
min |F − Fˆ |—* Fˆ ,:; UkEkV T (3)
k k
</equation>
<bodyText confidence="0.999822928571429">
where Fˆ is the best rank-k approximation in the
Frobenius-norm. This is commonly referred to as
truncated Singular Value Decomposition (SVD).
Finally, using cosine similarity,3 V is redundant
due to invariance under unitary transformations,
which means we can represent the principal com-
ponents of Fˆ in its most compact form Fˆ - UE
without any further comment.
This projection onto the eigenbasis does not
only provide an efficient compression of the sparse
co-occurrence data, but has also been shown to
improve the performance and noise tolerance of
DSMs (Sch¨utze, 1992; Landauer and Dumais,
1997; Bullinaria and Levy, 2012).
</bodyText>
<sectionHeader confidence="0.978989" genericHeader="method">
4 The Caron p-transform
</sectionHeader>
<bodyText confidence="0.984055">
Caron (2001) introduce a method for renormaliza-
tion of the latent variables through an exponent
factor p E R:
</bodyText>
<equation confidence="0.958036">
UE —* UEp (4)
</equation>
<bodyText confidence="0.903331148148148">
which is shown to improve the results of factorized
models using both information retrieval and ques-
tion answering test collections. We refer to this
renormalization as the Caron p-transform. Bulli-
naria and Levy (2012) further corroborate Caron’s
2It is well known that the Gaussian assumption does not
hold in reality, and consequently there are also other ap-
proaches to dimensionality reduction based on multinomial
distributions, which we will not consider in this paper.
3cos(wi, =wi·wi
|wi||wi|
result, and show that the optimum exponent pa-
rameter p for DSMs is with strong statistical sig-
nificance p &lt; 1. Moreover, due to the redistri-
bution of weight to the lower variance PCs, Bul-
linaria and Levy (2012) show that similar perfor-
mance improvements can be achieved by simply
removing the first PCs. We refer to this as PC-
removal. A highlight of their results is a perfect
score of 100% on the TOEFL synonym test.
Apart form the perfect score on the TOEFL test,
it is noteworthy that the PC-removal scheme is the
inverse of how SVD is normally used in DSMs; in-
stead of retaining only the first PCs – which is the
standard way of using the SVD in DSMs – the PC-
removal scheme deletes them, and instead retains
all the rest.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999992606060606">
We replicate the experiment setup of Bullinaria
and Levy (2012) by removing punctuation and
decapitalizing the ukWaC corpus (Baroni et al.,
2009). The DSM includes the 50,000 most fre-
quent words along with the remaining 23 TOEFL
words and is populated using a f2-sized context
window. Co-occurrence counts are weighted with
PPMI, and SVD is applied to the resulting ma-
trix, reducing the dimensionality to 5,000. The
results of removing the first PCs versus apply-
ing the Caron p-transform are shown in Figure 1,
which replicates the results from Bullinaria and
Levy (2012).
In order to better understand what influence the
transform has on the representations, we also pro-
vide results on the BLESS test (Baroni and Lenci,
2011), which lists a number of related terms to 200
target terms. The related terms represent 8 dif-
ferent kinds of semantic relations (co-hyponymy,
hypernymy, meronymy, attribute, event, and three
random classes corresponding to randomly se-
lected nouns, adjectives and verbs), and it is thus
possible to use the BLESS test to determine what
type of semantic relation a model favors. Since our
primary interest here is in paradigmatic relations,
we focus on the hypernymy and co-hyponymy re-
lations, and require that the model scores one of
the related terms from these classes higher than
the related terms from the other classes. The cor-
pus was split into different sizes to test the statisti-
cal significance of the weight redistribution effect.
Furthermore, it shows that the optimal weight dis-
tribution depends on the size of the data.
</bodyText>
<page confidence="0.995472">
228
</page>
<figureCaption confidence="0.738306">
Figure 1: TOEFL score for the PC-removal
scheme and the Caron p-transform for the span of
PCs.
Figure 2 shows the BLESS results for both the
PC removal scheme and the Caron p-transform for
different sizes of the corpus. The best score is
</figureCaption>
<bodyText confidence="0.998934904761905">
92.96% for the PC removal, and 92.46% for the
Caron p-transform, both using the full data set.
Similarly to the TOEFL results, we see better re-
sults for a larger number of removed PCs. Inter-
estingly, there is clearly a larger improvement in
performance of the Caron p-transform than for the
PC removal scheme.
Figure 3 shows how the redistribution affects
the different relations in the BLESS test. The vi-
olin plots are based on the maximum values of
each relation, and the width of the violin repre-
sents the normalized probability density of cosine
measures. The cosine distributions, OZ, are based
on the best matches for each category i, and nor-
malized by the total mean and variance amongst
all categories ˆOZ = Θi−µ
σ . Thus, the figure illus-
trates how well each category is separated from
each other, the larger separation the better.
The results in Figure 3 indicate that the top 120
PCs contain a higher level of co-hyponymy rela-
</bodyText>
<figureCaption confidence="0.9987502">
Figure 2: BLESS score for the PC-removal
scheme and the Caron p-transform for the span of
PCs.
Figure 3: BLESS targets versus categories from
1,400 PCs representation of the entire corpus.
</figureCaption>
<bodyText confidence="0.9974665">
tions than the lower; removing the top 120 PCs
gives a violin shape that resembles the inverse of
the plot for the top 120 PCs. Although neither part
of the PC span is significantly better in separating
the categories, it is clear that removing the first 120
PCs increases the variance within the categories
</bodyText>
<page confidence="0.99662">
229
</page>
<bodyText confidence="0.995999692307692">
and especially amongst the coord-category. This
is an interesting result, since it seems to contra-
dict the hypothesis that removing the first PCs im-
proves the semantic quality of the representations
– there is obviously valuable semantic information
in the first PCs.
Table 1 summarizes our top results on the
TOEFL, BLESS, and also the SimLex-999 simi-
larity test (Hill et al., 2014), and compares them
to a baseline score from the Skipgram model
(Mikolov et al., 2013a), trained on the same data
using a window size of 2, negative samples, and
400-dimensional vectors.
</bodyText>
<table confidence="0.99804825">
TOEFL BLESS SimLex-999
PC removal 100 92.96 46.52
Caron p 100 92.46 46.66
Skipgram 83.75 83.00 39.91
</table>
<tableCaption confidence="0.9762125">
Table 1: Top results for the PC removal and Caron
p on each test compared to the Skipgram model.
</tableCaption>
<bodyText confidence="0.999816636363636">
Unfortunately, the optimal redistribution of
weight on the PCs for the respective top scores
differ between the experiments. For the PC re-
moval the optimal number of removed PCs is 379
for TOEFL, 15 for BLESS and 128 for SimLex-
999, while the optimal number for the Caron p-
transform is -1.4 for TOEFL, 0.5 for BLESS and
-0.40 for SimLex-999. Hence, there is likely no
easy way to find a general expression of the opti-
mal redistribution of weight on the PCs for a given
application.
</bodyText>
<sectionHeader confidence="0.985579" genericHeader="method">
6 The Pareto Principle
</sectionHeader>
<bodyText confidence="0.999990235294118">
It is common practice to reduce the dimensional-
ity of an n-dimensional space to as many PCs as
it takes to cover 80% of the total eigenvalue mass.
This convention is known as the Pareto principle
(or 80/20-rule), and generally gives a good trade-
off between compression and precision. The re-
sults presented in the previous section suggest a
type of inversion of this principle in the case of
DSMs.
Given a computational and practical limit of the
number of PCs m with weights E = {Q1, ..., Qm},
the optimal redistribution of weight on these com-
ponents is such that the first l − m components
Q1, ..., Qm−l is transformed such that they consti-
tute 20% of the new total mass. Where l−m is the
number of components representing the last 20 %
of the original mass. In other words, the function
</bodyText>
<equation confidence="0.9848468">
f : E Eˆ performing this redistribution is such
that:
�m 1l Qi N 20% (5)
�m
i=1 ˆQi
</equation>
<bodyText confidence="0.943383666666667">
In this formulation, we can consider the Caron p-
transform and the PC-removal scheme as special
cases, where the Caron p-transform is given by:
</bodyText>
<equation confidence="0.607251333333333">
f(Qi) = Qpi Vi, p E R (6)
and the PC-removal scheme by:
f(Qi) = (1 − S(F))Qi Vi, F = {1....l} (7)
</equation>
<bodyText confidence="0.999611666666667">
where S(F) denotes the generalized Kronecker
delta function.
To test this claim, we form this quotient for
the distributions of weights at the optimal parame-
ters for the Caron p-transform and the PC-removal
scheme for both the BLESS and TOEFL tests for
each of the 40 sub-corpora.
Even though the results are not as optimal for
the BLESS test as for the TOEFL, the results in
Figure 4 point in favor of this measure. The opti-
mal mass distributions for the Caron p-transform
and the PC removal are all around 20 %.
</bodyText>
<figure confidence="0.855909">
(a) TOEFL - PC removal (b) BLESS - PC removal
(c) TOEFL - Caron p (d) BLESS - Caron p
</figure>
<figureCaption confidence="0.893558">
Figure 4: The mass redistribution ratio for the best
results on the 1,400 PC models.
</figureCaption>
<bodyText confidence="0.99937075">
This result does not only apply for 1,400 PCs,
but has also been verified on a smaller set of matri-
ces with sizes of 2,500 PCs, 4,000 PCs and 5,000
PCs. The results for 1,400 PCs and 5,000 PCs are
shown in Table 2. As can be seen in this table, the
rule of thumb yields reasonable good guesses for
both Caron p and PC removal, over the different
tests and for various number of PCs.
</bodyText>
<page confidence="0.990248">
230
</page>
<table confidence="0.983500583333333">
5,000 PC representation
PC removal Caron p
Parameter 493 -0.80
TOEFL 100 98.75
BLESS 89.45 88.95
SimLex 44.82 45.74
1,400 PC representation
PC removal Caron p
Parameter 204 -0.80
TOEFL 93.75 93.75
BLESS 89.95 90.95
SimLex 45.45 46.47
</table>
<tableCaption confidence="0.953474">
Table 2: Results for the PC removal and Caron p
using the 80/20 rule
</tableCaption>
<sectionHeader confidence="0.992697" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999985111111111">
This paper has discussed the method of redistribut-
ing the weight of the first PCs in factorized DSMs.
We have replicated previously published results,
and provided additional empirical justification for
the method. The method significantly outperforms
the baseline Skipgram model on all tests used in
the experiments. Our results also suggest a slight
refinement of the method, for which we have pro-
vided both theoretical and empirical justification.
The resulting rule of thumb method leads to stable
results that may be useful in practice.
Although the experiments in this paper has pro-
vided further evidence for the usefulness of re-
distributing the weight in factorized models, it
also raises additional interesting research ques-
tions. For example, does the method also im-
prove models that have been trained on smaller
data sets? Does it also hold for non-Gaussian
factorization like Non-negative Matrix Factoriza-
tion? How does the method affect the (local) struc-
tural properties of the representations; do factor-
ized models display the same type of structural
regularities as has been observed in word embed-
dings (Mikolov et al., 2013b), and would it be pos-
sible to use methods such as relative neighborhood
graphs (Gyllensten and Sahlgren, 2015) to explore
the local effects of the transformation?
</bodyText>
<sectionHeader confidence="0.996123" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928791666667">
Marco Baroni and Alessandro Lenci. 2011. How we
blessed distributional semantic evaluation. In Pro-
ceedings of GEMS, pages 1–10.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language resources
and evaluation, 43(3):209–226.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
ofACL, pages 238–247.
John Bullinaria and Joseph Levy. 2012. Extracting
semantic representations from word co-occurrence
statistics: stop-lists, stemming, and SVD. Behavior
Research Methods, 44(3):890–907.
John Caron. 2001. Experiments with LSA scoring:
Optimal rank and basis. In Michael Berry, editor,
Computational Information Retrieval, pages 157–
169.
Amaru Cuba Gyllensten and Magnus Sahlgren. 2015.
Navigating the semantic horizon using relative
neighborhood graph. In Proceedings of EMNLP.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. CoRR, abs/1408.3456.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological review, 104(2):211.
Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Pro-
ceedings of NIPS, pages 2177–2185.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings NAACL-HLT,
pages 746–751.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-
occurrence vectors from corpora vs. distance vec-
tors from dictionaries. In Proceedings of COLING,
pages 304–309.
Hinrich Sch¨utze. 1992. Dimensions of meaning. In
Proceedings of Supercomputing, pages 787–796.
</reference>
<page confidence="0.997931">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.679350">
<title confidence="0.999568">Factorization of Latent Variables in Distributional Semantic Models</title>
<author confidence="0.91652">Magnus Sahlgren KTH Royal Institute of Technology</author>
<author confidence="0.91652">Sweden Gavagai</author>
<author confidence="0.91652">Sweden</author>
<email confidence="0.812235">arvidos|dodling@kth.semange@gavagai.se</email>
<abstract confidence="0.997302769230769">This paper discusses the use of factorization techniques in distributional semantic models. We focus on a method for redistributing the weight of latent variables, which has previously been shown to improve the performance of distributional semantic models. However, this result has not been replicated and remains poorly understood. We refine the method, and provide additional theoretical justification, as well as empirical results that demonstrate the viability of the proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we blessed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="7175" citStr="Baroni and Lenci, 2011" startWordPosition="1195" endWordPosition="1198">ing the ukWaC corpus (Baroni et al., 2009). The DSM includes the 50,000 most frequent words along with the remaining 23 TOEFL words and is populated using a f2-sized context window. Co-occurrence counts are weighted with PPMI, and SVD is applied to the resulting matrix, reducing the dimensionality to 5,000. The results of removing the first PCs versus applying the Caron p-transform are shown in Figure 1, which replicates the results from Bullinaria and Levy (2012). In order to better understand what influence the transform has on the representations, we also provide results on the BLESS test (Baroni and Lenci, 2011), which lists a number of related terms to 200 target terms. The related terms represent 8 different kinds of semantic relations (co-hyponymy, hypernymy, meronymy, attribute, event, and three random classes corresponding to randomly selected nouns, adjectives and verbs), and it is thus possible to use the BLESS test to determine what type of semantic relation a model favors. Since our primary interest here is in paradigmatic relations, we focus on the hypernymy and co-hyponymy relations, and require that the model scores one of the related terms from these classes higher than the related terms</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we blessed distributional semantic evaluation. In Proceedings of GEMS, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="6594" citStr="Baroni et al., 2009" startWordPosition="1098" endWordPosition="1101">eved by simply removing the first PCs. We refer to this as PCremoval. A highlight of their results is a perfect score of 100% on the TOEFL synonym test. Apart form the perfect score on the TOEFL test, it is noteworthy that the PC-removal scheme is the inverse of how SVD is normally used in DSMs; instead of retaining only the first PCs – which is the standard way of using the SVD in DSMs – the PCremoval scheme deletes them, and instead retains all the rest. 5 Experiments We replicate the experiment setup of Bullinaria and Levy (2012) by removing punctuation and decapitalizing the ukWaC corpus (Baroni et al., 2009). The DSM includes the 50,000 most frequent words along with the remaining 23 TOEFL words and is populated using a f2-sized context window. Co-occurrence counts are weighted with PPMI, and SVD is applied to the resulting matrix, reducing the dimensionality to 5,000. The results of removing the first PCs versus applying the Caron p-transform are shown in Figure 1, which replicates the results from Bullinaria and Levy (2012). In order to better understand what influence the transform has on the representations, we also provide results on the BLESS test (Baroni and Lenci, 2011), which lists a num</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="1130" citStr="Baroni et al., 2014" startWordPosition="160" endWordPosition="164">t been replicated and remains poorly understood. We refine the method, and provide additional theoretical justification, as well as empirical results that demonstrate the viability of the proposed approach. 1 Introduction Distributional Semantic Models (DSMs) have become standard paraphernalia in the natural language processing toolbox, and even though there is a wide variety of models available, the basic parameters of DSMs (context type and size, frequency weighting, and dimension reduction) are now well understood. This is demonstrated by the recent convergence of state-of-the-art results (Baroni et al., 2014; Levy and Goldberg, 2014). However, there are a few notable exceptions. One is the performance improvements demonstrated in two different papers using a method for redistributing the weight of principal components (PCs) in factorized DSMs (Caron, 2001; Bullinaria and Levy, 2012). In the latter of these papers, the factorization of latent variables in DSMs is used to reach a perfect score of 100% correct answers on the TOEFL synonym test. This result is somewhat surprising, since the factorization method is the inverse of what is normally used. Neither the result nor the method has been replic</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings ofACL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bullinaria</author>
<author>Joseph Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD.</title>
<date>2012</date>
<journal>Behavior Research Methods,</journal>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="1410" citStr="Bullinaria and Levy, 2012" startWordPosition="203" endWordPosition="207">tandard paraphernalia in the natural language processing toolbox, and even though there is a wide variety of models available, the basic parameters of DSMs (context type and size, frequency weighting, and dimension reduction) are now well understood. This is demonstrated by the recent convergence of state-of-the-art results (Baroni et al., 2014; Levy and Goldberg, 2014). However, there are a few notable exceptions. One is the performance improvements demonstrated in two different papers using a method for redistributing the weight of principal components (PCs) in factorized DSMs (Caron, 2001; Bullinaria and Levy, 2012). In the latter of these papers, the factorization of latent variables in DSMs is used to reach a perfect score of 100% correct answers on the TOEFL synonym test. This result is somewhat surprising, since the factorization method is the inverse of what is normally used. Neither the result nor the method has been replicated, and therefore remains poorly understood. The goal of this paper is to replicate and explain the result. In the following sections, we first provide a brief review of DSMs and factorization, and review the method for redistributing the weight of latent variables. We then rep</context>
<context position="5059" citStr="Bullinaria and Levy, 2012" startWordPosition="834" endWordPosition="837">e Fˆ is the best rank-k approximation in the Frobenius-norm. This is commonly referred to as truncated Singular Value Decomposition (SVD). Finally, using cosine similarity,3 V is redundant due to invariance under unitary transformations, which means we can represent the principal components of Fˆ in its most compact form Fˆ - UE without any further comment. This projection onto the eigenbasis does not only provide an efficient compression of the sparse co-occurrence data, but has also been shown to improve the performance and noise tolerance of DSMs (Sch¨utze, 1992; Landauer and Dumais, 1997; Bullinaria and Levy, 2012). 4 The Caron p-transform Caron (2001) introduce a method for renormalization of the latent variables through an exponent factor p E R: UE —* UEp (4) which is shown to improve the results of factorized models using both information retrieval and question answering test collections. We refer to this renormalization as the Caron p-transform. Bullinaria and Levy (2012) further corroborate Caron’s 2It is well known that the Gaussian assumption does not hold in reality, and consequently there are also other approaches to dimensionality reduction based on multinomial distributions, which we will not</context>
<context position="6512" citStr="Bullinaria and Levy (2012)" startWordPosition="1086" endWordPosition="1089">e PCs, Bullinaria and Levy (2012) show that similar performance improvements can be achieved by simply removing the first PCs. We refer to this as PCremoval. A highlight of their results is a perfect score of 100% on the TOEFL synonym test. Apart form the perfect score on the TOEFL test, it is noteworthy that the PC-removal scheme is the inverse of how SVD is normally used in DSMs; instead of retaining only the first PCs – which is the standard way of using the SVD in DSMs – the PCremoval scheme deletes them, and instead retains all the rest. 5 Experiments We replicate the experiment setup of Bullinaria and Levy (2012) by removing punctuation and decapitalizing the ukWaC corpus (Baroni et al., 2009). The DSM includes the 50,000 most frequent words along with the remaining 23 TOEFL words and is populated using a f2-sized context window. Co-occurrence counts are weighted with PPMI, and SVD is applied to the resulting matrix, reducing the dimensionality to 5,000. The results of removing the first PCs versus applying the Caron p-transform are shown in Figure 1, which replicates the results from Bullinaria and Levy (2012). In order to better understand what influence the transform has on the representations, we </context>
</contexts>
<marker>Bullinaria, Levy, 2012</marker>
<rawString>John Bullinaria and Joseph Levy. 2012. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD. Behavior Research Methods, 44(3):890–907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Caron</author>
</authors>
<title>Experiments with LSA scoring: Optimal rank and basis.</title>
<date>2001</date>
<booktitle>Computational Information Retrieval,</booktitle>
<pages>157--169</pages>
<editor>In Michael Berry, editor,</editor>
<contexts>
<context position="1382" citStr="Caron, 2001" startWordPosition="201" endWordPosition="202">have become standard paraphernalia in the natural language processing toolbox, and even though there is a wide variety of models available, the basic parameters of DSMs (context type and size, frequency weighting, and dimension reduction) are now well understood. This is demonstrated by the recent convergence of state-of-the-art results (Baroni et al., 2014; Levy and Goldberg, 2014). However, there are a few notable exceptions. One is the performance improvements demonstrated in two different papers using a method for redistributing the weight of principal components (PCs) in factorized DSMs (Caron, 2001; Bullinaria and Levy, 2012). In the latter of these papers, the factorization of latent variables in DSMs is used to reach a perfect score of 100% correct answers on the TOEFL synonym test. This result is somewhat surprising, since the factorization method is the inverse of what is normally used. Neither the result nor the method has been replicated, and therefore remains poorly understood. The goal of this paper is to replicate and explain the result. In the following sections, we first provide a brief review of DSMs and factorization, and review the method for redistributing the weight of l</context>
<context position="5097" citStr="Caron (2001)" startWordPosition="842" endWordPosition="843">us-norm. This is commonly referred to as truncated Singular Value Decomposition (SVD). Finally, using cosine similarity,3 V is redundant due to invariance under unitary transformations, which means we can represent the principal components of Fˆ in its most compact form Fˆ - UE without any further comment. This projection onto the eigenbasis does not only provide an efficient compression of the sparse co-occurrence data, but has also been shown to improve the performance and noise tolerance of DSMs (Sch¨utze, 1992; Landauer and Dumais, 1997; Bullinaria and Levy, 2012). 4 The Caron p-transform Caron (2001) introduce a method for renormalization of the latent variables through an exponent factor p E R: UE —* UEp (4) which is shown to improve the results of factorized models using both information retrieval and question answering test collections. We refer to this renormalization as the Caron p-transform. Bullinaria and Levy (2012) further corroborate Caron’s 2It is well known that the Gaussian assumption does not hold in reality, and consequently there are also other approaches to dimensionality reduction based on multinomial distributions, which we will not consider in this paper. 3cos(wi, =wi·</context>
</contexts>
<marker>Caron, 2001</marker>
<rawString>John Caron. 2001. Experiments with LSA scoring: Optimal rank and basis. In Michael Berry, editor, Computational Information Retrieval, pages 157– 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amaru Cuba Gyllensten</author>
<author>Magnus Sahlgren</author>
</authors>
<title>Navigating the semantic horizon using relative neighborhood graph.</title>
<date>2015</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Gyllensten, Sahlgren, 2015</marker>
<rawString>Amaru Cuba Gyllensten and Magnus Sahlgren. 2015. Navigating the semantic horizon using relative neighborhood graph. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="10071" citStr="Hill et al., 2014" startWordPosition="1688" endWordPosition="1691">mbles the inverse of the plot for the top 120 PCs. Although neither part of the PC span is significantly better in separating the categories, it is clear that removing the first 120 PCs increases the variance within the categories 229 and especially amongst the coord-category. This is an interesting result, since it seems to contradict the hypothesis that removing the first PCs improves the semantic quality of the representations – there is obviously valuable semantic information in the first PCs. Table 1 summarizes our top results on the TOEFL, BLESS, and also the SimLex-999 similarity test (Hill et al., 2014), and compares them to a baseline score from the Skipgram model (Mikolov et al., 2013a), trained on the same data using a window size of 2, negative samples, and 400-dimensional vectors. TOEFL BLESS SimLex-999 PC removal 100 92.96 46.52 Caron p 100 92.46 46.66 Skipgram 83.75 83.00 39.91 Table 1: Top results for the PC removal and Caron p on each test compared to the Skipgram model. Unfortunately, the optimal redistribution of weight on the PCs for the respective top scores differ between the experiments. For the PC removal the optimal number of removed PCs is 379 for TOEFL, 15 for BLESS and 12</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="5031" citStr="Landauer and Dumais, 1997" startWordPosition="830" endWordPosition="833">Fˆ ,:; UkEkV T (3) k k where Fˆ is the best rank-k approximation in the Frobenius-norm. This is commonly referred to as truncated Singular Value Decomposition (SVD). Finally, using cosine similarity,3 V is redundant due to invariance under unitary transformations, which means we can represent the principal components of Fˆ in its most compact form Fˆ - UE without any further comment. This projection onto the eigenbasis does not only provide an efficient compression of the sparse co-occurrence data, but has also been shown to improve the performance and noise tolerance of DSMs (Sch¨utze, 1992; Landauer and Dumais, 1997; Bullinaria and Levy, 2012). 4 The Caron p-transform Caron (2001) introduce a method for renormalization of the latent variables through an exponent factor p E R: UE —* UEp (4) which is shown to improve the results of factorized models using both information retrieval and question answering test collections. We refer to this renormalization as the Caron p-transform. Bullinaria and Levy (2012) further corroborate Caron’s 2It is well known that the Gaussian assumption does not hold in reality, and consequently there are also other approaches to dimensionality reduction based on multinomial dist</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="1156" citStr="Levy and Goldberg, 2014" startWordPosition="165" endWordPosition="168"> remains poorly understood. We refine the method, and provide additional theoretical justification, as well as empirical results that demonstrate the viability of the proposed approach. 1 Introduction Distributional Semantic Models (DSMs) have become standard paraphernalia in the natural language processing toolbox, and even though there is a wide variety of models available, the basic parameters of DSMs (context type and size, frequency weighting, and dimension reduction) are now well understood. This is demonstrated by the recent convergence of state-of-the-art results (Baroni et al., 2014; Levy and Goldberg, 2014). However, there are a few notable exceptions. One is the performance improvements demonstrated in two different papers using a method for redistributing the weight of principal components (PCs) in factorized DSMs (Caron, 2001; Bullinaria and Levy, 2012). In the latter of these papers, the factorization of latent variables in DSMs is used to reach a perfect score of 100% correct answers on the TOEFL synonym test. This result is somewhat surprising, since the factorization method is the inverse of what is normally used. Neither the result nor the method has been replicated, and therefore remain</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In Proceedings of NIPS, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="10156" citStr="Mikolov et al., 2013" startWordPosition="1703" endWordPosition="1706">span is significantly better in separating the categories, it is clear that removing the first 120 PCs increases the variance within the categories 229 and especially amongst the coord-category. This is an interesting result, since it seems to contradict the hypothesis that removing the first PCs improves the semantic quality of the representations – there is obviously valuable semantic information in the first PCs. Table 1 summarizes our top results on the TOEFL, BLESS, and also the SimLex-999 similarity test (Hill et al., 2014), and compares them to a baseline score from the Skipgram model (Mikolov et al., 2013a), trained on the same data using a window size of 2, negative samples, and 400-dimensional vectors. TOEFL BLESS SimLex-999 PC removal 100 92.96 46.52 Caron p 100 92.46 46.66 Skipgram 83.75 83.00 39.91 Table 1: Top results for the PC removal and Caron p on each test compared to the Skipgram model. Unfortunately, the optimal redistribution of weight on the PCs for the respective top scores differ between the experiments. For the PC removal the optimal number of removed PCs is 379 for TOEFL, 15 for BLESS and 128 for SimLex999, while the optimal number for the Caron ptransform is -1.4 for TOEFL,</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013a. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings NAACL-HLT,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="10156" citStr="Mikolov et al., 2013" startWordPosition="1703" endWordPosition="1706">span is significantly better in separating the categories, it is clear that removing the first 120 PCs increases the variance within the categories 229 and especially amongst the coord-category. This is an interesting result, since it seems to contradict the hypothesis that removing the first PCs improves the semantic quality of the representations – there is obviously valuable semantic information in the first PCs. Table 1 summarizes our top results on the TOEFL, BLESS, and also the SimLex-999 similarity test (Hill et al., 2014), and compares them to a baseline score from the Skipgram model (Mikolov et al., 2013a), trained on the same data using a window size of 2, negative samples, and 400-dimensional vectors. TOEFL BLESS SimLex-999 PC removal 100 92.96 46.52 Caron p 100 92.46 46.66 Skipgram 83.75 83.00 39.91 Table 1: Top results for the PC removal and Caron p on each test compared to the Skipgram model. Unfortunately, the optimal redistribution of weight on the PCs for the respective top scores differ between the experiments. For the PC removal the optimal number of removed PCs is 379 for TOEFL, 15 for BLESS and 128 for SimLex999, while the optimal number for the Caron ptransform is -1.4 for TOEFL,</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings NAACL-HLT, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiki Niwa</author>
<author>Yoshihiko Nitta</author>
</authors>
<title>Cooccurrence vectors from corpora vs. distance vectors from dictionaries.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>304--309</pages>
<contexts>
<context position="2920" citStr="Niwa and Nitta, 1994" startWordPosition="471" endWordPosition="474">1, ... , w,,,} and a set of context words C = {c1, ... , cm}. The DSM representation is created by registering an occurrence of a word wi with a set of context words cj, ... , ck with a corresponding increment of the projection of wi on the cj, ... , ck bases. In other words, each cell fij in the matrix representation F represents a co-occurrence count between a word wi and a context cj. In the following, we use W = C, making the co-occurrence matrix symmetric F,,,×,,,. We also adhere to standard practice of weighting the co-occurrence counts with Positive Pointwise Mutual Information (PPMI) (Niwa and Nitta, 1994), which is a variation of the standard PMI weighting,1 which simply discards nonpositive PMI values. 3 Singular Value Decomposition The high dimensionality of the co-occurrence matrix makes it necessary in most practical applications to apply some form of dimensionality reduction to F, with the goal of finding a basis { ˆxj, ..., ˆxk} that restates the original basis {xk, ...} in a lower-dimensional space Fˆ, where Fˆ denotes the rank-k approximation of F: min |F − Fˆ |(1) Fˆ∈Rn×Rk 1 fij(Eij fij)2 PMI(fij) = log Ei fij E Eij fij . j fij 227 Proceedings of the 2015 Conference on Empirical Metho</context>
</contexts>
<marker>Niwa, Nitta, 1994</marker>
<rawString>Yoshiki Niwa and Yoshihiko Nitta. 1994. Cooccurrence vectors from corpora vs. distance vectors from dictionaries. In Proceedings of COLING, pages 304–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing,</booktitle>
<pages>787--796</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992. Dimensions of meaning. In Proceedings of Supercomputing, pages 787–796.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998973">
Discriminative Neural Sentence Modeling
by Tree-Based Convolution
</title>
<author confidence="0.91177">
Lili Mou∗, Hao Peng∗, Ge Li†, Yan Xu, Lu Zhang, Zhi Jin
</author>
<email confidence="0.896841">
{doublepower.mou, penghao.pku}@gmail.com
</email>
<note confidence="0.6849755">
{lige, xuyan14, zhanglu, zhijin}@sei.pku.edu.cn
Software Institute, Peking University, 100871, P. R. China
</note>
<sectionHeader confidence="0.943556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968428571429">
This paper proposes a tree-based con-
volutional neural network (TBCNN) for
discriminative sentence modeling. Our
model leverages either constituency trees
or dependency trees of sentences. The
tree-based convolution process extracts
sentences structural features, which are
then aggregated by max pooling. Such ar-
chitecture allows short propagation paths
between the output layer and underlying
feature detectors, enabling effective struc-
tural feature learning and extraction. We
evaluate our models on two tasks: senti-
ment analysis and question classification.
In both experiments, TBCNN outperforms
previous state-of-the-art results, including
existing neural networks and dedicated
feature/rule engineering. We also make
efforts to visualize the tree-based convo-
lution process, shedding light on how our
models work.
</bodyText>
<sectionHeader confidence="0.992524" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999746333333333">
Discriminative sentence modeling aims to capture
sentence meanings, and classify sentences accord-
ing to certain criteria (e.g., sentiment). It is related
to various tasks of interest, and has attracted much
attention in the NLP community (Allan et al.,
2003; Su and Markert, 2008; Zhao et al., 2015).
Feature engineering—for example, n-gram fea-
tures (Cui et al., 2006), dependency subtree fea-
tures (Nakagawa et al., 2010), or more dedicated
ones (Silva et al., 2011)—can play an important
role in modeling sentences. Kernel machines, e.g.,
SVM, are exploited in Moschitti (2006) and Re-
ichartz et al. (2010) by specifying a certain mea-
sure of similarity between sentences, without ex-
plicit feature representation.
</bodyText>
<note confidence="0.9877805">
∗These authors contribute equally to this paper.
†To whom correspondence should be addressed.
</note>
<bodyText confidence="0.999634973684211">
Recent advances of neural networks bring new
techniques in understanding natural languages,
and have exhibited considerable potential. Bengio
et al. (2003) and Mikolov et al. (2013) propose un-
supervised approaches to learn word embeddings,
mapping discrete words to real-valued vectors in
a meaning space. Le and Mikolov (2014) ex-
tend such approaches to learn sentences’ and para-
graphs’ representations. Compared with human
engineering, neural networks serve as a way of au-
tomatic feature learning (Bengio et al., 2013).
Two widely used neural sentence models are
convolutional neural networks (CNNs) and recur-
sive neural networks (RNNs). CNNs can extract
words’ neighboring features effectively with short
propagation paths, but they do not capture inher-
ent sentence structures (e.g., parse trees). RNNs
encode, to some extent, structural information by
recursive semantic composition along a parse tree.
However, they may have difficulties in learning
deep dependencies because of long propagation
paths (Erhan et al., 2009). (CNNs/RNNs and a
variant, recurrent networks, will be reviewed in
Section 2.)
A curious question is whether we can com-
bine the advantages of CNNs and RNNs, i.e.,
whether we can exploit sentence structures (like
RNNs) effectively with short propagation paths
(like CNNs).
In this paper, we propose a novel neural ar-
chitecture for discriminative sentence modeling,
called the Tree-Based Convolutional Neural Net-
work (TBCNN).1 Our models can leverage differ-
ent sentence parse trees, e.g., constituency trees
and dependency trees. The model variants are de-
noted as c-TBCNN and d-TBCNN, respectively.
The idea of tree-based convolution is to apply a set
of subtree feature detectors, sliding over the entire
</bodyText>
<note confidence="0.927179666666667">
1The model of tree-based convolution was firstly pro-
posed to process program source code in our (unpublished)
previous work (Mou et al., 2014).
2315
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2315–2325,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.905141">
(a) CNN (b) RNN (c) TBCNN
</figure>
<figureCaption confidence="0.7575785">
Figure 1: A comparison of information flow in the convolutional neural network (CNN), the recursive
neural network (RNN), and the tree-based convolutional neural network (TBCNN).
</figureCaption>
<figure confidence="0.988830933333333">
Max pooling
by heuristics
Softmax
Softmax
Softmax
Representing hidden
layers as vectors
recursively
W
W r
l
Hidden Output
layer layer
W
W
W
l
r
Extracted
features by
convolution
Embeddings
Word embeddings
Parsing tree
of a sentence
Extracted features
by tree-based convolution
Output layer
Pooling layer
x1 ... xt
</figure>
<bodyText confidence="0.997791785714286">
parse tree of a sentence; then pooling aggregates
these extracted feature vectors by taking the max-
imum value in each dimension. One merit of such
architecture is that all features, along the tree, have
short propagation paths to the output layer, and
hence structural information can be learned effec-
tively.
TBCNNs are evaluated on two tasks, sentiment
analysis and question classification; our models
have outperformed previous state-of-the-art re-
sults in both experiments. To understand how
TBCNNs work, we also visualize the network by
plotting the convolution process. We make our
code and results available on our project website.2
</bodyText>
<sectionHeader confidence="0.750477" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.934279666666667">
In this section, we present the background and re-
lated work regarding two prevailing neural archi-
tectures for discriminative sentence modeling.
</bodyText>
<subsectionHeader confidence="0.996947">
2.1 Convolutional Neural Networks
</subsectionHeader>
<bodyText confidence="0.9924832">
Convolutional neural networks (CNNs), early
used for image processing (LeCun, 1995), turn
out to be effective with natural languages as well.
Figure 1a depicts a classic convolution process on
a sentence (Collobert and Weston, 2008). A set
of fixed-width-window feature detectors slide over
the sentence, and output the extracted features. Let
t be the window size, and x1, · · · , xt ∈ Rne be
ne-dimensional word embeddings. The output of
convolution, evaluated at the current position, is
</bodyText>
<equation confidence="0.985327">
y = f (W · [x1; ··· ; xt] + b)
where y ∈ Rn. (nc is the number of feature detec-
tors). W ∈ Rn.×(t·ne) and b ∈ Rn. are parame-
2https://sites.google.com/site/tbcnnsentence/
</equation>
<bodyText confidence="0.972180133333333">
ters; f is the activation function. Semicolons rep-
resent column vector concatenation. After convo-
lution, the extracted features are pooled to a fixed-
size vector for classification.
Convolution can extract neighboring informa-
tion effectively. However, the features are
“local”—words that are not in a same convolu-
tion window do not interact with each other, even
though they may be semantically related. Blun-
som et al. (2014) build deep convolutional net-
works so that local features can mix at high-level
layers. Similar CNNs include Kim (2014) and Hu
et al. (2014). All these models are “flat,” by which
we mean no structural information is used explic-
itly.
</bodyText>
<subsectionHeader confidence="0.998991">
2.2 Recursive Neural Networks
</subsectionHeader>
<bodyText confidence="0.998506">
Recursive neural networks (RNNs), proposed in
Socher et al. (2011b), utilize sentence parse trees.
In the original version, RNN is built upon a
binarized constituency tree. Leaf nodes corre-
spond to words in a sentence, represented by ne-
dimensional embeddings. Non-leaf nodes are sen-
tence constituents, coded by child nodes recur-
sively. Let node p be the parent of c1 and c2, vec-
tor representations denoted as p, c1, and c2. The
parent’s representation is composited by
</bodyText>
<equation confidence="0.998534">
p = f(W · [c1; c2] + b) (1)
</equation>
<bodyText confidence="0.93663884375">
where W and b are parameters. This process is
done recursively along the tree; the root vector is
then used for supervised classification (Figure 1b).
Dependency parse and the combinatory cate-
gorical grammar can also be exploited as RNNs’
skeletons (Hermann and Blunsom, 2013; Iyyer et
al., 2014). Irsoy and Cardie (2014) build deep
RNNs to enhance information interaction. Im-
2316
provements for semantic compositionality include
matrix-vector interaction (Socher et al., 2012),
tensor interaction (Socher et al., 2013). They are
more suitable for capturing logical information in
sentences, such as negation and exclamation.
One potential problem of RNNs is that the long
propagation paths—through which leaf nodes are
connected to the output layer—may lead to infor-
mation loss. Thus, RNNs bury illuminating in-
formation under a complicated neural architecture.
Further, during back-propagation over a long path,
gradients tend to vanish (or blow up), which makes
training difficult (Erhan et al., 2009). Long short
term memory (LSTM), first proposed for model-
ing time-series data (Hochreiter and Schmidhuber,
1997), is integrated to RNNs to alleviate this prob-
lem (Tai et al., 2015; Le and Zuidema, 2015; Zhu
et al., 2015).
Recurrent networks. A variant class of RNNs
is the recurrent neural network (Bengio et al.,
1994; Shang et al., 2015), whose architecture is
a rightmost tree. In such models, meaningful tree
structures are also lost, similar to CNNs.
</bodyText>
<sectionHeader confidence="0.998104" genericHeader="method">
3 Tree-based Convolution
</sectionHeader>
<bodyText confidence="0.99995952631579">
This section introduces the proposed tree-based
convolutional neural networks (TBCNNs). Figure
1c depicts the convolution process on a tree.
First, a sentence is converted to a parse tree, ei-
ther a constituency or dependency tree. The corre-
sponding model variants are denoted as c-TBCNN
and d-TBCNN. Each node in the tree is repre-
sented as a distributed, real-valued vector.
Then, we design a set of fixed-depth subtree fea-
ture detectors, called the tree-based convolution
window. The window slides over the entire tree
to extract structural information of the sentence,
illustrated by a dashed triangle in Figure 1c. For-
mally, let us assume we have t nodes in the con-
volution window, x1, · · · , xt, each represented as
an ne-dimensional vector. Let nc be the number
of feature detectors. The output of the tree-based
convolution window, evaluated at the current sub-
tree, is given by the following generic equation.
</bodyText>
<equation confidence="0.995526">
�Wi·xi + b (2)
</equation>
<bodyText confidence="0.9999574">
where Wi ∈ Rnc×ne is the weight parameter asso-
ciated with node xi; b ∈ Rnc is the bias term.
Extracted features are thereafter packed into
one or more fixed-size vectors by max pooling,
that is, the maximum value in each dimension is
taken. Finally, we add a fully connected hidden
layer, and a softmax output layer.
From the designed architecture (Figure 1c), we
see that our TBCNN models allow short propaga-
tion paths between the output layer and any posi-
tion in the tree. Therefore structural feature learn-
ing becomes effective.
Several main technical points in tree-based con-
volution include: (1) How can we represent hid-
den nodes as vectors in constituency trees? (2)
How can we determine weights, Wi, for depen-
dency trees, where nodes may have different num-
bers of children? (3) How can we pool varying
sized and shaped features to fixed-size vectors?
In the rest of this section, we explain model
variants in detail. Particularly, Subsections 3.1 and
3.2 address the first and second problems; Sub-
section 3.3 deals with the third problem by intro-
ducing several pooling heuristics. Subsection 3.4
presents our training objective.
</bodyText>
<subsectionHeader confidence="0.997429">
3.1 c-TBCNN
</subsectionHeader>
<bodyText confidence="0.999948722222222">
Figure 2a illustrates an example of the con-
stituency tree, where leaf nodes are words in the
sentence, and non-leaf nodes represent a grammat-
ical constituent, e.g., a noun phrase. Sentences
are parsed by the Stanford parser;3 further, con-
stituency trees are binarized for simplicity.
One problem of constituency trees is that non-
leaf nodes do not have such vector representations
as word embeddings. Our strategy is to pretrain
the constituency tree with an RNN by Equation 1
(Socher et al., 2011b). After pretraining, vector
representations of nodes are fixed.
We now consider the tree-based convolution
process in c-TBCNN with a two-layer-subtree
convolution window, which operates on a parent
node p and its direct children cl and cr, their vec-
tor representations denoted as p, cl, and cr. The
convolution equation, specific for c-TBCNN, is
</bodyText>
<equation confidence="0.948273">
W((C) • p + Wi(�) • el + W (�) • er + b(�)
y = f ( I
</equation>
<bodyText confidence="0.884916">
where W(c)
p , W(c)
l , and W(c)
r are weights asso-
ciated with the parent and its child nodes. Su-
perscript (c) indicates that the weights are for c-
TBCNN. For leaf nodes, which do not have chil-
dren, we set cl and cr to be 0.
</bodyText>
<equation confidence="0.91677">
3http://nlp.stanford.edu/software/lex-parser.shtml
t
y = f
i=1
2317
</equation>
<figureCaption confidence="0.9473966">
Figure 2: Tree-based convolution in (a) c-TBCNN, and (b) d-TBCNN. The parse trees correspond to the
sentence “I loved it.” The dashed triangles illustrate a shared-weight convolution window sliding over
the tree. For clarity, only two positions are drawn in c-TBCNN. Notice that dotted arrows are not part of
neural connections; they merely indicate the topologies of tree structures. Specially, an edge a r→ bin
the dependency tree refers to a being governed by b with dependency type r.
</figureCaption>
<figure confidence="0.998233655172414">
(a) (b)
loved
nsubj dobj
I
I
it
Extracted features by
Constituency tree c-TBCNN Dependency tree
Extracted features by
d-TBCNN
loved it
GLOBAL
LOWER LOWER
LEFT RIGHT
TOP
... ...
y n �
W(d) i=1 W(d) ·ci + b(d)
p + r[ci]
p is the weight parameter for the par-
ent p (governing word); W (d) r[ci]is the weight for
child ci, who has grammatical relationship r[cil
(a) Global pooling (b) 3-slot pooling for c-TBCNN
k pooling slots (k = 2)
Each slot chooses the
maximum value
in a dimension
Extracted features by tree-based convolution in the order of words
(c) k-slot pooling for d-TBCNN
</figure>
<figureCaption confidence="0.890577">
Figure 3: Pooling heuristics. (a) Global pooling.
(b) 3-slot pooling for c-TBCNN. (c) k-slot pooling
for d-TBCNN.
</figureCaption>
<bodyText confidence="0.999234363636364">
to its parent, p. Superscript (d) indicates the pa-
rameters are for d-TBCNN. Note that we keep 15
most frequently occurred dependency types; oth-
ers appearing rarely in the corpus are mapped to
one shared weight matrix.
Both c-TBCNN and d-TBCNN have their own
advantages: d-TBCNN exploits structural features
more efficiently because of the compact expres-
siveness of dependency trees; c-TBCNN may be
more effective in integrating global features due
to the underneath pretrained RNN.
</bodyText>
<subsectionHeader confidence="0.999977">
3.3 Pooling Heuristics
</subsectionHeader>
<bodyText confidence="0.976550393939394">
As different sentences may have different lengths
and tree structures, the extracted features by tree-
based convolution also have topologies varying in
size and shape. Dynamic pooling (Socher et al.,
2011a) is a common technique for dealing with
Tree-based convolution windows can be ex-
tended to arbitrary depths straightforwardly. The
complexity is exponential to the depth of the
window, but linear to the number of nodes.
Hence, tree-based convolution, compared with
“flat” CNNs, does not add to computational cost,
provided the same amount of information to pro-
cess at a time. In our experiments, we use convo-
lution windows of depth 2.
3.2 d-TBCNN
Dependency trees are another representation of
sentence structures. The nature of dependency
representation leads to d-TBCNN’s major dif-
ference from traditional convolution: there ex-
ist nodes with different numbers of child nodes.
This causes trouble if we associate weight param-
eters according to positions in the window, which
is standard for traditional convolution, e.g., Col-
lobert and Weston (2008) or c-TBCNN.
To overcome the problem, we extend the no-
tion of convolution by assigning weights accord-
ing to dependency types (e.g, nsubj) rather than
positions. We believe this strategy makes much
sense because dependency types (de Marneffe et
al., 2006) reflect the relationship between a gov-
erning word and its child words. To be concrete,
the generic convolution formula (Equation 2) for
d-TBCNN becomes
</bodyText>
<table confidence="0.879377777777778">
= f
where W(d)
2318
Task Data samples Label
Sentiment Offers that rare combination of entertainment and education. ++
Analysis An idealistic love story that brings out the latent 15-year-old romantic in everyone. +
Its mysteries are transparently obvious, and it’s too slowly paced to be a thriller. −
Question What is the temperature at the center of the earth? number
Classification What state did the Battle of Bighorn take place in? location
</table>
<tableCaption confidence="0.8396545">
Table 1: Data samples in sentiment analysis and question classification. In the first task, “++” refers to
strongly positive; “+” and “−” refer to positive and negative, respectively.
</tableCaption>
<bodyText confidence="0.9997363">
this problem. We propose several heuristics for
pooling along a tree structure. Our generic de-
sign criteria for pooling include: (1) Nodes that
are pooled to one slot should be “neighboring”
from some viewpoint. (2) Each slot should have
similar numbers of nodes, in expectation, that are
pooled to it. Thus, (approximately) equal amount
of information is aggregated along different parts
of the tree. Following the above intuition, we pro-
pose pooling heuristics as follows.
</bodyText>
<listItem confidence="0.654878666666667">
• Global pooling. All features are pooled to
one vector, shown in Figure 3a. We take
the maximum value in each dimension. This
simple heuristic is applicable to any structure,
including c-TBCNN and d-TBCNN.
• 3-slot pooling for c-TBCNN. To preserve
</listItem>
<bodyText confidence="0.98243806060606">
more information over different parts of con-
stituency trees, we propose 3-slot pooling
(Figure 3b). If a tree has maximum depth
d, we pool nodes of less than α · d lay-
ers to a TOP slot (α is set to 0.6); lower
nodes are pooled to slots LOWER LEFT or
LOWER RIGHT according to their relative
position with respect to the root node.
For a constituency tree, it is not completely
obvious how to pool features to more than
3 slots and comply with the aforementioned
criteria at the same time. Therefore, we re-
gard 3-slot pooling for c-TBCNN is a “hard
mechanism” temporarily. Further improve-
ment can be addressed in future work.
• k-slot pooling for d-TBCNN. Different from
constituency trees, nodes in dependency trees
are one-one corresponding to words in a sen-
tence. Thus, a total order on features (af-
ter convolution) can be defined according
to their corresponding word orders. For k-
slot pooling, we can adopt an “equal allo-
cation” strategy, shown in Figure 3c. Let
i be the position of a word in a sentence
(i = 1, 2, · · · , n). Its extracted feature vec-
tor is pooled to the j-th slot, if
(j − 1)nk ≤ i ≤ jnk
We assess the efficacy of pooling quantitatively
in Section 4.3.1. As we shall see by the exper-
imental results, complicated pooling methods do
preserve more information along tree structures to
some extent, but the effect is not large. TBCNNs
are not very sensitive to pooling methods.
</bodyText>
<subsectionHeader confidence="0.994327">
3.4 Training Objective
</subsectionHeader>
<bodyText confidence="0.999987909090909">
After pooling, information is packed into one or
more fixed-size vectors (slots). We add a hidden
layer, and then a softmax layer to predict the prob-
ability of each target label in a classification task.
The error function of a sample is the standard cross
entropy loss, i.e., J = − Eci=1 ti log yi, where t is
the ground truth (one-hot represented), y the out-
put by softmax, and c the number of classes. To
regularize our model, we apply both `2 penalty and
dropout (Srivastava et al., 2014). Training details
are further presented in Section 4.1 and 4.2.
</bodyText>
<sectionHeader confidence="0.996792" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999852">
In this section, we evaluate our models with two
tasks, sentiment analysis and question classifica-
tion. We also conduct quantitative and qualitative
model analysis in Subsection 4.3.
</bodyText>
<subsectionHeader confidence="0.9861765">
4.1 Sentiment Analysis
4.1.1 The Task and Dataset
</subsectionHeader>
<bodyText confidence="0.9999347">
Sentiment analysis is a widely studied task for
discriminative sentence modeling. The Stanford
sentiment treebank4 consists of more than 10,000
movie reviews. Two settings are considered for
sentiment prediction: (1) fine-grained classifi-
cation with 5 labels (strongly positive,
positive, neutral, negative, and
strongly negative), and (2) coarse-gained
polarity classification with 2 labels (positive
versus negative). Some examples are shown in
</bodyText>
<footnote confidence="0.791293">
4http://nlp.stanford.edu/sentiment/
</footnote>
<page confidence="0.650796">
2319
</page>
<tableCaption confidence="0.9666205">
Table 1. We use the standard split for training, val-
idating, and testing, containing 8544/1101/2210
sentences for 5-class prediction. Binary classifi-
cation does not contain the neutral class.
</tableCaption>
<bodyText confidence="0.999781727272727">
In the dataset, phrases (sub-sentences) are also
tagged with sentiment labels. RNNs deal with
them naturally during the recursive process. We
regard sub-sentences as individual samples during
training, like Blunsom et al. (2014) and Le and
Mikolov (2014). The training set therefore has
more than 150,000 entries in total. For validating
and testing, only whole sentences (root labels) are
considered in our experiments.
Both c-TBCNN and d-TBCNN use the Stanford
parser for data preprocessing.
</bodyText>
<subsectionHeader confidence="0.545742">
4.1.2 Training Details
</subsectionHeader>
<bodyText confidence="0.999988217391304">
This subsection describes training details for d-
TBCNN, where hyperparameters are chosen by
validation. c-TBCNN is mostly tuned syn-
chronously (e.g., optimization algorithm, activa-
tion function) with some changes in hyperparam-
eters. c-TBCNN’s settings can be found on our
website.
In our d-TBCNN model, the number of units
is 300 for convolution and 200 for the last hid-
den layer. Word embeddings are 300 dimensional,
pretrained ourselves using word2vec (Mikolov
et al., 2013) on the English Wikipedia corpus. 2-
slot pooling is applied for d-TBCNN. (c-TBCNN
uses 3-slot pooling.)
To train our model, we compute gradient by
back-propagation and apply stochastic gradient
descent with mini-batch 200. We use ReLU (Nair
and Hinton, 2010) as the activation function.
For regularization, we add E2 penalty for
weights with a coefficient of 10−5. Dropout (Sri-
vastava et al., 2014) is further applied to both
weights and embeddings. All hidden layers are
dropped out by 50%, and embeddings 40%.
</bodyText>
<subsectionHeader confidence="0.5544">
4.1.3 Performance
</subsectionHeader>
<bodyText confidence="0.999955114285714">
Table 2 compares our models to state-of-the-art
results in the task of sentiment analysis. For 5-
class prediction, d-TBCNN yields 51.4% accu-
racy, outperforming the previous state-of-the-art
result, achieved by the RNN based on long-short
term memory (Tai et al., 2015). c-TBCNN is
slightly worse. It achieves 50.4% accuracy, rank-
ing third in the state-of-the-art list (including our
d-TBCNN model).
Regarding 2-class prediction, we adopted a sim-
ple strategy in Irsoy and Cardie (2014),5 where the
5-class network is “transferred” directly for binary
classification, with estimated target probabilities
(by 5-way softmax) reinterpreted for 2 classes.
(The neutral class is discarded as in other stud-
ies.) This strategy enables us to take a glance at the
stability of our TBCNN models, but places itself
in a difficult position. Nonetheless, our d-TBCNN
model achieves 87.9% accuracy, ranking forth in
the list.
In a more controlled comparison—with shal-
low architectures and the basic interaction (lin-
early transformed and non-linearly squashed)—
TBCNNs, of both variants, consistently outper-
form RNNs (Socher et al., 2011b) to a large ex-
tent (50.4–51.4% versus 43.2%); they also con-
sistently outperform “flat” CNNs by more than
10%. Such results show that structures are im-
portant when modeling sentences; tree-based con-
volution can capture these structural information
more effectively than RNNs.
We also observe d-TBCNN achieves higher per-
formance than c-TBCNN. This suggests that com-
pact tree expressiveness is more important than in-
tegrating global information in this task.
</bodyText>
<subsectionHeader confidence="0.995365">
4.2 Question Classification
</subsectionHeader>
<bodyText confidence="0.988556304347826">
We further evaluate TBCNN models on a ques-
tion classification task.6 The dataset contains
5452 annotated sentences plus 500 test sam-
ples in TREC 10. We also use the stan-
dard split, like Silva et al. (2011). Target la-
bels contain 6 classes, namely abbreviation,
entity, description, human, location,
and numeric. Some examples are also shown in
Table 1.
We chose this task to evaluate our models be-
cause the number of training samples is rather
small, so that we can know TBCNNs’ perfor-
mance when applied to datasets of different sizes.
To alleviate the problem of data sparseness, we set
the dimensions of convolutional layer and the last
hidden layer to 30 and 25, respectively. We do
not back-propagate gradient to embeddings in this
5Richard Socher, who first applies neural networks to this
task, thinks direct transfer is fine for binary classification. We
followed this strategy for simplicity as it is non-trivial to deal
with the neutral sub-sentences in the training set if we train a
separate model. Our website reviews some related work and
provides more discussions.
</bodyText>
<footnote confidence="0.70114">
6http://cogcomp.cs.illinois.edu/Data/QA/QC/
</footnote>
<table confidence="0.992415857142858">
2320
Group Method 5-class accuracy 2-class accuracy Reported in
Baseline SVM 40.7 79.4 Socher et al. (2013)
Naive Bayes 41.0 81.8 Socher et al. (2013)
CNNs 1-layer convolution 37.4 77.1 Blunsom et al. (2014)
Deep CNN 48.5 86.8 Blunsom et al. (2014)
Non-static 48.0 87.2 Kim (2014)
Multichannel 47.4 88.1 Kim (2014)
RNNs Basic 43.2 82.4 Socher et al. (2013)
Matrix-vector 44.4 82.9 Socher et al. (2013)
Tensor 45.7 85.4 Socher et al. (2013)
Tree LSTM (variant 1) 48.0 – Zhu et al. (2015)
Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015)
Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015)
Deep RNN 49.8 86.61 Irsoy and Cardie (2014)
Recurrent LSTM 45.8 86.7 Tai et al. (2015)
bi-LSTM 49.1 86.8 Tai et al. (2015)
Vector Word vector avg. 32.7 80.1 Socher et al. (2013)
Paragraph vector 48.7 87.8 Le and Mikolov (2014)
TBCNNs c-TBCNN 50.4 86.81 Our implementation
d-TBCNN 51.4 87.91 Our implementation
</table>
<tableCaption confidence="0.9783325">
Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate
that the network is transferred directly from that of 5-class.
</tableCaption>
<table confidence="0.9535142">
Method Acc. (%) Reported in
SVM 95.0 Silva et al. (2011)
10k features + 60 rules
CNN-non-static 93.6 Kim (2014)
CNN-mutlichannel 92.2 Kim (2014)
RNN 90.2 Zhao et al. (2015)
Deep-CNN 93.0 Blunsom et al. (2014)
Ada-CNN 92.4 Zhao et al. (2015)
c-TBCNN 94.8 Our implementation
d-TBCNN 96.0 Our implementation
</table>
<tableCaption confidence="0.99989">
Table 3: Accuracy of 6-way question classification.
</tableCaption>
<bodyText confidence="0.999725157894737">
task. Dropout rate for embeddings is 30%; hidden
layers are dropped out by 5%.
Table 3 compares our models to various other
methods. The first entry presents the previous
state-of-the-art result, achieved by traditional fea-
ture/rule engineering (Silva et al., 2011). Their
method utilizes more than 10k features and 60
hand-coded rules. On the contrary, our TBCNN
models do not use a single human-engineered fea-
ture or rule. Despite this, c-TBCNN achieves
similar accuracy compared with feature engineer-
ing; d-TBCNN pushes the state-of-the-art result to
96%. To the best of our knowledge, this is the first
time that neural networks beat dedicated human
engineering in this question classification task.
The result also shows that both c-TBCNN and
d-TBCNN reduce the error rate to a large extent,
compared with other neural architectures in this
task.
</bodyText>
<subsectionHeader confidence="0.999886">
4.3 Model Analysis
</subsectionHeader>
<bodyText confidence="0.999958333333333">
In this part, we analyze our models quantitatively
and qualitatively in several aspects, shedding some
light on the mechanism of TBCNNs.
</bodyText>
<subsectionHeader confidence="0.676613">
4.3.1 The Effect of Pooling
</subsectionHeader>
<bodyText confidence="0.999861192307692">
The extracted features by tree-based convolution
have topologies varying in size and shape. We pro-
pose in Section 3.3 several heuristics for pooling.
This subsection aims to provide a fair comparison
among these pooling methods.
One reasonable protocol for comparison is to
tune all hyperparameters for each setting and com-
pare the highest accuracy. This methodology,
however, is too time-consuming, and depends
largely on the quality of hyperparameter tuning.
An alternative is to predefine a set of sensible hy-
perparameters and report the accuracy under the
same setting. In this experiment, we chose the
latter protocol, where hidden layers are all 300-
dimensional; no E2 penalty is added. Each config-
uration was run five times with different random
initializations. We summarize the mean and stan-
dard deviation in Table 4.
As the results imply, complicated pooling is bet-
ter than global pooling to some degree for both
model variants. But the effect is not strong; our
models are not that sensitive to pooling methods,
which mainly serve as a necessity for dealing with
varying-structure data. In our experiments, we ap-
ply 3-slot pooling for c-TBCNN and 2-slot pool-
ing for d-TBCNN.
</bodyText>
<table confidence="0.939035125">
2321
Model Pooling method 5-class accuracy (%)
Global 48.48 f 0.54
c-TBCNN
3-slot 48.69 f 0.40
Global 49.39 f 0.24
d-TBCNN
2-slot 49.94 f 0.63
</table>
<tableCaption confidence="0.8247548">
Table 4: Accuracies of different pooling methods,
averaged over 5 random initializations. We chose
sensible hyperparameters manually in advance to
make a fair comparison. This leads to performance
degradation (1–2%) vis-a-vis Table 2.
</tableCaption>
<figureCaption confidence="0.989988">
Figure 4: Accuracies versus sentence lengths.
</figureCaption>
<bodyText confidence="0.999842571428571">
Comparing with other studies in the literature,
we also notice that pooling is very effective and ef-
ficient in information gathering. Irsoy and Cardie
(2014) report 200 epochs for training a deep RNN,
which achieves 49.8% accuracy in the 5-class sen-
timent classification. Our TBCNNs are typically
trained within 25 epochs.
</bodyText>
<subsectionHeader confidence="0.683548">
4.3.2 The Effect of Sentence Lengths
</subsectionHeader>
<bodyText confidence="0.99996456">
We analyze how sentence lengths affect our mod-
els. Sentences are split into 7 groups by length,
with granularity 5. A few too long or too short
sentences are grouped together for smoothing; the
numbers of sentences in each group vary from 126
to 457. Figure 4 presents accuracies versus lengths
in TBCNNs. For comparison, we also reimple-
mented RNN, achieving 42.7% overall accuracy,
slightly worse than 43.2% reported in Socher et
al. (2011b). Thus, we think our reimplementation
is fair and that the comparison is sensible.
We observe that c-TBCNN and d-TBCNN yield
very similar behaviors. They consistently outper-
form the RNN in all scenarios. We also notice the
gap, between TBCNNs and RNN, increases when
sentences contain more than 20 words. This re-
sult confirms our theoretical analysis in Section
2—for long sentences, the propagation paths in
RNNs are deep, causing RNNs’ difficulty in in-
formation processing. By contrast, our models ex-
plore structural information more effectively with
tree-based convolution. As information from any
part of the tree can propagate to the output layer
with short paths, TBCNNs are more capable for
sentence modeling, especially for long sentences.
</bodyText>
<subsectionHeader confidence="0.897001">
4.3.3 Visualization
</subsectionHeader>
<bodyText confidence="0.999862468085106">
Visualization is important to understanding the
mechanism of neural networks. For TBCNNs, we
would like to see how the extracted features (af-
ter convolution) are further processed by the max
pooling layer, and ultimately related to the super-
vised task.
To show this, we trace back where the max
pooling layer’s features come from. For each di-
mension, the pooling layer chooses the maximum
value from the nodes that are pooled to it. Thus,
we can count the fraction in which a node’s fea-
tures are gathered by pooling. Intuitively, if a
node’s features are more related to the task, the
fraction tends to be larger, and vice versa.
Figure 5 illustrates an example processed by d-
TBCNN in the task of sentiment analysis.7 Here,
we applied global pooling because information
tracing is more sensible with one pooling slot.
As shown in the figure, tree-based convolution
can effectively extract information relevant to the
task of interest. The 2-layer windows correspond-
ing to “visual will impress viewers,” “the stunning
dreamlike visual,” say, are discriminative to the
sentence’s sentiment. Hence, large fractions (0.24
and 0.19) of their features, after convolution, are
gathered by pooling. On the other hand, words
like the, will, even are known as stop words (Fox,
1989). They are mostly noninformative for sen-
timent; hence, no (or minimal) features are gath-
ered. Such results are consistent with human intu-
ition.
We further observe that tree-based convolution
does integrate information of different words in
the window. For example, the word stunning ap-
pears in two windows: (a) the window “stunning”
itself, and (b) the window of “the stunning dream-
like visual,” with root node visual, stunning acting
as a child. We see that Window b is more rel-
evant to the ultimate sentiment than Window a,
with fractions 0.19 versus 0.07, even though the
root visual itself is neutral in sentiment. In fact,
7We only have space to present one example in the paper.
This example was not chosen deliberately. Similar traits can
be found through out the entire gallery, available on our web-
site. Also, we only present d-TBCNN, noticing that depen-
dency trees are intrinsically more suitable for visualization
since we know the “meaning” of every node.
</bodyText>
<figure confidence="0.997304307692308">
&lt;9 10−14 15−19 20−24 25−29 30−34 &gt;35
Setence length
RNN
c-TBCNN
d-TBCNN
Accuracy (%)
50
40
30
20
10
0
2322
</figure>
<figureCaption confidence="0.979611">
Figure 5: Visualizing how features (after convolution) are related to the sentiment of a sentence. The
</figureCaption>
<bodyText confidence="0.911018333333333">
sample corresponds a sentence in the dataset, “The stunning dreamlike visual will impress even those
viewers who have little patience for Euro-film pretension.” The numbers in brackets denote the fraction
of a node’s features that are gathered by the max pooling layer (also indicated by colors).
</bodyText>
<figure confidence="0.996793681818182">
impress (.26)
visual (.19) will (.01) viewers (.05)
have (.06)
stunning
(.07)
dreamlike
(.02)
patience (.01)
for (.01)
pretension (.09)
Euro-film
(.04)
The
(0)
who
(.10)
even
(0)
those
(.03)
little
(.06)
</figure>
<bodyText confidence="0.990407">
Window a has a larger fraction than the sum of its
children’s (the windows of “the,” “stunning,” and
“dreamlike”).
</bodyText>
<sectionHeader confidence="0.997031" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999982384615385">
In this paper, we proposed a novel neural discrim-
inative sentence model based on sentence parsing
structures. Our model can be built upon either
constituency trees (denoted as c-TBCNN) or de-
pendency trees (d-TBCNN).
Both variants have achieved high performance
in sentiment analysis and question classification.
d-TBCNN is slightly better than c-TBCNN in our
experiments, and has outperformed previous state-
of-the-art results in both tasks. The results show
that tree-based convolution can capture sentences’
structural information effectively, which is useful
for sentence modeling.
</bodyText>
<sectionHeader confidence="0.994753" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9778402">
This research is supported by the National Basic
Research Program of China (the 973 Program) un-
der Grant No. 2015CB352201 and the National
Natural Science Foundation of China under Grant
Nos. 61232015 and 91318301.
</bodyText>
<sectionHeader confidence="0.98076" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99854525">
James Allan, Courtney Wade, and Alvaro Bolivar.
2003. Retrieval and novelty detection at the sen-
tence level. In Proceedings of the 26th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Informaion Retrieval, pages 314–
321. ACM.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neu-
ral Networks, 5(2):157–166.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Yoshua Bengio, Aaron Courville, and Pierre Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(8):1798–1828.
Phil Blunsom, Edward Grefenstette, and Nal Kalch-
brenner. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine learning, pages 160–167.
Hang Cui, Vibhu Mittal, and Mayur Datar. 2006.
Comparative experiments on sentiment classifica-
tion for online product reviews. In Proceedings
21st AAAI Conference on Artificial Intelligence, vol-
ume 6, pages 1265–1270.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of Language Resource and Evaluation
Conference, volume 6, pages 449–454.
Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua
Bengio, Samy Bengio, and Pascal Vincent. 2009.
The difficulty of training deep architectures and the
2323
effect of unsupervised pre-training. In Proceed-
ings of International Conference on Artificial Intel-
ligence and Statistics, pages 153–160.
Christopher Fox. 1989. A stop list for general text. In
ACM SIGIR Forum, volume 24, pages 19–21.
Karl M. Hermann and Phil Blunsom. 2013. The role
of syntax in vector space models of compositional
semantics. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 894–904.
Sepp Hochreiter and Jurgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems, pages 2042–2050.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, pages 633–644.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing.
Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of the 31st International Conference on
Machine Learning.
Phong Le and Willem Zuidema. 2015. Compositional
distributional semantics with long short term mem-
ory. arXiv preprint arXiv:1503.02510.
Yann LeCun. 1995. Comparison of learning algo-
rithms for handwritten digit recognition. In Pro-
ceedings of International Conference on Artificial
Neural Networks, volume 60, pages 53–60.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of European Conference of Machine
Learning, pages 318–329. Springer.
Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang.
2014. TBCNN: A tree-based convolutional neu-
ral network for programming language processing.
arXiv preprint arXiv:1409.5718.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted Boltzmann machines.
In Proceedings of the 27th International Conference
on Machine Learning, pages 807–814.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using CRFs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 786–794.
Frank Reichartz, Hannes Korte, and Gerhard Paass.
2010. Semantic relation extraction with kernels over
typed dependency trees. In Proceedings of the 16th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 773–782.
Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing, pages 1577–1586.
Joao Silva, Lu´ısa Coheur, Ana C. Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
formation in question classification. Artificial Intel-
ligence Review, 35(2):137–154.
Richard Socher, Eric H. Huang, Jeffrey Pennin,
Christopher D. Manning, and Andrew Y. Ng. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In Advances in
Neural Information Processing Systems, pages 801–
809.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of Conference on Empiri-
cal Methods in Natural Language Processing, pages
1631–1642.
2324
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.
Fangzhong Su and Katja Markert. 2008. From words
to senses: a case study of subjectivity recognition.
In Proceedings of the 22nd International Conference
on Computational Linguistics-Volume 1, pages 825–
832. Association for Computational Linguistics.
Kaisheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing, pages 1556–1566.
Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. In Pro-
ceedings of the Twenty-Fourth International Joint
Conference on Artificial Intelligence, pages 4069–
4076.
Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
In Proceedings of The 32nd International Confer-
ence on Machine Learning, pages 1604–1612.
</reference>
<page confidence="0.661165">
2325
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.265893">
<title confidence="0.9819005">Discriminative Neural Sentence by Tree-Based Convolution</title>
<author confidence="0.99751">Hao Ge Yan Xu</author>
<author confidence="0.99751">Lu Zhang</author>
<author confidence="0.99751">Zhi Jin</author>
<email confidence="0.564952">xuyan14,zhanglu,</email>
<address confidence="0.41413">Software Institute, Peking University, 100871, P. R. China</address>
<abstract confidence="0.9994485">This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling. Our model leverages either constituency trees or dependency trees of sentences. The tree-based convolution process extracts sentences structural features, which are then aggregated by max pooling. Such architecture allows short propagation paths between the output layer and underlying feature detectors, enabling effective structural feature learning and extraction. We evaluate our models on two tasks: sentiment analysis and question classification. In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Courtney Wade</author>
<author>Alvaro Bolivar</author>
</authors>
<title>Retrieval and novelty detection at the sentence level.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,</booktitle>
<pages>314--321</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1370" citStr="Allan et al., 2003" startWordPosition="181" endWordPosition="184">n. We evaluate our models on two tasks: sentiment analysis and question classification. In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work. 1 Introduction Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techn</context>
</contexts>
<marker>Allan, Wade, Bolivar, 2003</marker>
<rawString>James Allan, Courtney Wade, and Alvaro Bolivar. 2003. Retrieval and novelty detection at the sentence level. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, pages 314– 321. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult.</title>
<date>1994</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="8526" citStr="Bengio et al., 1994" startWordPosition="1294" endWordPosition="1297">ch leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015). Recurrent networks. A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs. 3 Tree-based Convolution This section introduces the proposed tree-based convolutional neural networks (TBCNNs). Figure 1c depicts the convolution process on a tree. First, a sentence is converted to a parse tree, either a constituency or dependency tree. The corresponding model variants are denoted as c-TBCNN and d-TBCNN. Each node in the tree is represented as a distributed, real-valued vector. Then, we design a set of fixed-depth subtree feature detectors</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2075" citStr="Bengio et al. (2003)" startWordPosition="286" endWordPosition="289">eatures (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space. Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representations. Compared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013). Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs). CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inher</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pierre Vincent</author>
</authors>
<title>Representation learning: A review and new perspectives.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>8</issue>
<contexts>
<context position="2439" citStr="Bengio et al., 2013" startWordPosition="343" endWordPosition="346">ation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space. Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representations. Compared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013). Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs). CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parse trees). RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree. However, they may have difficulties in learning deep dependencies because of long propagation paths (Erhan et al., 2009). (CNNs/RNNs and a variant, recurrent networks, will be reviewed in Section 2.) A curious que</context>
</contexts>
<marker>Bengio, Courville, Vincent, 2013</marker>
<rawString>Yoshua Bengio, Aaron Courville, and Pierre Vincent. 2013. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Edward Grefenstette</author>
<author>Nal Kalchbrenner</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6444" citStr="Blunsom et al. (2014)" startWordPosition="959" endWordPosition="963">at the current position, is y = f (W · [x1; ··· ; xt] + b) where y ∈ Rn. (nc is the number of feature detectors). W ∈ Rn.×(t·ne) and b ∈ Rn. are parame2https://sites.google.com/site/tbcnnsentence/ ters; f is the activation function. Semicolons represent column vector concatenation. After convolution, the extracted features are pooled to a fixedsize vector for classification. Convolution can extract neighboring information effectively. However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related. Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers. Similar CNNs include Kim (2014) and Hu et al. (2014). All these models are “flat,” by which we mean no structural information is used explicitly. 2.2 Recursive Neural Networks Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees. In the original version, RNN is built upon a binarized constituency tree. Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. </context>
<context position="19602" citStr="Blunsom et al. (2014)" startWordPosition="3119" endWordPosition="3122">, neutral, negative, and strongly negative), and (2) coarse-gained polarity classification with 2 labels (positive versus negative). Some examples are shown in 4http://nlp.stanford.edu/sentiment/ 2319 Table 1. We use the standard split for training, validating, and testing, containing 8544/1101/2210 sentences for 5-class prediction. Binary classification does not contain the neutral class. In the dataset, phrases (sub-sentences) are also tagged with sentiment labels. RNNs deal with them naturally during the recursive process. We regard sub-sentences as individual samples during training, like Blunsom et al. (2014) and Le and Mikolov (2014). The training set therefore has more than 150,000 entries in total. For validating and testing, only whole sentences (root labels) are considered in our experiments. Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing. 4.1.2 Training Details This subsection describes training details for dTBCNN, where hyperparameters are chosen by validation. c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters. c-TBCNN’s settings can be found on our website. In our d-TBCNN model, the number</context>
<context position="23833" citStr="Blunsom et al. (2014)" startWordPosition="3776" endWordPosition="3779">ient to embeddings in this 5Richard Socher, who first applies neural networks to this task, thinks direct transfer is fine for binary classification. We followed this strategy for simplicity as it is non-trivial to deal with the neutral sub-sentences in the training set if we train a separate model. Our website reviews some related work and provides more discussions. 6http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2320 Group Method 5-class accuracy 2-class accuracy Reported in Baseline SVM 40.7 79.4 Socher et al. (2013) Naive Bayes 41.0 81.8 Socher et al. (2013) CNNs 1-layer convolution 37.4 77.1 Blunsom et al. (2014) Deep CNN 48.5 86.8 Blunsom et al. (2014) Non-static 48.0 87.2 Kim (2014) Multichannel 47.4 88.1 Kim (2014) RNNs Basic 43.2 82.4 Socher et al. (2013) Matrix-vector 44.4 82.9 Socher et al. (2013) Tensor 45.7 85.4 Socher et al. (2013) Tree LSTM (variant 1) 48.0 – Zhu et al. (2015) Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Miko</context>
</contexts>
<marker>Blunsom, Grefenstette, Kalchbrenner, 2014</marker>
<rawString>Phil Blunsom, Edward Grefenstette, and Nal Kalchbrenner. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="5592" citStr="Collobert and Weston, 2008" startWordPosition="817" endWordPosition="820">lts in both experiments. To understand how TBCNNs work, we also visualize the network by plotting the convolution process. We make our code and results available on our project website.2 2 Background and Related Work In this section, we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling. 2.1 Convolutional Neural Networks Convolutional neural networks (CNNs), early used for image processing (LeCun, 1995), turn out to be effective with natural languages as well. Figure 1a depicts a classic convolution process on a sentence (Collobert and Weston, 2008). A set of fixed-width-window feature detectors slide over the sentence, and output the extracted features. Let t be the window size, and x1, · · · , xt ∈ Rne be ne-dimensional word embeddings. The output of convolution, evaluated at the current position, is y = f (W · [x1; ··· ; xt] + b) where y ∈ Rn. (nc is the number of feature detectors). W ∈ Rn.×(t·ne) and b ∈ Rn. are parame2https://sites.google.com/site/tbcnnsentence/ ters; f is the activation function. Semicolons represent column vector concatenation. After convolution, the extracted features are pooled to a fixedsize vector for classif</context>
<context position="14700" citStr="Collobert and Weston (2008)" startWordPosition="2317" endWordPosition="2321">ce, tree-based convolution, compared with “flat” CNNs, does not add to computational cost, provided the same amount of information to process at a time. In our experiments, we use convolution windows of depth 2. 3.2 d-TBCNN Dependency trees are another representation of sentence structures. The nature of dependency representation leads to d-TBCNN’s major difference from traditional convolution: there exist nodes with different numbers of child nodes. This causes trouble if we associate weight parameters according to positions in the window, which is standard for traditional convolution, e.g., Collobert and Weston (2008) or c-TBCNN. To overcome the problem, we extend the notion of convolution by assigning weights according to dependency types (e.g, nsubj) rather than positions. We believe this strategy makes much sense because dependency types (de Marneffe et al., 2006) reflect the relationship between a governing word and its child words. To be concrete, the generic convolution formula (Equation 2) for d-TBCNN becomes = f where W(d) 2318 Task Data samples Label Sentiment Offers that rare combination of entertainment and education. ++ Analysis An idealistic love story that brings out the latent 15-year-old ro</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine learning, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Vibhu Mittal</author>
<author>Mayur Datar</author>
</authors>
<title>Comparative experiments on sentiment classification for online product reviews.</title>
<date>2006</date>
<booktitle>In Proceedings 21st AAAI Conference on Artificial Intelligence,</booktitle>
<volume>6</volume>
<pages>1265--1270</pages>
<contexts>
<context position="1481" citStr="Cui et al., 2006" startWordPosition="199" endWordPosition="202">NN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work. 1 Introduction Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and M</context>
</contexts>
<marker>Cui, Mittal, Datar, 2006</marker>
<rawString>Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. Comparative experiments on sentiment classification for online product reviews. In Proceedings 21st AAAI Conference on Artificial Intelligence, volume 6, pages 1265–1270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of Language Resource and Evaluation Conference,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of Language Resource and Evaluation Conference, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dumitru Erhan</author>
<author>Pierre-Antoine Manzagol</author>
<author>Yoshua Bengio</author>
<author>Samy Bengio</author>
<author>Pascal Vincent</author>
</authors>
<title>The difficulty of training deep architectures and the effect of unsupervised pre-training.</title>
<date>2009</date>
<booktitle>In Proceedings of International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>153--160</pages>
<contexts>
<context position="2946" citStr="Erhan et al., 2009" startWordPosition="416" endWordPosition="419">pared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013). Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs). CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parse trees). RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree. However, they may have difficulties in learning deep dependencies because of long propagation paths (Erhan et al., 2009). (CNNs/RNNs and a variant, recurrent networks, will be reviewed in Section 2.) A curious question is whether we can combine the advantages of CNNs and RNNs, i.e., whether we can exploit sentence structures (like RNNs) effectively with short propagation paths (like CNNs). In this paper, we propose a novel neural architecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Network (TBCNN).1 Our models can leverage different sentence parse trees, e.g., constituency trees and dependency trees. The model variants are denoted as c-TBCNN and d-TBCNN, respectively. Th</context>
<context position="8208" citStr="Erhan et al., 2009" startWordPosition="1241" endWordPosition="1244">vements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013). They are more suitable for capturing logical information in sentences, such as negation and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015). Recurrent networks. A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs. 3 Tree-based Convolution This section introduces the proposed tree-based convolutional neural networks (TBCNNs). Figure 1c depicts the convoluti</context>
</contexts>
<marker>Erhan, Manzagol, Bengio, Bengio, Vincent, 2009</marker>
<rawString>Dumitru Erhan, Pierre-Antoine Manzagol, Yoshua Bengio, Samy Bengio, and Pascal Vincent. 2009. The difficulty of training deep architectures and the effect of unsupervised pre-training. In Proceedings of International Conference on Artificial Intelligence and Statistics, pages 153–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Fox</author>
</authors>
<title>A stop list for general text.</title>
<date>1989</date>
<journal>In ACM SIGIR Forum,</journal>
<volume>24</volume>
<pages>pages</pages>
<contexts>
<context position="30540" citStr="Fox, 1989" startWordPosition="4856" endWordPosition="4857">processed by dTBCNN in the task of sentiment analysis.7 Here, we applied global pooling because information tracing is more sensible with one pooling slot. As shown in the figure, tree-based convolution can effectively extract information relevant to the task of interest. The 2-layer windows corresponding to “visual will impress viewers,” “the stunning dreamlike visual,” say, are discriminative to the sentence’s sentiment. Hence, large fractions (0.24 and 0.19) of their features, after convolution, are gathered by pooling. On the other hand, words like the, will, even are known as stop words (Fox, 1989). They are mostly noninformative for sentiment; hence, no (or minimal) features are gathered. Such results are consistent with human intuition. We further observe that tree-based convolution does integrate information of different words in the window. For example, the word stunning appears in two windows: (a) the window “stunning” itself, and (b) the window of “the stunning dreamlike visual,” with root node visual, stunning acting as a child. We see that Window b is more relevant to the ultimate sentiment than Window a, with fractions 0.19 versus 0.07, even though the root visual itself is neu</context>
</contexts>
<marker>Fox, 1989</marker>
<rawString>Christopher Fox. 1989. A stop list for general text. In ACM SIGIR Forum, volume 24, pages 19–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl M Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>894--904</pages>
<contexts>
<context position="7480" citStr="Hermann and Blunsom, 2013" startWordPosition="1134" endWordPosition="1137">arized constituency tree. Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2. The parent’s representation is composited by p = f(W · [c1; c2] + b) (1) where W and b are parameters. This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b). Dependency parse and the combinatory categorical grammar can also be exploited as RNNs’ skeletons (Hermann and Blunsom, 2013; Iyyer et al., 2014). Irsoy and Cardie (2014) build deep RNNs to enhance information interaction. Im2316 provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013). They are more suitable for capturing logical information in sentences, such as negation and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, duri</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl M. Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of compositional semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 894–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>Jurgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="8320" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1257" endWordPosition="1260">sor interaction (Socher et al., 2013). They are more suitable for capturing logical information in sentences, such as negation and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015). Recurrent networks. A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs. 3 Tree-based Convolution This section introduces the proposed tree-based convolutional neural networks (TBCNNs). Figure 1c depicts the convolution process on a tree. First, a sentence is converted to a parse tree, either a constituency or dependency tree. </context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2042--2050</pages>
<contexts>
<context position="6584" citStr="Hu et al. (2014)" startWordPosition="984" endWordPosition="987">parame2https://sites.google.com/site/tbcnnsentence/ ters; f is the activation function. Semicolons represent column vector concatenation. After convolution, the extracted features are pooled to a fixedsize vector for classification. Convolution can extract neighboring information effectively. However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related. Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers. Similar CNNs include Kim (2014) and Hu et al. (2014). All these models are “flat,” by which we mean no structural information is used explicitly. 2.2 Recursive Neural Networks Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees. In the original version, RNN is built upon a binarized constituency tree. Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2. The parent’s representation is composited by p = f(W</context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems, pages 2042–2050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="7526" citStr="Irsoy and Cardie (2014)" startWordPosition="1142" endWordPosition="1145">to words in a sentence, represented by nedimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2. The parent’s representation is composited by p = f(W · [c1; c2] + b) (1) where W and b are parameters. This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b). Dependency parse and the combinatory categorical grammar can also be exploited as RNNs’ skeletons (Hermann and Blunsom, 2013; Iyyer et al., 2014). Irsoy and Cardie (2014) build deep RNNs to enhance information interaction. Im2316 provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013). They are more suitable for capturing logical information in sentences, such as negation and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradient</context>
<context position="21381" citStr="Irsoy and Cardie (2014)" startWordPosition="3394" endWordPosition="3397"> 2014) is further applied to both weights and embeddings. All hidden layers are dropped out by 50%, and embeddings 40%. 4.1.3 Performance Table 2 compares our models to state-of-the-art results in the task of sentiment analysis. For 5- class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory (Tai et al., 2015). c-TBCNN is slightly worse. It achieves 50.4% accuracy, ranking third in the state-of-the-art list (including our d-TBCNN model). Regarding 2-class prediction, we adopted a simple strategy in Irsoy and Cardie (2014),5 where the 5-class network is “transferred” directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes. (The neutral class is discarded as in other studies.) This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position. Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking forth in the list. In a more controlled comparison—with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)— TBCNNs, of both variants, consistently </context>
<context position="24260" citStr="Irsoy and Cardie (2014)" startWordPosition="3854" endWordPosition="3857">Method 5-class accuracy 2-class accuracy Reported in Baseline SVM 40.7 79.4 Socher et al. (2013) Naive Bayes 41.0 81.8 Socher et al. (2013) CNNs 1-layer convolution 37.4 77.1 Blunsom et al. (2014) Deep CNN 48.5 86.8 Blunsom et al. (2014) Non-static 48.0 87.2 Kim (2014) Multichannel 47.4 88.1 Kim (2014) RNNs Basic 43.2 82.4 Socher et al. (2013) Matrix-vector 44.4 82.9 Socher et al. (2013) Tensor 45.7 85.4 Socher et al. (2013) Tree LSTM (variant 1) 48.0 – Zhu et al. (2015) Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Mikolov (2014) TBCNNs c-TBCNN 50.4 86.81 Our implementation d-TBCNN 51.4 87.91 Our implementation Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate that the network is transferred directly from that of 5-class. Method Acc. (%) Reported in SVM 95.0 Silva et al. (2011) 10k features + 60 rules CNN-non-static 93.6 Kim (2014) CNN-mutlichannel 92.2 Kim (2014) RNN 90.2 Zhao et al. </context>
<context position="27855" citStr="Irsoy and Cardie (2014)" startWordPosition="4422" endWordPosition="4425">r c-TBCNN and 2-slot pooling for d-TBCNN. 2321 Model Pooling method 5-class accuracy (%) Global 48.48 f 0.54 c-TBCNN 3-slot 48.69 f 0.40 Global 49.39 f 0.24 d-TBCNN 2-slot 49.94 f 0.63 Table 4: Accuracies of different pooling methods, averaged over 5 random initializations. We chose sensible hyperparameters manually in advance to make a fair comparison. This leads to performance degradation (1–2%) vis-a-vis Table 2. Figure 4: Accuracies versus sentence lengths. Comparing with other studies in the literature, we also notice that pooling is very effective and efficient in information gathering. Irsoy and Cardie (2014) report 200 epochs for training a deep RNN, which achieves 49.8% accuracy in the 5-class sentiment classification. Our TBCNNs are typically trained within 25 epochs. 4.3.2 The Effect of Sentence Lengths We analyze how sentence lengths affect our models. Sentences are split into 7 groups by length, with granularity 5. A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126 to 457. Figure 4 presents accuracies versus lengths in TBCNNs. For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy, slightly worse</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Iyyer</author>
<author>Jordan Boyd-Graber</author>
<author>Leonardo Claudino</author>
<author>Richard Socher</author>
<author>Hal Daum´e</author>
</authors>
<title>A neural network for factoid question answering over paragraphs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>633--644</pages>
<marker>Iyyer, Boyd-Graber, Claudino, Socher, Daum´e, 2014</marker>
<rawString>Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. 2014. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 633–644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6563" citStr="Kim (2014)" startWordPosition="981" endWordPosition="982">nd b ∈ Rn. are parame2https://sites.google.com/site/tbcnnsentence/ ters; f is the activation function. Semicolons represent column vector concatenation. After convolution, the extracted features are pooled to a fixedsize vector for classification. Convolution can extract neighboring information effectively. However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related. Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers. Similar CNNs include Kim (2014) and Hu et al. (2014). All these models are “flat,” by which we mean no structural information is used explicitly. 2.2 Recursive Neural Networks Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees. In the original version, RNN is built upon a binarized constituency tree. Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2. The parent’s representation is </context>
<context position="23906" citStr="Kim (2014)" startWordPosition="3791" endWordPosition="3792">s task, thinks direct transfer is fine for binary classification. We followed this strategy for simplicity as it is non-trivial to deal with the neutral sub-sentences in the training set if we train a separate model. Our website reviews some related work and provides more discussions. 6http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2320 Group Method 5-class accuracy 2-class accuracy Reported in Baseline SVM 40.7 79.4 Socher et al. (2013) Naive Bayes 41.0 81.8 Socher et al. (2013) CNNs 1-layer convolution 37.4 77.1 Blunsom et al. (2014) Deep CNN 48.5 86.8 Blunsom et al. (2014) Non-static 48.0 87.2 Kim (2014) Multichannel 47.4 88.1 Kim (2014) RNNs Basic 43.2 82.4 Socher et al. (2013) Matrix-vector 44.4 82.9 Socher et al. (2013) Tensor 45.7 85.4 Socher et al. (2013) Tree LSTM (variant 1) 48.0 – Zhu et al. (2015) Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Mikolov (2014) TBCNNs c-TBCNN 50.4 86.81 Our implementation d-TBCNN 51.4 87.9</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2247" citStr="Mikolov (2014)" startWordPosition="315" endWordPosition="316">nel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space. Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representations. Compared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013). Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs). CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parse trees). RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree. However, they may hav</context>
<context position="19628" citStr="Mikolov (2014)" startWordPosition="3126" endWordPosition="3127"> negative), and (2) coarse-gained polarity classification with 2 labels (positive versus negative). Some examples are shown in 4http://nlp.stanford.edu/sentiment/ 2319 Table 1. We use the standard split for training, validating, and testing, containing 8544/1101/2210 sentences for 5-class prediction. Binary classification does not contain the neutral class. In the dataset, phrases (sub-sentences) are also tagged with sentiment labels. RNNs deal with them naturally during the recursive process. We regard sub-sentences as individual samples during training, like Blunsom et al. (2014) and Le and Mikolov (2014). The training set therefore has more than 150,000 entries in total. For validating and testing, only whole sentences (root labels) are considered in our experiments. Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing. 4.1.2 Training Details This subsection describes training details for dTBCNN, where hyperparameters are chosen by validation. c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters. c-TBCNN’s settings can be found on our website. In our d-TBCNN model, the number of units is 300 for convo</context>
<context position="24443" citStr="Mikolov (2014)" startWordPosition="3889" endWordPosition="3890">014) Deep CNN 48.5 86.8 Blunsom et al. (2014) Non-static 48.0 87.2 Kim (2014) Multichannel 47.4 88.1 Kim (2014) RNNs Basic 43.2 82.4 Socher et al. (2013) Matrix-vector 44.4 82.9 Socher et al. (2013) Tensor 45.7 85.4 Socher et al. (2013) Tree LSTM (variant 1) 48.0 – Zhu et al. (2015) Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Mikolov (2014) TBCNNs c-TBCNN 50.4 86.81 Our implementation d-TBCNN 51.4 87.91 Our implementation Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate that the network is transferred directly from that of 5-class. Method Acc. (%) Reported in SVM 95.0 Silva et al. (2011) 10k features + 60 rules CNN-non-static 93.6 Kim (2014) CNN-mutlichannel 92.2 Kim (2014) RNN 90.2 Zhao et al. (2015) Deep-CNN 93.0 Blunsom et al. (2014) Ada-CNN 92.4 Zhao et al. (2015) c-TBCNN 94.8 Our implementation d-TBCNN 96.0 Our implementation Table 3: Accuracy of 6-way question classifi</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31st International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>Compositional distributional semantics with long short term memory. arXiv preprint arXiv:1503.02510.</title>
<date>2015</date>
<contexts>
<context position="8409" citStr="Zuidema, 2015" startWordPosition="1276" endWordPosition="1277"> such as negation and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015). Recurrent networks. A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs. 3 Tree-based Convolution This section introduces the proposed tree-based convolutional neural networks (TBCNNs). Figure 1c depicts the convolution process on a tree. First, a sentence is converted to a parse tree, either a constituency or dependency tree. The corresponding model variants are denoted as c-TBCNN and d-TBCNN. Each node in the tre</context>
<context position="24216" citStr="Zuidema (2015)" startWordPosition="3848" endWordPosition="3849">llinois.edu/Data/QA/QC/ 2320 Group Method 5-class accuracy 2-class accuracy Reported in Baseline SVM 40.7 79.4 Socher et al. (2013) Naive Bayes 41.0 81.8 Socher et al. (2013) CNNs 1-layer convolution 37.4 77.1 Blunsom et al. (2014) Deep CNN 48.5 86.8 Blunsom et al. (2014) Non-static 48.0 87.2 Kim (2014) Multichannel 47.4 88.1 Kim (2014) RNNs Basic 43.2 82.4 Socher et al. (2013) Matrix-vector 44.4 82.9 Socher et al. (2013) Tensor 45.7 85.4 Socher et al. (2013) Tree LSTM (variant 1) 48.0 – Zhu et al. (2015) Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Mikolov (2014) TBCNNs c-TBCNN 50.4 86.81 Our implementation d-TBCNN 51.4 87.91 Our implementation Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate that the network is transferred directly from that of 5-class. Method Acc. (%) Reported in SVM 95.0 Silva et al. (2011) 10k features + 60 rules CNN-non-static 93.6 Kim (2014) CNN-mutlic</context>
</contexts>
<marker>Zuidema, 2015</marker>
<rawString>Phong Le and Willem Zuidema. 2015. Compositional distributional semantics with long short term memory. arXiv preprint arXiv:1503.02510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
</authors>
<title>Comparison of learning algorithms for handwritten digit recognition.</title>
<date>1995</date>
<booktitle>In Proceedings of International Conference on Artificial Neural Networks,</booktitle>
<volume>60</volume>
<pages>53--60</pages>
<contexts>
<context position="5443" citStr="LeCun, 1995" startWordPosition="795" endWordPosition="796">re evaluated on two tasks, sentiment analysis and question classification; our models have outperformed previous state-of-the-art results in both experiments. To understand how TBCNNs work, we also visualize the network by plotting the convolution process. We make our code and results available on our project website.2 2 Background and Related Work In this section, we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling. 2.1 Convolutional Neural Networks Convolutional neural networks (CNNs), early used for image processing (LeCun, 1995), turn out to be effective with natural languages as well. Figure 1a depicts a classic convolution process on a sentence (Collobert and Weston, 2008). A set of fixed-width-window feature detectors slide over the sentence, and output the extracted features. Let t be the window size, and x1, · · · , xt ∈ Rne be ne-dimensional word embeddings. The output of convolution, evaluated at the current position, is y = f (W · [x1; ··· ; xt] + b) where y ∈ Rn. (nc is the number of feature detectors). W ∈ Rn.×(t·ne) and b ∈ Rn. are parame2https://sites.google.com/site/tbcnnsentence/ ters; f is the activati</context>
</contexts>
<marker>LeCun, 1995</marker>
<rawString>Yann LeCun. 1995. Comparison of learning algorithms for handwritten digit recognition. In Proceedings of International Conference on Artificial Neural Networks, volume 60, pages 53–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2101" citStr="Mikolov et al. (2013)" startWordPosition="291" endWordPosition="294">), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space. Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representations. Compared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013). Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs). CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e</context>
<context position="20365" citStr="Mikolov et al., 2013" startWordPosition="3236" endWordPosition="3239">s (root labels) are considered in our experiments. Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing. 4.1.2 Training Details This subsection describes training details for dTBCNN, where hyperparameters are chosen by validation. c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters. c-TBCNN’s settings can be found on our website. In our d-TBCNN model, the number of units is 300 for convolution and 200 for the last hidden layer. Word embeddings are 300 dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the English Wikipedia corpus. 2- slot pooling is applied for d-TBCNN. (c-TBCNN uses 3-slot pooling.) To train our model, we compute gradient by back-propagation and apply stochastic gradient descent with mini-batch 200. We use ReLU (Nair and Hinton, 2010) as the activation function. For regularization, we add E2 penalty for weights with a coefficient of 10−5. Dropout (Srivastava et al., 2014) is further applied to both weights and embeddings. All hidden layers are dropped out by 50%, and embeddings 40%. 4.1.3 Performance Table 2 compares our models to state-of-the-art results in the task o</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of European Conference of Machine Learning,</booktitle>
<pages>318--329</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1691" citStr="Moschitti (2006)" startWordPosition="233" endWordPosition="234">n how our models work. 1 Introduction Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space. Le and Mikolov (2014) extend such approaches to learn sentences’ </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of European Conference of Machine Learning, pages 318–329. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Mou</author>
<author>Ge Li</author>
<author>Zhi Jin</author>
<author>Lu Zhang</author>
<author>Tao Wang</author>
</authors>
<title>TBCNN: A tree-based convolutional neural network for programming language processing. arXiv preprint arXiv:1409.5718.</title>
<date>2014</date>
<contexts>
<context position="3793" citStr="Mou et al., 2014" startWordPosition="548" endWordPosition="551"> with short propagation paths (like CNNs). In this paper, we propose a novel neural architecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Network (TBCNN).1 Our models can leverage different sentence parse trees, e.g., constituency trees and dependency trees. The model variants are denoted as c-TBCNN and d-TBCNN, respectively. The idea of tree-based convolution is to apply a set of subtree feature detectors, sliding over the entire 1The model of tree-based convolution was firstly proposed to process program source code in our (unpublished) previous work (Mou et al., 2014). 2315 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2315–2325, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. (a) CNN (b) RNN (c) TBCNN Figure 1: A comparison of information flow in the convolutional neural network (CNN), the recursive neural network (RNN), and the tree-based convolutional neural network (TBCNN). Max pooling by heuristics Softmax Softmax Softmax Representing hidden layers as vectors recursively W W r l Hidden Output layer layer W W W l r Extracted features by convolution Embeddings Word</context>
</contexts>
<marker>Mou, Li, Jin, Zhang, Wang, 2014</marker>
<rawString>Lili Mou, Ge Li, Zhi Jin, Lu Zhang, and Tao Wang. 2014. TBCNN: A tree-based convolutional neural network for programming language processing. arXiv preprint arXiv:1409.5718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve restricted Boltzmann machines.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="20624" citStr="Nair and Hinton, 2010" startWordPosition="3276" endWordPosition="3279">is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters. c-TBCNN’s settings can be found on our website. In our d-TBCNN model, the number of units is 300 for convolution and 200 for the last hidden layer. Word embeddings are 300 dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the English Wikipedia corpus. 2- slot pooling is applied for d-TBCNN. (c-TBCNN uses 3-slot pooling.) To train our model, we compute gradient by back-propagation and apply stochastic gradient descent with mini-batch 200. We use ReLU (Nair and Hinton, 2010) as the activation function. For regularization, we add E2 penalty for weights with a coefficient of 10−5. Dropout (Srivastava et al., 2014) is further applied to both weights and embeddings. All hidden layers are dropped out by 50%, and embeddings 40%. 4.1.3 Performance Table 2 compares our models to state-of-the-art results in the task of sentiment analysis. For 5- class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory (Tai et al., 2015). c-TBCNN is slightly worse. It achieves 50.4% accuracy, ra</context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E. Hinton. 2010. Rectified linear units improve restricted Boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning, pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using CRFs with hidden variables.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>786--794</pages>
<contexts>
<context position="1534" citStr="Nakagawa et al., 2010" startWordPosition="207" endWordPosition="210">, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work. 1 Introduction Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches </context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 786–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Reichartz</author>
<author>Hannes Korte</author>
<author>Gerhard Paass</author>
</authors>
<title>Semantic relation extraction with kernels over typed dependency trees.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>773--782</pages>
<contexts>
<context position="1719" citStr="Reichartz et al. (2010)" startWordPosition="236" endWordPosition="240">. 1 Introduction Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space. Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representati</context>
</contexts>
<marker>Reichartz, Korte, Paass, 2010</marker>
<rawString>Frank Reichartz, Hannes Korte, and Gerhard Paass. 2010. Semantic relation extraction with kernels over typed dependency trees. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 773–782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lifeng Shang</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
</authors>
<title>Neural responding machine for short-text conversation.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1577--1586</pages>
<contexts>
<context position="8547" citStr="Shang et al., 2015" startWordPosition="1298" endWordPosition="1301">nected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015). Recurrent networks. A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs. 3 Tree-based Convolution This section introduces the proposed tree-based convolutional neural networks (TBCNNs). Figure 1c depicts the convolution process on a tree. First, a sentence is converted to a parse tree, either a constituency or dependency tree. The corresponding model variants are denoted as c-TBCNN and d-TBCNN. Each node in the tree is represented as a distributed, real-valued vector. Then, we design a set of fixed-depth subtree feature detectors, called the tree-bas</context>
</contexts>
<marker>Shang, Lu, Li, 2015</marker>
<rawString>Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural responding machine for short-text conversation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1577–1586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joao Silva</author>
<author>Lu´ısa Coheur</author>
<author>Ana C Mendes</author>
<author>Andreas Wichert</author>
</authors>
<title>From symbolic to sub-symbolic information in question classification.</title>
<date>2011</date>
<journal>Artificial Intelligence Review,</journal>
<volume>35</volume>
<issue>2</issue>
<contexts>
<context position="1579" citStr="Silva et al., 2011" startWordPosition="215" endWordPosition="218">ed feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work. 1 Introduction Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential. Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete wo</context>
<context position="22708" citStr="Silva et al. (2011)" startWordPosition="3599" endWordPosition="3602">utperform “flat” CNNs by more than 10%. Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information more effectively than RNNs. We also observe d-TBCNN achieves higher performance than c-TBCNN. This suggests that compact tree expressiveness is more important than integrating global information in this task. 4.2 Question Classification We further evaluate TBCNN models on a question classification task.6 The dataset contains 5452 annotated sentences plus 500 test samples in TREC 10. We also use the standard split, like Silva et al. (2011). Target labels contain 6 classes, namely abbreviation, entity, description, human, location, and numeric. Some examples are also shown in Table 1. We chose this task to evaluate our models because the number of training samples is rather small, so that we can know TBCNNs’ performance when applied to datasets of different sizes. To alleviate the problem of data sparseness, we set the dimensions of convolutional layer and the last hidden layer to 30 and 25, respectively. We do not back-propagate gradient to embeddings in this 5Richard Socher, who first applies neural networks to this task, thin</context>
<context position="24750" citStr="Silva et al. (2011)" startWordPosition="3933" endWordPosition="3936"> 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Mikolov (2014) TBCNNs c-TBCNN 50.4 86.81 Our implementation d-TBCNN 51.4 87.91 Our implementation Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate that the network is transferred directly from that of 5-class. Method Acc. (%) Reported in SVM 95.0 Silva et al. (2011) 10k features + 60 rules CNN-non-static 93.6 Kim (2014) CNN-mutlichannel 92.2 Kim (2014) RNN 90.2 Zhao et al. (2015) Deep-CNN 93.0 Blunsom et al. (2014) Ada-CNN 92.4 Zhao et al. (2015) c-TBCNN 94.8 Our implementation d-TBCNN 96.0 Our implementation Table 3: Accuracy of 6-way question classification. task. Dropout rate for embeddings is 30%; hidden layers are dropped out by 5%. Table 3 compares our models to various other methods. The first entry presents the previous state-of-the-art result, achieved by traditional feature/rule engineering (Silva et al., 2011). Their method utilizes more than </context>
</contexts>
<marker>Silva, Coheur, Mendes, Wichert, 2011</marker>
<rawString>Joao Silva, Lu´ısa Coheur, Ana C. Mendes, and Andreas Wichert. 2011. From symbolic to sub-symbolic information in question classification. Artificial Intelligence Review, 35(2):137–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="6773" citStr="Socher et al. (2011" startWordPosition="1014" endWordPosition="1017">oled to a fixedsize vector for classification. Convolution can extract neighboring information effectively. However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related. Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers. Similar CNNs include Kim (2014) and Hu et al. (2014). All these models are “flat,” by which we mean no structural information is used explicitly. 2.2 Recursive Neural Networks Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees. In the original version, RNN is built upon a binarized constituency tree. Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2. The parent’s representation is composited by p = f(W · [c1; c2] + b) (1) where W and b are parameters. This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b). Dependency parse </context>
<context position="11265" citStr="Socher et al., 2011" startWordPosition="1748" endWordPosition="1751">h the third problem by introducing several pooling heuristics. Subsection 3.4 presents our training objective. 3.1 c-TBCNN Figure 2a illustrates an example of the constituency tree, where leaf nodes are words in the sentence, and non-leaf nodes represent a grammatical constituent, e.g., a noun phrase. Sentences are parsed by the Stanford parser;3 further, constituency trees are binarized for simplicity. One problem of constituency trees is that nonleaf nodes do not have such vector representations as word embeddings. Our strategy is to pretrain the constituency tree with an RNN by Equation 1 (Socher et al., 2011b). After pretraining, vector representations of nodes are fixed. We now consider the tree-based convolution process in c-TBCNN with a two-layer-subtree convolution window, which operates on a parent node p and its direct children cl and cr, their vector representations denoted as p, cl, and cr. The convolution equation, specific for c-TBCNN, is W((C) • p + Wi(�) • el + W (�) • er + b(�) y = f ( I where W(c) p , W(c) l , and W(c) r are weights associated with the parent and its child nodes. Superscript (c) indicates that the weights are for cTBCNN. For leaf nodes, which do not have children, w</context>
<context position="13849" citStr="Socher et al., 2011" startWordPosition="2186" endWordPosition="2189">keep 15 most frequently occurred dependency types; others appearing rarely in the corpus are mapped to one shared weight matrix. Both c-TBCNN and d-TBCNN have their own advantages: d-TBCNN exploits structural features more efficiently because of the compact expressiveness of dependency trees; c-TBCNN may be more effective in integrating global features due to the underneath pretrained RNN. 3.3 Pooling Heuristics As different sentences may have different lengths and tree structures, the extracted features by treebased convolution also have topologies varying in size and shape. Dynamic pooling (Socher et al., 2011a) is a common technique for dealing with Tree-based convolution windows can be extended to arbitrary depths straightforwardly. The complexity is exponential to the depth of the window, but linear to the number of nodes. Hence, tree-based convolution, compared with “flat” CNNs, does not add to computational cost, provided the same amount of information to process at a time. In our experiments, we use convolution windows of depth 2. 3.2 d-TBCNN Dependency trees are another representation of sentence structures. The nature of dependency representation leads to d-TBCNN’s major difference from tra</context>
<context position="22017" citStr="Socher et al., 2011" startWordPosition="3489" endWordPosition="3492">ass network is “transferred” directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes. (The neutral class is discarded as in other studies.) This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position. Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking forth in the list. In a more controlled comparison—with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)— TBCNNs, of both variants, consistently outperform RNNs (Socher et al., 2011b) to a large extent (50.4–51.4% versus 43.2%); they also consistently outperform “flat” CNNs by more than 10%. Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information more effectively than RNNs. We also observe d-TBCNN achieves higher performance than c-TBCNN. This suggests that compact tree expressiveness is more important than integrating global information in this task. 4.2 Question Classification We further evaluate TBCNN models on a question classification task.6 The dataset contains 5452 annotated sentences</context>
<context position="28498" citStr="Socher et al. (2011" startWordPosition="4528" endWordPosition="4531">raining a deep RNN, which achieves 49.8% accuracy in the 5-class sentiment classification. Our TBCNNs are typically trained within 25 epochs. 4.3.2 The Effect of Sentence Lengths We analyze how sentence lengths affect our models. Sentences are split into 7 groups by length, with granularity 5. A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126 to 457. Figure 4 presents accuracies versus lengths in TBCNNs. For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy, slightly worse than 43.2% reported in Socher et al. (2011b). Thus, we think our reimplementation is fair and that the comparison is sensible. We observe that c-TBCNN and d-TBCNN yield very similar behaviors. They consistently outperform the RNN in all scenarios. We also notice the gap, between TBCNNs and RNN, increases when sentences contain more than 20 words. This result confirms our theoretical analysis in Section 2—for long sentences, the propagation paths in RNNs are deep, causing RNNs’ difficulty in information processing. By contrast, our models explore structural information more effectively with tree-based convolution. As information from a</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennin, Christopher D. Manning, and Andrew Y. Ng. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801– 809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="6773" citStr="Socher et al. (2011" startWordPosition="1014" endWordPosition="1017">oled to a fixedsize vector for classification. Convolution can extract neighboring information effectively. However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related. Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers. Similar CNNs include Kim (2014) and Hu et al. (2014). All these models are “flat,” by which we mean no structural information is used explicitly. 2.2 Recursive Neural Networks Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees. In the original version, RNN is built upon a binarized constituency tree. Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2. The parent’s representation is composited by p = f(W · [c1; c2] + b) (1) where W and b are parameters. This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b). Dependency parse </context>
<context position="11265" citStr="Socher et al., 2011" startWordPosition="1748" endWordPosition="1751">h the third problem by introducing several pooling heuristics. Subsection 3.4 presents our training objective. 3.1 c-TBCNN Figure 2a illustrates an example of the constituency tree, where leaf nodes are words in the sentence, and non-leaf nodes represent a grammatical constituent, e.g., a noun phrase. Sentences are parsed by the Stanford parser;3 further, constituency trees are binarized for simplicity. One problem of constituency trees is that nonleaf nodes do not have such vector representations as word embeddings. Our strategy is to pretrain the constituency tree with an RNN by Equation 1 (Socher et al., 2011b). After pretraining, vector representations of nodes are fixed. We now consider the tree-based convolution process in c-TBCNN with a two-layer-subtree convolution window, which operates on a parent node p and its direct children cl and cr, their vector representations denoted as p, cl, and cr. The convolution equation, specific for c-TBCNN, is W((C) • p + Wi(�) • el + W (�) • er + b(�) y = f ( I where W(c) p , W(c) l , and W(c) r are weights associated with the parent and its child nodes. Superscript (c) indicates that the weights are for cTBCNN. For leaf nodes, which do not have children, w</context>
<context position="13849" citStr="Socher et al., 2011" startWordPosition="2186" endWordPosition="2189">keep 15 most frequently occurred dependency types; others appearing rarely in the corpus are mapped to one shared weight matrix. Both c-TBCNN and d-TBCNN have their own advantages: d-TBCNN exploits structural features more efficiently because of the compact expressiveness of dependency trees; c-TBCNN may be more effective in integrating global features due to the underneath pretrained RNN. 3.3 Pooling Heuristics As different sentences may have different lengths and tree structures, the extracted features by treebased convolution also have topologies varying in size and shape. Dynamic pooling (Socher et al., 2011a) is a common technique for dealing with Tree-based convolution windows can be extended to arbitrary depths straightforwardly. The complexity is exponential to the depth of the window, but linear to the number of nodes. Hence, tree-based convolution, compared with “flat” CNNs, does not add to computational cost, provided the same amount of information to process at a time. In our experiments, we use convolution windows of depth 2. 3.2 d-TBCNN Dependency trees are another representation of sentence structures. The nature of dependency representation leads to d-TBCNN’s major difference from tra</context>
<context position="22017" citStr="Socher et al., 2011" startWordPosition="3489" endWordPosition="3492">ass network is “transferred” directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes. (The neutral class is discarded as in other studies.) This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position. Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking forth in the list. In a more controlled comparison—with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)— TBCNNs, of both variants, consistently outperform RNNs (Socher et al., 2011b) to a large extent (50.4–51.4% versus 43.2%); they also consistently outperform “flat” CNNs by more than 10%. Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information more effectively than RNNs. We also observe d-TBCNN achieves higher performance than c-TBCNN. This suggests that compact tree expressiveness is more important than integrating global information in this task. 4.2 Question Classification We further evaluate TBCNN models on a question classification task.6 The dataset contains 5452 annotated sentences</context>
<context position="28498" citStr="Socher et al. (2011" startWordPosition="4528" endWordPosition="4531">raining a deep RNN, which achieves 49.8% accuracy in the 5-class sentiment classification. Our TBCNNs are typically trained within 25 epochs. 4.3.2 The Effect of Sentence Lengths We analyze how sentence lengths affect our models. Sentences are split into 7 groups by length, with granularity 5. A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126 to 457. Figure 4 presents accuracies versus lengths in TBCNNs. For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy, slightly worse than 43.2% reported in Socher et al. (2011b). Thus, we think our reimplementation is fair and that the comparison is sensible. We observe that c-TBCNN and d-TBCNN yield very similar behaviors. They consistently outperform the RNN in all scenarios. We also notice the gap, between TBCNNs and RNN, increases when sentences contain more than 20 words. This result confirms our theoretical analysis in Section 2—for long sentences, the propagation paths in RNNs are deep, causing RNNs’ difficulty in information processing. By contrast, our models explore structural information more effectively with tree-based convolution. As information from a</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="7682" citStr="Socher et al., 2012" startWordPosition="1162" endWordPosition="1165">parent of c1 and c2, vector representations denoted as p, c1, and c2. The parent’s representation is composited by p = f(W · [c1; c2] + b) (1) where W and b are parameters. This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b). Dependency parse and the combinatory categorical grammar can also be exploited as RNNs’ skeletons (Hermann and Blunsom, 2013; Iyyer et al., 2014). Irsoy and Cardie (2014) build deep RNNs to enhance information interaction. Im2316 provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013). They are more suitable for capturing logical information in sentences, such as negation and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series d</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="7724" citStr="Socher et al., 2013" startWordPosition="1168" endWordPosition="1171">s denoted as p, c1, and c2. The parent’s representation is composited by p = f(W · [c1; c2] + b) (1) where W and b are parameters. This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b). Dependency parse and the combinatory categorical grammar can also be exploited as RNNs’ skeletons (Hermann and Blunsom, 2013; Iyyer et al., 2014). Irsoy and Cardie (2014) build deep RNNs to enhance information interaction. Im2316 provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013). They are more suitable for capturing logical information in sentences, such as negation and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is</context>
<context position="23733" citStr="Socher et al. (2013)" startWordPosition="3759" endWordPosition="3762">olutional layer and the last hidden layer to 30 and 25, respectively. We do not back-propagate gradient to embeddings in this 5Richard Socher, who first applies neural networks to this task, thinks direct transfer is fine for binary classification. We followed this strategy for simplicity as it is non-trivial to deal with the neutral sub-sentences in the training set if we train a separate model. Our website reviews some related work and provides more discussions. 6http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2320 Group Method 5-class accuracy 2-class accuracy Reported in Baseline SVM 40.7 79.4 Socher et al. (2013) Naive Bayes 41.0 81.8 Socher et al. (2013) CNNs 1-layer convolution 37.4 77.1 Blunsom et al. (2014) Deep CNN 48.5 86.8 Blunsom et al. (2014) Non-static 48.0 87.2 Kim (2014) Multichannel 47.4 88.1 Kim (2014) RNNs Basic 43.2 82.4 Socher et al. (2013) Matrix-vector 44.4 82.9 Socher et al. (2013) Tensor 45.7 85.4 Socher et al. (2013) Tree LSTM (variant 1) 48.0 – Zhu et al. (2015) Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangzhong Su</author>
<author>Katja Markert</author>
</authors>
<title>From words to senses: a case study of subjectivity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>825--832</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1392" citStr="Su and Markert, 2008" startWordPosition="185" endWordPosition="188">odels on two tasks: sentiment analysis and question classification. In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work. 1 Introduction Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding</context>
</contexts>
<marker>Su, Markert, 2008</marker>
<rawString>Fangzhong Su and Katja Markert. 2008. From words to senses: a case study of subjectivity recognition. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 825– 832. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaisheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1556--1566</pages>
<contexts>
<context position="8387" citStr="Tai et al., 2015" startWordPosition="1270" endWordPosition="1273">information in sentences, such as negation and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015). Recurrent networks. A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs. 3 Tree-based Convolution This section introduces the proposed tree-based convolutional neural networks (TBCNNs). Figure 1c depicts the convolution process on a tree. First, a sentence is converted to a parse tree, either a constituency or dependency tree. The corresponding model variants are denoted as c-TBCNN and d-TBCNN</context>
<context position="21165" citStr="Tai et al., 2015" startWordPosition="3361" endWordPosition="3364">ic gradient descent with mini-batch 200. We use ReLU (Nair and Hinton, 2010) as the activation function. For regularization, we add E2 penalty for weights with a coefficient of 10−5. Dropout (Srivastava et al., 2014) is further applied to both weights and embeddings. All hidden layers are dropped out by 50%, and embeddings 40%. 4.1.3 Performance Table 2 compares our models to state-of-the-art results in the task of sentiment analysis. For 5- class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory (Tai et al., 2015). c-TBCNN is slightly worse. It achieves 50.4% accuracy, ranking third in the state-of-the-art list (including our d-TBCNN model). Regarding 2-class prediction, we adopted a simple strategy in Irsoy and Cardie (2014),5 where the 5-class network is “transferred” directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes. (The neutral class is discarded as in other studies.) This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position. Nonetheless, our d-TBCNN model achieves 8</context>
<context position="24162" citStr="Tai et al. (2015)" startWordPosition="3836" endWordPosition="3839"> work and provides more discussions. 6http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2320 Group Method 5-class accuracy 2-class accuracy Reported in Baseline SVM 40.7 79.4 Socher et al. (2013) Naive Bayes 41.0 81.8 Socher et al. (2013) CNNs 1-layer convolution 37.4 77.1 Blunsom et al. (2014) Deep CNN 48.5 86.8 Blunsom et al. (2014) Non-static 48.0 87.2 Kim (2014) Multichannel 47.4 88.1 Kim (2014) RNNs Basic 43.2 82.4 Socher et al. (2013) Matrix-vector 44.4 82.9 Socher et al. (2013) Tensor 45.7 85.4 Socher et al. (2013) Tree LSTM (variant 1) 48.0 – Zhu et al. (2015) Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Mikolov (2014) TBCNNs c-TBCNN 50.4 86.81 Our implementation d-TBCNN 51.4 87.91 Our implementation Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate that the network is transferred directly from that of 5-class. Method Acc. (%) Reported in SVM 95.0 Silva et al. (2011) 10k feature</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kaisheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1556–1566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Zhao</author>
<author>Zhengdong Lu</author>
<author>Pascal Poupart</author>
</authors>
<title>Self-adaptive hierarchical sentence model.</title>
<date>2015</date>
<booktitle>In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>4069--4076</pages>
<contexts>
<context position="1412" citStr="Zhao et al., 2015" startWordPosition="189" endWordPosition="192">ntiment analysis and question classification. In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering. We also make efforts to visualize the tree-based convolution process, shedding light on how our models work. 1 Introduction Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015). Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences. Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation. ∗These authors contribute equally to this paper. †To whom correspondence should be addressed. Recent advances of neural networks bring new techniques in understanding natural languages, </context>
<context position="24866" citStr="Zhao et al. (2015)" startWordPosition="3952" endWordPosition="3955">ardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Mikolov (2014) TBCNNs c-TBCNN 50.4 86.81 Our implementation d-TBCNN 51.4 87.91 Our implementation Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate that the network is transferred directly from that of 5-class. Method Acc. (%) Reported in SVM 95.0 Silva et al. (2011) 10k features + 60 rules CNN-non-static 93.6 Kim (2014) CNN-mutlichannel 92.2 Kim (2014) RNN 90.2 Zhao et al. (2015) Deep-CNN 93.0 Blunsom et al. (2014) Ada-CNN 92.4 Zhao et al. (2015) c-TBCNN 94.8 Our implementation d-TBCNN 96.0 Our implementation Table 3: Accuracy of 6-way question classification. task. Dropout rate for embeddings is 30%; hidden layers are dropped out by 5%. Table 3 compares our models to various other methods. The first entry presents the previous state-of-the-art result, achieved by traditional feature/rule engineering (Silva et al., 2011). Their method utilizes more than 10k features and 60 hand-coded rules. On the contrary, our TBCNN models do not use a single human-engineered feature</context>
</contexts>
<marker>Zhao, Lu, Poupart, 2015</marker>
<rawString>Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015. Self-adaptive hierarchical sentence model. In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, pages 4069– 4076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Parinaz Sobhani</author>
<author>Hongyu Guo</author>
</authors>
<title>Long short-term memory over tree structures.</title>
<date>2015</date>
<booktitle>In Proceedings of The 32nd International Conference on Machine Learning,</booktitle>
<pages>1604--1612</pages>
<contexts>
<context position="8428" citStr="Zhu et al., 2015" startWordPosition="1278" endWordPosition="1281">on and exclamation. One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture. Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009). Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015). Recurrent networks. A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree. In such models, meaningful tree structures are also lost, similar to CNNs. 3 Tree-based Convolution This section introduces the proposed tree-based convolutional neural networks (TBCNNs). Figure 1c depicts the convolution process on a tree. First, a sentence is converted to a parse tree, either a constituency or dependency tree. The corresponding model variants are denoted as c-TBCNN and d-TBCNN. Each node in the tree is represented as</context>
<context position="24112" citStr="Zhu et al. (2015)" startWordPosition="3826" endWordPosition="3829">a separate model. Our website reviews some related work and provides more discussions. 6http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2320 Group Method 5-class accuracy 2-class accuracy Reported in Baseline SVM 40.7 79.4 Socher et al. (2013) Naive Bayes 41.0 81.8 Socher et al. (2013) CNNs 1-layer convolution 37.4 77.1 Blunsom et al. (2014) Deep CNN 48.5 86.8 Blunsom et al. (2014) Non-static 48.0 87.2 Kim (2014) Multichannel 47.4 88.1 Kim (2014) RNNs Basic 43.2 82.4 Socher et al. (2013) Matrix-vector 44.4 82.9 Socher et al. (2013) Tensor 45.7 85.4 Socher et al. (2013) Tree LSTM (variant 1) 48.0 – Zhu et al. (2015) Tree LSTM (variant 2) 51.0 88.0 Tai et al. (2015) Tree LSTM (variant 3) 49.9 88.0 Le and Zuidema (2015) Deep RNN 49.8 86.61 Irsoy and Cardie (2014) Recurrent LSTM 45.8 86.7 Tai et al. (2015) bi-LSTM 49.1 86.8 Tai et al. (2015) Vector Word vector avg. 32.7 80.1 Socher et al. (2013) Paragraph vector 48.7 87.8 Le and Mikolov (2014) TBCNNs c-TBCNN 50.4 86.81 Our implementation d-TBCNN 51.4 87.91 Our implementation Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “†” remarks indicate that the network is transferred directly from that of 5-class. Method Acc. (%) Re</context>
</contexts>
<marker>Zhu, Sobhani, Guo, 2015</marker>
<rawString>Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over tree structures. In Proceedings of The 32nd International Conference on Machine Learning, pages 1604–1612.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
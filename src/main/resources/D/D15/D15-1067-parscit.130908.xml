<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000511">
<title confidence="0.976877">
Online Sentence Novelty Scoring for Topical Document Streams
</title>
<author confidence="0.707446">
Sungjin Lee
</author>
<affiliation confidence="0.356079">
Yahoo Labs
</affiliation>
<address confidence="0.203024">
229 West 43rd Street, New York, NY 10036, USA
</address>
<email confidence="0.411918">
junion@yahoo-inc.com
</email>
<sectionHeader confidence="0.981649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998604">
The enormous amount of information on the
Internet has raised the challenge of highlight-
ing new information in the context of already
viewed content. This type of intelligent inter-
face can save users time and prevent frustra-
tion. Our goal is to scale up novelty detec-
tion to large web properties like Google News
and Yahoo News. We present a set of light-
weight features for online novelty scoring and
fast nonlinear feature transformation methods.
Our experimental results on the TREC 2004
shared task datasets show that the proposed
method is not only efficient but also very pow-
erful, significantly surpassing the best system
at TREC 2004.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997291780822">
The Internet supplies a wealth of news content with a
corresponding problem: finding the right content for
different users. Search engines are helpful if a user
is looking for something specific that can be cast as
a keyword query. If a user does not know what to
look for, recommendation engines can make personal-
ized suggestions for stories that may interest the user.
But both types of systems frequently represent content
that the user has already consumed, leading to delay
and frustration. Consequently, identifying novel infor-
mation has been an essential aspect of studies on news
information retrieval. Newsjunkie (Gabrilovich et al.,
2004), for instance, describes a system that personal-
izes a newsfeed based on a measure of information nov-
elty: the user can be presented custom tailored news
feeds that are novel in the context of documents that
have already been reviewed. This will spare the user
from hunting through duplicate and redundant content
for new nuggets of information. Identifying genuinely
novel information is also an essential aspect of update
summarization (Nenkova and McKeown, 2012; Gao et
al., 2013; Guo et al., 2013; Wang and Li, 2010; Ben-
tivogli et al., 2011). But the temporal dynamics of a
document stream are not generally the focus. Novelty
detection has also been studied in Topic Detection and
Tracking field for the First Story Detection task (Allan,
2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai
and Zhang, 2011) where the aim is to detect novel doc-
uments given previously seen documents. In this paper,
we examine a slightly different problem; we perform
novelty detection at the sentence level to highlight sen-
tences that contain novel information.
The novelty track in TREC was designed to serve
as a shared task for exactly this type of research: find-
ing novel, on-topic sentences from a news stream (Har-
man, 2002). There were four tasks in the novelty track
but we only focus on task 2 in this paper: “given rele-
vant sentences in all documents, identify all novel sen-
tences.” The track changed slightly from year to year.
The data of the first run in 2002 (Harman, 2002) used
old topics and judgments which proved to be problem-
atic due to the small percentage of relevant sentences.
TREC 2003 (Soboroff and Harman, 2003) included
50 new topics with an improved balance of relevant
and novel sentences and chronologically ordered docu-
ments. TREC 2004 (Soboroff and Harman, 2005) used
the same task settings and the same number of topics,
but made a major change through the inclusion of irrel-
evant documents.
Although the participants in the novelty track of
TREC and many followup studies have investigated a
wide ranging set of features and algorithms (Soboroff
and Harman, 2005), almost none were specifically fo-
cused on scalability. However, modern news aggrega-
tors are usually visited by millions of unique users and
consume millions of stories each day. Moreover, every
few minutes item churn takes place and the stories of
interest are likely to be the ones that appeared in the
last couple of hours. As real-time processing on a large
scale gains more attention (Osborne et al., 2014), we
investigate features that are both effective and efficient,
and so could be used in a scalable online novelty scor-
ing engine for making personalized newsfeeds on large
web properties like Google News and Yahoo News.
To achieve this goal, our contributions are two-fold.
First, we present a set of effective, light-weight fea-
tures: KL divergence with asymmetric smoothing, non-
linear transformation of unseen word count, relative
sentence position and word embedding-based similar-
ity. Note that we restrict ourselves to only surface-level
text features and algorithms that have time complexity
of O(W) where W is the number of unique words seen
so far (previous studies often employed quite expensive
</bodyText>
<page confidence="0.989511">
567
</page>
<note confidence="0.6597795">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 567–572,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999812545454546">
features and algorithms that have time complexity of
at least O(WT) where T is the number of sentences
so far). To fully comply with the online setting, we
also exclude very popular methods for measuring sim-
ilarity such as tf-idf, since we are not allowed to see
the entire corpus. Second, we propose efficient fea-
ture transformation methods: recursive feature averag-
ing and Deep Neural Network (DNN)-based nonlinear
transformation. We evaluate our system on task 2 of
the 2004 TREC novelty track. Interestingly, our exper-
iment results indicate that our light-weight features are
actually very powerful when used in conjunction with
the proposed feature transformation; we obtain a signif-
icant performance improvement over the best challenge
system.
The rest of this paper is structured as follows: Sec-
tion 2 presents a brief summary of related work. Sec-
tion 3 describes our algorithm and features. Section 4
outlines the experimental setup and reports the results
of comparative analysis with challenge systems. We
finish with some conclusions and future directions in
Section 5.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9777923125">
There were 13 groups and 54 submitted entries for the
2004 TREC novelty track task 2. The participants used
a wide range of methods which can be roughly cate-
gorized into statistical and linguistic methods. Statisti-
cal methods included traditional information retrieval
models such as tf-idf and Okapi, and metrics such
as importance value, new sentence value, conceptual
fuzziness, scarcity measure, information weakness, un-
seen item count with a threshold optimized for detect-
ing novel sentences (Blott et al., 2004; Zhang et al.,
2004; Abdul-Jaleel et al., 2004; Eichmann et al., 2004;
Erkan, 2004; Schiffman and McKeown, 2004). Thresh-
olds are either learned on the 2003 data or determined
in an ad hoc manner. Some groups also used machine
learning algorithms such as SVMs by casting the prob-
lem as a binary classification (Tomiyama et al., 2004).
Many groups adopted a variety of preprocessing steps
including expansion of the sentences using dictionar-
ies, ontologies or corpus-based methods and named en-
tity recognition. Graph-based analysis has also been
applied where directed edges are established by cosine
similarity and chronological order. After this graph is
constructed, the eigenvector centrality score for each
sentence was computed by using a power method. The
sentences with low centrality scores were considered
as new (Erkan, 2004). Graph-based approaches were
further pursued by Gamon (2006) that drew a richer
set of features from graph topology and its changes,
resulting in a system that ties with the best system at
TREC 2004 (i.e. Blott et al. (2004)). On the other
hand, deep linguistic methods included parsing, coref-
erence resolution, matching discourse entities, search-
ing for particular verbs and verb phrases, standardiz-
ing acronyms, building a named-entity lexicon, and
Algorithm 1: Novelty scoring for a topical docu-
ment stream
Data: a document stream
Result: a document stream with novelty
annotation
Initialize a context C0;
while not at end of the document stream do
read a document;
split the document into sentences;
while not at end of the document do
read a sentence St;
perform preprocessing on St;
compute novelty score as the posterior
probability of a binary novelty random
</bodyText>
<subsubsectionHeader confidence="0.322609">
variable Nt, p(NtlSt, Ct−1);
</subsubsectionHeader>
<bodyText confidence="0.710065">
update the context Ct with Ct−1 and St;
</bodyText>
<listItem confidence="0.62914825">
end
compute a document-level score (e.g. average
out all sentence-level scores)
end
</listItem>
<bodyText confidence="0.9998535">
matching concepts to manually-constructed ontology
for topic-specific concepts (Amrani et al., 2004). The
difficulty of the novelty detection task is evident from
the relatively low score achieved by even the best sys-
tems at TREC 2004 (Soboroff and Harman, 2005). The
top scoring systems were mostly based on statistical
methods while deep linguistic approaches achieved the
highest precision at the cost of poor recall.
</bodyText>
<sectionHeader confidence="0.981659" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.9997934">
For the purpose of this paper, we formulate task 2 of
the TREC novelty detection track as an online proba-
bilistic inference task. More specifically, we compute
the novelty score as the posterior probability of a binary
novelty random variable N:
</bodyText>
<equation confidence="0.987942333333333">
1 � p(Nt|St,Ct−1) = Z exp wifi(Nt, St, Ct−1)
i
(1)
</equation>
<bodyText confidence="0.9998500625">
in which the fi are feature functions, wi model param-
eters, St the sentence in focus and Ct−1 a context con-
taining information about previously seen sentences S1
through St−1 across documents.
The overall procedure is listed in Algorithm 1. The
algorithm takes as input documents which have been
clustered by topic and chronologically ordered. For
each sentence St in each document, basic preprocess-
ing is performed (e.g. simple tokenization, stopword
filtering and stemming (Porter, 1980)), then the infer-
ence is made whether St is novel given the context
Ct−1. Without the use of the context, the time com-
plexity of our algorithm would depend on the number
of sentences so far. Thus, the features and the model
for the context are important for efficiency. Note that
our method only takes time complexity of O(W) for
</bodyText>
<page confidence="0.989552">
568
</page>
<bodyText confidence="0.932779">
both context update and feature generation.
</bodyText>
<subsectionHeader confidence="0.845909">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.925000833333333">
KL divergence with asymmetric smoothing. KL
divergence has been successfully adopted to measure
the distance between a document and a set of docu-
ments (Gabrilovich et al., 2004; Gamon, 2006). We
use it to measure the distance between context C and
sentence S:
</bodyText>
<equation confidence="0.989171666666667">
E pC(w) log pC(w) (2)
pS(w)
w
</equation>
<bodyText confidence="0.9954346">
The intuition is that the more distant the distributions
are, the more likely it is that the sentence is novel.
Since KL divergence is asymmetric, both directions
are used as features, with and without scale normal-
ization. The computation of KL divergence requires
both pC and pS to be non-zero; while simple add-one
smoothing is employed in previous work, we adopt
novel asymmetric smoothing. We add a larger smooth-
ing factor s for already seen words than the factor u
for unseen words. The rationale behind this is that we
intensify the difference caused by unseen words and at-
tenuate the difference caused by seen words (Figure 1.)
Asymmetric smoothing with various smoothing factors
consistently showed better performance than symmet-
ric smoothing in our experiments.
</bodyText>
<figureCaption confidence="0.96890975">
Figure 1: KL divergence with symmetric (left) and
asymmetric (right) smoothing. Pink and blue corre-
spond to two distributions while light yellow and or-
ange to smoothing factors.
</figureCaption>
<bodyText confidence="0.98329175">
Nonlinear transformation of unseen word count.
One of the simplest metrics to measure novelty is the
plain count of unseen words. This measure, however,
does not necessarily reflect human perception of nov-
elty given the prevalence of nonlinearity in human per-
ception (Kingdom and Prins, 2009). Thus, we explored
the use of a simple nonlinear transformation of unseen
word counts instead of the plain count (Figure 2):
</bodyText>
<equation confidence="0.998653">
T(n) = (αn + 0)γ (3)
</equation>
<bodyText confidence="0.999664333333333">
where n is the number of new words and α, 0 and y are
parameters. In our experiments, the use of a nonlinear
transformation helped yield better results.
</bodyText>
<figureCaption confidence="0.852302">
Figure 2: Nonlinear transformation of unseen word
count with parameters set via cross-validation on the
TREC training data: α = 0.5, 0 = 0 and y = 1.5.
</figureCaption>
<bodyText confidence="0.999665333333333">
Relative position in a document. Relative position
of a sentence in a document is simple yet has been
proven effective for summarization. Relative position
is also closely related to novelty detection as follows:
1) There is in general a good chance that earlier sen-
tences are more novel than the later ones. 2) We found
a pattern that news articles coming in later are apt to
present novel information first and then a summary of
old information.
Word embedding-based similarity. Neural word em-
bedding techniques can be effective in capturing syn-
tactic and semantic relationships, and more computa-
tionally efficient than many other competitors (Socher
et al., 2012; Mikolov et al., 2013). As reported in (Tai
et al., 2015), a simple averaging scheme was found to
be very competitive to more complex models for rep-
resenting a sentence vector. These observations lead
us to adopt the following additional features derived
from word embeddings: 1) cosine similarity between
the mean vectors of the context C and sentence S, 2)
sigmoid function value for the dot product of the mean
vectors of the context C and sentence S. The mean
vectors of C and S are computed by taking the average
of the word vectors of each unique word in C and S,
respectively. We use word embedding with 100 dimen-
sions trained on Wikipedia using the word2vec toolkit
(https://code.google.com/p/word2vec).
</bodyText>
<subsectionHeader confidence="0.998963">
3.2 Feature transformation
</subsectionHeader>
<bodyText confidence="0.9999741">
Recursive feature averaging. A large portion of the
novel sentences in the TREC 2004 data appear in con-
secutive runs of two or more (Schiffman and McKe-
own, 2004). Sequential labeling would be a natural ap-
proach to take advantage of this characteristic of the
problem, but the use of sequential labeling will make
time complexity depend on the number of sentences T.
Thus we came up with another way to exploit this char-
acteristic, recursively averaging over previous feature
vectors and augmenting the current feature vector with
</bodyText>
<page confidence="0.970157">
569
</page>
<equation confidence="0.869884666666667">
the average:
Rt = ηFt−1 + (1 − η)Rt−1 (4)
Ft0 =Ft::Rt (5)
</equation>
<bodyText confidence="0.999482133333333">
where F is a feature vector, R the average vector of
previous ones, F0 the augmented feature vector, η the
weight of the last feature vector in averaging and ::
means concatenation.
DNN-based feature transformation. In order to bet-
ter capture non-trivial interactions between the features
described above, we adopt a DNN with a bottleneck.
DNNs with a bottleneck have been successfully ex-
plored for nonlinear feature transformation (Gr´ezl et
al., 2007). The feature transformation is normally
achieved from narrow hidden layers that retain only
the information useful to classification. This leads us
to introduce bottleneck hidden layers between the in-
put layer and the Logistic Regression output layer (Fig-
ure 3.)
</bodyText>
<figureCaption confidence="0.7075565">
Figure 3: Flowchart for a bottleneck DNN. The dotted-
box represents bottleneck generating hidden layers.
</figureCaption>
<sectionHeader confidence="0.995476" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99970275">
Following the guidelines of task 2 for the TREC
2004 novelty detection track, we used the TREC 2003
dataset as training data and the TREC 2004 dataset
as test data. The training data includes 10,226 novel
sentences out of 15,557 sentences. The test data in-
cludes 3,454 novel ones out of 8,343 sentences. We
trained a DNN-based classifier and several logistic re-
gression classifiers (which are the same model with the
DNN model except without the hidden layers) using the
Theano toolkit (Bergstra et al., 2010) to verify the ef-
fectiveness of each feature and feature transformation.
We optimized all models by minimizing logloss with
the stochastic gradient decent algorithm with momen-
tum. We classified a sentence as novel if the posterior
probability is greater than 0.5. We performed a search
based on five-fold cross validation to identify optimal
values for the parameters defined in Section 3, and ob-
tained the following values: s = 10, u = 0.1, α = 0.5,
β = 0, y = 1.5 and η = 0.5. For the DNN classi-
fier, we used a set of five bottleneck hidden layers. The
number of nodes for each hidden layer were set to 10,
5, 3, 5 and 10, respectively.
Comparative evaluation results in F-score (following
the TREC protocol) are shown in Table 1. In Table 1,
the first four entries refer to the best top systems from
TREC 2004 and followup studies, KLdiv to a system
using only KL divergence features, TransCount to a
system using only nonlinear transformation of unseen
word count features, RelPos to a system using only
relative position features, Word2Uec to a system using
only word embedding features, All to a system using
all features, All + Recursive to All with recursive fea-
ture averaging applied, All + DNN to All with DNN-
based feature transformation applied and All + Recur-
sive + DNN to All + Recursive with DNN-based fea-
ture transformation applied. The best result (in bold)
is significantly better than the best system results from
TREC 2004, while still being very computationally ef-
ficient and therefore scalable. In terms of individual
features, KLdiv (ties for 5th place at TREC 2004) and
TransCount (outperforms the 6th entry) showed very
strong results. Although RelPos and Word2Uec did not
yield good results, we found them complementary to
other features; performance was degraded to 0.621 and
0.624, respectively, when they were excluded from All
+ Recursive + DNN. The DNN-based feature transfor-
mation generally yielded better results. In particular,
it becomes very effective in conjunction with recursive
feature averaging. This result indicates that the DNN-
based transformation allows the system to capture the
non-trivial interactions between previous sentences and
the current one.
</bodyText>
<table confidence="0.998837307692308">
Systems F-score
Blott et al. (2004) / Gamon (2006) 0.622
Tomiyama et al. (2004) 0.619
Abdul-Jaleel et al. (2004) 0.618
Schiffman and McKeown (2004) 0.617
KLdiv 0.614
TransCount 0.611
RelPos 0.577
Word2Vec 0.577
All 0.615
All + Recursive 0.615
All + DNN 0.617
All + Recursive + DNN 0.625
</table>
<tableCaption confidence="0.999676">
Table 1: Performance breakdown. The best result is
</tableCaption>
<bodyText confidence="0.9699465">
significantly better than the other configurations (p &lt;
0.01) based on the McNemar test. Since the systems’
output is not available, we are not able to calculate sta-
tistical significance against TREC systems.
</bodyText>
<sectionHeader confidence="0.997452" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999835">
We explored the space of light-weight features and
their nonlinear transformation with the goal of support-
ing online web-scale sentence novelty detection. The
experiment results show that these features are not only
efficient but also very powerful; a combination of these
</bodyText>
<page confidence="0.981228">
570
</page>
<bodyText confidence="0.999903833333333">
features with a simple, scalable classification approach
significantly surpassed the best challenge system at
TREC 2004. For future work, it would be interesting
to see if more sophisticated DNN training techniques
(e.g. unsupervised pre-training and different optimiza-
tion algorithms) would yield a better performance.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99931671">
Nasreen Abdul-Jaleel, James Allan, W Bruce Croft,
Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D
Smucker, and Courtney Wade. 2004. UMass at
TREC 2004: Novelty and HARD. In Proceedings
of TREC.
James Allan. 2002. Introduction to topic detection and
tracking. In Topic detection and tracking, pages 1–
16. Springer.
Ahmed Amrani, J´erˆome Az´e, Thomas Heitz, Yves Ko-
dratoff, and Mathieu Roche. 2004. From the texts to
the concepts they contain: a chain of linguistic treat-
ments. In In Proceedings of TREC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. Proceed-
ings of TAC, 2011.
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and GPU
math expression compiler. In Proceedings of the
Python for Scientific Computing Conference (SciPy),
June. Oral Presentation.
Stephen Blott, Oisin Boydell, Fabrice Camous,
Paul Ferguson, Georgina Gaughan, Cathal Gurrin,
Gareth JF Jones, Noel Murphy, Noel E O’Connor,
and Alan F Smeaton. 2004. Experiments in terabyte
searching, genomic retrieval and novelty detection
for TREC 2004.
David Eichmann, Yi Zhang, Shannon Bradshaw,
Xin Ying Qiu, Li Zhou, Padmini Srinivasan,
Aditya Kumar Sehgal, and Hudon Wong. 2004.
Novelty, question answering and genomics: The
University of Iowa response. In Proceedings of
TREC.
G¨unes Erkan. 2004. The University of Michigan in
novelty 2004. In Proceedings of TREC.
Evgeniy Gabrilovich, Susan Dumais, and Eric Horvitz.
2004. Newsjunkie: providing personalized news-
feeds via analysis of information novelty. In Pro-
ceedings of WWW.
Michael Gamon. 2006. Graph-based text represen-
tation for novelty detection. In Proceedings of the
First Workshop on Graph Based Methods for Natu-
ral Language Processing.
Dehong Gao, Wenjie Li, and Renxian Zhang. 2013.
Sequential summarization: A new application for
timely updated Twitter trending topics. In Proceed-
ings of the ACL.
Frantisek Gr´ezl, Martin Karafi´at, Stanislav Kont´ar, and
Jan Cernocky. 2007. Probabilistic and bottle-neck
features for lvcsr of meetings. In Acoustics, Speech
and Signal Processing, 2007. ICASSP 2007. IEEE
International Conference on, volume 4, pages IV–
757. IEEE.
Qi Guo, Fernando Diaz, and Elad Yom-Tov. 2013. Up-
dating users about time critical events. In Advances
in Information Retrieval, pages 483–494. Springer.
Donna Harman. 2002. Overview of the trec 2002 nov-
elty track. In TREC.
Margarita Karkali, Franc¸ois Rousseau, Alexandros
Ntoulas, and Michalis Vazirgiannis. 2014. Using
temporal IDF for efficient novelty detection in text
streams. CoRR, abs/1401.1456.
Margarita Karkali1, Alexandros Ntoulas, Franois
Rousseau, and Michalis Vazirgiannis. 2013. Effi-
cient online novelty detection in news streams. In
Proceedings of Web Information Systems Engineer-
ing.
Frederick Kingdom and Nicolaas Prins. 2009. Psy-
chophysics: a practical introduction. Academic
Press.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.
Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Mining Text
Data, pages 43–76. Springer.
Miles Osborne, Sean Moran, Richard McCreadie,
Alexander Von Lunen, Martin D Sykora, Elizabeth
Cano, Neil Ireson, Craig Macdonald, Iadh Ounis,
Yulan He, et al. 2014. Real-time detection, tracking,
and monitoring of automatically discovered events in
social media.
Martin F Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130–137.
Barry Schiffman and Kathleen McKeown. 2004.
Columbia University in the novelty track at TREC
2004. In Proceedings of TREC.
Ian Soboroff and Donna Harman. 2003. Overview of
the trec 2003 novelty track. In TREC, pages 38–53.
Ian Soboroff and Donna Harman. 2005. Novelty de-
tection: the TREC experience. In Proceedings of
HLT-EMNLP.
Richard Socher, Yoshua Bengio, and Christopher D.
Manning. 2012. Deep learning for nlp (without
magic). In Tutorial Abstracts of ACL 2012, ACL
’12, pages 5–5, Stroudsburg, PA, USA. Association
for Computational Linguistics.
</reference>
<page confidence="0.972739">
571
</page>
<reference confidence="0.99945855">
Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.
Tomoe Tomiyama, Kosuke Karoji, Takeshi Kondo,
Yuichi Kakuta, Tomohiro Takagi, Akiko Aizawa,
and Teruhito Kanazawa. 2004. Meiji University
web, novelty and genomic track experiments. In
Proceedings of TREC.
FloraS. Tsai and Yi Zhang. 2011. D2s: Document-to-
sentence framework for novelty detection. Knowl-
edge and Information Systems, 29(2):419–433.
Dingding Wang and Tao Li. 2010. Document up-
date summarization using incremental hierarchical
clustering. In Proceedings of the 19th ACM Inter-
national Conference on Information and Knowledge
Management.
Huaping Zhang, Hongbo Xu, Shuo Bai, Bin Wang, and
Xueqi Cheng. 2004. Experiments in TREC 2004
novelty track at CAS-ICT. In Proceedings of TREC.
</reference>
<page confidence="0.997405">
572
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.329856">
<title confidence="0.903699">Online Sentence Novelty Scoring for Topical Document Streams Sungjin</title>
<author confidence="0.490237">Yahoo</author>
<address confidence="0.988195">229 West 43rd Street, New York, NY 10036,</address>
<email confidence="0.999752">junion@yahoo-inc.com</email>
<abstract confidence="0.9812660625">The enormous amount of information on the Internet has raised the challenge of highlighting new information in the context of already viewed content. This type of intelligent interface can save users time and prevent frustration. Our goal is to scale up novelty detection to large web properties like Google News and Yahoo News. We present a set of lightweight features for online novelty scoring and fast nonlinear feature transformation methods. Our experimental results on the TREC 2004 shared task datasets show that the proposed method is not only efficient but also very powerful, significantly surpassing the best system at TREC 2004.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nasreen Abdul-Jaleel</author>
<author>James Allan</author>
<author>W Bruce Croft</author>
<author>Fernando Diaz</author>
<author>Leah Larkey</author>
<author>Xiaoyan Li</author>
<author>Mark D Smucker</author>
<author>Courtney Wade</author>
</authors>
<title>UMass at TREC 2004: Novelty and HARD.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="6525" citStr="Abdul-Jaleel et al., 2004" startWordPosition="1047" endWordPosition="1050">onclusions and future directions in Section 5. 2 Related Work There were 13 groups and 54 submitted entries for the 2004 TREC novelty track task 2. The participants used a wide range of methods which can be roughly categorized into statistical and linguistic methods. Statistical methods included traditional information retrieval models such as tf-idf and Okapi, and metrics such as importance value, new sentence value, conceptual fuzziness, scarcity measure, information weakness, unseen item count with a threshold optimized for detecting novel sentences (Blott et al., 2004; Zhang et al., 2004; Abdul-Jaleel et al., 2004; Eichmann et al., 2004; Erkan, 2004; Schiffman and McKeown, 2004). Thresholds are either learned on the 2003 data or determined in an ad hoc manner. Some groups also used machine learning algorithms such as SVMs by casting the problem as a binary classification (Tomiyama et al., 2004). Many groups adopted a variety of preprocessing steps including expansion of the sentences using dictionaries, ontologies or corpus-based methods and named entity recognition. Graph-based analysis has also been applied where directed edges are established by cosine similarity and chronological order. After this </context>
<context position="17586" citStr="Abdul-Jaleel et al. (2004)" startWordPosition="2861" endWordPosition="2864">d2Uec did not yield good results, we found them complementary to other features; performance was degraded to 0.621 and 0.624, respectively, when they were excluded from All + Recursive + DNN. The DNN-based feature transformation generally yielded better results. In particular, it becomes very effective in conjunction with recursive feature averaging. This result indicates that the DNNbased transformation allows the system to capture the non-trivial interactions between previous sentences and the current one. Systems F-score Blott et al. (2004) / Gamon (2006) 0.622 Tomiyama et al. (2004) 0.619 Abdul-Jaleel et al. (2004) 0.618 Schiffman and McKeown (2004) 0.617 KLdiv 0.614 TransCount 0.611 RelPos 0.577 Word2Vec 0.577 All 0.615 All + Recursive 0.615 All + DNN 0.617 All + Recursive + DNN 0.625 Table 1: Performance breakdown. The best result is significantly better than the other configurations (p &lt; 0.01) based on the McNemar test. Since the systems’ output is not available, we are not able to calculate statistical significance against TREC systems. 5 Conclusions We explored the space of light-weight features and their nonlinear transformation with the goal of supporting online web-scale sentence novelty detecti</context>
</contexts>
<marker>Abdul-Jaleel, Allan, Croft, Diaz, Larkey, Li, Smucker, Wade, 2004</marker>
<rawString>Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D Smucker, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
</authors>
<title>Introduction to topic detection and tracking.</title>
<date>2002</date>
<booktitle>In Topic detection and tracking,</booktitle>
<pages>1--16</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2213" citStr="Allan, 2002" startWordPosition="356" endWordPosition="357">om tailored news feeds that are novel in the context of documents that have already been reviewed. This will spare the user from hunting through duplicate and redundant content for new nuggets of information. Identifying genuinely novel information is also an essential aspect of update summarization (Nenkova and McKeown, 2012; Gao et al., 2013; Guo et al., 2013; Wang and Li, 2010; Bentivogli et al., 2011). But the temporal dynamics of a document stream are not generally the focus. Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task (Allan, 2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai and Zhang, 2011) where the aim is to detect novel documents given previously seen documents. In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information. The novelty track in TREC was designed to serve as a shared task for exactly this type of research: finding novel, on-topic sentences from a news stream (Harman, 2002). There were four tasks in the novelty track but we only focus on task 2 in this paper: “given relevant sentences in all docum</context>
</contexts>
<marker>Allan, 2002</marker>
<rawString>James Allan. 2002. Introduction to topic detection and tracking. In Topic detection and tracking, pages 1– 16. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Amrani</author>
<author>J´erˆome Az´e</author>
<author>Thomas Heitz</author>
<author>Yves Kodratoff</author>
<author>Mathieu Roche</author>
</authors>
<title>From the texts to the concepts they contain: a chain of linguistic treatments. In</title>
<date>2004</date>
<booktitle>In Proceedings of TREC.</booktitle>
<marker>Amrani, Az´e, Heitz, Kodratoff, Roche, 2004</marker>
<rawString>Ahmed Amrani, J´erˆome Az´e, Thomas Heitz, Yves Kodratoff, and Mathieu Roche. 2004. From the texts to the concepts they contain: a chain of linguistic treatments. In In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Dang</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>The seventh pascal recognizing textual entailment challenge.</title>
<date>2011</date>
<booktitle>Proceedings of TAC,</booktitle>
<contexts>
<context position="2010" citStr="Bentivogli et al., 2011" startWordPosition="320" endWordPosition="324">of studies on news information retrieval. Newsjunkie (Gabrilovich et al., 2004), for instance, describes a system that personalizes a newsfeed based on a measure of information novelty: the user can be presented custom tailored news feeds that are novel in the context of documents that have already been reviewed. This will spare the user from hunting through duplicate and redundant content for new nuggets of information. Identifying genuinely novel information is also an essential aspect of update summarization (Nenkova and McKeown, 2012; Gao et al., 2013; Guo et al., 2013; Wang and Li, 2010; Bentivogli et al., 2011). But the temporal dynamics of a document stream are not generally the focus. Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task (Allan, 2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai and Zhang, 2011) where the aim is to detect novel documents given previously seen documents. In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information. The novelty track in TREC was designed to serve as a shared task for exactly this type of </context>
</contexts>
<marker>Bentivogli, Clark, Dagan, Dang, Giampiccolo, 2011</marker>
<rawString>Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang, and Danilo Giampiccolo. 2011. The seventh pascal recognizing textual entailment challenge. Proceedings of TAC, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<title>Theano: a CPU and GPU math expression compiler.</title>
<date>2010</date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy),</booktitle>
<tech>Oral Presentation.</tech>
<contexts>
<context position="15277" citStr="Bergstra et al., 2010" startWordPosition="2478" endWordPosition="2481">rt for a bottleneck DNN. The dottedbox represents bottleneck generating hidden layers. 4 Experiments and Results Following the guidelines of task 2 for the TREC 2004 novelty detection track, we used the TREC 2003 dataset as training data and the TREC 2004 dataset as test data. The training data includes 10,226 novel sentences out of 15,557 sentences. The test data includes 3,454 novel ones out of 8,343 sentences. We trained a DNN-based classifier and several logistic regression classifiers (which are the same model with the DNN model except without the hidden layers) using the Theano toolkit (Bergstra et al., 2010) to verify the effectiveness of each feature and feature transformation. We optimized all models by minimizing logloss with the stochastic gradient decent algorithm with momentum. We classified a sentence as novel if the posterior probability is greater than 0.5. We performed a search based on five-fold cross validation to identify optimal values for the parameters defined in Section 3, and obtained the following values: s = 10, u = 0.1, α = 0.5, β = 0, y = 1.5 and η = 0.5. For the DNN classifier, we used a set of five bottleneck hidden layers. The number of nodes for each hidden layer were se</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June. Oral Presentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Blott</author>
<author>Oisin Boydell</author>
<author>Fabrice Camous</author>
<author>Paul Ferguson</author>
<author>Georgina Gaughan</author>
<author>Cathal Gurrin</author>
<author>Gareth JF Jones</author>
<author>Noel Murphy</author>
<author>Noel E O’Connor</author>
<author>Alan F Smeaton</author>
</authors>
<title>Experiments in terabyte searching, genomic retrieval and novelty detection for TREC</title>
<date>2004</date>
<marker>Blott, Boydell, Camous, Ferguson, Gaughan, Gurrin, Jones, Murphy, O’Connor, Smeaton, 2004</marker>
<rawString>Stephen Blott, Oisin Boydell, Fabrice Camous, Paul Ferguson, Georgina Gaughan, Cathal Gurrin, Gareth JF Jones, Noel Murphy, Noel E O’Connor, and Alan F Smeaton. 2004. Experiments in terabyte searching, genomic retrieval and novelty detection for TREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Eichmann</author>
<author>Yi Zhang</author>
<author>Shannon Bradshaw</author>
<author>Xin Ying Qiu</author>
<author>Li Zhou</author>
<author>Padmini Srinivasan</author>
<author>Aditya Kumar Sehgal</author>
<author>Hudon Wong</author>
</authors>
<title>Novelty, question answering and genomics: The University of Iowa response.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="6548" citStr="Eichmann et al., 2004" startWordPosition="1051" endWordPosition="1054">tions in Section 5. 2 Related Work There were 13 groups and 54 submitted entries for the 2004 TREC novelty track task 2. The participants used a wide range of methods which can be roughly categorized into statistical and linguistic methods. Statistical methods included traditional information retrieval models such as tf-idf and Okapi, and metrics such as importance value, new sentence value, conceptual fuzziness, scarcity measure, information weakness, unseen item count with a threshold optimized for detecting novel sentences (Blott et al., 2004; Zhang et al., 2004; Abdul-Jaleel et al., 2004; Eichmann et al., 2004; Erkan, 2004; Schiffman and McKeown, 2004). Thresholds are either learned on the 2003 data or determined in an ad hoc manner. Some groups also used machine learning algorithms such as SVMs by casting the problem as a binary classification (Tomiyama et al., 2004). Many groups adopted a variety of preprocessing steps including expansion of the sentences using dictionaries, ontologies or corpus-based methods and named entity recognition. Graph-based analysis has also been applied where directed edges are established by cosine similarity and chronological order. After this graph is constructed, t</context>
</contexts>
<marker>Eichmann, Zhang, Bradshaw, Qiu, Zhou, Srinivasan, Sehgal, Wong, 2004</marker>
<rawString>David Eichmann, Yi Zhang, Shannon Bradshaw, Xin Ying Qiu, Li Zhou, Padmini Srinivasan, Aditya Kumar Sehgal, and Hudon Wong. 2004. Novelty, question answering and genomics: The University of Iowa response. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
</authors>
<title>The University of Michigan in novelty</title>
<date>2004</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="6561" citStr="Erkan, 2004" startWordPosition="1055" endWordPosition="1056">elated Work There were 13 groups and 54 submitted entries for the 2004 TREC novelty track task 2. The participants used a wide range of methods which can be roughly categorized into statistical and linguistic methods. Statistical methods included traditional information retrieval models such as tf-idf and Okapi, and metrics such as importance value, new sentence value, conceptual fuzziness, scarcity measure, information weakness, unseen item count with a threshold optimized for detecting novel sentences (Blott et al., 2004; Zhang et al., 2004; Abdul-Jaleel et al., 2004; Eichmann et al., 2004; Erkan, 2004; Schiffman and McKeown, 2004). Thresholds are either learned on the 2003 data or determined in an ad hoc manner. Some groups also used machine learning algorithms such as SVMs by casting the problem as a binary classification (Tomiyama et al., 2004). Many groups adopted a variety of preprocessing steps including expansion of the sentences using dictionaries, ontologies or corpus-based methods and named entity recognition. Graph-based analysis has also been applied where directed edges are established by cosine similarity and chronological order. After this graph is constructed, the eigenvecto</context>
</contexts>
<marker>Erkan, 2004</marker>
<rawString>G¨unes Erkan. 2004. The University of Michigan in novelty 2004. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Susan Dumais</author>
<author>Eric Horvitz</author>
</authors>
<title>Newsjunkie: providing personalized newsfeeds via analysis of information novelty.</title>
<date>2004</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="1465" citStr="Gabrilovich et al., 2004" startWordPosition="231" endWordPosition="234"> wealth of news content with a corresponding problem: finding the right content for different users. Search engines are helpful if a user is looking for something specific that can be cast as a keyword query. If a user does not know what to look for, recommendation engines can make personalized suggestions for stories that may interest the user. But both types of systems frequently represent content that the user has already consumed, leading to delay and frustration. Consequently, identifying novel information has been an essential aspect of studies on news information retrieval. Newsjunkie (Gabrilovich et al., 2004), for instance, describes a system that personalizes a newsfeed based on a measure of information novelty: the user can be presented custom tailored news feeds that are novel in the context of documents that have already been reviewed. This will spare the user from hunting through duplicate and redundant content for new nuggets of information. Identifying genuinely novel information is also an essential aspect of update summarization (Nenkova and McKeown, 2012; Gao et al., 2013; Guo et al., 2013; Wang and Li, 2010; Bentivogli et al., 2011). But the temporal dynamics of a document stream are no</context>
<context position="10117" citStr="Gabrilovich et al., 2004" startWordPosition="1624" endWordPosition="1627">enization, stopword filtering and stemming (Porter, 1980)), then the inference is made whether St is novel given the context Ct−1. Without the use of the context, the time complexity of our algorithm would depend on the number of sentences so far. Thus, the features and the model for the context are important for efficiency. Note that our method only takes time complexity of O(W) for 568 both context update and feature generation. 3.1 Features KL divergence with asymmetric smoothing. KL divergence has been successfully adopted to measure the distance between a document and a set of documents (Gabrilovich et al., 2004; Gamon, 2006). We use it to measure the distance between context C and sentence S: E pC(w) log pC(w) (2) pS(w) w The intuition is that the more distant the distributions are, the more likely it is that the sentence is novel. Since KL divergence is asymmetric, both directions are used as features, with and without scale normalization. The computation of KL divergence requires both pC and pS to be non-zero; while simple add-one smoothing is employed in previous work, we adopt novel asymmetric smoothing. We add a larger smoothing factor s for already seen words than the factor u for unseen words</context>
</contexts>
<marker>Gabrilovich, Dumais, Horvitz, 2004</marker>
<rawString>Evgeniy Gabrilovich, Susan Dumais, and Eric Horvitz. 2004. Newsjunkie: providing personalized newsfeeds via analysis of information novelty. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Graph-based text representation for novelty detection.</title>
<date>2006</date>
<booktitle>In Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="7374" citStr="Gamon (2006)" startWordPosition="1179" endWordPosition="1180">s a binary classification (Tomiyama et al., 2004). Many groups adopted a variety of preprocessing steps including expansion of the sentences using dictionaries, ontologies or corpus-based methods and named entity recognition. Graph-based analysis has also been applied where directed edges are established by cosine similarity and chronological order. After this graph is constructed, the eigenvector centrality score for each sentence was computed by using a power method. The sentences with low centrality scores were considered as new (Erkan, 2004). Graph-based approaches were further pursued by Gamon (2006) that drew a richer set of features from graph topology and its changes, resulting in a system that ties with the best system at TREC 2004 (i.e. Blott et al. (2004)). On the other hand, deep linguistic methods included parsing, coreference resolution, matching discourse entities, searching for particular verbs and verb phrases, standardizing acronyms, building a named-entity lexicon, and Algorithm 1: Novelty scoring for a topical document stream Data: a document stream Result: a document stream with novelty annotation Initialize a context C0; while not at end of the document stream do read a d</context>
<context position="10131" citStr="Gamon, 2006" startWordPosition="1628" endWordPosition="1629">ing and stemming (Porter, 1980)), then the inference is made whether St is novel given the context Ct−1. Without the use of the context, the time complexity of our algorithm would depend on the number of sentences so far. Thus, the features and the model for the context are important for efficiency. Note that our method only takes time complexity of O(W) for 568 both context update and feature generation. 3.1 Features KL divergence with asymmetric smoothing. KL divergence has been successfully adopted to measure the distance between a document and a set of documents (Gabrilovich et al., 2004; Gamon, 2006). We use it to measure the distance between context C and sentence S: E pC(w) log pC(w) (2) pS(w) w The intuition is that the more distant the distributions are, the more likely it is that the sentence is novel. Since KL divergence is asymmetric, both directions are used as features, with and without scale normalization. The computation of KL divergence requires both pC and pS to be non-zero; while simple add-one smoothing is employed in previous work, we adopt novel asymmetric smoothing. We add a larger smoothing factor s for already seen words than the factor u for unseen words. The rational</context>
<context position="17524" citStr="Gamon (2006)" startWordPosition="2853" endWordPosition="2854">wed very strong results. Although RelPos and Word2Uec did not yield good results, we found them complementary to other features; performance was degraded to 0.621 and 0.624, respectively, when they were excluded from All + Recursive + DNN. The DNN-based feature transformation generally yielded better results. In particular, it becomes very effective in conjunction with recursive feature averaging. This result indicates that the DNNbased transformation allows the system to capture the non-trivial interactions between previous sentences and the current one. Systems F-score Blott et al. (2004) / Gamon (2006) 0.622 Tomiyama et al. (2004) 0.619 Abdul-Jaleel et al. (2004) 0.618 Schiffman and McKeown (2004) 0.617 KLdiv 0.614 TransCount 0.611 RelPos 0.577 Word2Vec 0.577 All 0.615 All + Recursive 0.615 All + DNN 0.617 All + Recursive + DNN 0.625 Table 1: Performance breakdown. The best result is significantly better than the other configurations (p &lt; 0.01) based on the McNemar test. Since the systems’ output is not available, we are not able to calculate statistical significance against TREC systems. 5 Conclusions We explored the space of light-weight features and their nonlinear transformation with th</context>
</contexts>
<marker>Gamon, 2006</marker>
<rawString>Michael Gamon. 2006. Graph-based text representation for novelty detection. In Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dehong Gao</author>
<author>Wenjie Li</author>
<author>Renxian Zhang</author>
</authors>
<title>Sequential summarization: A new application for timely updated Twitter trending topics.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1947" citStr="Gao et al., 2013" startWordPosition="308" endWordPosition="311">tifying novel information has been an essential aspect of studies on news information retrieval. Newsjunkie (Gabrilovich et al., 2004), for instance, describes a system that personalizes a newsfeed based on a measure of information novelty: the user can be presented custom tailored news feeds that are novel in the context of documents that have already been reviewed. This will spare the user from hunting through duplicate and redundant content for new nuggets of information. Identifying genuinely novel information is also an essential aspect of update summarization (Nenkova and McKeown, 2012; Gao et al., 2013; Guo et al., 2013; Wang and Li, 2010; Bentivogli et al., 2011). But the temporal dynamics of a document stream are not generally the focus. Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task (Allan, 2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai and Zhang, 2011) where the aim is to detect novel documents given previously seen documents. In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information. The novelty track in TREC w</context>
</contexts>
<marker>Gao, Li, Zhang, 2013</marker>
<rawString>Dehong Gao, Wenjie Li, and Renxian Zhang. 2013. Sequential summarization: A new application for timely updated Twitter trending topics. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frantisek Gr´ezl</author>
<author>Martin Karafi´at</author>
<author>Stanislav Kont´ar</author>
<author>Jan Cernocky</author>
</authors>
<title>Probabilistic and bottle-neck features for lvcsr of meetings.</title>
<date>2007</date>
<booktitle>In Acoustics, Speech and Signal Processing,</booktitle>
<volume>4</volume>
<pages>757</pages>
<publisher>IEEE.</publisher>
<marker>Gr´ezl, Karafi´at, Kont´ar, Cernocky, 2007</marker>
<rawString>Frantisek Gr´ezl, Martin Karafi´at, Stanislav Kont´ar, and Jan Cernocky. 2007. Probabilistic and bottle-neck features for lvcsr of meetings. In Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, volume 4, pages IV– 757. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Guo</author>
<author>Fernando Diaz</author>
<author>Elad Yom-Tov</author>
</authors>
<title>Updating users about time critical events.</title>
<date>2013</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>483--494</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1965" citStr="Guo et al., 2013" startWordPosition="312" endWordPosition="315">rmation has been an essential aspect of studies on news information retrieval. Newsjunkie (Gabrilovich et al., 2004), for instance, describes a system that personalizes a newsfeed based on a measure of information novelty: the user can be presented custom tailored news feeds that are novel in the context of documents that have already been reviewed. This will spare the user from hunting through duplicate and redundant content for new nuggets of information. Identifying genuinely novel information is also an essential aspect of update summarization (Nenkova and McKeown, 2012; Gao et al., 2013; Guo et al., 2013; Wang and Li, 2010; Bentivogli et al., 2011). But the temporal dynamics of a document stream are not generally the focus. Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task (Allan, 2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai and Zhang, 2011) where the aim is to detect novel documents given previously seen documents. In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information. The novelty track in TREC was designed to ser</context>
</contexts>
<marker>Guo, Diaz, Yom-Tov, 2013</marker>
<rawString>Qi Guo, Fernando Diaz, and Elad Yom-Tov. 2013. Updating users about time critical events. In Advances in Information Retrieval, pages 483–494. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
</authors>
<title>Overview of the trec</title>
<date>2002</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="2687" citStr="Harman, 2002" startWordPosition="436" endWordPosition="438">ly the focus. Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task (Allan, 2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai and Zhang, 2011) where the aim is to detect novel documents given previously seen documents. In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information. The novelty track in TREC was designed to serve as a shared task for exactly this type of research: finding novel, on-topic sentences from a news stream (Harman, 2002). There were four tasks in the novelty track but we only focus on task 2 in this paper: “given relevant sentences in all documents, identify all novel sentences.” The track changed slightly from year to year. The data of the first run in 2002 (Harman, 2002) used old topics and judgments which proved to be problematic due to the small percentage of relevant sentences. TREC 2003 (Soboroff and Harman, 2003) included 50 new topics with an improved balance of relevant and novel sentences and chronologically ordered documents. TREC 2004 (Soboroff and Harman, 2005) used the same task settings and the</context>
</contexts>
<marker>Harman, 2002</marker>
<rawString>Donna Harman. 2002. Overview of the trec 2002 novelty track. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margarita Karkali</author>
<author>Franc¸ois Rousseau</author>
</authors>
<title>Alexandros Ntoulas, and Michalis Vazirgiannis.</title>
<date>2014</date>
<location>CoRR, abs/1401.1456.</location>
<marker>Karkali, Rousseau, 2014</marker>
<rawString>Margarita Karkali, Franc¸ois Rousseau, Alexandros Ntoulas, and Michalis Vazirgiannis. 2014. Using temporal IDF for efficient novelty detection in text streams. CoRR, abs/1401.1456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margarita Karkali1</author>
</authors>
<title>Alexandros Ntoulas, Franois Rousseau, and Michalis Vazirgiannis.</title>
<date>2013</date>
<booktitle>In Proceedings of Web Information Systems Engineering.</booktitle>
<marker>Karkali1, 2013</marker>
<rawString>Margarita Karkali1, Alexandros Ntoulas, Franois Rousseau, and Michalis Vazirgiannis. 2013. Efficient online novelty detection in news streams. In Proceedings of Web Information Systems Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Kingdom</author>
<author>Nicolaas Prins</author>
</authors>
<title>Psychophysics: a practical introduction.</title>
<date>2009</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="11472" citStr="Kingdom and Prins, 2009" startWordPosition="1845" endWordPosition="1848">words (Figure 1.) Asymmetric smoothing with various smoothing factors consistently showed better performance than symmetric smoothing in our experiments. Figure 1: KL divergence with symmetric (left) and asymmetric (right) smoothing. Pink and blue correspond to two distributions while light yellow and orange to smoothing factors. Nonlinear transformation of unseen word count. One of the simplest metrics to measure novelty is the plain count of unseen words. This measure, however, does not necessarily reflect human perception of novelty given the prevalence of nonlinearity in human perception (Kingdom and Prins, 2009). Thus, we explored the use of a simple nonlinear transformation of unseen word counts instead of the plain count (Figure 2): T(n) = (αn + 0)γ (3) where n is the number of new words and α, 0 and y are parameters. In our experiments, the use of a nonlinear transformation helped yield better results. Figure 2: Nonlinear transformation of unseen word count with parameters set via cross-validation on the TREC training data: α = 0.5, 0 = 0 and y = 1.5. Relative position in a document. Relative position of a sentence in a document is simple yet has been proven effective for summarization. Relative p</context>
</contexts>
<marker>Kingdom, Prins, 2009</marker>
<rawString>Frederick Kingdom and Nicolaas Prins. 2009. Psychophysics: a practical introduction. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="12612" citStr="Mikolov et al., 2013" startWordPosition="2040" endWordPosition="2043"> a document is simple yet has been proven effective for summarization. Relative position is also closely related to novelty detection as follows: 1) There is in general a good chance that earlier sentences are more novel than the later ones. 2) We found a pattern that news articles coming in later are apt to present novel information first and then a summary of old information. Word embedding-based similarity. Neural word embedding techniques can be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Socher et al., 2012; Mikolov et al., 2013). As reported in (Tai et al., 2015), a simple averaging scheme was found to be very competitive to more complex models for representing a sentence vector. These observations lead us to adopt the following additional features derived from word embeddings: 1) cosine similarity between the mean vectors of the context C and sentence S, 2) sigmoid function value for the dot product of the mean vectors of the context C and sentence S. The mean vectors of C and S are computed by taking the average of the word vectors of each unique word in C and S, respectively. We use word embedding with 100 dimensi</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>A survey of text summarization techniques.</title>
<date>2012</date>
<booktitle>In Mining Text Data,</booktitle>
<pages>43--76</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1929" citStr="Nenkova and McKeown, 2012" startWordPosition="304" endWordPosition="307">tration. Consequently, identifying novel information has been an essential aspect of studies on news information retrieval. Newsjunkie (Gabrilovich et al., 2004), for instance, describes a system that personalizes a newsfeed based on a measure of information novelty: the user can be presented custom tailored news feeds that are novel in the context of documents that have already been reviewed. This will spare the user from hunting through duplicate and redundant content for new nuggets of information. Identifying genuinely novel information is also an essential aspect of update summarization (Nenkova and McKeown, 2012; Gao et al., 2013; Guo et al., 2013; Wang and Li, 2010; Bentivogli et al., 2011). But the temporal dynamics of a document stream are not generally the focus. Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task (Allan, 2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai and Zhang, 2011) where the aim is to detect novel documents given previously seen documents. In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information. The novel</context>
</contexts>
<marker>Nenkova, McKeown, 2012</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2012. A survey of text summarization techniques. In Mining Text Data, pages 43–76. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Sean Moran</author>
<author>Richard McCreadie</author>
<author>Alexander Von Lunen</author>
<author>Martin D Sykora</author>
</authors>
<title>Real-time detection, tracking, and monitoring of automatically discovered events in social media.</title>
<date>2014</date>
<location>Elizabeth Cano, Neil Ireson, Craig Macdonald, Iadh Ounis, Yulan He, et</location>
<marker>Osborne, Moran, McCreadie, Von Lunen, Sykora, 2014</marker>
<rawString>Miles Osborne, Sean Moran, Richard McCreadie, Alexander Von Lunen, Martin D Sykora, Elizabeth Cano, Neil Ireson, Craig Macdonald, Iadh Ounis, Yulan He, et al. 2014. Real-time detection, tracking, and monitoring of automatically discovered events in social media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="9550" citStr="Porter, 1980" startWordPosition="1529" endWordPosition="1530">core as the posterior probability of a binary novelty random variable N: 1 � p(Nt|St,Ct−1) = Z exp wifi(Nt, St, Ct−1) i (1) in which the fi are feature functions, wi model parameters, St the sentence in focus and Ct−1 a context containing information about previously seen sentences S1 through St−1 across documents. The overall procedure is listed in Algorithm 1. The algorithm takes as input documents which have been clustered by topic and chronologically ordered. For each sentence St in each document, basic preprocessing is performed (e.g. simple tokenization, stopword filtering and stemming (Porter, 1980)), then the inference is made whether St is novel given the context Ct−1. Without the use of the context, the time complexity of our algorithm would depend on the number of sentences so far. Thus, the features and the model for the context are important for efficiency. Note that our method only takes time complexity of O(W) for 568 both context update and feature generation. 3.1 Features KL divergence with asymmetric smoothing. KL divergence has been successfully adopted to measure the distance between a document and a set of documents (Gabrilovich et al., 2004; Gamon, 2006). We use it to meas</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Schiffman</author>
<author>Kathleen McKeown</author>
</authors>
<title>Columbia University in the novelty track at TREC</title>
<date>2004</date>
<booktitle>In Proceedings of TREC.</booktitle>
<contexts>
<context position="6591" citStr="Schiffman and McKeown, 2004" startWordPosition="1057" endWordPosition="1060">here were 13 groups and 54 submitted entries for the 2004 TREC novelty track task 2. The participants used a wide range of methods which can be roughly categorized into statistical and linguistic methods. Statistical methods included traditional information retrieval models such as tf-idf and Okapi, and metrics such as importance value, new sentence value, conceptual fuzziness, scarcity measure, information weakness, unseen item count with a threshold optimized for detecting novel sentences (Blott et al., 2004; Zhang et al., 2004; Abdul-Jaleel et al., 2004; Eichmann et al., 2004; Erkan, 2004; Schiffman and McKeown, 2004). Thresholds are either learned on the 2003 data or determined in an ad hoc manner. Some groups also used machine learning algorithms such as SVMs by casting the problem as a binary classification (Tomiyama et al., 2004). Many groups adopted a variety of preprocessing steps including expansion of the sentences using dictionaries, ontologies or corpus-based methods and named entity recognition. Graph-based analysis has also been applied where directed edges are established by cosine similarity and chronological order. After this graph is constructed, the eigenvector centrality score for each se</context>
<context position="13490" citStr="Schiffman and McKeown, 2004" startWordPosition="2187" endWordPosition="2191">ngs: 1) cosine similarity between the mean vectors of the context C and sentence S, 2) sigmoid function value for the dot product of the mean vectors of the context C and sentence S. The mean vectors of C and S are computed by taking the average of the word vectors of each unique word in C and S, respectively. We use word embedding with 100 dimensions trained on Wikipedia using the word2vec toolkit (https://code.google.com/p/word2vec). 3.2 Feature transformation Recursive feature averaging. A large portion of the novel sentences in the TREC 2004 data appear in consecutive runs of two or more (Schiffman and McKeown, 2004). Sequential labeling would be a natural approach to take advantage of this characteristic of the problem, but the use of sequential labeling will make time complexity depend on the number of sentences T. Thus we came up with another way to exploit this characteristic, recursively averaging over previous feature vectors and augmenting the current feature vector with 569 the average: Rt = ηFt−1 + (1 − η)Rt−1 (4) Ft0 =Ft::Rt (5) where F is a feature vector, R the average vector of previous ones, F0 the augmented feature vector, η the weight of the last feature vector in averaging and :: means co</context>
<context position="17621" citStr="Schiffman and McKeown (2004)" startWordPosition="2866" endWordPosition="2869"> we found them complementary to other features; performance was degraded to 0.621 and 0.624, respectively, when they were excluded from All + Recursive + DNN. The DNN-based feature transformation generally yielded better results. In particular, it becomes very effective in conjunction with recursive feature averaging. This result indicates that the DNNbased transformation allows the system to capture the non-trivial interactions between previous sentences and the current one. Systems F-score Blott et al. (2004) / Gamon (2006) 0.622 Tomiyama et al. (2004) 0.619 Abdul-Jaleel et al. (2004) 0.618 Schiffman and McKeown (2004) 0.617 KLdiv 0.614 TransCount 0.611 RelPos 0.577 Word2Vec 0.577 All 0.615 All + Recursive 0.615 All + DNN 0.617 All + Recursive + DNN 0.625 Table 1: Performance breakdown. The best result is significantly better than the other configurations (p &lt; 0.01) based on the McNemar test. Since the systems’ output is not available, we are not able to calculate statistical significance against TREC systems. 5 Conclusions We explored the space of light-weight features and their nonlinear transformation with the goal of supporting online web-scale sentence novelty detection. The experiment results show tha</context>
</contexts>
<marker>Schiffman, McKeown, 2004</marker>
<rawString>Barry Schiffman and Kathleen McKeown. 2004. Columbia University in the novelty track at TREC 2004. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Soboroff</author>
<author>Donna Harman</author>
</authors>
<title>Overview of the trec</title>
<date>2003</date>
<booktitle>In TREC,</booktitle>
<pages>38--53</pages>
<contexts>
<context position="3094" citStr="Soboroff and Harman, 2003" startWordPosition="508" endWordPosition="511">highlight sentences that contain novel information. The novelty track in TREC was designed to serve as a shared task for exactly this type of research: finding novel, on-topic sentences from a news stream (Harman, 2002). There were four tasks in the novelty track but we only focus on task 2 in this paper: “given relevant sentences in all documents, identify all novel sentences.” The track changed slightly from year to year. The data of the first run in 2002 (Harman, 2002) used old topics and judgments which proved to be problematic due to the small percentage of relevant sentences. TREC 2003 (Soboroff and Harman, 2003) included 50 new topics with an improved balance of relevant and novel sentences and chronologically ordered documents. TREC 2004 (Soboroff and Harman, 2005) used the same task settings and the same number of topics, but made a major change through the inclusion of irrelevant documents. Although the participants in the novelty track of TREC and many followup studies have investigated a wide ranging set of features and algorithms (Soboroff and Harman, 2005), almost none were specifically focused on scalability. However, modern news aggregators are usually visited by millions of unique users and</context>
</contexts>
<marker>Soboroff, Harman, 2003</marker>
<rawString>Ian Soboroff and Donna Harman. 2003. Overview of the trec 2003 novelty track. In TREC, pages 38–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Soboroff</author>
<author>Donna Harman</author>
</authors>
<title>Novelty detection: the TREC experience.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="3251" citStr="Soboroff and Harman, 2005" startWordPosition="532" endWordPosition="535">ng novel, on-topic sentences from a news stream (Harman, 2002). There were four tasks in the novelty track but we only focus on task 2 in this paper: “given relevant sentences in all documents, identify all novel sentences.” The track changed slightly from year to year. The data of the first run in 2002 (Harman, 2002) used old topics and judgments which proved to be problematic due to the small percentage of relevant sentences. TREC 2003 (Soboroff and Harman, 2003) included 50 new topics with an improved balance of relevant and novel sentences and chronologically ordered documents. TREC 2004 (Soboroff and Harman, 2005) used the same task settings and the same number of topics, but made a major change through the inclusion of irrelevant documents. Although the participants in the novelty track of TREC and many followup studies have investigated a wide ranging set of features and algorithms (Soboroff and Harman, 2005), almost none were specifically focused on scalability. However, modern news aggregators are usually visited by millions of unique users and consume millions of stories each day. Moreover, every few minutes item churn takes place and the stories of interest are likely to be the ones that appeared</context>
<context position="8596" citStr="Soboroff and Harman, 2005" startWordPosition="1373" endWordPosition="1376"> a document; split the document into sentences; while not at end of the document do read a sentence St; perform preprocessing on St; compute novelty score as the posterior probability of a binary novelty random variable Nt, p(NtlSt, Ct−1); update the context Ct with Ct−1 and St; end compute a document-level score (e.g. average out all sentence-level scores) end matching concepts to manually-constructed ontology for topic-specific concepts (Amrani et al., 2004). The difficulty of the novelty detection task is evident from the relatively low score achieved by even the best systems at TREC 2004 (Soboroff and Harman, 2005). The top scoring systems were mostly based on statistical methods while deep linguistic approaches achieved the highest precision at the cost of poor recall. 3 Method For the purpose of this paper, we formulate task 2 of the TREC novelty detection track as an online probabilistic inference task. More specifically, we compute the novelty score as the posterior probability of a binary novelty random variable N: 1 � p(Nt|St,Ct−1) = Z exp wifi(Nt, St, Ct−1) i (1) in which the fi are feature functions, wi model parameters, St the sentence in focus and Ct−1 a context containing information about pr</context>
</contexts>
<marker>Soboroff, Harman, 2005</marker>
<rawString>Ian Soboroff and Donna Harman. 2005. Novelty detection: the TREC experience. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Yoshua Bengio</author>
<author>Christopher D Manning</author>
</authors>
<title>Deep learning for nlp (without magic).</title>
<date>2012</date>
<booktitle>In Tutorial Abstracts of ACL 2012, ACL ’12,</booktitle>
<pages>5--5</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12589" citStr="Socher et al., 2012" startWordPosition="2036" endWordPosition="2039">tion of a sentence in a document is simple yet has been proven effective for summarization. Relative position is also closely related to novelty detection as follows: 1) There is in general a good chance that earlier sentences are more novel than the later ones. 2) We found a pattern that news articles coming in later are apt to present novel information first and then a summary of old information. Word embedding-based similarity. Neural word embedding techniques can be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Socher et al., 2012; Mikolov et al., 2013). As reported in (Tai et al., 2015), a simple averaging scheme was found to be very competitive to more complex models for representing a sentence vector. These observations lead us to adopt the following additional features derived from word embeddings: 1) cosine similarity between the mean vectors of the context C and sentence S, 2) sigmoid function value for the dot product of the mean vectors of the context C and sentence S. The mean vectors of C and S are computed by taking the average of the word vectors of each unique word in C and S, respectively. We use word emb</context>
</contexts>
<marker>Socher, Bengio, Manning, 2012</marker>
<rawString>Richard Socher, Yoshua Bengio, and Christopher D. Manning. 2012. Deep learning for nlp (without magic). In Tutorial Abstracts of ACL 2012, ACL ’12, pages 5–5, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</title>
<date>2015</date>
<contexts>
<context position="12647" citStr="Tai et al., 2015" startWordPosition="2047" endWordPosition="2050">en effective for summarization. Relative position is also closely related to novelty detection as follows: 1) There is in general a good chance that earlier sentences are more novel than the later ones. 2) We found a pattern that news articles coming in later are apt to present novel information first and then a summary of old information. Word embedding-based similarity. Neural word embedding techniques can be effective in capturing syntactic and semantic relationships, and more computationally efficient than many other competitors (Socher et al., 2012; Mikolov et al., 2013). As reported in (Tai et al., 2015), a simple averaging scheme was found to be very competitive to more complex models for representing a sentence vector. These observations lead us to adopt the following additional features derived from word embeddings: 1) cosine similarity between the mean vectors of the context C and sentence S, 2) sigmoid function value for the dot product of the mean vectors of the context C and sentence S. The mean vectors of C and S are computed by taking the average of the word vectors of each unique word in C and S, respectively. We use word embedding with 100 dimensions trained on Wikipedia using the </context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoe Tomiyama</author>
</authors>
<title>Kosuke Karoji, Takeshi Kondo, Yuichi Kakuta, Tomohiro Takagi, Akiko Aizawa, and Teruhito Kanazawa.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC.</booktitle>
<marker>Tomiyama, 2004</marker>
<rawString>Tomoe Tomiyama, Kosuke Karoji, Takeshi Kondo, Yuichi Kakuta, Tomohiro Takagi, Akiko Aizawa, and Teruhito Kanazawa. 2004. Meiji University web, novelty and genomic track experiments. In Proceedings of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsai</author>
<author>Yi Zhang</author>
</authors>
<title>D2s: Document-tosentence framework for novelty detection.</title>
<date>2011</date>
<journal>Knowledge and Information Systems,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="2281" citStr="Tsai and Zhang, 2011" startWordPosition="366" endWordPosition="369">cuments that have already been reviewed. This will spare the user from hunting through duplicate and redundant content for new nuggets of information. Identifying genuinely novel information is also an essential aspect of update summarization (Nenkova and McKeown, 2012; Gao et al., 2013; Guo et al., 2013; Wang and Li, 2010; Bentivogli et al., 2011). But the temporal dynamics of a document stream are not generally the focus. Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task (Allan, 2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai and Zhang, 2011) where the aim is to detect novel documents given previously seen documents. In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information. The novelty track in TREC was designed to serve as a shared task for exactly this type of research: finding novel, on-topic sentences from a news stream (Harman, 2002). There were four tasks in the novelty track but we only focus on task 2 in this paper: “given relevant sentences in all documents, identify all novel sentences.” The track changed slightly from</context>
</contexts>
<marker>Tsai, Zhang, 2011</marker>
<rawString>FloraS. Tsai and Yi Zhang. 2011. D2s: Document-tosentence framework for novelty detection. Knowledge and Information Systems, 29(2):419–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
</authors>
<title>Document update summarization using incremental hierarchical clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="1984" citStr="Wang and Li, 2010" startWordPosition="316" endWordPosition="319">n essential aspect of studies on news information retrieval. Newsjunkie (Gabrilovich et al., 2004), for instance, describes a system that personalizes a newsfeed based on a measure of information novelty: the user can be presented custom tailored news feeds that are novel in the context of documents that have already been reviewed. This will spare the user from hunting through duplicate and redundant content for new nuggets of information. Identifying genuinely novel information is also an essential aspect of update summarization (Nenkova and McKeown, 2012; Gao et al., 2013; Guo et al., 2013; Wang and Li, 2010; Bentivogli et al., 2011). But the temporal dynamics of a document stream are not generally the focus. Novelty detection has also been studied in Topic Detection and Tracking field for the First Story Detection task (Allan, 2002; Karkali1 et al., 2013; Karkali et al., 2014; Tsai and Zhang, 2011) where the aim is to detect novel documents given previously seen documents. In this paper, we examine a slightly different problem; we perform novelty detection at the sentence level to highlight sentences that contain novel information. The novelty track in TREC was designed to serve as a shared task</context>
</contexts>
<marker>Wang, Li, 2010</marker>
<rawString>Dingding Wang and Tao Li. 2010. Document update summarization using incremental hierarchical clustering. In Proceedings of the 19th ACM International Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
<author>Hongbo Xu</author>
<author>Shuo Bai</author>
<author>Bin Wang</author>
<author>Xueqi Cheng</author>
</authors>
<title>novelty track at CAS-ICT.</title>
<date>2004</date>
<booktitle>Experiments in TREC</booktitle>
<contexts>
<context position="6498" citStr="Zhang et al., 2004" startWordPosition="1043" endWordPosition="1046">e finish with some conclusions and future directions in Section 5. 2 Related Work There were 13 groups and 54 submitted entries for the 2004 TREC novelty track task 2. The participants used a wide range of methods which can be roughly categorized into statistical and linguistic methods. Statistical methods included traditional information retrieval models such as tf-idf and Okapi, and metrics such as importance value, new sentence value, conceptual fuzziness, scarcity measure, information weakness, unseen item count with a threshold optimized for detecting novel sentences (Blott et al., 2004; Zhang et al., 2004; Abdul-Jaleel et al., 2004; Eichmann et al., 2004; Erkan, 2004; Schiffman and McKeown, 2004). Thresholds are either learned on the 2003 data or determined in an ad hoc manner. Some groups also used machine learning algorithms such as SVMs by casting the problem as a binary classification (Tomiyama et al., 2004). Many groups adopted a variety of preprocessing steps including expansion of the sentences using dictionaries, ontologies or corpus-based methods and named entity recognition. Graph-based analysis has also been applied where directed edges are established by cosine similarity and chron</context>
</contexts>
<marker>Zhang, Xu, Bai, Wang, Cheng, 2004</marker>
<rawString>Huaping Zhang, Hongbo Xu, Shuo Bai, Bin Wang, and Xueqi Cheng. 2004. Experiments in TREC 2004 novelty track at CAS-ICT. In Proceedings of TREC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.442481">
Hierarchical Recurrent Neural Network for Document Modeling
Rui Lin†∗, Shujie Liu‡, Muyun Yang†, Mu Li‡, Ming Zhou‡, Sheng Li††Harbin Institute of Technology
</note>
<email confidence="0.742899">
{linrui,ymy}@mtlab.hit.edu.cn lisheng@hit.edu.cn
</email>
<author confidence="0.204248">
‡Microsoft Research
</author>
<email confidence="0.992735">
{shujliu,muli,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.994627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858272727273">
This paper proposes a novel hierarchical
recurrent neural network language model
(HRNNLM) for document modeling. Af-
ter establishing a RNN to capture the co-
herence between sentences in a documen-
t, HRNNLM integrates it as the sentence
history information into the word level
RNN to predict the word sequence with
cross-sentence contextual information. A
two-step training approach is designed, in
which sentence-level and word-level lan-
guage models are approximated for the
convergence in a pipeline style. Examined
by the standard sentence reordering sce-
nario, HRNNLM is proved for its better
accuracy in modeling the sentence coher-
ence. And at the word level, experimen-
tal results also indicate a significant low-
er model perplexity, followed by a practi-
cal better translation result when applied
to a Chinese-English document translation
reranking task.
</bodyText>
<sectionHeader confidence="0.998875" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996263">
Deep Neural Network (DNN), a neural network
with multiple layers, has been proven powerful in
many different domains, such as visual recogni-
tion (Kavukcuoglu et al., 2010) and speech recog-
nition (Dahl et al., 2012), ever since Hinton et al.
(2006) formulated an efficient training method for
it.
In addition to the applications mentioned above,
many neural network based methods have also
been applied to natural language processing (NLP)
tasks with great success. For example, Collobert et
al. (2011) propose a generalized DNN framework
for a variety of fundamental NLP tasks, including
part-of-speech tagging (postag), chunking, named
*Contribution during internship at Microsoft Research.
entity recognition (NER), and semantic role label-
ing.
DNN is successfully introduced to do word-
level language modeling, aka., to predict the next
word given the history words. Bengio et al. (2003)
propose a feedforward neural network to train a
word-level language model with a limited n-gram
history. To leverage as much history as possible,
Mikolov et al. (2010) apply recurrent neural net-
work to word-level language modeling. The mod-
el absorbs one word each time, keeps the informa-
tion in a history vector, and predicts the next word
with all the word history in the vector.
Word-level language model can only learn the
relationship between words in one sentence. For
sentences in one document which talks about one
or several specific topics, the words in the next
sentence are chosen partially in accordance with
the previous sentences. To model this kind of co-
herence of sentences, Le and Mikolov (2014) ex-
tend word embedding learning network (Mikolov
et al., 2013) to learn the paragraph embedding as
a fixed-length vector representation for paragraph
or sentence. Li and Hovy (2014) propose a neu-
ral network coherence model which employs dis-
tributed sentence representation and then predict
the probability of whether a sequence of sentences
is coherent or not.
In contrast to the methods mentioned above
which learn the word relationship in or between
the sentences separately, we propose a hierar-
chical recurrent neural network language model
(HRNNLM) to capture the word sequence across
the sentence boundaries at the document level.
HRNNLM is essentially a combination of a word-
level language model and a sentence-level lan-
guage model, both of which are recurrent neu-
ral networks. The word-level recurrent neural
network follows (Mikolov et al., 2010). The
sentence-level language model is another recur-
rent neural network that takes sentence represen-
</bodyText>
<page confidence="0.984897">
899
</page>
<note confidence="0.9850345">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 899–907,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999961487179487">
tation as input, and predicts the words in the next
sentence. Similar to (Mikolov et al., 2010), the
hidden layer in the sentence-level recurrent neural
network contains the sentence history information.
The hidden layer containing the history informa-
tion of previous sentences is then linked as an in-
put to the word-level recurrent neural network to
predict the next word together with the word-level
history vector. This allows the language model to
predict the next word probability distribution be-
yond the words in the current sentence.
We propose a two-step training approach to op-
timize the parameters of HRNNLM. In the first
step, we train the sentence-level language model-
s independently . And then, we connect the hid-
den layer of the sentence-level language model to
the input of word-level RNNLM and train the two
models jointly until converged. At sentence level,
we evaluate our model with a sentence ordering
task and the result shows our method can outper-
form a maximum entropy based and another state-
of-the-art solution. At word level, we compare our
method with the conventional recurrent neural net-
work based language model, finding the perplexity
is reduced significantly. We also apply our method
to rank machine translation output and conduct ex-
periments on a Chinese-English document transla-
tion task, yielding a better translation results com-
pared with a state-of-the-art baseline system.
The rest of this paper is organized as follows:
Section 2 introduces work related to applying neu-
ral network to document modeling and SMT. Sec-
tion 3 introduces the general framework for doc-
ument modeling. Our sentence-level language
model and its training is described in Section 4,
and the overall HRNNLM and its training is pre-
sented in Section 5. Section 6 presents our exper-
iments and their results. Finally, we conclude in
Section 7.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99971193442623">
In this section, we introduce previous efforts on
applying neural network to model words coher-
ence across sentence boundaries as well as works
on improving machine translation performance at
discourse level.
Mikolov and Zweig (2012) propose a RNN-
LDA model to implement a context dependent lan-
guage model. They augment the contextual infor-
mation into the conventional RNNLM via a real-
valued input vector, which is the probability distri-
bution computed by LDA topics for using a block
of preceding text. They train a Latent Dirichlet Al-
location (LDA) model using documents consisting
of about 10 sentences long text from Penn Tree-
bank (PTB) training data. Their approach outper-
forms RNNLM in perplexity on PTB data with a
limited context history over topics instead of com-
plete information of preceding sentences.
Le and Mikolov (2014) extend the Continu-
ous Bag-of-Words Model (CBOW) and Continu-
ous Skip-gram Model (Skip-gram) (Mikolov et al.,
2013) by introducing a paragraph vector. In their
method, the paragraph vector is learnt in a simi-
lar way of word vector model, and there will be
N x P parameters, if there are N paragraphs and
each paragraph is mapped to P dimensions. Dif-
ferent from them, the sentence vectors of our mod-
el are learnt with nearly unlimited sentence his-
tory based on a RNN framework, in which, bag
of words in the sentence are used as input. The
sentence vector is no longer related with the sen-
tence id, but only based on the words in the sen-
tence. And our sentence vector also integrates n-
early all the history information of previous sen-
tences, while their model cannot.
Li and Hovy (2014) implement a neural net-
work model to predict discourse coherence qual-
ity in essays. In their work, recurrent (Sutskever
et al., 2011) and recursive (Socher et al., 2013)
neural networks are both examined to learn dis-
tributed sentence representation given pre-trained
word embedding. The distributed sentence repre-
sentation is assigned to capture both syntactic and
semantic information. With a slide window of the
distributed sentence representation, a neural net-
work classifier is trained to evaluate the coherence
of the text. Successful as it is in scoring the co-
herence for a given sequence of sentences, this
method is attempted to discriminate the different
word order within a sentence.
An attempt of introducing RNN into convolu-
tional neural network (CNN) is investigated by (X-
u and Sarikaya, 2014) for spoken language un-
derstanding (SLU). To alleviate more contextual
information, they apply a CNN with Jordan-type
(Jordan, 1997) recurrent connections. The recur-
rent connections send the distribution of the last
softmax layer’s output to the current input layer
as additional features. Aimed to improve SLU
domain classification, their model is essentially a
kind of document representation with certain text
</bodyText>
<page confidence="0.99157">
900
</page>
<bodyText confidence="0.9999242">
For the sentence Sk containing words w1, w2, w3,
..., wT, p(Sk|S1, S2,..., Sk−1) is defined as:
information, neglecting the coherence information
between sentences.
Following the thread modeling the word se-
quence relationship within and across sentences,
we propose a hierarchical recurrent neural net-
work language model consist of a sentence-level
language model and a word-level language model.
This overall network is trained to capture the co-
herence between sentences and predict words se-
quence with preceding sentence contexts.
For statistical machine translation (SMT) in
which we checked out model as a scenario, DNN
has also been revealed for certain good results in
several components. Yang et al. (2013) adapt and
extend the CD-DNN-HMM (Dahl et al., 2012)
model to the HMM-based word alignment model.
In their method, they use bilingual word embed-
ding to capture the lexical translation information
and modeling the context with surrounding word-
s. Liu et al. (2014) propose a recursive recurrent
neural network (R2NN) for end-to-end decoding
to help improve translation quality. And Cho et
al. (2014) propose a RNN Encoder-Decoder which
is a joint recurrent neural network model at the
sentence level as conventional SMT decoder does.
However, at the discourse level, there is little re-
port on applying DNN to boost the translation re-
sult of a document.
</bodyText>
<sectionHeader confidence="0.985219" genericHeader="method">
3 Document Language Modeling
</sectionHeader>
<bodyText confidence="0.999888454545454">
Statistical language model assigns a probability to
a natural language sequence. Conventional lan-
guage models only focus on the word sequence
within a sentence. For sentences in one documen-
t talking about one or several specific topics, the
adjacent sentences should be in a coherent order.
Therefore, the words in the next sentence are also
dependent on the preceding sentences. To mod-
el the coherence of sentences in the document D,
which contains N sentences S1, S2, S3, ..., SN,
we need to maximize the objective as follow:
</bodyText>
<equation confidence="0.977054714285714">
p(D) =p(S1, S2,..., SN)
=p(S1) - p(S2|S1) - p(S3|S1,S2) (1)
...p(SN|S1, S2, ..., SN−1)
p(Sk|S1, S2, ..., Sk−1)
= p(w1, w2, ..., wT |S1, ..., Sk−1)
= p(w1|S1, ..., Sk−1) - p(w2|w1, S1, ..., Sk−1)
...p(wT|w1, w2, ..., wT−1, S1, ..., Sk−1)
</equation>
<bodyText confidence="0.999967538461538">
As a special case of approximation to this, clas-
sical n-gram language model keep only sever-
al words as history, discarding any information
across the sentence boundaries. Recurrent neural
network language model (Mikolov et al., 2010) us-
es a hidden layer which employs a real-valued vec-
tor recurrently as network’s input to keep as many
history as possible. This makes RNNLM be able
to extend for capturing history beyond a sentence.
To prevent the potential exponential decay
of the history, the history length in RNN
can not be too long. Here we approximate
the history information of previous sentences,
</bodyText>
<equation confidence="0.917867333333333">
p(Sk|S1, S2, ..., Sk−1), by the following:
p(Sk|S1, S2, ..., Sk−1) =
p(BoWSk|BoWS1, ..., BoWSk_1) - p(Sk|BoWSk)
</equation>
<bodyText confidence="0.905673666666667">
where BoWSk denotes the bag of words for the
sentence Sk. The document is thus generated in
two steps.
</bodyText>
<listItem confidence="0.979291857142857">
• Given the previous sentences BoWS1 , ...,
BoWSk_1 (treating them as bag of words here),
first generate the words which will show in the
next sentence without considering their order
with p(BoWSk|BoWS1, ..., BoWSk_1)
• Generate the words one by one with
p(Sk|BoWSk).
</listItem>
<bodyText confidence="0.999927571428571">
The first phase actually completes sentence-level
language modeling, and the second addresses the
word-level language modeling. Because recurrent
neural network has a natural advantage in process-
ing sequential data, we investigate how to model
the whole process under a unified framework of
recurrent neural network.
</bodyText>
<sectionHeader confidence="0.982564" genericHeader="method">
4 Sentence-level Language Model
</sectionHeader>
<bodyText confidence="0.9999265">
In this section, we describe how to leverage re-
current neural network for sentence-level language
modeling. Mikolov et al. (2010) demonstrate a re-
current neural network language model (RNNLM)
</bodyText>
<page confidence="0.989899">
901
</page>
<bodyText confidence="0.999966142857143">
for word ordering. It overcomes the limitations of
classical language model in capturing only a fixed-
length history, yielding a significant performance
improvements in terms of perplexity reduction and
speech recognition accuracy. Here we adept this
framework for a RNN based sentence-level lan-
guage modeling, i.e. RNNSLM.
</bodyText>
<subsectionHeader confidence="0.998029">
4.1 Model
</subsectionHeader>
<bodyText confidence="0.999314363636364">
A conventional language model reads a word each
time, keeps several words as history and then pre-
dict the probability distribution of the next word.
Similar to this, our sentence-level language model
reads a sentence which is a bag of words repre-
sentation. And then it stores the sentence history
which captures coherence of sentences in a real-
valued history vector. With the history vector, our
model can predict which words are most likely to
appear in the next sentence. All these will be mod-
eled by a recurrent neural network.
</bodyText>
<figure confidence="0.403238">
xsJ
</figure>
<figureCaption confidence="0.942711">
Figure 1: Recurrent Neural Network for Sentence-
</figureCaption>
<bodyText confidence="0.984860083333333">
level Language Modeling
As shown in Figure 1, similar to the convention-
al recurrent neural network, for the sentence j, our
network has two input layers xsj and hsj_1. xsj
is the current sentence representation, and hsj_1
is the history information vector before sentence
j. The model has a hidden layer hsj, which will
combine the history information of hsj_1 and the
current sentence input xsj, and an output layer
ysj+1, which generates the probabilities of the
words in the sentence j + 1. The layers are com-
puted as follows:
</bodyText>
<equation confidence="0.999794">
hsj = f(Us · hsj_1 + Ws · xsj) (4)
ysj+1 = g(Vs · hsj) (5)
</equation>
<bodyText confidence="0.807911">
where Ws, Us and Vs denote the weight matrix.
f(z) is a HTanh function:
</bodyText>
<equation confidence="0.997423428571429">
f(zj) = { −1 zj &lt; −1
z −1 &lt; zj &lt; 1
1 zj &gt; 1 (6)
and g(z) is a softmax function:
ezj
g(zj) = E (7)
k ezk
</equation>
<bodyText confidence="0.999729428571429">
The output layer ysj+1 is a 1xV vector that repre-
sents probability distribution of words in the next
sentence given the current sentence xsj and pre-
vious history hsj_1, where V denotes vocabulary
size.
To emphasize coherence between the adjacen-
t sentences, we further add some bigram-like bag
of words feature to the output layer. As mentioned
in (Mikolov, 2012), this is kind of maximum en-
tropy feature which can be derived by a two-layer
neural network. Some experiments show that per-
plexity significantly decreases after adding these
features. Following (Mikolov, 2012), where, the
maximum entropy bigram features are added to
our RNNSLM by a direct connection between the
feature input array and output layer ysj+1. Fol-
lowing (Mahoney, 2000), we map bigram maxi-
mum entropy features to a fixed-length array to re-
duce the memory complexity of direct connections
with feature hashing. Then the output layer can be
computed as follow:
</bodyText>
<equation confidence="0.997729">
ysj+1(t) = g(Vs(t) · hsj + � Dhash(w,t))
wExsj
(8)
</equation>
<bodyText confidence="0.999927142857143">
where (t) denotes the t-th row of a vector or a ma-
trix. D denotes that the hash array contains feature
weights and hash(wZ7 wj) denotes the hash func-
tion for mapping bigram features to a fixed-length
array. For a output ysj+1, multiple connections
may be activated according to the words in sen-
tence xsj.
</bodyText>
<subsectionHeader confidence="0.977861">
4.2 Training
</subsectionHeader>
<bodyText confidence="0.9988668">
The training objective of our RNNSLM is to find
the best parameters for predicting the words of
next sentence. Formally, given the next sentence
5k containing words w1, w2, w3, ..., wT. The
training objective according to (Mikolov et al.,
</bodyText>
<figure confidence="0.962369333333334">
hs;-1
hs;
S
WS
VS
YsJ+1
</figure>
<page confidence="0.851026">
902
</page>
<equation confidence="0.8752255">
2013) can be denoted by:
log(p(BoWSk|BoWS1, ...,
</equation>
<bodyText confidence="0.999857818181818">
For weight matrix Ws, Us, Vs and hash feature
weight D, the parameter are trained similar to the
conventional recurrent neural network. The learn-
ing rate a is set to 0.1 at the start of the training
as suggested in (Mikolov et al., 2010). After each
epoch, it can be determined by the training loss of
network. If the loss decreases significantly, train-
ing continues with the same learning rate. Other-
wise, if the loss increases, the training will be ex-
ecuted with a new learning rate a/2. The training
process will be terminated after about 30 epochs.
</bodyText>
<subsectionHeader confidence="0.969638">
4.3 Initialization
</subsectionHeader>
<bodyText confidence="0.9999587">
All elements in weight matrix Ws and Us are ini-
tialized by randomly sampling from a uniform dis-
tribution [− K11, K11 ], where K1 is the size of the
input layer. Elements in weight matrix Vs are ini-
tialized by randomly sampling from a uniform dis-
tribution [− K�1, K 1 ], where K2 denotes the size of
the hidden layer. The hash feature weight array D
is initialized as 0.
For the initialization of hs0, it can be set to a
vector of the same values, which is 0.1.
</bodyText>
<sectionHeader confidence="0.947785" genericHeader="method">
5 Hierarchical Recurrent Neural
Network
</sectionHeader>
<bodyText confidence="0.999984625">
In the previous section, we propose a RNNSLM
which models the coherence between sentences
but ignores the word sequence within a sentence.
Ideally, a perfect document model should not only
capture the information between sentences but al-
so the information with sentence. So we propose
a hierarchical recurrent neural network language
model (HRNNLM) to fulfill this issue.
</bodyText>
<subsectionHeader confidence="0.966716">
5.1 Model
</subsectionHeader>
<bodyText confidence="0.999858">
A hierarchical recurrent neural network consists of
two independent recurrent neural network. For a
conventional word-level language model, it pre-
dict the next word only using the word history
within the sentence. To capture the longer his-
tory, we integrate the sentence history into the
word-level language model from sentence-level
language model, which forms a hierarchical recur-
rent neural network.
</bodyText>
<figureCaption confidence="0.98073">
Figure 2: Hierarchical recurrent neural network
</figureCaption>
<bodyText confidence="0.99956080952381">
As illustrated Figure 2, the upper part is the un-
folded illustration of conventional recurrent neural
network based language model. It takes one word
wi each time with the previous history informa-
tion hwi−1 together and predicts the probability of
the next word p(wi+1) with the information kep-
t in the history vector hwi. The lower part is our
RNNSLM, which takes the bag of words represen-
tation of a sentence xsj each time with the history
information of previous sentences hsj−1 together
and predicts the bag of words in the next sentence
p(sj+1) with the information kept in hsj.
We integrate these two recurrent neural net-
works together by adding connections between the
sentence-level history vector hsj−1 and word level
history vector hwi. So while predicting the nex-
t word wi+1 of the current sentence, our model
will consider the current word wi, history of previ-
ous sentences hsj−1 and history of previous words
hwi−1. The new word-level history vector hwi is
computed as:
</bodyText>
<equation confidence="0.9979965">
hwi = f(Uw · hwi−1 + Ww · xwi + Usw · hsj−1)
(10)
</equation>
<bodyText confidence="0.998780666666667">
where f(z) is a HTanh function. For HRNNLM,
we also add a bigram hash feature, similar as we
do for RNNSLM.
</bodyText>
<subsectionHeader confidence="0.994212">
5.2 Training
</subsectionHeader>
<bodyText confidence="0.999449333333333">
The HRNNLM can be trained from scratch fol-
lowing Mikolov et al. (2010) with a dual objec-
tive. But this is not without problem. Beginning
</bodyText>
<figure confidence="0.982078333333333">
.#$ -.
!&amp;quot;#$
%(!&amp;quot;) &apos;!&amp;quot; %(!&amp;quot;&amp;$)&apos;!&amp;quot;&amp;$ %(!&amp;quot;&amp;,) &apos;!+
&apos;-. %(-/&amp;$)
!&amp;quot;
Sentence j Sentence j+1
!&amp;quot;&amp;* !+
.&amp;*
Document J
</figure>
<equation confidence="0.8249522">
logp(wt|BoWS1
1 T
T t=1
BoWSk−1))
, ..., BoWSk−1) (9)
</equation>
<page confidence="0.993656">
903
</page>
<bodyText confidence="0.999979894736842">
of training phase, the sentence history is unstable
since the parameters of sentence-level language
model are kept updating. Consequently, the train-
ing of HRNNLM will be also unstable and hard to
converge with unstable sentence history.
In this paper, we approximate the whole training
of HRNNLM by a two-step training method. We
first train a RNNSLM until it converges. Then we
connect the hidden layer of RNNSLM to the hid-
den layer of RNNWLM. To increase the training
speed, all the parameters of RNNSLM are fixed
while training HRNNLM. We only update the ran-
dom initialized parameters in HRNNLM, though
ideally the gradient of the sentence history vector
could change and the RNNSLM could be updated
again. The learning rate α is set to 0.1 and the up-
dating of learning rate is the same as suggested in
Section 4.2. All the parameters can be initialize as
suggested in Section 4.3.
</bodyText>
<sectionHeader confidence="0.999603" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99984075">
We evaluate the sentence-level performance of
HRNNLM by the common coherence evaluation
of sentence ordering task, its word-level perfor-
mance by perplexity measure. We also apply
our HRNNLM to SMT reranking task in an open
Chinese-English translation dataset. The trans-
lation performance index is the IBM version of
BLEU-4 (Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.998807">
6.1 Sentence Ordering
</subsectionHeader>
<bodyText confidence="0.99994278125">
We follow (Barzilay and Lapata, 2008) to evaluate
our sentence-level language model via a sentence
ordering task with test set 2010 (tst2010), 2011
(tst2011) and 2012 (tst2012) from IWSLT 2014,
totaling 37 English documents. 20 random permu-
tations of sentences for each document are gener-
ated. Each permutation and its original document
are combined as an article pair. Our goal is to find
the original one among all the article pairs.
The training data for sentence-level language
model is the 1,414 English documents from the
parallel corpus also provided by the IWSLT 2014
spoken language translation task. 90% of the doc-
uments are for training and the rest are reserved
for validation. The size of the hidden layer is set
to 30 and hash array size is 107.
We define the log probability of a given docu-
ment as its coherence score. The document with
the higher score is regarded as the original docu-
ment.
We provide two baselines for sentence order-
ing. One is the state-of-the-art recursive neural
network based method proposed by (Li and Hov-
y, 2014). We implement their model trained and
tested with our data. The other is a maximum en-
tropy classifier trained with bag of words features
of adjacent sentences which can generate a coher-
ent probability of adjacent sentences. The docu-
ment with the higher sum of log probability for
each adjacency sentences is regarded as the origi-
nal document. Table 1 shows the accuracy of our
system and baseline.
</bodyText>
<subsectionHeader confidence="0.794852">
Setting Accuracy
</subsectionHeader>
<tableCaption confidence="0.617115">
Table 1: Accuracy of the sentence ordering task
for each system
</tableCaption>
<bodyText confidence="0.999898">
From Table 1 we can see that the maximum
entropy model and the recursive neural network
model has almost the same performance. Com-
pared with the baseline systems, the proposed
HRNNLM achieves significant improvement with
nearly 4.3% improvement in term of accuracy.
The experimental result shows that the HRNNLM
can model document coherence and capture cross-
sentence information.
</bodyText>
<subsectionHeader confidence="0.995462">
6.2 Word-level Model Perplexity
</subsectionHeader>
<bodyText confidence="0.999997727272727">
We compare the word level performance of
HRNNLM with the most popular RNNLM in
terms of model perplexity. For a fair compari-
son, we follow (Mikolov et al., 2010) and train
the model also on 90% of the 1.414 English docu-
ments form IWSLT 2014, totaling about 3M word-
s. Then we train our model with the same hidden
layer size and hash array size as the baseline sys-
tem. The perplexity of these two models is eval-
uated on held-out documents, about 370K words.
The results are shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.691859">
Setting Perplexity
</subsectionHeader>
<equation confidence="0.537039">
RNNLM-30 183
HRNNLM-30 174
</equation>
<tableCaption confidence="0.97852625">
Table 2: Perplexity of the different language mod-
el
According to Table 2, it is reasonable to claim
that, by integrating history information of previous
</tableCaption>
<figure confidence="0.961650333333333">
Recursive
91.39%
ME system
Our system
91.89%
95.68%
</figure>
<page confidence="0.997221">
904
</page>
<bodyText confidence="0.999605">
sentences, the model perplexity decreased signif-
icantly. Empirically, this confirms the hypothesis
that the words selection for the next sentence is
dependent on its preceding sentences in the same
document.
</bodyText>
<subsectionHeader confidence="0.999728">
6.3 Spoken Language Translation
</subsectionHeader>
<bodyText confidence="0.999862">
The conventional SMT systems translate sen-
tences independently, without considering the co-
herence of the sentences in the same document. In
order to learn translation coherence between sen-
tences, we apply the HRNNLM to machine trans-
lation reranking task.
</bodyText>
<subsectionHeader confidence="0.950506">
6.3.1 Data Setting and Baselines
</subsectionHeader>
<bodyText confidence="0.999982523809524">
The data comes from the IWSLT 2014 spoken lan-
guage translation task. The training data consists
of 1,414 documents on TED talks, and contain-
s 179k sentence pairs, about 3M Chinese words,
and 3.3M English words. The language model for
SMT is a 4-gram language model trained with the
English documents in the training data. The devel-
opment set is specified by IWSLT as dev2010, and
the test set contains 37 documents from tst2010,
tst2011 and tst2012.
The IWSLT 2014 baseline system is built upon
the open-source machine translation toolkit Moses
at the default configuration, proposed by (Cetto-
lo et al., 2012). We also train a decoder, which
is an in-house Bracketing Transduction Grammar
(BTG) (Wu, 1997) in a CKY-style decoder with
a lexical reordering model trained with maximum
entropy (Xiong et al., 2006). The decoder uses
commonly used features, such as translation prob-
abilities, lexical weights, a language model, word
penalty, and distortion probabilities.
</bodyText>
<subsectionHeader confidence="0.976132">
6.3.2 Rerank System
</subsectionHeader>
<bodyText confidence="0.999959142857143">
Our reranking system is a linear model with sev-
eral features, including the SMT system final s-
cores, sentence-level language model scores, and
HRNNLM scores. It should be noted all these fea-
tures are actually employed by the SMT model ex-
cept for the HRNNLM score. Since Minimum Er-
ror Rate Training (MERT) (Och, 2003) is the most
general method adopted in SMT systems for tun-
ing, the feature weights are fixed by MERT.
For our reranking system, to score the transla-
tion of one sentence we need the translation re-
sults of all the previous sentences in the documen-
t. Our SMT decoder generates 10-best results of
all the sentences of the documents and the rerank-
ing system select the best translation result for the
first sentence at first. With the translation of first
sentence, we score all the translation candidates of
the second sentence and select the best one as the
result. Following this procedure, we can get the
translation results for all the sentences in the doc-
ument.
</bodyText>
<sectionHeader confidence="0.576614" genericHeader="evaluation">
6.3.3 Results
</sectionHeader>
<bodyText confidence="0.999634375">
The HRNNLM focus on exploiting longer context,
esp. cross-sentence word dependencies. Therefor
the translation data for IWSLT 2014 is organized
as documents instead of sentences for our rerank
system. We hope HRNNLM will enable a context-
sensitive reranking process, capturing the syntac-
tic and logic relationships between the sentences
in the same document.
</bodyText>
<table confidence="0.99825825">
Setting tst2010 tst2011 tst2012
IWSLT 11.12 13.34 -
Baseline 12.40 15.09 13.52
SMT + Rerank 12.55 15.23 13.70
</table>
<tableCaption confidence="0.995111">
Table 3: BLEU scores of SMT systems. The I-
</tableCaption>
<bodyText confidence="0.995308111111111">
WSLT is a public baseline which issued by the or-
ganizer of IWSLT 2014, as described in (Cettolo
et al., 2012).
The translation performance comparison is
shown in Table 3. From Table 3, we can find
that the rerank system improves SMT performance
consistently. For a single sentence without the
context information, there are several appropriate
translations and it is hard to tell which one is bet-
ter. When considering the context of a document
(previous sentences for our model), some transla-
tion candidates may not be coherent with the oth-
ers which should not be selected. Our model can
generate the most coherent translation results by
considering previous sentence history.
For example, we have the following two Chi-
nese sentence in one document together with their
correct translation:
</bodyText>
<equation confidence="0.9668335">
我 拍摄 过 的 冰山, 有些 冰 是 非常
轻 - - 几千 年 年龄
</equation>
<bodyText confidence="0.997880666666667">
Some of the ice in the icebergs that I pho-
tograph is very young - - a couple thou-
sand years old.
</bodyText>
<sectionHeader confidence="0.236622" genericHeader="conclusions">
AA ViK &amp;d +&apos;) &apos;If
</sectionHeader>
<bodyText confidence="0.5875445">
And some of the ice is over 100,000 years
old.
</bodyText>
<page confidence="0.995782">
905
</page>
<bodyText confidence="0.993001444444444">
Chinese word “ 有些” means “some” in En-
glish. But when it is used in parallelism sentences,
it means “some of” instead of “some”. The tradi-
tional SMT system translates the italics part with-
out considering the context. The translation result
for this kind of system is:
Some ice more than 100,000 years.
For our system, the HRNNLM can take previ-
ous sentence as context and learn the parallelism
between the two sentences. It can select the best
translation “some of” for有些, and the output of
our system is:
Some of the ice more than 100,000 years.
We also calculate the BLEU increase ratio of
our system on document level. The ratio is de-
fined as N#(BleuDrerank &gt; BleuDbaseline),
where N denotes the number of documents, and
#(BleuDrerank &gt; BleuDbaseline) denotes the
number of documents for which document level
BLEU score of reranking system is higher than the
baselines. The results are shown in Table 4.
In the future, we will explore better sentence
representation such as distributed sentence repre-
sentation as input for our sentence-level language
model to better model document coherence. We
can even update the gradient from different RNN
to get a better performance.
</bodyText>
<sectionHeader confidence="0.996881" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999955666666667">
We are grateful to the three anonymous reviewers
for their helpful comments and suggestions. We
also thank Dongdong Zhang, Lei Cui for useful
discussions. This paper is supported by the project
of National Natural Science Foundation of China
(Grant No. 61272384, 61370170 &amp;61402134).
</bodyText>
<sectionHeader confidence="0.996895" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.904456">
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.
</reference>
<table confidence="0.790303142857143">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
tst2010 tst2011 tst2012
71.43%
72.73% 75%
</table>
<tableCaption confidence="0.925611">
Table 4: Experimental results to test BLEU in-
</tableCaption>
<bodyText confidence="0.954382">
crease ratio after reranking
From Table 4, we can find that, for all the three
test data sets, our reranking system can achieve
better performance for more than 70% documents.
</bodyText>
<sectionHeader confidence="0.990002" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.996593263157895">
In this paper, we propose a hierarchical recurren-
t neural network language model for document
modeling. We first built a RNNSLM to capture
the information between sentences. Then we in-
tegrate the hidden layer of RNNSLM into the in-
put layer of word-level language model to form
a hierarchical recurrent neural network. This en-
ables the model be able to capture both in-sentence
and cross-sentence information in a unified RN-
N. Compared with conventional language models,
our model can perceive a longer history than other
language models and captures the context patterns
in the previous sentences. At sentence level, we
examine our model with sentence ordering task.
At word level, we test the model perplexity. We
also conduct a SMT rerank experiment on IWSLT
2014 data set. All these experimental results show
that our hierarchical recurrent neural network has
a satisfying performance.
</bodyText>
<reference confidence="0.973329821428571">
Mauro Cettolo, Christian Girardi, and Marcello Federi-
co. 2012. Wit3: Web inventory of transcribed and
translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 261–268, Trento, Italy,
May.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. pages 1724–
1734, October.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
Audio, Speech, and Language Processing, IEEE
Transactions on, 20(1):30–42.
Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh.
2006. A fast learning algorithm for deep belief nets.
Neural computation, 18(7):1527–1554.
Michael I Jordan. 1997. Serial order: A parallel dis-
tributed processing approach. Advances in psychol-
ogy, 121:471–495.
</reference>
<page confidence="0.987565">
906
</page>
<reference confidence="0.999879666666667">
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Micha¨el Mathieu, and Yann L Cun.
2010. Learning convolutional feature hierarchies for
visual recognition. In Advances in neural informa-
tion processing systems, pages 1090–1098.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. pages 1188–
1196.
Jiwei Li and Eduard Hovy. 2014. A model of co-
herence based on distributed sentence representa-
tion. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 2039–2048. Association for Com-
putational Linguistics.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.
A recursive recurrent neural network for statistical
machine translation. In Proceedings of ACL, pages
1491–1500.
Matthew V Mahoney. 2000. Fast text compression
with neural networks. In FLAIRS Conference, pages
230–234.
Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In Spoken Language Technology Workshop, IEEE,
pages 234 – 239.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tom´aˇs Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Ph. D. the-
sis, Brno University of Technology.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentimen-
t treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP), volume 1631, page 1642. Citeseer.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017–1024.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proceedings of
the 21st International Conference on Computation-
al Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 521–
528. Association for Computational Linguistics.
Puyang Xu and Ruhi Sarikaya. 2014. Contextual do-
main classification in spoken language understand-
ing systems using recurrent neural network. In A-
coustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pages 136–
140. IEEE.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai
Yu. 2013. Word alignment modeling with contex-
t dependent deep neural network. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Paper-
s), pages 166–175, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.997532">
907
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.512415">
<title confidence="0.7592795">Hierarchical Recurrent Neural Network for Document Modeling Shujie Muyun Mu Ming Sheng Institute of</title>
<abstract confidence="0.999442565217391">This paper proposes a novel hierarchical recurrent neural network language model (HRNNLM) for document modeling. After establishing a RNN to capture the coherence between sentences in a document, HRNNLM integrates it as the sentence history information into the word level RNN to predict the word sequence with cross-sentence contextual information. A two-step training approach is designed, in which sentence-level and word-level language models are approximated for the convergence in a pipeline style. Examined by the standard sentence reordering scenario, HRNNLM is proved for its better accuracy in modeling the sentence coherence. And at the word level, experimental results also indicate a significant lower model perplexity, followed by a practical better translation result when applied to a Chinese-English document translation reranking task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="20362" citStr="Barzilay and Lapata, 2008" startWordPosition="3324" endWordPosition="3327">NNSLM could be updated again. The learning rate α is set to 0.1 and the updating of learning rate is the same as suggested in Section 4.2. All the parameters can be initialize as suggested in Section 4.3. 6 Experiments We evaluate the sentence-level performance of HRNNLM by the common coherence evaluation of sentence ordering task, its word-level performance by perplexity measure. We also apply our HRNNLM to SMT reranking task in an open Chinese-English translation dataset. The translation performance index is the IBM version of BLEU-4 (Papineni et al., 2002). 6.1 Sentence Ordering We follow (Barzilay and Lapata, 2008) to evaluate our sentence-level language model via a sentence ordering task with test set 2010 (tst2010), 2011 (tst2011) and 2012 (tst2012) from IWSLT 2014, totaling 37 English documents. 20 random permutations of sentences for each document are generated. Each permutation and its original document are combined as an article pair. Our goal is to find the original one among all the article pairs. The training data for sentence-level language model is the 1,414 English documents from the parallel corpus also provided by the IWSLT 2014 spoken language translation task. 90% of the documents are fo</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>Wit3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>261--268</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="24151" citStr="Cettolo et al., 2012" startWordPosition="3951" endWordPosition="3955">d Baselines The data comes from the IWSLT 2014 spoken language translation task. The training data consists of 1,414 documents on TED talks, and contains 179k sentence pairs, about 3M Chinese words, and 3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. 6.3.2 Rerank System Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores. It should be noted all these features are actually employed by t</context>
<context position="26141" citStr="Cettolo et al., 2012" startWordPosition="4282" endWordPosition="4285">s The HRNNLM focus on exploiting longer context, esp. cross-sentence word dependencies. Therefor the translation data for IWSLT 2014 is organized as documents instead of sentences for our rerank system. We hope HRNNLM will enable a contextsensitive reranking process, capturing the syntactic and logic relationships between the sentences in the same document. Setting tst2010 tst2011 tst2012 IWSLT 11.12 13.34 - Baseline 12.40 15.09 13.52 SMT + Rerank 12.55 15.23 13.70 Table 3: BLEU scores of SMT systems. The IWSLT is a public baseline which issued by the organizer of IWSLT 2014, as described in (Cettolo et al., 2012). The translation performance comparison is shown in Table 3. From Table 3, we can find that the rerank system improves SMT performance consistently. For a single sentence without the context information, there are several appropriate translations and it is hard to tell which one is better. When considering the context of a document (previous sentences for our model), some translation candidates may not be coherent with the others which should not be selected. Our model can generate the most coherent translation results by considering previous sentence history. For example, we have the followi</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT), pages 261–268, Trento, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<pages>1724--1734</pages>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. pages 1724– 1734, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1644" citStr="Collobert et al. (2011)" startWordPosition="235" endWordPosition="238">practical better translation result when applied to a Chinese-English document translation reranking task. 1 Introduction Deep Neural Network (DNN), a neural network with multiple layers, has been proven powerful in many different domains, such as visual recognition (Kavukcuoglu et al., 2010) and speech recognition (Dahl et al., 2012), ever since Hinton et al. (2006) formulated an efficient training method for it. In addition to the applications mentioned above, many neural network based methods have also been applied to natural language processing (NLP) tasks with great success. For example, Collobert et al. (2011) propose a generalized DNN framework for a variety of fundamental NLP tasks, including part-of-speech tagging (postag), chunking, named *Contribution during internship at Microsoft Research. entity recognition (NER), and semantic role labeling. DNN is successfully introduced to do wordlevel language modeling, aka., to predict the next word given the history words. Bengio et al. (2003) propose a feedforward neural network to train a word-level language model with a limited n-gram history. To leverage as much history as possible, Mikolov et al. (2010) apply recurrent neural network to word-level</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Dong Yu</author>
<author>Li Deng</author>
<author>Alex Acero</author>
</authors>
<title>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing,</title>
<date>2012</date>
<journal>IEEE Transactions on,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1357" citStr="Dahl et al., 2012" startWordPosition="191" endWordPosition="194">or the convergence in a pipeline style. Examined by the standard sentence reordering scenario, HRNNLM is proved for its better accuracy in modeling the sentence coherence. And at the word level, experimental results also indicate a significant lower model perplexity, followed by a practical better translation result when applied to a Chinese-English document translation reranking task. 1 Introduction Deep Neural Network (DNN), a neural network with multiple layers, has been proven powerful in many different domains, such as visual recognition (Kavukcuoglu et al., 2010) and speech recognition (Dahl et al., 2012), ever since Hinton et al. (2006) formulated an efficient training method for it. In addition to the applications mentioned above, many neural network based methods have also been applied to natural language processing (NLP) tasks with great success. For example, Collobert et al. (2011) propose a generalized DNN framework for a variety of fundamental NLP tasks, including part-of-speech tagging (postag), chunking, named *Contribution during internship at Microsoft Research. entity recognition (NER), and semantic role labeling. DNN is successfully introduced to do wordlevel language modeling, ak</context>
<context position="9365" citStr="Dahl et al., 2012" startWordPosition="1468" endWordPosition="1471">etween sentences. Following the thread modeling the word sequence relationship within and across sentences, we propose a hierarchical recurrent neural network language model consist of a sentence-level language model and a word-level language model. This overall network is trained to capture the coherence between sentences and predict words sequence with preceding sentence contexts. For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) model to the HMM-based word alignment model. In their method, they use bilingual word embedding to capture the lexical translation information and modeling the context with surrounding words. Liu et al. (2014) propose a recursive recurrent neural network (R2NN) for end-to-end decoding to help improve translation quality. And Cho et al. (2014) propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does. However, at the discourse level, there is little report on applying DNN to boost the translation result of a document. 3</context>
</contexts>
<marker>Dahl, Yu, Deng, Acero, 2012</marker>
<rawString>George E Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whye Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets.</title>
<date>2006</date>
<booktitle>Neural computation,</booktitle>
<pages>18--7</pages>
<contexts>
<context position="1390" citStr="Hinton et al. (2006)" startWordPosition="197" endWordPosition="200">e style. Examined by the standard sentence reordering scenario, HRNNLM is proved for its better accuracy in modeling the sentence coherence. And at the word level, experimental results also indicate a significant lower model perplexity, followed by a practical better translation result when applied to a Chinese-English document translation reranking task. 1 Introduction Deep Neural Network (DNN), a neural network with multiple layers, has been proven powerful in many different domains, such as visual recognition (Kavukcuoglu et al., 2010) and speech recognition (Dahl et al., 2012), ever since Hinton et al. (2006) formulated an efficient training method for it. In addition to the applications mentioned above, many neural network based methods have also been applied to natural language processing (NLP) tasks with great success. For example, Collobert et al. (2011) propose a generalized DNN framework for a variety of fundamental NLP tasks, including part-of-speech tagging (postag), chunking, named *Contribution during internship at Microsoft Research. entity recognition (NER), and semantic role labeling. DNN is successfully introduced to do wordlevel language modeling, aka., to predict the next word give</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Jordan</author>
</authors>
<title>Serial order: A parallel distributed processing approach.</title>
<date>1997</date>
<booktitle>Advances in psychology,</booktitle>
<pages>121--471</pages>
<contexts>
<context position="8315" citStr="Jordan, 1997" startWordPosition="1310" endWordPosition="1311"> assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence. An attempt of introducing RNN into convolutional neural network (CNN) is investigated by (Xu and Sarikaya, 2014) for spoken language understanding (SLU). To alleviate more contextual information, they apply a CNN with Jordan-type (Jordan, 1997) recurrent connections. The recurrent connections send the distribution of the last softmax layer’s output to the current input layer as additional features. Aimed to improve SLU domain classification, their model is essentially a kind of document representation with certain text 900 For the sentence Sk containing words w1, w2, w3, ..., wT, p(Sk|S1, S2,..., Sk−1) is defined as: information, neglecting the coherence information between sentences. Following the thread modeling the word sequence relationship within and across sentences, we propose a hierarchical recurrent neural network language </context>
</contexts>
<marker>Jordan, 1997</marker>
<rawString>Michael I Jordan. 1997. Serial order: A parallel distributed processing approach. Advances in psychology, 121:471–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koray Kavukcuoglu</author>
<author>Pierre Sermanet</author>
<author>Y-Lan Boureau</author>
<author>Karol Gregor</author>
<author>Micha¨el Mathieu</author>
<author>Yann L Cun</author>
</authors>
<title>Learning convolutional feature hierarchies for visual recognition. In Advances in neural information processing systems,</title>
<date>2010</date>
<pages>1090--1098</pages>
<marker>Kavukcuoglu, Sermanet, Boureau, Gregor, Micha¨el Mathieu, Cun, 2010</marker>
<rawString>Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Micha¨el Mathieu, and Yann L Cun. 2010. Learning convolutional feature hierarchies for visual recognition. In Advances in neural information processing systems, pages 1090–1098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<pages>1188--1196</pages>
<contexts>
<context position="2744" citStr="Mikolov (2014)" startWordPosition="412" endWordPosition="413">story. To leverage as much history as possible, Mikolov et al. (2010) apply recurrent neural network to word-level language modeling. The model absorbs one word each time, keeps the information in a history vector, and predicts the next word with all the word history in the vector. Word-level language model can only learn the relationship between words in one sentence. For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences. To model this kind of coherence of sentences, Le and Mikolov (2014) extend word embedding learning network (Mikolov et al., 2013) to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence. Li and Hovy (2014) propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not. In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentenc</context>
<context position="6593" citStr="Mikolov (2014)" startWordPosition="1022" endWordPosition="1023"> Zweig (2012) propose a RNNLDA model to implement a context dependent language model. They augment the contextual information into the conventional RNNLM via a realvalued input vector, which is the probability distribution computed by LDA topics for using a block of preceding text. They train a Latent Dirichlet Allocation (LDA) model using documents consisting of about 10 sentences long text from Penn Treebank (PTB) training data. Their approach outperforms RNNLM in perplexity on PTB data with a limited context history over topics instead of complete information of preceding sentences. Le and Mikolov (2014) extend the Continuous Bag-of-Words Model (CBOW) and Continuous Skip-gram Model (Skip-gram) (Mikolov et al., 2013) by introducing a paragraph vector. In their method, the paragraph vector is learnt in a similar way of word vector model, and there will be N x P parameters, if there are N paragraphs and each paragraph is mapped to P dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. pages 1188– 1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Eduard Hovy</author>
</authors>
<title>A model of coherence based on distributed sentence representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>2039--2048</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2925" citStr="Li and Hovy (2014)" startWordPosition="438" endWordPosition="441"> the information in a history vector, and predicts the next word with all the word history in the vector. Word-level language model can only learn the relationship between words in one sentence. For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences. To model this kind of coherence of sentences, Le and Mikolov (2014) extend word embedding learning network (Mikolov et al., 2013) to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence. Li and Hovy (2014) propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not. In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level. HRNNLM is essentially a combination of a wordlevel language model and a sentence-level language model, both of which are recurrent neural network</context>
<context position="7377" citStr="Li and Hovy (2014)" startWordPosition="1162" endWordPosition="1165"> the paragraph vector is learnt in a similar way of word vector model, and there will be N x P parameters, if there are N paragraphs and each paragraph is mapped to P dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but only based on the words in the sentence. And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot. Li and Hovy (2014) implement a neural network model to predict discourse coherence quality in essays. In their work, recurrent (Sutskever et al., 2011) and recursive (Socher et al., 2013) neural networks are both examined to learn distributed sentence representation given pre-trained word embedding. The distributed sentence representation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of se</context>
<context position="21377" citStr="Li and Hovy, 2014" startWordPosition="3496" endWordPosition="3500">. The training data for sentence-level language model is the 1,414 English documents from the parallel corpus also provided by the IWSLT 2014 spoken language translation task. 90% of the documents are for training and the rest are reserved for validation. The size of the hidden layer is set to 30 and hash array size is 107. We define the log probability of a given document as its coherence score. The document with the higher score is regarded as the original document. We provide two baselines for sentence ordering. One is the state-of-the-art recursive neural network based method proposed by (Li and Hovy, 2014). We implement their model trained and tested with our data. The other is a maximum entropy classifier trained with bag of words features of adjacent sentences which can generate a coherent probability of adjacent sentences. The document with the higher sum of log probability for each adjacency sentences is regarded as the original document. Table 1 shows the accuracy of our system and baseline. Setting Accuracy Table 1: Accuracy of the sentence ordering task for each system From Table 1 we can see that the maximum entropy model and the recursive neural network model has almost the same perfor</context>
</contexts>
<marker>Li, Hovy, 2014</marker>
<rawString>Jiwei Li and Eduard Hovy. 2014. A model of coherence based on distributed sentence representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1491--1500</pages>
<contexts>
<context position="9575" citStr="Liu et al. (2014)" startWordPosition="1502" endWordPosition="1505">model and a word-level language model. This overall network is trained to capture the coherence between sentences and predict words sequence with preceding sentence contexts. For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) model to the HMM-based word alignment model. In their method, they use bilingual word embedding to capture the lexical translation information and modeling the context with surrounding words. Liu et al. (2014) propose a recursive recurrent neural network (R2NN) for end-to-end decoding to help improve translation quality. And Cho et al. (2014) propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does. However, at the discourse level, there is little report on applying DNN to boost the translation result of a document. 3 Document Language Modeling Statistical language model assigns a probability to a natural language sequence. Conventional language models only focus on the word sequence within a sentence. For sentences in one </context>
</contexts>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proceedings of ACL, pages 1491–1500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew V Mahoney</author>
</authors>
<title>Fast text compression with neural networks.</title>
<date>2000</date>
<booktitle>In FLAIRS Conference,</booktitle>
<pages>230--234</pages>
<contexts>
<context position="14817" citStr="Mahoney, 2000" startWordPosition="2379" endWordPosition="2380">tence xsj and previous history hsj_1, where V denotes vocabulary size. To emphasize coherence between the adjacent sentences, we further add some bigram-like bag of words feature to the output layer. As mentioned in (Mikolov, 2012), this is kind of maximum entropy feature which can be derived by a two-layer neural network. Some experiments show that perplexity significantly decreases after adding these features. Following (Mikolov, 2012), where, the maximum entropy bigram features are added to our RNNSLM by a direct connection between the feature input array and output layer ysj+1. Following (Mahoney, 2000), we map bigram maximum entropy features to a fixed-length array to reduce the memory complexity of direct connections with feature hashing. Then the output layer can be computed as follow: ysj+1(t) = g(Vs(t) · hsj + � Dhash(w,t)) wExsj (8) where (t) denotes the t-th row of a vector or a matrix. D denotes that the hash array contains feature weights and hash(wZ7 wj) denotes the hash function for mapping bigram features to a fixed-length array. For a output ysj+1, multiple connections may be activated according to the words in sentence xsj. 4.2 Training The training objective of our RNNSLM is t</context>
</contexts>
<marker>Mahoney, 2000</marker>
<rawString>Matthew V Mahoney. 2000. Fast text compression with neural networks. In FLAIRS Conference, pages 230–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Context dependent recurrent neural network language model.</title>
<date>2012</date>
<booktitle>In Spoken Language Technology Workshop, IEEE,</booktitle>
<pages>234--239</pages>
<contexts>
<context position="5992" citStr="Mikolov and Zweig (2012)" startWordPosition="920" endWordPosition="923">oduces work related to applying neural network to document modeling and SMT. Section 3 introduces the general framework for document modeling. Our sentence-level language model and its training is described in Section 4, and the overall HRNNLM and its training is presented in Section 5. Section 6 presents our experiments and their results. Finally, we conclude in Section 7. 2 Related work In this section, we introduce previous efforts on applying neural network to model words coherence across sentence boundaries as well as works on improving machine translation performance at discourse level. Mikolov and Zweig (2012) propose a RNNLDA model to implement a context dependent language model. They augment the contextual information into the conventional RNNLM via a realvalued input vector, which is the probability distribution computed by LDA topics for using a block of preceding text. They train a Latent Dirichlet Allocation (LDA) model using documents consisting of about 10 sentences long text from Penn Treebank (PTB) training data. Their approach outperforms RNNLM in perplexity on PTB data with a limited context history over topics instead of complete information of preceding sentences. Le and Mikolov (2014</context>
</contexts>
<marker>Mikolov, Zweig, 2012</marker>
<rawString>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In Spoken Language Technology Workshop, IEEE, pages 234 – 239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="2806" citStr="Mikolov et al., 2013" startWordPosition="420" endWordPosition="423"> et al. (2010) apply recurrent neural network to word-level language modeling. The model absorbs one word each time, keeps the information in a history vector, and predicts the next word with all the word history in the vector. Word-level language model can only learn the relationship between words in one sentence. For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences. To model this kind of coherence of sentences, Le and Mikolov (2014) extend word embedding learning network (Mikolov et al., 2013) to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence. Li and Hovy (2014) propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not. In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level. HRNNLM is essentially a co</context>
<context position="6707" citStr="Mikolov et al., 2013" startWordPosition="1037" endWordPosition="1040">extual information into the conventional RNNLM via a realvalued input vector, which is the probability distribution computed by LDA topics for using a block of preceding text. They train a Latent Dirichlet Allocation (LDA) model using documents consisting of about 10 sentences long text from Penn Treebank (PTB) training data. Their approach outperforms RNNLM in perplexity on PTB data with a limited context history over topics instead of complete information of preceding sentences. Le and Mikolov (2014) extend the Continuous Bag-of-Words Model (CBOW) and Continuous Skip-gram Model (Skip-gram) (Mikolov et al., 2013) by introducing a paragraph vector. In their method, the paragraph vector is learnt in a similar way of word vector model, and there will be N x P parameters, if there are N paragraphs and each paragraph is mapped to P dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but only based on the words in the sentence. And our sentence vector also integrates nearly all the history informati</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
</authors>
<title>Statistical language models based on neural networks.</title>
<date>2012</date>
<tech>Ph.D. thesis, Ph. D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="14434" citStr="Mikolov, 2012" startWordPosition="2319" endWordPosition="2320">uted as follows: hsj = f(Us · hsj_1 + Ws · xsj) (4) ysj+1 = g(Vs · hsj) (5) where Ws, Us and Vs denote the weight matrix. f(z) is a HTanh function: f(zj) = { −1 zj &lt; −1 z −1 &lt; zj &lt; 1 1 zj &gt; 1 (6) and g(z) is a softmax function: ezj g(zj) = E (7) k ezk The output layer ysj+1 is a 1xV vector that represents probability distribution of words in the next sentence given the current sentence xsj and previous history hsj_1, where V denotes vocabulary size. To emphasize coherence between the adjacent sentences, we further add some bigram-like bag of words feature to the output layer. As mentioned in (Mikolov, 2012), this is kind of maximum entropy feature which can be derived by a two-layer neural network. Some experiments show that perplexity significantly decreases after adding these features. Following (Mikolov, 2012), where, the maximum entropy bigram features are added to our RNNSLM by a direct connection between the feature input array and output layer ysj+1. Following (Mahoney, 2000), we map bigram maximum entropy features to a fixed-length array to reduce the memory complexity of direct connections with feature hashing. Then the output layer can be computed as follow: ysj+1(t) = g(Vs(t) · hsj + </context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´aˇs Mikolov. 2012. Statistical language models based on neural networks. Ph.D. thesis, Ph. D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24845" citStr="Och, 2003" startWordPosition="4065" endWordPosition="4066">) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. 6.3.2 Rerank System Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores. It should be noted all these features are actually employed by the SMT model except for the HRNNLM score. Since Minimum Error Rate Training (MERT) (Och, 2003) is the most general method adopted in SMT systems for tuning, the feature weights are fixed by MERT. For our reranking system, to score the translation of one sentence we need the translation results of all the previous sentences in the document. Our SMT decoder generates 10-best results of all the sentences of the documents and the reranking system select the best translation result for the first sentence at first. With the translation of first sentence, we score all the translation candidates of the second sentence and select the best one as the result. Following this procedure, we can get </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20301" citStr="Papineni et al., 2002" startWordPosition="3315" endWordPosition="3318">ent of the sentence history vector could change and the RNNSLM could be updated again. The learning rate α is set to 0.1 and the updating of learning rate is the same as suggested in Section 4.2. All the parameters can be initialize as suggested in Section 4.3. 6 Experiments We evaluate the sentence-level performance of HRNNLM by the common coherence evaluation of sentence ordering task, its word-level performance by perplexity measure. We also apply our HRNNLM to SMT reranking task in an open Chinese-English translation dataset. The translation performance index is the IBM version of BLEU-4 (Papineni et al., 2002). 6.1 Sentence Ordering We follow (Barzilay and Lapata, 2008) to evaluate our sentence-level language model via a sentence ordering task with test set 2010 (tst2010), 2011 (tst2011) and 2012 (tst2012) from IWSLT 2014, totaling 37 English documents. 20 random permutations of sentences for each document are generated. Each permutation and its original document are combined as an article pair. Our goal is to find the original one among all the article pairs. The training data for sentence-level language model is the 1,414 English documents from the parallel corpus also provided by the IWSLT 2014 </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing (EMNLP),</booktitle>
<volume>1631</volume>
<pages>1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="7546" citStr="Socher et al., 2013" startWordPosition="1190" endWordPosition="1193">dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but only based on the words in the sentence. And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot. Li and Hovy (2014) implement a neural network model to predict discourse coherence quality in essays. In their work, recurrent (Sutskever et al., 2011) and recursive (Socher et al., 2013) neural networks are both examined to learn distributed sentence representation given pre-trained word embedding. The distributed sentence representation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence. An attempt of introducing RNN into convolutional neural network (CNN) is in</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>1017--1024</pages>
<contexts>
<context position="7510" citStr="Sutskever et al., 2011" startWordPosition="1184" endWordPosition="1187">aphs and each paragraph is mapped to P dimensions. Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input. The sentence vector is no longer related with the sentence id, but only based on the words in the sentence. And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot. Li and Hovy (2014) implement a neural network model to predict discourse coherence quality in essays. In their work, recurrent (Sutskever et al., 2011) and recursive (Socher et al., 2013) neural networks are both examined to learn distributed sentence representation given pre-trained word embedding. The distributed sentence representation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence. An attempt of introducing RNN into conv</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="24247" citStr="Wu, 1997" startWordPosition="3969" endWordPosition="3970">of 1,414 documents on TED talks, and contains 179k sentence pairs, about 3M Chinese words, and 3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. 6.3.2 Rerank System Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores. It should be noted all these features are actually employed by the SMT model except for the HRNNLM score. Since Minimum Error Rate Training (MERT) (Och, 2003) i</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24352" citStr="Xiong et al., 2006" startWordPosition="3984" endWordPosition="3987">3.3M English words. The language model for SMT is a 4-gram language model trained with the English documents in the training data. The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012. The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012). We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006). The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities. 6.3.2 Rerank System Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores. It should be noted all these features are actually employed by the SMT model except for the HRNNLM score. Since Minimum Error Rate Training (MERT) (Och, 2003) is the most general method adopted in SMT systems for tuning, the feature weights are fixed by MERT. For o</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 521– 528. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Puyang Xu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Contextual domain classification in spoken language understanding systems using recurrent neural network.</title>
<date>2014</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on,</booktitle>
<pages>136--140</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="8183" citStr="Xu and Sarikaya, 2014" startWordPosition="1288" endWordPosition="1292">s are both examined to learn distributed sentence representation given pre-trained word embedding. The distributed sentence representation is assigned to capture both syntactic and semantic information. With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text. Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence. An attempt of introducing RNN into convolutional neural network (CNN) is investigated by (Xu and Sarikaya, 2014) for spoken language understanding (SLU). To alleviate more contextual information, they apply a CNN with Jordan-type (Jordan, 1997) recurrent connections. The recurrent connections send the distribution of the last softmax layer’s output to the current input layer as additional features. Aimed to improve SLU domain classification, their model is essentially a kind of document representation with certain text 900 For the sentence Sk containing words w1, w2, w3, ..., wT, p(Sk|S1, S2,..., Sk−1) is defined as: information, neglecting the coherence information between sentences. Following the thre</context>
</contexts>
<marker>Xu, Sarikaya, 2014</marker>
<rawString>Puyang Xu and Ruhi Sarikaya. 2014. Contextual domain classification in spoken language understanding systems using recurrent neural network. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 136– 140. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Nenghai Yu</author>
</authors>
<title>Word alignment modeling with context dependent deep neural network.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>166--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="9313" citStr="Yang et al. (2013)" startWordPosition="1459" endWordPosition="1462"> information, neglecting the coherence information between sentences. Following the thread modeling the word sequence relationship within and across sentences, we propose a hierarchical recurrent neural network language model consist of a sentence-level language model and a word-level language model. This overall network is trained to capture the coherence between sentences and predict words sequence with preceding sentence contexts. For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components. Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) model to the HMM-based word alignment model. In their method, they use bilingual word embedding to capture the lexical translation information and modeling the context with surrounding words. Liu et al. (2014) propose a recursive recurrent neural network (R2NN) for end-to-end decoding to help improve translation quality. And Cho et al. (2014) propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does. However, at the discourse level, there is little report on applying </context>
</contexts>
<marker>Yang, Liu, Li, Zhou, Yu, 2013</marker>
<rawString>Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. 2013. Word alignment modeling with context dependent deep neural network. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 166–175, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
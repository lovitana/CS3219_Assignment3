<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.998136">
Classifying Relations via Long Short Term Memory Networks
along Shortest Dependency Paths
</title>
<author confidence="0.995788">
Yan Xu,† Lili Mou,† Ge Li,†∗ Yunchuan Chen,‡ Hao Peng,† Zhi Jin†∗
</author>
<affiliation confidence="0.970629">
†Software Institute, Peking University, 100871, P. R. China
</affiliation>
<email confidence="0.90206">
{xuyan14,lige,zhijin}@sei.pku.edu.cn,{doublepower.mou,penghao.pku}@gmail.com
</email>
<affiliation confidence="0.655126">
‡University of Chinese Academy of Sciences, chenyunchuan11@mails.ucas.ac.cn
</affiliation>
<sectionHeader confidence="0.980435" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99975752">
Relation classification is an important re-
search arena in the field of natural lan-
guage processing (NLP). In this paper, we
present SDP-LSTM, a novel neural net-
work to classify the relation of two enti-
ties in a sentence. Our neural architecture
leverages the shortest dependency path
(SDP) between two entities; multichan-
nel recurrent neural networks, with long
short term memory (LSTM) units, pick
up heterogeneous information along the
SDP. Our proposed model has several dis-
tinct features: (1) The shortest dependency
paths retain most relevant information (to
relation classification), while eliminating
irrelevant words in the sentence. (2) The
multichannel LSTM networks allow ef-
fective information integration from het-
erogeneous sources over the dependency
paths. (3) A customized dropout strategy
regularizes the neural network to allevi-
ate overfitting. We test our model on the
SemEval 2010 relation classification task,
and achieve an F1-score of 83.7%, higher
than competing methods in the literature.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990800394736842">
Relation classification is an important NLP task.
It plays a key role in various scenarios, e.g., in-
formation extraction (Wu and Weld, 2010), ques-
tion answering (Yao and Van Durme, 2014), med-
ical informatics (Wang and Fan, 2014), ontol-
ogy learning (Xu et al., 2014), etc. The aim
of relation classification is to categorize into pre-
defined classes the relations between pairs of
marked entities in given texts. For instance, in
the sentence “A trillion gallons of [water],, have
been poured into an empty [region],, of outer
∗Corresponding authors.
space,” the entities water and region are of rela-
tion Entity-Destination(e1, e2).
Traditional relation classification approaches
rely largely on feature representation (Kambhatla,
2004), or kernel design (Zelenko et al., 2003;
Bunescu and Mooney, 2005). The former method
usually incorporates a large set of features; it is
difficult to improve the model performance if the
feature set is not very well chosen. The latter ap-
proach, on the other hand, depends largely on the
designed kernel, which summarizes all data infor-
mation. Deep neural networks, emerging recently,
provide a way of highly automatic feature learning
(Bengio et al., 2013), and have exhibited consid-
erable potential (Zeng et al., 2014; dos Santos et
al., 2015). However, human engineering—that is,
incorporating human knowledge to the network’s
architecture—is still important and beneficial.
This paper proposes a new neural network,
SDP-LSTM, for relation classification. Our model
utilizes the shortest dependency path (SDP) be-
tween two entities in a sentence; we also design a
long short term memory (LSTM)-based recurrent
neural network for information processing. The
neural architecture is mainly inspired by the fol-
lowing observations.
</bodyText>
<listItem confidence="0.565769">
• Shortest dependency paths are informative
(Fundel et al., 2007; Chen et al., 2014). To
determine the two entities’ relation, we find it
mostly sufficient to use only the words along
the SDP: they concentrate on most relevant
</listItem>
<bodyText confidence="0.9123023">
information while diminishing less relevant
noise. Figure 1 depicts the dependency parse
tree of the aforementioned sentence. Words
along the SDP form a trimmed phrase (gal-
lons of water poured into region) of the orig-
inal sentence, which conveys much informa-
tion about the target relation. Other words,
such as a, trillion, outer space, are less infor-
mative and may bring noise if not dealt with
properly.
</bodyText>
<page confidence="0.925692">
1785
</page>
<note confidence="0.9811905">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1785–1794,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<listItem confidence="0.7792345">
• Direction matters. Dependency trees are a
kind of directed graph. The dependency re-
</listItem>
<bodyText confidence="0.965130295454545">
lation between into and region is PREP; such
relation hardly makes any sense if the di-
rected edge is reversed. Moreover, the enti-
ties’ relation distinguishes its directionality,
that is, r(a, b) differs from r(b, a), for a same
given relation r and two entities a, b. There-
fore, we think it necessary to let the neu-
ral model process information in a direction-
sensitive manner. Out of this consideration,
we separate an SDP into two sub-paths, each
from an entity to the common ancestor node.
The extracted features along the two sub-
paths are concatenated to make final classi-
fication.
• Linguistic information helps. For exam-
ple, with prior knowledge of hyponymy, we
know “water is a kind of substance.” This
is a hint that the entities, water and region,
are more of Entity-Destination rela-
tion than, say, Communication-Topic.
To gather heterogeneous information along
SDP, we design a multichannel recurrent neu-
ral network. It makes use of information
from various sources, including words them-
selves, POS tags, WordNet hypernyms, and
the grammatical relations between governing
words and their children.
For effective information propagation and inte-
gration, our model leverages LSTM units during
recurrent propagation. We also customize a new
dropout strategy for our SDP-LSTM network to
alleviate the problem of overfitting. To the best
of our knowledge, we are the first to use LSTM-
based recurrent neural networks for the relation
classification task.
We evaluate our proposed method on the
SemEval 2010 relation classification task, and
achieve an Fl-score of 83.7%, higher than com-
peting methods in the literature.
In the rest of this paper, we review related work
in Section 2. In Section 3, we describe our SDP-
LSTM model in detail. Section 4 presents quan-
titative experimental results. Finally, we have our
conclusion in Section 5.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.966539">
Relation classification is a widely studied task
in the NLP community. Various existing meth-
</bodyText>
<figure confidence="0.870876142857143">
poured
gallons have been into
trillion of [region]
A [water] an empty of
e1
space
outer
</figure>
<figureCaption confidence="0.9296265">
Figure 1: The dependency parse tree correspond-
ing to the sentence “A trillion gallons of water
</figureCaption>
<bodyText confidence="0.961933457142857">
have been poured into an empty region of outer
space.” Red lines indicate the shortest dependency
path between entities water and region. An edge
a → b refers to a being governed by b. Depen-
dency types are labeled by the parser, but not pre-
sented in the figure for clarity.
ods mainly fall into three classes: feature-based,
kernel-based, and neural network-based.
In feature-based approaches, different sets of
features are extracted and fed to a chosen classifier
(e.g., logistic regression). Generally, three types of
features are often used. Lexical features concen-
trate on the entities of interest, e.g., entities per se,
entity POS, entity neighboring information. Syn-
tactic features include chunking, parse trees, etc.
Semantic features are exemplified by the concept
hierarchy, entity class, entity mention. Kamb-
hatla (2004) uses a maximum entropy model to
combine these features for relation classification.
However, different sets of handcrafted features are
largely complementary to each other (e.g., hyper-
nyms versus named-entity tags), and thus it is hard
to improve performance in this way (GuoDong et
al., 2005).
Kernel-based approaches specify some measure
of similarity between two data samples, with-
out explicit feature representation. Zelenko et
al. (2003) compute the similarity of two trees by
utilizing their common subtrees. Bunescu and
Mooney (2005) propose a shortest path depen-
dency kernel for relation classification. Its main
idea is that the relation strongly relies on the de-
pendency path between two given entities. Wang
(2008) provides a systematic analysis of several
kernels and show that relation extraction can bene-
</bodyText>
<page confidence="0.729111">
e2
1786
</page>
<bodyText confidence="0.999975878787879">
fit from combining convolution kernel and syntac-
tic features. Plank and Moschitti (2013) introduce
semantic information into kernel methods in ad-
dition to considering structural information only.
One potential difficulty of kernel methods is that
all data information is completely summarized by
the kernel function (similarity measure), and thus
designing an effective kernel becomes crucial.
Deep neural networks, emerging recently, can
learn underlying features automatically, and have
attracted growing interest in the literature. Socher
et al. (2011) propose a recursive neural network
(RNN) along sentences’ parse trees for sentiment
analysis; such model can also be used to clas-
sify relations (Socher et al., 2012). Hashimoto et
al. (2013) explicitly weight phrases’ importance
in RNNs to improve performance. Ebrahimi and
Dou (2015) rebuild an RNN on the dependency
path between two marked entities. Zeng et al.
(2014) explore convolutional neural networks, by
which they utilize sequential information of sen-
tences. dos Santos et al. (2015) also use the convo-
lutional network; besides, they propose a ranking
loss function with data cleaning, and achieve the
state-of-the-art result in SemEval-2010 Task 8.
In addition to the above studies, which mainly
focus on relation classification approaches and
models, other related research trends include in-
formation extraction from Web documents in a
semi-supervised manner (Bunescu and Mooney,
2007; Banko et al., 2007), dealing with small
datasets without enough labels by distant super-
vision techniques (Mintz et al., 2009), etc.
</bodyText>
<sectionHeader confidence="0.994557" genericHeader="method">
3 The Proposed SDP-LSTM Model
</sectionHeader>
<bodyText confidence="0.999942090909091">
In this section, we describe our SDP-LSTM model
in detail. Subsection 3.1 delineates the overall ar-
chitecture of our model. Subsection 3.2 presents
the rationale of using SDPs. Four different infor-
mation channels along the SDP are explained in
Subsection 3.3. Subsection 3.4 introduces the re-
current neural network with long short term mem-
ory, which is built upon the dependency path. Sub-
section 3.5 customizes a dropout strategy for our
network to alleviate overfitting. We finally present
our training objective in Subsection 3.6.
</bodyText>
<subsectionHeader confidence="0.98434">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.98519252">
Figure 2 depicts the overall architecture of our
SDP-LSTM network.
First, a sentence is parsed to a dependency tree
by the Stanford parser;1 the shortest dependency
path (SDP) is extracted as the input of our net-
work. Along the SDP, four different types of
information—referred to as channels—are used,
including the words, POS tags, grammatical rela-
tions, and WordNet hypernyms. (See Figure 2a.)
In each channel, discrete inputs, e.g., words, are
mapped to real-valued vectors, called embeddings,
which capture the underlying meanings of the in-
puts.
Two recurrent neural networks (Figure 2b) pick
up information along the left and right sub-paths
of the SDP, respecitvely. (The path is separated by
the common ancestor node of two entities.) Long
short term memory (LSTM) units are used in the
recurrent networks for effective information prop-
agation. A max pooling layer thereafter gathers
information from LSTM nodes in each path.
The pooling layers from different channels are
concatenated, and then connected to a hidden
layer. Finally, we have a softmax output layer for
classification. (See again Figure 2a.)
</bodyText>
<subsectionHeader confidence="0.998375">
3.2 The Shortest Dependency Path
</subsectionHeader>
<bodyText confidence="0.997499913043478">
The dependency parse tree is naturally suitable for
relation classification because it focuses on the ac-
tion and agents in a sentence (Socher et al., 2014).
Moreover, the shortest path between entities, as
discussed in Section 1, condenses most illuminat-
ing information for entities’ relation.
We also observe that the sub-paths, separated by
the common ancestor node of two entities, provide
strong hints for the relation’s directionality. Take
Figure 1 as an example. Two entities water and
region have their common ancestor node, poured,
which separates the SDP into two parts:
[water],, → of → gallons → poured
and
poured ← into ← [region],,,
The first sub-path captures information of e1,
whereas the second sub-path is mainly about
e2. By examining the two sub-paths sepa-
rately, we know e1 and e2 are of relation
Entity-Destination(e1, e2), rather than
Entity-Destination(e2, e1).
Following the above intuition, we design
two recurrent neural networks, which propagate
</bodyText>
<footnote confidence="0.976911">
1http://nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.985128">
1787
</page>
<figure confidence="0.999696607142857">
LSTM for
word
embeddings
(a)
LSTM for
POS
embeddings
Hidden layer
Softmax (b)
LSTM for
GR
embeddings
Dependency
paths
LSTM for
WordNet
embeddings
LSTM
LSTM
Left sub-path
Pool
LSTM
LSTM LSTM
Hidden layer
Right sub-path
LSTM
Pool
LSTM
</figure>
<figureCaption confidence="0.896169666666667">
Figure 2: (a) The overall architecture of SDP-LSTM. (b) One channel of the recurrent neural networks
built upon the shortest dependency path. The channels are words, part-of-speech (POS) tags, grammatical
relations (abbreviated as GR in the figure), and WordNet hypernyms.
</figureCaption>
<bodyText confidence="0.971940333333333">
bottom-up from the entities to their common an-
cestor. In this way, our model is direction-
sensitive.
</bodyText>
<subsectionHeader confidence="0.994161">
3.3 Channels
</subsectionHeader>
<bodyText confidence="0.9999336">
We make use of four types of information along
the SDP for relation classification. We call them
channels as these information sources do not inter-
act during recurrent propagation. Detailed channel
descriptions are as follows.
</bodyText>
<listItem confidence="0.818731793103448">
• Word representations. Each word in a given
sentence is mapped to a real-valued vector by
looking up in a word embedding table. Un-
supervisedly trained on a large corpus, word
embeddings are thought to be able to well
capture words’ syntactic and semantic infor-
mation (Mikolov et al., 2013b).
• Part-of-speech tags. Since word embed-
dings are obtained on a generic corpus of a
large scale, the information they contain may
not agree with a specific sentence. We deal
with this problem by allying each input word
with its POS tag, e.g., noun, verb, etc.
In our experiment, we only take into use a
coarse-grained POS category, containing 15
different tags.
• Grammatical relations. The dependency
relations between a governing word and its
children makes a difference in meaning. A
same word pair may have different depen-
dency relation types. For example, “beats
−−−→ it” is distinct from “beats dobj
nsubj −−−→ it.”
Thus, it is necessary to capture such gram-
matical relations in SDPs. In our experi-
ment, grammatical relations are grouped into
19 classes, mainly based on a coarse-grained
classification (De Marneffe et al., 2006).
• WordNet hypernyms. As illustrated in Sec-
</listItem>
<bodyText confidence="0.996702869565217">
tion 1, hyponymy information is also useful
for relation classification. (Details are not re-
peated here.) To leverage WordNet hyper-
nyms, we use a tool developed by Ciaramita
and Altun (2006).2 The tool assigns a hy-
pernym to each word, from 41 predefined
concepts in WordNet, e.g., noun.food,
verb.motion, etc. Given its hypernym,
each word gains a more abstract concept,
which helps to build a linkage between dif-
ferent but conceptual similar words.
As we can see, POS tags, grammatical rela-
tions, and WordNet hypernyms are also discrete
(like words per se). However, no prevailing em-
bedding learning method exists for POS tags, say.
Hence, we randomly initialize their embeddings,
and tune them in a supervised fashion during train-
ing. We notice that these information sources con-
tain much fewer symbols, 15, 19, and 41, than the
vocabulary size (greater than 25,000). Hence, we
believe our strategy of random initialization is fea-
sible, because they can be adequately tuned during
supervised training.
</bodyText>
<subsectionHeader confidence="0.9538605">
3.4 Recurrent Neural Network with Long
Short Term Memory Units
</subsectionHeader>
<bodyText confidence="0.998021">
The recurrent neural network is suitable for mod-
eling sequential data by nature, as it keeps a hid-
</bodyText>
<footnote confidence="0.967424">
2http://sourceforge.net/projects/supersensetag
</footnote>
<page confidence="0.970069">
1788
</page>
<figure confidence="0.74422">
ht
</figure>
<figureCaption confidence="0.9637704">
Figure 3: A long short term memory unit. h: hid-
den unit. c: memory cell. i: input gate. f: for-
get gate. o: output gate. g: candidate cell. ®:
element-wise multiplication. ∼: activation func-
tion.
</figureCaption>
<bodyText confidence="0.990364363636364">
den state vector h, which changes with input data
at each step accordingly. We use the recurrent net-
work to gather information along each sub-path in
the SDP (Figure 2b).
The hidden state ht, for the t-th word in the
sub-path, is a function of its previous state ht−1
and the current word xt. Traditional recurrent net-
works have a basic interaction, that is, the input is
linearly transformed by a weight matrix and non-
linearly squashed by an activation function. For-
mally, we have
</bodyText>
<equation confidence="0.776827">
ht = f(Winxt + Wrecht−1 + bh)
</equation>
<bodyText confidence="0.999948258064516">
where Win and Wrec are weight matrices for the
input and recurrent connections, respectively. bh
is a bias term for the hidden state vector, and fh a
non-linear activation function (e.g., tanh).
One problem of the above model is known
as gradient vanishing or exploding. The train-
ing of neural networks requires gradient back-
propagation. If the propagation sequence (path) is
too long, the gradient may probably either grow, or
decay, exponentially, depending on the magnitude
of Wrec. This leads to the difficulty of training.
Long short term memory (LSTM) units are pro-
posed in Hochreiter (1998) to overcome this prob-
lem. The main idea is to introduce an adaptive gat-
ing mechanism, which decides the degree to which
LSTM units keep the previous state and memo-
rize the extracted features of the current data in-
put. Many LSTM variants have been proposed in
the literature. We adopt in our method a variant
introduced by Zaremba and Sutskever (2014), also
used in Zhu et al. (2015).
Concretely, the LSTM-based recurrent neural
network comprises four components: an input gate
it, a forget gate ft, an output gate ot, and a mem-
ory cell ct (depicted in Figure 3 and formalized
through Equations 1–6 as bellow).
The three adaptive gates it, ft, and ot depend
on the previous state ht−1 and the current input
xt (Equations 1–3). An extracted feature vector
gt is also computed, by Equation 4, serving as the
candidate memory cell.
</bodyText>
<equation confidence="0.9943305">
it = Q(Wi·xt + Ui·ht−1 + bi)
ft = Q(Wf ·xt + Uf ·ht−1 + bf)
ot = Q(Wo·xt + Uo·ht−1 + bo)
gt = tanh(Wg ·xt + Ug·ht−1 + bg)
</equation>
<bodyText confidence="0.99980275">
The current memory cell ct is a combination of
the previous cell content ct−1 and the candidate
content gt, weighted by the input gate it and forget
gate ft, respectively. (See Equation 5 below.)
</bodyText>
<equation confidence="0.956101">
ct = it ® gt + ft ® ct−1 (5)
</equation>
<bodyText confidence="0.999131666666667">
The output of LSTM units is the the recur-
rent network’s hidden state, which is computed by
Equation 6 as follows.
</bodyText>
<equation confidence="0.871042">
ht = ot ® tanh(ct) (6)
</equation>
<bodyText confidence="0.9999675">
In the above equations, Q denotes a sigmoid
function; ® denotes element-wise multiplication.
</bodyText>
<subsectionHeader confidence="0.951657">
3.5 Dropout Strategies
</subsectionHeader>
<bodyText confidence="0.999404928571428">
A good regularization approach is needed to al-
leviate overfitting. Dropout, proposed recently
by Hinton et al. (2012), has been very successful
on feed-forward networks. By randomly omitting
feature detectors from the network during train-
ing, it can obtain less interdependent network units
and achieve better performance. However, the
conventional dropout does not work well with re-
current neural networks with LSTM units, since
dropout may hurt the valuable memorization abil-
ity of memory units.
As there is no consensus on how to drop
out LSTM units in the literature, we try several
dropout strategies for our SDP-LSTM network:
</bodyText>
<listItem confidence="0.996987666666667">
• Dropout embeddings;
• Dropout inner cells in memory units, includ-
ing it, gt, ot, ct, and ht; and
</listItem>
<figure confidence="0.998474823529412">
xt
ht-1
ht-1
xt
xt
o ~
ht-1
~
ct-1
xt
ct
~
f
ht-1
g i
~
~
</figure>
<page confidence="0.968829">
1789
</page>
<listItem confidence="0.976643">
• Dropout the penultimate layer.
</listItem>
<bodyText confidence="0.999917888888889">
As we shall see in Section 4.2, dropping out
LSTM units turns out to be inimical to our model,
whereas the other two strategies boost in perfor-
mance.
The following equations formalize the dropout
operations on the embedding layers, where D de-
notes the dropout operator. Each dimension in the
embedding vector, xt, is set to zero with a prede-
fined dropout rate.
</bodyText>
<equation confidence="0.9947164">
it = u(Wi·D(xt) + Ui·ht−1 + bi)
ft = u(Wf ·D(xt) + Uf ·ht−1 + bf)
ot = u(Wo·D(xt) + Uo·ht−1 + bo)
gt = tanh Wg·D(xt) + Ug·ht−1 + bg
� �
</equation>
<subsectionHeader confidence="0.989496">
3.6 Training Objective
</subsectionHeader>
<bodyText confidence="0.999997307692308">
The SDP-LSTM described above propagates in-
formation along a sub-path from an entity to the
common ancestor node (of the two entities). A
max pooling layer packs, for each sub-path, the
recurrent network’s states, h’s, to a fixed vector
by taking the maximum value in each dimension.
Such architecture applies to all channels,
namely, words, POS tags, grammatical relations,
and WordNet hypernyms. The pooling vectors in
these channels are concatenated, and fed to a fully
connected hidden layer. Finally, we add a softmax
output layer for classification. The training objec-
tive is the penalized cross-entropy error, given by
</bodyText>
<equation confidence="0.907275">
kWik2 F +
</equation>
<bodyText confidence="0.999961375">
where t ∈ Rnc is the one-hot represented ground
truth and y ∈ Rnc is the estimated probability for
each class by softmax. (nc is the number of target
classes.) k · kF denotes the Frobenius norm of a
matrix; w and v are the numbers of weight matri-
ces (for W’s and U’s, respectively). A is a hyper-
parameter that specifies the magnitude of penalty
on weights. Note that we do not add E2 penalty to
biase parameters.
We pretrained word embeddings by word2vec
(Mikolov et al., 2013a) on the English Wikipedia
corpus; other parameters are initialized randomly.
We apply stochastic gradient descent (with mini-
batch 10) for optimization; gradients are computed
by standard back-propagation. Training details are
further introduced in Section 4.2.
</bodyText>
<sectionHeader confidence="0.999206" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999976">
In this section, we present our experiments in de-
tail. Our implementation is built upon Mou et al.
(2015). Section 4.1 introduces the dataset; Section
4.2 describes hyperparameter settings. In Section
4.3, we compare SDP-LSTM’s performance with
other methods in the literature. We also analyze
the effect of different channels in Section 4.4.
</bodyText>
<subsectionHeader confidence="0.967099">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9998195">
The SemEval-2010 Task 8 dataset is a widely used
benchmark for relation classification (Hendrickx
et al., 2009). The dataset contains 8,000 sentences
for training, and 2,717 for testing. We split 1/10
samples out of the training set for validation.
The target contains 19 labels: 9 directed rela-
tions, and an undirected Other class. The di-
rected relations are list as below.
</bodyText>
<listItem confidence="0.999970888888889">
• Cause-Effect
• Component-Whole
• Content-Container
• Entity-Destination
• Entity-Origin
• Message-Topic
• Member-Collection
• Instrument-Agency
• Product-Producer
</listItem>
<bodyText confidence="0.997219409090909">
In the following are illustrated two sample sen-
tences with directed relations.
[People]e1 have been moving back into
[downtown]e2.
Financial [stress]e1 is one of the main
causes of [divorce]e2.
The target labels are Entity-Destination
(e1, e2), and Cause-Effect(e1, e2), respec-
tively.
The dataset also contains an undirected Other
class. Hence, there are 19 target labels in total.
The undirected Other class takes in entities that
do not fit into the above categories, illustrated by
the following example.
A misty [ridge]e1 uprises from the
[surge]e2.
We use the official macro-averaged F1-score to
evaluate model performance. This official mea-
surement excludes the Other relation. Nonethe-
less, we have no special treatment of Other class
in our experiments, which is typical in other stud-
ies.
</bodyText>
<equation confidence="0.991623888888889">
�nc
i=1
J = −
�ω
i=1
ti log yi+A
�kUik2F
�υ
i=1
</equation>
<page confidence="0.975754">
1790
</page>
<figure confidence="0.998386">
Fl-score (%)
80
85
70
75
Fl-score (%)
80
85
70
75
Fl-score (%)
80
85
70
75
0 0.1 0.2 0.3 0.4 0.5 0.6 0 0.1 0.2 0.3 0.4 0.5 0.6 0 0.1 0.2 0.3 0.4 0.5 0.6
Dropout rate Dropout rate Dropout rate
(a) Dropout word embeddings (b) Dropout inner cells of memory units (c) Dropout the penultimate layer
</figure>
<figureCaption confidence="0.872614">
Figure 4: F1-scores versus dropout rates. We first evaluate the effect of dropout embeddings (a). Then
the dropout of the inner cells (b) and the penultimate layer (c) is tested with word embeddings being
dropped out by 0.5.
</figureCaption>
<subsectionHeader confidence="0.989015">
4.2 Hyperparameters and Training Details
</subsectionHeader>
<bodyText confidence="0.999967032258065">
This subsection presents hyperparameter tuning
for our model. We set word-embeddings to
be 200-dimensional; POS, WordNet hyponymy,
and grammatical relation embeddings are 50-
dimensional. Each channel of the LSTM network
contains the same number of units as its source
embeddings (either 200 or 50). The penultimate
hidden layer is 100-dimensional. As it is not fea-
sible to perform full grid search for all hyperpa-
rameters, the above values are chosen empirically.
We add E2 penalty for weights with coefficient
10−5, which was chosen by validation from the set
{10−2,10−3,· ·· ,10−7}.
We thereafter validate the proposed dropout
strategies in Section 3.5. Since network units in
different channels do not interact with each other
during information propagation, we herein take
one channel of LSTM networks to assess the ef-
ficacy. Taking the word channel as an example,
we first drop out word embeddings. Then with a
fixed dropout rate of word embeddings, we test the
effect of dropping out LSTM inner cells and the
penultimate units, respectively.
We find that, dropout of LSTM units hurts the
model, even if the dropout rate is small, 0.1,
say (Figure 4b). Dropout of embeddings im-
proves model performance by 2.16% (Figure 4a);
dropout of the penultimate layer further improves
by 0.16% (Figure 4c). This analysis also provides,
for other studies, some clues for dropout in LSTM
networks.
</bodyText>
<subsectionHeader confidence="0.835745">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.996958205128205">
Table 4 compares our SDT-LSTM with other state-
of-the-art methods. The first entry in the ta-
ble presents the highest performance achieved by
traditional feature engineering. Hendrickx et al.
(2009) leverage a variety of handcrafted features,
and use SVM for classification; they achieve an
F1-score of 82.2%.
Neural networks are first used in this task in
Socher et al. (2012). They build a recursive neural
network (RNN) along a constituency tree for re-
lation classification. They extend the basic RNN
with matrix-vector interaction and achieve an F1-
score of 82.4%.
Zeng et al. (2014) treat a sentence as sequen-
tial data and exploit the convolutional neural net-
work (CNN); they also integrate word position
information into their model. dos Santos et al.
(2015) design a model called CR-CNN; they pro-
pose a ranking-based cost function and elaborately
diminish the impact of the Other class, which is
not counted in the official F1-measure. In this way,
they achieve the state-of-the-art result with the F1-
score of 84.1%. Without such special treatment,
their F1-score is 82.7%.
Yu et al. (2014) propose a Feature-rich Com-
positional Embedding Model (FCM) for relation
classification, which combines unlexicalized lin-
guistic contexts and word embeddings. They
achieve an F1-score of 83.0%.
Our proposed SDT-LSTM model yields an F1-
score of 83.7%. It outperforms existing compet-
ing approaches, in a fair condition of softmax with
cross-entropy error.
It is worth to note that we have also conducted
two controlled experiments: (1) Traditional RNN
without LSTM units, achieving an F1-score of
82.8%; (2) LSTM network over the entire depen-
dency path (instead of two sub-paths), achieving
an F1-score of 82.2%. These results demonstrate
</bodyText>
<page confidence="0.95824">
1791
</page>
<table confidence="0.999896">
Classifier Feature set Fi
SVM POS, WordNet, prefixes and other morphological features, 82.2
depdency parse, Levin classes, PropBank, FanmeNet,
NomLex-Plus, Google n-gram, paraphrases, TextRunner
Word embeddings 74.8
RNN Word embeddings, POS, NER, WordNet 77.6
MVRNN Word embeddings 79.1
Word embeddings, POS, NER, WordNet 82.4
CNN Word embeddings 69.7
Word embeddings, word position embeddings, WordNet 82.7
Chain CNN Word embeddings, POS, NER, WordNet 82.7
FCM Word embeddings 80.6
Word embeddings, depedency parsing, NER 83.0
Word embeddings 82.81
CR-CNN Word embeddings, position embeddings 82.7
Word embeddings, position embeddings 84.11
Word embeddings 82.4
SDP-LSTM Word embeddings, POS embeddings, WordNet embeddings, 83.7
grammar relation embeddings
</table>
<tableCaption confidence="0.9366745">
Table 1: Comparison of relation classification systems. The “†” remark refers to special treatment for
the Other class.
</tableCaption>
<bodyText confidence="0.645173">
the effectiveness of LSTM and directionality in re-
lation classification.
</bodyText>
<subsectionHeader confidence="0.99949">
4.4 Effect of Different Channels
</subsectionHeader>
<bodyText confidence="0.999891695652174">
This subsection analyzes how different channels
affect our model. We first used word embeddings
only as a baseline; then we added POS tags, gram-
matical relations, and WordNet hypernyms, re-
spectively; we also combined all these channels
into our models. Note that we did not try the latter
three channels alone, because each single of them
(e.g., POS) does not carry much information.
We see from Table 2 that word embeddings
alone in SDP-LSTM yield a remarkable perfor-
mance of 82.35%, compared with CNNs 69.7%,
RNNs 74.9–79.1%, and FCM 80.6%.
Adding either grammatical relations or Word-
Net hypernyms outperforms other existing meth-
ods (data cleaning not considered here). POS tag-
ging is comparatively less informative, but still
boosts the Fi-score by 0.63%.
We notice that, the boosts are not simply added
when channels are combined. This suggests that
these information sources are complementary to
each other in some linguistic aspects. Nonethe-
less, incorporating all four channels further pushes
the Fl-score to 83.70%.
</bodyText>
<table confidence="0.893591166666667">
Channels Fi
Word embeddings 82.35
+ POS embeddings (only) 82.98
+ GR embeddings (only) 83.21
+ WordNet embeddings (only) 83.03
+ POS + GR + WordNet embeddings 83.70
</table>
<tableCaption confidence="0.988672">
Table 2: Effect of different channels.
</tableCaption>
<sectionHeader confidence="0.991365" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.997469777777778">
In this paper, we propose a novel neural network
model, named SDP-LSTM, for relation classifi-
cation. It learns features for relation classifica-
tion iteratively along the shortest dependency path.
Several types of information (word themselves,
POS tags, grammatical relations and WordNet hy-
pernyms) along the path are used. Meanwhile,
we leverage LSTM units for long-range infor-
mation propagation and integration. We demon-
strate the effectiveness of SDP-LSTM by evalu-
ating the model on SemEval-2010 relation clas-
sification task, outperforming existing state-of-art
methods (in a fair condition without data clean-
ing). Our result sheds some light in the relation
classification task as follows.
• The shortest dependency path can be a valu-
able resource for relation classification, cov-
ering mostly sufficient information of target
</bodyText>
<page confidence="0.980205">
1792
</page>
<bodyText confidence="0.857384">
relations.
</bodyText>
<listItem confidence="0.866016416666667">
• Classifying relation is a challenging task due
to the inherent ambiguity of natural lan-
guages and the diversity of sentence expres-
sion. Thus, integrating heterogeneous lin-
guistic knowledge is beneficial to the task.
• Treating the shortest dependency path as two
sub-paths, mapping two different neural net-
works, helps to capture the directionality of
relations.
• LSTM units are effective in feature detec-
tion and propagation along the shortest de-
pendency path.
</listItem>
<sectionHeader confidence="0.998508" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9575918">
This research is supported by the National Basic
Research Program of China (the 973 Program) un-
der Grant No. 2015CB352201 and the National
Natural Science Foundation of China under Grant
Nos. 61232015 and 91318301.
</bodyText>
<sectionHeader confidence="0.996855" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999092279069767">
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In Pro-
ceedings of twentieth International Joint Conference
on Artificial Intelligence, volume 7, pages 2670–
2676.
Yoshua Bengio, Aaron Courville, and Pierre Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(8):1798–1828.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 724–731.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using mini-
mal supervision. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, volume 45, pages 576–583.
Yun-Nung Chen, Dilek Hakkani-Tur, and Gokan Tur.
2014. Deriving local relational surface forms from
dependency-based entity embeddings for unsuper-
vised spoken language understanding. In Spoken
Language Technology Workshop (SLT), 2014 IEEE,
pages 242–247.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
594–602.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation, volume 6,
pages 449–454.
Cıcero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
convolutional neural networks. In Proceedings of
53rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 626–634.
Javid Ebrahimi and Dejing Dou. 2015. Chain based
rnn for relation classification. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1244–1249.
Katrin Fundel, Robert K¨uffner, and Ralf Zimmer.
2007. Relexłrelation extraction using dependency
parse trees. Bioinformatics, 23(3):365–371.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
427–434.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple cus-
tomization of recursive neural networks for semantic
relation classification. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1372–1376.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task
8: Multi-way classification of semantic relations
between pairs of nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions), pages 94–99.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors. arXiv
preprint arXiv:1207.0580.
Sepp Hochreiter. 1998. The vanishing gradient
problem during learning recurrent neural nets and
problem solutions. International Journal of Un-
certainty, Fuzziness and Knowledge-Based Systems,
6(02):107–116.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive Poster and Demonstra-
tion Sessions, page 22. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.571509">
1793
</page>
<reference confidence="0.999830921348315">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013a. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.
Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin. 2015. Discriminative neural sentence
modeling by tree-based convolution. arXiv preprint
arXiv:1504.01106.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 1498–1507.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.
Chang Wang and James Fan. 2014. Medical rela-
tion extraction with manifold models. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 828–838.
Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing, pages 841–846.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 118–127.
Yan Xu, Ge Li, Lili Mou, and Yangyang Lu. 2014.
Learning non-taxonomic relations on demand for
ontology extension. International Journal of Soft-
ware Engineering and Knowledge Engineering,
24(08):1159–1175.
Xuchen Yao and Benjamin Van Durme. 2014. Infor-
mation extraction over structured data: Question an-
swering with freebase. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 956–966.
Mo Yu, Matthew Gormley, and Mark Dredze. 2014.
Factor-based compositional embedding models. In
NIPS Workshop on Learning Semantics.
Wojciech Zaremba and Ilya Sutskever. 2014. Learning
to execute. arXiv preprint arXiv:1410.4615.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083–1106.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via
convolutional deep neural network. In Proceedings
of COLING 2014, the 25th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 2335–2344.
Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
In Proceedings of The 32nd International Confer-
ence on Machine Learning, pages 1604–1612.
</reference>
<page confidence="0.994704">
1794
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.560989">
<title confidence="0.7757075">Classifying Relations via Long Short Term Memory along Shortest Dependency Paths</title>
<note confidence="0.9218865">Institute, Peking University, 100871, P. R. of Chinese Academy of Sciences, chenyunchuan11@mails.ucas.ac.cn</note>
<abstract confidence="0.999460269230769">Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, achieve an of 83.7%, higher than competing methods in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction for the web. In</title>
<date>2007</date>
<booktitle>Proceedings of twentieth International Joint Conference on Artificial Intelligence,</booktitle>
<volume>7</volume>
<pages>2670--2676</pages>
<contexts>
<context position="9352" citStr="Banko et al., 2007" startWordPosition="1424" endWordPosition="1427">N on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8. In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and Mooney, 2007; Banko et al., 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al., 2009), etc. 3 The Proposed SDP-LSTM Model In this section, we describe our SDP-LSTM model in detail. Subsection 3.1 delineates the overall architecture of our model. Subsection 3.2 presents the rationale of using SDPs. Four different information channels along the SDP are explained in Subsection 3.3. Subsection 3.4 introduces the recurrent neural network with long short term memory, which is built upon the dependency path. Subsection 3.5 customizes a dropout strategy for our network to alleviat</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open information extraction for the web. In Proceedings of twentieth International Joint Conference on Artificial Intelligence, volume 7, pages 2670– 2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pierre Vincent</author>
</authors>
<title>Representation learning: A review and new perspectives.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>8</issue>
<contexts>
<context position="2598" citStr="Bengio et al., 2013" startWordPosition="378" endWordPosition="381">ter and region are of relation Entity-Destination(e1, e2). Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen. The latter approach, on the other hand, depends largely on the designed kernel, which summarizes all data information. Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al., 2014; dos Santos et al., 2015). However, human engineering—that is, incorporating human knowledge to the network’s architecture—is still important and beneficial. This paper proposes a new neural network, SDP-LSTM, for relation classification. Our model utilizes the shortest dependency path (SDP) between two entities in a sentence; we also design a long short term memory (LSTM)-based recurrent neural network for information processing. The neural architecture is mainly inspired by the following observations. • Shortest dependency paths </context>
</contexts>
<marker>Bengio, Courville, Vincent, 2013</marker>
<rawString>Yoshua Bengio, Aaron Courville, and Pierre Vincent. 2013. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<contexts>
<context position="2207" citStr="Bunescu and Mooney, 2005" startWordPosition="314" endWordPosition="317">rme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc. The aim of relation classification is to categorize into predefined classes the relations between pairs of marked entities in given texts. For instance, in the sentence “A trillion gallons of [water],, have been poured into an empty [region],, of outer ∗Corresponding authors. space,” the entities water and region are of relation Entity-Destination(e1, e2). Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen. The latter approach, on the other hand, depends largely on the designed kernel, which summarizes all data information. Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al., 2014; dos Santos et al., 2015). However, human engineering—that is, incorporating human knowledge to the network’s architecture—is still important and </context>
<context position="7593" citStr="Bunescu and Mooney (2005)" startWordPosition="1161" endWordPosition="1164">exemplified by the concept hierarchy, entity class, entity mention. Kambhatla (2004) uses a maximum entropy model to combine these features for relation classification. However, different sets of handcrafted features are largely complementary to each other (e.g., hypernyms versus named-entity tags), and thus it is hard to improve performance in this way (GuoDong et al., 2005). Kernel-based approaches specify some measure of similarity between two data samples, without explicit feature representation. Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation classification. Its main idea is that the relation strongly relies on the dependency path between two given entities. Wang (2008) provides a systematic analysis of several kernels and show that relation extraction can benee2 1786 fit from combining convolution kernel and syntactic features. Plank and Moschitti (2013) introduce semantic information into kernel methods in addition to considering structural information only. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel functio</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 724–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>576--583</pages>
<contexts>
<context position="9331" citStr="Bunescu and Mooney, 2007" startWordPosition="1420" endWordPosition="1423">d Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8. In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and Mooney, 2007; Banko et al., 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al., 2009), etc. 3 The Proposed SDP-LSTM Model In this section, we describe our SDP-LSTM model in detail. Subsection 3.1 delineates the overall architecture of our model. Subsection 3.2 presents the rationale of using SDPs. Four different information channels along the SDP are explained in Subsection 3.3. Subsection 3.4 introduces the recurrent neural network with long short term memory, which is built upon the dependency path. Subsection 3.5 customizes a dropout strategy for ou</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, volume 45, pages 576–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>Dilek Hakkani-Tur</author>
<author>Gokan Tur</author>
</authors>
<title>Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding.</title>
<date>2014</date>
<booktitle>In Spoken Language Technology Workshop (SLT), 2014 IEEE,</booktitle>
<pages>242--247</pages>
<contexts>
<context position="3254" citStr="Chen et al., 2014" startWordPosition="474" endWordPosition="477">ntial (Zeng et al., 2014; dos Santos et al., 2015). However, human engineering—that is, incorporating human knowledge to the network’s architecture—is still important and beneficial. This paper proposes a new neural network, SDP-LSTM, for relation classification. Our model utilizes the shortest dependency path (SDP) between two entities in a sentence; we also design a long short term memory (LSTM)-based recurrent neural network for information processing. The neural architecture is mainly inspired by the following observations. • Shortest dependency paths are informative (Fundel et al., 2007; Chen et al., 2014). To determine the two entities’ relation, we find it mostly sufficient to use only the words along the SDP: they concentrate on most relevant information while diminishing less relevant noise. Figure 1 depicts the dependency parse tree of the aforementioned sentence. Words along the SDP form a trimmed phrase (gallons of water poured into region) of the original sentence, which conveys much information about the target relation. Other words, such as a, trillion, outer space, are less informative and may bring noise if not dealt with properly. 1785 Proceedings of the 2015 Conference on Empirica</context>
</contexts>
<marker>Chen, Hakkani-Tur, Tur, 2014</marker>
<rawString>Yun-Nung Chen, Dilek Hakkani-Tur, and Gokan Tur. 2014. Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding. In Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 242–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>594--602</pages>
<contexts>
<context position="14428" citStr="Ciaramita and Altun (2006)" startWordPosition="2224" endWordPosition="2227">s children makes a difference in meaning. A same word pair may have different dependency relation types. For example, “beats −−−→ it” is distinct from “beats dobj nsubj −−−→ it.” Thus, it is necessary to capture such grammatical relations in SDPs. In our experiment, grammatical relations are grouped into 19 classes, mainly based on a coarse-grained classification (De Marneffe et al., 2006). • WordNet hypernyms. As illustrated in Section 1, hyponymy information is also useful for relation classification. (Details are not repeated here.) To leverage WordNet hypernyms, we use a tool developed by Ciaramita and Altun (2006).2 The tool assigns a hypernym to each word, from 41 predefined concepts in WordNet, e.g., noun.food, verb.motion, etc. Given its hypernym, each word gains a more abstract concept, which helps to build a linkage between different but conceptual similar words. As we can see, POS tags, grammatical relations, and WordNet hypernyms are also discrete (like words per se). However, no prevailing embedding learning method exists for POS tags, say. Hence, we randomly initialize their embeddings, and tune them in a supervised fashion during training. We notice that these information sources contain much</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 594–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the International Conference on Language Resources and Evaluation, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cıcero Nogueira dos Santos</author>
<author>Bing Xiang</author>
<author>Bowen Zhou</author>
</authors>
<title>Classifying relations by ranking with convolutional neural networks.</title>
<date>2015</date>
<booktitle>In Proceedings of 53rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>626--634</pages>
<contexts>
<context position="2686" citStr="Santos et al., 2015" startWordPosition="393" endWordPosition="396">cation approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen. The latter approach, on the other hand, depends largely on the designed kernel, which summarizes all data information. Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al., 2014; dos Santos et al., 2015). However, human engineering—that is, incorporating human knowledge to the network’s architecture—is still important and beneficial. This paper proposes a new neural network, SDP-LSTM, for relation classification. Our model utilizes the shortest dependency path (SDP) between two entities in a sentence; we also design a long short term memory (LSTM)-based recurrent neural network for information processing. The neural architecture is mainly inspired by the following observations. • Shortest dependency paths are informative (Fundel et al., 2007; Chen et al., 2014). To determine the two entities’</context>
<context position="8928" citStr="Santos et al. (2015)" startWordPosition="1361" endWordPosition="1364">ntly, can learn underlying features automatically, and have attracted growing interest in the literature. Socher et al. (2011) propose a recursive neural network (RNN) along sentences’ parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012). Hashimoto et al. (2013) explicitly weight phrases’ importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8. In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and Mooney, 2007; Banko et al., 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al., 2009), etc. 3 The Proposed SDP-LSTM Model In this section, we describe our </context>
<context position="25396" citStr="Santos et al. (2015)" startWordPosition="4064" endWordPosition="4067">ditional feature engineering. Hendrickx et al. (2009) leverage a variety of handcrafted features, and use SVM for classification; they achieve an F1-score of 82.2%. Neural networks are first used in this task in Socher et al. (2012). They build a recursive neural network (RNN) along a constituency tree for relation classification. They extend the basic RNN with matrix-vector interaction and achieve an F1- score of 82.4%. Zeng et al. (2014) treat a sentence as sequential data and exploit the convolutional neural network (CNN); they also integrate word position information into their model. dos Santos et al. (2015) design a model called CR-CNN; they propose a ranking-based cost function and elaborately diminish the impact of the Other class, which is not counted in the official F1-measure. In this way, they achieve the state-of-the-art result with the F1- score of 84.1%. Without such special treatment, their F1-score is 82.7%. Yu et al. (2014) propose a Feature-rich Compositional Embedding Model (FCM) for relation classification, which combines unlexicalized linguistic contexts and word embeddings. They achieve an F1-score of 83.0%. Our proposed SDT-LSTM model yields an F1- score of 83.7%. It outperform</context>
</contexts>
<marker>Santos, Xiang, Zhou, 2015</marker>
<rawString>Cıcero Nogueira dos Santos, Bing Xiang, and Bowen Zhou. 2015. Classifying relations by ranking with convolutional neural networks. In Proceedings of 53rd Annual Meeting of the Association for Computational Linguistics, pages 626–634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javid Ebrahimi</author>
<author>Dejing Dou</author>
</authors>
<title>Chain based rnn for relation classification.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1244--1249</pages>
<contexts>
<context position="8719" citStr="Ebrahimi and Dou (2015)" startWordPosition="1328" endWordPosition="1331">lty of kernel methods is that all data information is completely summarized by the kernel function (similarity measure), and thus designing an effective kernel becomes crucial. Deep neural networks, emerging recently, can learn underlying features automatically, and have attracted growing interest in the literature. Socher et al. (2011) propose a recursive neural network (RNN) along sentences’ parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012). Hashimoto et al. (2013) explicitly weight phrases’ importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8. In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and </context>
</contexts>
<marker>Ebrahimi, Dou, 2015</marker>
<rawString>Javid Ebrahimi and Dejing Dou. 2015. Chain based rnn for relation classification. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1244–1249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Fundel</author>
<author>Robert K¨uffner</author>
<author>Ralf Zimmer</author>
</authors>
<title>Relexłrelation extraction using dependency parse trees.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Fundel, K¨uffner, Zimmer, 2007</marker>
<rawString>Katrin Fundel, Robert K¨uffner, and Ralf Zimmer. 2007. Relexłrelation extraction using dependency parse trees. Bioinformatics, 23(3):365–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
<author>Zhang Jie</author>
<author>Zhang Min</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>427--434</pages>
<contexts>
<context position="7346" citStr="GuoDong et al., 2005" startWordPosition="1126" endWordPosition="1129"> three types of features are often used. Lexical features concentrate on the entities of interest, e.g., entities per se, entity POS, entity neighboring information. Syntactic features include chunking, parse trees, etc. Semantic features are exemplified by the concept hierarchy, entity class, entity mention. Kambhatla (2004) uses a maximum entropy model to combine these features for relation classification. However, different sets of handcrafted features are largely complementary to each other (e.g., hypernyms versus named-entity tags), and thus it is hard to improve performance in this way (GuoDong et al., 2005). Kernel-based approaches specify some measure of similarity between two data samples, without explicit feature representation. Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation classification. Its main idea is that the relation strongly relies on the dependency path between two given entities. Wang (2008) provides a systematic analysis of several kernels and show that relation extraction can benee2 1786 fit from combining convolution kernel and syntactic features. Plank</context>
</contexts>
<marker>GuoDong, Jian, Jie, Min, 2005</marker>
<rawString>Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuma Hashimoto</author>
<author>Makoto Miwa</author>
<author>Yoshimasa Tsuruoka</author>
<author>Takashi Chikayama</author>
</authors>
<title>Simple customization of recursive neural networks for semantic relation classification.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1372--1376</pages>
<contexts>
<context position="8625" citStr="Hashimoto et al. (2013)" startWordPosition="1315" endWordPosition="1318">o kernel methods in addition to considering structural information only. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel function (similarity measure), and thus designing an effective kernel becomes crucial. Deep neural networks, emerging recently, can learn underlying features automatically, and have attracted growing interest in the literature. Socher et al. (2011) propose a recursive neural network (RNN) along sentences’ parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012). Hashimoto et al. (2013) explicitly weight phrases’ importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8. In addition to the above studies, which mainly focus on relation classification approaches and models, other related research tren</context>
</contexts>
<marker>Hashimoto, Miwa, Tsuruoka, Chikayama, 2013</marker>
<rawString>Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple customization of recursive neural networks for semantic relation classification. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1372–1376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions),</booktitle>
<pages>94--99</pages>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2009</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions), pages 94–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="18308" citStr="Hinton et al. (2012)" startWordPosition="2891" endWordPosition="2894">h(Wg ·xt + Ug·ht−1 + bg) The current memory cell ct is a combination of the previous cell content ct−1 and the candidate content gt, weighted by the input gate it and forget gate ft, respectively. (See Equation 5 below.) ct = it ® gt + ft ® ct−1 (5) The output of LSTM units is the the recurrent network’s hidden state, which is computed by Equation 6 as follows. ht = ot ® tanh(ct) (6) In the above equations, Q denotes a sigmoid function; ® denotes element-wise multiplication. 3.5 Dropout Strategies A good regularization approach is needed to alleviate overfitting. Dropout, proposed recently by Hinton et al. (2012), has been very successful on feed-forward networks. By randomly omitting feature detectors from the network during training, it can obtain less interdependent network units and achieve better performance. However, the conventional dropout does not work well with recurrent neural networks with LSTM units, since dropout may hurt the valuable memorization ability of memory units. As there is no consensus on how to drop out LSTM units in the literature, we try several dropout strategies for our SDP-LSTM network: • Dropout embeddings; • Dropout inner cells in memory units, including it, gt, ot, ct</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
</authors>
<title>The vanishing gradient problem during learning recurrent neural nets and problem solutions.</title>
<date>1998</date>
<journal>International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,</journal>
<volume>6</volume>
<issue>02</issue>
<contexts>
<context position="16762" citStr="Hochreiter (1998)" startWordPosition="2612" endWordPosition="2613">xt + Wrecht−1 + bh) where Win and Wrec are weight matrices for the input and recurrent connections, respectively. bh is a bias term for the hidden state vector, and fh a non-linear activation function (e.g., tanh). One problem of the above model is known as gradient vanishing or exploding. The training of neural networks requires gradient backpropagation. If the propagation sequence (path) is too long, the gradient may probably either grow, or decay, exponentially, depending on the magnitude of Wrec. This leads to the difficulty of training. Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed in the literature. We adopt in our method a variant introduced by Zaremba and Sutskever (2014), also used in Zhu et al. (2015). Concretely, the LSTM-based recurrent neural network comprises four components: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct (depicted in Figure 3 and formalized through Equations 1–6 a</context>
</contexts>
<marker>Hochreiter, 1998</marker>
<rawString>Sepp Hochreiter. 1998. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL 2004 on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>page</pages>
<contexts>
<context position="2140" citStr="Kambhatla, 2004" startWordPosition="305" endWordPosition="306">on (Wu and Weld, 2010), question answering (Yao and Van Durme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc. The aim of relation classification is to categorize into predefined classes the relations between pairs of marked entities in given texts. For instance, in the sentence “A trillion gallons of [water],, have been poured into an empty [region],, of outer ∗Corresponding authors. space,” the entities water and region are of relation Entity-Destination(e1, e2). Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen. The latter approach, on the other hand, depends largely on the designed kernel, which summarizes all data information. Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al., 2014; dos Santos et al., 2015). However, human engineering—that is, incorporating hu</context>
<context position="7052" citStr="Kambhatla (2004)" startWordPosition="1082" endWordPosition="1084"> the parser, but not presented in the figure for clarity. ods mainly fall into three classes: feature-based, kernel-based, and neural network-based. In feature-based approaches, different sets of features are extracted and fed to a chosen classifier (e.g., logistic regression). Generally, three types of features are often used. Lexical features concentrate on the entities of interest, e.g., entities per se, entity POS, entity neighboring information. Syntactic features include chunking, parse trees, etc. Semantic features are exemplified by the concept hierarchy, entity class, entity mention. Kambhatla (2004) uses a maximum entropy model to combine these features for relation classification. However, different sets of handcrafted features are largely complementary to each other (e.g., hypernyms versus named-entity tags), and thus it is hard to improve performance in this way (GuoDong et al., 2005). Kernel-based approaches specify some measure of similarity between two data samples, without explicit feature representation. Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation cla</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In Proceedings of the ACL 2004 on Interactive Poster and Demonstration Sessions, page 22. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="13356" citStr="Mikolov et al., 2013" startWordPosition="2048" endWordPosition="2051"> from the entities to their common ancestor. In this way, our model is directionsensitive. 3.3 Channels We make use of four types of information along the SDP for relation classification. We call them channels as these information sources do not interact during recurrent propagation. Detailed channel descriptions are as follows. • Word representations. Each word in a given sentence is mapped to a real-valued vector by looking up in a word embedding table. Unsupervisedly trained on a large corpus, word embeddings are thought to be able to well capture words’ syntactic and semantic information (Mikolov et al., 2013b). • Part-of-speech tags. Since word embeddings are obtained on a generic corpus of a large scale, the information they contain may not agree with a specific sentence. We deal with this problem by allying each input word with its POS tag, e.g., noun, verb, etc. In our experiment, we only take into use a coarse-grained POS category, containing 15 different tags. • Grammatical relations. The dependency relations between a governing word and its children makes a difference in meaning. A same word pair may have different dependency relation types. For example, “beats −−−→ it” is distinct from “be</context>
<context position="20649" citStr="Mikolov et al., 2013" startWordPosition="3305" endWordPosition="3308">yer. Finally, we add a softmax output layer for classification. The training objective is the penalized cross-entropy error, given by kWik2 F + where t ∈ Rnc is the one-hot represented ground truth and y ∈ Rnc is the estimated probability for each class by softmax. (nc is the number of target classes.) k · kF denotes the Frobenius norm of a matrix; w and v are the numbers of weight matrices (for W’s and U’s, respectively). A is a hyperparameter that specifies the magnitude of penalty on weights. Note that we do not add E2 penalty to biase parameters. We pretrained word embeddings by word2vec (Mikolov et al., 2013a) on the English Wikipedia corpus; other parameters are initialized randomly. We apply stochastic gradient descent (with minibatch 10) for optimization; gradients are computed by standard back-propagation. Training details are further introduced in Section 4.2. 4 Experiments In this section, we present our experiments in detail. Our implementation is built upon Mou et al. (2015). Section 4.1 introduces the dataset; Section 4.2 describes hyperparameter settings. In Section 4.3, we compare SDP-LSTM’s performance with other methods in the literature. We also analyze the effect of different chann</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013a. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="13356" citStr="Mikolov et al., 2013" startWordPosition="2048" endWordPosition="2051"> from the entities to their common ancestor. In this way, our model is directionsensitive. 3.3 Channels We make use of four types of information along the SDP for relation classification. We call them channels as these information sources do not interact during recurrent propagation. Detailed channel descriptions are as follows. • Word representations. Each word in a given sentence is mapped to a real-valued vector by looking up in a word embedding table. Unsupervisedly trained on a large corpus, word embeddings are thought to be able to well capture words’ syntactic and semantic information (Mikolov et al., 2013b). • Part-of-speech tags. Since word embeddings are obtained on a generic corpus of a large scale, the information they contain may not agree with a specific sentence. We deal with this problem by allying each input word with its POS tag, e.g., noun, verb, etc. In our experiment, we only take into use a coarse-grained POS category, containing 15 different tags. • Grammatical relations. The dependency relations between a governing word and its children makes a difference in meaning. A same word pair may have different dependency relation types. For example, “beats −−−→ it” is distinct from “be</context>
<context position="20649" citStr="Mikolov et al., 2013" startWordPosition="3305" endWordPosition="3308">yer. Finally, we add a softmax output layer for classification. The training objective is the penalized cross-entropy error, given by kWik2 F + where t ∈ Rnc is the one-hot represented ground truth and y ∈ Rnc is the estimated probability for each class by softmax. (nc is the number of target classes.) k · kF denotes the Frobenius norm of a matrix; w and v are the numbers of weight matrices (for W’s and U’s, respectively). A is a hyperparameter that specifies the magnitude of penalty on weights. Note that we do not add E2 penalty to biase parameters. We pretrained word embeddings by word2vec (Mikolov et al., 2013a) on the English Wikipedia corpus; other parameters are initialized randomly. We apply stochastic gradient descent (with minibatch 10) for optimization; gradients are computed by standard back-propagation. Training details are further introduced in Section 4.2. 4 Experiments In this section, we present our experiments in detail. Our implementation is built upon Mou et al. (2015). Section 4.1 introduces the dataset; Section 4.2 describes hyperparameter settings. In Section 4.3, we compare SDP-LSTM’s performance with other methods in the literature. We also analyze the effect of different chann</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9458" citStr="Mintz et al., 2009" startWordPosition="1440" endWordPosition="1443">orks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8. In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and Mooney, 2007; Banko et al., 2007), dealing with small datasets without enough labels by distant supervision techniques (Mintz et al., 2009), etc. 3 The Proposed SDP-LSTM Model In this section, we describe our SDP-LSTM model in detail. Subsection 3.1 delineates the overall architecture of our model. Subsection 3.2 presents the rationale of using SDPs. Four different information channels along the SDP are explained in Subsection 3.3. Subsection 3.4 introduces the recurrent neural network with long short term memory, which is built upon the dependency path. Subsection 3.5 customizes a dropout strategy for our network to alleviate overfitting. We finally present our training objective in Subsection 3.6. 3.1 Overview Figure 2 depicts </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Mou</author>
<author>Hao Peng</author>
<author>Ge Li</author>
<author>Yan Xu</author>
<author>Lu Zhang</author>
<author>Zhi Jin</author>
</authors>
<title>Discriminative neural sentence modeling by tree-based convolution. arXiv preprint arXiv:1504.01106.</title>
<date>2015</date>
<contexts>
<context position="21031" citStr="Mou et al. (2015)" startWordPosition="3361" endWordPosition="3364">matrices (for W’s and U’s, respectively). A is a hyperparameter that specifies the magnitude of penalty on weights. Note that we do not add E2 penalty to biase parameters. We pretrained word embeddings by word2vec (Mikolov et al., 2013a) on the English Wikipedia corpus; other parameters are initialized randomly. We apply stochastic gradient descent (with minibatch 10) for optimization; gradients are computed by standard back-propagation. Training details are further introduced in Section 4.2. 4 Experiments In this section, we present our experiments in detail. Our implementation is built upon Mou et al. (2015). Section 4.1 introduces the dataset; Section 4.2 describes hyperparameter settings. In Section 4.3, we compare SDP-LSTM’s performance with other methods in the literature. We also analyze the effect of different channels in Section 4.4. 4.1 Dataset The SemEval-2010 Task 8 dataset is a widely used benchmark for relation classification (Hendrickx et al., 2009). The dataset contains 8,000 sentences for training, and 2,717 for testing. We split 1/10 samples out of the training set for validation. The target contains 19 labels: 9 directed relations, and an undirected Other class. The directed rela</context>
</contexts>
<marker>Mou, Peng, Li, Xu, Zhang, Jin, 2015</marker>
<rawString>Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2015. Discriminative neural sentence modeling by tree-based convolution. arXiv preprint arXiv:1504.01106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1498--1507</pages>
<contexts>
<context position="7967" citStr="Plank and Moschitti (2013)" startWordPosition="1220" endWordPosition="1223">2005). Kernel-based approaches specify some measure of similarity between two data samples, without explicit feature representation. Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation classification. Its main idea is that the relation strongly relies on the dependency path between two given entities. Wang (2008) provides a systematic analysis of several kernels and show that relation extraction can benee2 1786 fit from combining convolution kernel and syntactic features. Plank and Moschitti (2013) introduce semantic information into kernel methods in addition to considering structural information only. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel function (similarity measure), and thus designing an effective kernel becomes crucial. Deep neural networks, emerging recently, can learn underlying features automatically, and have attracted growing interest in the literature. Socher et al. (2011) propose a recursive neural network (RNN) along sentences’ parse trees for sentiment analysis; such model can also be used to classif</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498–1507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="8434" citStr="Socher et al. (2011)" startWordPosition="1284" endWordPosition="1287">everal kernels and show that relation extraction can benee2 1786 fit from combining convolution kernel and syntactic features. Plank and Moschitti (2013) introduce semantic information into kernel methods in addition to considering structural information only. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel function (similarity measure), and thus designing an effective kernel becomes crucial. Deep neural networks, emerging recently, can learn underlying features automatically, and have attracted growing interest in the literature. Socher et al. (2011) propose a recursive neural network (RNN) along sentences’ parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012). Hashimoto et al. (2013) explicitly weight phrases’ importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="8600" citStr="Socher et al., 2012" startWordPosition="1311" endWordPosition="1314">mantic information into kernel methods in addition to considering structural information only. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel function (similarity measure), and thus designing an effective kernel becomes crucial. Deep neural networks, emerging recently, can learn underlying features automatically, and have attracted growing interest in the literature. Socher et al. (2011) propose a recursive neural network (RNN) along sentences’ parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012). Hashimoto et al. (2013) explicitly weight phrases’ importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8. In addition to the above studies, which mainly focus on relation classification approaches and models, ot</context>
<context position="25008" citStr="Socher et al. (2012)" startWordPosition="4001" endWordPosition="4004"> Dropout of embeddings improves model performance by 2.16% (Figure 4a); dropout of the penultimate layer further improves by 0.16% (Figure 4c). This analysis also provides, for other studies, some clues for dropout in LSTM networks. 4.3 Results Table 4 compares our SDT-LSTM with other stateof-the-art methods. The first entry in the table presents the highest performance achieved by traditional feature engineering. Hendrickx et al. (2009) leverage a variety of handcrafted features, and use SVM for classification; they achieve an F1-score of 82.2%. Neural networks are first used in this task in Socher et al. (2012). They build a recursive neural network (RNN) along a constituency tree for relation classification. They extend the basic RNN with matrix-vector interaction and achieve an F1- score of 82.4%. Zeng et al. (2014) treat a sentence as sequential data and exploit the convolutional neural network (CNN); they also integrate word position information into their model. dos Santos et al. (2015) design a model called CR-CNN; they propose a ranking-based cost function and elaborately diminish the impact of the Other class, which is not counted in the official F1-measure. In this way, they achieve the sta</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--207</pages>
<contexts>
<context position="11345" citStr="Socher et al., 2014" startWordPosition="1738" endWordPosition="1741">is separated by the common ancestor node of two entities.) Long short term memory (LSTM) units are used in the recurrent networks for effective information propagation. A max pooling layer thereafter gathers information from LSTM nodes in each path. The pooling layers from different channels are concatenated, and then connected to a hidden layer. Finally, we have a softmax output layer for classification. (See again Figure 2a.) 3.2 The Shortest Dependency Path The dependency parse tree is naturally suitable for relation classification because it focuses on the action and agents in a sentence (Socher et al., 2014). Moreover, the shortest path between entities, as discussed in Section 1, condenses most illuminating information for entities’ relation. We also observe that the sub-paths, separated by the common ancestor node of two entities, provide strong hints for the relation’s directionality. Take Figure 1 as an example. Two entities water and region have their common ancestor node, poured, which separates the SDP into two parts: [water],, → of → gallons → poured and poured ← into ← [region],,, The first sub-path captures information of e1, whereas the second sub-path is mainly about e2. By examining </context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Wang</author>
<author>James Fan</author>
</authors>
<title>Medical relation extraction with manifold models.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<volume>1</volume>
<pages>828--838</pages>
<contexts>
<context position="1634" citStr="Wang and Fan, 2014" startWordPosition="229" endWordPosition="232">nce. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature. 1 Introduction Relation classification is an important NLP task. It plays a key role in various scenarios, e.g., information extraction (Wu and Weld, 2010), question answering (Yao and Van Durme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc. The aim of relation classification is to categorize into predefined classes the relations between pairs of marked entities in given texts. For instance, in the sentence “A trillion gallons of [water],, have been poured into an empty [region],, of outer ∗Corresponding authors. space,” the entities water and region are of relation Entity-Destination(e1, e2). Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually</context>
</contexts>
<marker>Wang, Fan, 2014</marker>
<rawString>Chang Wang and James Fan. 2014. Medical relation extraction with manifold models. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 828–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
</authors>
<title>A re-examination of dependency path kernels for relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>841--846</pages>
<contexts>
<context position="7778" citStr="Wang (2008)" startWordPosition="1193" endWordPosition="1194">dcrafted features are largely complementary to each other (e.g., hypernyms versus named-entity tags), and thus it is hard to improve performance in this way (GuoDong et al., 2005). Kernel-based approaches specify some measure of similarity between two data samples, without explicit feature representation. Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation classification. Its main idea is that the relation strongly relies on the dependency path between two given entities. Wang (2008) provides a systematic analysis of several kernels and show that relation extraction can benee2 1786 fit from combining convolution kernel and syntactic features. Plank and Moschitti (2013) introduce semantic information into kernel methods in addition to considering structural information only. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel function (similarity measure), and thus designing an effective kernel becomes crucial. Deep neural networks, emerging recently, can learn underlying features automatically, and have attracted </context>
</contexts>
<marker>Wang, 2008</marker>
<rawString>Mengqiu Wang. 2008. A re-examination of dependency path kernels for relation extraction. In Proceedings of the Third International Joint Conference on Natural Language Processing, pages 841–846.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>118--127</pages>
<contexts>
<context position="1546" citStr="Wu and Weld, 2010" startWordPosition="214" endWordPosition="217">formation (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature. 1 Introduction Relation classification is an important NLP task. It plays a key role in various scenarios, e.g., information extraction (Wu and Weld, 2010), question answering (Yao and Van Durme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc. The aim of relation classification is to categorize into predefined classes the relations between pairs of marked entities in given texts. For instance, in the sentence “A trillion gallons of [water],, have been poured into an empty [region],, of outer ∗Corresponding authors. space,” the entities water and region are of relation Entity-Destination(e1, e2). Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or k</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yan Xu</author>
<author>Ge Li</author>
<author>Lili Mou</author>
<author>Yangyang Lu</author>
</authors>
<title>Learning non-taxonomic relations on demand for ontology extension.</title>
<date>2014</date>
<journal>International Journal of Software Engineering and Knowledge Engineering,</journal>
<volume>24</volume>
<issue>08</issue>
<contexts>
<context position="1671" citStr="Xu et al., 2014" startWordPosition="236" endWordPosition="239">allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature. 1 Introduction Relation classification is an important NLP task. It plays a key role in various scenarios, e.g., information extraction (Wu and Weld, 2010), question answering (Yao and Van Durme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc. The aim of relation classification is to categorize into predefined classes the relations between pairs of marked entities in given texts. For instance, in the sentence “A trillion gallons of [water],, have been poured into an empty [region],, of outer ∗Corresponding authors. space,” the entities water and region are of relation Entity-Destination(e1, e2). Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually incorporates a large set of features</context>
</contexts>
<marker>Xu, Li, Mou, Lu, 2014</marker>
<rawString>Yan Xu, Ge Li, Lili Mou, and Yangyang Lu. 2014. Learning non-taxonomic relations on demand for ontology extension. International Journal of Software Engineering and Knowledge Engineering, 24(08):1159–1175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Information extraction over structured data: Question answering with freebase.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>956--966</pages>
<marker>Yao, Van Durme, 2014</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2014. Information extraction over structured data: Question answering with freebase. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 956–966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Matthew Gormley</author>
<author>Mark Dredze</author>
</authors>
<title>Factor-based compositional embedding models.</title>
<date>2014</date>
<booktitle>In NIPS Workshop on Learning Semantics.</booktitle>
<contexts>
<context position="25731" citStr="Yu et al. (2014)" startWordPosition="4119" endWordPosition="4122">xtend the basic RNN with matrix-vector interaction and achieve an F1- score of 82.4%. Zeng et al. (2014) treat a sentence as sequential data and exploit the convolutional neural network (CNN); they also integrate word position information into their model. dos Santos et al. (2015) design a model called CR-CNN; they propose a ranking-based cost function and elaborately diminish the impact of the Other class, which is not counted in the official F1-measure. In this way, they achieve the state-of-the-art result with the F1- score of 84.1%. Without such special treatment, their F1-score is 82.7%. Yu et al. (2014) propose a Feature-rich Compositional Embedding Model (FCM) for relation classification, which combines unlexicalized linguistic contexts and word embeddings. They achieve an F1-score of 83.0%. Our proposed SDT-LSTM model yields an F1- score of 83.7%. It outperforms existing competing approaches, in a fair condition of softmax with cross-entropy error. It is worth to note that we have also conducted two controlled experiments: (1) Traditional RNN without LSTM units, achieving an F1-score of 82.8%; (2) LSTM network over the entire dependency path (instead of two sub-paths), achieving an F1-scor</context>
</contexts>
<marker>Yu, Gormley, Dredze, 2014</marker>
<rawString>Mo Yu, Matthew Gormley, and Mark Dredze. 2014. Factor-based compositional embedding models. In NIPS Workshop on Learning Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Zaremba</author>
<author>Ilya Sutskever</author>
</authors>
<title>Learning to execute. arXiv preprint arXiv:1410.4615.</title>
<date>2014</date>
<contexts>
<context position="17113" citStr="Zaremba and Sutskever (2014)" startWordPosition="2672" endWordPosition="2675"> backpropagation. If the propagation sequence (path) is too long, the gradient may probably either grow, or decay, exponentially, depending on the magnitude of Wrec. This leads to the difficulty of training. Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed in the literature. We adopt in our method a variant introduced by Zaremba and Sutskever (2014), also used in Zhu et al. (2015). Concretely, the LSTM-based recurrent neural network comprises four components: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct (depicted in Figure 3 and formalized through Equations 1–6 as bellow). The three adaptive gates it, ft, and ot depend on the previous state ht−1 and the current input xt (Equations 1–3). An extracted feature vector gt is also computed, by Equation 4, serving as the candidate memory cell. it = Q(Wi·xt + Ui·ht−1 + bi) ft = Q(Wf ·xt + Uf ·ht−1 + bf) ot = Q(Wo·xt + Uo·ht−1 + bo) gt = tanh(Wg ·xt + Ug·ht−1 + bg) </context>
</contexts>
<marker>Zaremba, Sutskever, 2014</marker>
<rawString>Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="2180" citStr="Zelenko et al., 2003" startWordPosition="310" endWordPosition="313">wering (Yao and Van Durme, 2014), medical informatics (Wang and Fan, 2014), ontology learning (Xu et al., 2014), etc. The aim of relation classification is to categorize into predefined classes the relations between pairs of marked entities in given texts. For instance, in the sentence “A trillion gallons of [water],, have been poured into an empty [region],, of outer ∗Corresponding authors. space,” the entities water and region are of relation Entity-Destination(e1, e2). Traditional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen. The latter approach, on the other hand, depends largely on the designed kernel, which summarizes all data information. Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al., 2014; dos Santos et al., 2015). However, human engineering—that is, incorporating human knowledge to the network’s architect</context>
<context position="7495" citStr="Zelenko et al. (2003)" startWordPosition="1146" endWordPosition="1149">ing information. Syntactic features include chunking, parse trees, etc. Semantic features are exemplified by the concept hierarchy, entity class, entity mention. Kambhatla (2004) uses a maximum entropy model to combine these features for relation classification. However, different sets of handcrafted features are largely complementary to each other (e.g., hypernyms versus named-entity tags), and thus it is hard to improve performance in this way (GuoDong et al., 2005). Kernel-based approaches specify some measure of similarity between two data samples, without explicit feature representation. Zelenko et al. (2003) compute the similarity of two trees by utilizing their common subtrees. Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation classification. Its main idea is that the relation strongly relies on the dependency path between two given entities. Wang (2008) provides a systematic analysis of several kernels and show that relation extraction can benee2 1786 fit from combining convolution kernel and syntactic features. Plank and Moschitti (2013) introduce semantic information into kernel methods in addition to considering structural information only. One potential diffic</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daojian Zeng</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
</authors>
<title>Relation classification via convolutional deep neural network.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>2335--2344</pages>
<contexts>
<context position="2660" citStr="Zeng et al., 2014" startWordPosition="388" endWordPosition="391">ional relation classification approaches rely largely on feature representation (Kambhatla, 2004), or kernel design (Zelenko et al., 2003; Bunescu and Mooney, 2005). The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen. The latter approach, on the other hand, depends largely on the designed kernel, which summarizes all data information. Deep neural networks, emerging recently, provide a way of highly automatic feature learning (Bengio et al., 2013), and have exhibited considerable potential (Zeng et al., 2014; dos Santos et al., 2015). However, human engineering—that is, incorporating human knowledge to the network’s architecture—is still important and beneficial. This paper proposes a new neural network, SDP-LSTM, for relation classification. Our model utilizes the shortest dependency path (SDP) between two entities in a sentence; we also design a long short term memory (LSTM)-based recurrent neural network for information processing. The neural architecture is mainly inspired by the following observations. • Shortest dependency paths are informative (Fundel et al., 2007; Chen et al., 2014). To d</context>
<context position="8805" citStr="Zeng et al. (2014)" startWordPosition="1343" endWordPosition="1346">unction (similarity measure), and thus designing an effective kernel becomes crucial. Deep neural networks, emerging recently, can learn underlying features automatically, and have attracted growing interest in the literature. Socher et al. (2011) propose a recursive neural network (RNN) along sentences’ parse trees for sentiment analysis; such model can also be used to classify relations (Socher et al., 2012). Hashimoto et al. (2013) explicitly weight phrases’ importance in RNNs to improve performance. Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. Zeng et al. (2014) explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. (2015) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8. In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include information extraction from Web documents in a semi-supervised manner (Bunescu and Mooney, 2007; Banko et al., 2007), dealing with small datasets without enough labels b</context>
<context position="25219" citStr="Zeng et al. (2014)" startWordPosition="4035" endWordPosition="4038">t in LSTM networks. 4.3 Results Table 4 compares our SDT-LSTM with other stateof-the-art methods. The first entry in the table presents the highest performance achieved by traditional feature engineering. Hendrickx et al. (2009) leverage a variety of handcrafted features, and use SVM for classification; they achieve an F1-score of 82.2%. Neural networks are first used in this task in Socher et al. (2012). They build a recursive neural network (RNN) along a constituency tree for relation classification. They extend the basic RNN with matrix-vector interaction and achieve an F1- score of 82.4%. Zeng et al. (2014) treat a sentence as sequential data and exploit the convolutional neural network (CNN); they also integrate word position information into their model. dos Santos et al. (2015) design a model called CR-CNN; they propose a ranking-based cost function and elaborately diminish the impact of the Other class, which is not counted in the official F1-measure. In this way, they achieve the state-of-the-art result with the F1- score of 84.1%. Without such special treatment, their F1-score is 82.7%. Yu et al. (2014) propose a Feature-rich Compositional Embedding Model (FCM) for relation classification,</context>
</contexts>
<marker>Zeng, Liu, Lai, Zhou, Zhao, 2014</marker>
<rawString>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335–2344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Parinaz Sobhani</author>
<author>Hongyu Guo</author>
</authors>
<title>Long short-term memory over tree structures.</title>
<date>2015</date>
<booktitle>In Proceedings of The 32nd International Conference on Machine Learning,</booktitle>
<pages>1604--1612</pages>
<contexts>
<context position="17145" citStr="Zhu et al. (2015)" startWordPosition="2679" endWordPosition="2682">ce (path) is too long, the gradient may probably either grow, or decay, exponentially, depending on the magnitude of Wrec. This leads to the difficulty of training. Long short term memory (LSTM) units are proposed in Hochreiter (1998) to overcome this problem. The main idea is to introduce an adaptive gating mechanism, which decides the degree to which LSTM units keep the previous state and memorize the extracted features of the current data input. Many LSTM variants have been proposed in the literature. We adopt in our method a variant introduced by Zaremba and Sutskever (2014), also used in Zhu et al. (2015). Concretely, the LSTM-based recurrent neural network comprises four components: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct (depicted in Figure 3 and formalized through Equations 1–6 as bellow). The three adaptive gates it, ft, and ot depend on the previous state ht−1 and the current input xt (Equations 1–3). An extracted feature vector gt is also computed, by Equation 4, serving as the candidate memory cell. it = Q(Wi·xt + Ui·ht−1 + bi) ft = Q(Wf ·xt + Uf ·ht−1 + bf) ot = Q(Wo·xt + Uo·ht−1 + bo) gt = tanh(Wg ·xt + Ug·ht−1 + bg) The current memory cell ct is a </context>
</contexts>
<marker>Zhu, Sobhani, Guo, 2015</marker>
<rawString>Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long short-term memory over tree structures. In Proceedings of The 32nd International Conference on Machine Learning, pages 1604–1612.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
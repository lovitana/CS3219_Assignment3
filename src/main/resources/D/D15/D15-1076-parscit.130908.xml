<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9991195">
Question-Answer Driven Semantic Role Labeling:
Using Natural Language to Annotate Natural Language
</title>
<author confidence="0.998542">
Luheng He Mike Lewis Luke Zettlemoyer
</author>
<affiliation confidence="0.995242">
Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.907195">
Seattle, WA
</address>
<email confidence="0.999458">
{luheng,mlewis,lsz}@cs.washington.edu
</email>
<sectionHeader confidence="0.993912" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998724">
This paper introduces the task of question-
answer driven semantic role labeling
(QA-SRL), where question-answer pairs
are used to represent predicate-argument
structure. For example, the verb “intro-
duce” in the previous sentence would be
labeled with the questions “What is in-
troduced?”, and “What introduces some-
thing?”, each paired with the phrase from
the sentence that gives the correct answer.
Posing the problem this way allows the
questions themselves to define the set of
possible roles, without the need for prede-
fined frame or thematic role ontologies. It
also allows for scalable data collection by
annotators with very little training and no
linguistic expertise. We gather data in two
domains, newswire text and Wikipedia
articles, and introduce simple classifier-
based models for predicting which ques-
tions to ask and what their answers should
be. Our results show that non-expert anno-
tators can produce high quality QA-SRL
data, and also establish baseline perfor-
mance levels for future work on this task.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999099666666666">
Semantic role labeling (SRL) is the widely stud-
ied challenge of recovering predicate-argument
structure for natural language words, typically
verbs. The goal is to determine “who does what to
whom,” “when,” and “where,” etc. (Palmer et al.,
2010; Johansson and Nugues, 2008). However,
this intuition is difficult to formalize and funda-
mental aspects of the task vary across efforts, for
example FrameNet (Baker et al., 1998) models a
large set of interpretable thematic roles (AGENT,
PATIENT, etc.) while PropBank (Palmer et al.,
2005) uses a small set of verb-specific roles
</bodyText>
<figureCaption confidence="0.9089245">
Figure 1: QA-SRL annotations for a Wikipedia
sentence.
</figureCaption>
<bodyText confidence="0.999767">
(ARG0, ARG1, etc.). Existing task definitions can
be complex and require significant linguistic ex-
pertise to understand,1 causing challenges for data
annotation and use in many target applications.
In this paper, we introduce a new question-
answer driven SRL task formulation (QA-SRL),
which uses question-answer pairs to label verbal
predicate-argument structure. For example, for the
sentence in Figure 1, we can ask a short ques-
tion containing a verb, e.g. “Who finished some-
thing?”, and whose answer is a phrase from the
original sentence, in this case “UCD.” The answer
tells us that “UCD” is an argument of “finished,”
while the question provides an indirect label on
the role that “UCD” plays. Enumerating all such
pairs, as we will see later, provides a relatively
complete representation of the original verb’s ar-
guments and modifiers.
The QA-SRL task formulation has a number of
advantages. It can be easily explained to non-
expert annotators with a short tutorial and a few
examples. Moreover, the formulation does not
depend on any pre-defined inventory of semantic
roles or frames, or build on any existing gram-
</bodyText>
<footnote confidence="0.99506575">
1The PropBank annotation guide is 89 pages (Bonial et
al., 2010), and the FrameNet guide is 119 pages (Ruppen-
hofer et al., 2006). Our QA-driven annotation instructions
are 5 pages.
</footnote>
<page confidence="0.956701">
643
</page>
<note confidence="0.985251">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.996362366666667">
mar formalisms. Nonetheless, as we will show, it
still represents the argument and modifier attach-
ment decisions that have motivated previous SRL
definitions, and which are of crucial importance
for semantic understanding in a range of NLP
tasks, such as machine translation (Liu and Gildea,
2010) and coreference resolution (Ponzetto and
Strube, 2006). The annotations also, perhaps sur-
prisingly, capture other implicit arguments that
cannot be read directly off of the syntax, as was re-
quired for previous SRL approaches. For example,
in “It was his mother’s birthday, so he was going
to play her favorite tune”, annotators created the
QA pair “When would someone play something?
His mother’s birthday” which describes an im-
plicit temporal relation. Finally, QA-SRL data can
be easily examined, proofread, and improved by
anyone who speaks the language and understands
the sentence; we use natural language to label the
structure of natural language.
We present a scalable approach for QA-SRL an-
notation and baseline models for predicting QA
pairs. Given a sentence and target word (the verb),
we ask annotators to provide as many question-
answer pairs as possible, where the question
comes from a templated space of wh-questions2
and the answer is a phrase from the original sen-
tence. This approach guides annotators to quickly
construct high quality questions within a very
large space of possibilities. Given a corpus of QA-
SRL annotated sentences, we also train baseline
classifiers for both predicting a set of questions to
ask, and what their answers should be. The ques-
tion generation aspect of QA-SRL is unique to our
formulation, and corresponds roughly to identify-
ing what semantic role labels are present in pre-
vious formulations of the task. For example, the
question “Who finished something” in Figure 1
corresponds to the AGENT role in FrameNet. Ta-
ble 1 also shows examples of similar correspon-
dences for PropBank roles. Instead of pre-defining
the labels, as done in previous work, the questions
themselves define the set of possibilities.
Experiments demonstrate high quality data an-
notation with very little annotator training and es-
tablish baseline performance levels for the task.
We hired non-expert, part-time annotators on Up-
work (previously oDesk) to label over 3,000 sen-
tences (nearly 8,000 verbs) across two domains
2Questions starting with a wh-word, such as who, what,
when, how, etc.
(newswire and Wikipedia) at a cost of approxi-
mately $0.50 per verb. We show that the data is
high quality, rivaling PropBank in many aspects
including coverage, and easily gathered in non-
newswire domains.3 The baseline performance
levels for question generation and answering re-
inforce the quality of the data and highlight the
potential for future work on this task.
In summary, our contributions are:
</bodyText>
<listItem confidence="0.954276769230769">
• We introduce the task of question-answer
driven semantic role labeling (QA-SRL), by
using question-answer pairs to specify ver-
bal arguments and the roles they play, without
predefining an inventory of frames or seman-
tic roles.
• We present a novel, lightweight template-
based scheme (Section 3) that enables the
high quality QA-SRL data annotation with
very little training and no linguistic expertise.
• We define two new QA-SRL sub-tasks, ques-
tion generation and answer identification, and
present baseline learning approaches for both
</listItem>
<bodyText confidence="0.748910333333333">
(Sections 4 and 5). The results demonstrate
that our data is high-quality and supports the
study of better learning algorithms.
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999809">
The success of syntactic annotation projects such
as the Penn Treebank (Marcus et al., 1993) has led
to numerous efforts to create semantic annotations
for large corpora. The major distinguishing fea-
tures of our approach are that it is not tied to any
linguistic theory and that it can be annotated by
non-experts with minimal training.
Existing SRL task formulations are closely re-
lated to our work. FrameNet (Baker et al., 1998)
contains a detailed lexicon of verb senses and the-
matic roles. However, this complexity increases
the difficulty of annotation. While the FrameNet
project is decades old, the largest fully anno-
tated corpus contains about 3,000 sentences (Chen
et al., 2010). We were able to annotate over
3,000 sentences within weeks. PropBank (Kings-
bury and Palmer, 2002), NomBank (Meyers et al.,
2004) and OntoNotes (Hovy et al., 2006) circum-
vent the need for a large lexicon of roles, by defin-
</bodyText>
<footnote confidence="0.99856875">
3Our hope is that this approach will generalize not only
across different domains in English, as we show in this paper,
but also to other languages. We will leave those explorations
to future work.
</footnote>
<page confidence="0.995533">
644
</page>
<subsectionHeader confidence="0.825221">
Sentence CoNLL-2009 QA-SRL
</subsectionHeader>
<bodyText confidence="0.92644805">
(1) Stock-fund managers, meantime , A0 they Who had held something? Stock-fund managers / they
went into October with less cash on AM-TMP year When had someone held something? earlier this year
hand than they held earlier this year. What had someone held? less cash on hand
Where had someone held something? on hand
(2) Mr. Spielvogel added pointedly: “ A0 Spielvogel Who added something? Mr. Spielvogel
The pressure on commissions did n’t A1 did What was added? “ The pressure on commissions did n’t
begin with Al Achenbaum . ” begin with Al Achenbaum . ”
AM-MNR pointedly How was something added? pointedly
(3) Even a federal measure in June A0 houses What added something? houses
allowing houses to add research fees to A1 fees What was added? research fees
their commissions did n’t stop it . A2 to
When was something added? June
(4) This year, Mr. Wathen says the firm A0 firm Who will service something? the firm
will be able to service debt and still A1 debt What will be serviced? debt
turn a modest profit. When will something be serviced? this year
(5) Clad in his trademark black velvet A0 he Who would play something? the soft - spoken clarinetist / he
suit, the soft - spoken clarinetist A1 tune What would be played? her favorite tune from the record
announced that ... and that it was his When would someone play something? his mother ’s birthday
mother ’s birthday , so he was going to
play her favorite tune from the record.
</bodyText>
<tableCaption confidence="0.927692">
Table 1: Comparison between CoNLL-2009 relations and QA-SRL annotations. While closely related
</tableCaption>
<bodyText confidence="0.931896">
to PropBank predicate-argument relations, QA pairs also contain information about within-sentence co-
reference (Ex 3, 5, 6, 9), implicit or inferred relations (Ex 4, 7, 8, 9) and roles that are not defined in
PropBank (Ex 1, 5). Annotation mistakes are rare, but for example include missing pronouns (Ex 5) and
prepositional attachment errors (Ex 6).
</bodyText>
<figure confidence="0.906303708333333">
(6) He claimed losses totaling $ 42,455
– and the IRS denied them all.
(7) The consumer - products and
newsprint company said net rose to $
108.8 million, or $ 1.35 a share, from
$ 90.5 million, or $ 1.12 a share, a
year ago.
A0 IRS Who denied something? IRS
A1 them What was denied? losses / them
A1 net What rose? net
A3 $/ago What did something rise from? $ 90.5 million, or $ 1.12 a share
A4 to What did something rise to? $ 108.8 million, or $ 1.35 a share
When did something rise? a year ago
(8) Mr. Agnew was vice president of A0 he Who resigned from something? Mr. Agnew
the U.S. from 1969 until he resigned in AM-TMP in When did someone resign from some- 1973
1973 . thing?
What did someone resign from? vice president of the U.S.
(9) Mr. Gorbachev badly needs a A0 Gorbachev Who needs something? Gorbachev/he
diversion from the serious economic A1 diversion What does someone need? a diversion from the serious economic
problems and ethnic unrest he faces at problems and ethnic unrest he faces at
home. home
AM-ADV badly How does someone need something? badly
What does someone need something
from?
</figure>
<bodyText confidence="0.994953113636363">
the serious economic problems and eth-
nic unrest he faces at home .
ing the core semantic roles in a predicate-specific
manner. This means that frames need to be created
for every verb, and it requires experts to distin-
guish between different senses and different roles.
Our work is also related to recent, more gen-
eral semantic annotation efforts. Abstract Mean-
ing Representation (Banarescu et al., 2013) can
be viewed as an extension of PropBank with ad-
ditional semantic information. Sentences take 8-
13 minutes to annotate—which is slower than
ours, but the annotations are more detailed. Uni-
versal Cognitive Conceptual Annotation (UCCA)
(Abend and Rappoport, 2013) is an attempt to cre-
ate a linguistically universal annotation scheme by
using general labels such as argument or scene.
The UCCA foundational layer does not distin-
guish semantic roles, so Frogs eat herons and
Herons eat frogs will receive identical annotation
— thereby discarding information which is po-
tentially useful for translation or question answer-
ing. They report similar agreement with Prop-
Bank to our approach (roughly 90%), but an-
notator training time was an order-of-magnitude
higher (30-40 hours). The Groningen Meaning
Bank (Basile et al., 2012) project annotates text by
manually correcting the output of existing seman-
tic parsers. They show that some annotation can
be crowdsourced using “games with a purpose”
— however, this does not include its predicate-
argument structure, which requires expert knowl-
edge of their syntactic and semantic formalisms.
Finally, Reisinger et al. (2015) study crowdsourc-
ing semantic role labels based on Dowty’s proto-
roles, given gold predicate and argument men-
tions. This work directly complements our focus
on labeling predicate-argument structure.
The idea of expressing the meaning of natural
language in terms of natural language is related
to natural logic (MacCartney and Manning, 2007),
in which they use natural language for logical in-
ference. Similarly, we model predicate-argument
structure of a sentence with a set of question-
</bodyText>
<page confidence="0.996397">
645
</page>
<table confidence="0.5526103">
Field Description Example of Values No. Values
WH* Question words (wh-words) who, what, when, where, why, how, how much 7
AUX Auxiliary verbs is, have, could, is n’t 36
SBJ Place-holding words for the subject position someone, something 2
TRG* Some form of the target word built, building, been built ≈ 12
OBJ1 Place-holding words for the object position someone, something 2
PP Frequent prepositions (by, to, for, with, about)
and prepositions (unigrams or bigrams) that oc-
cur in the sentence
to, for, from, by ≈ 10
</table>
<tableCaption confidence="0.6286029">
OBJ2 Similar to OBJ1, but with more options someone, something, do, do something, doing 9
something
Table 2: Fields in our question annotation template, with descriptions, example values, and the total
number of possible values for each. WH* and TRG* are required; all other fields can be left empty.
WH* AUX SBJ TRG* OBJ1 PP OBJ2
Who built something ?
What had someone said ?
What was someone expected to do ?
Where might something rise from ?
Table 3: Four example questions written with our question annotation template.
</tableCaption>
<bodyText confidence="0.9995806875">
answer pairs. While existing work on natural logic
has relied on small entailment datasets for train-
ing, our method allows practical large-scale anno-
tation of training data.
Parser evaluation using textual entailment
(Yuret et al., 2010) is a method for evaluating syn-
tactic parsers based on entailment examples. In
a similar spirit to our work, they abstract away
from linguistic formalisms by using natural lan-
guage inference. We focus on semantic rather
than syntactic annotation, and introduce a scal-
able method for gathering data that allows both
training and evaluation. Stern and Dagan (2014)
applied textual entailment to recognize implicit
predicate-argument structure that are not explicitly
expressed in syntactic structure.
</bodyText>
<sectionHeader confidence="0.966373" genericHeader="method">
3 QA-based Semantic Dataset
</sectionHeader>
<bodyText confidence="0.9999885">
This section describes our annotation process in
more detail, and discusses agreement between our
annotations and PropBank. Table 1 shows exam-
ples provided by non-expert annotators.4
</bodyText>
<subsectionHeader confidence="0.999438">
3.1 Annotation Task Design
</subsectionHeader>
<bodyText confidence="0.999981166666667">
We annotate verbs with pairs of questions and an-
swers that provide information about predicate-
argument structure. Given a sentence s and a ver-
bal predicate v in the sentence, annotators must
produce a set of wh-questions that contain v and
whose answers are phrases in s.
</bodyText>
<footnote confidence="0.803605">
4Our dataset is freely available at:
https://dada.cs.washington.edu/qasrl.
</footnote>
<bodyText confidence="0.99996972">
To speed annotation and simplify downstream
processing, we define a small grammar over possi-
ble questions. The questions are constrained with
a template with seven fields, q E WH x AUX x
SBJ x TRG x OBJ1 x PP x OBJ2, each asso-
ciated with a list of possible options. Descriptions
for each field are shown in Table 2. The gram-
mar is sufficiently general to capture a wide-range
of questions about predicate-argument structure—
some examples are given in Table 3.
The precise form of the question template is a
function of the verb v and sentence s, for two of
the fields. For the TRG field, we generate a list of
inflections forms of v using the Wiktionary dictio-
nary. For the PP field, the candidates are all the
prepositions that occurred in the sentence s, and
some frequently-used prepositions - by, to, for,
with, and about. We also include preposition bi-
grams (e.g., out for) from s.
Answers are constrained to be a subset of the
words in the sentence but do not necessarily have
to be contiguous spans. We also allow questions to
have multiple answers, which is useful for annotat-
ing graph structured dependencies such as those in
examples 3 and 6 in Table 1.
</bodyText>
<subsectionHeader confidence="0.99885">
3.2 Data Preparation
</subsectionHeader>
<bodyText confidence="0.992245666666667">
We annotated over 3000 sentences (nearly 8,000
verbs) in total across two domains: newswire
(PropBank) and Wikipedia. Table 4 shows the
full data statistics. In the newswire domain,
we sampled sentences from the English training
data of CoNLL-2009 shared task (Hajiˇc et al.,
</bodyText>
<page confidence="0.997204">
646
</page>
<table confidence="0.999642571428571">
Dataset Sentences Verbs QAs
newswire-train 744 2020 4904
newswire-dev 249 664 1606
newswire-test 248 652 1599
Wikipedia-train 1174 2647 6414
Wikipedia-dev 392 895 2183
Wikipedia-test 393 898 2201
</table>
<tableCaption confidence="0.999612">
Table 4: Annotated data statistics.
</tableCaption>
<bodyText confidence="0.998931875">
2009), excluding questions and sentences with
fewer than 10 words. For the Wikipedia do-
main, we randomly sampled sentences from the
English Wikipedia, excluding questions and sen-
tences with fewer than 10 or more than 60 words.
In each sentence, we need to first identify
the candidates for verbal predicates. In princi-
ple, a separate stage of annotation could iden-
tify verbs—but for simplicity, we instead used
POS-tags. We used gold POS-tags for newswire,
and predicted POS-tags (using Stanford tagger
(Toutanova et al., 2003)) in Wikipedia. Annota-
tors can choose to skip a candidate verb if they
are unable to write questions for it. Annotators
skipped 136 verbs (3%) in Wikipedia data and 50
verbs (1.5%) in PropBank data.
</bodyText>
<subsectionHeader confidence="0.987501">
3.3 Annotation Process
</subsectionHeader>
<bodyText confidence="0.99998">
For annotation, we hired 10 part-time, non-expert
annotators from Upwork (previously oDesk) and
paid $10 per hour for their work. The average
cost was $0.58 per verb ($1.57 per sentence) for
newswire text and $0.45 per verb ($1.01 per sen-
tence) on the Wikipedia domain. The annotators
are given a short tutorial and a small set of sam-
ple annotations (about 10 sentences). Annotators
were hired if they showed good understanding of
English and our task. The entire screening process
usually took less than 2 hours.
Writing QA pairs for each sentence takes 6 min-
utes on average for Wikipedia and 9 minutes on
newswire, depending on the length and complex-
ity of the sentence and the domain of the text.
</bodyText>
<subsectionHeader confidence="0.7254255">
3.4 Agreement with Gold PropBank Data
(CoNLL-2009)
</subsectionHeader>
<bodyText confidence="0.9994875">
PropBank is the most widely used annotation of
predicate-argument structure. While our anno-
tation captures different information from Prop-
Bank, it is closely related. To investigate the sim-
ilarity between the annotation schemes, we mea-
sured the overlap between the newswire domain
</bodyText>
<table confidence="0.999487">
All Roles Core Adjuncts
Precision 81.4 85.9 59.9
Recall 86.3 89.8 63.6
</table>
<tableCaption confidence="0.990914">
Table 5: Agreement with gold PropBank (CoNLL-
</tableCaption>
<bodyText confidence="0.9899515">
2009) for all roles, core roles, and adjuncts. Preci-
sion is the percentage of QA pairs covering exactly
one PropBank relation. Recall is the percentage
of PropBank relations covered by exactly one QA
pair.
(1241 sentences) of our QA-SRL dataset and the
PropBank dataset.
For each PropBank predicate that we have an-
notated with our scheme, we compute the agree-
ment between the PropBank arguments and the
QA-SRL answers. We ignore modality, reference,
discourse and negation roles, as they are outside
the scope of our current annotation. An annotated
answer is judged to match the PropBank argument
if either (1) the gold argument head is within the
annotated answer span, or (2) the gold argument
head is a preposition and at least one of its chil-
dren is within the answer span.
We measure the macro-averaged precision and
recall of our annotation against PropBank, with
the proportion of our QA-pairs that are match a
PropBank relation, and the proportion of Prop-
Bank relations covered by our annotation. The re-
sults are shown in Table 5, and demonstrate high
overall agreement with PropBank. Agreement for
core arguments 5 is especially strong, showing
much of the expert linguist annotation in Prop-
Bank can be recovered with our simple scheme.
Agreement for adjuncts is lower, because the an-
notated QAs often contain inferred roles, espe-
cially for why, when and where questions (See ex-
amples 4, 7 and 8 in Table 1). These inferred roles
are typically correct, but outside of the scope of
PropBank annotations; they point to exciting op-
portunities for future work with QA-SRL data. On
the other hand, the adverbial arguments in Prop-
Bank are sometimes neglected by annotators, thus
becoming a major source of recall loss.
Table 6 shows the overlap between our anno-
tated question words and PropBank argument la-
bels. There are many unsurprising correlations—
who questions are strongly associated with Prop-
</bodyText>
<footnote confidence="0.996485">
5In PropBank, A0-A5 are the core arguments. In QA-
SRL, the core arguments include QA pairs with a question
that starts with Who or What.
</footnote>
<page confidence="0.983224">
647
</page>
<figure confidence="0.9880659">
Average Number of QA Pairs vs. Number of Annotators on Newswire Data
Number of Annotators
Average Number of QA Pairs vs. Number of Annotators on Wikipedia Data
Number of Annotators
Average Number of QA Pairs per Predicate
2.8
2.6
2.4
2.2
3.6
3.4
3.2
2
3
2 3 4 5
Average Number of QA Pairs
Average Number of Agreed QA Pairs
Average Number of QA Pairs per Predicate
2.8
2.6
2.4
2.2
3.6
3.4
3.2
2
3
1 2 3 4 5
Average Number of QA Pairs
Average Number of Agreed QA Pairs
</figure>
<figureCaption confidence="0.999363333333333">
Figure 2: Inter-annotator agreement measured on 100 newswire sentences and 108 Wikipedia sentences,
comparing the total number of annotators to the number of unique QA pairs produced and the number of
agreed pairs. A pair is considered agreed if two or more annotators produced it.
</figureCaption>
<table confidence="0.99991205882353">
WHO WHAT WHEN WHEREWHY HOW HOW
MUCH
A0 1575 414 3 5 17 28 2
A1 285 2481 4 25 20 23 95
A2 85 364 2 49 17 51 74
A3 11 62 7 8 4 16 31
A4 2 30 5 11 2 4 30
A5 0 0 0 1 0 2 0
ADV 5 44 9 2 25 27 6
CAU 0 3 1 0 23 1 0
DIR 0 6 1 13 0 4 0
EXT 0 4 0 0 0 5 5
LOC 1 35 10 89 0 13 11
MNR 5 47 2 8 4 108 14
PNC 2 21 0 1 39 7 2
PRD 1 1 0 0 0 1 0
TMP 2 51 341 2 11 20 10
</table>
<tableCaption confidence="0.988015">
Table 6: Co-occurrence of wh-words in QA-SRL
annoations and role labels in PropBank.
</tableCaption>
<bodyText confidence="0.999468076923077">
Bank agents (A0), and where and when ques-
tions correspond to PropBank temporal and loca-
tive roles, respectively. Some types of questions
are divided much more evenly among PropBank
roles, such as How much. These cases show how
our questions can produce a more easily inter-
pretable annotation than PropBank labels, which
are predicate-specific and can be difficult to un-
derstand without reference to the frame files.
Together, these results suggest that non-experts
can annotate much of the information contained in
PropBank, and produce a more easily interpretable
annotation.
</bodyText>
<sectionHeader confidence="0.769097" genericHeader="method">
3.5 Inter-Annotator Agreement
</sectionHeader>
<bodyText confidence="0.99994562962963">
To judge the reliability of the data, we measured
agreement on a portion of the data (100 sentences
in the newswire domain and 108 sentences in the
Wikipedia domain) annotated by five annotators.
Measuring agreement is complicated by the fact
that the same question can be asked in multiple
ways—for example “Who resigned?” and “Who
resigned from something?”—and annotators may
choose different, although usually highly overlap-
ping, answer spans. We consider two QA pairs to
be equivalent if (1) they have the same wh-word
and (2) they have overlapping answer spans. In
this analysis, Who and What are considered to be
the same wh-word.
Figure 2 shows how the number of different
QA pairs (both overall and agreed) increases with
number of annotations. A QA pair is considered to
be agreed upon if it is proposed by at least two of
the five annotators. After five annotators, the num-
ber of agreed QA pairs starts to asymptote. A sin-
gle annotator finds roughly 80% of the agreed QA
pairs that are found by five annotators, suggesting
that high recall can be achieved with a single stage
of annotation. To further improve precision, future
work should explore a second stage of annotation
where annotators check each other’s work, for ex-
ample by answering each other’s questions.
</bodyText>
<sectionHeader confidence="0.985871" genericHeader="method">
4 Question Generation
</sectionHeader>
<bodyText confidence="0.999942833333333">
Given a sentence s and a target verb v, we want
to automatically generate a set of questions con-
taining v that are answerable with phrases from
s. This task is important because generating
answerable questions requires understanding the
predicate-argument structure of the sentence. In
</bodyText>
<page confidence="0.99736">
648
</page>
<bodyText confidence="0.99693325">
essence, questions play the part of semantic roles
in our approach.6
We present a baseline that breaks down ques-
tion generation into two steps: (1) we first use a
classifier to predict a set of roles for verb v that
are likely present in the sentence, from a small,
heuristically defined set of possibilities and then
(2) generate one question for each predicted role,
using templates extracted from the training set.
Mapping Question Fields to Semantic Roles
To generate questions, we first have to decide the
primary role we want to target; each question’s an-
swer is associated with a specific semantic role.
For example, given the sentence UCD finished the
2006 championship and target verb finished, we
could ask either: (Q1) Who finished something?
or (Q2) What did someone finish?. Q1 targets the
role associated with the person doing the finish-
ing, while Q2 focuses on the thing being finished.
To generate high quality questions, it is also often
necessary to refer to roles other than the primary
role, with pronouns. For example, Q2 uses “some-
one” to refer to the finisher.
Although it is difficult to know a priori the ideal
set of possible roles, our baseline uses a simple
discrete set, and introduces heuristics for identi-
fying the roles a question refers to. The roles R
include:
</bodyText>
<equation confidence="0.726016">
R ={R0, R1, R2, R2[p], w, w[p]}
w E{Where, When, Why, How, HowMuch}
p EPrepositions
</equation>
<bodyText confidence="0.999983642857143">
We then normalize the annotated questions by
mapping its fields WH, SBJ, OBJ1 and OBJ2 to
the roles r E R, using a small set of rules listed
in Table 7. In our example, the WH field of the
Q1 (Who) and the SBJ of Q2 (someone) are both
mapped to role R0. The WH of Q2 (What) and the
OBJ1 of Q1 (something) are mapped to role R1.
Some roles can be subclassed with prepositions.
For example, the WH field of the question What
did something rise from? is mapped to R2[from].
In most cases, R0 is related to the A0/AGENT
roles in PropBank/FrameNet, and R1/R2 are re-
lated to A1/PATIENT roles. Since our questions
are defined in a templated space, we are able to do
</bodyText>
<footnote confidence="0.6494978">
6The task also has applications to semi-automatic annota-
tion of sentences with our scheme, if we could generate ques-
tions with high enough recall and only require annotators to
provide all the answers. We leave this important direction to
future work.
</footnote>
<table confidence="0.99909535">
wh E {Who, Whatl h voice = active
WH — R0 WH — R1 WH — R2[p]
SBJ = ¢ SBJ — R0 SBJ — R0
OBJ1 — R1 OBJ1 = ¢ OBJ1 — R1
OBJ2 — R2[p] OBJ2 — R2[p] OBJ2 = ¢
wh E {Who, Whatl h voice = passive
WH — R1 WH — R2
SBJ = ¢ SBJ — R1
OBJ1 — R2 OBJ1 = ¢
OBJ2 — R2[p] OBJ2 — R2[p]
wh E {When, Where, Why, How, HowMuchl h voice = active
WH — wh[p] WH — wh
SBJ — R0 SBJ — R0
OBJ1 — R1 OBJ1 — R1
OBJ2 = ¢ OBJ2 — R2[p]
wh E {When, Where, Why, How, HowMuchl h voice = passive
WH — wh[p] WH — wh
SBJ — R1 SBJ — R1
OBJ1 — R2 OBJ1 — R2
OBJ2 = ¢ OBJ2 — R2[p]
</table>
<tableCaption confidence="0.995759">
Table 7: Mapping question fields to roles in R.
</tableCaption>
<bodyText confidence="0.943143193548387">
The mapping is based on whether certain question
fields are empty and the voice of the verb in the
question (active or passive). O indicates that a field
is either an empty string or equals “do/doing”. If
a question is in passive voice and contains the
preposition “by”, then OBJ2 is tagged with R0 in-
stead, as in What is built by someone?
this mapping heuristically with reasonable accu-
racy. In the future, we might try to induce the set
of possible roles given each target verb, follow-
ing the semantic role induction work of Titov and
Klementiev (2012) and Lang and Lapata (2011),
or use crowdsourcing to label proto-roles, follow-
ing Reisinger et al. (2015).
Predicting Question Roles Given this space of
possible roles, our first step in generation is to de-
termine which roles are present in a sentence, and
select the pronouns that could be used to refer to
them in the resulting questions. We formulate this
task as a supervised multi-label learning problem.
We define the set of possible labels L by combin-
ing the roles in R with different pronoun values:
L ={role:val  |role E R}
val E{O, someone, something, do something,
doing something}
For example, to support the generation of the
questions Who finished something? and What did
someone finish?, we need to first predict the labels
R0:someone and R1:something. Adjunct roles,
such as When and How, always take an empty pro-
noun value.
</bodyText>
<page confidence="0.997692">
649
</page>
<table confidence="0.9306366">
Abstract Question
Question
WH SBJ Voice OBJ1 OBJ2
Who finished something? R0 / active R1 /
What did someone finish? R1 R0 active / /
</table>
<tableCaption confidence="0.9340205">
Table 8: Example surface realization templates
from abstract questions.
</tableCaption>
<bodyText confidence="0.999871131578947">
For each sentence s and verb v, the set of posi-
tive training samples corresponds to the set of la-
bels in the annotated questions, and the negative
samples are all the other labels in Ltrai,,, the sub-
set of labels appeared in training data.7 We train a
binary classifier for every label in Ltrai,, using L2-
regularized logistic regression by Liblinear (Fan et
al., 2008), with hyper-parameter C = 0.1. Fea-
tures of the binary classifiers are listed in Table
10. For each sentence s and verb v in the test data,
we take the k highest-scoring labels, and generate
questions from these.
Question Generation After predicting the set of
labels for a verb, we generate a question to query
each role. First, we define the concept of an ab-
stract question, which provides a template that
specifies the role to be queried, other roles to in-
clude in the question, and the voice of the verb.
Abstract questions can be read directly from our
training data.
We can map an abstract question to a sur-
face realization by substituting the slots with
the pronoun values of the predicted labels. Ta-
ble 8 shows the abstract questions we could use
to query roles R0 and R1; and the generated
questions, based on the set of predicted labels
{R0:someone, R1:something}.
Therefore, to generate a question to query a role
r E R, we simply return the most frequent ab-
stract question that occurred in training data that
matches the role being queried, and the set of other
predicted labels.
Experiments Native English speakers manually
evaluated 500 automatically generated questions
(5 questions per verb). Annotators judged whether
the questions were grammatical 8 and answerable
from the sentence.
We evaluated the top k questions produced by
</bodyText>
<footnote confidence="0.989809857142857">
7We pruned the negative samples that contain prepositions
that are not in the sentence or in the set of frequently-used
prepositions (by, to, for, with, about).
8Some automatically generated questions are ungrammat-
ical because of label prediction errors, such as Who sneezed
someone?, where the label R1:someone shouldn’t be pre-
dicted.
</footnote>
<table confidence="0.9725954">
Newswire Wikipedia
Ans. Gram. Ans. Gram.
prec@1 66.0 84.0 72.0 90.0
prec@3 51.3 78.7 53.3 86.0
prec@5 38.4 77.2 40.0 82.0
</table>
<tableCaption confidence="0.930692">
Table 9: Manual evaluation results for question
</tableCaption>
<bodyText confidence="0.9973931">
generation in two domains, including the averaged
number of distinct questions that are answerable
given the sentence (Ans.) and the averaged num-
ber of questions that are grammatical (Gram.).
our baseline technique. The results in Table 9
show that our system is able to produce questions
which are both grammatical and answerable. The
average number of QA pairs per verb collected
by human annotator is roughly 2.5, demonstrating
significant room for improving these results.
</bodyText>
<sectionHeader confidence="0.990532" genericHeader="method">
5 Answer Identification
</sectionHeader>
<bodyText confidence="0.999837838709678">
The goal of the answer identification task is to pre-
dict an answer a given sentence s, target verb v
and a question q. Our annotated answers can be
a series of spans, so the space of all possible an-
swers is 2|s|. To simplify the problem, we trans-
form our span-based answer annotation to answer
head words, thus reducing the answer space to |s|.
We model whether a word is the head of an answer
as a binary classification problem.
Each training sample is a tuple (s, v, q, a, f1).
The answer head a is extracted from the k-best de-
pendency parses and the annotated answer span.
Given a dependency tree, if any word in the an-
notated answer span has a parent coming from
outside the span, then it is considered an answer
head. Therefore, a gold question-answer pair can
be transformed into multiple positive training sam-
ples. The negative samples come from all the
words in the sentence that are not an answer head.
For learning, we train a binary classifier for every
word in the sentence (except for the verb v).
Experiments We use L2-regularized logistic re-
gression by Liblinear (Fan et al., 2008) for binary
classification. Features are listed in Table 10.
The performance of our answer identification
approach is measured by accuracy. For evaluation,
given each test sentence s, verb v and question q,
we output the word with highest predicted score
using the binary classifier. If the predicted word
is contained inside the annotated answer span, it is
considered a correct prediction. We also use the
</bodyText>
<page confidence="0.993327">
650
</page>
<table confidence="0.998508666666667">
Feature Class Question Generation Answer Identification
Predicate Token, Predicted POS-tag, Lemma extracted from Wiktionary
Dependency parent and edge label, dependency children and edge label
Question Question role label, Wh-word, Preposition
Answer Word / Syntactic parent and edge label, Left/Right-most syntactic children,
Predicate-Answer / Relative position (left or right), Syntactic relation, Syntactic path
</table>
<bodyText confidence="0.91290075">
Table 10: Indicator features that are included in our role classifiers for question generation (Section 4)
and the answer identification classifier (Section 5). Many come from previous work in SRL (Johansson
and Nugues, 2008; Xue and Palmer, 2004). To mitigate syntactic errors, we used 10-best dependency
parses from the Stanford parser (Klein and Manning, 2003).
</bodyText>
<table confidence="0.798350333333333">
Newswire Wikipedia
Classifier 78.7 82.3
Random 26.3 26.9
</table>
<tableCaption confidence="0.924005">
Table 11: Answer identification accuracy on
newswire and Wikipedia text.
</tableCaption>
<bodyText confidence="0.998814">
baseline method that predicts a random syntactic
child from the 1-best parse for each question.
In each of the two domains, we train the bi-
nary classifiers on the training set of that domain
(See Table 4 for dataset size). Table 11 shows
experiment results for answer identification. Our
classifier-based method outputs a correct answer
head for 80% of the test questions, establishing a
useful baseline for future work on this task.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="discussions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99996904">
We introduced the task of QA-SRL, where
question-answer pairs are used to specify
predicate-argument structure. We also presented a
scalable annotation approach with high coverage,
as compared to existing SRL resources, and intro-
duced baselines for two core QA-SRL subtasks:
question generation and answering.
Our annotation scheme has a number of advan-
tages. It is low cost, easily interpretable, and can
be performed with very little training and no lin-
guistic expertise. These advantages come, in large
part, from the relatively open nature of the QA-
SRL task, which does not depend on any linguis-
tic theory of meaning or make use of any frame or
role ontologies. We are simply using natural lan-
guage to annotate natural language.
Although we studied verbal predicate-argument
structure, there are significant opportunities for fu-
ture work to investigate annotating nominal and
adjectival predicates. We have also made few
language-specific assumptions, and believe the an-
notation can be generalized to other languages—
a major advantage over alternative annotation
schemes that require new lexicons to be created
for each language.
The biggest challenge in annotating sentences
with our scheme is choosing the questions. We in-
troduced a method for generating candidate ques-
tions automatically, which has the potential to en-
able very large-scale annotation by only asking the
annotators to provide answers. This will only be
possible if performance can be improved to the
point where we achieve high recall question with
acceptable levels of precision.
Finally, future work will also explore applica-
tions of our annotation. Most obviously, the anno-
tation can be used for training question-answering
systems, as it directly encodes question-answer
pairs. More ambitiously, the annotation has the
potential to be used for training parsers. A joint
syntactic and semantic parser, such as that of
Lewis et al. (2015), could be trained directly on
the annotations to improve both the syntactic and
semantic models, for example in domain transfer
settings. Alternatively, the annotation could be
used for active learning: we envisage a scheme
where parsers, when faced with ambiguous attach-
ment decisions, can generate a human-readable
question whose answer will resolve the attach-
ment.
</bodyText>
<sectionHeader confidence="0.997483" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99987775">
This research was supported in part by the NSF
(IIS-1252835), DARPA under the DEFT program
through the AFRL (FA8750-13-2-0019), an Allen
Distinguished Investigator Award, and a gift from
Google. We are grateful to Kenton Lee and
Mark Yatskar for evaluating the question gener-
ation task, and Eunsol Choi, Yejin Choi, Chlo´e
Kiddon, Victoria Lin, and Swabha Swayamdipta
for their helpful comments on the paper. We
would also like to thank our freelance workers on
oDesk/Upwork for their annotation and the anony-
mous reviewers for their valuable feedback.
</bodyText>
<page confidence="0.998363">
651
</page>
<sectionHeader confidence="0.989418" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999865363636364">
Omri Abend and Ari Rappoport. 2013. Universal con-
ceptual cognitive annotation (ucca). In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics, pages 228–238.
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 17th International Conference on Com-
putational Linguistics, volume 1, pages 86–90. As-
sociation for Computational Linguistics.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the Linguistic
Annotation Workshop.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the 2012 In-
ternational Conference on Language Resources and
Evaluation, volume 12, pages 3196–3200.
Claire Bonial, Olga Babko-Malaya, Jinho D Choi, Jena
Hwang, and Martha Palmer. 2010. Propbank an-
notation guidelines. Center for Computational Lan-
guage and Education Research, CU-Boulder.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 264–267. Association for Computational
Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1–18. Associa-
tion for Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association of Compu-
tational Linguistics, pages 57–60. Association for
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of prop-
bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 69–78. Association for Computational
Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 2002 In-
ternational Conference on Language Resources and
Evaluation. Citeseer.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, volume 1, pages 1117–1126.
Association for Computational Linguistics.
Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015.
Joint a* ccg parsing and semantic role labeling. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing.
Ding Liu and Daniel Gildea. 2010. Semantic role
features for machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 716–724. Association for
Computational Linguistics.
Bill MacCartney and Christopher D Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing, pages 193–200. Associa-
tion for Computational Linguistics.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313–330.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In HLT-NAACL 2004 workshop:
Frontiers in corpus annotation, pages 24–31.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1–103.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, wordnet and
wikipedia for coreference resolution. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation of Computational Linguistics, pages 192–199.
Association for Computational Linguistics.
Drew Reisinger, Rachel Rudinger, Francis Ferraro,
Craig Harman, Kyle Rawlins, and Benjamin Van
</reference>
<page confidence="0.979515">
652
</page>
<reference confidence="0.999725114285714">
Durme. 2015. Semantic proto-roles. Transac-
tions of the Association for Computational Linguis-
tics, 3:475–488.
Josef Ruppenhofer, Michael Ellsworth, Miriam RL
Petruck, Christopher R Johnson, and Jan Schef-
fczyk. 2006. Framenet ii: Extended theory and
practice.
Asher Stern and Ido Dagan. 2014. Recognizing im-
plied predicate-argument relationships in textual in-
ference. Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies.
Ivan Titov and Alexandre Klementiev. 2012. A
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 12–22. Association for
Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing, pages 88–94.
Deniz Yuret, Aydin Han, and Zehra Turgut. 2010.
Semeval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
51–56. Association for Computational Linguistics.
</reference>
<page confidence="0.999265">
653
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797465">
<title confidence="0.998499">Question-Answer Driven Semantic Role Using Natural Language to Annotate Natural Language</title>
<author confidence="0.999704">Luheng He Mike Lewis Luke Zettlemoyer</author>
<affiliation confidence="0.9969625">Science University of</affiliation>
<address confidence="0.812168">Seattle,</address>
<abstract confidence="0.999642192307692">This paper introduces the task of questionanswer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure. For example, the verb “introduce” in the previous sentence would be labeled with the questions “What is introduced?”, and “What introduces something?”, each paired with the phrase from the sentence that gives the correct answer. Posing the problem this way allows the questions themselves to define the set of possible roles, without the need for predefined frame or thematic role ontologies. It also allows for scalable data collection by annotators with very little training and no linguistic expertise. We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classifierbased models for predicting which questions to ask and what their answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Omri Abend</author>
<author>Ari Rappoport</author>
</authors>
<title>Universal conceptual cognitive annotation (ucca).</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>228--238</pages>
<contexts>
<context position="11718" citStr="Abend and Rappoport, 2013" startWordPosition="1904" endWordPosition="1907">nrest he faces at home . ing the core semantic roles in a predicate-specific manner. This means that frames need to be created for every verb, and it requires experts to distinguish between different senses and different roles. Our work is also related to recent, more general semantic annotation efforts. Abstract Meaning Representation (Banarescu et al., 2013) can be viewed as an extension of PropBank with additional semantic information. Sentences take 8- 13 minutes to annotate—which is slower than ours, but the annotations are more detailed. Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013) is an attempt to create a linguistically universal annotation scheme by using general labels such as argument or scene. The UCCA foundational layer does not distinguish semantic roles, so Frogs eat herons and Herons eat frogs will receive identical annotation — thereby discarding information which is potentially useful for translation or question answering. They report similar agreement with PropBank to our approach (roughly 90%), but annotator training time was an order-of-magnitude higher (30-40 hours). The Groningen Meaning Bank (Basile et al., 2012) project annotates text by manually corr</context>
</contexts>
<marker>Abend, Rappoport, 2013</marker>
<rawString>Omri Abend and Ari Rappoport. 2013. Universal conceptual cognitive annotation (ucca). In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1710" citStr="Baker et al., 1998" startWordPosition="253" endWordPosition="256">r answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task. 1 Introduction Semantic role labeling (SRL) is the widely studied challenge of recovering predicate-argument structure for natural language words, typically verbs. The goal is to determine “who does what to whom,” “when,” and “where,” etc. (Palmer et al., 2010; Johansson and Nugues, 2008). However, this intuition is difficult to formalize and fundamental aspects of the task vary across efforts, for example FrameNet (Baker et al., 1998) models a large set of interpretable thematic roles (AGENT, PATIENT, etc.) while PropBank (Palmer et al., 2005) uses a small set of verb-specific roles Figure 1: QA-SRL annotations for a Wikipedia sentence. (ARG0, ARG1, etc.). Existing task definitions can be complex and require significant linguistic expertise to understand,1 causing challenges for data annotation and use in many target applications. In this paper, we introduce a new questionanswer driven SRL task formulation (QA-SRL), which uses question-answer pairs to label verbal predicate-argument structure. For example, for the sentence</context>
<context position="7344" citStr="Baker et al., 1998" startWordPosition="1148" endWordPosition="1151">present baseline learning approaches for both (Sections 4 and 5). The results demonstrate that our data is high-quality and supports the study of better learning algorithms. 2 Related Work The success of syntactic annotation projects such as the Penn Treebank (Marcus et al., 1993) has led to numerous efforts to create semantic annotations for large corpora. The major distinguishing features of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th International Conference on Computational Linguistics, volume 1, pages 86–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop.</booktitle>
<contexts>
<context position="11454" citStr="Banarescu et al., 2013" startWordPosition="1864" endWordPosition="1867"> a diversion from the serious economic problems and ethnic unrest he faces at problems and ethnic unrest he faces at home. home AM-ADV badly How does someone need something? badly What does someone need something from? the serious economic problems and ethnic unrest he faces at home . ing the core semantic roles in a predicate-specific manner. This means that frames need to be created for every verb, and it requires experts to distinguish between different senses and different roles. Our work is also related to recent, more general semantic annotation efforts. Abstract Meaning Representation (Banarescu et al., 2013) can be viewed as an extension of PropBank with additional semantic information. Sentences take 8- 13 minutes to annotate—which is slower than ours, but the annotations are more detailed. Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013) is an attempt to create a linguistically universal annotation scheme by using general labels such as argument or scene. The UCCA foundational layer does not distinguish semantic roles, so Frogs eat herons and Herons eat frogs will receive identical annotation — thereby discarding information which is potentially useful for translatio</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the Linguistic Annotation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valerio Basile</author>
<author>Johan Bos</author>
<author>Kilian Evang</author>
<author>Noortje Venhuizen</author>
</authors>
<title>Developing a large semantically annotated corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International Conference on Language Resources and Evaluation,</booktitle>
<volume>12</volume>
<pages>3196--3200</pages>
<contexts>
<context position="12278" citStr="Basile et al., 2012" startWordPosition="1992" endWordPosition="1995">ive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013) is an attempt to create a linguistically universal annotation scheme by using general labels such as argument or scene. The UCCA foundational layer does not distinguish semantic roles, so Frogs eat herons and Herons eat frogs will receive identical annotation — thereby discarding information which is potentially useful for translation or question answering. They report similar agreement with PropBank to our approach (roughly 90%), but annotator training time was an order-of-magnitude higher (30-40 hours). The Groningen Meaning Bank (Basile et al., 2012) project annotates text by manually correcting the output of existing semantic parsers. They show that some annotation can be crowdsourced using “games with a purpose” — however, this does not include its predicateargument structure, which requires expert knowledge of their syntactic and semantic formalisms. Finally, Reisinger et al. (2015) study crowdsourcing semantic role labels based on Dowty’s protoroles, given gold predicate and argument mentions. This work directly complements our focus on labeling predicate-argument structure. The idea of expressing the meaning of natural language in te</context>
</contexts>
<marker>Basile, Bos, Evang, Venhuizen, 2012</marker>
<rawString>Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012. Developing a large semantically annotated corpus. In Proceedings of the 2012 International Conference on Language Resources and Evaluation, volume 12, pages 3196–3200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Bonial</author>
<author>Olga Babko-Malaya</author>
<author>Jinho D Choi</author>
<author>Jena Hwang</author>
<author>Martha Palmer</author>
</authors>
<date>2010</date>
<booktitle>Propbank annotation guidelines. Center for Computational Language and Education Research, CU-Boulder.</booktitle>
<contexts>
<context position="3103" citStr="Bonial et al., 2010" startWordPosition="477" endWordPosition="480"> answer tells us that “UCD” is an argument of “finished,” while the question provides an indirect label on the role that “UCD” plays. Enumerating all such pairs, as we will see later, provides a relatively complete representation of the original verb’s arguments and modifiers. The QA-SRL task formulation has a number of advantages. It can be easily explained to nonexpert annotators with a short tutorial and a few examples. Moreover, the formulation does not depend on any pre-defined inventory of semantic roles or frames, or build on any existing gram1The PropBank annotation guide is 89 pages (Bonial et al., 2010), and the FrameNet guide is 119 pages (Ruppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages. 643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. mar formalisms. Nonetheless, as we will show, it still represents the argument and modifier attachment decisions that have motivated previous SRL definitions, and which are of crucial importance for semantic understanding in a range of NLP tasks, such as machine translation (Liu and Gi</context>
</contexts>
<marker>Bonial, Babko-Malaya, Choi, Hwang, Palmer, 2010</marker>
<rawString>Claire Bonial, Olga Babko-Malaya, Jinho D Choi, Jena Hwang, and Martha Palmer. 2010. Propbank annotation guidelines. Center for Computational Language and Education Research, CU-Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desai Chen</author>
<author>Nathan Schneider</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semafor: Frame argument resolution with log-linear models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>264--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7601" citStr="Chen et al., 2010" startWordPosition="1188" endWordPosition="1191">k (Marcus et al., 1993) has led to numerous efforts to create semantic annotations for large corpora. The major distinguishing features of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work. 644 Sentence CoNLL-2009 QA-SRL (1) Stock-fund managers, meantime , A0 they Who had held something? Stock-fund managers / they went into October with less cash on AM-TMP year When </context>
</contexts>
<marker>Chen, Schneider, Das, Smith, 2010</marker>
<rawString>Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A Smith. 2010. Semafor: Frame argument resolution with log-linear models. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 264–267. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="29337" citStr="Fan et al., 2008" startWordPosition="4968" endWordPosition="4971">, always take an empty pronoun value. 649 Abstract Question Question WH SBJ Voice OBJ1 OBJ2 Who finished something? R0 / active R1 / What did someone finish? R1 R0 active / / Table 8: Example surface realization templates from abstract questions. For each sentence s and verb v, the set of positive training samples corresponds to the set of labels in the annotated questions, and the negative samples are all the other labels in Ltrai,,, the subset of labels appeared in training data.7 We train a binary classifier for every label in Ltrai,, using L2- regularized logistic regression by Liblinear (Fan et al., 2008), with hyper-parameter C = 0.1. Features of the binary classifiers are listed in Table 10. For each sentence s and verb v in the test data, we take the k highest-scoring labels, and generate questions from these. Question Generation After predicting the set of labels for a verb, we generate a question to query each role. First, we define the concept of an abstract question, which provides a template that specifies the role to be queried, other roles to include in the question, and the voice of the verb. Abstract questions can be read directly from our training data. We can map an abstract ques</context>
<context position="32787" citStr="Fan et al., 2008" startWordPosition="5551" endWordPosition="5554"> The answer head a is extracted from the k-best dependency parses and the annotated answer span. Given a dependency tree, if any word in the annotated answer span has a parent coming from outside the span, then it is considered an answer head. Therefore, a gold question-answer pair can be transformed into multiple positive training samples. The negative samples come from all the words in the sentence that are not an answer head. For learning, we train a binary classifier for every word in the sentence (except for the verb v). Experiments We use L2-regularized logistic regression by Liblinear (Fan et al., 2008) for binary classification. Features are listed in Table 10. The performance of our answer identification approach is measured by accuracy. For evaluation, given each test sentence s, verb v and question q, we output the word with highest predicted score using the binary classifier. If the predicted word is contained inside the annotated answer span, it is considered a correct prediction. We also use the 650 Feature Class Question Generation Answer Identification Predicate Token, Predicted POS-tag, Lemma extracted from Wiktionary Dependency parent and edge label, dependency children and edge l</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, et al. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7765" citStr="Hovy et al., 2006" startWordPosition="1215" endWordPosition="1218">is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work. 644 Sentence CoNLL-2009 QA-SRL (1) Stock-fund managers, meantime , A0 they Who had held something? Stock-fund managers / they went into October with less cash on AM-TMP year When had someone held something? earlier this year hand than they held earlier this year. What had someone held? less cash on hand Where had someone held something? on h</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 57–60. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of propbank.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>69--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1560" citStr="Johansson and Nugues, 2008" startWordPosition="229" endWordPosition="232">her data in two domains, newswire text and Wikipedia articles, and introduce simple classifierbased models for predicting which questions to ask and what their answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task. 1 Introduction Semantic role labeling (SRL) is the widely studied challenge of recovering predicate-argument structure for natural language words, typically verbs. The goal is to determine “who does what to whom,” “when,” and “where,” etc. (Palmer et al., 2010; Johansson and Nugues, 2008). However, this intuition is difficult to formalize and fundamental aspects of the task vary across efforts, for example FrameNet (Baker et al., 1998) models a large set of interpretable thematic roles (AGENT, PATIENT, etc.) while PropBank (Palmer et al., 2005) uses a small set of verb-specific roles Figure 1: QA-SRL annotations for a Wikipedia sentence. (ARG0, ARG1, etc.). Existing task definitions can be complex and require significant linguistic expertise to understand,1 causing challenges for data annotation and use in many target applications. In this paper, we introduce a new questionans</context>
<context position="33839" citStr="Johansson and Nugues, 2008" startWordPosition="5703" endWordPosition="5706">Question Generation Answer Identification Predicate Token, Predicted POS-tag, Lemma extracted from Wiktionary Dependency parent and edge label, dependency children and edge label Question Question role label, Wh-word, Preposition Answer Word / Syntactic parent and edge label, Left/Right-most syntactic children, Predicate-Answer / Relative position (left or right), Syntactic relation, Syntactic path Table 10: Indicator features that are included in our role classifiers for question generation (Section 4) and the answer identification classifier (Section 5). Many come from previous work in SRL (Johansson and Nugues, 2008; Xue and Palmer, 2004). To mitigate syntactic errors, we used 10-best dependency parses from the Stanford parser (Klein and Manning, 2003). Newswire Wikipedia Classifier 78.7 82.3 Random 26.3 26.9 Table 11: Answer identification accuracy on newswire and Wikipedia text. baseline method that predicts a random syntactic child from the 1-best parse for each question. In each of the two domains, we train the binary classifiers on the training set of that domain (See Table 4 for dataset size). Table 11 shows experiment results for answer identification. Our classifier-based method outputs a correct</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of propbank. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69–78. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From treebank to propbank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 International Conference on Language Resources and Evaluation. Citeseer.</booktitle>
<contexts>
<context position="7700" citStr="Kingsbury and Palmer, 2002" startWordPosition="1203" endWordPosition="1207">ge corpora. The major distinguishing features of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work. 644 Sentence CoNLL-2009 QA-SRL (1) Stock-fund managers, meantime , A0 they Who had held something? Stock-fund managers / they went into October with less cash on AM-TMP year When had someone held something? earlier this year hand than they held earlier this year. What had someo</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From treebank to propbank. In Proceedings of the 2002 International Conference on Language Resources and Evaluation. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33978" citStr="Klein and Manning, 2003" startWordPosition="5724" endWordPosition="5727">l, dependency children and edge label Question Question role label, Wh-word, Preposition Answer Word / Syntactic parent and edge label, Left/Right-most syntactic children, Predicate-Answer / Relative position (left or right), Syntactic relation, Syntactic path Table 10: Indicator features that are included in our role classifiers for question generation (Section 4) and the answer identification classifier (Section 5). Many come from previous work in SRL (Johansson and Nugues, 2008; Xue and Palmer, 2004). To mitigate syntactic errors, we used 10-best dependency parses from the Stanford parser (Klein and Manning, 2003). Newswire Wikipedia Classifier 78.7 82.3 Random 26.3 26.9 Table 11: Answer identification accuracy on newswire and Wikipedia text. baseline method that predicts a random syntactic child from the 1-best parse for each question. In each of the two domains, we train the binary classifiers on the training set of that domain (See Table 4 for dataset size). Table 11 shows experiment results for answer identification. Our classifier-based method outputs a correct answer head for 80% of the test questions, establishing a useful baseline for future work on this task. 6 Discussion and Future Work We in</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Lang</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised semantic role induction via split-merge clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>1117--1126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27945" citStr="Lang and Lapata (2011)" startWordPosition="4731" endWordPosition="4734">: Mapping question fields to roles in R. The mapping is based on whether certain question fields are empty and the voice of the verb in the question (active or passive). O indicates that a field is either an empty string or equals “do/doing”. If a question is in passive voice and contains the preposition “by”, then OBJ2 is tagged with R0 instead, as in What is built by someone? this mapping heuristically with reasonable accuracy. In the future, we might try to induce the set of possible roles given each target verb, following the semantic role induction work of Titov and Klementiev (2012) and Lang and Lapata (2011), or use crowdsourcing to label proto-roles, following Reisinger et al. (2015). Predicting Question Roles Given this space of possible roles, our first step in generation is to determine which roles are present in a sentence, and select the pronouns that could be used to refer to them in the resulting questions. We formulate this task as a supervised multi-label learning problem. We define the set of possible labels L by combining the roles in R with different pronoun values: L ={role:val |role E R} val E{O, someone, something, do something, doing something} For example, to support the generat</context>
</contexts>
<marker>Lang, Lapata, 2011</marker>
<rawString>Joel Lang and Mirella Lapata. 2011. Unsupervised semantic role induction via split-merge clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 1117–1126. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Luheng He</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Joint a* ccg parsing and semantic role labeling.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="36487" citStr="Lewis et al. (2015)" startWordPosition="6116" endWordPosition="6119">, which has the potential to enable very large-scale annotation by only asking the annotators to provide answers. This will only be possible if performance can be improved to the point where we achieve high recall question with acceptable levels of precision. Finally, future work will also explore applications of our annotation. Most obviously, the annotation can be used for training question-answering systems, as it directly encodes question-answer pairs. More ambitiously, the annotation has the potential to be used for training parsers. A joint syntactic and semantic parser, such as that of Lewis et al. (2015), could be trained directly on the annotations to improve both the syntactic and semantic models, for example in domain transfer settings. Alternatively, the annotation could be used for active learning: we envisage a scheme where parsers, when faced with ambiguous attachment decisions, can generate a human-readable question whose answer will resolve the attachment. Acknowledgments This research was supported in part by the NSF (IIS-1252835), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google. We are grateful to K</context>
</contexts>
<marker>Lewis, He, Zettlemoyer, 2015</marker>
<rawString>Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015. Joint a* ccg parsing and semantic role labeling. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Semantic role features for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>716--724</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3714" citStr="Liu and Gildea, 2010" startWordPosition="567" endWordPosition="570">al., 2010), and the FrameNet guide is 119 pages (Ruppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages. 643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. mar formalisms. Nonetheless, as we will show, it still represents the argument and modifier attachment decisions that have motivated previous SRL definitions, and which are of crucial importance for semantic understanding in a range of NLP tasks, such as machine translation (Liu and Gildea, 2010) and coreference resolution (Ponzetto and Strube, 2006). The annotations also, perhaps surprisingly, capture other implicit arguments that cannot be read directly off of the syntax, as was required for previous SRL approaches. For example, in “It was his mother’s birthday, so he was going to play her favorite tune”, annotators created the QA pair “When would someone play something? His mother’s birthday” which describes an implicit temporal relation. Finally, QA-SRL data can be easily examined, proofread, and improved by anyone who speaks the language and understands the sentence; we use natur</context>
</contexts>
<marker>Liu, Gildea, 2010</marker>
<rawString>Ding Liu and Daniel Gildea. 2010. Semantic role features for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 716–724. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Natural logic for textual inference.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>193--200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12960" citStr="MacCartney and Manning, 2007" startWordPosition="2095" endWordPosition="2098">tput of existing semantic parsers. They show that some annotation can be crowdsourced using “games with a purpose” — however, this does not include its predicateargument structure, which requires expert knowledge of their syntactic and semantic formalisms. Finally, Reisinger et al. (2015) study crowdsourcing semantic role labels based on Dowty’s protoroles, given gold predicate and argument mentions. This work directly complements our focus on labeling predicate-argument structure. The idea of expressing the meaning of natural language in terms of natural language is related to natural logic (MacCartney and Manning, 2007), in which they use natural language for logical inference. Similarly, we model predicate-argument structure of a sentence with a set of question645 Field Description Example of Values No. Values WH* Question words (wh-words) who, what, when, where, why, how, how much 7 AUX Auxiliary verbs is, have, could, is n’t 36 SBJ Place-holding words for the subject position someone, something 2 TRG* Some form of the target word built, building, been built ≈ 12 OBJ1 Place-holding words for the object position someone, something 2 PP Frequent prepositions (by, to, for, with, about) and prepositions (unigr</context>
</contexts>
<marker>MacCartney, Manning, 2007</marker>
<rawString>Bill MacCartney and Christopher D Manning. 2007. Natural logic for textual inference. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 193–200. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7006" citStr="Marcus et al., 1993" startWordPosition="1091" endWordPosition="1094">s they play, without predefining an inventory of frames or semantic roles. • We present a novel, lightweight templatebased scheme (Section 3) that enables the high quality QA-SRL data annotation with very little training and no linguistic expertise. • We define two new QA-SRL sub-tasks, question generation and answer identification, and present baseline learning approaches for both (Sections 4 and 5). The results demonstrate that our data is high-quality and supports the study of better learning algorithms. 2 Related Work The success of syntactic annotation projects such as the Penn Treebank (Marcus et al., 1993) has led to numerous efforts to create semantic annotations for large corpora. The major distinguishing features of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLT-NAACL</booktitle>
<pages>24--31</pages>
<contexts>
<context position="7731" citStr="Meyers et al., 2004" startWordPosition="1209" endWordPosition="1212">eatures of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely related to our work. FrameNet (Baker et al., 1998) contains a detailed lexicon of verb senses and thematic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully annotated corpus contains about 3,000 sentences (Chen et al., 2010). We were able to annotate over 3,000 sentences within weeks. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004) and OntoNotes (Hovy et al., 2006) circumvent the need for a large lexicon of roles, by defin3Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work. 644 Sentence CoNLL-2009 QA-SRL (1) Stock-fund managers, meantime , A0 they Who had held something? Stock-fund managers / they went into October with less cash on AM-TMP year When had someone held something? earlier this year hand than they held earlier this year. What had someone held? less cash on hand Wher</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. In HLT-NAACL 2004 workshop: Frontiers in corpus annotation, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1821" citStr="Palmer et al., 2005" startWordPosition="270" endWordPosition="273">o establish baseline performance levels for future work on this task. 1 Introduction Semantic role labeling (SRL) is the widely studied challenge of recovering predicate-argument structure for natural language words, typically verbs. The goal is to determine “who does what to whom,” “when,” and “where,” etc. (Palmer et al., 2010; Johansson and Nugues, 2008). However, this intuition is difficult to formalize and fundamental aspects of the task vary across efforts, for example FrameNet (Baker et al., 1998) models a large set of interpretable thematic roles (AGENT, PATIENT, etc.) while PropBank (Palmer et al., 2005) uses a small set of verb-specific roles Figure 1: QA-SRL annotations for a Wikipedia sentence. (ARG0, ARG1, etc.). Existing task definitions can be complex and require significant linguistic expertise to understand,1 causing challenges for data annotation and use in many target applications. In this paper, we introduce a new questionanswer driven SRL task formulation (QA-SRL), which uses question-answer pairs to label verbal predicate-argument structure. For example, for the sentence in Figure 1, we can ask a short question containing a verb, e.g. “Who finished something?”, and whose answer i</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Nianwen Xue</author>
</authors>
<date>2010</date>
<booktitle>Semantic role labeling. Synthesis Lectures on Human Language Technologies,</booktitle>
<pages>3--1</pages>
<contexts>
<context position="1531" citStr="Palmer et al., 2010" startWordPosition="225" endWordPosition="228">tic expertise. We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classifierbased models for predicting which questions to ask and what their answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task. 1 Introduction Semantic role labeling (SRL) is the widely studied challenge of recovering predicate-argument structure for natural language words, typically verbs. The goal is to determine “who does what to whom,” “when,” and “where,” etc. (Palmer et al., 2010; Johansson and Nugues, 2008). However, this intuition is difficult to formalize and fundamental aspects of the task vary across efforts, for example FrameNet (Baker et al., 1998) models a large set of interpretable thematic roles (AGENT, PATIENT, etc.) while PropBank (Palmer et al., 2005) uses a small set of verb-specific roles Figure 1: QA-SRL annotations for a Wikipedia sentence. (ARG0, ARG1, etc.). Existing task definitions can be complex and require significant linguistic expertise to understand,1 causing challenges for data annotation and use in many target applications. In this paper, w</context>
</contexts>
<marker>Palmer, Gildea, Xue, 2010</marker>
<rawString>Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010. Semantic role labeling. Synthesis Lectures on Human Language Technologies, 3(1):1–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>192--199</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3769" citStr="Ponzetto and Strube, 2006" startWordPosition="574" endWordPosition="577">uppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages. 643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. mar formalisms. Nonetheless, as we will show, it still represents the argument and modifier attachment decisions that have motivated previous SRL definitions, and which are of crucial importance for semantic understanding in a range of NLP tasks, such as machine translation (Liu and Gildea, 2010) and coreference resolution (Ponzetto and Strube, 2006). The annotations also, perhaps surprisingly, capture other implicit arguments that cannot be read directly off of the syntax, as was required for previous SRL approaches. For example, in “It was his mother’s birthday, so he was going to play her favorite tune”, annotators created the QA pair “When would someone play something? His mother’s birthday” which describes an implicit temporal relation. Finally, QA-SRL data can be easily examined, proofread, and improved by anyone who speaks the language and understands the sentence; we use natural language to label the structure of natural language.</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2006. Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 192–199. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drew Reisinger</author>
<author>Rachel Rudinger</author>
<author>Francis Ferraro</author>
<author>Craig Harman</author>
<author>Kyle Rawlins</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Semantic proto-roles.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--475</pages>
<marker>Reisinger, Rudinger, Ferraro, Harman, Rawlins, Van Durme, 2015</marker>
<rawString>Drew Reisinger, Rachel Rudinger, Francis Ferraro, Craig Harman, Kyle Rawlins, and Benjamin Van Durme. 2015. Semantic proto-roles. Transactions of the Association for Computational Linguistics, 3:475–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam RL Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>Framenet ii: Extended theory and practice.</title>
<date>2006</date>
<contexts>
<context position="3167" citStr="Ruppenhofer et al., 2006" startWordPosition="488" endWordPosition="492">hile the question provides an indirect label on the role that “UCD” plays. Enumerating all such pairs, as we will see later, provides a relatively complete representation of the original verb’s arguments and modifiers. The QA-SRL task formulation has a number of advantages. It can be easily explained to nonexpert annotators with a short tutorial and a few examples. Moreover, the formulation does not depend on any pre-defined inventory of semantic roles or frames, or build on any existing gram1The PropBank annotation guide is 89 pages (Bonial et al., 2010), and the FrameNet guide is 119 pages (Ruppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages. 643 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643–653, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. mar formalisms. Nonetheless, as we will show, it still represents the argument and modifier attachment decisions that have motivated previous SRL definitions, and which are of crucial importance for semantic understanding in a range of NLP tasks, such as machine translation (Liu and Gildea, 2010) and coreference resolution (Ponzetto and Strube, 200</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam RL Petruck, Christopher R Johnson, and Jan Scheffczyk. 2006. Framenet ii: Extended theory and practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asher Stern</author>
<author>Ido Dagan</author>
</authors>
<title>Recognizing implied predicate-argument relationships in textual inference.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the Association</booktitle>
<contexts>
<context position="14749" citStr="Stern and Dagan (2014)" startWordPosition="2389" endWordPosition="2392">n annotation template. answer pairs. While existing work on natural logic has relied on small entailment datasets for training, our method allows practical large-scale annotation of training data. Parser evaluation using textual entailment (Yuret et al., 2010) is a method for evaluating syntactic parsers based on entailment examples. In a similar spirit to our work, they abstract away from linguistic formalisms by using natural language inference. We focus on semantic rather than syntactic annotation, and introduce a scalable method for gathering data that allows both training and evaluation. Stern and Dagan (2014) applied textual entailment to recognize implicit predicate-argument structure that are not explicitly expressed in syntactic structure. 3 QA-based Semantic Dataset This section describes our annotation process in more detail, and discusses agreement between our annotations and PropBank. Table 1 shows examples provided by non-expert annotators.4 3.1 Annotation Task Design We annotate verbs with pairs of questions and answers that provide information about predicateargument structure. Given a sentence s and a verbal predicate v in the sentence, annotators must produce a set of wh-questions that</context>
</contexts>
<marker>Stern, Dagan, 2014</marker>
<rawString>Asher Stern and Ido Dagan. 2014. Recognizing implied predicate-argument relationships in textual inference. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>A bayesian approach to unsupervised semantic role induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>12--22</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27918" citStr="Titov and Klementiev (2012)" startWordPosition="4726" endWordPosition="4729">R2 OBJ2 = ¢ OBJ2 — R2[p] Table 7: Mapping question fields to roles in R. The mapping is based on whether certain question fields are empty and the voice of the verb in the question (active or passive). O indicates that a field is either an empty string or equals “do/doing”. If a question is in passive voice and contains the preposition “by”, then OBJ2 is tagged with R0 instead, as in What is built by someone? this mapping heuristically with reasonable accuracy. In the future, we might try to induce the set of possible roles given each target verb, following the semantic role induction work of Titov and Klementiev (2012) and Lang and Lapata (2011), or use crowdsourcing to label proto-roles, following Reisinger et al. (2015). Predicting Question Roles Given this space of possible roles, our first step in generation is to determine which roles are present in a sentence, and select the pronouns that could be used to refer to them in the resulting questions. We formulate this task as a supervised multi-label learning problem. We define the set of possible labels L by combining the roles in R with different pronoun values: L ={role:val |role E R} val E{O, someone, something, do something, doing something} For exam</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. A bayesian approach to unsupervised semantic role induction. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–22. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17696" citStr="Toutanova et al., 2003" startWordPosition="2866" endWordPosition="2869">6414 Wikipedia-dev 392 895 2183 Wikipedia-test 393 898 2201 Table 4: Annotated data statistics. 2009), excluding questions and sentences with fewer than 10 words. For the Wikipedia domain, we randomly sampled sentences from the English Wikipedia, excluding questions and sentences with fewer than 10 or more than 60 words. In each sentence, we need to first identify the candidates for verbal predicates. In principle, a separate stage of annotation could identify verbs—but for simplicity, we instead used POS-tags. We used gold POS-tags for newswire, and predicted POS-tags (using Stanford tagger (Toutanova et al., 2003)) in Wikipedia. Annotators can choose to skip a candidate verb if they are unable to write questions for it. Annotators skipped 136 verbs (3%) in Wikipedia data and 50 verbs (1.5%) in PropBank data. 3.3 Annotation Process For annotation, we hired 10 part-time, non-expert annotators from Upwork (previously oDesk) and paid $10 per hour for their work. The average cost was $0.58 per verb ($1.57 per sentence) for newswire text and $0.45 per verb ($1.01 per sentence) on the Wikipedia domain. The annotators are given a short tutorial and a small set of sample annotations (about 10 sentences). Annota</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, volume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>88--94</pages>
<contexts>
<context position="33862" citStr="Xue and Palmer, 2004" startWordPosition="5707" endWordPosition="5710">dentification Predicate Token, Predicted POS-tag, Lemma extracted from Wiktionary Dependency parent and edge label, dependency children and edge label Question Question role label, Wh-word, Preposition Answer Word / Syntactic parent and edge label, Left/Right-most syntactic children, Predicate-Answer / Relative position (left or right), Syntactic relation, Syntactic path Table 10: Indicator features that are included in our role classifiers for question generation (Section 4) and the answer identification classifier (Section 5). Many come from previous work in SRL (Johansson and Nugues, 2008; Xue and Palmer, 2004). To mitigate syntactic errors, we used 10-best dependency parses from the Stanford parser (Klein and Manning, 2003). Newswire Wikipedia Classifier 78.7 82.3 Random 26.3 26.9 Table 11: Answer identification accuracy on newswire and Wikipedia text. baseline method that predicts a random syntactic child from the 1-best parse for each question. In each of the two domains, we train the binary classifiers on the training set of that domain (See Table 4 for dataset size). Table 11 shows experiment results for answer identification. Our classifier-based method outputs a correct answer head for 80% of</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 88–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Aydin Han</author>
<author>Zehra Turgut</author>
</authors>
<title>Semeval-2010 task 12: Parser evaluation using textual entailments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>51--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14387" citStr="Yuret et al., 2010" startWordPosition="2331" endWordPosition="2334"> template, with descriptions, example values, and the total number of possible values for each. WH* and TRG* are required; all other fields can be left empty. WH* AUX SBJ TRG* OBJ1 PP OBJ2 Who built something ? What had someone said ? What was someone expected to do ? Where might something rise from ? Table 3: Four example questions written with our question annotation template. answer pairs. While existing work on natural logic has relied on small entailment datasets for training, our method allows practical large-scale annotation of training data. Parser evaluation using textual entailment (Yuret et al., 2010) is a method for evaluating syntactic parsers based on entailment examples. In a similar spirit to our work, they abstract away from linguistic formalisms by using natural language inference. We focus on semantic rather than syntactic annotation, and introduce a scalable method for gathering data that allows both training and evaluation. Stern and Dagan (2014) applied textual entailment to recognize implicit predicate-argument structure that are not explicitly expressed in syntactic structure. 3 QA-based Semantic Dataset This section describes our annotation process in more detail, and discuss</context>
</contexts>
<marker>Yuret, Han, Turgut, 2010</marker>
<rawString>Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. Semeval-2010 task 12: Parser evaluation using textual entailments. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 51–56. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
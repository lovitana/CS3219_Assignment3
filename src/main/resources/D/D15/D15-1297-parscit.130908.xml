<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000227">
<title confidence="0.9923105">
JEAM: A Novel Model for Cross-Domain Sentiment Classification
Based on Emotion Analysis
</title>
<author confidence="0.99941">
Kun-Hu Luo, Zhi-Hong Deng, Liang-Chen Wei, Hongliang Yu
</author>
<affiliation confidence="0.9646795">
School of Electronic Engineering and Computer Science
Peking University, Beijing, China
</affiliation>
<email confidence="0.920984">
{dr.tiger126@gmail.com, zhdeng@cis.pku.edu.cn, pkuhaywire@gmail.com,
yuhongliang324@gmail.com}
</email>
<sectionHeader confidence="0.996407" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999494964285714">
Cross-domain sentiment classification
(CSC) aims at learning a sentiment
classifier for unlabeled data in the target
domain based on the labeled data from a
different source domain. Due to the
differences of data distribution of two
domains in terms of the raw features, the
CSC problem is difficult and challenging.
Previous researches mainly focused on
concepts mining by clustering words
across data domains, which ignored the
importance of authors’ emotion contained
in data, or the different representations of
the emotion between domains. In this
paper, we propose a novel framework to
solve the CSC problem, by modelling the
emotion across domains. We first develop
a probabilistic model named JEAM to
model author’s emotion state when
writing. Then, an EM algorithm is
introduced to solve the likelihood
maximum problem and to obtain the latent
emotion distribution of the author. Finally,
a supervised learning method is utilized to
assign the sentiment polarity to a given
online review. Experiments show that our
approach is effective and outperforms
state-of-the-art approaches.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9873752">
Cross-domain sentiment classification (CSC) is
the task that learns a sentiment classifier for
unlabeled data in the target domain based on the
labeled data from the source domain. With the
increasing amount of opinion information
</bodyText>
<subsectionHeader confidence="0.408391">
 Corresponding author
</subsectionHeader>
<bodyText confidence="0.999894842105263">
available on the Internet, CSC has become a hot
spot in recent years. Traditional machine learning
algorithms often train a classifier utilizing the
labeled data for CSC. However, in some practical
cases, we may have many labeled data for some
domains (source domains) but very few or no
labeled data for other domains (target domains).
Due to the differences of the distribution of two
domains in terms of raw features, e.g. raw term
frequency, the classifier trained from the source
domain often performs badly on the target domain.
To overcome this issue, several feature-based
studies have been proposed to improve the
sentiment classification domain adaptation
[Zhuang et al., 2013; He et al., 2011; Gao and Li,
2011; Li et al., 2012; Dai et al., 2007; Zhuang et
al., 2010; Pan et al., 2010; Wang et al., 2011; Long
et al., 2012; Lin and He, 2009].
Existing studies build various generative
models to solve the domain adaptation problems
for CSC. In most cases, the models are trained by
using the whole corpora without specifying on the
sentiment of the texts. For example, [Zhuang et al.,
2013] propose a general framework HIDC to mine
high-level concepts (e.g. word clusters) across
various domains. However, their learned concepts
contain many topics not restricted to the sentiment.
On the other hand, some researchers focus on the
usage of the sentiment in CSC study [Mitra et al.,
2013a; Mitra et al., 2013b; He et al., 2011]. [He et
al., 2011] modify JST model [Lin and He, 2009]
by incorporating word polarity priors through
adjusting the topic-word Dirichlet priors.
However, they fail to consider the expression
differences among various domains.
To overcome the above issues, we employ
“emotion”, for its ubiquity among domains. The
sentiment words in different domains might vary
</bodyText>
<page confidence="0.745074">
2503
</page>
<note confidence="0.627835">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2503–2508,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9999798">
significantly, but the emotion can be effectively
transferred. For example, when expressing the
emotion “happiness”, one uses “bravo” in the
domain of sport, while “yummy” in the domain of
food. Therefore, we propose an EA framework to
model the latent emotions which are commonly
contained in subjective articles and expressed by
“emotional words”. We infer the sentiment
polarity of a document based on the emotion state.
The hierarchy of EA is composed by four layers:
</bodyText>
<listItem confidence="0.694727">
(1) Sentiment Layer
</listItem>
<bodyText confidence="0.9994772">
Normally, the sentiment of a document is the
general opinion towards a certain event or object.
For example, a movie review in IMDB might
voice the feeling about the movie by a reviewer
[Yu et al., 2013].
</bodyText>
<listItem confidence="0.615345">
(2) Emotion Layer
</listItem>
<bodyText confidence="0.999812923076923">
Based on the emotion classification theories
in psychology [Plutchik, 2002], the emotion can
be classified into the basic ones influenced by the
physiological factors, e.g. happiness, sadness,
anger, etc., and dozens of complicated ones
formed under some specific social conditions, e.g.
shame, guilt, abashment, etc. Additionally, the
emotion can be classified as positive and negative
(similar to the sentiment classification) based on
dimensional models of emotion [Schlosberg,
1954; Plutchik, 2002; Rubin and Talerico, 2009].
Intuitively, we assume that a document tends to
contain the emotions of similar polarity.
</bodyText>
<listItem confidence="0.861788">
(3) Lexicon Layer
</listItem>
<bodyText confidence="0.99859275">
To build the connection between words and
the emotion, we introduce emotional words
instead of raw word features into our model. By
utilizing the emotional lexicon MPQA [Wiebe et
al., 2005], we select groups of strong polar words,
which get high scores in the emotional lexicon.
These words are considered highly correlated to
the certain emotion of the same polarity. And
these strong polar words have invariant polarity
across domains. Therefore, the emotion can be
substantialized by a series of emotional words
drawn from corresponding probability distribution.
</bodyText>
<listItem confidence="0.639295">
(4) Expression Layer
</listItem>
<bodyText confidence="0.999896">
In many practical cases, data come from
different domains. We suppose that the
correlation between emotion state and sentiment
orientation is stable over domains, but one
emotion may have different expressions when
domain varies. E.g., “satisfaction” may be
expressed as “interesting” or “attractive” for a
book; meanwhile, it may be expressed “efficient”
for an electronics device. Formally, we have
</bodyText>
<equation confidence="0.999984">
𝑝(𝑒|𝑦,𝑟1) = 𝑝(𝑒|𝑦,𝑟2) = 𝑝(𝑒|𝑦) (1)
𝑝(𝑤𝑒|𝑒,𝑟1) ≠ 𝑝(𝑤𝑒|𝑒,𝑟2) (2)
</equation>
<bodyText confidence="0.999992">
where 𝑒 denotes the emotion, y denotes the
author’s sentiment orientation, 𝑟1 and 𝑟2
denotes two different domains, and 𝑤𝑒 denotes
the emotional words.
Along this line, we propose the Joint
Emotion Analysis Model (named JEAM for
abbreviation) utilizing the probabilistic methods.
See details in the next section.
</bodyText>
<sectionHeader confidence="0.997517" genericHeader="method">
2 Proposed Model
</sectionHeader>
<subsectionHeader confidence="0.999148">
2.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.943513736842105">
The CSC problem can be formulated as follows:
Suppose we have two sets of data, denoted as 𝐷𝑠
and 𝐷𝑡, which represent the source domain data
and the target domain data respectively. In the
CSC problem, the source domain data consist of
labeled instances, denoted by 𝐷𝑠 =
{(𝑥𝑖 (𝑠),𝑦𝑖 (𝑠))}|𝑖=1
𝑛𝑠 , where 𝑥𝑖(𝑠) ∈ ℝ𝑘 is an input
vector, 𝑦𝑖(𝑠) ∈ {0,1} is the output label, and 𝑛𝑠
is the number of documents in 𝐷𝑠. Unlike that of
the source domain, the target domain data consists
of samples without any label information, denoted
by 𝐷𝑡 = {𝑥𝑖 (𝑡)}|𝑖=1
𝑛𝑡 , where 𝑥𝑖(𝑡) ∈ ℝ𝑘 is an input
vector, and 𝑛𝑡 is the number of documents in 𝐷𝑡.
The task of CSC is to leverage the training data of
source domain 𝐷𝑠 to predict the label 𝑦𝑖(𝑡)
corresponding to input vector 𝑥𝑖(𝑡) of target
domain 𝐷𝑡.
</bodyText>
<subsectionHeader confidence="0.985786">
2.2 The JEAM Model
</subsectionHeader>
<bodyText confidence="0.9999725">
To model the author’s emotion state contained in
the document, we propose the JEAM model based
on the probabilistic graphical principle. Note that
all the factors and edges in JEAM are derived
from the specific concepts and relations in EA,
e.g., Eq(1) and Eq(2). We draw the graphical
representation of JEAM in Figure 1, and show the
notations of this paper in Table 1.
In Figure 1, y denotes the sentiment
orientation of the author, which is a latent variable
in this model. 𝑒 denotes any emotion (topic)
generated by y from a conditional
probability 𝑝(𝑒 |𝑦). 𝑒 is also a latent variable in
this model. 𝑟 denotes any data domain, e.g.,
books, dvd, kitchen, and electronics etc. 𝑑
denotes any document chosen from domain r with
label y. For documents from the source domain,
the conditional probability 𝑝 (𝑑 |𝑟, 𝑦) is known,
which can be used to supervise the modeling
process. 𝑢 denotes the prior sentiment polarity of
the corresponding emotional word. In practice, 𝑢
can be obtained from the emotional lexicon,
</bodyText>
<page confidence="0.994471">
2504
</page>
<figureCaption confidence="0.984048666666667">
Figure 1. The Graphical representation of JEAM.
All the latent variables are marked in white, and
all the observed variables are marked in gray.
</figureCaption>
<table confidence="0.9991145">
𝑒 Emotion
𝑤𝑒 Emotional word
𝑟 Domain
𝑑 Document
𝑢 Prior sentiment polarity of the emotional word
𝑦 Sentiment polarity of the document
𝑋 All the observed variables
𝜃 All the model parameters
</table>
<tableCaption confidence="0.999788">
Table 1. Means of Symbols
</tableCaption>
<bodyText confidence="0.997398933333333">
which classifies a series of words into positive and
negative categories. 𝑤𝑒 denotes any emotional
word with polarity u, which is chosen over words
conditioned on emotion e and domain r from
conditional probability 𝑝 (𝑤𝑒 |𝑒, 𝑟, 𝑢) . In this
paper, we only select emotional words with strong
sentiment polarities to represent the vector of the
document. Therefore, we rebuild the data with the
help of emotional lexicon cutting out the non-
emotional words. As a result, any word chosen
from the rebuilt data will be an emotional word,
which is supposed not to change its polarity in
different domains. Additionally, the joint
probability over all the observed variables can be
defined as follows based on the hidden variables:
</bodyText>
<equation confidence="0.79327">
𝑝 (𝑤𝑒, 𝑑, 𝑟, 𝑢) = ∑𝑒,𝑦 𝑝 (𝑒, 𝑦, 𝑤𝑒, 𝑑, 𝑟, 𝑢) (3)
Based on the graphical model, we have:
𝑝 (𝑒, 𝑦, 𝑤𝑒, 𝑑, 𝑟, 𝑢) =
𝑝(𝑤𝑒|𝑒,𝑟,𝑢)𝑝(𝑑|𝑟,𝑦)𝑝(𝑒|𝑦)𝑝(𝑟)𝑝(𝑦)𝑝(𝑢) (4)
</equation>
<bodyText confidence="0.998719">
We need to learn the unobservable
probabilities (e.g., 𝑝 (𝑤𝑒 |𝑒, 𝑟, 𝑢), 𝑝 (𝑑 |𝑟, 𝑦),
𝑝 (𝑒 |𝑦), 𝑝 (𝑦) ) to infer the hidden emotion
distribution. Therefore, we develop an
Expectation-Maximization (EM) algorithm to
maximize the log likelihood of generating the
whole dataset and obtain the iterative formula in
E-step as follows:
</bodyText>
<equation confidence="0.96405225">
𝑝 (𝑒, 𝑦 |𝑤𝑒, 𝑑, 𝑟, 𝑢) =
𝑝(𝑤𝑒 |𝑒, 𝑟, 𝑢)𝑝(𝑑 |𝑟, 𝑦)𝑝(𝑒 |𝑦)𝑝(𝑟)𝑝(𝑦)𝑝(𝑢)
(5)
∑𝑒,𝑦𝑝(𝑤𝑒|𝑒, 𝑟,𝑢)𝑝(𝑑|𝑟,𝑦)𝑝(𝑒|𝑦)𝑝(𝑟)𝑝(𝑦)𝑝(𝑢)
</equation>
<bodyText confidence="0.999088666666667">
where all the factors are calculated in M-step
similar to PLSA and HIDC (Hoffman, 1999;
Zhuang et al., 2013).
</bodyText>
<subsectionHeader confidence="0.964902">
2.3 CSC via JEAM
</subsectionHeader>
<bodyText confidence="0.999774157894737">
To use JEAM to solve CSC problems, we adopt
two optimizations:
First, we supervise the EM optimization with
the polarity information of emotional words and
instances respectively in the source domain. On
the one hand, we estimate 𝑝 (𝑒, 𝑦 |𝑤𝑒, 𝑑, 𝑟, 𝑢)
utilizing the polarity label of the emotional words.
Let the emotion set 𝐸 be divided into positive set
𝐸𝑝 and negative set 𝐸𝑛 . We set
𝑝(𝑒𝑖, 𝑦|𝑤𝑒, 𝑑, 𝑟, 𝑢𝑤𝑒) = 0 during the whole EM
process when the polarities of the emotion and
current emotional word are different. On the other
hand, we estimate the probability 𝑝 (𝑑 |𝑟, 𝑦) with
the label information of instances in the source
domain. When the document is from the source
domain, we set 𝑝 (𝑑 |𝑟, 𝑦) = 0 if 𝑦 is
different with the ground truth.
Second, we reconstruct the document as
follows,
</bodyText>
<equation confidence="0.97310575">
— 1 + ,n𝑊𝑒𝑖𝑟 |, 𝑖𝑓
𝑊𝑒𝑖𝑟≠ ∅
1 &apos;&apos;WeT |(6)
𝑑∗ = [𝑒1, 𝑒2,... 𝑒𝑚], e� �= i
0, 𝑜𝑡ℎ𝑒𝑟𝑠
, 𝑟
𝑒𝑤𝑒 =
𝑎𝑟𝑔𝑚𝑎𝑥𝑒 𝑝 (𝑒 |𝑤𝑒, 𝑟) , and 𝑝 (𝑒 |𝑤𝑒, 𝑟) can be
</equation>
<bodyText confidence="0.997037">
computed based on 𝑝 (𝑤𝑒 |𝑒, 𝑟, 𝑢) , 𝑝 (𝑒 |𝑦), 𝑝 (𝑢)
and 𝑝 (𝑟) obtained after EM algorithm. The main
function of this step is to process a new given
document faster, avoiding training JEAM again
with the new input. Finally, a machine learning
method Support Vector Machine (SVM) is
introduced to train a classifier with the labeled
data from the source domain and assign polarities
to documents from the target domain based on our
reconstructed data.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999087">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.990680272727273">
We demonstrate the effectiveness of JEAM on the
Multi-Domain sentiment data set [Blitzer et al.,
2007] which contains four types (domains) of
real-world product documents taken from
Amazon.com, which are books, dvd, electronics
and kitchen. We randomly select 1800 documents
from the one domain (source domain) and 200
documents from another domain (target domain).
Then, we train a sentiment classifier using
documents selected from the source domain and
where [𝑒1, 𝑒2, ... 𝑒 𝑚] is the distribution over
</bodyText>
<equation confidence="0.959339">
emotions, 𝑊𝑒 𝑖𝑟 = {𝑤𝑒|𝑒𝑤𝑒
𝑟 = 𝑖}
</equation>
<page confidence="0.911148">
2505
</page>
<bodyText confidence="0.999944166666667">
assign labels to documents selected from the
target domain, which generates 12 classification
tasks. We preform 10 random selections and
report the average results over 10 different runs.
We use MPQA subjective lexicon 1 as the
emotional lexicon. In our experiments, only
strongly subjective clues are considered as
emotional words, consisting of 1717 positive and
3621 negative words. We rebuild the dataset by
cutting out the non-emotional words. For
experiment parameters, we set p = 25, n = 25,
and T = 100 after plenty of experiments.
Considering the data in practice, the sentiment
orientation y has only two forms, positive or
negative. Note that we do neither instance
selection nor complicated feature selection (only
filter the low-frequency words) to our proposed
method and other methods in comparison.
</bodyText>
<subsectionHeader confidence="0.938947">
3.2 Experimental Result
Performance of Emotional Words
</subsectionHeader>
<bodyText confidence="0.999987571428571">
We show the effectiveness of introducing
emotional words to solve the CSC problem. In
JEAM, we reconstruct the documents by cutting
out the non-emotional words. To compare the
classification accuracy on the original documents
and the reconstructed (emotional) documents, we
choose two common classification algorithms,
linear SVM and PLSA (topic size=10) for
experiment respectively. The experiment results
shows that both SVM and PLSA perform better
on the emotional documents (60.43% and 60.48%)
than on the original documents (57.73% and
56.69%) for the average accuracy over 12
classification tasks.
</bodyText>
<subsectionHeader confidence="0.680522">
Effectiveness of using domain information
</subsectionHeader>
<bodyText confidence="0.984430818181818">
and word polarity
We show the effectiveness of using domain
information and word polarity, which are
employed in our approach. For this purpose, we
repeat the experiment without introducing domain
and word polarity (node u and node r) into the
model. Figure 2 shows the results. As it is clear,
the highest performance can be achieved when
domain information and word polarity are both
used, while the lowest performance is obtained
when neither of them is used.
</bodyText>
<subsectionHeader confidence="0.880512">
Comparison with the Baselines
</subsectionHeader>
<bodyText confidence="0.9997724">
We compare our proposed approach with PLSA,
SVM, SFA [Pan et al., 2010], JST [He et al., 2011]
and HIDC [Zhuang et al., 2013]. The
experimental results of the 12 classification tasks
are shown in Figure 3. It can be observed that our
</bodyText>
<footnote confidence="0.960887">
1 http://www.cs.pitt.edu/mpqa
</footnote>
<figureCaption confidence="0.999523333333333">
Figure 2: Effectiveness of using domain
information and word polarity
Figure 3: Comparison with the Baselines
</figureCaption>
<bodyText confidence="0.998964714285714">
proposed approach outperforms all the other
approaches in general. Note that in order to obtain
a more precise comparison of the algorithms, we
do neither the instance selection nor the
complicated feature selection. The result of our
proposed approach can possibly be improved with
the help of these selection strategies.
</bodyText>
<sectionHeader confidence="0.994383" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999824666666667">
In this paper, we propose a novel framework to
solve the CSC problem, by modelling emotions
across domains. We deeply analyze the relation
between the author and the document based on the
emotion theories in the field of psychology. Along
this line, we propose a framework named EA,
which takes the emotions and domains into
account. Based on EA, we propose a novel model
named JEAM to model the author’s emotion state
for Cross-domain sentiment classification. We
conduct extensive experiments on real datasets to
evaluate JEAM. The experiment results show that
emotion plays an important role in CSC and
JEAM outperforms existing state-of-the-art
methods on the task of CSC.
</bodyText>
<page confidence="0.985753">
2506
</page>
<sectionHeader confidence="0.998003" genericHeader="conclusions">
5 Acknowledgement
</sectionHeader>
<bodyText confidence="0.999841428571428">
This work is partially supported by Project
61170091 supported by National Natural Science
Foundation of China and Project 2015AA015403
supported by the National High Technology
Research and Development Program of China
(863 Program). We would also like to thank the
anonymous reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.997817" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.926722875">
John Blitzer, Mark Dredze, Fernando Pereira, et
al.2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment
classification. In ACL, volume 7, pages 440–447.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Co-clustering based classification for out-
of-domain documents. In Proceedings of the 13th
ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 210–
219. ACM.
Hal Daum′e III. 2009. Frustratingly easy domain
adaptation. arXiv preprint arXiv:0907.1815.
Sheng Gao and Haizhou Li. 2011. A cross-domain
adaptation method for sentiment classification using
probabilistic latent analysis. In Proceedings of the
20th ACM international conference on Information
</reference>
<bodyText confidence="0.751189571428571">
and knowledge management, pages 1047–1052.
ACM.
Boqing Gong, Kristen Grauman, and Fei Sha. 2013.
Connecting the dots with landmarks:
Discriminatively learning domain-invariant features
for unsupervised domain adaptation. In Proceedings
of The 30th International Conference on Machine
</bodyText>
<reference confidence="0.998101243243243">
Learning, pages 222–230.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 123–131.
Association for Computational Linguistics.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual
international ACM SIGIR conference on Research
and development in information retrieval, pages 50–
57. ACM.
Shoushan Li, Chu-Ren Huang, Guodong Zhou, and
Sophia Yat Mei Lee. 2010. Employing personal/
impersonal views in supervised and semisupervised
sentiment classification. In Proceedings of the 48th
annual meeting of the association for computational
linguistics, pages 414–423. Association for
Computational Linguistics.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and
Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the
50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume 1,
pages 410–419. Association for Computational
Linguistics.
Chenghua Lin and Yulan He. 2009. Joint sentiment/
topic model for sentiment analysis. In Proceedings
of the 18th ACMconference on Information and
knowledge management, pages 375–384. ACM.
Mingsheng Long, Jianmin Wang, Guiguang Ding, Wei
Cheng, Xiang Zhang, and Wei Wang. 2012. Dual
transfer learning. In SDM, pages 540–551. SIAM.
Mitra Mohtarami. Man lan, and chew lim tan. 2013a.
from semantic to emotional space in probabilistic
sense sentiment analysis. In the 27th AAAI
Conference on Artificial Intelligence.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain
sentiment classification via spectral feature
alignment. In Proceedings of the 19th international
conference on World wide web, pages 751–760.
ACM.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in
information retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 79–
86. Association for Computational Linguistics.
Robert Plutchik. 1980. Emotion: A psychoevolutionary
synthesis. Harpercollins College Division.
Robert Plutchik. 2002. Emotion and life.
David C Rubin and Jennifer M Talarico. 2009. A
comparison of dimensional models of emotion:
Evidence from emotions, prototypical events,
autobiographical memories, and words. Memory,
17(8):802–808.
Harold Schlosberg. 1954. Three dimensions of
emotion. Psychological review, 61(2):81.
Hua Wang, Heng Huang, Feiping Nie, and Chris Ding.
2011. Cross-language web page classification via
dual knowledge transfer using nonnegative matrix
trifactorization. In Proceedings of the 34th
international ACM SIGIR conference on Research
and development in Information Retrieval, pages
933–942. ACM.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language resources and
evaluation, 39(2-3):165–210.
</reference>
<page confidence="0.740651">
2507
</page>
<reference confidence="0.999386875">
Hongliang Yu, Zhi-Hong Deng, and Shiyingxue Li.
2013. Identifying sentiment words using an
optimization-based model without seed words. In
ACL (2), pages 855–859.
Fuzhen Zhuang, Ping Luo, Peifeng Yin, Qing He, and
Zhongzhi Shi. 2013. Concept learning for
crossdomain text classification: A general
probabilistic framework. In Proceedings of the
Twenty-Third international joint conference on
Artificial Intelligence, pages 1960–1966. AAAI
Press.
Fuzhen Zhuang, Ping Luo, Changying Du, Qing He,
Zhongzhi Shi, and Hui Xiong. 2014. Triplex transfer
learning: exploiting both shared and distinct
concepts for text classification. Cybernetics, IEEE
Transactions on, 44(7):1191–1203.
</reference>
<page confidence="0.990237">
2508
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.690640">
<title confidence="0.9977135">JEAM: A Novel Model for Cross-Domain Sentiment Based on Emotion Analysis</title>
<author confidence="0.992852">Zhi-Hong Liang-Chen Wei Luo</author>
<author confidence="0.992852">Hongliang</author>
<affiliation confidence="0.999597">School of Electronic Engineering and Computer Peking University, Beijing,</affiliation>
<email confidence="0.708779">{dr.tiger126@gmail.com,zhdeng@cis.pku.edu.cn,</email>
<abstract confidence="0.999508034482759">Cross-domain sentiment classification (CSC) aims at learning a sentiment classifier for unlabeled data in the target domain based on the labeled data from a different source domain. Due to the differences of data distribution of two domains in terms of the raw features, the CSC problem is difficult and challenging. Previous researches mainly focused on concepts mining by clustering words across data domains, which ignored the of contained in data, or the different representations of the emotion between domains. In this paper, we propose a novel framework to solve the CSC problem, by modelling the emotion across domains. We first develop a probabilistic model named JEAM to author’s state when writing. Then, an EM algorithm is introduced to solve the likelihood maximum problem and to obtain the latent emotion distribution of the author. Finally, a supervised learning method is utilized to assign the sentiment polarity to a given online review. Experiments show that our approach is effective and outperforms state-of-the-art approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
</authors>
<title>Fernando Pereira, et al.2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<booktitle>In ACL,</booktitle>
<volume>7</volume>
<pages>440--447</pages>
<marker>Blitzer, Dredze, </marker>
<rawString>John Blitzer, Mark Dredze, Fernando Pereira, et al.2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL, volume 7, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenyuan Dai</author>
<author>Gui-Rong Xue</author>
<author>Qiang Yang</author>
<author>Yong Yu</author>
</authors>
<title>Co-clustering based classification for outof-domain documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>210--219</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2453" citStr="Dai et al., 2007" startWordPosition="363" endWordPosition="366">zing the labeled data for CSC. However, in some practical cases, we may have many labeled data for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sentiment. On the other hand, some researchers focus on the usage of the s</context>
</contexts>
<marker>Dai, Xue, Yang, Yu, 2007</marker>
<rawString>Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong Yu. 2007. Co-clustering based classification for outof-domain documents. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 210– 219. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum′e</author>
</authors>
<title>Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815.</title>
<date>2009</date>
<marker>Daum′e, 2009</marker>
<rawString>Hal Daum′e III. 2009. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sheng Gao</author>
<author>Haizhou Li</author>
</authors>
<title>A cross-domain adaptation method for sentiment classification using probabilistic latent analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information Learning,</booktitle>
<pages>222--230</pages>
<contexts>
<context position="2418" citStr="Gao and Li, 2011" startWordPosition="355" endWordPosition="358">thms often train a classifier utilizing the labeled data for CSC. However, in some practical cases, we may have many labeled data for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sentiment. On the other hand, some rese</context>
</contexts>
<marker>Gao, Li, 2011</marker>
<rawString>Sheng Gao and Haizhou Li. 2011. A cross-domain adaptation method for sentiment classification using probabilistic latent analysis. In Proceedings of the 20th ACM international conference on Information Learning, pages 222–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Chenghua Lin</author>
<author>Harith Alani</author>
</authors>
<title>Automatically extracting polarity-bearing topics for cross-domain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>123--131</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2400" citStr="He et al., 2011" startWordPosition="351" endWordPosition="354">e learning algorithms often train a classifier utilizing the labeled data for CSC. However, in some practical cases, we may have many labeled data for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sentiment. On the oth</context>
<context position="14152" citStr="He et al., 2011" startWordPosition="2300" endWordPosition="2303"> of using domain information and word polarity We show the effectiveness of using domain information and word polarity, which are employed in our approach. For this purpose, we repeat the experiment without introducing domain and word polarity (node u and node r) into the model. Figure 2 shows the results. As it is clear, the highest performance can be achieved when domain information and word polarity are both used, while the lowest performance is obtained when neither of them is used. Comparison with the Baselines We compare our proposed approach with PLSA, SVM, SFA [Pan et al., 2010], JST [He et al., 2011] and HIDC [Zhuang et al., 2013]. The experimental results of the 12 classification tasks are shown in Figure 3. It can be observed that our 1 http://www.cs.pitt.edu/mpqa Figure 2: Effectiveness of using domain information and word polarity Figure 3: Comparison with the Baselines proposed approach outperforms all the other approaches in general. Note that in order to obtain a more precise comparison of the algorithms, we do neither the instance selection nor the complicated feature selection. The result of our proposed approach can possibly be improved with the help of these selection strategi</context>
</contexts>
<marker>He, Lin, Alani, 2011</marker>
<rawString>Yulan He, Chenghua Lin, and Harith Alani. 2011. Automatically extracting polarity-bearing topics for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 123–131. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>50--57</pages>
<publisher>ACM.</publisher>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50– 57. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoushan Li</author>
<author>Chu-Ren Huang</author>
<author>Guodong Zhou</author>
<author>Sophia Yat Mei Lee</author>
</authors>
<title>Employing personal/ impersonal views in supervised and semisupervised sentiment classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>414--423</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Li, Huang, Zhou, Lee, 2010</marker>
<rawString>Shoushan Li, Chu-Ren Huang, Guodong Zhou, and Sophia Yat Mei Lee. 2010. Employing personal/ impersonal views in supervised and semisupervised sentiment classification. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 414–423. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Sinno Jialin Pan</author>
<author>Ou Jin</author>
<author>Qiang Yang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Cross-domain co-extraction of sentiment and topic lexicons.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>410--419</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2435" citStr="Li et al., 2012" startWordPosition="359" endWordPosition="362"> classifier utilizing the labeled data for CSC. However, in some practical cases, we may have many labeled data for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sentiment. On the other hand, some researchers focus on </context>
</contexts>
<marker>Li, Pan, Jin, Yang, Zhu, 2012</marker>
<rawString>Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and Xiaoyan Zhu. 2012. Cross-domain co-extraction of sentiment and topic lexicons. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 410–419. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/ topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACMconference on Information and knowledge management,</booktitle>
<pages>375--384</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2548" citStr="Lin and He, 2009" startWordPosition="383" endWordPosition="386">for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sentiment. On the other hand, some researchers focus on the usage of the sentiment in CSC study [Mitra et al., 2013a; Mitra et al., 2013b; He et al., 2011]. [He et al., </context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/ topic model for sentiment analysis. In Proceedings of the 18th ACMconference on Information and knowledge management, pages 375–384. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingsheng Long</author>
<author>Jianmin Wang</author>
<author>Guiguang Ding</author>
<author>Wei Cheng</author>
<author>Xiang Zhang</author>
<author>Wei Wang</author>
</authors>
<title>Dual transfer learning.</title>
<date>2012</date>
<booktitle>In SDM,</booktitle>
<pages>540--551</pages>
<publisher>SIAM.</publisher>
<contexts>
<context position="2530" citStr="Long et al., 2012" startWordPosition="379" endWordPosition="382"> many labeled data for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sentiment. On the other hand, some researchers focus on the usage of the sentiment in CSC study [Mitra et al., 2013a; Mitra et al., 2013b; He et al., 2</context>
</contexts>
<marker>Long, Wang, Ding, Cheng, Zhang, Wang, 2012</marker>
<rawString>Mingsheng Long, Jianmin Wang, Guiguang Ding, Wei Cheng, Xiang Zhang, and Wei Wang. 2012. Dual transfer learning. In SDM, pages 540–551. SIAM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Man lan</author>
</authors>
<title>and chew lim tan. 2013a. from semantic to emotional space in probabilistic sense sentiment analysis.</title>
<booktitle>In the 27th AAAI Conference on Artificial Intelligence.</booktitle>
<marker>lan, </marker>
<rawString>Mitra Mohtarami. Man lan, and chew lim tan. 2013a. from semantic to emotional space in probabilistic sense sentiment analysis. In the 27th AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Cross-domain sentiment classification via spectral feature alignment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>751--760</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2492" citStr="Pan et al., 2010" startWordPosition="371" endWordPosition="374"> in some practical cases, we may have many labeled data for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sentiment. On the other hand, some researchers focus on the usage of the sentiment in CSC study [Mitra et al., 20</context>
<context position="14129" citStr="Pan et al., 2010" startWordPosition="2295" endWordPosition="2298">ion tasks. Effectiveness of using domain information and word polarity We show the effectiveness of using domain information and word polarity, which are employed in our approach. For this purpose, we repeat the experiment without introducing domain and word polarity (node u and node r) into the model. Figure 2 shows the results. As it is clear, the highest performance can be achieved when domain information and word polarity are both used, while the lowest performance is obtained when neither of them is used. Comparison with the Baselines We compare our proposed approach with PLSA, SVM, SFA [Pan et al., 2010], JST [He et al., 2011] and HIDC [Zhuang et al., 2013]. The experimental results of the 12 classification tasks are shown in Figure 3. It can be observed that our 1 http://www.cs.pitt.edu/mpqa Figure 2: Effectiveness of using domain information and word polarity Figure 3: Comparison with the Baselines proposed approach outperforms all the other approaches in general. Note that in order to obtain a more precise comparison of the algorithms, we do neither the instance selection nor the complicated feature selection. The result of our proposed approach can possibly be improved with the help of t</context>
</contexts>
<marker>Pan, Ni, Sun, Yang, Chen, 2010</marker>
<rawString>Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain sentiment classification via spectral feature alignment. In Proceedings of the 19th international conference on World wide web, pages 751–760. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>pages</pages>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79– 86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Plutchik</author>
</authors>
<title>Emotion: A psychoevolutionary synthesis. Harpercollins College Division. Robert Plutchik.</title>
<date>1980</date>
<marker>Plutchik, 1980</marker>
<rawString>Robert Plutchik. 1980. Emotion: A psychoevolutionary synthesis. Harpercollins College Division. Robert Plutchik. 2002. Emotion and life.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David C Rubin</author>
<author>Jennifer M Talarico</author>
</authors>
<title>A comparison of dimensional models of emotion: Evidence from emotions, prototypical events, autobiographical memories, and words.</title>
<date>2009</date>
<journal>Memory,</journal>
<volume>17</volume>
<issue>8</issue>
<marker>Rubin, Talarico, 2009</marker>
<rawString>David C Rubin and Jennifer M Talarico. 2009. A comparison of dimensional models of emotion: Evidence from emotions, prototypical events, autobiographical memories, and words. Memory, 17(8):802–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Schlosberg</author>
</authors>
<title>Three dimensions of emotion.</title>
<date>1954</date>
<journal>Psychological review,</journal>
<volume>61</volume>
<issue>2</issue>
<contexts>
<context position="4895" citStr="Schlosberg, 1954" startWordPosition="745" endWordPosition="746">ct. For example, a movie review in IMDB might voice the feeling about the movie by a reviewer [Yu et al., 2013]. (2) Emotion Layer Based on the emotion classification theories in psychology [Plutchik, 2002], the emotion can be classified into the basic ones influenced by the physiological factors, e.g. happiness, sadness, anger, etc., and dozens of complicated ones formed under some specific social conditions, e.g. shame, guilt, abashment, etc. Additionally, the emotion can be classified as positive and negative (similar to the sentiment classification) based on dimensional models of emotion [Schlosberg, 1954; Plutchik, 2002; Rubin and Talerico, 2009]. Intuitively, we assume that a document tends to contain the emotions of similar polarity. (3) Lexicon Layer To build the connection between words and the emotion, we introduce emotional words instead of raw word features into our model. By utilizing the emotional lexicon MPQA [Wiebe et al., 2005], we select groups of strong polar words, which get high scores in the emotional lexicon. These words are considered highly correlated to the certain emotion of the same polarity. And these strong polar words have invariant polarity across domains. Therefore</context>
</contexts>
<marker>Schlosberg, 1954</marker>
<rawString>Harold Schlosberg. 1954. Three dimensions of emotion. Psychological review, 61(2):81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wang</author>
<author>Heng Huang</author>
<author>Feiping Nie</author>
<author>Chris Ding</author>
</authors>
<title>Cross-language web page classification via dual knowledge transfer using nonnegative matrix trifactorization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval,</booktitle>
<pages>933--942</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2511" citStr="Wang et al., 2011" startWordPosition="375" endWordPosition="378"> cases, we may have many labeled data for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sentiment. On the other hand, some researchers focus on the usage of the sentiment in CSC study [Mitra et al., 2013a; Mitra et al., </context>
</contexts>
<marker>Wang, Huang, Nie, Ding, 2011</marker>
<rawString>Hua Wang, Heng Huang, Feiping Nie, and Chris Ding. 2011. Cross-language web page classification via dual knowledge transfer using nonnegative matrix trifactorization. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 933–942. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="5236" citStr="Wiebe et al., 2005" startWordPosition="797" endWordPosition="800">d dozens of complicated ones formed under some specific social conditions, e.g. shame, guilt, abashment, etc. Additionally, the emotion can be classified as positive and negative (similar to the sentiment classification) based on dimensional models of emotion [Schlosberg, 1954; Plutchik, 2002; Rubin and Talerico, 2009]. Intuitively, we assume that a document tends to contain the emotions of similar polarity. (3) Lexicon Layer To build the connection between words and the emotion, we introduce emotional words instead of raw word features into our model. By utilizing the emotional lexicon MPQA [Wiebe et al., 2005], we select groups of strong polar words, which get high scores in the emotional lexicon. These words are considered highly correlated to the certain emotion of the same polarity. And these strong polar words have invariant polarity across domains. Therefore, the emotion can be substantialized by a series of emotional words drawn from corresponding probability distribution. (4) Expression Layer In many practical cases, data come from different domains. We suppose that the correlation between emotion state and sentiment orientation is stable over domains, but one emotion may have different exp</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongliang Yu</author>
<author>Zhi-Hong Deng</author>
<author>Shiyingxue Li</author>
</authors>
<title>Identifying sentiment words using an optimization-based model without seed words.</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>855--859</pages>
<contexts>
<context position="4389" citStr="Yu et al., 2013" startWordPosition="672" endWordPosition="675">n expressing the emotion “happiness”, one uses “bravo” in the domain of sport, while “yummy” in the domain of food. Therefore, we propose an EA framework to model the latent emotions which are commonly contained in subjective articles and expressed by “emotional words”. We infer the sentiment polarity of a document based on the emotion state. The hierarchy of EA is composed by four layers: (1) Sentiment Layer Normally, the sentiment of a document is the general opinion towards a certain event or object. For example, a movie review in IMDB might voice the feeling about the movie by a reviewer [Yu et al., 2013]. (2) Emotion Layer Based on the emotion classification theories in psychology [Plutchik, 2002], the emotion can be classified into the basic ones influenced by the physiological factors, e.g. happiness, sadness, anger, etc., and dozens of complicated ones formed under some specific social conditions, e.g. shame, guilt, abashment, etc. Additionally, the emotion can be classified as positive and negative (similar to the sentiment classification) based on dimensional models of emotion [Schlosberg, 1954; Plutchik, 2002; Rubin and Talerico, 2009]. Intuitively, we assume that a document tends to c</context>
</contexts>
<marker>Yu, Deng, Li, 2013</marker>
<rawString>Hongliang Yu, Zhi-Hong Deng, and Shiyingxue Li. 2013. Identifying sentiment words using an optimization-based model without seed words. In ACL (2), pages 855–859.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuzhen Zhuang</author>
<author>Ping Luo</author>
<author>Peifeng Yin</author>
<author>Qing He</author>
<author>Zhongzhi Shi</author>
</authors>
<title>Concept learning for crossdomain text classification: A general probabilistic framework.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,</booktitle>
<pages>1960--1966</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="2383" citStr="Zhuang et al., 2013" startWordPosition="347" endWordPosition="350">s. Traditional machine learning algorithms often train a classifier utilizing the labeled data for CSC. However, in some practical cases, we may have many labeled data for some domains (source domains) but very few or no labeled data for other domains (target domains). Due to the differences of the distribution of two domains in terms of raw features, e.g. raw term frequency, the classifier trained from the source domain often performs badly on the target domain. To overcome this issue, several feature-based studies have been proposed to improve the sentiment classification domain adaptation [Zhuang et al., 2013; He et al., 2011; Gao and Li, 2011; Li et al., 2012; Dai et al., 2007; Zhuang et al., 2010; Pan et al., 2010; Wang et al., 2011; Long et al., 2012; Lin and He, 2009]. Existing studies build various generative models to solve the domain adaptation problems for CSC. In most cases, the models are trained by using the whole corpora without specifying on the sentiment of the texts. For example, [Zhuang et al., 2013] propose a general framework HIDC to mine high-level concepts (e.g. word clusters) across various domains. However, their learned concepts contain many topics not restricted to the sent</context>
<context position="10066" citStr="Zhuang et al., 2013" startWordPosition="1612" endWordPosition="1615">e: 𝑝 (𝑒, 𝑦, 𝑤𝑒, 𝑑, 𝑟, 𝑢) = 𝑝(𝑤𝑒|𝑒,𝑟,𝑢)𝑝(𝑑|𝑟,𝑦)𝑝(𝑒|𝑦)𝑝(𝑟)𝑝(𝑦)𝑝(𝑢) (4) We need to learn the unobservable probabilities (e.g., 𝑝 (𝑤𝑒 |𝑒, 𝑟, 𝑢), 𝑝 (𝑑 |𝑟, 𝑦), 𝑝 (𝑒 |𝑦), 𝑝 (𝑦) ) to infer the hidden emotion distribution. Therefore, we develop an Expectation-Maximization (EM) algorithm to maximize the log likelihood of generating the whole dataset and obtain the iterative formula in E-step as follows: 𝑝 (𝑒, 𝑦 |𝑤𝑒, 𝑑, 𝑟, 𝑢) = 𝑝(𝑤𝑒 |𝑒, 𝑟, 𝑢)𝑝(𝑑 |𝑟, 𝑦)𝑝(𝑒 |𝑦)𝑝(𝑟)𝑝(𝑦)𝑝(𝑢) (5) ∑𝑒,𝑦𝑝(𝑤𝑒|𝑒, 𝑟,𝑢)𝑝(𝑑|𝑟,𝑦)𝑝(𝑒|𝑦)𝑝(𝑟)𝑝(𝑦)𝑝(𝑢) where all the factors are calculated in M-step similar to PLSA and HIDC (Hoffman, 1999; Zhuang et al., 2013). 2.3 CSC via JEAM To use JEAM to solve CSC problems, we adopt two optimizations: First, we supervise the EM optimization with the polarity information of emotional words and instances respectively in the source domain. On the one hand, we estimate 𝑝 (𝑒, 𝑦 |𝑤𝑒, 𝑑, 𝑟, 𝑢) utilizing the polarity label of the emotional words. Let the emotion set 𝐸 be divided into positive set 𝐸𝑝 and negative set 𝐸𝑛 . We set 𝑝(𝑒𝑖, 𝑦|𝑤𝑒, 𝑑, 𝑟, 𝑢𝑤𝑒) = 0 during the whole EM process when the polarities of the emotion and current emotional word are different. On the other hand, we estimate the probability 𝑝 (𝑑 |𝑟, 𝑦) wi</context>
<context position="14183" citStr="Zhuang et al., 2013" startWordPosition="2306" endWordPosition="2309">n and word polarity We show the effectiveness of using domain information and word polarity, which are employed in our approach. For this purpose, we repeat the experiment without introducing domain and word polarity (node u and node r) into the model. Figure 2 shows the results. As it is clear, the highest performance can be achieved when domain information and word polarity are both used, while the lowest performance is obtained when neither of them is used. Comparison with the Baselines We compare our proposed approach with PLSA, SVM, SFA [Pan et al., 2010], JST [He et al., 2011] and HIDC [Zhuang et al., 2013]. The experimental results of the 12 classification tasks are shown in Figure 3. It can be observed that our 1 http://www.cs.pitt.edu/mpqa Figure 2: Effectiveness of using domain information and word polarity Figure 3: Comparison with the Baselines proposed approach outperforms all the other approaches in general. Note that in order to obtain a more precise comparison of the algorithms, we do neither the instance selection nor the complicated feature selection. The result of our proposed approach can possibly be improved with the help of these selection strategies. 4 Conclusion In this paper,</context>
</contexts>
<marker>Zhuang, Luo, Yin, He, Shi, 2013</marker>
<rawString>Fuzhen Zhuang, Ping Luo, Peifeng Yin, Qing He, and Zhongzhi Shi. 2013. Concept learning for crossdomain text classification: A general probabilistic framework. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 1960–1966. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuzhen Zhuang</author>
<author>Ping Luo</author>
<author>Changying Du</author>
<author>Qing He</author>
<author>Zhongzhi Shi</author>
<author>Hui Xiong</author>
</authors>
<title>Triplex transfer learning: exploiting both shared and distinct concepts for text classification.</title>
<date>2014</date>
<journal>Cybernetics, IEEE Transactions on,</journal>
<volume>44</volume>
<issue>7</issue>
<marker>Zhuang, Luo, Du, He, Shi, Xiong, 2014</marker>
<rawString>Fuzhen Zhuang, Ping Luo, Changying Du, Qing He, Zhongzhi Shi, and Hui Xiong. 2014. Triplex transfer learning: exploiting both shared and distinct concepts for text classification. Cybernetics, IEEE Transactions on, 44(7):1191–1203.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000179">
<title confidence="0.994432">
Experiments with Generative Models for Dependency Tree Linearization
</title>
<author confidence="0.997191">
Richard Futrell and Edward Gibson
</author>
<affiliation confidence="0.9981025">
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.9901">
{futrell, egibson}@mit.edu
</email>
<sectionHeader confidence="0.993636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984090909091">
We present experiments with generative
models for linearization of unordered la-
beled syntactic dependency trees (Belz et
al., 2011; Rajkumar and White, 2014).
Our linearization models are derived from
generative models for dependency struc-
ture (Eisner, 1996). We present a series of
generative dependency models designed
to capture successively more information
about ordering constraints among sister
dependents. We give a dynamic program-
ming algorithm for computing the condi-
tional probability of word orders given tree
structures under these models. The models
are tested on corpora of 11 languages us-
ing test-set likelihood, and human ratings
for generated forms are collected for En-
glish. Our models benefit from represent-
ing local order constraints among sisters
and from backing off to less sparse distri-
butions, including distributions not condi-
tioned on the head.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969">
We explore generative models for producing lin-
earizations of unordered labeled syntactic depen-
dency trees. This specific task has attracted at-
tention in recent years (Filippova and Strube,
2009; He et al., 2009; Belz et al., 2011; Bohnet
et al., 2012; Zhang, 2013) because it forms
a useful part of a natural language generation
pipeline, especially in machine translation (Chang
and Toutanova, 2007) and summarization (Barzi-
lay and McKeown, 2005). Closely related tasks
are generation of sentences given CCG parses
(White and Rajkumar, 2012), bags of words (Liu
et al., 2015), and semantic graphs (Braune et al.,
2014).
Here we focus narrowly on testing probabilistic
generative models for dependency tree lineariza-
tion. In contrast, the approach in most previ-
ous work is to apply a variety of scoring func-
tions to trees and linearizations and search for an
optimally-scoring tree among some set. The prob-
abilistic linearization models we investigate are
derived from generative models for dependency
trees (Eisner, 1996), as most commonly used in
unsupervised grammar induction (Klein and Man-
ning, 2004; Gelling et al., 2012). Generative de-
pendency models have typically been evaluated
in a parsing task (Eisner, 1997). Here, we are
interested in the inverse task: inferring a distri-
bution over linear orders given unordered depen-
dency trees.
This is the first work to consider generative de-
pendency models from the perspective of word
ordering. The results can potentially shed light
on how ordering constraints are best represented
in such models. In addition, the use of proba-
bilistic models means that we can easily define
well-motivated normalized probability distribu-
tions over orders of dependency trees. These dis-
tributions are useful for answering scientific ques-
tions about crosslinguistic word order in quan-
titative linguistics, where obtaining robust esti-
mates has proven challenging due to data sparsity
(Futrell et al., 2015).
The remainder of the work is organized as fol-
lows. In Section 2 we present a set of generative
linearization models. In Section 3 we compare
the performance of the different models as mea-
sured by test-set probability and human accept-
ability ratings. We also compare our performance
with other systems from the literature. Section 4
concludes.
</bodyText>
<sectionHeader confidence="0.958205" genericHeader="method">
2 Generative Models for Projective
Dependency Tree Linearization
</sectionHeader>
<bodyText confidence="0.996105">
We investigate head-outward projective generative
dependency models. In these models, an ordered
dependency tree is generated by the following kind
</bodyText>
<page confidence="0.962502">
1978
</page>
<note confidence="0.859288">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1978–1983,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.96620575">
ROOT
root
comes
from the this
</figure>
<figureCaption confidence="0.9082245">
Figure 1: Example unordered dependency tree.
Possible linearizations include (1) This story
</figureCaption>
<bodyText confidence="0.954943181818182">
comes from the AP and (2) From the AP comes this
story. Order 2 is the original order in the corpus,
but order 1 is much more likely under our models.
of procedure. Given a head node, we use some
generative process G to generate a depth-1 sub-
tree rooted in that head node. Then we apply
the procedure recursively to each of the depen-
dent nodes. By applying the procedure starting at
a ROOT node, we generate a dependency tree. For
example, to generate the dependency tree in Fig-
ure 1 from the node comes down, we take the head
</bodyText>
<equation confidence="0.917624">
nmod
nsubj
</equation>
<bodyText confidence="0.860400166666667">
comes and generate the subtree story AP ,
story
then we take the head story and generate
and so on. In this work, we experiment with differ-
ent specific generative processes G which generate
a local subtree conditioned on a head.
</bodyText>
<subsectionHeader confidence="0.997211">
2.1 Model Types
</subsectionHeader>
<bodyText confidence="0.999981323529412">
Here we describe some possible generative pro-
cesses G which generate subtrees conditioned on
a head. These models contain progressively more
information about ordering relations among sister
dependents.
A common starting point for G is
Eisner Model C (Eisner, 1996). In this model,
dependents on one side of the head are generated
by repeatedly sampling from a categorical distri-
bution until a special stop-symbol is generated.
The model only captures the propensity of depen-
dents to appear on the left or right of the head, and
does not capture any order constraints between
sister dependents on one side of the head.
We consider a generalization of Eisner Model C
which we call Dependent N-gram models. In
a Dependent N-gram model, we generate depen-
dents on each side the head by sampling a se-
quence of dependents from an N-gram model.
Each dependent is generated conditional on the
N − 1 previously generated dependents from
the head outwards. We have two separate N-
gram sequence distributions for left and right
dependents. Eisner Model C can be seen as a
Dependent N-gram model with N = 1.
We also consider a model which can capture
many more ordering relations among sister depen-
dents: given a head h, sample a subtree whose
head is h from a Categorical distribution over sub-
trees. We call this the Observed Orders model
because in practice we are simply sampling one of
the observed orders from the training data. This
generative process has the capacity to capture the
most ordering relations between sister dependents.
</bodyText>
<subsectionHeader confidence="0.4967855">
2.1.1 Distributions over Permutations of
Dependents
</subsectionHeader>
<bodyText confidence="0.999975928571429">
We have discussed generative models for ordered
dependency trees. Here we discuss how to use
them to make generative models for word orders
conditional on unordered dependency trees.
Suppose we have a generative process G for de-
pendency trees which takes a head h and gener-
ates a sequence of dependents wl to the left of h
and a sequence of dependents wr to the right of h.
Let w denote the pair (wl, wr), which we call the
configuration of dependents. To get the probabil-
ity of some w given an unordered subtree u, we
want to calculate the probability of w given that G
has generated the particular multiset W of depen-
dents corresponding to u. To do this, we calculate:
</bodyText>
<equation confidence="0.9988355">
p(w|W) = p(w, W)
(1)
</equation>
<bodyText confidence="0.617442">
where
</bodyText>
<equation confidence="0.9988685">
�Z = p(w�) (2)
w&apos;∈W
</equation>
<bodyText confidence="0.999866076923077">
and W is the set of all possible configurations
(wl, wr) compatible with multiset W. That is, W
is the set of pairs of permutations of multisets Wl
and Wr for all possible partitions of W into Wl
and Wr. The generative dependency model gives
us the probability p(w).
It remains to calculate the normalizing constant
Z, the sum of probabilities of possible configura-
tions. For the Observed Orders model, Z is the
sum of probabilities of subtrees with the same de-
pendents as subtree u. For the Dependent N-gram
models of order N, we calculate Z using a dy-
namic programming algorithm, presented in Al-
</bodyText>
<figure confidence="0.993544923076923">
nmod
AP
nsubj
det
case det
story
comes
det
this
,
p(W)
p(w)
Z ,
</figure>
<page confidence="0.977425">
1979
</page>
<bodyText confidence="0.9771725">
gorithm 1 as memoized recursive functions. When
N = 1 (Eisner Model C), Z is more simply:
</bodyText>
<equation confidence="0.9893246">
Zemc = pL(stop) × pR(stop)
|Wl|! × |Wr|!
(Wl,W,)∈PARTS(W)
�× pL(w) �
w∈Wl w∈W,
</equation>
<bodyText confidence="0.999778647058823">
where PARTS(W) is the set of all partitions of
multiset W into two multisets Wl and Wr, pL
is the probability mass function for a dependent to
the left of the head, pR is the function for a depen-
dent to the right, and stop is a special symbol in
the support of pL and pR which indicates that gen-
eration of dependents should halt. The probability
mass functions may be conditional on the head h.
These methods for calculating Z make it possible
to transform a generative dependency model into a
model of dependency tree ordering conditional on
local subtree structure.
Algorithm 1 Compute the sum of probabilities
of all configurations of dependents W under a
Dependent N-gram model with two component N-
gram models of order N: pR for sequences to the
right of the head and pL for sequences to the left.
</bodyText>
<equation confidence="0.931768272727273">
memoized function RIGHT NORM(r, c)
if |r |= 0 then
return pR(stop  |c)
end if
Z ← 0
for i = 1 : |r |do
r0 ← elements of r except the ith
c0 ← append ri to c then truncate to length N − 1
Z ← Z + pR(ri|c) × RIGHT NORM(r0, c0)
end for
return Z
end memoized function
memoized function LEFT NORM(r, c)
Z ← pL(stop  |c) × RIGHT NORM([start], r)
for i = 1 : |r |do
r0 ← elements of r except the ith
c0 ← append ri to c then truncate to length N − 1
Z ← Z + pL(ri|c) × LEFT NORM(r0, c0)
end for
return Z
end memoized function
Result is LEFT NORM(W, [start])
</equation>
<subsectionHeader confidence="0.997122">
2.2 Labelling
</subsectionHeader>
<bodyText confidence="0.99994605">
The previous section discussed the question of
the structure of the generative process for depen-
dency trees. Here we discuss an orthogonal mod-
eling question, which we call labelling: what in-
formation about the labels on dependency tree
nodes and edges should be included in our mod-
els. Dependency tree nodes are labeled with word-
forms, lemmas, and parts-of-speech (POS) tags;
and dependency tree edges are labeled with rela-
tion types. A model might generate orders of de-
pendents conditioned on all of these labels, or a
subset of them. For example, a generative depend-
necy model might generate (relation type, depen-
dent POS tag) tuples conditioned on the POS tag
of the head of the phrase. When we use such a
model for dependency linearization, we would say
the model’s labelling is relation type, dependent
POS, and head POS. In this study, we avoid in-
cluding wordforms or lemmas in the labelling, to
avoid data sparsity issues.
</bodyText>
<subsectionHeader confidence="0.999484">
2.3 Model Estimation and Smoothing
</subsectionHeader>
<bodyText confidence="0.9999752">
In order to alleviate data sparsity in fitting our
models, we adopt two smoothing methods from
the language modelling literature.
All categorical distributions are estimated us-
ing add-k smoothing where k = 0.01. For the
Dependent N-gram models, this means adding k
pseudocounts for each possible dependent in each
context. For the Observed Orders model, this
means adding k pseudocounts for each possible
permutation of the head and its dependents.
We also experiment with combining our mod-
els into mixture distributions. This can be viewed
as a kind of back-off smoothing (Katz, 1987),
where the Observed Orders model is the model
with the most context, and Dependent N-grams
and Eisner Model C are backoff distributions with
successively less context. Similarly, models with
less information in the labelling can serve as back-
off distributions for models with more information
in the labelling. For example, a model which is
conditioned on the POS of the head can be backed
off to a model which does not condition on the
head at all. We find optimal mixture weights us-
ing the Baum-Welch algorithm tuned on a held-out
development set.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9996115">
Here we empirically evaluate some options for
model type and model labelling as described
above. We are interested in how many of the pos-
sible orders of a sentence our model can generate
(recall), and in how many of our generated orders
really are acceptable (precision). As a recall-like
measure, we quantify the probability of the word
orders of held-out test sentences. Low probabil-
</bodyText>
<figure confidence="0.842299666666667">
×
(3)
pR(w),
</figure>
<page confidence="0.911518">
1980
</page>
<table confidence="0.999481769230769">
Labelling Model Basque Czech English Finnish French German Hebrew Indonesian Persian Spanish Swedish
DR oo -6.83 -7.58 -5.23 -7.35 -10.86 -8.36 -9.74 -8.99 -10.39 -11.31 -8.83
H n1 -6.12 -8.97 -5.08 -7.15 -11.54 -9.81 -9.63 -8.68 -10.63 -13.19 -8.37
n2 -4.86 -6.35 -2.87 -5.30 -6.86 -6.60 -5.91 -5.98 -5.54 -7.47 -4.92
n3 -5.92 -6.59 -3.13 -5.68 -7.34 -7.02 -6.81 -6.69 -6.49 -8.06 -5.68
n123 -4.58 -6.18 -2.60 -5.11 -6.67 -6.19 -5.77 -5.73 -5.51 -7.36 -4.72
oo+n123 -4.52 -5.95 -2.57 -5.04 -6.58 -5.92 -5.68 -5.68 -5.47 -7.27 -4.68
R oo -5.56 -6.78 -3.94 -6.25 -9.63 -7.42 -7.95 -7.51 -9.19 -9.54 -7.28
+R n1 -6.08 -8.97 -5.07 -7.16 -11.54 -9.79 -9.58 -8.67 -10.62 -13.17 -8.35
D n2 -4.49 -6.31 -2.62 -5.17 -6.79 -6.34 -5.62 -5.67 -5.42 -7.40 -4.67
H n3 -4.86 -6.41 -2.61 -5.20 -7.08 -6.43 -6.07 -6.02 -6.04 -7.70 -5.02
n123 -4.41 -6.15 -2.48 -5.01 -6.59 -5.99 -5.54 -5.53 -5.42 -7.29 -4.53
oo+n123 -4.29 -5.84 -2.44 -4.88 -6.50 -5.74 -5.40 -5.47 -5.38 -7.09 -4.46
</table>
<tableCaption confidence="0.998841">
Table 1: Average log likelihood of word order per sentence in test set under various models. Under
</tableCaption>
<bodyText confidence="0.915741705882353">
“Labelling”, HDR means conditioning on Head POS, Dependent POS, and Relation Type, and R means
conditioning on Relation Type alone (see Section 2.2). Under “Model”, oo is the Observed Orders model,
n1 is the Dependent 1-gram model (Eisner Model C), n2 is the Dependent 2-gram model, and n3 is the
Dependent 3-gram model (see Section 2.1). In both columns, x+y means a mixture of model x and
model y; n123 means n1+n2+n3.
ities assigned to held-out sentences indicate that
there are possible orders which our model is miss-
ing. As a precision-like measure, we get human
acceptability ratings for sentence reorderings gen-
erated by our model.
We carry out our evaluations using the de-
pendency corpora of the Universal Dependen-
cies project (v1.1) (Agi´c et al., 2015), with the
train/dev/test splits provided in that dataset. We
remove nodes and edges dealing with punctuation.
Due to space constraints, we only present results
from 11 languages here.
</bodyText>
<subsectionHeader confidence="0.990361">
3.1 Test-Set Probability
</subsectionHeader>
<bodyText confidence="0.999933095238095">
Here we calculate average probabilities of word
orders per sentence in the test set. This number can
be interpreted as the (negative) average amount of
information contained in the word order of a sen-
tence beyond information about dependency rela-
tions.
The results for selected languages are shown
in Table 1. The biggest gains come from us-
ing Dependent N-gram models with N &gt; 1,
and from backing off the model labelling. The
Observed Orders model does poorly on its own,
likely due to data sparsity; its performance is
much improved when backing off from condition-
ing on the head. Eisner Model C (n1) also per-
forms poorly, likely because it cannot represent
any ordering constraints among sister dependents.
The fact it helps to back off to distributions not
conditioned on the head suggests that there are
commonalities among distributions of dependents
of different heads, which could be exploited in fur-
ther generative dependency models.
</bodyText>
<subsectionHeader confidence="0.999379">
3.2 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.996727068965517">
We collected human ratings for sentence reorder-
ings sampled from the English models from 54 na-
tive American English speakers on Amazon Me-
chanical Turk. We randomly selected a set of 90
sentences from the test set of the English Universal
Dependencies corpus. We generated a reordering
of each sentence according to each of 12 model
configurations in Table 1. Each participant saw
an original sentence and a reordering of it, and
was asked to rate how natural each version of the
sentence sounded, on a scale of 1 to 5. The or-
der of presentation of the original and reordered
forms was randomized, so that participants were
not aware of which form was the original and
which was a reordering. Each participant rated
56 sentence pairs. Participants were also asked
whether the two sentences in a pair meant the same
thing, with “can’t tell” as a possible answer.
Table 2 shows average human acceptability rat-
ings for reorderings, and the proportion of sen-
tence pairs judged to mean the same thing. The
original sentences have an average acceptability
rating of 4.48/5. The very best performing models
are those which do not back off to a distribution
not conditioned on the head. However, in the case
of the Observed Orders and other sparse models,
we see consistent improvement from this backoff.
Figure 2 shows the acceptability ratings (out of
5) plotted against test set probability. We see that
</bodyText>
<page confidence="0.969606">
1981
</page>
<table confidence="0.999111642857143">
Labelling Model Acceptability Same Meaning
oo HDR 0.58
n1 2.92 0.44
n2 2.06 0.78
n3 3.42 0.85
n123 3.48 0.79
oo+n123 3.56 0.75
3.45
R oo 3.11 0.72
+R n1 2.11 0.49
D n2 3.32 0.80
H n3 3.52 0.77
n123 3.31 0.76
oo+n123 3.43 0.80
</table>
<tableCaption confidence="0.995542">
Table 2: Mean acceptability rating out of 5, and
</tableCaption>
<bodyText confidence="0.639204333333333">
proportion of reordered sentences with the same
meaning as the original, for English models. La-
bels as in Table 1.
</bodyText>
<figure confidence="0.959708428571429">
3.6
3.2
2.8
2.4
2.0
−5 −4 −3
Test set log probability
</figure>
<figureCaption confidence="0.8414055">
Figure 2: Comparison of test set probability (Ta-
ble 1) and acceptability ratings (Table 2) for En-
</figureCaption>
<bodyText confidence="0.8742075">
glish across models. A least-squares linear regres-
sion line is shown. Labels as in Table 1.
the models which yield poor test set probability
also have poor acceptability ratings.
</bodyText>
<subsectionHeader confidence="0.999978">
3.3 Comparison with other systems
</subsectionHeader>
<bodyText confidence="0.999976341463415">
Previous work has focused on the ability to cor-
rectly reconstruct the word order of an observed
dependency tree. Our goal is to explicitly model a
distribution over possible orders, rather than to re-
cover a single correct order, because many orders
are often possible, and the particulator order that a
dependency tree originally appeared in might not
be the most natural. For example, our models typ-
ically reorder the sentence “From the AP comes
this story” (in Figure 1) as “This story comes from
the AP”; the second order is arguably more natu-
ral, though the first is idiomatic for this particular
phrase. So we do not believe that BLEU scores
and other metrics of similarity to a “correct” or-
dering are particularly relevant for our task.
Previous work uses BLEU scores (Papineni et
al., 2002) and human ratings to evaluate genera-
tion of word orders. To provide some comparabil-
ity with previous work, we report BLEU scores on
the 2011 Shared Task data here. The systems re-
ported in Belz et al. (2011) achieve BLEU scores
ranging from 23 to 89 for English; subsequent
work achieves BLEU scores of 91.6 on the same
data (Bohnet et al., 2012). Drawing the highest-
probability orderings from our models, we achieve
a top BLEU score of 57.7 using the model config-
uration hdr/oo. Curiously, hdr/oo is typically the
worst model configuration in the test set probabil-
ity evaluation (Section 3.1). The BLEU perfor-
mance is in the middle range of the Shared Task
systems. The human evaluation of our models is
more optimistic: the best score for Meaning Sim-
ilarity in the Shared Task was 84/100 (Bohnet et
al., 2011), while sentences ordered according to
our models were judged to have the same meaning
as the original in 85% of cases (Table 2), though
these figures are based on different data. These
comparisons suggest that these generative models
do not provide state-of-the-art performance, but do
capture some of the same information as previous
models.
</bodyText>
<subsectionHeader confidence="0.591606">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999973428571429">
Overall, the most effective models are the
Dependent N-gram models. The naive approach
to modeling order relations among sister depen-
dents, as embodied in the Observed Orders model,
does not generalize well. The result suggests that
models like the Dependent N-gram model might
be effective as generative dependency models.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999953285714286">
We have discussed generative models for depen-
dency tree linearization, exploring a path less trav-
eled by in the dependency linearization literature.
We believe this approach has value for answering
scientific questions in quantitative linguistics and
for better understanding the linguistic adequacy of
generative dependency models.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998952">
We thank William P. Li, Kyle Mahowald, and Tim
O’Donnell for helpful discussions, and Michael
White for help accessing data.
</bodyText>
<figure confidence="0.995336181818182">
Mean acceptability
modeltype
n1
n123
n2
n3
oo
oo+n123
labelling
hdr
hdr+r
</figure>
<page confidence="0.981359">
1982
</page>
<sectionHeader confidence="0.98791" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999847614678899">
ˇZeljko Agi´c, Maria Jesus Aranzabe, Aitziber Atutxa,
Cristina Bosco, Jinho Choi, Marie-Catherine
de Marneffe, Timothy Dozat, Rich´ard Farkas,
Jennifer Foster, Filip Ginter, Iakes Goenaga,
Koldo Gojenola, Yoav Goldberg, Jan Hajiˇc, An-
ders Trærup Johannsen, Jenna Kanerva, Juha
Kuokkala, Veronika Laippala, Alessandro Lenci,
Krister Lind´en, Nikola Ljubeˇsi´c, Teresa Lynn,
Christopher Manning, H´ector Alonso Martinez,
Ryan McDonald, Anna Missil¨a, Simonetta Monte-
magni, Joakim Nivre, Hanna Nurmi, Petya Osen-
ova, Slav Petrov, Jussi Piitulainen, Barbara Plank,
Prokopis Prokopidis, Sampo Pyysalo, Wolfgang
Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi,
Kiril Simov, Aaron Smith, Reut Tsarfaty, Veronika
Vincze, and Daniel Zeman. 2015. Universal de-
pendencies 1.1. LINDAT/CLARIN digital library at
Institute of Formal and Applied Linguistics, Charles
University in Prague.
Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297–
328.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 217–226.
Bernd Bohnet, Simon Mille, Benoit Favre, and Leo
Wanner. 2011. StuMaBa : from deep represen-
tation to surface. In Proceedings of the 13th Eu-
ropean workshop on natural language generation,
pages 232–235.
Bernd Bohnet, Anders Bj¨orkelund, Jonas Kuhn, Wolf-
gang Seeker, and Sina Zarrieß. 2012. Generating
non-projective word order in statistical linearization.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 928–939.
Fabienne Braune, Daniel Bauer, and Kevin Knight.
2014. Mapping between english strings and reen-
trant semantic graphs. In Int. Conf. on Language
Resources and Evaluation (LREC).
Pi-Chuan Chang and Kristina Toutanova. 2007. A
discriminative syntactic word order model for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics, page 9.
Jason M Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of the 16th Conference on Computational
Linguistics, pages 340–345.
Jason M Eisner. 1997. An empirical comparison of
probability models for dependency grammar. Tech-
nical report, IRCS Report 96–11, University of
Pennsylvania.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Proceedings of NAACL-HLT
(Short Papers).
Richard Futrell, Kyle Mahowald, and Edward Gibson.
2015. Quantifying word order freedom in depen-
dency corpora. In Proceedings of the Third In-
ternational Conference on Dependency Linguistics
(Depling 2015), pages 91–100, Uppsala, Sweden.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac¸a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure, pages
64–80.
Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu.
2009. Dependency based Chinese sentence realiza-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 809–816.
Slava M Katz. 1987. Estimation of probabilities from
sparse data for the language model component of
a speech recognizer. Acoustics, Speech and Signal
Processing, IEEE Transactions on, 35(3):400–401.
Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics, page 478.
Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.
2015. Transition-based syntactic linearization. In
Proceedings of NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 311–318.
Rajakrishnan Rajkumar and Michael White. 2014.
Better surface realization through psycholinguistics.
Language and Linguistics Compass, 8(10):428–448.
Michael White and Rajakrishnan Rajkumar. 2012.
Minimal dependency length in realization ranking.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 244–255.
Yue Zhang. 2013. Partial-tree linearization: gener-
alized word ordering for text synthesis. In Pro-
ceedings of the Twenty-Third international joint con-
ference on Artificial Intelligence, pages 2232–2238.
AAAI Press.
</reference>
<page confidence="0.984089">
1983
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.926991">
<title confidence="0.999829">Experiments with Generative Models for Dependency Tree Linearization</title>
<author confidence="0.999696">Richard Futrell</author>
<author confidence="0.999696">Edward</author>
<affiliation confidence="0.991779">Department of Brain and Cognitive Massachusetts Institute of</affiliation>
<abstract confidence="0.99731847826087">We present experiments with generative models for linearization of unordered labeled syntactic dependency trees (Belz et al., 2011; Rajkumar and White, 2014). Our linearization models are derived from generative models for dependency structure (Eisner, 1996). We present a series of generative dependency models designed to capture successively more information about ordering constraints among sister dependents. We give a dynamic programming algorithm for computing the conditional probability of word orders given tree structures under these models. The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for English. Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>ˇZeljko Agi´c</author>
<author>Maria Jesus Aranzabe</author>
<author>Aitziber Atutxa</author>
<author>Cristina Bosco</author>
<author>Jinho Choi</author>
<author>Marie-Catherine de Marneffe</author>
<author>Timothy Dozat</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
<author>Filip Ginter</author>
</authors>
<title>Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Jan Hajiˇc, Anders Trærup Johannsen, Jenna Kanerva, Juha Kuokkala, Veronika Laippala, Alessandro Lenci, Krister Lind´en, Nikola Ljubeˇsi´c,</title>
<date>2015</date>
<institution>Missil¨a, Simonetta Montemagni, Joakim Nivre, Hanna Nurmi, Petya Osenova, Slav Petrov, Jussi Piitulainen, Barbara Plank, Prokopis</institution>
<location>Teresa Lynn, Christopher Manning, H´ector Alonso Martinez, Ryan McDonald, Anna</location>
<marker>Agi´c, Aranzabe, Atutxa, Bosco, Choi, de Marneffe, Dozat, Farkas, Foster, Ginter, 2015</marker>
<rawString>ˇZeljko Agi´c, Maria Jesus Aranzabe, Aitziber Atutxa, Cristina Bosco, Jinho Choi, Marie-Catherine de Marneffe, Timothy Dozat, Rich´ard Farkas, Jennifer Foster, Filip Ginter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Jan Hajiˇc, Anders Trærup Johannsen, Jenna Kanerva, Juha Kuokkala, Veronika Laippala, Alessandro Lenci, Krister Lind´en, Nikola Ljubeˇsi´c, Teresa Lynn, Christopher Manning, H´ector Alonso Martinez, Ryan McDonald, Anna Missil¨a, Simonetta Montemagni, Joakim Nivre, Hanna Nurmi, Petya Osenova, Slav Petrov, Jussi Piitulainen, Barbara Plank, Prokopis Prokopidis, Sampo Pyysalo, Wolfgang Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi, Kiril Simov, Aaron Smith, Reut Tsarfaty, Veronika Vincze, and Daniel Zeman. 2015. Universal dependencies 1.1. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>328</pages>
<contexts>
<context position="1555" citStr="Barzilay and McKeown, 2005" startWordPosition="224" endWordPosition="228">efit from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head. 1 Introduction We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set. The probabilistic linearization models we investigate are derived from generative models for dependency trees (Eisner, 1996), as most commonly used i</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297– 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Michael White</author>
<author>Dominic Espinosa</author>
<author>Eric Kow</author>
<author>Deirdre Hogan</author>
<author>Amanda Stent</author>
</authors>
<title>The first surface realisation shared task: Overview and evaluation results.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>217--226</pages>
<contexts>
<context position="1337" citStr="Belz et al., 2011" startWordPosition="192" endWordPosition="195">ord orders given tree structures under these models. The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for English. Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head. 1 Introduction We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and lin</context>
<context position="17888" citStr="Belz et al. (2011)" startWordPosition="3020" endWordPosition="3023">our models typically reorder the sentence “From the AP comes this story” (in Figure 1) as “This story comes from the AP”; the second order is arguably more natural, though the first is idiomatic for this particular phrase. So we do not believe that BLEU scores and other metrics of similarity to a “correct” ordering are particularly relevant for our task. Previous work uses BLEU scores (Papineni et al., 2002) and human ratings to evaluate generation of word orders. To provide some comparability with previous work, we report BLEU scores on the 2011 Shared Task data here. The systems reported in Belz et al. (2011) achieve BLEU scores ranging from 23 to 89 for English; subsequent work achieves BLEU scores of 91.6 on the same data (Bohnet et al., 2012). Drawing the highestprobability orderings from our models, we achieve a top BLEU score of 57.7 using the model configuration hdr/oo. Curiously, hdr/oo is typically the worst model configuration in the test set probability evaluation (Section 3.1). The BLEU performance is in the middle range of the Shared Task systems. The human evaluation of our models is more optimistic: the best score for Meaning Similarity in the Shared Task was 84/100 (Bohnet et al., 2</context>
</contexts>
<marker>Belz, White, Espinosa, Kow, Hogan, Stent, 2011</marker>
<rawString>Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first surface realisation shared task: Overview and evaluation results. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Simon Mille</author>
<author>Benoit Favre</author>
<author>Leo Wanner</author>
</authors>
<title>StuMaBa : from deep representation to surface.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European workshop on natural language generation,</booktitle>
<pages>232--235</pages>
<contexts>
<context position="18492" citStr="Bohnet et al., 2011" startWordPosition="3124" endWordPosition="3127">lz et al. (2011) achieve BLEU scores ranging from 23 to 89 for English; subsequent work achieves BLEU scores of 91.6 on the same data (Bohnet et al., 2012). Drawing the highestprobability orderings from our models, we achieve a top BLEU score of 57.7 using the model configuration hdr/oo. Curiously, hdr/oo is typically the worst model configuration in the test set probability evaluation (Section 3.1). The BLEU performance is in the middle range of the Shared Task systems. The human evaluation of our models is more optimistic: the best score for Meaning Similarity in the Shared Task was 84/100 (Bohnet et al., 2011), while sentences ordered according to our models were judged to have the same meaning as the original in 85% of cases (Table 2), though these figures are based on different data. These comparisons suggest that these generative models do not provide state-of-the-art performance, but do capture some of the same information as previous models. 3.4 Discussion Overall, the most effective models are the Dependent N-gram models. The naive approach to modeling order relations among sister dependents, as embodied in the Observed Orders model, does not generalize well. The result suggests that models l</context>
</contexts>
<marker>Bohnet, Mille, Favre, Wanner, 2011</marker>
<rawString>Bernd Bohnet, Simon Mille, Benoit Favre, and Leo Wanner. 2011. StuMaBa : from deep representation to surface. In Proceedings of the 13th European workshop on natural language generation, pages 232–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Anders Bj¨orkelund</author>
<author>Jonas Kuhn</author>
<author>Wolfgang Seeker</author>
<author>Sina Zarrieß</author>
</authors>
<title>Generating non-projective word order in statistical linearization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>928--939</pages>
<marker>Bohnet, Bj¨orkelund, Kuhn, Seeker, Zarrieß, 2012</marker>
<rawString>Bernd Bohnet, Anders Bj¨orkelund, Jonas Kuhn, Wolfgang Seeker, and Sina Zarrieß. 2012. Generating non-projective word order in statistical linearization. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 928–939.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Braune</author>
<author>Daniel Bauer</author>
<author>Kevin Knight</author>
</authors>
<title>Mapping between english strings and reentrant semantic graphs.</title>
<date>2014</date>
<booktitle>In Int. Conf. on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="1727" citStr="Braune et al., 2014" startWordPosition="253" endWordPosition="256"> We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set. The probabilistic linearization models we investigate are derived from generative models for dependency trees (Eisner, 1996), as most commonly used in unsupervised grammar induction (Klein and Manning, 2004; Gelling et al., 2012). Generative dependency models have typically been evaluated in a parsing task (Eisner, 1997</context>
</contexts>
<marker>Braune, Bauer, Knight, 2014</marker>
<rawString>Fabienne Braune, Daniel Bauer, and Kevin Knight. 2014. Mapping between english strings and reentrant semantic graphs. In Int. Conf. on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Kristina Toutanova</author>
</authors>
<title>A discriminative syntactic word order model for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9</pages>
<contexts>
<context position="1508" citStr="Chang and Toutanova, 2007" startWordPosition="218" endWordPosition="221">orms are collected for English. Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head. 1 Introduction We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set. The probabilistic linearization models we investigate are derived from generative models for dependenc</context>
</contexts>
<marker>Chang, Toutanova, 2007</marker>
<rawString>Pi-Chuan Chang and Kristina Toutanova. 2007. A discriminative syntactic word order model for machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, page 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Conference on Computational Linguistics,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="2130" citStr="Eisner, 1996" startWordPosition="317" endWordPosition="318">arization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set. The probabilistic linearization models we investigate are derived from generative models for dependency trees (Eisner, 1996), as most commonly used in unsupervised grammar induction (Klein and Manning, 2004; Gelling et al., 2012). Generative dependency models have typically been evaluated in a parsing task (Eisner, 1997). Here, we are interested in the inverse task: inferring a distribution over linear orders given unordered dependency trees. This is the first work to consider generative dependency models from the perspective of word ordering. The results can potentially shed light on how ordering constraints are best represented in such models. In addition, the use of probabilistic models means that we can easily </context>
<context position="4977" citStr="Eisner, 1996" startWordPosition="774" endWordPosition="775">ple, to generate the dependency tree in Figure 1 from the node comes down, we take the head nmod nsubj comes and generate the subtree story AP , story then we take the head story and generate and so on. In this work, we experiment with different specific generative processes G which generate a local subtree conditioned on a head. 2.1 Model Types Here we describe some possible generative processes G which generate subtrees conditioned on a head. These models contain progressively more information about ordering relations among sister dependents. A common starting point for G is Eisner Model C (Eisner, 1996). In this model, dependents on one side of the head are generated by repeatedly sampling from a categorical distribution until a special stop-symbol is generated. The model only captures the propensity of dependents to appear on the left or right of the head, and does not capture any order constraints between sister dependents on one side of the head. We consider a generalization of Eisner Model C which we call Dependent N-gram models. In a Dependent N-gram model, we generate dependents on each side the head by sampling a sequence of dependents from an N-gram model. Each dependent is generated</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th Conference on Computational Linguistics, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>An empirical comparison of probability models for dependency grammar.</title>
<date>1997</date>
<tech>Technical report, IRCS Report 96–11,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2328" citStr="Eisner, 1997" startWordPosition="348" endWordPosition="349">t al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set. The probabilistic linearization models we investigate are derived from generative models for dependency trees (Eisner, 1996), as most commonly used in unsupervised grammar induction (Klein and Manning, 2004; Gelling et al., 2012). Generative dependency models have typically been evaluated in a parsing task (Eisner, 1997). Here, we are interested in the inverse task: inferring a distribution over linear orders given unordered dependency trees. This is the first work to consider generative dependency models from the perspective of word ordering. The results can potentially shed light on how ordering constraints are best represented in such models. In addition, the use of probabilistic models means that we can easily define well-motivated normalized probability distributions over orders of dependency trees. These distributions are useful for answering scientific questions about crosslinguistic word order in quan</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Jason M Eisner. 1997. An empirical comparison of probability models for dependency grammar. Technical report, IRCS Report 96–11, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Tree linearization in English: Improving language model based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT (Short Papers).</booktitle>
<contexts>
<context position="1301" citStr="Filippova and Strube, 2009" startWordPosition="184" endWordPosition="187">or computing the conditional probability of word orders given tree structures under these models. The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for English. Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head. 1 Introduction We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety o</context>
</contexts>
<marker>Filippova, Strube, 2009</marker>
<rawString>Katja Filippova and Michael Strube. 2009. Tree linearization in English: Improving language model based approaches. In Proceedings of NAACL-HLT (Short Papers).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Futrell</author>
<author>Kyle Mahowald</author>
<author>Edward Gibson</author>
</authors>
<title>Quantifying word order freedom in dependency corpora.</title>
<date>2015</date>
<booktitle>In Proceedings of the Third International Conference on Dependency Linguistics (Depling</booktitle>
<pages>91--100</pages>
<location>Uppsala,</location>
<contexts>
<context position="3049" citStr="Futrell et al., 2015" startWordPosition="457" endWordPosition="460">rdered dependency trees. This is the first work to consider generative dependency models from the perspective of word ordering. The results can potentially shed light on how ordering constraints are best represented in such models. In addition, the use of probabilistic models means that we can easily define well-motivated normalized probability distributions over orders of dependency trees. These distributions are useful for answering scientific questions about crosslinguistic word order in quantitative linguistics, where obtaining robust estimates has proven challenging due to data sparsity (Futrell et al., 2015). The remainder of the work is organized as follows. In Section 2 we present a set of generative linearization models. In Section 3 we compare the performance of the different models as measured by test-set probability and human acceptability ratings. We also compare our performance with other systems from the literature. Section 4 concludes. 2 Generative Models for Projective Dependency Tree Linearization We investigate head-outward projective generative dependency models. In these models, an ordered dependency tree is generated by the following kind 1978 Proceedings of the 2015 Conference on</context>
</contexts>
<marker>Futrell, Mahowald, Gibson, 2015</marker>
<rawString>Richard Futrell, Kyle Mahowald, and Edward Gibson. 2015. Quantifying word order freedom in dependency corpora. In Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 91–100, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Gelling</author>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Joao Grac¸a</author>
</authors>
<title>The pascal challenge on grammar induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure,</booktitle>
<pages>64--80</pages>
<marker>Gelling, Cohn, Blunsom, Grac¸a, 2012</marker>
<rawString>Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao Grac¸a. 2012. The pascal challenge on grammar induction. In Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 64–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei He</author>
<author>Haifeng Wang</author>
<author>Yuqing Guo</author>
<author>Ting Liu</author>
</authors>
<title>Dependency based Chinese sentence realization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>809--816</pages>
<contexts>
<context position="1318" citStr="He et al., 2009" startWordPosition="188" endWordPosition="191"> probability of word orders given tree structures under these models. The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for English. Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head. 1 Introduction We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functio</context>
</contexts>
<marker>He, Wang, Guo, Liu, 2009</marker>
<rawString>Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu. 2009. Dependency based Chinese sentence realization. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 809–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing,</title>
<date>1987</date>
<journal>IEEE Transactions on,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="10760" citStr="Katz, 1987" startWordPosition="1811" endWordPosition="1812"> Estimation and Smoothing In order to alleviate data sparsity in fitting our models, we adopt two smoothing methods from the language modelling literature. All categorical distributions are estimated using add-k smoothing where k = 0.01. For the Dependent N-gram models, this means adding k pseudocounts for each possible dependent in each context. For the Observed Orders model, this means adding k pseudocounts for each possible permutation of the head and its dependents. We also experiment with combining our models into mixture distributions. This can be viewed as a kind of back-off smoothing (Katz, 1987), where the Observed Orders model is the model with the most context, and Dependent N-grams and Eisner Model C are backoff distributions with successively less context. Similarly, models with less information in the labelling can serve as backoff distributions for models with more information in the labelling. For example, a model which is conditioned on the POS of the head can be backed off to a model which does not condition on the head at all. We find optimal mixture weights using the Baum-Welch algorithm tuned on a held-out development set. 3 Evaluation Here we empirically evaluate some op</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing, IEEE Transactions on, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>478</pages>
<contexts>
<context position="2212" citStr="Klein and Manning, 2004" startWordPosition="327" endWordPosition="331">tion of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set. The probabilistic linearization models we investigate are derived from generative models for dependency trees (Eisner, 1996), as most commonly used in unsupervised grammar induction (Klein and Manning, 2004; Gelling et al., 2012). Generative dependency models have typically been evaluated in a parsing task (Eisner, 1997). Here, we are interested in the inverse task: inferring a distribution over linear orders given unordered dependency trees. This is the first work to consider generative dependency models from the perspective of word ordering. The results can potentially shed light on how ordering constraints are best represented in such models. In addition, the use of probabilistic models means that we can easily define well-motivated normalized probability distributions over orders of dependen</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, page 478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijia Liu</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Bing Qin</author>
</authors>
<title>Transition-based syntactic linearization.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1684" citStr="Liu et al., 2015" startWordPosition="246" endWordPosition="249"> conditioned on the head. 1 Introduction We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set. The probabilistic linearization models we investigate are derived from generative models for dependency trees (Eisner, 1996), as most commonly used in unsupervised grammar induction (Klein and Manning, 2004; Gelling et al., 2012). Generative dependency models have typically bee</context>
</contexts>
<marker>Liu, Zhang, Che, Qin, 2015</marker>
<rawString>Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin. 2015. Transition-based syntactic linearization. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="17681" citStr="Papineni et al., 2002" startWordPosition="2982" endWordPosition="2985">rders, rather than to recover a single correct order, because many orders are often possible, and the particulator order that a dependency tree originally appeared in might not be the most natural. For example, our models typically reorder the sentence “From the AP comes this story” (in Figure 1) as “This story comes from the AP”; the second order is arguably more natural, though the first is idiomatic for this particular phrase. So we do not believe that BLEU scores and other metrics of similarity to a “correct” ordering are particularly relevant for our task. Previous work uses BLEU scores (Papineni et al., 2002) and human ratings to evaluate generation of word orders. To provide some comparability with previous work, we report BLEU scores on the 2011 Shared Task data here. The systems reported in Belz et al. (2011) achieve BLEU scores ranging from 23 to 89 for English; subsequent work achieves BLEU scores of 91.6 on the same data (Bohnet et al., 2012). Drawing the highestprobability orderings from our models, we achieve a top BLEU score of 57.7 using the model configuration hdr/oo. Curiously, hdr/oo is typically the worst model configuration in the test set probability evaluation (Section 3.1). The B</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajakrishnan Rajkumar</author>
<author>Michael White</author>
</authors>
<title>Better surface realization through psycholinguistics.</title>
<date>2014</date>
<journal>Language and Linguistics Compass,</journal>
<volume>8</volume>
<issue>10</issue>
<marker>Rajkumar, White, 2014</marker>
<rawString>Rajakrishnan Rajkumar and Michael White. 2014. Better surface realization through psycholinguistics. Language and Linguistics Compass, 8(10):428–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Rajakrishnan Rajkumar</author>
</authors>
<title>Minimal dependency length in realization ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>244--255</pages>
<contexts>
<context position="1650" citStr="White and Rajkumar, 2012" startWordPosition="239" endWordPosition="242">distributions, including distributions not conditioned on the head. 1 Introduction We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optimally-scoring tree among some set. The probabilistic linearization models we investigate are derived from generative models for dependency trees (Eisner, 1996), as most commonly used in unsupervised grammar induction (Klein and Manning, 2004; Gelling et al., 2012). Generative de</context>
</contexts>
<marker>White, Rajkumar, 2012</marker>
<rawString>Michael White and Rajakrishnan Rajkumar. 2012. Minimal dependency length in realization ranking. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 244–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
</authors>
<title>Partial-tree linearization: generalized word ordering for text synthesis.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,</booktitle>
<pages>2232--2238</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1372" citStr="Zhang, 2013" startWordPosition="200" endWordPosition="201">hese models. The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for English. Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head. 1 Introduction We explore generative models for producing linearizations of unordered labeled syntactic dependency trees. This specific task has attracted attention in recent years (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2012; Zhang, 2013) because it forms a useful part of a natural language generation pipeline, especially in machine translation (Chang and Toutanova, 2007) and summarization (Barzilay and McKeown, 2005). Closely related tasks are generation of sentences given CCG parses (White and Rajkumar, 2012), bags of words (Liu et al., 2015), and semantic graphs (Braune et al., 2014). Here we focus narrowly on testing probabilistic generative models for dependency tree linearization. In contrast, the approach in most previous work is to apply a variety of scoring functions to trees and linearizations and search for an optim</context>
</contexts>
<marker>Zhang, 2013</marker>
<rawString>Yue Zhang. 2013. Partial-tree linearization: generalized word ordering for text synthesis. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 2232–2238. AAAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
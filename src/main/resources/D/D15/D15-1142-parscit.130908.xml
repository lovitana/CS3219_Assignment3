<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9933935">
Semi-supervised Chinese Word Segmentation based on Bilingual
Information
</title>
<author confidence="0.996789">
Wei Chen
</author>
<affiliation confidence="0.837181666666667">
Institute of Automation,
Chinese Academy of Sciences,
Beijing, 100190, China
</affiliation>
<email confidence="0.992716">
wei.chen.media@ia.ac.cn
</email>
<sectionHeader confidence="0.997336" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984774193548">
This paper presents a bilingual semi-
supervised Chinese word segmentation
(CWS) method that leverages the nat-
ural segmenting information of English
sentences. The proposed method in-
volves learning three levels of features,
namely, character-level, phrase-level and
sentence-level, provided by multiple sub-
models. We use a sub-model of condi-
tional random fields (CRF) to learn mono-
lingual grammars, a sub-model based on
character-based alignment to obtain ex-
plicit segmenting knowledge, and anoth-
er sub-model based on transliteration sim-
ilarity to detect out-of-vocabulary (OOV)
words. Moreover, we propose a sub-model
leveraging neural network to ensure the
proper treatment of the semantic gap and
a phrase-based translation sub-model to s-
core the translation probability of the Chi-
nese segmentation and its corresponding
English sentences. A cascaded log-linear
model is employed to combine these fea-
tures to segment bilingual unlabeled data,
the results of which are used to justify the
original supervised CWS model. The eval-
uation shows that our method results in su-
perior results compared with those of the
state-of-the-art monolingual and bilingual
semi-supervised models that have been re-
ported in the literature.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999543571428571">
Chinese word segmentation (CWS) is generally
accepted to be a necessary first step in most Chi-
nese NLP tasks because Chinese sentences are
written in continuous sequences of characters with
no explicit delimiters (e.g., the spaces in English).
Many studies have been conducted in this area, re-
sulting in extensive investigation of the problem of
</bodyText>
<note confidence="0.656634">
Bo Xu
Institute of Automation,
Chinese Academy of Sciences,
Beijing, 100190, China
</note>
<email confidence="0.974326">
xubo@ia.ac.cn
</email>
<bodyText confidence="0.999869024390244">
CWS using machine learning techniques in recent
years. However, the reliability of CWS that can
be achieved using machine learning techniques re-
lies heavily on the availability of a large amount of
high-quality, manually segmented data. Because
hand-labeling individual words and word bound-
aries is very difficult (Jiao et al., 2006), producing
segmented Chinese texts is very time-consuming
and expensive. Although a number of manual-
ly segmented datasets have been constructed by
various organizations, it is not feasible to com-
bine them into a single complete dataset because
of their incompatibility due to the use of various
segmenting standards. Thus, it is difficult to build
a large-scale manually segmented corpus, and the
resulting lack of such a corpus is detrimental to
further enhancement of the accuracy of CWS.
To address the scarcity of manually segment-
ed corpora, a number of semi-supervised CWS
approaches have been intensively investigated in
recent years. These approaches attempt to ei-
ther learn the predicted label distribution (Jiao et
al., 2006) or extract mutual information ((Liang
et al., 2005); (Sun and Xu, 2011); (Zeng et al.,
2013a)) from large-scale monolingual unlabeled
data to update the baseline model (from manual-
ly segmented corpora). In addition to these tech-
niques, several co-training approaches (Zeng et
al., 2013b) using character-based and word-based
models have also been employed. However, be-
cause monolingual unlabeled data contain limit-
ed natural segmenting information, in most semi-
supervised methods, the objective function tend-
s to be optimized based on the personal experi-
ence and knowledge of the researchers. This prac-
tice means that these methods can typically yield
high performance in certain specialized domain-
s, but they lack generalizability. In contrast with
these methods, we propose to leverage bilingual
unlabeled data, i.e., a Chinese-English corpus with
sentence alignment. Because English sentences
</bodyText>
<page confidence="0.935951">
1207
</page>
<note confidence="0.9958685">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1207–1216,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.694266666666667">
Figure 1: The examples of different segmentation
on the same Chinese sentences guided by the En-
glish sentences
</figureCaption>
<bodyText confidence="0.999229794871795">
are naturally segmented, extracting information
from a bilingual corpus is a much more objective
task. As the example presented in Fig 1 shows,
the English sentences that correspond to Chinese
text can easily help guide better segmentation, and
thus, the learning of segmenting information from
bilingual data is a very promising approach.
In this paper, to obtain high-quality segment-
ing information from bilingual unlabeled data, we
leverage multilevel features using the following
steps: first, we integrate character-level features
calculated using a conditional random field (CRF)
model, which is used to capture the monolingual
grammars. Then, we employ a statistical align-
er to perform character-based alignment. Given
the results of this character-based alignment, we
apply several phrase-level features to extract ex-
plicit and implicit segmenting information: (1) we
use two types of English-Chinese co-occurrence
features (one-to-many and many-to-many) to learn
the explicit segmenting information of the English
sentences, (2) we use the transliteration similarity
feature to detect out-of-vocabulary (OOV) words
using a phrase-based translation model, and (3) we
employ a neural network to calculate the seman-
tic gap between the Chinese and English words
to ensure that the Chinese segmentation follows
the semantic meanings of the corresponding En-
glish sentences as closely as possible. Finally, we
employ another phrase-based translation model to
perform a sentence-level calculation of the trans-
lation probability of the Chinese segmentation and
its corresponding English sentences. After obtain-
ing these multilevel features, we normalize them
and combine them into two log-linear models in
a cascaded structure, which is illustrated in Fig
2. Finally, we segment the bilingual unlabeled da-
ta using the proposed model and use the segmen-
tation of those data to justify the original super-
</bodyText>
<figureCaption confidence="0.6710045">
Figure 2: The structure of cascaded log-linear
model with multilevel features
vised CWS model, which was trained on a stan-
dard manually segmented corpus.
</figureCaption>
<bodyText confidence="0.999689304347826">
In fact, several semi-supervised CWS methods
have previously been proposed that leverage bilin-
gual unlabeled data ((Xu et al., 2008); (Chang
et al., 2008); (Ma and Way, 2009);(Chung et al.,
2009); (Xi et al., 2012)). However, most were de-
veloped for statistical machine translation (SMT),
causing them to focus on decreasing the perplex-
ity of the bilingual data and the word alignmen-
t process rather than on achieving more accurate
segmentation. These methods achieve significan-
t improvement in SMT performance but are not
very suitable for common NLP tasks because in
many situations, they ignore the standard gram-
mars to satisfy the needs of SMT. By contrast,
we employ various types of features to capture
both monolingual standard grammars and bilin-
gual segmenting information, which allows our
semi-supervised CWS model to be very efficient
at other NLP tasks and endows it with higher gen-
eralizability.
Our evaluation also shows that our method sig-
nificantly outperforms the state-of-the-art mono-
lingual and bilingual semi-supervised approaches.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999990666666667">
First, we review related work on monolingual
supervised and semi-supervised CWS methods.
Then, we review bilingual semi-supervised CWS.
</bodyText>
<subsectionHeader confidence="0.9598895">
2.1 Monolingual Supervised and
Semi-supervised CWS Methods
</subsectionHeader>
<bodyText confidence="0.99991625">
Considerable efforts have been made in the NLP
community in the study of Chinese word segmen-
tation. The most popular supervised approach
treats word segmentation as a sequence labeling
problem, as first proposed by (Xue et al., 2003).
Most previous systems have addressed this task
using linear statistical models with carefully de-
signed features ((Peng et al., 2004); (Asahara et
</bodyText>
<page confidence="0.985428">
1208
</page>
<bodyText confidence="0.999278413793103">
al., 2005); (Zhang and Clark, 2007); (Zhao et
al., 2010)). However, the primary shortcoming
of these approaches is that they rely heavily on a
large amount of labeled data, which is very time-
consuming and expensive to produce. Thus, the s-
cale of available manually labeled data has placed
considerable limitations on the further enhance-
ment of supervised CWS methods.
To address this problem, a number of semi-
supervised CWS approaches have been intensive-
ly investigated in recent years. For example,
(Sun and Xu, 2011) enhanced their segmenta-
tion results by interpolating statistics-based fea-
tures derived from unlabeled data into a CRF mod-
el. (Zeng et al., 2013a) introduced a graph-based
semi-supervised joint model of Chinese word seg-
mentation and part-of-speech tagging and regular-
ized the learning of a linear CRF model based on
the label distributions derived from unlabeled da-
ta. However, because monolingual unlabeled data
lack natural segmenting information, most previ-
ous semi-supervised CWS methods have required
certain assumptions to be made regarding their ob-
jective functions based on the researchers’ person-
al experiences. By contrast, we leverage bilingual
unlabeled data that contain the natural segmenta-
tion that is present in English sentences and can
therefore extract linguistic knowledge without any
manual assumptions or bias.
</bodyText>
<subsectionHeader confidence="0.999749">
2.2 Bilingual Semi-supervised CWS Methods
</subsectionHeader>
<bodyText confidence="0.999963678571428">
Some previous work ((Xu et al., 2008); (Chang
et al., 2008); (Ma and Way, 2009);(Chung et al.,
2009); (Xi et al., 2012)) has been performed on
leveraging bilingual unlabeled data to achieve bet-
ter segmentation, although most such studies have
focused on statistical machine translation (SMT).
These approaches leverage the mappings of indi-
vidual English words to one or more consecutive
Chinese characters either to construct a Chinese
word dictionary for maximum-matching segmen-
tation (Xu et al., 2004) or to form a labeled dataset
for training a sequence labeling model (Peng et al.,
2004). (Zeng et al., 2014) also used such map-
pings to bias a supervised segmentation model to-
ward a better solution for SMT. However, because
most of these approaches focus on SMT perfor-
mance, they emphasize decreasing the perplexity
of the bilingual data and word alignment rather
than improving the CWS accuracy. Thus, they
sometimes ignore the standard grammars during
segmentation in favor of satisfying the needs of
SMT, thereby causing these methods to be rather
unsuitable for other NLP tasks. By contrast, we
propose to use various types of features to capture
syntactic and semantic information and a cascaded
log-linear model to maintain balance between the
monolingual grammars and the bilingual knowl-
edge.
</bodyText>
<sectionHeader confidence="0.984462" genericHeader="method">
3 Multilevel Features
</sectionHeader>
<bodyText confidence="0.99987625">
In this section, we describe the three levels of fea-
tures used in our approach. We propose to use
character-level features to capture monolingual
grammars and phrase-level and sentence-level fea-
tures to obtain bilingual segmenting information.
Moreover, we describe a cascaded log-linear mod-
el by proposing both inner and outer log-linear
models.
</bodyText>
<subsectionHeader confidence="0.998354">
3.1 Character-level Feature
</subsectionHeader>
<bodyText confidence="0.9987616">
The conditional random field (CRF) (Lafferty et
al., 2001) model was first used for CWS tasks by
(Xue et al., 2003) who treated the CWS task as a
sequence tagging problem and demonstrated this
model’s effectiveness in detecting OOV words.
In this paper, we score the character-level fea-
ture in the same manner defined by (Xue et al.,
2003). For the jth character cj in the sentence
cJ1 = c1...cJ, the score can be calculated as fol-
lows:
</bodyText>
<equation confidence="0.530505666666667">
� λkfk(yj−1, yj, cJ1 , j) (1)
fCRF (j) =
k
</equation>
<bodyText confidence="0.999966777777778">
where fk(yj−1, yj, cJ1 , j) is a feature function
and Ak is a learned weight that corresponds to the
feature fk. j represents the index of the character
in the sentence. yj−1 and yj represent the tags of
the previous and current characters, respectively.
We do not introduce the CRF-based CWS mod-
el in detail here, but more information can be ob-
tained from (Lafferty et al., 2001) and (Xue et al.,
2003).
</bodyText>
<subsectionHeader confidence="0.998497">
3.2 Phrase-level Features
</subsectionHeader>
<bodyText confidence="0.999995571428571">
In this section, we first describe English-Chinese
character-based alignment. Then, we propose sev-
eral phrase-level features to obtain explicit and im-
plicit segmenting information from the character-
based alignment. Finally, we describe the in-
ner log-linear model that is used to combine the
character-level and phrase-level features.
</bodyText>
<page confidence="0.975365">
1209
</page>
<subsectionHeader confidence="0.707706">
3.2.1 English-Chinese Character-based
Alignment
</subsectionHeader>
<bodyText confidence="0.999952304347826">
To avoid introducing omissions and mistakes in-
to the linguistic information in the initial segmen-
tations of the bilingual data, we perform a statis-
tical character-based alignment: First, every Chi-
nese character in the bitexts is separated by white
spaces so that individual characters are recognized
as unique &amp;quot;words&amp;quot; or alignment targets. Then,
they are associated with English words using a s-
tatistical word aligner.
By representing the English and Chinese sen-
tences as eI1 = e1e2...eI and cJ1 = c1c2...cJ, re-
spectively, where ei and cj represent single ele-
ments of the sentences, we define their alignment
as aK1 , of which each element is a span ak =&lt;
s, t &gt; and represents the alignment of the English
word es with the Chinese character ct. Then, the
corpus of unlabeled bilingual data can be repre-
sented as the set of sentence tuples &lt;eI1, cJ1 , aK1 &gt;
To obtain the character-based alignment, we
employ an open-source toolkit Pialign1 ((Neubig
et al., 2011); (Neubig et al., 2012)) which us-
es Bayesian learning and inversion transduction
grammars.
</bodyText>
<subsubsectionHeader confidence="0.531518">
3.2.2 Features Obtained from the
</subsubsectionHeader>
<subsectionHeader confidence="0.959021">
Character-based Alignment
</subsectionHeader>
<bodyText confidence="0.9999898">
Given the English-Chinese character-based align-
ment aK1 , we extract several phrase-level features
to optimize the segmentation. For the jth char-
acter in cJ1 , we assume that one of the segmen-
tations of the substring cj1 can be represented as
</bodyText>
<equation confidence="0.9860086">
wN+1 j1+1...cj
= w1w2w3...wN+1 = cj1
1 cjN +1.
j2
1
</equation>
<bodyText confidence="0.99517725">
Then, we calculate the scores of each Chinese
word wn = cjn
jm (jm = jn−1 + 1) in wN+1 1us-
ing the following features.
</bodyText>
<subsectionHeader confidence="0.391841">
English-Chinese One-to-Many Alignment
</subsectionHeader>
<bodyText confidence="0.999112454545455">
To evaluate the probability that a sequence of
Chinese characters cjn
jm = cjmcjm+1...cjn should
be combined into a word wn based on the corre-
sponding English sentence, we integrate the fea-
ture of English-Chinese one-to-many alignmen-
t (one English word is aligned with multiple Chi-
nese characters). First, for any English word ei in
eI1, the phrase tuple &lt; ei, cjn jm&gt; can be defined as
an aligned One-to-Many phrase tuple if it satisfies
the following conditions:
</bodyText>
<listItem confidence="0.9552705">
(1) &lt; i,jm &gt; ∈ aK1 , &lt; i,jn &gt; ∈ aK1
(2) ∀j0 ∈/ [jm, jn], &lt; i, j0 &gt;/∈ aK1
</listItem>
<footnote confidence="0.86856">
1http://www.phontron.com/pialign/
</footnote>
<bodyText confidence="0.8924216">
(3) ∀i0 =6 i,∀j0 ∈ [jm, jn], &lt; i0, j0 &gt;/∈ aK1
Then, for any phrase tuple &lt; ei, cjn
jm &gt; that sat-
isfies these conditions, the span &lt; i, jm, jn &gt; is
defined as a One-to-Many span and as a member
of the set AOne.
Thus, for each span &lt; i, jm, jn &gt;, the One-to-
Many score can be calculated as follows:
�
t(cjn
</bodyText>
<equation confidence="0.973968666666667">
jm|ei) if &lt; i, jm, jn &gt;E AOne
s(&lt; i, jm, jn &gt;)
(2)
t(cjn
jm|ei)
cjn
jm|ei.
cjn
jm
</equation>
<bodyText confidence="0.919798833333333">
ved as follows:
where
represents the translation proba-
bility of the phrase tuple
Finally, the score for the feature of English-
Chinese One-to-Many alignment for wn =
</bodyText>
<equation confidence="0.868778444444444">
is
deri
fOne−to−Many(n) = argmax s(&lt; i, jm, jn &gt;) (3)
i∈[1,I]
Then, for any phrase tuple &lt;
&gt; that sat-
isfies these conditions, the span &lt;
i2,
&gt;
</equation>
<bodyText confidence="0.8397185">
is defined as a Many-to-Many span and as a mem-
Thus, for each span &lt;
</bodyText>
<figure confidence="0.445769909090909">
&gt;, the
Many-to-Many score can be calculated as foll
ei2
i1,cjn
jm
i1,
j1,jn
AMany.
i1,i2,j1,jn
ows:
English-Chinese Many-to-Many Alignment
</figure>
<bodyText confidence="0.958657857142857">
The second phrase-level feature, called English-
Chinese Many-to-Many Alignment (multiple En-
glish words are aligned with multiple Chinese
characters), is used to evaluate the probability that
a space should be inserted between cn and cn+1.
Similar to One-to-Many alignment, for any se-
quence of English words ei2
i1 and the Chinese word
wn = cjn
jm, the phrase tuple &lt; ei2
i1, cjn
j1 &gt; is de-
fined as an aligned Many-to-Many phrase tuple if
it satisfies the following conditions:
</bodyText>
<figure confidence="0.518497333333333">
(1) j1 ≤ jm, and j1 is the beginning character
of a word in wn1
(1) &lt; i1, j1 &gt;∈ aK1 , &lt; i2, jm &gt;∈ aK1
(2) ∀j0 ∈/ [j1, jm], ∀i0 ∈ [i1, i2], &lt; i0, j0 &gt;/∈ aK1
(3) ∀j0 ∈ [j1, jm], ∀i0 ∈/ [i1, i2], &lt; i0, j0 &gt;/∈ aK1
ber of the set
I
if &lt;
i2,
t(cjn
j1|ei2
i1)
i1,
j1,jn &gt;E AMany
0 else
</figure>
<bodyText confidence="0.9190735">
where
represents the translation proba-
</bodyText>
<equation confidence="0.9373305">
bili
t(cjn
j1|ei2
i1)
</equation>
<bodyText confidence="0.930897">
ty of the phrase tuple &lt; ei2
</bodyText>
<equation confidence="0.9404915">
i1, cjn
j1 &gt;.
0 else
s(&lt; i1, i2, j1, jn &gt;) =
</equation>
<page confidence="0.686766">
1210
</page>
<bodyText confidence="0.8705235">
Finally, the score for the feature of English-
Chinese Many-to-Many alignment for wn =cjn
</bodyText>
<equation confidence="0.8323646">
jm
is derived as follows:
fMany−to−Many(n) = argmax s(i1, i2, j1, jn)
i1E[1,I]i2E[i1,I]j1&lt;_jm
(5)
</equation>
<subsectionHeader confidence="0.480652">
Transliteration Feature
</subsectionHeader>
<bodyText confidence="0.984651428571429">
To account for named entities (NEs), which suf-
fer from sparsity and thus make it difficult to cal-
culate the probabilities discussed above, we intro-
duce a transliteration feature to evaluate the simi-
larities between the pronunciations of Chinese and
English words because many NEs are translated
via transliteration. To perform this task, we first
introduce an initial NE dictionary and convert each
dictionary item—for example, we convert ” UR
AT/Alice” into ”ai l i s i/a l i c e” —by transform-
ing the Chinese word into its pronunciation (rep-
resented by the function Fpy(·)) and splitting the
English word into its constituent letters (represent-
ed by the function Flet(·)). Then, we train two
phrase-based translation models (Chinese-English
and English-Chinese) on the data obtained from
the converted NE dictionary.
Specifically, we apply two standard log-linear
phrase-based SMT models. The GIZA++ align-
er is adopted to obtain word alignments (Och
and Ney, 2000) from the converted NE dictio-
nary. The heuristic strategy of grow-diag-final-and
(Koehn et al., 2003) is used to combine the bidi-
rectional alignments to extract phrase translations
and to reorder tables. A 5-gram language mod-
el with Kneser-Ney smoothing is trained using S-
RILM (Stolcke et al., 2002) on the target language.
Moses (Koehn et al., 2007) is used as a decoder.
Minimum error rate training (MERT) (Och et al.,
2003) is applied to tune the feature parameters on
the development dataset.
Given these two phrase-based translation mod-
els, we calculate each span &lt; i, jm, jn &gt; in AOne
for the Chinese word wn using the following for-
mula:
Str(&lt; i, jm, jn &gt;) = Sch−en(&lt; i, jm, jn &gt;) +Sen−ch(&lt; i, jm, jn &gt;) (6)
where Sch−en(&lt;i,jm,jn&gt;) = DLev(Fletei,
PTch−en(Fpy(cjn
jm))) means that the pronuncia-
tion conversion in the Chinese-English direction
is performed as follows: First, the English word
ei is split into its constituent letters; Second, the
sequence of Chinese characters cjn
jm is converted
into its pronunciation; Third, this pronunciation is
input into the Chinese-English phrase-based trans-
lation model, and the corresponding translation re-
sult is obtained; And finally, the Levenshtein dis-
tance between the English letters and the transla-
tion result is returned.
Sen−ch(&lt;i, jm, jn&gt;) can be calculated in ex-
actly the same way.
We set any span that does not belong to AOne
to zero, and the transliteration feature score of a
word wn = cjn
jm is derived as follows:
</bodyText>
<equation confidence="0.9987255">
ftransliteration(n) = argmax Str(&lt; i, jm, jn &gt;) (7)
iE[1,I]
</equation>
<bodyText confidence="0.997572842105263">
English-Chinese semantic gap feature
To guarantee that the semantic meanings of the
Chinese segmentation match those of the corre-
sponding English sentences as closely as possible,
we propose to use a feature based on the English-
Chinese semantic gap to ensure the retention of se-
mantic meaning during the segmentation process.
First, we pre-train word embeddings using the
open-source toolkit Word2Vec (Mikolov et al.,
2013) on the Chinese (segmented using character-
level features only) and English sentences sepa-
rately, thereby obtaining the vocabularies Vch and
Ven and their corresponding embedding matrixes
Lch E Rn×|Vch |and Len E Rn×|Ven|. Given a Chi-
nese word wn with an index i in the vocabulary, it
is then straightforward to retrieve the word’s vec-
tor representation via simple multiplication with a
binary vector d that is equal to zero at all positions
except that with index i:
</bodyText>
<equation confidence="0.869239">
Xi = Lchdi ∈ Rn (8)
</equation>
<bodyText confidence="0.997635">
Because the word embeddings for the two lan-
guages (Lch and Len) are learned separately and
located in different vector spaces, we suppose that
a transformation exists between these two seman-
tic embedding spaces. Thus, we collect all the
One-to-Many phrase tuples &lt; e1, cj2
</bodyText>
<equation confidence="0.7532675">
j1 &gt; that sat-
isfy el E Ven and cj2
</equation>
<bodyText confidence="0.990419571428571">
j1 E Vch from the entire corpus
of bilingual data. Then, we insert the word embed-
ding tuple of each One-to-Many phrase tuple into
the set Aembed. Let us consider a word embedding
tuple &lt; ps, pt &gt; in Aembed as an example. We
define a bidirectional semantic distance using the
parameter θ as follows:
</bodyText>
<page confidence="0.701336">
1211
</page>
<equation confidence="0.9648775">
Esem(ps, pt; O) = Esem(ps|pt, O) + Esem(pt|ps, O) (9)
Here, Esem(ps|pt, 0) = Esem(pt, f(W ch
en ps +
bch
</equation>
<bodyText confidence="0.875879933333333">
en)) represents the transformation of ps and is
performed as follows: We first multiply a parame-
ter matrix W ch
en by ps, and after adding a bias term
bch
en, we apply an element-wise activation function
f = tanh(·). Finally, we calculate their Euclidean
distance:
the output scores of each sub-model. After nor-
malization, the scores have means and standard
deviations of zero. We represent the normalization
function by Norm(·).
Thus, for the substring cj1 (j E [1, J)) in cJ1 of
the sentence tuple &lt;eI1, cJ1 , aK1 &gt;, assuming that
one of its candidate segmentations is wN+1 =
</bodyText>
<equation confidence="0.902346">
1
w1w2w3...wN+1 = cj1
1 cj1+1...cj
j2 jN+1, the feature
</equation>
<bodyText confidence="0.9989765">
score of the inner log-linear model is derived as
follows:
</bodyText>
<equation confidence="0.997711">
Efinner =
j&apos;∈[1,j]
(14)
(E
k
Eλ1
n∈[1,N+1]
Norm(fCRF(j&apos;))+
Norm(fk(n)))
Esem(ps|pt, O) = 1hch
2||pt − f(Wenps + ben)
||2 (10)
</equation>
<bodyText confidence="0.9976932">
Esem(pt|ps, 0) can be calculated in exactly the
same way.
Given the definition of the semantic distance of
each word-embedding tuple in Aembed, we wish to
minimize the following objective function:
</bodyText>
<equation confidence="0.965659">
J = � Esem(ps, pt; 0) (11)
&lt;ps,pt&gt;EAembed
</equation>
<bodyText confidence="0.999193666666667">
We apply the Stochastic Gradient Descent (S-
GD) algorithm to optimize each parameter and ul-
timately obtain the optimized parameters 0*.
Using 0*, we can calculate the semantic gap for
any possible span for wn, such as &lt; i, jm, jn &gt;,
as follows:
</bodyText>
<equation confidence="0.997462">
1
Esem(p�s|p&apos;t,θ∗)
if ei E Ven cjn
jm E Vch
&lt; i, jm, jn &gt;E AOne
0 else
(12)
</equation>
<bodyText confidence="0.937975666666667">
where p&apos;s and pt&apos; are the word vector representation
of ei and cjn
jm, respectively. Thus, the semantic gap
feature score of the word wn = cjn
jm is derived as
follows:
</bodyText>
<equation confidence="0.990596">
fsem(wn) = argmax Sgap(&lt; i, jm, jn &gt;) (13)
iE[1,I]
</equation>
<subsubsectionHeader confidence="0.498236">
3.2.3 Normalization and the Inner
</subsubsectionHeader>
<subsectionHeader confidence="0.894093">
Log-Linear Model
</subsectionHeader>
<bodyText confidence="0.999892333333333">
Because the output scores of each sub-model de-
scribed above are not probabilistic and they vary
by orders of magnitude, we must first normalize
where fk(n) represents the phrase-level features.
Then, we tune the weight λ1 from 0 to 1 in equal
increments of 0.1 to optimize its value.
</bodyText>
<subsectionHeader confidence="0.995204">
3.3 Sentence-level Features
</subsectionHeader>
<bodyText confidence="0.999991">
In this section, we describe the sentence-level fea-
tures calculated using the phrase-based translation
model and the outer log-linear model that is used
to combine the sentence-level features with the
features in the inner log-linear model.
</bodyText>
<subsectionHeader confidence="0.450995">
3.3.1 Features Obtained from the
Phrase-based Translation Model
</subsectionHeader>
<bodyText confidence="0.917337217391304">
Let us consider the last character cJ in cJ1 and as-
sume that its candidate segmentation (according
to the inner log-linear model only) is wN+1 =
1
w1w2w3...wN+1. We now add a sentence-level
feature to incorporate into the inner log-linear
model. This sentence-level feature is obtained us-
ing a phrase-based translation model. We segmen-
t the Chinese sentences from the bilingual unla-
beled data using character-level features only and
train a phrase-based translation model on the bilin-
gual data that is similar to the phrase-based trans-
lation model used for the transliteration features.
Unlike the usage of the phrase-based translation
model in the case of the transliteration features,
here, we input both the source and target sentences
and achieve the output of translation probability.
Thus, we perform a force decoding for the sen-
tence tuple &lt;wN+1
1 , eI1&gt; and obtain the set of de-
coding paths P(wN+1
1 ), where each element acts
as a decoding path that can translate wN+1
</bodyText>
<footnote confidence="0.6318085">
1 into
eI1. Finally, we define the sentence-level feature
</footnote>
<table confidence="0.671433714285714">
score of &lt;wN+1
1 , eI1&gt; as follows:
Sgap(&lt; i, jm, jn &gt;) = I
1212
fsent(wN+1 Ftrans(p(wN+1 =1, 2) represents the learned weights of the fea-
1 ) = argmax 1 )) tures of the CRF models.
p(wN+11)∈P(wN+1) (15) 5 The Datasets
</table>
<bodyText confidence="0.999715333333333">
where Ftrans(·) returns the translation score of
the given decoding path based on the phrase-based
translation model.
</bodyText>
<subsectionHeader confidence="0.994002">
3.3.2 The Outer Log-Linear Model
</subsectionHeader>
<bodyText confidence="0.999998">
Finally, we normalize the sentence-level features
in a manner similar to that described previously
and construct the outer log-linear model by com-
bining the inner log-linear model and the sentence-
level features as follows:
</bodyText>
<equation confidence="0.999463">
fouter = finner + A2Norm(fsent(wN+1
1 )) (16)
</equation>
<bodyText confidence="0.999665">
Then, we also tune the weight A2 from 0 to 1 in
equal increments of 0.1 to optimize its value.
</bodyText>
<subsectionHeader confidence="0.724677">
3.3.3 Decoder
</subsectionHeader>
<bodyText confidence="0.999951166666667">
A traditional viterbi beam search procedure is ap-
plied in the decoder to seek the segmented se-
quence with the highest score. Given a sentence
tuple &lt; eI1, cJ1, aK1 &gt;, the decoding procedure will
proceed in a left-right fashion using a dynamic
programming approach. At each position j in the
sequence cJ1 , we maintain a vector of size N to s-
tore the top N candidate segmentations of subse-
quence cj1 which are scored using the inner log-
linear model (j ∈ [1, J)) or the outer log-linear
model (j = J). Finally, we return the best seg-
mentation.
</bodyText>
<sectionHeader confidence="0.960406" genericHeader="method">
4 Justifying the Original CWS Model
</sectionHeader>
<bodyText confidence="0.99998025">
We justify the original CWS model (the CRF-
based model trained on manually segmented data)
using the new CRF model trained on the segmen-
tation of unlabeled bilingual data. To avoid over-
weakening the influence of the small-scale manu-
ally segmented data, we again utilized a log-linear
model to balance their weights. The formula can
be described as follows:
</bodyText>
<equation confidence="0.948093">
�fnew mono =
k1
J
Ak2fk2(yj−11yj1e1, 9)
</equation>
<bodyText confidence="0.9988678125">
where θ3 represents the weights of the second CR-
F model, which are set via minimum error rate
training using the developing dataset, and Aki (i
In this paper, we conduct our experiments on the
corpus of People’s daily of 1998 (from January to
June) as the standard (manually segmented) train-
ing corpus, the corpus of Bakeoff-2 CWS evalua-
tion as the developing and testing dataset. As the
corpus of Bakeoff-2 is made up of several sets pro-
vided by different organizations, we only select t-
wo sets whose segmenting standards are similar to
the training corpus. For each set, we take 3000
sentences as the developing dataset and the others
as the testing dataset. The statistics of every set
and the standard training corpus are shown in Ta-
ble 1.
</bodyText>
<table confidence="0.994179">
Data Set of sent. of words
Training 120K 7.28M
AS 708K 5.45M
PKU 19K 1.1M
</table>
<tableCaption confidence="0.999863">
Table 1: Statistics of training and testing datasets
</tableCaption>
<bodyText confidence="0.999844142857143">
Moreover, the bilingual unlabeled data is
formed by a large in-house Chinese-English par-
allel corpus (Tian et al., 2014). There are in total
2,215,000 Chinese-English sentence pairs crawled
from online resources, concentrated in 5 different
domains including laws, novels, spoken, news and
miscellaneous.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.997253647058823">
In our evaluation, the F-score was used as the ac-
curacy measure. The precision p is defined as the
percentage of words in the decoder output that are
segmented correctly, and the recall r is the percent-
age of gold-standard output words that are correct-
ly segmented by the decoder. The balanced F- s-
core is calculated as 2pr/(p + r). We also report
the recall of OOV words in our experiments. In
the following, we refer to our methods as ”SLBD”
(segmenter leveraging bilingual data).
Initially, we evaluated state-of-the-art super-
vised CWS methods, i.e., those of (Peng et al.,
2004) (Peng); (Asahara et al., 2005) (Asahara);
(Zhang and Clark, 2007) (Z&amp;C); (Zhao et al.,
2010) (Zhao), whose models are trained only on
manually segmented data. Moreover, we also e-
valuated the performance of our sub-models by
</bodyText>
<figure confidence="0.863426166666667">
J
Ak1fk1 (yj−1� yj � e1 9)
(17)
�
+B3
k2
</figure>
<page confidence="0.865749">
1213
</page>
<table confidence="0.997830333333333">
methods AS PKU
F OOV F OOV
Peng 91.6 52.5 91.1 59
Asahara 92.2 63.1 91.4 61.6
Z&amp;C 92.9 69.9 91.6 67.9
Zhao 93.1 72 92.3 60.6
character-level 92.3 58.6 92.9 60.8
Inner log-linear 95.9 78.8 96.1 81
Outer log-linear 96.7 80.8 97.1 85
</table>
<tableCaption confidence="0.995112">
Table 2: Word segmentation performance of SLB-
D and supervised CWS methods[%]
</tableCaption>
<bodyText confidence="0.999140315789474">
segmenting the bilingual unlabeled dataset using
character-level features only, the inner log-linear
model (which includes character-level and phrase-
level features) and the outer log-linear model (the
full SLBD approach). After applying these three
segmentations using the different sub-models, we
trained the new CRF models on the results of the
three segmentations to justify the original CWS
model. The evaluation results for the supervised
CWS methods and the sub-models are presented
in Table 2.
It can be seen that we achieved significant im-
provement in performance when we combined the
character-level and phrase-level features in the in-
ner log-linear model, demonstrating that the pro-
posed phrase-level features can be used to effi-
ciently obtain bilingual segmenting information.
Moreover, the outer log-linear model achieves a
further enhancement, thereby demonstrating that
the sentence-level features can be used to effec-
tively re-rank the candidate segmentations pro-
duced by the inner log-linear model.
Next, we compared the SLBD method with sev-
eral state-of-the-art monolingual semi-supervised
methods, including those of (Sun et al., 2012)
(Sun); (Sun and Xu, 2011) (5&amp;X); (Zeng et al.,
2013b) (Zeng). To ensure a fair comparison, we
performed the evaluation in two steps. First, we
input the entire bilingual unlabeled dataset into
the SLBD method and input only the Chinese sen-
tences from the bilingual unlabeled dataset into the
other semi-supervised methods. Then, because the
available monolingual unlabeled dataset was much
larger than the bilingual unlabeled dataset in natu-
ral, we used the XIN CMN portion of Chinese Gi-
gaword 2.0 as an additional unlabeled dataset for
the monolingual semi-supervised methods. which
contains 204 million words, more than ten times
</bodyText>
<table confidence="0.998733833333333">
methods Bilingual data Monolingual data
F OOV F OOV
Sun 93.9 63.1 94.6 67.9
5&amp;X 94.1 66 94.4 71
Zeng 94.0 64.5 94.8 63.2
SLBD 96.7 80.8 - -
</table>
<tableCaption confidence="0.972342333333333">
Table 3: Word segmentation performance of SLB-
D and other monolingual semi-supervised CWS
methods[%]
</tableCaption>
<table confidence="0.998859285714286">
methods AS PKU
F OOV F OOV
Xu 92.8 70.5 92.1 66
Ma 93.1 73 92.6 71.1
Xi 90.2 63 90.9 67.2
Zeng2014 93.5 76 93.2 73.3
SLBD 96.7 80.8 97.1 85
</table>
<tableCaption confidence="0.818481">
Table 4: Word segmentation performance of SLB-
D and other bilingual semi-supervised CWS meth-
ods[%]
</tableCaption>
<bodyText confidence="0.999570916666667">
the number of words in the bilingual unlabeled
dataset. The testing data was the set of AS only.
The evaluation is summarized in Table 3.
The results demonstrate that either leveraging
the same unlabeled data or providing a much larg-
er unlabeled dataset for the monolingual semi-
supervised methods, the SLBD method can sig-
nificantly outperform the evaluated monolingual
semi-supervised methods, which indicates that the
segmenting information obtained using SLBD is
much more efficient at optimizing segmentation.
Finally, we evaluated SLBD in comparison with
other bilingual semi-supervised methods, includ-
ing (Xu et al., 2008) (Xu); (Ma and Way, 2009)
(Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014)
(Zeng2014). The results presented in Table 4 indi-
cate that SLBD demonstrates much stronger per-
formance, primarily because these other methods
were developed with a focus on SMT, which caus-
es them to preferentially decrease the perplexity of
the subsequent SMT steps rather than producing a
highly accurate segmentation. In contrast to these
methods, the SLBD method exhibits greater gen-
eralizability.
</bodyText>
<page confidence="0.992269">
1214
</page>
<sectionHeader confidence="0.999099" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999911923076923">
In this paper, we propose a cascaded log-linear
model to involve learning three levels of bilin-
gual linguistic features to semi-supervisedly learn
a new CWS model. Different from other mono-
lingual and bilingual semi-supervised approach-
es, we employ various types of features to cap-
ture both monolingual grammars and bilingual
segmenting information, which allows our mod-
el to be very efficient at other NLP tasks and en-
dows it with higher generalizability. The evalu-
ation shows that our method significantly outper-
forms the state-of-the-art monolingual and bilin-
gual semi-supervised approaches.
</bodyText>
<sectionHeader confidence="0.999403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866615384615">
Masayuki Asahara, Kenta Fukuoka, Ai Azuma,
ChooiLing Goh, Yotaro Watanabe, Yuji Matsumo-
to, and Takahashi Tsuzuki. 2005. Combination
of machine learning methods for optimum chinese
word segmentation. In Proceedings of The Fourth
SIGHAN Workshop, pages 134-137.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of WMT, pages 224-232. Association for
Computational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Un-
supervised Tokenization for Machine Translation.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2. Association for Computational Lin-
guistics, 2009: 718-726.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language.. Master&apos;s thesis.
Yanjun Ma and Andy Way. 2009. Bilingually moti-
vated domain-adapted word segmentation for statis-
tical machine translation. Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, 2009: 549-557.
Mikolov, Tomas, et al. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems. 2013.
Graham Neubig, Taro Watanabe et al. 2011. An Unsu-
pervised Model for Joint Phrase Alignment and Ex-
traction. Proceedings of ACL 2011.
Graham Neubig, Taro Watanabe et al. 2012. Machine
Translation without Words through Substring Align-
ment. Proceedings of ACL 2012.
Feng Jiao, Shaojun Wang, and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Pro-
ceedings of ACL, pages 209-216, Sydney, Australia.
Koehn, Philipp, Franz Josef Och, and Daniel Mar-
cu. 2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1. Association for Computational Linguis-
tics, 2003.
Koehn, Philipp, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions. Asso-
ciation for Computational Linguistics, 2007: 177-
180.
Franz Josef Och and Hermann Ney. 2000. Improved s-
tatistical alignment models. In Proceedings of ACL,
pages 440-447.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1. Association for Com-
putational Linguistics, 2003: 160-167.
Fuchun Peng, Fangfang Feng and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. Proceedings of
the 20th international conference on Computational
Linguistics.
Stolcke, Andreas. 2002. SRILM-an extensible lan-
guage modeling toolkit. In INTERSPEECH. 2002
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data.. In Pro-
ceedings of EMNLP 2011.
Xu Sun, Houfeng Wang, Wenjie Li. 2012. Fast Online
Training with Frequency-Adaptive Learning Rates
for Chinese Word Segmentation and New Word De-
tection. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 253-262
Tian, Liang, et al. 2014. UM-Corpus: a large English-
Chinese parallel corpus for statistical machine trans-
lation. In Proceedings of the 9th International Con-
ference on Language Resources and Evaluation. EL-
RA Reykjavik, Iceland, 2014.
Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang,
and Jiajun Chen. 2012. Enhancing statistical ma-
chine translation with character alignment. In Pro-
ceedings of ACL, pages 285-290. Association for
Computational Linguistics.
</reference>
<page confidence="0.784342">
1215
</page>
<reference confidence="0.999258921052632">
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical
machine translation?. Proceedings of the Third
SIGHAN Workshop on Chinese Language Learn-
ing, pages 122õ128.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Herman-
n Ney. 2008. Bayesian Semi-Supervised Chinese
Word Segmentation for Statistical Machine Transla-
tion. Proceedings of Coling 2008.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, pages 29õ48.
Zhang and Clark. 2007. Chinese segmentation with a
word-based perceptron algorithm.. Proceedings of
ACL 2007.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing,
9(2):5:1-5:32, June.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao and
Isabel Trancoso. 2013a. Graph-based Semi-
Supervised Model for Joint Chinese Word Segmenta-
tion and Part-of-Speech Tagging. Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao and Is-
abel Trancoso 2013b. Co-regularizing character-
based and word-based models for semi-supervised
Chinese word segmentation. Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics.
Xiaodong Zeng, Derek F. Wong et al. 2014. To-
ward Better Chinese Word Segmentation for SMT vi-
a Bilingual Constraints. Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
2014: 1360-1369.
</reference>
<page confidence="0.990829">
1216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.580609">
<title confidence="0.9979075">Semi-supervised Chinese Word Segmentation based on Bilingual Information</title>
<author confidence="0.995478">Wei</author>
<affiliation confidence="0.85384">Institute of Chinese Academy of</affiliation>
<address confidence="0.99464">Beijing, 100190,</address>
<email confidence="0.975122">wei.chen.media@ia.ac.cn</email>
<abstract confidence="0.99486959375">This paper presents a bilingual semisupervised Chinese word segmentation (CWS) method that leverages the natural segmenting information of English sentences. The proposed method involves learning three levels of features, namely, character-level, phrase-level and sentence-level, provided by multiple submodels. We use a sub-model of conditional random fields (CRF) to learn monolingual grammars, a sub-model based on character-based alignment to obtain explicit segmenting knowledge, and another sub-model based on transliteration similarity to detect out-of-vocabulary (OOV) words. Moreover, we propose a sub-model leveraging neural network to ensure the proper treatment of the semantic gap and a phrase-based translation sub-model to score the translation probability of the Chinese segmentation and its corresponding English sentences. A cascaded log-linear model is employed to combine these features to segment bilingual unlabeled data, the results of which are used to justify the original supervised CWS model. The evaluation shows that our method results in superior results compared with those of the state-of-the-art monolingual and bilingual semi-supervised models that have been reported in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Kenta Fukuoka</author>
<author>Ai Azuma</author>
<author>ChooiLing Goh</author>
<author>Yotaro Watanabe</author>
<author>Yuji Matsumoto</author>
<author>Takahashi Tsuzuki</author>
</authors>
<title>Combination of machine learning methods for optimum chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of The Fourth SIGHAN Workshop,</booktitle>
<pages>134--137</pages>
<contexts>
<context position="27341" citStr="Asahara et al., 2005" startWordPosition="4475" endWordPosition="4478">riments In our evaluation, the F-score was used as the accuracy measure. The precision p is defined as the percentage of words in the decoder output that are segmented correctly, and the recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. The balanced F- score is calculated as 2pr/(p + r). We also report the recall of OOV words in our experiments. In the following, we refer to our methods as ”SLBD” (segmenter leveraging bilingual data). Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&amp;C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data. Moreover, we also evaluated the performance of our sub-models by J Ak1fk1 (yj−1� yj � e1 9) (17) � +B3 k2 1213 methods AS PKU F OOV F OOV Peng 91.6 52.5 91.1 59 Asahara 92.2 63.1 91.4 61.6 Z&amp;C 92.9 69.9 91.6 67.9 Zhao 93.1 72 92.3 60.6 character-level 92.3 58.6 92.9 60.8 Inner log-linear 95.9 78.8 96.1 81 Outer log-linear 96.7 80.8 97.1 85 Table 2: Word segmentation performance of SLBD and supervised CWS methods[%] segmenting the bilingual unlabeled dataset using cha</context>
</contexts>
<marker>Asahara, Fukuoka, Azuma, Goh, Watanabe, Matsumoto, Tsuzuki, 2005</marker>
<rawString>Masayuki Asahara, Kenta Fukuoka, Ai Azuma, ChooiLing Goh, Yotaro Watanabe, Yuji Matsumoto, and Takahashi Tsuzuki. 2005. Combination of machine learning methods for optimum chinese word segmentation. In Proceedings of The Fourth SIGHAN Workshop, pages 134-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6356" citStr="Chang et al., 2008" startWordPosition="937" endWordPosition="940">tences. After obtaining these multilevel features, we normalize them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original superFigure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus. In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standard grammars and biling</context>
<context position="9302" citStr="Chang et al., 2008" startWordPosition="1388" endWordPosition="1391"> the label distributions derived from unlabeled data. However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers’ personal experiences. By contrast, we leverage bilingual unlabeled data that contain the natural segmentation that is present in English sentences and can therefore extract linguistic knowledge without any manual assumptions or bias. 2.2 Bilingual Semi-supervised CWS Methods Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004). (Zeng et al., 2014) also used such mappings to bias a supervised s</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of WMT, pages 224-232. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised Tokenization for Machine Translation.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2. Association for Computational Linguistics,</booktitle>
<pages>718--726</pages>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised Tokenization for Machine Translation. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2. Association for Computational Linguistics, 2009: 718-726.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning.</booktitle>
<contexts>
<context position="11002" citStr="Lafferty et al., 2001" startWordPosition="1651" endWordPosition="1654">atures to capture syntactic and semantic information and a cascaded log-linear model to maintain balance between the monolingual grammars and the bilingual knowledge. 3 Multilevel Features In this section, we describe the three levels of features used in our approach. We propose to use character-level features to capture monolingual grammars and phrase-level and sentence-level features to obtain bilingual segmenting information. Moreover, we describe a cascaded log-linear model by proposing both inner and outer log-linear models. 3.1 Character-level Feature The conditional random field (CRF) (Lafferty et al., 2001) model was first used for CWS tasks by (Xue et al., 2003) who treated the CWS task as a sequence tagging problem and demonstrated this model’s effectiveness in detecting OOV words. In this paper, we score the character-level feature in the same manner defined by (Xue et al., 2003). For the jth character cj in the sentence cJ1 = c1...cJ, the score can be calculated as follows: � λkfk(yj−1, yj, cJ1 , j) (1) fCRF (j) = k where fk(yj−1, yj, cJ1 , j) is a feature function and Ak is a learned weight that corresponds to the feature fk. j represents the index of the character in the sentence. yj−1 and</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.. Master&apos;s thesis.</title>
<date>2005</date>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language.. Master&apos;s thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>Bilingually motivated domain-adapted word segmentation for statistical machine translation.</title>
<date>2009</date>
<booktitle>Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>549--557</pages>
<contexts>
<context position="6376" citStr="Ma and Way, 2009" startWordPosition="941" endWordPosition="944">g these multilevel features, we normalize them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original superFigure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus. In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standard grammars and bilingual segmenting infor</context>
<context position="9322" citStr="Ma and Way, 2009" startWordPosition="1392" endWordPosition="1395">ns derived from unlabeled data. However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers’ personal experiences. By contrast, we leverage bilingual unlabeled data that contain the natural segmentation that is present in English sentences and can therefore extract linguistic knowledge without any manual assumptions or bias. 2.2 Bilingual Semi-supervised CWS Methods Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004). (Zeng et al., 2014) also used such mappings to bias a supervised segmentation model to</context>
<context position="30805" citStr="Ma and Way, 2009" startWordPosition="5031" endWordPosition="5034">l unlabeled dataset. The testing data was the set of AS only. The evaluation is summarized in Table 3. The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation. Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014). The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. In contrast to these methods, the SLBD method exhibits greater generalizability. 1214 7 Conclusion In this paper, we propose a cascaded log-linear model to involve learning three levels of bilingual linguistic features to semi-sup</context>
</contexts>
<marker>Ma, Way, 2009</marker>
<rawString>Yanjun Ma and Andy Way. 2009. Bilingually motivated domain-adapted word segmentation for statistical machine translation. Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2009: 549-557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<marker>Mikolov, 2013</marker>
<rawString>Mikolov, Tomas, et al. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
</authors>
<title>An Unsupervised Model for Joint Phrase Alignment and Extraction.</title>
<date>2011</date>
<booktitle>Proceedings of ACL</booktitle>
<marker>Neubig, Watanabe, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe et al. 2011. An Unsupervised Model for Joint Phrase Alignment and Extraction. Proceedings of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
</authors>
<title>Machine Translation without Words through Substring Alignment.</title>
<date>2012</date>
<booktitle>Proceedings of ACL</booktitle>
<marker>Neubig, Watanabe, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe et al. 2012. Machine Translation without Words through Substring Alignment. Proceedings of ACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jiao</author>
<author>Shaojun Wang</author>
<author>Chi-Hoon Lee</author>
</authors>
<title>Semi-supervised conditional random fields for improved sequence segmentation and labeling.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>209--216</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2204" citStr="Jiao et al., 2006" startWordPosition="320" endWordPosition="323">us sequences of characters with no explicit delimiters (e.g., the spaces in English). Many studies have been conducted in this area, resulting in extensive investigation of the problem of Bo Xu Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China xubo@ia.ac.cn CWS using machine learning techniques in recent years. However, the reliability of CWS that can be achieved using machine learning techniques relies heavily on the availability of a large amount of high-quality, manually segmented data. Because hand-labeling individual words and word boundaries is very difficult (Jiao et al., 2006), producing segmented Chinese texts is very time-consuming and expensive. Although a number of manually segmented datasets have been constructed by various organizations, it is not feasible to combine them into a single complete dataset because of their incompatibility due to the use of various segmenting standards. Thus, it is difficult to build a large-scale manually segmented corpus, and the resulting lack of such a corpus is detrimental to further enhancement of the accuracy of CWS. To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been </context>
</contexts>
<marker>Jiao, Wang, Lee, 2006</marker>
<rawString>Feng Jiao, Shaojun Wang, and Chi-Hoon Lee. 2006. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In Proceedings of ACL, pages 209-216, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="17450" citStr="Koehn et al., 2003" startWordPosition="2784" endWordPosition="2787">lice” into ”ai l i s i/a l i c e” —by transforming the Chinese word into its pronunciation (represented by the function Fpy(·)) and splitting the English word into its constituent letters (represented by the function Flet(·)). Then, we train two phrase-based translation models (Chinese-English and English-Chinese) on the data obtained from the converted NE dictionary. Specifically, we apply two standard log-linear phrase-based SMT models. The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary. The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables. A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language. Moses (Koehn et al., 2007) is used as a decoder. Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset. Given these two phrase-based translation models, we calculate each span &lt; i, jm, jn &gt; in AOne for the Chinese word wn using the following formula: Str(&lt; i, jm, jn &gt;) = Sch−en(&lt; i, jm, jn &gt;) +Sen−ch(&lt; i, j</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1. Association for Computational Linguistics, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th annual meeting of the ACL on</booktitle>
<pages>177--180</pages>
<marker>Koehn, 2007</marker>
<rawString>Koehn, Philipp, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions. Association for Computational Linguistics, 2007: 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="17349" citStr="Och and Ney, 2000" startWordPosition="2769" endWordPosition="2772">ntroduce an initial NE dictionary and convert each dictionary item—for example, we convert ” UR AT/Alice” into ”ai l i s i/a l i c e” —by transforming the Chinese word into its pronunciation (represented by the function Fpy(·)) and splitting the English word into its constituent letters (represented by the function Flet(·)). Then, we train two phrase-based translation models (Chinese-English and English-Chinese) on the data obtained from the converted NE dictionary. Specifically, we apply two standard log-linear phrase-based SMT models. The GIZA++ aligner is adopted to obtain word alignments (Och and Ney, 2000) from the converted NE dictionary. The heuristic strategy of grow-diag-final-and (Koehn et al., 2003) is used to combine the bidirectional alignments to extract phrase translations and to reorder tables. A 5-gram language model with Kneser-Ney smoothing is trained using SRILM (Stolcke et al., 2002) on the target language. Moses (Koehn et al., 2007) is used as a decoder. Minimum error rate training (MERT) (Och et al., 2003) is applied to tune the feature parameters on the development dataset. Given these two phrase-based translation models, we calculate each span &lt; i, jm, jn &gt; in AOne for the C</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL, pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<marker>Och, 2003</marker>
<rawString>Och, Franz Josef. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. Association for Computational Linguistics, 2003: 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th international conference on Computational Linguistics.</booktitle>
<contexts>
<context position="7831" citStr="Peng et al., 2004" startWordPosition="1162" endWordPosition="1165">nd bilingual semi-supervised approaches. 2 Related Work First, we review related work on monolingual supervised and semi-supervised CWS methods. Then, we review bilingual semi-supervised CWS. 2.1 Monolingual Supervised and Semi-supervised CWS Methods Considerable efforts have been made in the NLP community in the study of Chinese word segmentation. The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003). Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et 1208 al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods. To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistic</context>
<context position="9834" citStr="Peng et al., 2004" startWordPosition="1471" endWordPosition="1474">emi-supervised CWS Methods Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004). (Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT. However, because most of these approaches focus on SMT performance, they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy. Thus, they sometimes ignore the standard grammars during segmentation in favor of satisfying the needs of SMT, thereby causing these methods to be rather unsuitable for other NLP tasks. By contrast, we propose to use various types of features to capture syntactic and semantic information a</context>
<context position="27310" citStr="Peng et al., 2004" startWordPosition="4470" endWordPosition="4473">ws and miscellaneous. 6 Experiments In our evaluation, the F-score was used as the accuracy measure. The precision p is defined as the percentage of words in the decoder output that are segmented correctly, and the recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. The balanced F- score is calculated as 2pr/(p + r). We also report the recall of OOV words in our experiments. In the following, we refer to our methods as ”SLBD” (segmenter leveraging bilingual data). Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&amp;C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data. Moreover, we also evaluated the performance of our sub-models by J Ak1fk1 (yj−1� yj � e1 9) (17) � +B3 k2 1213 methods AS PKU F OOV F OOV Peng 91.6 52.5 91.1 59 Asahara 92.2 63.1 91.4 61.6 Z&amp;C 92.9 69.9 91.6 67.9 Zhao 93.1 72 92.3 60.6 character-level 92.3 58.6 92.9 60.8 Inner log-linear 95.9 78.8 96.1 81 Outer log-linear 96.7 80.8 97.1 85 Table 2: Word segmentation performance of SLBD and supervised CWS methods[%] segmenting the biling</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. Proceedings of the 20th international conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM-an extensible language modeling toolkit. In INTERSPEECH. 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing chinese word segmentation using unlabeled data..</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="3011" citStr="Sun and Xu, 2011" startWordPosition="446" endWordPosition="449">to combine them into a single complete dataset because of their incompatibility due to the use of various segmenting standards. Thus, it is difficult to build a large-scale manually segmented corpus, and the resulting lack of such a corpus is detrimental to further enhancement of the accuracy of CWS. To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years. These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora). In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed. However, because monolingual unlabeled data contain limited natural segmenting information, in most semisupervised methods, the objective function tends to be optimized based on the personal experience and knowledge of the researchers. This practice means that these methods can typically yield high performance in</context>
<context position="8368" citStr="Sun and Xu, 2011" startWordPosition="1250" endWordPosition="1253"> linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et 1208 al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods. To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model. (Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data. However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers’ p</context>
<context position="29068" citStr="Sun and Xu, 2011" startWordPosition="4747" endWordPosition="4750">rformance when we combined the character-level and phrase-level features in the inner log-linear model, demonstrating that the proposed phrase-level features can be used to efficiently obtain bilingual segmenting information. Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model. Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&amp;X); (Zeng et al., 2013b) (Zeng). To ensure a fair comparison, we performed the evaluation in two steps. First, we input the entire bilingual unlabeled dataset into the SLBD method and input only the Chinese sentences from the bilingual unlabeled dataset into the other semi-supervised methods. Then, because the available monolingual unlabeled dataset was much larger than the bilingual unlabeled dataset in natural, we used the XIN CMN portion of Chinese Gigaword 2.0 as an additional unlabeled dataset for the monolingual semi-supervised methods. which contains 204 million words, more than ten</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing chinese word segmentation using unlabeled data.. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Houfeng Wang</author>
<author>Wenjie Li</author>
</authors>
<title>Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>253--262</pages>
<contexts>
<context position="29042" citStr="Sun et al., 2012" startWordPosition="4742" endWordPosition="4745">nificant improvement in performance when we combined the character-level and phrase-level features in the inner log-linear model, demonstrating that the proposed phrase-level features can be used to efficiently obtain bilingual segmenting information. Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model. Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&amp;X); (Zeng et al., 2013b) (Zeng). To ensure a fair comparison, we performed the evaluation in two steps. First, we input the entire bilingual unlabeled dataset into the SLBD method and input only the Chinese sentences from the bilingual unlabeled dataset into the other semi-supervised methods. Then, because the available monolingual unlabeled dataset was much larger than the bilingual unlabeled dataset in natural, we used the XIN CMN portion of Chinese Gigaword 2.0 as an additional unlabeled dataset for the monolingual semi-supervised methods. which contains 204 mi</context>
</contexts>
<marker>Sun, Wang, Li, 2012</marker>
<rawString>Xu Sun, Houfeng Wang, Wenjie Li. 2012. Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253-262</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Tian</author>
</authors>
<title>UM-Corpus: a large EnglishChinese parallel corpus for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation. ELRA Reykjavik,</booktitle>
<location>Iceland,</location>
<marker>Tian, 2014</marker>
<rawString>Tian, Liang, et al. 2014. UM-Corpus: a large EnglishChinese parallel corpus for statistical machine translation. In Proceedings of the 9th International Conference on Language Resources and Evaluation. ELRA Reykjavik, Iceland, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ning Xi</author>
<author>Guangchao Tang</author>
<author>Xinyu Dai</author>
<author>Shujian Huang</author>
<author>Jiajun Chen</author>
</authors>
<title>Enhancing statistical machine translation with character alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>285--290</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6416" citStr="Xi et al., 2012" startWordPosition="948" endWordPosition="951"> them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original superFigure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus. In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standard grammars and bilingual segmenting information, which allows our semi-supervised</context>
<context position="9362" citStr="Xi et al., 2012" startWordPosition="1399" endWordPosition="1402">because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers’ personal experiences. By contrast, we leverage bilingual unlabeled data that contain the natural segmentation that is present in English sentences and can therefore extract linguistic knowledge without any manual assumptions or bias. 2.2 Bilingual Semi-supervised CWS Methods Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004). (Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT. However,</context>
<context position="30829" citStr="Xi et al., 2012" startWordPosition="5036" endWordPosition="5039">testing data was the set of AS only. The evaluation is summarized in Table 3. The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation. Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014). The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. In contrast to these methods, the SLBD method exhibits greater generalizability. 1214 7 Conclusion In this paper, we propose a cascaded log-linear model to involve learning three levels of bilingual linguistic features to semi-supervisedly learn a new CW</context>
</contexts>
<marker>Xi, Tang, Dai, Huang, Chen, 2012</marker>
<rawString>Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang, and Jiajun Chen. 2012. Enhancing statistical machine translation with character alignment. In Proceedings of ACL, pages 285-290. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Do we need Chinese word segmentation for statistical machine translation?.</title>
<date>2004</date>
<booktitle>Proceedings of the Third SIGHAN Workshop on Chinese Language Learning,</booktitle>
<pages>122--128</pages>
<contexts>
<context position="9746" citStr="Xu et al., 2004" startWordPosition="1455" endWordPosition="1458">e extract linguistic knowledge without any manual assumptions or bias. 2.2 Bilingual Semi-supervised CWS Methods Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004). (Zeng et al., 2014) also used such mappings to bias a supervised segmentation model toward a better solution for SMT. However, because most of these approaches focus on SMT performance, they emphasize decreasing the perplexity of the bilingual data and word alignment rather than improving the CWS accuracy. Thus, they sometimes ignore the standard grammars during segmentation in favor of satisfying the needs of SMT, thereby causing these methods to be rather unsuitable for other NLP tasks. By contrast, we </context>
</contexts>
<marker>Xu, Zens, Ney, 2004</marker>
<rawString>Jia Xu, Richard Zens, and Hermann Ney. 2004. Do we need Chinese word segmentation for statistical machine translation?. Proceedings of the Third SIGHAN Workshop on Chinese Language Learning, pages 122õ128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>Proceedings of Coling</booktitle>
<contexts>
<context position="6334" citStr="Xu et al., 2008" startWordPosition="933" endWordPosition="936">ponding English sentences. After obtaining these multilevel features, we normalize them and combine them into two log-linear models in a cascaded structure, which is illustrated in Fig 2. Finally, we segment the bilingual unlabeled data using the proposed model and use the segmentation of those data to justify the original superFigure 2: The structure of cascaded log-linear model with multilevel features vised CWS model, which was trained on a standard manually segmented corpus. In fact, several semi-supervised CWS methods have previously been proposed that leverage bilingual unlabeled data ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)). However, most were developed for statistical machine translation (SMT), causing them to focus on decreasing the perplexity of the bilingual data and the word alignment process rather than on achieving more accurate segmentation. These methods achieve significant improvement in SMT performance but are not very suitable for common NLP tasks because in many situations, they ignore the standard grammars to satisfy the needs of SMT. By contrast, we employ various types of features to capture both monolingual standa</context>
<context position="9280" citStr="Xu et al., 2008" startWordPosition="1384" endWordPosition="1387"> CRF model based on the label distributions derived from unlabeled data. However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers’ personal experiences. By contrast, we leverage bilingual unlabeled data that contain the natural segmentation that is present in English sentences and can therefore extract linguistic knowledge without any manual assumptions or bias. 2.2 Bilingual Semi-supervised CWS Methods Some previous work ((Xu et al., 2008); (Chang et al., 2008); (Ma and Way, 2009);(Chung et al., 2009); (Xi et al., 2012)) has been performed on leveraging bilingual unlabeled data to achieve better segmentation, although most such studies have focused on statistical machine translation (SMT). These approaches leverage the mappings of individual English words to one or more consecutive Chinese characters either to construct a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004) or to form a labeled dataset for training a sequence labeling model (Peng et al., 2004). (Zeng et al., 2014) also used such mappings </context>
<context position="30780" citStr="Xu et al., 2008" startWordPosition="5026" endWordPosition="5029">of words in the bilingual unlabeled dataset. The testing data was the set of AS only. The evaluation is summarized in Table 3. The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation. Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu); (Ma and Way, 2009) (Ma); (Xi et al., 2012) (Xi);(Zeng et al., 2014) (Zeng2014). The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. In contrast to these methods, the SLBD method exhibits greater generalizability. 1214 7 Conclusion In this paper, we propose a cascaded log-linear model to involve learning three levels of bilingual lingui</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation. Proceedings of Coling 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>29--48</pages>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, pages 29õ48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhang</author>
<author>Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm..</title>
<date>2007</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="7885" citStr="Zhang and Clark, 2007" startWordPosition="1171" endWordPosition="1174"> Work First, we review related work on monolingual supervised and semi-supervised CWS methods. Then, we review bilingual semi-supervised CWS. 2.1 Monolingual Supervised and Semi-supervised CWS Methods Considerable efforts have been made in the NLP community in the study of Chinese word segmentation. The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003). Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et 1208 al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods. To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CR</context>
<context position="27376" citStr="Zhang and Clark, 2007" startWordPosition="4480" endWordPosition="4483">core was used as the accuracy measure. The precision p is defined as the percentage of words in the decoder output that are segmented correctly, and the recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. The balanced F- score is calculated as 2pr/(p + r). We also report the recall of OOV words in our experiments. In the following, we refer to our methods as ”SLBD” (segmenter leveraging bilingual data). Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&amp;C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data. Moreover, we also evaluated the performance of our sub-models by J Ak1fk1 (yj−1� yj � e1 9) (17) � +B3 k2 1213 methods AS PKU F OOV F OOV Peng 91.6 52.5 91.1 59 Asahara 92.2 63.1 91.4 61.6 Z&amp;C 92.9 69.9 91.6 67.9 Zhao 93.1 72 92.3 60.6 character-level 92.3 58.6 92.9 60.8 Inner log-linear 95.9 78.8 96.1 81 Outer log-linear 96.7 80.8 97.1 85 Table 2: Word segmentation performance of SLBD and supervised CWS methods[%] segmenting the bilingual unlabeled dataset using character-level features only, the inn</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Zhang and Clark. 2007. Chinese segmentation with a word-based perceptron algorithm.. Proceedings of ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>A unified character-based tagging framework for chinese word segmentation.</title>
<date>2010</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<pages>9--2</pages>
<contexts>
<context position="7906" citStr="Zhao et al., 2010" startWordPosition="1175" endWordPosition="1178">lated work on monolingual supervised and semi-supervised CWS methods. Then, we review bilingual semi-supervised CWS. 2.1 Monolingual Supervised and Semi-supervised CWS Methods Considerable efforts have been made in the NLP community in the study of Chinese word segmentation. The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003). Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004); (Asahara et 1208 al., 2005); (Zhang and Clark, 2007); (Zhao et al., 2010)). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods. To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model. (Zeng et al.</context>
<context position="27403" citStr="Zhao et al., 2010" startWordPosition="4485" endWordPosition="4488">easure. The precision p is defined as the percentage of words in the decoder output that are segmented correctly, and the recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. The balanced F- score is calculated as 2pr/(p + r). We also report the recall of OOV words in our experiments. In the following, we refer to our methods as ”SLBD” (segmenter leveraging bilingual data). Initially, we evaluated state-of-the-art supervised CWS methods, i.e., those of (Peng et al., 2004) (Peng); (Asahara et al., 2005) (Asahara); (Zhang and Clark, 2007) (Z&amp;C); (Zhao et al., 2010) (Zhao), whose models are trained only on manually segmented data. Moreover, we also evaluated the performance of our sub-models by J Ak1fk1 (yj−1� yj � e1 9) (17) � +B3 k2 1213 methods AS PKU F OOV F OOV Peng 91.6 52.5 91.1 59 Asahara 92.2 63.1 91.4 61.6 Z&amp;C 92.9 69.9 91.6 67.9 Zhao 93.1 72 92.3 60.6 character-level 92.3 58.6 92.9 60.8 Inner log-linear 95.9 78.8 96.1 81 Outer log-linear 96.7 80.8 97.1 85 Table 2: Word segmentation performance of SLBD and supervised CWS methods[%] segmenting the bilingual unlabeled dataset using character-level features only, the inner log-linear model (which </context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2010</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2010. A unified character-based tagging framework for chinese word segmentation. ACM Transactions on Asian Language Information Processing, 9(2):5:1-5:32, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Isabel Trancoso</author>
</authors>
<title>Graph-based SemiSupervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging.</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3031" citStr="Zeng et al., 2013" startWordPosition="450" endWordPosition="453"> a single complete dataset because of their incompatibility due to the use of various segmenting standards. Thus, it is difficult to build a large-scale manually segmented corpus, and the resulting lack of such a corpus is detrimental to further enhancement of the accuracy of CWS. To address the scarcity of manually segmented corpora, a number of semi-supervised CWS approaches have been intensively investigated in recent years. These approaches attempt to either learn the predicted label distribution (Jiao et al., 2006) or extract mutual information ((Liang et al., 2005); (Sun and Xu, 2011); (Zeng et al., 2013a)) from large-scale monolingual unlabeled data to update the baseline model (from manually segmented corpora). In addition to these techniques, several co-training approaches (Zeng et al., 2013b) using character-based and word-based models have also been employed. However, because monolingual unlabeled data contain limited natural segmenting information, in most semisupervised methods, the objective function tends to be optimized based on the personal experience and knowledge of the researchers. This practice means that these methods can typically yield high performance in certain specialized</context>
<context position="8512" citStr="Zeng et al., 2013" startWordPosition="1273" endWordPosition="1276"> al., 2010)). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods. To address this problem, a number of semisupervised CWS approaches have been intensively investigated in recent years. For example, (Sun and Xu, 2011) enhanced their segmentation results by interpolating statistics-based features derived from unlabeled data into a CRF model. (Zeng et al., 2013a) introduced a graph-based semi-supervised joint model of Chinese word segmentation and part-of-speech tagging and regularized the learning of a linear CRF model based on the label distributions derived from unlabeled data. However, because monolingual unlabeled data lack natural segmenting information, most previous semi-supervised CWS methods have required certain assumptions to be made regarding their objective functions based on the researchers’ personal experiences. By contrast, we leverage bilingual unlabeled data that contain the natural segmentation that is present in English sentence</context>
<context position="29094" citStr="Zeng et al., 2013" startWordPosition="4752" endWordPosition="4755"> the character-level and phrase-level features in the inner log-linear model, demonstrating that the proposed phrase-level features can be used to efficiently obtain bilingual segmenting information. Moreover, the outer log-linear model achieves a further enhancement, thereby demonstrating that the sentence-level features can be used to effectively re-rank the candidate segmentations produced by the inner log-linear model. Next, we compared the SLBD method with several state-of-the-art monolingual semi-supervised methods, including those of (Sun et al., 2012) (Sun); (Sun and Xu, 2011) (5&amp;X); (Zeng et al., 2013b) (Zeng). To ensure a fair comparison, we performed the evaluation in two steps. First, we input the entire bilingual unlabeled dataset into the SLBD method and input only the Chinese sentences from the bilingual unlabeled dataset into the other semi-supervised methods. Then, because the available monolingual unlabeled dataset was much larger than the bilingual unlabeled dataset in natural, we used the XIN CMN portion of Chinese Gigaword 2.0 as an additional unlabeled dataset for the monolingual semi-supervised methods. which contains 204 million words, more than ten times methods Bilingual d</context>
</contexts>
<marker>Zeng, Wong, Chao, Trancoso, 2013</marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao and Isabel Trancoso. 2013a. Graph-based SemiSupervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
<author>S Lidia</author>
</authors>
<title>Chao and Isabel Trancoso 2013b. Co-regularizing characterbased and word-based models for semi-supervised Chinese word segmentation.</title>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Zeng, Wong, Lidia, </marker>
<rawString>Xiaodong Zeng, Derek F. Wong, Lidia S. Chao and Isabel Trancoso 2013b. Co-regularizing characterbased and word-based models for semi-supervised Chinese word segmentation. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Zeng</author>
<author>Derek F Wong</author>
</authors>
<title>Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints.</title>
<date>2014</date>
<booktitle>Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<pages>1360--1369</pages>
<marker>Zeng, Wong, 2014</marker>
<rawString>Xiaodong Zeng, Derek F. Wong et al. 2014. Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints. Proceedings of the Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. 2014: 1360-1369.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
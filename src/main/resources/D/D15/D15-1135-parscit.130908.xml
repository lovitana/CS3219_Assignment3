<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9975915">
Automatically Solving Number Word Problems
by Semantic Parsing and Reasoning
</title>
<author confidence="0.997591">
Shuming Shi1, Yuehui Wang2*, Chin-Yew Lin1, Xiaojiang Liu1 and Yong Rui1
</author>
<affiliation confidence="0.941287">
1 Microsoft Research
</affiliation>
<email confidence="0.923242">
{shumings, cyl, xiaojl, yongrui}@microsoft.com
</email>
<affiliation confidence="0.992494">
2 University of Science and Technology of China
</affiliation>
<email confidence="0.98881">
wyh9346@mail.ustc.edu.cn
</email>
<sectionHeader confidence="0.998539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876916666667">
This paper presents a semantic parsing
and reasoning approach to automatically
solving math word problems. A new
meaning representation language is de-
signed to bridge natural language text and
math expressions. A CFG parser is imple-
mented based on 9,600 semi-automati-
cally created grammar rules. We conduct
experiments on a test set of over 1,500
number word problems (i.e., verbally ex-
pressed number problems) and yield
95.4% precision and 60.2% recall.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999095263157895">
Computers, since their creation, have exceeded
human beings in (speed and accuracy of) mathe-
matical calculation. However, it is still a big chal-
lenge nowadays to design algorithms to automat-
ically solve even primary-school-level math word
problems (i.e., math problems described in natural
language).
Efforts to automatically solve math word prob-
lems date back to the 1960s (Bobrow, 1964a, b).
Previous work on this topic falls into two catego-
ries: symbolic approaches and statistical learning
methods. In symbolic approaches (Bobrow,
1964a, b; Charniak, 1968; Bakman, 2007; Liguda
&amp; Pfeiffer, 2012), math problem sentences are
transformed to certain structures by pattern
matching or verb categorization. Equations are
then derived from the structures. Statistical learn-
ing methods are employed in two recent papers
(Kushman et al., 2014; Hosseini et al., 2014).
</bodyText>
<note confidence="0.610335">
* Work done while this author was an intern at Microsoft
Research
</note>
<bodyText confidence="0.841898730769231">
Most (if not all) previous symbolic approaches
suffer from two major shortcomings. First, natural
language (NL) sentences are processed by simply
applying pattern matching and/or transformation
rules in an ad-hoc manner (refer to the related
work section for more details). Second, surpris-
ingly, they seldom report evaluation results about
the effectiveness of the methods (except for some
examples for demonstration purposes). For the
small percentage of work with evaluation results
available, it is unclear whether the patterns and
rules are specially designed for specific sentences
in a test set.
1). One number is 16 more than another. If the
smaller number is subtracted from 2/3 of the larger,
the result is 1/4 of the sum of the two numbers. Find
the numbers.
2). Nine plus the sum of an even integer and its
square is 3 raised to the power of 4. What is the num-
ber?
3). The tens digit of a two-digit number is 3 more
than the units digit. If the number is 8 more than 6
times the sum of the digits, find the number.
4). If the first and third of three consecutive even in-
tegers are added, the result is 12 less than three times
the second integer. Find the integers.
</bodyText>
<figureCaption confidence="0.991751">
Figure 1: Number word problem examples
</figureCaption>
<bodyText confidence="0.999933153846154">
In this paper, we present a computer system
called SigmaDolphin which automatically solves
math word problems by semantic parsing and rea-
soning. We design a meaning representation lan-
guage called DOL (abbreviation of dolphin lan-
guage) as the structured semantic representation
of NL text. A semantic parser is implemented to
transform math problem text into DOL trees. A
reasoning module is included to derive math ex-
pressions from DOL trees and to calculate final
answers. Our approach falls into the symbolic cat-
egory, but makes improvements over previous
symbolic methods in the following ways,
</bodyText>
<page confidence="0.959532">
1132
</page>
<note confidence="0.985765">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1132–1142,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9622315">
1) We introduce a systematic way of parsing
NL text, based on context-free grammar (CFG).
2) Evaluation is enhanced in terms of both data
set construction and evaluation mechanisms. We
split the problem set into a development set
(called dev set) and a test set. Only the dev set is
accessible during our algorithm design (especially
in designing CFG rules and in implementing the
parsing algorithm), which avoids over-tuning to-
wards the test set. Three metrics (precision, recall,
and F1) are employed to measure system perfor-
mance from multiple perspectives, in contrast to
all previous work (including the statistical ones)
which only measures accuracy.
We target, in experiments, a subtype of word
problems: number word problems (i.e., verbally
expressed number problems, as shown in Figure
1). We hope to extend our techniques to handle
general math word problems in the future.
We build a test set of over 1,500 problems and
make a quantitative comparison with state-of-the-
art statistical methods. Evaluation results show
that our approach significantly outperforms base-
line methods on our test set. Our system yields an
extremely high precision of 95.4% and a reasona-
ble recall of 60.2%, which shows promising appli-
cation of our system in precision-critical situa-
tions.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.990158">
2.1 Math word problem solving
</subsectionHeader>
<bodyText confidence="0.999916">
Most previous work on automatic word problem
solving is symbolic. STUDENT (Bobrow, 1964a,
b) handles algebraic problems by first transform-
ing NL sentences into kernel sentences using a
small set of transformation patterns. The kernel
sentences are then transformed to math expres-
sions by recursive use of pattern matching.
CARPS (Charniak, 1968, 1969) uses a similar ap-
proach to solve English rate problems. The major
difference is the introduction of a tree structure as
the internal representation of the information
gathered for one object. Liguda &amp; Pfeiffer (2012)
propose modeling math word problems with aug-
mented semantic networks. Addition/subtraction
problems are studied most in early research (Bri-
ars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa,
1986; Bakman, 2007; Ma et al., 2010). Please re-
fer to Mukherjee &amp; Garain (2008) for a review of
symbolic approaches before 2008.
</bodyText>
<footnote confidence="0.879324">
1 http://www.wolframalpha.com
</footnote>
<bodyText confidence="0.998806892857143">
No empirical evaluation results are reported in
most of the above work. Almost all of these ap-
proaches parse NL text by simply applying pattern
matching rules in an ad-hoc manner. For example,
as mentioned in Bobrow (1964b), due to the pat-
tern “($, AND $)”, the system would incorrectly
divide “Tom has 2 apples, 3 bananas, and 4
pears.” into two “sentences”: “Tom has 2 apples,
3 bananas.” and “4 pears.”
WolframAlpha1 shows some examples2 of au-
tomatically solving elementary math word prob-
lems, with technique details unknown to the gen-
eral public. Other examples on the web site
demonstrate a large coverage of short phrase que-
ries on math and other domains. By randomly se-
lecting problems from our dataset and manually
testing on their web site, we find that it fails to
handle most problems in our problem collection.
Statistical learning methods have been pro-
posed recently in two papers: Hosseini et al.
(2014) solve single step or multi-step homoge-
nous addition and subtraction problems by learn-
ing verb categories from the training data. Kush-
man et al. (2014) can solve a wide range of word
problems, given that the equation systems and so-
lutions are attached to problems in the training set.
The method of the latter paper (referred to as
KAZB henceforth) is used as one of our baselines.
</bodyText>
<subsectionHeader confidence="0.999783">
2.2 Semantic parsing
</subsectionHeader>
<bodyText confidence="0.999940818181818">
There has been much work on analyzing the se-
mantic structure of NL strings. In semantic role
labeling and frame-semantic parsing (Gildea &amp;
Jurafsky, 2002; Carreras &amp; Marquez, 2004;
Marquez et al., 2008; Baker et al., 2007; Das et
al., 2014), predicate-argument structures are dis-
covered from text as their shallow semantic repre-
sentation. In math problem solving, we need a
deeper and richer semantic representation from
which to facilitate the deriving of math expres-
sions.
Another type of semantic parsing work (Zelle
&amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005;
Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney,
2007; Cai &amp; Yates, 2013; Berant et al., 2013;
Kwiatkowski et al., 2013; Berant &amp; Liang, 2014)
maps NL text into logical forms by supervised or
semi-supervised learning. Some of them are based
on or related to combinatory categorial grammar
(CCG) (Steedman, 2000). Abstract Meaning Rep-
resentation (AMR) (Banarescu et al., 2013) keeps
richer semantic information than CCG and logical
</bodyText>
<footnote confidence="0.997737">
2 https://www.wolframalpha.com/examples/Elementary-
Math.html (bottom-right part)
</footnote>
<page confidence="0.988622">
1133
</page>
<bodyText confidence="0.9882825">
forms. In Section 3.1.4, we discuss the differences
between DOL, AMR, and CCG, and explain why
we choose DOL as the meaning representation
language for math problem solving.
</bodyText>
<sectionHeader confidence="0.99414" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.942413666666666">
Consider the first problem in Figure 1 (written be-
low for convenience),
One number is 16 more than another. If the smaller
number is subtracted from 2/3 of the larger, the result
is 1/4 of the sum of the two numbers. Find the numbers.
To automatically solve this problem, the com-
puter system needs to figure out, somehow, that 1)
two numbers x, y are demanded, and 2) they sat-
isfy the equations below,
</bodyText>
<equation confidence="0.999562">
x = 16 + y (1)
(2/3)x – y = (x + y) / 4 (2)
</equation>
<bodyText confidence="0.99944425">
To achieve this, reasoning must be performed
based on common sense knowledge and the infor-
mation provided by the source problem. Given the
difficulty of performing reasoning directly on un-
structured and ambiguous natural language text, it
is reasonable to transform the source text into a
structured, less ambiguous representation.
Our approach contains three modules:
</bodyText>
<listItem confidence="0.998613">
1) A meaning representation language called
DOL newly designed by us as the semantic
representation of natural language text.
2) A semantic parser which transforms natu-
ral language sentences of a math problem
into DOL representation.
3) A reasoning module to derive math expres-
sions from DOL representation.
</listItem>
<figureCaption confidence="0.991067">
Figure 2: DOL example
</figureCaption>
<subsectionHeader confidence="0.991002">
3.1 DOL: Meaning representation language
</subsectionHeader>
<bodyText confidence="0.99999">
Every meaningful piece of NL text is represented
in DOL as a semantic tree of various node types.
Figure 2 shows the DOL representation of the sec-
ond problem of Figure 1. It contains two semantic
trees, corresponding to the two sentences.
</bodyText>
<subsectionHeader confidence="0.876915">
3.1.1 Node types
</subsectionHeader>
<bodyText confidence="0.909456">
Node types of a DOL tree include constants, clas-
ses, and functions. Each interim node of a tree is
always a function; and each leaf node can be a
constant, a class, or a zero-argument function.
Constants in DOL refer to specific objects in
the world. A constant can be a number (e.g., 3.57),
a lexical string (like “New York”), or an entity.
Classes: An entity class refers to a category of
entities sharing common semantic properties. For
example, all cities are represented by the class lo-
cation.city; and math.number is a class for all
numbers. It is clear that,
</bodyText>
<equation confidence="0.925014">
3.14159 E math.number
city.new_york E location.city
</equation>
<bodyText confidence="0.958765">
A class C1 is a sub-class (denoted by S) of an-
other class C2 if and only if every instance of C1
are in C2. The following holds according to com-
mon sense knowledge,
math.number S math.expression
person.pianist S person.performer
Template classes are classes with one or more
parameters, just like template classes in C++. The
most important template class in DOL is
t.list&lt;c,m,n&gt;
where c is a class; m and n are integers. Each in-
stance of this class is a list containing at least m
and at most n elements of type c. For example,
each instance of t.list&lt;math.number,2,+oo&gt; is a
list containing at least 2 numbers.
Functions are used in DOL as the major way
to form larger language units from smaller ones.
A function is comprised of a name, a list of core
arguments, and a return type. DOL enables func-
tion overloading (again borrowing ideas from pro-
gramming languages). That is, one function name
can have multiple core-argument specifications.
Below are two specifications for fn.math.sum
(which appears in the example of Figure 2).
</bodyText>
<construct confidence="0.6468415">
nf.math.sum!1:
$1: math.expression; $2: math.expression
return type: math.expression
return value: The sum of its arguments
nf.math.sum!2:
$1: t.list&lt;math.expression,2,+oo&gt;
return type: math.expression
return value: The sum of the elements in $1
</construct>
<bodyText confidence="0.913945333333333">
English: Nine plus the sum of an even integer and
its square is 3 raised to the power of 4. What is the
number?
</bodyText>
<figure confidence="0.988972818181818">
DOL trees:
vf.be.equ
nf.math.sum nf.math.power
nf.math.sum
9
3 4
nf.list-v1 nf.math.2nd_power
math.integer 1 mf.number.even nf.it-v1
vf.be.equ
nf.list-v1 nf.what
math.number 1
</figure>
<page confidence="0.988961">
1134
</page>
<bodyText confidence="0.965570071428571">
Here “$1: math.expression” means the first ar-
gument has type math.expression.
DOL supports three kinds of functions: noun
functions, verb functions, and modifier functions.
Noun functions map entities to their properties
or to other entities having specific relations with
the argument(s). For example, nf.math.sum maps
math expressions to their sum. Noun functions are
used to represent noun phrases in natural language
text. More noun functions are shown in Table 1.
Among all noun functions, nf.list has a special
important position due to its high frequency in
DOL trees. The function is specified below,
nf.list
</bodyText>
<listItem confidence="0.457933">
$1: class; $2: math.number
return type: t.list&lt;$1&gt;
return value: An entity list with cardinality $2
</listItem>
<bodyText confidence="0.971378379310345">
and element type $1
For example nf.list(math.number,5) returns a
list containing 5 elements of type math.number. It
is the semantic representation of “five numbers”.
Pronoun functions are special zero-argument
noun functions. Examples are nf.it (representing
an already-mentioned entity or event) and nf.what
(denoting an unknown entity or entity list).
Verb functions act as sentences or sub-sen-
tences in DOL. As an example, vf.be.equ (in Fig-
ure 2) is a verb function that has two arguments of
the quantity type.
vf.be.equ
$1: quantity.generic; $2: quantity.generic
return type: t.vf
Meaning: Two quantities $1 and $2 have the
same value
In addition to core arguments ($1, $2, etc.),
many functions can take additional extended ar-
guments as their modifiers. Our last function type
called modifier functions often take the role of ex-
tended arguments, to modify noun functions, verb
functions, or other modifier functions. Modifier
functions are used in DOL as the semantic repre-
sentation of adjectives, adverb phrases (including
conjunctive adverb phrases), and prepositional
phrases in natural languages. In the example of
Figure 2, the function mf.number.even modifies
the noun function nf.list as its extended argument.
</bodyText>
<subsectionHeader confidence="0.626038">
3.1.2 Entity variables
</subsectionHeader>
<bodyText confidence="0.884013391304348">
Variables are assigned to DOL sub-trees for indi-
cating the co-reference of sub-trees to entities and
for facilitating the construction of logical forms
and math expressions from DOL. In Figure 2, the
same variable v1 (meaning a variable with ID 1)
is assigned to two sub-trees in the first sentence
and one sub-tree in the second sentence. Thus the
three sub-trees refer to the same entity.
Function Remarks
nf.math.numerator Get the numerator of fraction
$1: math.fraction $1
ret: math.number
nf.math.gcd Get the greatest common di-
$1: t.list&lt;math.integer,2,+∞&gt; visor of the elements of $1
ret: math.integer
nf.e.height Get the height of $1 which is
$1: e.concrete a concrete entity
ret: quantity.length
vf.believe Agent $1 believes that $2 is
$1: e.agent; $2: t.vf.std true as a predicate
ret: t.vf
mf.number.even Indicating the property of be-
ret: t.mf.adj ing an even number
</bodyText>
<tableCaption confidence="0.993229">
Table 1: Example DOL functions
</tableCaption>
<subsectionHeader confidence="0.394758">
3.1.3 Key features of DOL
</subsectionHeader>
<bodyText confidence="0.976789828571428">
DOL has some nice characteristics that are critical
to building a high-precision math problem solving
system. That is why we invent DOL as our mean-
ing representation language instead of employing
an existing one.
First, DOL is a strongly typed language. Every
function has clearly defined argument types and a
return type. A valid DOL tree must satisfy the
type-compatibility property:
Type-compatibility: The type of each child of a
function node should match the corresponding ar-
gument type of the function.
For example, in Figure 2, the return type of
nf.math.power is math.expression, which matches
the second argument of vf.be.equ. However, the
following two trees (yielded from the correspond-
ing pieces of text) are invalid because they do not
satisfy type-compatibility.
sum of 100 [unreasonable text]
nf.math.sum!2(100) [invalid DOL tree]
sum of 3 and Jordan [unreasonable text]
nf.math.sum!2({3, “Jordan”}) [invalid tree]
Second, we maintain in DOL an open-domain
type system. The type system contains over 1000
manually verified classes and more automatically
generated ones (refer to Section 3.2.1 for more de-
tails). Such a comprehensive type system makes it
possible to define various kinds of functions and
to perform type-compatibility checking. In con-
trast, most previous semantic languages have at
most 100+ types at the grammar level. In addition,
by introducing template classes, we avoid main-
taining a lot of potentially duplicate types and re-
duce the type system management efforts. To the
best of our knowledge, template classes are not
</bodyText>
<page confidence="0.955037">
1135
</page>
<bodyText confidence="0.998937333333333">
available in other semantic representation lan-
guages.
Third, DOL has built-in data structures like
t.list and nf.list which greatly facilitate both func-
tion declaration and text representation (espe-
cially math text representation). For example, the
two variants of nf.math.sum (refer to Section 3.1.1
for their specifications) are enough to represent
the following English phrases:
</bodyText>
<equation confidence="0.890862833333333">
3 plus 5
4 nf.math.sum!1(3, 5)
sum of 3, 5, 7, and 9
4 nf.math.sum!2(nf.list(3, 5, 7, 9))
sum of ten thousand numbers
4 nf.math.sum!2(nf.list(math.number,10000))
</equation>
<bodyText confidence="0.981469">
Without t.list or nf.list, we would have to define
a lot of overloaded functions for nf.math.sum to
deal with different numbers of addends.
</bodyText>
<subsectionHeader confidence="0.94357">
3.1.4 Comparing with other languages
</subsectionHeader>
<bodyText confidence="0.994026083333333">
Among all meaning representation languages,
AMR (Banarescu et al., 2013) is most similar to
DOL. Their major differences are: First, they use
very different mechanisms to represent noun
phrases. In AMR, a sentence (e.g., “the boy de-
stroyed the room”) and a noun phrase (e.g., “the
boy’s destruction of the room”) can have the same
representation. While in DOL, a sentence is al-
ways represented by a verb function; and a noun
phrase is always a noun function or a constant.
Second, DOL has a larger type system and is
stricter in type compatibility checking. Third,
DOL has template classes and built-in data struc-
tures like t.list and nf.list to facilitate the represen-
tation of math concepts.
CCG (Steedman, 2000) provides a transparent
interface between syntax and semantics. In CCG,
semantic information is defined on words (e.g.,
“λx.odd(x)” for “odd” and “λx.number(x)” for
“number”). In contrast, DOL explicitly connects
NL text patterns to semantic elements. For exam-
ple, as shown in Table 2 (Section 3.2.1), one CFG
grammar rule connects pattern “{$1} raised to the
power of {$2}” to function nf.math.power.
Logical forms are another way of meaning rep-
resentation. We choose not to transform NL text
directly to logical forms for two reasons: On one
hand, state-of-the-art methods for mapping NL
text into logical forms typically target short, one-
sentence queries in restricted domains. However,
many math word problems are long and contain
multiple sentences. On the other hand, variable-id
assignment is a big issue in direct logical form
construction for many math problems. Let’s use
the following problem (i.e., the first problem of
Figure 1) to illustrate,
One number is 16 more than another. If the smaller
number is subtracted from 2/3 of the larger, the result
is 1/4 of the sum of the two numbers. Find the numbers.
For this problem, it is difficult to determine
whether “the smaller number” refers to “one num-
ber” or “another” in directly constructing logical
forms. It is therefore a challenge to construct a
correct logical form for such kinds of problems.
Our solution to the above challenge is assigning
a new variable ID (which is different from the IDs
of “one number” and “another”) and to delay the
final variable-ID assignment to the reasoning
stage. To enable this mechanism, the meaning
representation language should support a lazy var-
iable ID assignment and keep as much infor-
mation (e.g., determiners, plurals, modifiers) from
the noun phrases as possible. DOL is a language
that always keeps the structure information of
phrases, whether or not it has been assigned a var-
iable ID.
In summary, compared with other languages,
DOL has some unique features which make it
more suitable for our math problem solving sce-
nario.
</bodyText>
<subsectionHeader confidence="0.998977">
3.2 Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.9999886">
Our parsing algorithm is based on context-free
grammar (CFG) (Chomsky, 1956; Backus, 1959;
Jurafsky &amp; Martin, 2000), a commonly used
mathematical system for modeling constituent
structure in natural languages.
</bodyText>
<sectionHeader confidence="0.525867" genericHeader="method">
3.2.1 CFG for connecting DOL and IL
</sectionHeader>
<bodyText confidence="0.999889705882353">
The core part of a CFG is the set of grammar
rules. Example English grammar rules for build-
ing syntactic parsers include “S → NP VP”, “NP →
CD  |DT NN  |NP PP”, etc. Table 2 shows some
example CFG rules in our system for mapping
DOL nodes to natural language word sequences.
The left side of each rule is a DOL element (a
function, class, or constant); and the right side is a
sequence of words and arguments. The grammar
rules are consumed by our parser for building
DOL trees from NL text.
So far there are 9,600 grammar rules in our sys-
tem. For every DOL node type, the lexicon and
grammar rules are constructed together in a semi-
automatic way. Math-related classes, functions,
and constants and their grammar rules are manu-
ally built by referring to text books and online tu-
</bodyText>
<page confidence="0.96922">
1136
</page>
<bodyText confidence="0.999924677419355">
torials. About 35 classes and 200 functions are ob-
tained in this way. Additional instances of each
element type are constructed in the ways below.
Classes: Additional classes and grammar rules
are obtained from two data sources: Freebase3
types, and automatically extracted lexical seman-
tic data. By treating Freebase types as DOL clas-
ses and the mapping from types to lexical names
as grammar rules, we get the first version of gram-
mar for classes. To improve coverage, we run a
term peer similarity and hypernym extraction al-
gorithm (Hearst, 1992; Shi et al., 2010; Zhang et
al., 2011) on a web snapshot of 3 billion pages,
and get a peer-similarity graph and a collection of
is-a pairs. An is-a pair example is (Megan Fox,
actress), where “Megan Fox” and “actress” are in-
stance and type names respectively. In our peer
similarity graph, “Megan Fox” and “Britney
Spears” have a high similarity score. The peer
similarity graph is used to clean the is-a data col-
lection (with the idea that peer terms often share
some common type names). Given the cleaned is-
a data, we sort the type names by weight and man-
ually create classes for top-1000 type names. For
example, create a class person.actress and add a
grammar rule “person.actress → actress”. For the
other 2000 type names in the top 3000, we create
classes and rules automatically, in the form of
“class.TN → TN”, where TN is a type name. For
example, create rule “class.succulent → succu-
lent” for name “succulent”.
</bodyText>
<equation confidence="0.992127947368421">
vf.be.equ($1,$2) → {$1} be equal to {$2}
 |{$1} equal {$2}
 |{$1} be {$2}
vf.give($1,$2,$3) → {$1} give {$2} to {$3}
 |{$1} give {$3} {$2}
nf.math.sum!1($1,$2) → {$1} plus {$2}
 |{$2} added to {$1}
nf.math.sum!2($1) → sum of {$1}
 |addition of {$1}
nf.math.power($1,$2)
→ {$1} raised to the {power|exponent} of {$2}
nf.list($1,$2) → {$2} {$1}
mf.number.even → even
mf.condition.if($1) → if {$1}
mf.approximately → approximately
 |roughly
education.university → university
math.number → number
math.integer → integer
</equation>
<tableCaption confidence="0.925359">
Table 2: Example grammar for connecting DOL
and NL
</tableCaption>
<footnote confidence="0.576962">
3 Freebase: http://www.freebase.com/
</footnote>
<bodyText confidence="0.999170066666667">
Functions: Additional noun functions are auto-
matically created from Freebase properties and at-
tribute extraction results (Pasca et al., 2006;
Durme et al., 2008), using a similar procedure
with creating classes from Freebase types and is-
a extraction results. We have over 50 manually
defined math-related verb functions. Our future
plan is automatically generating verb functions
from databases like PropBank (Kingsbury &amp;
Palmer, 2002), FrameNet (Fillmore et al., 2003),
and VerbNet4 (Schuler, 2005). Additional modi-
fier functions are automatically created from an
English adjective and adverb list, in the form of
“mf.adj.TN → TN” and “mf.adv.TN → TN”
where TN is the name of an adjective or adverb.
</bodyText>
<figureCaption confidence="0.941921">
Figure 3: The DOL semantic parse tree for “Nine
plus an integer is equal to 314”
Figure 4: A syntactic parse tree
3.2.2 Parsing
</figureCaption>
<bodyText confidence="0.9984756">
Parsing for CFG is a well-studied topic with lots
of algorithms invented (Kasami, 1965; Earley,
1970). The core idea behind almost all the algo-
rithms is exploiting dynamic programming to
achieve efficient search through the space of pos-
sible parse trees. For syntactic parsing, a well-
known serious problem is ambiguity: the appear-
ance of many syntactically correct but semanti-
cally unreasonable parse trees. Modern syntactic
parsers reply on statistical information to reduce
</bodyText>
<footnote confidence="0.5922265">
4 VerbNet: http://verbs.colorado.edu/~mpalmer/pro-
jects/verbnet.html
</footnote>
<figure confidence="0.999158366666667">
Nine
9 list
nf.math.sum
plus an integer is equal to 314
math.integer 1
vf.be.equ
314
Nine
CD
NP PP
NP
plus
IN
DT
an
NP
integer
S
NN
VB
is
equal
VP
JJ PP
ADJP
IN
to
314
NP
CD
</figure>
<page confidence="0.989776">
1137
</page>
<bodyText confidence="0.998079782608696">
ambiguity. They are often based on probabilistic
CFGs (PCFGs) or probabilistic lexicalized CFGs
trained on hand-labeled TreeBanks.
With the new set of DOL-NL grammar rules
(examples in Table 2) and the type-compatibility
property (Section 3.1.3), ambiguity can hopefully
be greatly reduced, because semantically unrea-
sonable parsing often results in invalid DOL trees.
We implement a top-down parser for our new
CFG of Section 3.2.1, following the Earley algo-
rithm (Earley, 1970). No probabilistic information
is attached in the grammar rules because no Tree-
banks are available for learning statistical proba-
bilities for the new CFG. Figure 3 shows the parse
tree returned by our parser when processing a sim-
ple sentence. The DOL tree can be obtained by re-
moving the dotted lines (corresponding to the
non-argument part in the right side of the grammar
rules). A traditional syntactic parse tree is shown
in Figure 4 for reference.
During parsing, a score is calculated for each
DOL node. The score of a tree T is the weighted
average of the scores of its sub-trees,
</bodyText>
<equation confidence="0.992675">
∑ 𝑳(𝑻𝒊) ∙ 𝑺(𝑻𝒊)
𝒌
𝒊=𝟏
𝑺(𝑻) = ∙ 𝒑(𝑻) (3)
∑ 𝑳(𝑻𝒊)
𝒌
𝒊=𝟏
</equation>
<bodyText confidence="0.999944777777778">
where 𝑇𝑖 is a sub-tree, and 𝐿(𝑇𝑖) is the number of
words to which the sub-tree corresponds in the
original text. If the type-compatibility property for
T is satisfied, 𝑝(𝑇)=1; otherwise 𝑝(𝑇)=0.
All leaf nodes are assigned a score of 1.0, ex-
cept for pure lexical string nodes (which are used
as named entity names). The score of a lexical
string node is set to 1/(1+𝜇n), where n is the num-
ber of words in the node, and 𝜇 (=0.2 in experi-
ments) is a parameter whose value does not have
much impact on parsing results. Such a score
function encourages interpreting a word sequence
with our grammar than treating it as an entity
name.
Among all candidate DOL trees yielded during
parsing, we return the one with the highest score
as the final parsing result. A null tree is returned
if the highest score is zero.
</bodyText>
<subsectionHeader confidence="0.995063">
3.3 Reasoning
</subsectionHeader>
<bodyText confidence="0.99886828125">
The reasoning module is responsible for deriving
math expressions from DOL trees and calculating
problem answers by solving equation systems.
Math expressions have different definitions in dif-
ferent contexts. In some definitions, equations and
inequations are excluded from math expressions.
In this paper, equations and inequations (like
“a=b” and “ax+b&gt;0”) are called s-expressions be-
cause they represent mathematical sentences,
while other math expressions (like “x+5”) are
named n-expressions since they are essentially
noun phrases. Our definition of “math expres-
sions” therefore includes both n-expressions and
s-expressions.
Different types of nodes may generate different
types of math expressions. In most cases, s-ex-
pressions are derived from verb function nodes
and modifier function nodes, while n-expressions
are generated from constants and noun function
nodes. For example, the s-expression “9+x=314”
can be derived from the DOL tree of Figure 3, if
variable x represents the integer. In the same Fig-
ure, The n-expression “9+x” is derived from the
left sub-tree.
The pseudo-codes of our math expression deri-
vation algorithm are shown in Figure 5. The algo-
rithm generates the math expression for a DOL
tree T by first calling the expression derivation
procedure of sub-trees, and then applying the se-
mantic interpretation of T. All the s-expressions
derived so far are stored in an expression list
named XL.
</bodyText>
<table confidence="0.8328515">
Algorithm MathExpDerivation
Input: DOL tree T
Output: Math expression X(T)
Global data structure: Expression list XL
</table>
<listItem confidence="0.869034571428571">
1: For each child Ci of T
2: X(Ci) = MathExpDerivation(Ci)
3: If X(Ci) is an s-expression
4: Add X(Ci) to XL
5: X(T) ← Applying the semantic interpretation
of T
6: Return X(T)
</listItem>
<figureCaption confidence="0.984296">
Figure 5: Math expression derivation algorithm
</figureCaption>
<equation confidence="0.999356">
vf.be.equ($1,$2) → X($1) = X($2) (1)
nf.math.sum!1($1,$2) → X($1) + X($2) (2)
nf.math.sum!2($1) → ∑𝐞∈$𝟏𝐗(𝐞) (3)
nf.math.gcd($1) → gcd({X(e)  |𝐞 ∈ $𝟏}) (4)
nf.list($1,$2) → V = (v1, v2..., vn), n=X($2) (5)
mf.number.even → X($↑) % 2 = 0 (6)
</equation>
<tableCaption confidence="0.988138">
Table 3: Example semantic interpretations
</tableCaption>
<bodyText confidence="0.999704888888889">
The semantic interpretation of DOL nodes
plays a critical role in the algorithm. Table 3
shows some example interpretations of some rep-
resentative DOL functions. In the table, $1, $2 etc.
are function arguments, and $↑ for a modifier
node denotes the node which the modifier modi-
fies. So far the semantic interpretations are built
manually. Please note that it is not necessary to
make semantic interpretations for every DOL
</bodyText>
<page confidence="0.985944">
1138
</page>
<bodyText confidence="0.999808">
node in solving number word problems. For ex-
ample, most class nodes and many adverb nodes
can have null interpretations at the moment.
</bodyText>
<sectionHeader confidence="0.999711" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999315">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999932863636364">
Datasets: Our problem collection5 contains 1,878
math number word problems, collected from two
web sites: algebra.com6 (a web site for users to
post math problems and get help from tutors) and
answers.yahoo.com7. Problems on both sites are
organized into categories. For algebra.com, prob-
lems are randomly sampled from the number
word problems category; for answers.yahoo.com,
we first randomly sample an initial set of prob-
lems from the math category and then ask human
annotators to manually choose number word
problems from them. Math equations8 and an-
swers to the problems are manually added by hu-
man annotators.
We randomly split the dataset into a dev set (for
algorithm design and debugging) and a test set.
More subsets are extracted to meet the require-
ments of the baseline methods (see below). Table
4 shows the statistics of the datasets.
Baseline methods: We compare our approach
with two baselines: KAZB (Kushman et al., 2014)
and BasicSim.
KAZB is a learning-based statistical method
which solves a problem by mapping it to one of
the equation templates determined by the anno-
tated equations in the training data. We run the
ALLEQ version of their algorithm since it per-
forms much better than the other two (i.e., 5EQ
and 5EQ+ANS). Their codes support only linear
equations and require that there are at least two
problems for each equation template (otherwise
an exception will be thrown). By choosing prob-
lems from the collection that meet these require-
ments, we build a sub-dataset called LinearT2. In
the dataset of KAZB, each equation template cor-
responds to at least 6 problems. So we form an-
other sub-dataset called LinearT6 by removing
from the test set the problems for which the asso-
ciated equation template appears less than 6 times.
BasicSim is a simple statistical method which
works by computing the similarities between a
testing problem and those in the training set, and
then applying the equations of the most similar
problem. This method has similar performance
</bodyText>
<footnote confidence="0.996734666666667">
5 Available from http://research.microsoft.com/en-us/pro-
jects/dolphin/
6 http://www.algebra.com
</footnote>
<bodyText confidence="0.986831166666667">
with KAZB on their dataset, but does not have the
two limitations mentioned above. Therefore we
adopt it as the second baseline.
For both baselines, experiments are conducted
using 5-fold cross-validation with the dev set al-
ways included in the training data. In other words,
we always use the dev set and 4/5 of the test set as
training data for each fold.
Evaluation metrics: Evaluation is performed in
the setting that a system can choose NOT to an-
swer all problems in the test set. In other words,
one has the flexibility of generating answers only
when she knows how to solve it or she is confident
about her answer. In this setting, the following
three metrics are adopted in reporting evaluation
results (assuming, in a test set of size n, a system
generates answers for m problems, where k of
them are correct):
</bodyText>
<table confidence="0.976651692307692">
Precision: k/m
Recall (or coverage): k/n
F1: 2PR/(P+R) = 2k/(m+n)
Dataset #problems #sentences #words
(average) (average)
All dev 374 1.79 20.3
test 1,504 1.75 22.5
Linear dev 247 1.78 19.6
test 986 1.72 19.0
LinearT2 dev 172 1.85 18.8
test 669 1.71 17.4
LinearT6 dev 71 1.96 16.8
test 348 1.80 16.1
</table>
<tableCaption confidence="0.832947666666667">
Table 4: Dataset statistics (Linear: problems with
linear equations; T2: problems corresponding to
template size ≥ 2)
</tableCaption>
<subsectionHeader confidence="0.945579">
4.2 Experimental results
</subsectionHeader>
<bodyText confidence="0.999746230769231">
The Overall evaluation results are summarized in
Table 5, where “Dolphin” represents our ap-
proach. The results show that our approach signif-
icantly outperforms (with p&lt;&lt;0.01 according to
two-tailed t-test) the two baselines on every test
set, in terms of precision, recall, and F-measure.
Our approach achieves a particularly high preci-
sion of 95%. That means once an answer is pro-
vided by our approach, it has a very high proba-
bility of being correct.
Please note that our grammar rules and parsing
algorithm are NOT tuned for the evaluation data.
Only the dev set is referred to in system building.
</bodyText>
<footnote confidence="0.858794">
7 https://answers.yahoo.com/
8 Math equations are used in the baseline approaches as part
of training data.
</footnote>
<page confidence="0.996117">
1139
</page>
<bodyText confidence="0.996616166666667">
Since the baselines generate results for all prob-
lems, the precision, recall, and F1 are all the same
for each dataset.
grammar to improve the coverage of properties,
relations, and actions. We also plan to study the
mechanism of modeling relations and actions.
</bodyText>
<table confidence="0.999618">
Dataset Method Precision Recall F1
(%) (%) (%)
LinearT6 KAZB 49.1 49.1 49.1
BasicSim 59.7 59.7 59.7
Dolphin 98.1 72.9 83.6
LinearT2 KAZB 37.5 37.5 37.5
BasicSim 46.3 46.3 46.3
Dolphin 97.3 68.0 80.0
Linear BasicSim 32.3 32.3 32.3
Dolphin 95.7 63.6 76.4
Test set BasicSim 29.0 29.0 29.0
all
Dolphin 95.4 60.2 73.8
</table>
<tableCaption confidence="0.999559">
Table 5: Evaluation results
</tableCaption>
<bodyText confidence="0.999973447368421">
The reason for such a high precision is that, by
transforming NL text to DOL trees, the system
“understands” the problem (or has structured and
accurate information about quantity relations).
Therefore it is more likely to generate correct re-
sults than statistical methods who simply “guess”
according to features. By examining the problems
in the dev set that we cannot generate answers, we
find that most of them are due to empty parsing
results.
On the other hand, statistical approaches have
the advantage of generating answers without un-
derstanding the semantic meaning of problems (as
long as there are similar problems in the training
data). So they are able to handle (with probably
low precision) problems that are complex in terms
of language and logic.
Please pay attention that our experimental re-
sults reported here are on number word problems.
General math word problems are much harder to
our approach because the entity types, properties,
relations, and actions contained in general word
problems are much larger in quantity and more
complex in quality. We are working on extending
our approach to general math word problems.
Now our DOL language and CFG grammar al-
ready have a good coverage on common entity
types, but the coverage on properties, relations,
and actions is quite limited. As a result, our parser
fails to parse many sentences in general math
word problems because they contain properties,
relations or actions that are unknown to our sys-
tem. We also observe that sometimes we are able
to parse a problem successfully, but cannot derive
math expressions in the reasoning stage. This is
often because some relations or actions in the
problem are not modeled appropriately. As future
work, we plan to extend our DOL lexicon and
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991285714286">
We proposed a semantic parsing and reasoning
approach to automatically solve math number
word problems. We have designed a new meaning
representation language DOL to bridge NL text
and math expressions. A CFG parser is imple-
mented to parse NL text to DOL trees. A reason-
ing module is implemented to derive math expres-
sions from DOL trees, by applying the semantic
interpretation of DOL nodes. We achieve a high
precision and a reasonable recall on our test set of
over 1,500 problems. We hope to extend our tech-
niques to handling general math word problems
and to other domains (like physics and chemistry)
in the future.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999216">
We would like to thank the annotators for their ef-
forts in assigning math equations and answers to
the problems in our dataset. Thanks to the anony-
mous reviewers for their helpful comments and
suggestions.
</bodyText>
<sectionHeader confidence="0.950462" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.923585739130435">
J.W. Backus. 1959. The syntax and semantics of the
proposed international algebraic language of the
Zurich ACM-GAMM conference. Proceedings of
the International Conference on Information Pro-
cessing, 1959.
Y. Bakman. 2007. Robust understanding of word prob-
lems with extraneous information. http://arxiv.org/
abs/math/0701393. Accessed Feb. 2nd, 2015.
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-
2007 Task 19: Frame semantic structure extraction.
In Proceedings of SemEval.
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K.
Griffitt, U. Hermjakob, K. Knight, P. Koehn, M.
Palmer, and N. Schneider. 2013. Abstract meaning
representation for sembanking. In Proc. of the Lin-
guistic Annotation Workshop and Interoperability
with Discourse.
J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Se-
mantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).
J. Berant and P. Liang. 2014. Semantic Parsing via Par-
aphrasing. In ACL&apos;2014.
</reference>
<page confidence="0.910775">
1140
</page>
<reference confidence="0.999720456310679">
D.G. Bobrow. 1964a. Natural language input for a
computer problem solving system. Report MAC-
TR-1, Project MAC, MIT, Cambridge, June
D.G. Bobrow. 1964b. Natural language input for a
computer problem solving system. Ph.D. Thesis,
Department of Mathematics, MIT, Cambridge
D.L. Briars, J.H. Larkin. 1984. An integrated model of
skill in solving elementary word problems. Cogni-
tion and Instruction, 1984, 1 (3) 245-296.
Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).
X. Carreras. and L. Marquez. 2004. Introduction to the
CoNLL-2004 shared task: Semantic role labeling. In
Proceedings of CoNLL.
E. Charniak. 1968. CARPS: a program which solves
calculus word problems. Report MAC-TR-51, Pro-
ject MAC, MIT, Cambridge, July
E. Charniak. 1969. Computer solution of calculus word
problems. In Proceedings of international joint con-
ference on artificial intelligence. Washington, DC,
pp 303–316
N. Chomsky. 1956. Three models for the description of
language. Information Theory, IRE Transactions on,
2(3), 113-124.
S. Clark, and J. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4):493-552.
D. Das, D. Chen, A.F.T. Martins, N. Schneider and
N.A. Smith. 2014. Frame-Semantic Parsing. Com-
putational Linguistics 40:1, pages 9-56
D. Dellarosa. 1986. A computer simulation of chil-
dren’s arithmetic word problem solving. Behavior
Research Methods, Instruments, &amp; Computers,
18:147–154
V. Durme, T. Qian, and L. Schubert. 2008. Class-
driven attribute extraction. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pp. 921-928. Association for
Computational Linguistics, 2008.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2), 94-102.
C.J. Fillmore, C.R. Johnson, and M.R. Petruck. 2003.
Background to FrameNet. International Journal of
Lexicography, 16(3).
C.R. Fletcher. 1985. Understanding and solving arith-
metic word problems: a computer simulation. Be-
havior Research Methods, Instruments, &amp; Comput-
ers, 17:565–571
D. Gildea, and D. Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3).
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Fourteenth Interna-
tional Conference on Computational Linguistics,
Nantes, France.
M.J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kush-
man. 2014. Learning to Solve Arithmetic Word
Problems with Verb Categorization. In
EMNLP’2014.
D. Jurafsky, and J.H. Martin. 2000. Speech &amp; language
processing. Pearson Education India.
T. Kasami. 1965. An efficient recognition and syntax-
analysis algorithm for context-free languages
(Technical report). AFCRL. 65-758.
P. Kingsbury, and M. Palmer. 2002. From TreeBank to
PropBank. In Proceedings of LREC.
N. Kushman, Y. Artzi, L. Zettlemoyer, and R. Barzi-
lay. 2014. Learning to automatically solve algebra
word problems. In Proc. of the Annual Meeting of
the Association for Computational Linguistics
(ACL).
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).
I. Lev, B. MacCartney, C. Manning, and R. Levy.
2004. Solving logic puzzles: From robust pro-
cessing to precise semantics. In Proceedings of the
Workshop on Text Meaning and Interpretation. As-
sociation for Computational Linguistics.
C. Liguda, T. Pfeiffer. 2012. Modeling Math Word
Problems with Augmented Semantic Networks.
NLDB’2012, pp. 247-252.
Y. Ma, Y. Zhou, G. Cui, R. Yun, R. Huang. 2010.
Frame-based calculus of solving arithmetic multi-
step addition and subtraction word problems. In In-
ternational Workshop on Education Technology and
Computer Science, vol. 2, pp. 476–479.
L. Marquez, X. Carreras, K.C. Litkowski, and S. Ste-
venson. 2008. Semantic role labeling: an introduc-
tion to the special issue. Computational Linguistics,
34(2).
A. Mukherjee and U. Garain. 2008. A review of meth-
ods for automatic understanding of natural language
mathematical problems. Artificial Intelligence Re-
view, 29(2).
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Organizing and searching the world wide web
of facts-step one: the one-million fact extraction
challenge. In AAAI (Vol. 6, pp. 1400-1405).
K.K. Schuler. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Dissertation. http://reposi-
tory.upenn.edu/dissertations/AAI3179808
</reference>
<page confidence="0.850169">
1141
</page>
<reference confidence="0.999801580645161">
S. Shi, H. Zhang, X. Yuan, and J.-R. Wen. 2010. Cor-
pus-based semantic class mining: distributional vs.
pattern-based approaches. In Proceedings of the
23rd International Conference on Computational
Linguistics, pages 993–1001. Association for Com-
putational Linguistics.
M. Steedman. 2000. The Syntactic Process. The MIT
Press.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with
lambda calculus. In Association for Computational
Linguistics (ACL), pages 960–967.
M. Zelle and R.J. Mooney. 1996. Learning to parse da-
tabase queries using inductive logic proramming. In
Association for the Advancement of Artificial Intel-
ligence (AAAI), pages 1050–1055.
L.S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. In
Uncertainty in Artificial Intelligence (UAI), pages
658–666.
L.S. Zettlemoyer and M. Collins. 2007. Online Learn-
ing of Relaxed CCG Grammars for Parsing to Log-
ical Form. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning (EMNLP-CoNLL).
F. Zhang, S. Shi, J. Liu, S. Sun, and C.-Y. Lin. 2011.
Nonlinear evidence fusion and propagation for hyp-
onymy relation mining. In ACL, volume 11, pages
1159–1168.
</reference>
<page confidence="0.995381">
1142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.224986">
<title confidence="0.9903385">Automatically Solving Number Word by Semantic Parsing and Reasoning</title>
<author confidence="0.934429">Yuehui Chin-Yew Xiaojiang</author>
<author confidence="0.934429">Yong</author>
<affiliation confidence="0.689478">Research</affiliation>
<email confidence="0.886786">shumings@microsoft.com</email>
<email confidence="0.886786">cyl@microsoft.com</email>
<email confidence="0.886786">xiaojl@microsoft.com</email>
<email confidence="0.886786">yongrui@microsoft.com</email>
<note confidence="0.431873">of Science and Technology of wyh9346@mail.ustc.edu.cn</note>
<abstract confidence="0.990592769230769">This paper presents a semantic parsing and reasoning approach to automatically solving math word problems. A new meaning representation language is designed to bridge natural language text and math expressions. A CFG parser is implemented based on 9,600 semi-automatically created grammar rules. We conduct experiments on a test set of over 1,500 number word problems (i.e., verbally expressed number problems) and yield 95.4% precision and 60.2% recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J W Backus</author>
</authors>
<title>The syntax and semantics of the proposed international algebraic language of the Zurich</title>
<date>1959</date>
<booktitle>ACM-GAMM conference. Proceedings of the International Conference on Information Processing,</booktitle>
<contexts>
<context position="20209" citStr="Backus, 1959" startWordPosition="3221" endWordPosition="3222">nt to the reasoning stage. To enable this mechanism, the meaning representation language should support a lazy variable ID assignment and keep as much information (e.g., determiners, plurals, modifiers) from the noun phrases as possible. DOL is a language that always keeps the structure information of phrases, whether or not it has been assigned a variable ID. In summary, compared with other languages, DOL has some unique features which make it more suitable for our math problem solving scenario. 3.2 Semantic Parsing Our parsing algorithm is based on context-free grammar (CFG) (Chomsky, 1956; Backus, 1959; Jurafsky &amp; Martin, 2000), a commonly used mathematical system for modeling constituent structure in natural languages. 3.2.1 CFG for connecting DOL and IL The core part of a CFG is the set of grammar rules. Example English grammar rules for building syntactic parsers include “S → NP VP”, “NP → CD |DT NN |NP PP”, etc. Table 2 shows some example CFG rules in our system for mapping DOL nodes to natural language word sequences. The left side of each rule is a DOL element (a function, class, or constant); and the right side is a sequence of words and arguments. The grammar rules are consumed by o</context>
</contexts>
<marker>Backus, 1959</marker>
<rawString>J.W. Backus. 1959. The syntax and semantics of the proposed international algebraic language of the Zurich ACM-GAMM conference. Proceedings of the International Conference on Information Processing, 1959.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bakman</author>
</authors>
<title>Robust understanding of word problems with extraneous information. http://arxiv.org/ abs/math/0701393. Accessed</title>
<date>2007</date>
<contexts>
<context position="1343" citStr="Bakman, 2007" startWordPosition="195" endWordPosition="196">ision and 60.2% recall. 1 Introduction Computers, since their creation, have exceeded human beings in (speed and accuracy of) mathematical calculation. However, it is still a big challenge nowadays to design algorithms to automatically solve even primary-school-level math word problems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda &amp; Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). * Work done while this author was an intern at Microsoft Research Most (if not all) previous symbolic approaches suffer from two major shortcomings. First, natural language (NL) sentences are processed by simply applying pattern matching and/or transformation rules in an ad-hoc manner (refer to the related w</context>
<context position="5809" citStr="Bakman, 2007" startWordPosition="910" endWordPosition="911">ernel sentences using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com No empirical evaluation results are reported in most of the above work. Almost all of these approaches parse NL text by simply applying pattern matching rules in an ad-hoc manner. For example, as mentioned in Bobrow (1964b), due to the pattern “($, AND $)”, the system would incorrectly divide “Tom has 2 apples, 3 bananas, and 4 pears.” into two “sentences”: “Tom has 2 apples, 3 bananas.” and “4 pears.” WolframAlpha1 shows some examples2 of automatically s</context>
</contexts>
<marker>Bakman, 2007</marker>
<rawString>Y. Bakman. 2007. Robust understanding of word problems with extraneous information. http://arxiv.org/ abs/math/0701393. Accessed Feb. 2nd, 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>M Ellsworth</author>
<author>K Erk</author>
</authors>
<title>SemEval2007 Task 19: Frame semantic structure extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="7491" citStr="Baker et al., 2007" startWordPosition="1194" endWordPosition="1197"> single step or multi-step homogenous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) can solve a wide range of word problems, given that the equation systems and solutions are attached to problems in the training set. The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to com</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval2007 Task 19: Frame semantic structure extraction. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Banarescu</author>
<author>C Bonial</author>
<author>S Cai</author>
<author>M Georgescu</author>
<author>K Griffitt</author>
<author>U Hermjakob</author>
<author>K Knight</author>
<author>P Koehn</author>
<author>M Palmer</author>
<author>N Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proc. of the Linguistic Annotation Workshop and Interoperability with Discourse.</booktitle>
<contexts>
<context position="8205" citStr="Banarescu et al., 2013" startWordPosition="1306" endWordPosition="1309">w semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first problem in Figure 1 (written below for convenience), One number is 16 more than another. If the smaller number is subtracted from 2/3 of the larger, the result is 1/4 of the sum of the two numbers. Find the numbers. To automatically solve this problem, th</context>
<context position="17417" citStr="Banarescu et al., 2013" startWordPosition="2760" endWordPosition="2763">nd text representation (especially math text representation). For example, the two variants of nf.math.sum (refer to Section 3.1.1 for their specifications) are enough to represent the following English phrases: 3 plus 5 4 nf.math.sum!1(3, 5) sum of 3, 5, 7, and 9 4 nf.math.sum!2(nf.list(3, 5, 7, 9)) sum of ten thousand numbers 4 nf.math.sum!2(nf.list(math.number,10000)) Without t.list or nf.list, we would have to define a lot of overloaded functions for nf.math.sum to deal with different numbers of addends. 3.1.4 Comparing with other languages Among all meaning representation languages, AMR (Banarescu et al., 2013) is most similar to DOL. Their major differences are: First, they use very different mechanisms to represent noun phrases. In AMR, a sentence (e.g., “the boy destroyed the room”) and a noun phrase (e.g., “the boy’s destruction of the room”) can have the same representation. While in DOL, a sentence is always represented by a verb function; and a noun phrase is always a noun function or a constant. Second, DOL has a larger type system and is stricter in type compatibility checking. Third, DOL has template classes and built-in data structures like t.list and nf.list to facilitate the representat</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and N. Schneider. 2013. Abstract meaning representation for sembanking. In Proc. of the Linguistic Annotation Workshop and Interoperability with Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>A Chou</author>
<author>R Frostig</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="7923" citStr="Berant et al., 2013" startWordPosition="1263" endWordPosition="1266">ing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 A</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>P Liang</author>
</authors>
<title>Semantic Parsing via Paraphrasing. In</title>
<date>2014</date>
<booktitle>ACL&apos;2014.</booktitle>
<contexts>
<context position="7972" citStr="Berant &amp; Liang, 2014" startWordPosition="1271" endWordPosition="1274">mantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first problem in Figure 1 (w</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>J. Berant and P. Liang. 2014. Semantic Parsing via Paraphrasing. In ACL&apos;2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Bobrow</author>
</authors>
<title>Natural language input for a computer problem solving system.</title>
<date>1964</date>
<tech>Report MACTR-1,</tech>
<location>Project MAC, MIT, Cambridge,</location>
<contexts>
<context position="1157" citStr="Bobrow, 1964" startWordPosition="168" endWordPosition="169">on 9,600 semi-automatically created grammar rules. We conduct experiments on a test set of over 1,500 number word problems (i.e., verbally expressed number problems) and yield 95.4% precision and 60.2% recall. 1 Introduction Computers, since their creation, have exceeded human beings in (speed and accuracy of) mathematical calculation. However, it is still a big challenge nowadays to design algorithms to automatically solve even primary-school-level math word problems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda &amp; Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). * Work done while this author was an intern at Microsoft Research Most (if not all) previous symbolic approaches suffer fro</context>
<context position="5123" citStr="Bobrow, 1964" startWordPosition="806" endWordPosition="807">. We hope to extend our techniques to handle general math word problems in the future. We build a test set of over 1,500 problems and make a quantitative comparison with state-of-theart statistical methods. Evaluation results show that our approach significantly outperforms baseline methods on our test set. Our system yields an extremely high precision of 95.4% and a reasonable recall of 60.2%, which shows promising application of our system in precision-critical situations. 2 Related Work 2.1 Math word problem solving Most previous work on automatic word problem solving is symbolic. STUDENT (Bobrow, 1964a, b) handles algebraic problems by first transforming NL sentences into kernel sentences using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most i</context>
</contexts>
<marker>Bobrow, 1964</marker>
<rawString>D.G. Bobrow. 1964a. Natural language input for a computer problem solving system. Report MACTR-1, Project MAC, MIT, Cambridge, June</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Bobrow</author>
</authors>
<title>Natural language input for a computer problem solving system.</title>
<date>1964</date>
<tech>Ph.D. Thesis,</tech>
<institution>Department of Mathematics, MIT, Cambridge</institution>
<contexts>
<context position="1157" citStr="Bobrow, 1964" startWordPosition="168" endWordPosition="169">on 9,600 semi-automatically created grammar rules. We conduct experiments on a test set of over 1,500 number word problems (i.e., verbally expressed number problems) and yield 95.4% precision and 60.2% recall. 1 Introduction Computers, since their creation, have exceeded human beings in (speed and accuracy of) mathematical calculation. However, it is still a big challenge nowadays to design algorithms to automatically solve even primary-school-level math word problems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda &amp; Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). * Work done while this author was an intern at Microsoft Research Most (if not all) previous symbolic approaches suffer fro</context>
<context position="5123" citStr="Bobrow, 1964" startWordPosition="806" endWordPosition="807">. We hope to extend our techniques to handle general math word problems in the future. We build a test set of over 1,500 problems and make a quantitative comparison with state-of-theart statistical methods. Evaluation results show that our approach significantly outperforms baseline methods on our test set. Our system yields an extremely high precision of 95.4% and a reasonable recall of 60.2%, which shows promising application of our system in precision-critical situations. 2 Related Work 2.1 Math word problem solving Most previous work on automatic word problem solving is symbolic. STUDENT (Bobrow, 1964a, b) handles algebraic problems by first transforming NL sentences into kernel sentences using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most i</context>
</contexts>
<marker>Bobrow, 1964</marker>
<rawString>D.G. Bobrow. 1964b. Natural language input for a computer problem solving system. Ph.D. Thesis, Department of Mathematics, MIT, Cambridge</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Briars</author>
<author>J H Larkin</author>
</authors>
<title>An integrated model of skill in solving elementary word problems. Cognition and Instruction,</title>
<date>1984</date>
<volume>1</volume>
<issue>3</issue>
<pages>245--296</pages>
<contexts>
<context position="5762" citStr="Briars &amp; Larkin, 1984" startWordPosition="901" endWordPosition="905">braic problems by first transforming NL sentences into kernel sentences using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com No empirical evaluation results are reported in most of the above work. Almost all of these approaches parse NL text by simply applying pattern matching rules in an ad-hoc manner. For example, as mentioned in Bobrow (1964b), due to the pattern “($, AND $)”, the system would incorrectly divide “Tom has 2 apples, 3 bananas, and 4 pears.” into two “sentences”: “Tom has 2 apples, 3 bananas.” and “4 pears.” Wolfra</context>
</contexts>
<marker>Briars, Larkin, 1984</marker>
<rawString>D.L. Briars, J.H. Larkin. 1984. An integrated model of skill in solving elementary word problems. Cognition and Instruction, 1984, 1 (3) 245-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Cai</author>
<author>A Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7902" citStr="Cai &amp; Yates, 2013" startWordPosition="1259" endWordPosition="1262">much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Q. Cai and A. Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Marquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="7449" citStr="Marquez, 2004" startWordPosition="1188" endWordPosition="1189"> papers: Hosseini et al. (2014) solve single step or multi-step homogenous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) can solve a wide range of word problems, given that the equation systems and solutions are attached to problems in the training set. The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. S</context>
</contexts>
<marker>Marquez, 2004</marker>
<rawString>X. Carreras. and L. Marquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>CARPS: a program which solves calculus word problems.</title>
<date>1968</date>
<tech>Report MAC-TR-51, Project</tech>
<contexts>
<context position="1329" citStr="Charniak, 1968" startWordPosition="193" endWordPosition="194">yield 95.4% precision and 60.2% recall. 1 Introduction Computers, since their creation, have exceeded human beings in (speed and accuracy of) mathematical calculation. However, it is still a big challenge nowadays to design algorithms to automatically solve even primary-school-level math word problems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda &amp; Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). * Work done while this author was an intern at Microsoft Research Most (if not all) previous symbolic approaches suffer from two major shortcomings. First, natural language (NL) sentences are processed by simply applying pattern matching and/or transformation rules in an ad-hoc manner (refer to</context>
<context position="5380" citStr="Charniak, 1968" startWordPosition="845" endWordPosition="846">icantly outperforms baseline methods on our test set. Our system yields an extremely high precision of 95.4% and a reasonable recall of 60.2%, which shows promising application of our system in precision-critical situations. 2 Related Work 2.1 Math word problem solving Most previous work on automatic word problem solving is symbolic. STUDENT (Bobrow, 1964a, b) handles algebraic problems by first transforming NL sentences into kernel sentences using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com No empirical evaluation result</context>
</contexts>
<marker>Charniak, 1968</marker>
<rawString>E. Charniak. 1968. CARPS: a program which solves calculus word problems. Report MAC-TR-51, Project MAC, MIT, Cambridge, July</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Computer solution of calculus word problems.</title>
<date>1969</date>
<booktitle>In Proceedings of international joint conference on artificial intelligence.</booktitle>
<pages>303--316</pages>
<location>Washington, DC,</location>
<marker>Charniak, 1969</marker>
<rawString>E. Charniak. 1969. Computer solution of calculus word problems. In Proceedings of international joint conference on artificial intelligence. Washington, DC, pp 303–316</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Three models for the description of language.</title>
<date>1956</date>
<journal>Information Theory, IRE Transactions on,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>113--124</pages>
<contexts>
<context position="20195" citStr="Chomsky, 1956" startWordPosition="3219" endWordPosition="3220">ble-ID assignment to the reasoning stage. To enable this mechanism, the meaning representation language should support a lazy variable ID assignment and keep as much information (e.g., determiners, plurals, modifiers) from the noun phrases as possible. DOL is a language that always keeps the structure information of phrases, whether or not it has been assigned a variable ID. In summary, compared with other languages, DOL has some unique features which make it more suitable for our math problem solving scenario. 3.2 Semantic Parsing Our parsing algorithm is based on context-free grammar (CFG) (Chomsky, 1956; Backus, 1959; Jurafsky &amp; Martin, 2000), a commonly used mathematical system for modeling constituent structure in natural languages. 3.2.1 CFG for connecting DOL and IL The core part of a CFG is the set of grammar rules. Example English grammar rules for building syntactic parsers include “S → NP VP”, “NP → CD |DT NN |NP PP”, etc. Table 2 shows some example CFG rules in our system for mapping DOL nodes to natural language word sequences. The left side of each rule is a DOL element (a function, class, or constant); and the right side is a sequence of words and arguments. The grammar rules are</context>
</contexts>
<marker>Chomsky, 1956</marker>
<rawString>N. Chomsky. 1956. Three models for the description of language. Information Theory, IRE Transactions on, 2(3), 113-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--4</pages>
<marker>Clark, Curran, 2007</marker>
<rawString>S. Clark, and J. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493-552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>D Chen</author>
<author>A F T Martins</author>
<author>N Schneider</author>
<author>N A Smith</author>
</authors>
<date>2014</date>
<booktitle>Frame-Semantic Parsing. Computational Linguistics 40:1,</booktitle>
<pages>9--56</pages>
<contexts>
<context position="7510" citStr="Das et al., 2014" startWordPosition="1198" endWordPosition="1201">i-step homogenous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) can solve a wide range of word problems, given that the equation systems and solutions are attached to problems in the training set. The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial</context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>D. Das, D. Chen, A.F.T. Martins, N. Schneider and N.A. Smith. 2014. Frame-Semantic Parsing. Computational Linguistics 40:1, pages 9-56</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dellarosa</author>
</authors>
<title>A computer simulation of children’s arithmetic word problem solving.</title>
<date>1986</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<pages>18--147</pages>
<contexts>
<context position="5795" citStr="Dellarosa, 1986" startWordPosition="908" endWordPosition="909"> sentences into kernel sentences using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com No empirical evaluation results are reported in most of the above work. Almost all of these approaches parse NL text by simply applying pattern matching rules in an ad-hoc manner. For example, as mentioned in Bobrow (1964b), due to the pattern “($, AND $)”, the system would incorrectly divide “Tom has 2 apples, 3 bananas, and 4 pears.” into two “sentences”: “Tom has 2 apples, 3 bananas.” and “4 pears.” WolframAlpha1 shows some examples2 of a</context>
</contexts>
<marker>Dellarosa, 1986</marker>
<rawString>D. Dellarosa. 1986. A computer simulation of children’s arithmetic word problem solving. Behavior Research Methods, Instruments, &amp; Computers, 18:147–154</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Durme</author>
<author>T Qian</author>
<author>L Schubert</author>
</authors>
<title>Classdriven attribute extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>921--928</pages>
<contexts>
<context position="23377" citStr="Durme et al., 2008" startWordPosition="3751" endWordPosition="3754">ath.sum!1($1,$2) → {$1} plus {$2} |{$2} added to {$1} nf.math.sum!2($1) → sum of {$1} |addition of {$1} nf.math.power($1,$2) → {$1} raised to the {power|exponent} of {$2} nf.list($1,$2) → {$2} {$1} mf.number.even → even mf.condition.if($1) → if {$1} mf.approximately → approximately |roughly education.university → university math.number → number math.integer → integer Table 2: Example grammar for connecting DOL and NL 3 Freebase: http://www.freebase.com/ Functions: Additional noun functions are automatically created from Freebase properties and attribute extraction results (Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury &amp; Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. Figure 3: The DOL semantic parse tree for “Nine plus an inte</context>
</contexts>
<marker>Durme, Qian, Schubert, 2008</marker>
<rawString>V. Durme, T. Qian, and L. Schubert. 2008. Classdriven attribute extraction. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pp. 921-928. Association for Computational Linguistics, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>94--102</pages>
<contexts>
<context position="24146" citStr="Earley, 1970" startWordPosition="3877" endWordPosition="3878">ns. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury &amp; Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. Figure 3: The DOL semantic parse tree for “Nine plus an integer is equal to 314” Figure 4: A syntactic parse tree 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possible parse trees. For syntactic parsing, a wellknown serious problem is ambiguity: the appearance of many syntactically correct but semantically unreasonable parse trees. Modern syntactic parsers reply on statistical information to reduce 4 VerbNet: http://verbs.colorado.edu/~mpalmer/projects/verbnet.html Nine 9 list nf.math.sum plus an integer is equal to 314 math.integer 1 vf.be.equ 314 Nine CD NP PP NP plus IN DT an NP integer S NN VB is equal VP JJ PP ADJP</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2), 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C R Johnson</author>
<author>M R Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="23685" citStr="Fillmore et al., 2003" startWordPosition="3796" endWordPosition="3799">versity → university math.number → number math.integer → integer Table 2: Example grammar for connecting DOL and NL 3 Freebase: http://www.freebase.com/ Functions: Additional noun functions are automatically created from Freebase properties and attribute extraction results (Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury &amp; Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. Figure 3: The DOL semantic parse tree for “Nine plus an integer is equal to 314” Figure 4: A syntactic parse tree 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possibl</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>C.J. Fillmore, C.R. Johnson, and M.R. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Fletcher</author>
</authors>
<title>Understanding and solving arithmetic word problems: a computer simulation.</title>
<date>1985</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<pages>17--565</pages>
<contexts>
<context position="5778" citStr="Fletcher, 1985" startWordPosition="906" endWordPosition="907"> transforming NL sentences into kernel sentences using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com No empirical evaluation results are reported in most of the above work. Almost all of these approaches parse NL text by simply applying pattern matching rules in an ad-hoc manner. For example, as mentioned in Bobrow (1964b), due to the pattern “($, AND $)”, the system would incorrectly divide “Tom has 2 apples, 3 bananas, and 4 pears.” into two “sentences”: “Tom has 2 apples, 3 bananas.” and “4 pears.” WolframAlpha1 shows so</context>
</contexts>
<marker>Fletcher, 1985</marker>
<rawString>C.R. Fletcher. 1985. Understanding and solving arithmetic word problems: a computer simulation. Behavior Research Methods, Instruments, &amp; Computers, 17:565–571</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="7423" citStr="Gildea &amp; Jurafsky, 2002" startWordPosition="1182" endWordPosition="1185">s have been proposed recently in two papers: Hosseini et al. (2014) solve single step or multi-step homogenous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) can solve a wide range of word problems, given that the equation systems and solutions are attached to problems in the training set. The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or s</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea, and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>In Fourteenth International Conference on Computational Linguistics,</booktitle>
<location>Nantes, France.</location>
<contexts>
<context position="21695" citStr="Hearst, 1992" startWordPosition="3480" endWordPosition="3481">are manually built by referring to text books and online tu1136 torials. About 35 classes and 200 functions are obtained in this way. Additional instances of each element type are constructed in the ways below. Classes: Additional classes and grammar rules are obtained from two data sources: Freebase3 types, and automatically extracted lexical semantic data. By treating Freebase types as DOL classes and the mapping from types to lexical names as grammar rules, we get the first version of grammar for classes. To improve coverage, we run a term peer similarity and hypernym extraction algorithm (Hearst, 1992; Shi et al., 2010; Zhang et al., 2011) on a web snapshot of 3 billion pages, and get a peer-similarity graph and a collection of is-a pairs. An is-a pair example is (Megan Fox, actress), where “Megan Fox” and “actress” are instance and type names respectively. In our peer similarity graph, “Megan Fox” and “Britney Spears” have a high similarity score. The peer similarity graph is used to clean the is-a data collection (with the idea that peer terms often share some common type names). Given the cleaned isa data, we sort the type names by weight and manually create classes for top-1000 type na</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In Fourteenth International Conference on Computational Linguistics, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Hosseini</author>
<author>H Hajishirzi</author>
<author>O Etzioni</author>
<author>N Kushman</author>
</authors>
<title>Learning to Solve Arithmetic Word Problems with Verb Categorization.</title>
<date>2014</date>
<booktitle>In EMNLP’2014.</booktitle>
<contexts>
<context position="1632" citStr="Hosseini et al., 2014" startWordPosition="236" endWordPosition="239">roblems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda &amp; Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). * Work done while this author was an intern at Microsoft Research Most (if not all) previous symbolic approaches suffer from two major shortcomings. First, natural language (NL) sentences are processed by simply applying pattern matching and/or transformation rules in an ad-hoc manner (refer to the related work section for more details). Second, surprisingly, they seldom report evaluation results about the effectiveness of the methods (except for some examples for demonstration purposes). For the small percentage of work with evaluation results available, it is unclear whether the patterns a</context>
<context position="6867" citStr="Hosseini et al. (2014)" startWordPosition="1087" endWordPosition="1090">Tom has 2 apples, 3 bananas, and 4 pears.” into two “sentences”: “Tom has 2 apples, 3 bananas.” and “4 pears.” WolframAlpha1 shows some examples2 of automatically solving elementary math word problems, with technique details unknown to the general public. Other examples on the web site demonstrate a large coverage of short phrase queries on math and other domains. By randomly selecting problems from our dataset and manually testing on their web site, we find that it fails to handle most problems in our problem collection. Statistical learning methods have been proposed recently in two papers: Hosseini et al. (2014) solve single step or multi-step homogenous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) can solve a wide range of word problems, given that the equation systems and solutions are attached to problems in the training set. The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., </context>
</contexts>
<marker>Hosseini, Hajishirzi, Etzioni, Kushman, 2014</marker>
<rawString>M.J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kushman. 2014. Learning to Solve Arithmetic Word Problems with Verb Categorization. In EMNLP’2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech &amp; language processing.</title>
<date>2000</date>
<publisher>Pearson Education</publisher>
<contexts>
<context position="20235" citStr="Jurafsky &amp; Martin, 2000" startWordPosition="3223" endWordPosition="3226">oning stage. To enable this mechanism, the meaning representation language should support a lazy variable ID assignment and keep as much information (e.g., determiners, plurals, modifiers) from the noun phrases as possible. DOL is a language that always keeps the structure information of phrases, whether or not it has been assigned a variable ID. In summary, compared with other languages, DOL has some unique features which make it more suitable for our math problem solving scenario. 3.2 Semantic Parsing Our parsing algorithm is based on context-free grammar (CFG) (Chomsky, 1956; Backus, 1959; Jurafsky &amp; Martin, 2000), a commonly used mathematical system for modeling constituent structure in natural languages. 3.2.1 CFG for connecting DOL and IL The core part of a CFG is the set of grammar rules. Example English grammar rules for building syntactic parsers include “S → NP VP”, “NP → CD |DT NN |NP PP”, etc. Table 2 shows some example CFG rules in our system for mapping DOL nodes to natural language word sequences. The left side of each rule is a DOL element (a function, class, or constant); and the right side is a sequence of words and arguments. The grammar rules are consumed by our parser for building DOL</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>D. Jurafsky, and J.H. Martin. 2000. Speech &amp; language processing. Pearson Education India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntaxanalysis algorithm for context-free languages (Technical report).</title>
<date>1965</date>
<journal>AFCRL.</journal>
<pages>65--758</pages>
<contexts>
<context position="24131" citStr="Kasami, 1965" startWordPosition="3875" endWordPosition="3876">d verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury &amp; Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. Figure 3: The DOL semantic parse tree for “Nine plus an integer is equal to 314” Figure 4: A syntactic parse tree 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possible parse trees. For syntactic parsing, a wellknown serious problem is ambiguity: the appearance of many syntactically correct but semantically unreasonable parse trees. Modern syntactic parsers reply on statistical information to reduce 4 VerbNet: http://verbs.colorado.edu/~mpalmer/projects/verbnet.html Nine 9 list nf.math.sum plus an integer is equal to 314 math.integer 1 vf.be.equ 314 Nine CD NP PP NP plus IN DT an NP integer S NN VB is equa</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>T. Kasami. 1965. An efficient recognition and syntaxanalysis algorithm for context-free languages (Technical report). AFCRL. 65-758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From TreeBank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="23651" citStr="Kingsbury &amp; Palmer, 2002" startWordPosition="3791" endWordPosition="3794"> approximately |roughly education.university → university math.number → number math.integer → integer Table 2: Example grammar for connecting DOL and NL 3 Freebase: http://www.freebase.com/ Functions: Additional noun functions are automatically created from Freebase properties and attribute extraction results (Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury &amp; Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. Figure 3: The DOL semantic parse tree for “Nine plus an integer is equal to 314” Figure 4: A syntactic parse tree 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient s</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>P. Kingsbury, and M. Palmer. 2002. From TreeBank to PropBank. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushman</author>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
<author>R Barzilay</author>
</authors>
<title>Learning to automatically solve algebra word problems.</title>
<date>2014</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1608" citStr="Kushman et al., 2014" startWordPosition="232" endWordPosition="235">hool-level math word problems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda &amp; Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). * Work done while this author was an intern at Microsoft Research Most (if not all) previous symbolic approaches suffer from two major shortcomings. First, natural language (NL) sentences are processed by simply applying pattern matching and/or transformation rules in an ad-hoc manner (refer to the related work section for more details). Second, surprisingly, they seldom report evaluation results about the effectiveness of the methods (except for some examples for demonstration purposes). For the small percentage of work with evaluation results available, it is unclea</context>
<context position="7018" citStr="Kushman et al. (2014)" startWordPosition="1111" endWordPosition="1115">omatically solving elementary math word problems, with technique details unknown to the general public. Other examples on the web site demonstrate a large coverage of short phrase queries on math and other domains. By randomly selecting problems from our dataset and manually testing on their web site, we find that it fails to handle most problems in our problem collection. Statistical learning methods have been proposed recently in two papers: Hosseini et al. (2014) solve single step or multi-step homogenous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) can solve a wide range of word problems, given that the equation systems and solutions are attached to problems in the training set. The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math p</context>
<context position="30295" citStr="Kushman et al., 2014" startWordPosition="4882" endWordPosition="4885"> the number word problems category; for answers.yahoo.com, we first randomly sample an initial set of problems from the math category and then ask human annotators to manually choose number word problems from them. Math equations8 and answers to the problems are manually added by human annotators. We randomly split the dataset into a dev set (for algorithm design and debugging) and a test set. More subsets are extracted to meet the requirements of the baseline methods (see below). Table 4 shows the statistics of the datasets. Baseline methods: We compare our approach with two baselines: KAZB (Kushman et al., 2014) and BasicSim. KAZB is a learning-based statistical method which solves a problem by mapping it to one of the equation templates determined by the annotated equations in the training data. We run the ALLEQ version of their algorithm since it performs much better than the other two (i.e., 5EQ and 5EQ+ANS). Their codes support only linear equations and require that there are at least two problems for each equation template (otherwise an exception will be thrown). By choosing problems from the collection that meet these requirements, we build a sub-dataset called LinearT2. In the dataset of KAZB,</context>
</contexts>
<marker>Kushman, Artzi, Zettlemoyer, Barzilay, 2014</marker>
<rawString>N. Kushman, Y. Artzi, L. Zettlemoyer, and R. Barzilay. 2014. Learning to automatically solve algebra word problems. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>E Choi</author>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="7949" citStr="Kwiatkowski et al., 2013" startWordPosition="1267" endWordPosition="1270">cture of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Lev</author>
<author>B MacCartney</author>
<author>C Manning</author>
<author>R Levy</author>
</authors>
<title>Solving logic puzzles: From robust processing to precise semantics.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Meaning and Interpretation. Association for Computational Linguistics.</booktitle>
<marker>Lev, MacCartney, Manning, Levy, 2004</marker>
<rawString>I. Lev, B. MacCartney, C. Manning, and R. Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. In Proceedings of the Workshop on Text Meaning and Interpretation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Liguda</author>
<author>T Pfeiffer</author>
</authors>
<date>2012</date>
<booktitle>Modeling Math Word Problems with Augmented Semantic Networks. NLDB’2012,</booktitle>
<pages>247--252</pages>
<contexts>
<context position="1369" citStr="Liguda &amp; Pfeiffer, 2012" startWordPosition="197" endWordPosition="200">% recall. 1 Introduction Computers, since their creation, have exceeded human beings in (speed and accuracy of) mathematical calculation. However, it is still a big challenge nowadays to design algorithms to automatically solve even primary-school-level math word problems (i.e., math problems described in natural language). Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b). Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods. In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda &amp; Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization. Equations are then derived from the structures. Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014). * Work done while this author was an intern at Microsoft Research Most (if not all) previous symbolic approaches suffer from two major shortcomings. First, natural language (NL) sentences are processed by simply applying pattern matching and/or transformation rules in an ad-hoc manner (refer to the related work section for more detai</context>
<context position="5604" citStr="Liguda &amp; Pfeiffer (2012)" startWordPosition="879" endWordPosition="882">ituations. 2 Related Work 2.1 Math word problem solving Most previous work on automatic word problem solving is symbolic. STUDENT (Bobrow, 1964a, b) handles algebraic problems by first transforming NL sentences into kernel sentences using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com No empirical evaluation results are reported in most of the above work. Almost all of these approaches parse NL text by simply applying pattern matching rules in an ad-hoc manner. For example, as mentioned in Bobrow (1964b), due to the pattern “($, AND $</context>
</contexts>
<marker>Liguda, Pfeiffer, 2012</marker>
<rawString>C. Liguda, T. Pfeiffer. 2012. Modeling Math Word Problems with Augmented Semantic Networks. NLDB’2012, pp. 247-252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ma</author>
<author>Y Zhou</author>
<author>G Cui</author>
<author>R Yun</author>
<author>R Huang</author>
</authors>
<title>Frame-based calculus of solving arithmetic multistep addition and subtraction word problems.</title>
<date>2010</date>
<booktitle>In International Workshop on Education Technology and Computer Science,</booktitle>
<volume>2</volume>
<pages>476--479</pages>
<contexts>
<context position="5827" citStr="Ma et al., 2010" startWordPosition="912" endWordPosition="915">s using a small set of transformation patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com No empirical evaluation results are reported in most of the above work. Almost all of these approaches parse NL text by simply applying pattern matching rules in an ad-hoc manner. For example, as mentioned in Bobrow (1964b), due to the pattern “($, AND $)”, the system would incorrectly divide “Tom has 2 apples, 3 bananas, and 4 pears.” into two “sentences”: “Tom has 2 apples, 3 bananas.” and “4 pears.” WolframAlpha1 shows some examples2 of automatically solving elementary </context>
</contexts>
<marker>Ma, Zhou, Cui, Yun, Huang, 2010</marker>
<rawString>Y. Ma, Y. Zhou, G. Cui, R. Yun, R. Huang. 2010. Frame-based calculus of solving arithmetic multistep addition and subtraction word problems. In International Workshop on Education Technology and Computer Science, vol. 2, pp. 476–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Marquez</author>
<author>X Carreras</author>
<author>K C Litkowski</author>
<author>S Stevenson</author>
</authors>
<title>Semantic role labeling: an introduction to the special issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="7471" citStr="Marquez et al., 2008" startWordPosition="1190" endWordPosition="1193">ni et al. (2014) solve single step or multi-step homogenous addition and subtraction problems by learning verb categories from the training data. Kushman et al. (2014) can solve a wide range of word problems, given that the equation systems and solutions are attached to problems in the training set. The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based </context>
</contexts>
<marker>Marquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>L. Marquez, X. Carreras, K.C. Litkowski, and S. Stevenson. 2008. Semantic role labeling: an introduction to the special issue. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mukherjee</author>
<author>U Garain</author>
</authors>
<title>A review of methods for automatic understanding of natural language mathematical problems.</title>
<date>2008</date>
<journal>Artificial Intelligence Review,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="5870" citStr="Mukherjee &amp; Garain (2008)" startWordPosition="920" endWordPosition="923">ion patterns. The kernel sentences are then transformed to math expressions by recursive use of pattern matching. CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object. Liguda &amp; Pfeiffer (2012) propose modeling math word problems with augmented semantic networks. Addition/subtraction problems are studied most in early research (Briars &amp; Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010). Please refer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008. 1 http://www.wolframalpha.com No empirical evaluation results are reported in most of the above work. Almost all of these approaches parse NL text by simply applying pattern matching rules in an ad-hoc manner. For example, as mentioned in Bobrow (1964b), due to the pattern “($, AND $)”, the system would incorrectly divide “Tom has 2 apples, 3 bananas, and 4 pears.” into two “sentences”: “Tom has 2 apples, 3 bananas.” and “4 pears.” WolframAlpha1 shows some examples2 of automatically solving elementary math word problems, with technique details </context>
</contexts>
<marker>Mukherjee, Garain, 2008</marker>
<rawString>A. Mukherjee and U. Garain. 2008. A review of methods for automatic understanding of natural language mathematical problems. Artificial Intelligence Review, 29(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts-step one: the one-million fact extraction challenge.</title>
<date>2006</date>
<journal>In AAAI</journal>
<volume>6</volume>
<pages>1400--1405</pages>
<contexts>
<context position="23356" citStr="Pasca et al., 2006" startWordPosition="3747" endWordPosition="3750"> give {$3} {$2} nf.math.sum!1($1,$2) → {$1} plus {$2} |{$2} added to {$1} nf.math.sum!2($1) → sum of {$1} |addition of {$1} nf.math.power($1,$2) → {$1} raised to the {power|exponent} of {$2} nf.list($1,$2) → {$2} {$1} mf.number.even → even mf.condition.if($1) → if {$1} mf.approximately → approximately |roughly education.university → university math.number → number math.integer → integer Table 2: Example grammar for connecting DOL and NL 3 Freebase: http://www.freebase.com/ Functions: Additional noun functions are automatically created from Freebase properties and attribute extraction results (Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury &amp; Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. Figure 3: The DOL semantic parse tree f</context>
</contexts>
<marker>Pasca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Organizing and searching the world wide web of facts-step one: the one-million fact extraction challenge. In AAAI (Vol. 6, pp. 1400-1405).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K K Schuler</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<note>Dissertation. http://repository.upenn.edu/dissertations/AAI3179808</note>
<contexts>
<context position="23715" citStr="Schuler, 2005" startWordPosition="3802" endWordPosition="3803">ber math.integer → integer Table 2: Example grammar for connecting DOL and NL 3 Freebase: http://www.freebase.com/ Functions: Additional noun functions are automatically created from Freebase properties and attribute extraction results (Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and isa extraction results. We have over 50 manually defined math-related verb functions. Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury &amp; Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005). Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.TN → TN” and “mf.adv.TN → TN” where TN is the name of an adjective or adverb. Figure 3: The DOL semantic parse tree for “Nine plus an integer is equal to 314” Figure 4: A syntactic parse tree 3.2.2 Parsing Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970). The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possible parse trees. For syntactic p</context>
</contexts>
<marker>Schuler, 2005</marker>
<rawString>K.K. Schuler. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon. Dissertation. http://repository.upenn.edu/dissertations/AAI3179808</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shi</author>
<author>H Zhang</author>
<author>X Yuan</author>
<author>J-R Wen</author>
</authors>
<title>Corpus-based semantic class mining: distributional vs. pattern-based approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>993--1001</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21713" citStr="Shi et al., 2010" startWordPosition="3482" endWordPosition="3485">uilt by referring to text books and online tu1136 torials. About 35 classes and 200 functions are obtained in this way. Additional instances of each element type are constructed in the ways below. Classes: Additional classes and grammar rules are obtained from two data sources: Freebase3 types, and automatically extracted lexical semantic data. By treating Freebase types as DOL classes and the mapping from types to lexical names as grammar rules, we get the first version of grammar for classes. To improve coverage, we run a term peer similarity and hypernym extraction algorithm (Hearst, 1992; Shi et al., 2010; Zhang et al., 2011) on a web snapshot of 3 billion pages, and get a peer-similarity graph and a collection of is-a pairs. An is-a pair example is (Megan Fox, actress), where “Megan Fox” and “actress” are instance and type names respectively. In our peer similarity graph, “Megan Fox” and “Britney Spears” have a high similarity score. The peer similarity graph is used to clean the is-a data collection (with the idea that peer terms often share some common type names). Given the cleaned isa data, we sort the type names by weight and manually create classes for top-1000 type names. For example, </context>
</contexts>
<marker>Shi, Zhang, Yuan, Wen, 2010</marker>
<rawString>S. Shi, H. Zhang, X. Yuan, and J.-R. Wen. 2010. Corpus-based semantic class mining: distributional vs. pattern-based approaches. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 993–1001. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8141" citStr="Steedman, 2000" startWordPosition="1299" endWordPosition="1300">ment structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. 3 Approach Consider the first problem in Figure 1 (written below for convenience), One number is 16 more than another. If the smaller number is subtracted from 2/3 of the larger, the result is 1/4 of the sum of the two nu</context>
<context position="18059" citStr="Steedman, 2000" startWordPosition="2872" endWordPosition="2873">eir major differences are: First, they use very different mechanisms to represent noun phrases. In AMR, a sentence (e.g., “the boy destroyed the room”) and a noun phrase (e.g., “the boy’s destruction of the room”) can have the same representation. While in DOL, a sentence is always represented by a verb function; and a noun phrase is always a noun function or a constant. Second, DOL has a larger type system and is stricter in type compatibility checking. Third, DOL has template classes and built-in data structures like t.list and nf.list to facilitate the representation of math concepts. CCG (Steedman, 2000) provides a transparent interface between syntax and semantics. In CCG, semantic information is defined on words (e.g., “λx.odd(x)” for “odd” and “λx.number(x)” for “number”). In contrast, DOL explicitly connects NL text patterns to semantic elements. For example, as shown in Table 2 (Section 3.2.1), one CFG grammar rule connects pattern “{$1} raised to the power of {$2}” to function nf.math.power. Logical forms are another way of meaning representation. We choose not to transform NL text directly to logical forms for two reasons: On one hand, state-of-the-art methods for mapping NL text into </context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics (ACL),</booktitle>
<pages>960--967</pages>
<contexts>
<context position="7883" citStr="Wong &amp; Mooney, 2007" startWordPosition="1255" endWordPosition="1258">rsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representatio</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Association for Computational Linguistics (ACL), pages 960–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic proramming.</title>
<date>1996</date>
<booktitle>In Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="7804" citStr="Zelle &amp; Mooney, 1996" startWordPosition="1243" endWordPosition="1246">referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences betwee</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>M. Zelle and R.J. Mooney. 1996. Learning to parse database queries using inductive logic proramming. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>658--666</pages>
<contexts>
<context position="7833" citStr="Zettlemoyer &amp; Collins, 2005" startWordPosition="1247" endWordPosition="1250">nceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and expl</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L.S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Uncertainty in Artificial Intelligence (UAI), pages 658–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online Learning of Relaxed CCG Grammars for Parsing to Logical Form.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="7862" citStr="Zettlemoyer &amp; Collins, 2007" startWordPosition="1251" endWordPosition="1254">ur baselines. 2.2 Semantic parsing There has been much work on analyzing the semantic structure of NL strings. In semantic role labeling and frame-semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions. Another type of semantic parsing work (Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins, 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant &amp; Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning. Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000). Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical 2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part) 1133 forms. In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the </context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>L.S. Zettlemoyer and M. Collins. 2007. Online Learning of Relaxed CCG Grammars for Parsing to Logical Form. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Zhang</author>
<author>S Shi</author>
<author>J Liu</author>
<author>S Sun</author>
<author>C-Y Lin</author>
</authors>
<title>Nonlinear evidence fusion and propagation for hyponymy relation mining.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<volume>11</volume>
<pages>1159--1168</pages>
<contexts>
<context position="21734" citStr="Zhang et al., 2011" startWordPosition="3486" endWordPosition="3489">to text books and online tu1136 torials. About 35 classes and 200 functions are obtained in this way. Additional instances of each element type are constructed in the ways below. Classes: Additional classes and grammar rules are obtained from two data sources: Freebase3 types, and automatically extracted lexical semantic data. By treating Freebase types as DOL classes and the mapping from types to lexical names as grammar rules, we get the first version of grammar for classes. To improve coverage, we run a term peer similarity and hypernym extraction algorithm (Hearst, 1992; Shi et al., 2010; Zhang et al., 2011) on a web snapshot of 3 billion pages, and get a peer-similarity graph and a collection of is-a pairs. An is-a pair example is (Megan Fox, actress), where “Megan Fox” and “actress” are instance and type names respectively. In our peer similarity graph, “Megan Fox” and “Britney Spears” have a high similarity score. The peer similarity graph is used to clean the is-a data collection (with the idea that peer terms often share some common type names). Given the cleaned isa data, we sort the type names by weight and manually create classes for top-1000 type names. For example, create a class person</context>
</contexts>
<marker>Zhang, Shi, Liu, Sun, Lin, 2011</marker>
<rawString>F. Zhang, S. Shi, J. Liu, S. Sun, and C.-Y. Lin. 2011. Nonlinear evidence fusion and propagation for hyponymy relation mining. In ACL, volume 11, pages 1159–1168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
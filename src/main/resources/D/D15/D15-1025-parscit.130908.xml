<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000041">
<title confidence="0.921295">
Non-lexical neural architecture for fine-grained POS Tagging
</title>
<author confidence="0.724477">
Matthieu Labeau, Kevin L¨oser, Alexandre Allauzen
</author>
<note confidence="0.73676425">
Universit´e Paris-Sud and LIMSI-CNRS,
Rue John von Neumann
91403 Orsay cedex
France
</note>
<email confidence="0.98874">
firstname.lastname@limsi.fr
</email>
<sectionHeader confidence="0.997237" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999157705882353">
In this paper we explore a POS tagging ap-
plication of neural architectures that can
infer word representations from the raw
character stream. It relies on two mod-
elling stages that are jointly learnt: a
convolutional network that infers a word
representation directly from the character
stream, followed by a prediction stage.
Models are evaluated on a POS and mor-
phological tagging task for German. Ex-
perimental results show that the convolu-
tional network can infer meaningful word
representations, while for the prediction
stage, a well designed and structured strat-
egy allows the model to outperform state-
of-the-art results, without any feature en-
gineering.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991585">
Most modern statistical models for natural lan-
guage processing (NLP) applications are strongly
or fully lexicalized, for instance part-of-speech
(POS) and named entity taggers, as well as lan-
guage models, and parsers. In these models, the
observed word form is considered as the elemen-
tary unit, while its morphological properties re-
main neglected. As a result, the vocabulary ob-
served on training data heavily restricts the gener-
alization power of lexicalized models.
Designing subword-level systems is appealing
for several reasons. First, words sharing morpho-
logical properties often share grammatical func-
tion and meaning, and leveraging that information
can yield improved word representations. Sec-
ond, a subword-level analysis can address the out-
of-vocabulary issue i.e the fact that word-level
models fail to meaningfully process unseen word
forms. This allows a better processing of morpho-
logically rich languages in which there is a com-
binatorial explosion of word forms, most of which
are not observed during training. Finally, using
subword units could allow processing of noisy text
such as user-generated content on the Web, where
abbreviations, slang usage and spelling mistakes
cause the number of word types to explode.
This work investigates models that do not rely
on a fixed vocabulary to make a linguistic predic-
tion. Our main focus in this paper is POS tag-
ging, yet the proposed approach could be applied
to a wide variety of language processing tasks.
Our main contribution is to show that neural net-
works can successfully learn unlexicalized mod-
els that infer a useful word representation from
the character stream. This approach achieves state
of-the-art performance on a German POS tagging
task. This task is difficult because German is a
morphologically rich language1, as reflected by
the large number of morphological tags (255) in
our study, yielding a grand total of more than
600 POS+MORPH tags. An aggravating factor
is that these morphological categories are overtly
marked by a handful of highly ambiguous inflec-
tion marks (suffixes). We therefore believe that
this case study is well suited to assess both the rep-
resentation and prediction power of our models.
The architecture we explore in section 2 differs
from previous work that only consider the charac-
ter level. Following (Santos and Zadrozny, 2014),
it consists in two stages that are jointly learnt. The
lower stage is a convolutional network that infers
a word embedding from a character string of ar-
bitrary size, while the higher network infers the
POS tags based on this word embedding sequence.
For the latter, we investigate different architec-
tures of increasing complexities: from a feedfor-
ward and context-free inference to a bi-recurrent
network that predicts the global sequence. Exper-
imental results (section 4) show that the proposed
approach can achieve state of the art performance
</bodyText>
<footnote confidence="0.9987445">
1Besides inflected forms, German is characterized by a
possibly infinite and evolving set of compound nouns.
</footnote>
<page confidence="0.854727">
232
</page>
<note confidence="0.684307">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 232–237,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9998105">
and that the choice of architecture for the predic-
tion part of the model has a significant impact.
</bodyText>
<sectionHeader confidence="0.992935" genericHeader="method">
2 Network Architectures
</sectionHeader>
<bodyText confidence="0.999958333333333">
The different architectures we propose act in two
stages to infer, for a sentence s = {w1, ... , w|s|},
a sequence of tags {t1, ... , t|s|}. Each tag belongs
to the tagset T . The first stage is designed to rep-
resent each word locally, and focuses on capturing
the meaningful morphological information. In the
second stage, we investigate different ways to pre-
dict the tag sequence that differ in how the global
information is used.
</bodyText>
<subsectionHeader confidence="0.984868">
2.1 From character to word level
</subsectionHeader>
<bodyText confidence="0.9999268">
To obtain word embeddings, the usual approach
introduced by (Bengio et al., 2003) relies on a
fixed vocabulary W and each word w E W is
mapped to a vector of nf real valued features by
a look-up matrix W E R|W|∗nf. To avoid the use
of a fixed vocabulary, we propose to derive a word
representation from a sequence of character em-
bedding: if C denotes the finite set of characters,
each character is mapped on a vector of nc features
gathered in the look-up matrix C.
To infer a word embedding , we use a convo-
lution layer (Waibel et al., 1990; Collobert et al.,
2011), build as in (Santos and Zadrozny, 2014).
As illustrated in figure 1, a word w is a character
sequence {c1, .., c|w|} represented by their embed-
dings {Cc1, .., Cc|w|}, where Cci denotes the row
in C associated to the character ci. A convolu-
tion filter Wconv E Rnf x Rdc∗nc is applied over
a sliding window of dc characters, producing local
features:
</bodyText>
<equation confidence="0.671541">
xn = Wconv(Ccn−dc+1 :..: Ccn)T + bconv ,
</equation>
<bodyText confidence="0.99989825">
where xn is a vector of size nf obtained for each
position n in the word2. The i-th element of the
embedding of w is the maximum over the i-th ele-
ments of the feature vectors :
</bodyText>
<equation confidence="0.980969">
[f]i = tanh( max [xn]i)
1≤n≤|s|
</equation>
<bodyText confidence="0.969669">
Using a maximum after a sliding convolution win-
dow ensures that the embedding combines local
features from the whole word, and selects the more
2Two padding character tokens are used to deal with bor-
der effects. The first is added at the beginning and the second
at the end of the word, as many times as it is necessary to ob-
tain the same number of windows than the length of the word.
Their embeddings are added to C.
</bodyText>
<figure confidence="0.6390305">
Csow Cc1 Cc2 Cc3 Cc4 Ccs Ceow
S e h e n
</figure>
<figureCaption confidence="0.90483275">
Figure 1: Architecture of the layer for character-
level encoding of words.
useful ones. The parameters of the layer are the
matrices C and Wconv and the bias bconv.
</figureCaption>
<subsectionHeader confidence="0.964497">
2.2 From words to prediction
</subsectionHeader>
<bodyText confidence="0.999997">
To predict the tag sequence associated to a sen-
tence s, we first use a feedforward architecture,
with a single hidden layer. To compute the proba-
bility of tagging the n-th word in the sentence with
tag ti, we use a window of dw word embeddings3
centered around the word wn:
</bodyText>
<equation confidence="0.858634">
xn = fn−dw−1 : ... : fn+dw−1,
2 2
followed by a hidden and output layers:
sn = Wo tanh(Whxn + bh) + bo. (1)
</equation>
<bodyText confidence="0.999315454545454">
The parameters of the hidden an output layers
are respectively Wh, bh and Wo, bo.
We also experiment with a a bidirectional re-
current layer, as described in (Graves et al.,
2013). The forward and backward passes allow
each prediction to be conditioned on the complete
past and future contexts, instead of merely a neigh-
boring window. As illustrated in figure 2, the for-
ward hidden state, at position n, will be computed
using the previous forward hidden state and the
word embedding in position n:
</bodyText>
<equation confidence="0.398664875">
hn = tanh( −−→ −−→ −−→
−→ Wfhfn + Whh hn−1 + bh)
3Similarly, we use special word tokens for padding.
f1 f2 f3 f4 f5
max(.)
Wconv x (.)T + bconv
nc
nf
</equation>
<page confidence="0.939244">
233
</page>
<figureCaption confidence="0.954">
Figure 2: Bidirectional recurrent architecture for
tag prediction. The upper part is used in the case
of structured inference.
</figureCaption>
<bodyText confidence="0.946992">
−−→ −−→
W fh and W hh are the transition matrices of
the forward part of the layer, and bh is the bias.
The backward hidden states are computed simi-
larly, and the hidden states of each direction are
concatenated to pass through an output layer:
</bodyText>
<equation confidence="0.890884">
sn = W°(−→hn : ←−hn) + b°. (2)
</equation>
<subsectionHeader confidence="0.896893">
2.3 Inference and Training
</subsectionHeader>
<bodyText confidence="0.999992571428571">
To infer the tag sequence from the sequence of
output layers defined by equations 1 or 2, we ex-
plore two strategies. The first simply applies a
softmax function to the output layer of the net-
work described in the previous section. In this
case, each tag prediction is made independently of
the surrounding predictions.
For sequence labeling, a more appropriate so-
lution relies on the approach of (Collobert, 2011),
also used in (Santos and Zadrozny, 2014). Let con-
sider each possible tag sequence {t1, ... , t|s|} as a
possible path over a sequence of hidden states. We
can add a transition matrix Wtrans and then com-
pute the score of a sequence as follows:
</bodyText>
<equation confidence="0.6890585">
rtrans r 1 1
Wtn−1,tn + lsnitn
</equation>
<bodyText confidence="0.9998766">
The Viterbi algorithm (Viterbi, 1967) offers an ex-
act solution to infer the path that gives the max-
imum score. It is worth noticing that both these
strategies can be applied to the feedforward and
bidirectional recurrent networks. For both strate-
gies, the whole network can estimate conditional
log-likelihood of a tag sequence given a sentence
s and the set of parameters θ. This criterion can
then be optimized using a stochastic gradient as-
cent with the back-propagation algorithm.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.99996046875">
The choice to consider words from the charac-
ter level has recently been more and more ex-
plored. While its raw application to language
modeling did not achieve clear improvement over
the word-based models (Mikolov et al., 2012), this
approach shown impressive results for text gen-
eration (Sutskever et al., 2011; Graves, 2013).
However, for this line of work, the main issue is
to learn long range dependencies at the character
level since the word level is not considered by the
model.
More recently, the character level was con-
sidered as more interpretable and convenient
way to explore and understand recurrent net-
works (Karpathy et al., 2015). In (Zhang and Le-
Cun, 2015), the authors build a text understand-
ing model that does not require any knowledge
and uses hierarchical feature extraction. Here the
character level allows the model to ignore the def-
inition a priori of a vocabulary and let the model
build its own representation of a sentence or a doc-
ument, directly from the character level. To some
extent, our work can be considered as an extension
of their work, tailored for POS tagging.
(Santos and Zadrozny, 2014) applies a very sim-
ilar model to the POS tagging of Portuguese and
English. (Luong et al., 2013) also descends lower
than the word level, using a dictionary of mor-
phemes and recursive neural networks to model
the structure of the words. Similarly, this allows
a better representation of rare and complex words,
evaluated on a word similarity task.
</bodyText>
<sectionHeader confidence="0.99908" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999941222222222">
Experiments are carried out on the Part-of-Speech
and Morphological tagging tasks using the Ger-
man corpus TIGER Treebank (Brants et al., 2002).
To the best of our knowledge, the best results on
this task were published in (Mueller et al., 2013),
who applied a high-order CRF that includes an in-
tensive feature engineering to five different lan-
guages. German was highlighted as having ’the
most ambiguous morphology’. The corpus, de-
</bodyText>
<equation confidence="0.618536222222222">
f1 f2 f3 f4 f5
nh
|T |
nf
�
s({t}|s|
1 , {w}|s|
1 ) =
1≤n≤|s|
</equation>
<page confidence="0.994453">
234
</page>
<table confidence="0.999893333333333">
Architecture Encoding Output POS POS+Morph
Dev Test Dev Test
Feedforward Lex. Simple 4.22 f 0.05 5.89 f 0.07 13.97 f 0.14 17.46 f 0.14
Non-lex. Struct. 3.90 f 0.05 5.33 f 0.09 12.22 f 0.13 15.34 f 0.13
Both Simple 3.31 f 0.07 4.22 f 0.07 13.50 f 0.16 16.23 f 0.13
Struct. 2.92 f 0.02 3.82 f 0.04 11.65 f 0.11 14.43 f 0.19
Simple 2.59 f 0.05 3.34 f 0.09 11.89 f 0.14 14.63 f 0.22
Struct. 2.22 f 0.03* 2.86 f 0.03* 9.11 f 0.14 11.29 f 0.06
biRNN Lex Simple 6.03 f 0.06 8.05 f 0.05 17.83 f 0.11 21.33 f 0.26
Non-Lex Struct. 3.89 f 0.06 5.26 f 0.05 11.88 f 0.05 17.78 f 0.12
Both Simple 4.46 f 0.08 5.84 f 0.19 16.61 f 0.18 19.39 f 0.12
Struct. 2.74 f 0.07 3.59 f 0.07 10.09 f 0.09 12.88 f 0.28
Simple 3.63 f 0.06 4.63 f 0.04 14.83 f 0.11 17.54 f 0.13
Struct. 2.21 f 0.04* 2.86 f 0.05* 8.63 f 0.21* 10.97 f 0.19*
CRF 2.06 2.56 9.40 11.42
</table>
<tableCaption confidence="0.966461">
Table 1: Comparison of the feedforward and bidirectional recurrent architectures for predictions, with
</tableCaption>
<figureCaption confidence="0.778536666666667">
different settings. The non-lexical encoding is convolutional. CRF refers to state-of-the-art system of
(Mueller et al., 2013). Simple and Struct. respectively denote the position-by-position and structured
prediction. * indicates our best configuration.
</figureCaption>
<bodyText confidence="0.994199833333333">
scribed in details in (Fraser et al., 2013), contains
a training set of 40472 sentences, a development
and a test set of both 5000 sentences. We consider
the two tagging tasks, with first a coarse tagset (54
tags), and then a morpho-syntactical rich tagset
(619 items observed on the the training set).
</bodyText>
<subsectionHeader confidence="0.992766">
4.1 Experimental settings
</subsectionHeader>
<bodyText confidence="0.996533384615385">
All the models are implemented4 with the Theano
library (Bergstra et al., 2010). For optimization,
we use Adagrad (Duchi et al., 2011), with a learn-
ing rate of 0.1. The other hyperparameters are:
the window sizes, d, and dw, respectively set to
5 and 9, the dimension of character embeddings,
word embeddings and of the hidden layer, n,, nf
and nh, that are respectively of 100, 200 and 2005.
The models were trained on 7 epochs. Parame-
ter initialization and corpus ordering are random,
and the results presented are the average and stan-
dard deviation of the POS Tagging error rate over
5 runs.
</bodyText>
<footnote confidence="0.844387333333333">
4Implementation is available at https://github.
com/MatthieuLabeau/NonlexNN
5For both the learning rate and the embedding sizes, re-
sults does not differ in a significant way in a large range of hy-
perparameters, and their impact resides more in convergence
speed and computation time
</footnote>
<subsectionHeader confidence="0.511475">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.99997188">
The first experiment aims to evaluate the efficiency
of a convolutional encoding with the basic feed-
forward architecture for prediction. We compare
a completely non-lexicalized model which relies
only on a character-level encoding with a lexical-
ized model where we use conventional word em-
beddings stored with a fixed vocabulary6. Re-
sults are reported in Table 1 along with with the
state-of-the-art results published in (Mueller et al.,
2013). Results show that a character-level en-
coding yields better results than the conventional
word-level encoding. Moreover, the structured in-
ference allows the model to achieve accuracy rea-
sonably close to the performance of a high-order
CRF that uses handcrafted features. Finally, the
model that uses the concatenation of both the char-
acter and word-level embeddings outperforms the
state-of-the-art system on the more difficult task,
without any feature engineering.
To give an idea of how a simple model
would perform on such task, the reader can refer
to (Schmid and Laws, 2008) and (Mueller et al.,
2013). For instance in the former, by choosing the
most probable tag position-by-position, the error
rate on the development set of the TIGER dataset
</bodyText>
<footnote confidence="0.98951">
6Every word that appears in the training set.
</footnote>
<page confidence="0.997551">
235
</page>
<bodyText confidence="0.9991195">
is 32.7 for the simple POS Tagging task.
We further analyze the results by looking at
the error rates respectively on known and un-
known words7. From table 2, we observe that
the number of unknown words wrongly labeled
is divided by 3 for POS and almost divided by
2 for POS+Morph tagging, showing the ability
of character-level encoding to generalize to new
words. Moreover, a strictly non-lexical encoding
makes slightly more mistakes on words already
seen, whereas the model that concatenates both
embeddings will make less mistakes for both un-
known and known words.
This shows that information from the context
and from the morphology are complementary,
which is conjectured in (Mueller et al., 2013) by
using a morphological analyzer in complement of
higher-order CRF.
</bodyText>
<table confidence="0.998334">
Lex. Non-lex. Both
POS Unknown 2970 1054 1010
Known 1974 2981 1620
POS+Morph Unknown 5827 3472 3384
Known 8652 10205 7232
</table>
<tableCaption confidence="0.992657">
Table 2: Error counts for known/unknown words
</tableCaption>
<bodyText confidence="0.936696428571428">
in the test set, with a structured feedforward pre-
diction model for the tagging task.
In the second set of experiments, we evaluate
the convolutional encoding with a bidirectional re-
current network for prediction. Results are pre-
sented in the second half of Table 1. Surprisingly,
this architecture performs poorly with simple in-
ference, but clearly improves when predicting a
structured output using the Viterbi algorithm, both
for training and testing. Moreover, a non-lexical
model trained to infer a tag sequence with the
Viterbi algorithm achieves results that are close to
the state-of-the-art, thus validating our approach.
We consider that this improvement comes from the
synergy between using a global training objective
with a global hidden representation, complexify-
ing the model but allowing a more efficient solu-
tion. Finally, the model that uses the combination
of both the character and word-level embeddings
yields the best results. It is interesting to notice
that the predictive architecture has no influence on
the results of the simple task when the prediction is
7Unknown words refer to words present in the develop-
ment or test sets, but not in the training set.
structured, but improves them on the difficult task.
This also shows that the contribution of word em-
beddings to our model corresponds to a difference
of 1.5 to 2 points in performance.
</bodyText>
<sectionHeader confidence="0.993937" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999984185185185">
In this paper, we explored new models that can in-
fer meaningful word representations from the raw
character stream, allowing the model to exploit the
morphological properties of words without using
any handcrafted features or external tools. These
models can therefore efficiently process words that
were unseen in the training data. The evaluation
was carried out on a POS and morphological tag-
ging task for German. We described different ar-
chitectures that act in two stages: the first stage is a
convolutional network that infers a word represen-
tation directly from the character stream, while the
second stage performs the prediction. For the pre-
diction stage, we investigated different solutions
showing that a bidirectional recurrent network can
outperform state-of-the-art results when using a
structured inference algorithm.
Our results showed that character-level encod-
ing can address the unknown words problem for
morphologically complex languages. In the fu-
ture, we plan to extend these models to other tasks
such as syntactic parsing and machine translation.
Moreover, we will also investigate other architec-
tures to infer word embeddings from the character
level. For instance, preliminary experiments show
that bidirectional recurrent network can achieve
very competitive and promising results.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994395">
We would like to thank the anonymous review-
ers for their helpful comments and suggestions.
This work has been partly funded by the Eu-
ropean Unions Horizon 2020 research and in-
novation programme under grant agreement No.
645452 (QT21).
</bodyText>
<sectionHeader confidence="0.999323" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9992645">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137 1155.
James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
</reference>
<page confidence="0.990888">
236
</page>
<reference confidence="0.997905441860465">
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the workshop on tree-
banks and linguistic theories, pages 24–41.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In Proceedings of the Four-
teenth International Conference on Artificial Intel-
ligence and Statistics, AISTATS 2011, Fort Laud-
erdale, USA, April 11-13, 2011, pages 224–232.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.
Alexander Fraser, Helmut Schmid, Rich´ard Farkas,
Renjing Wang, and Hinrich Sch¨utze. 2013. Knowl-
edge sources for constituent parsing of German, a
morphologically rich and less-configurational lan-
guage. Comput. Linguist., 39(1):57–85, March.
Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional LSTM. In 2013 IEEE Workshop on
Automatic Speech Recognition and Understanding,
Olomouc, Czech Republic, December 8-12, 2013,
pages 273–278.
Alex Graves. 2013. Generating sequences with recur-
rent neural networks. CoRR, abs/1308.0850.
Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.
Visualizing and understanding recurrent networks.
CoRR, abs/1506.02078.
Thang Luong, Richard Socher, and Christopher D.
Manning. 2013. Better word representations with
recursive neural networks for morphology. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning, CoNLL 2013,
Sofia, Bulgaria, August 8-9, 2013, pages 104–113.
Tomas Mikolov, Ilya Sutskever, Anoop Deoras, Hai-
Son Le, Stefan Kombrink, and Jan Cernocky. 2012.
Subword language modeling with neural networks.
Unpublished.
Thomas Mueller, Helmut Schmid, and Hinrich
Sch¨utze. 2013. Efficient higher-order CRFs for
morphological tagging. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 322–332, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
Cicero D. Santos and Bianca Zadrozny. 2014.
Learning character-level representations for part-of-
speech tagging. In Tony Jebara and Eric P. Xing, ed-
itors, Proceedings of the 31st International Confer-
ence on Machine Learning (ICML-14), pages 1818–
1826. JMLR Workshop and Conference Proceed-
ings.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained pos tagging. In
Proceedings of the 22Nd International Conference
on Computational Linguistics - Volume 1, COLING
’08, pages 777–784, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-
works. In Lise Getoor and Tobias Scheffer, editors,
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ’11, pages
1017–1024, New York, NY, USA, June. ACM.
Andrew Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. IEEE Trans. Inf. Theor., 13(2):260–269,
April.
Alexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-
ton, Kiyohiro Shikano, and Kevin J. Lang, 1990.
Readings in Speech Recognition, chapter Phoneme
Recognition Using Time-delay Neural Networks,
pages 393–404. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Xiang Zhang and Yann LeCun. 2015. Text understand-
ing from scratch. CoRR, abs/1502.01710.
</reference>
<page confidence="0.997408">
237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.278460">
<title confidence="0.999468">Non-lexical neural architecture for fine-grained POS Tagging</title>
<author confidence="0.997606">Matthieu Labeau</author>
<author confidence="0.997606">Kevin L¨oser</author>
<author confidence="0.997606">Alexandre</author>
<affiliation confidence="0.6599545">Universit´e Paris-Sud and Rue John von</affiliation>
<address confidence="0.984288">91403 Orsay</address>
<email confidence="0.99832">firstname.lastname@limsi.fr</email>
<abstract confidence="0.984133222222222">In this paper we explore a POS tagging application of neural architectures that can infer word representations from the raw character stream. It relies on two modelling stages that are jointly learnt: a convolutional network that infers a word representation directly from the character stream, followed by a prediction stage. Models are evaluated on a POS and morphological tagging task for German. Experimental results show that the convolutional network can infer meaningful word representations, while for the prediction stage, a well designed and structured strategy allows the model to outperform stateof-the-art results, without any feature engineering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1155</pages>
<contexts>
<context position="4773" citStr="Bengio et al., 2003" startWordPosition="743" endWordPosition="746">tion part of the model has a significant impact. 2 Network Architectures The different architectures we propose act in two stages to infer, for a sentence s = {w1, ... , w|s|}, a sequence of tags {t1, ... , t|s|}. Each tag belongs to the tagset T . The first stage is designed to represent each word locally, and focuses on capturing the meaningful morphological information. In the second stage, we investigate different ways to predict the tag sequence that differ in how the global information is used. 2.1 From character to word level To obtain word embeddings, the usual approach introduced by (Bengio et al., 2003) relies on a fixed vocabulary W and each word w E W is mapped to a vector of nf real valued features by a look-up matrix W E R|W|∗nf. To avoid the use of a fixed vocabulary, we propose to derive a word representation from a sequence of character embedding: if C denotes the finite set of characters, each character is mapped on a vector of nc features gathered in the look-up matrix C. To infer a word embedding , we use a convolution layer (Waibel et al., 1990; Collobert et al., 2011), build as in (Santos and Zadrozny, 2014). As illustrated in figure 1, a word w is a character sequence {c1, .., c</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137 1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Olivier Breuleux</author>
<author>Fr´ed´eric Bastien</author>
<author>Pascal Lamblin</author>
<author>Razvan Pascanu</author>
<author>Guillaume Desjardins</author>
<author>Joseph Turian</author>
<author>David Warde-Farley</author>
<author>Yoshua Bengio</author>
</authors>
<date>2010</date>
<note>Theano: a CPU and</note>
<contexts>
<context position="12764" citStr="Bergstra et al., 2010" startWordPosition="2190" endWordPosition="2193">ing is convolutional. CRF refers to state-of-the-art system of (Mueller et al., 2013). Simple and Struct. respectively denote the position-by-position and structured prediction. * indicates our best configuration. scribed in details in (Fraser et al., 2013), contains a training set of 40472 sentences, a development and a test set of both 5000 sentences. We consider the two tagging tasks, with first a coarse tagset (54 tags), and then a morpho-syntactical rich tagset (619 items observed on the the training set). 4.1 Experimental settings All the models are implemented4 with the Theano library (Bergstra et al., 2010). For optimization, we use Adagrad (Duchi et al., 2011), with a learning rate of 0.1. The other hyperparameters are: the window sizes, d, and dw, respectively set to 5 and 9, the dimension of character embeddings, word embeddings and of the hidden layer, n,, nf and nh, that are respectively of 100, 200 and 2005. The models were trained on 7 epochs. Parameter initialization and corpus ordering are random, and the results presented are the average and standard deviation of the POS Tagging error rate over 5 runs. 4Implementation is available at https://github. com/MatthieuLabeau/NonlexNN 5For bot</context>
</contexts>
<marker>Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, Bengio, 2010</marker>
<rawString>James Bergstra, Olivier Breuleux, Fr´ed´eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a CPU and</rawString>
</citation>
<citation valid="true">
<authors>
<author>GPU math</author>
</authors>
<title>expression compiler.</title>
<date></date>
<booktitle>In Proceedings of the Python for Scientific Computing Conference (SciPy),</booktitle>
<tech>Oral Presentation.</tech>
<marker>math, </marker>
<rawString>GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June. Oral Presentation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the workshop on treebanks and linguistic theories,</booktitle>
<pages>24--41</pages>
<contexts>
<context position="10809" citStr="Brants et al., 2002" startWordPosition="1832" endWordPosition="1835">k can be considered as an extension of their work, tailored for POS tagging. (Santos and Zadrozny, 2014) applies a very similar model to the POS tagging of Portuguese and English. (Luong et al., 2013) also descends lower than the word level, using a dictionary of morphemes and recursive neural networks to model the structure of the words. Similarly, this allows a better representation of rare and complex words, evaluated on a word similarity task. 4 Experiments and Results Experiments are carried out on the Part-of-Speech and Morphological tagging tasks using the German corpus TIGER Treebank (Brants et al., 2002). To the best of our knowledge, the best results on this task were published in (Mueller et al., 2013), who applied a high-order CRF that includes an intensive feature engineering to five different languages. German was highlighted as having ’the most ambiguous morphology’. The corpus, def1 f2 f3 f4 f5 nh |T | nf � s({t}|s| 1 , {w}|s| 1 ) = 1≤n≤|s| 234 Architecture Encoding Output POS POS+Morph Dev Test Dev Test Feedforward Lex. Simple 4.22 f 0.05 5.89 f 0.07 13.97 f 0.14 17.46 f 0.14 Non-lex. Struct. 3.90 f 0.05 5.33 f 0.09 12.22 f 0.13 15.34 f 0.13 Both Simple 3.31 f 0.07 4.22 f 0.07 13.50 f</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proceedings of the workshop on treebanks and linguistic theories, pages 24–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="5259" citStr="Collobert et al., 2011" startWordPosition="838" endWordPosition="841">l information is used. 2.1 From character to word level To obtain word embeddings, the usual approach introduced by (Bengio et al., 2003) relies on a fixed vocabulary W and each word w E W is mapped to a vector of nf real valued features by a look-up matrix W E R|W|∗nf. To avoid the use of a fixed vocabulary, we propose to derive a word representation from a sequence of character embedding: if C denotes the finite set of characters, each character is mapped on a vector of nc features gathered in the look-up matrix C. To infer a word embedding , we use a convolution layer (Waibel et al., 1990; Collobert et al., 2011), build as in (Santos and Zadrozny, 2014). As illustrated in figure 1, a word w is a character sequence {c1, .., c|w|} represented by their embeddings {Cc1, .., Cc|w|}, where Cci denotes the row in C associated to the character ci. A convolution filter Wconv E Rnf x Rdc∗nc is applied over a sliding window of dc characters, producing local features: xn = Wconv(Ccn−dc+1 :..: Ccn)T + bconv , where xn is a vector of size nf obtained for each position n in the word2. The i-th element of the embedding of w is the maximum over the i-th elements of the feature vectors : [f]i = tanh( max [xn]i) 1≤n≤|s|</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011,</booktitle>
<pages>224--232</pages>
<location>Fort Lauderdale, USA,</location>
<contexts>
<context position="8389" citStr="Collobert, 2011" startWordPosition="1423" endWordPosition="1424">as. The backward hidden states are computed similarly, and the hidden states of each direction are concatenated to pass through an output layer: sn = W°(−→hn : ←−hn) + b°. (2) 2.3 Inference and Training To infer the tag sequence from the sequence of output layers defined by equations 1 or 2, we explore two strategies. The first simply applies a softmax function to the output layer of the network described in the previous section. In this case, each tag prediction is made independently of the surrounding predictions. For sequence labeling, a more appropriate solution relies on the approach of (Collobert, 2011), also used in (Santos and Zadrozny, 2014). Let consider each possible tag sequence {t1, ... , t|s|} as a possible path over a sequence of hidden states. We can add a transition matrix Wtrans and then compute the score of a sequence as follows: rtrans r 1 1 Wtn−1,tn + lsnitn The Viterbi algorithm (Viterbi, 1967) offers an exact solution to infer the path that gives the maximum score. It is worth noticing that both these strategies can be applied to the feedforward and bidirectional recurrent networks. For both strategies, the whole network can estimate conditional log-likelihood of a tag seque</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011, pages 224–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2121</pages>
<contexts>
<context position="12819" citStr="Duchi et al., 2011" startWordPosition="2199" endWordPosition="2202">m of (Mueller et al., 2013). Simple and Struct. respectively denote the position-by-position and structured prediction. * indicates our best configuration. scribed in details in (Fraser et al., 2013), contains a training set of 40472 sentences, a development and a test set of both 5000 sentences. We consider the two tagging tasks, with first a coarse tagset (54 tags), and then a morpho-syntactical rich tagset (619 items observed on the the training set). 4.1 Experimental settings All the models are implemented4 with the Theano library (Bergstra et al., 2010). For optimization, we use Adagrad (Duchi et al., 2011), with a learning rate of 0.1. The other hyperparameters are: the window sizes, d, and dw, respectively set to 5 and 9, the dimension of character embeddings, word embeddings and of the hidden layer, n,, nf and nh, that are respectively of 100, 200 and 2005. The models were trained on 7 epochs. Parameter initialization and corpus ordering are random, and the results presented are the average and standard deviation of the POS Tagging error rate over 5 runs. 4Implementation is available at https://github. com/MatthieuLabeau/NonlexNN 5For both the learning rate and the embedding sizes, results do</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
<author>Rich´ard Farkas</author>
<author>Renjing Wang</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Knowledge sources for constituent parsing of German, a morphologically rich and less-configurational language.</title>
<date>2013</date>
<journal>Comput. Linguist.,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Fraser, Schmid, Farkas, Wang, Sch¨utze, 2013</marker>
<rawString>Alexander Fraser, Helmut Schmid, Rich´ard Farkas, Renjing Wang, and Hinrich Sch¨utze. 2013. Knowledge sources for constituent parsing of German, a morphologically rich and less-configurational language. Comput. Linguist., 39(1):57–85, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Navdeep Jaitly</author>
<author>Abdel-rahman Mohamed</author>
</authors>
<title>Hybrid speech recognition with deep bidirectional LSTM.</title>
<date>2013</date>
<booktitle>In 2013 IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>273--278</pages>
<location>Olomouc, Czech Republic,</location>
<contexts>
<context position="7072" citStr="Graves et al., 2013" startWordPosition="1185" endWordPosition="1188"> and Wconv and the bias bconv. 2.2 From words to prediction To predict the tag sequence associated to a sentence s, we first use a feedforward architecture, with a single hidden layer. To compute the probability of tagging the n-th word in the sentence with tag ti, we use a window of dw word embeddings3 centered around the word wn: xn = fn−dw−1 : ... : fn+dw−1, 2 2 followed by a hidden and output layers: sn = Wo tanh(Whxn + bh) + bo. (1) The parameters of the hidden an output layers are respectively Wh, bh and Wo, bo. We also experiment with a a bidirectional recurrent layer, as described in (Graves et al., 2013). The forward and backward passes allow each prediction to be conditioned on the complete past and future contexts, instead of merely a neighboring window. As illustrated in figure 2, the forward hidden state, at position n, will be computed using the previous forward hidden state and the word embedding in position n: hn = tanh( −−→ −−→ −−→ −→ Wfhfn + Whh hn−1 + bh) 3Similarly, we use special word tokens for padding. f1 f2 f3 f4 f5 max(.) Wconv x (.)T + bconv nc nf 233 Figure 2: Bidirectional recurrent architecture for tag prediction. The upper part is used in the case of structured inference.</context>
</contexts>
<marker>Graves, Jaitly, Mohamed, 2013</marker>
<rawString>Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. 2013. Hybrid speech recognition with deep bidirectional LSTM. In 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, Olomouc, Czech Republic, December 8-12, 2013, pages 273–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Generating sequences with recurrent neural networks.</title>
<date>2013</date>
<tech>CoRR, abs/1308.0850.</tech>
<contexts>
<context position="9490" citStr="Graves, 2013" startWordPosition="1611" endWordPosition="1612">ecurrent networks. For both strategies, the whole network can estimate conditional log-likelihood of a tag sequence given a sentence s and the set of parameters θ. This criterion can then be optimized using a stochastic gradient ascent with the back-propagation algorithm. 3 Related Work The choice to consider words from the character level has recently been more and more explored. While its raw application to language modeling did not achieve clear improvement over the word-based models (Mikolov et al., 2012), this approach shown impressive results for text generation (Sutskever et al., 2011; Graves, 2013). However, for this line of work, the main issue is to learn long range dependencies at the character level since the word level is not considered by the model. More recently, the character level was considered as more interpretable and convenient way to explore and understand recurrent networks (Karpathy et al., 2015). In (Zhang and LeCun, 2015), the authors build a text understanding model that does not require any knowledge and uses hierarchical feature extraction. Here the character level allows the model to ignore the definition a priori of a vocabulary and let the model build its own rep</context>
</contexts>
<marker>Graves, 2013</marker>
<rawString>Alex Graves. 2013. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Justin Johnson</author>
<author>Fei-Fei Li</author>
</authors>
<title>Visualizing and understanding recurrent networks.</title>
<date>2015</date>
<tech>CoRR, abs/1506.02078.</tech>
<contexts>
<context position="9810" citStr="Karpathy et al., 2015" startWordPosition="1663" endWordPosition="1666">words from the character level has recently been more and more explored. While its raw application to language modeling did not achieve clear improvement over the word-based models (Mikolov et al., 2012), this approach shown impressive results for text generation (Sutskever et al., 2011; Graves, 2013). However, for this line of work, the main issue is to learn long range dependencies at the character level since the word level is not considered by the model. More recently, the character level was considered as more interpretable and convenient way to explore and understand recurrent networks (Karpathy et al., 2015). In (Zhang and LeCun, 2015), the authors build a text understanding model that does not require any knowledge and uses hierarchical feature extraction. Here the character level allows the model to ignore the definition a priori of a vocabulary and let the model build its own representation of a sentence or a document, directly from the character level. To some extent, our work can be considered as an extension of their work, tailored for POS tagging. (Santos and Zadrozny, 2014) applies a very similar model to the POS tagging of Portuguese and English. (Luong et al., 2013) also descends lower </context>
</contexts>
<marker>Karpathy, Johnson, Li, 2015</marker>
<rawString>Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015. Visualizing and understanding recurrent networks. CoRR, abs/1506.02078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL 2013,</booktitle>
<pages>104--113</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="10389" citStr="Luong et al., 2013" startWordPosition="1765" endWordPosition="1768">current networks (Karpathy et al., 2015). In (Zhang and LeCun, 2015), the authors build a text understanding model that does not require any knowledge and uses hierarchical feature extraction. Here the character level allows the model to ignore the definition a priori of a vocabulary and let the model build its own representation of a sentence or a document, directly from the character level. To some extent, our work can be considered as an extension of their work, tailored for POS tagging. (Santos and Zadrozny, 2014) applies a very similar model to the POS tagging of Portuguese and English. (Luong et al., 2013) also descends lower than the word level, using a dictionary of morphemes and recursive neural networks to model the structure of the words. Similarly, this allows a better representation of rare and complex words, evaluated on a word similarity task. 4 Experiments and Results Experiments are carried out on the Part-of-Speech and Morphological tagging tasks using the German corpus TIGER Treebank (Brants et al., 2002). To the best of our knowledge, the best results on this task were published in (Mueller et al., 2013), who applied a high-order CRF that includes an intensive feature engineering </context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, CoNLL 2013, Sofia, Bulgaria, August 8-9, 2013, pages 104–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Ilya Sutskever, Anoop Deoras,</title>
<date>2012</date>
<publisher>Unpublished.</publisher>
<location>HaiSon</location>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Anoop Deoras, HaiSon Le, Stefan Kombrink, and Jan Cernocky. 2012. Subword language modeling with neural networks. Unpublished.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mueller</author>
<author>Helmut Schmid</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Efficient higher-order CRFs for morphological tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>322--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<marker>Mueller, Schmid, Sch¨utze, 2013</marker>
<rawString>Thomas Mueller, Helmut Schmid, and Hinrich Sch¨utze. 2013. Efficient higher-order CRFs for morphological tagging. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 322–332, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicero D Santos</author>
<author>Bianca Zadrozny</author>
</authors>
<title>Learning character-level representations for part-ofspeech tagging.</title>
<date>2014</date>
<booktitle>Proceedings of the 31st International Conference on Machine Learning (ICML-14),</booktitle>
<pages>1818--1826</pages>
<editor>In Tony Jebara and Eric P. Xing, editors,</editor>
<contexts>
<context position="3249" citStr="Santos and Zadrozny, 2014" startWordPosition="497" endWordPosition="500">task. This task is difficult because German is a morphologically rich language1, as reflected by the large number of morphological tags (255) in our study, yielding a grand total of more than 600 POS+MORPH tags. An aggravating factor is that these morphological categories are overtly marked by a handful of highly ambiguous inflection marks (suffixes). We therefore believe that this case study is well suited to assess both the representation and prediction power of our models. The architecture we explore in section 2 differs from previous work that only consider the character level. Following (Santos and Zadrozny, 2014), it consists in two stages that are jointly learnt. The lower stage is a convolutional network that infers a word embedding from a character string of arbitrary size, while the higher network infers the POS tags based on this word embedding sequence. For the latter, we investigate different architectures of increasing complexities: from a feedforward and context-free inference to a bi-recurrent network that predicts the global sequence. Experimental results (section 4) show that the proposed approach can achieve state of the art performance 1Besides inflected forms, German is characterized by</context>
<context position="5300" citStr="Santos and Zadrozny, 2014" startWordPosition="845" endWordPosition="848">ter to word level To obtain word embeddings, the usual approach introduced by (Bengio et al., 2003) relies on a fixed vocabulary W and each word w E W is mapped to a vector of nf real valued features by a look-up matrix W E R|W|∗nf. To avoid the use of a fixed vocabulary, we propose to derive a word representation from a sequence of character embedding: if C denotes the finite set of characters, each character is mapped on a vector of nc features gathered in the look-up matrix C. To infer a word embedding , we use a convolution layer (Waibel et al., 1990; Collobert et al., 2011), build as in (Santos and Zadrozny, 2014). As illustrated in figure 1, a word w is a character sequence {c1, .., c|w|} represented by their embeddings {Cc1, .., Cc|w|}, where Cci denotes the row in C associated to the character ci. A convolution filter Wconv E Rnf x Rdc∗nc is applied over a sliding window of dc characters, producing local features: xn = Wconv(Ccn−dc+1 :..: Ccn)T + bconv , where xn is a vector of size nf obtained for each position n in the word2. The i-th element of the embedding of w is the maximum over the i-th elements of the feature vectors : [f]i = tanh( max [xn]i) 1≤n≤|s| Using a maximum after a sliding convolut</context>
<context position="8431" citStr="Santos and Zadrozny, 2014" startWordPosition="1428" endWordPosition="1431">re computed similarly, and the hidden states of each direction are concatenated to pass through an output layer: sn = W°(−→hn : ←−hn) + b°. (2) 2.3 Inference and Training To infer the tag sequence from the sequence of output layers defined by equations 1 or 2, we explore two strategies. The first simply applies a softmax function to the output layer of the network described in the previous section. In this case, each tag prediction is made independently of the surrounding predictions. For sequence labeling, a more appropriate solution relies on the approach of (Collobert, 2011), also used in (Santos and Zadrozny, 2014). Let consider each possible tag sequence {t1, ... , t|s|} as a possible path over a sequence of hidden states. We can add a transition matrix Wtrans and then compute the score of a sequence as follows: rtrans r 1 1 Wtn−1,tn + lsnitn The Viterbi algorithm (Viterbi, 1967) offers an exact solution to infer the path that gives the maximum score. It is worth noticing that both these strategies can be applied to the feedforward and bidirectional recurrent networks. For both strategies, the whole network can estimate conditional log-likelihood of a tag sequence given a sentence s and the set of para</context>
<context position="10293" citStr="Santos and Zadrozny, 2014" startWordPosition="1747" endWordPosition="1750">he character level was considered as more interpretable and convenient way to explore and understand recurrent networks (Karpathy et al., 2015). In (Zhang and LeCun, 2015), the authors build a text understanding model that does not require any knowledge and uses hierarchical feature extraction. Here the character level allows the model to ignore the definition a priori of a vocabulary and let the model build its own representation of a sentence or a document, directly from the character level. To some extent, our work can be considered as an extension of their work, tailored for POS tagging. (Santos and Zadrozny, 2014) applies a very similar model to the POS tagging of Portuguese and English. (Luong et al., 2013) also descends lower than the word level, using a dictionary of morphemes and recursive neural networks to model the structure of the words. Similarly, this allows a better representation of rare and complex words, evaluated on a word similarity task. 4 Experiments and Results Experiments are carried out on the Part-of-Speech and Morphological tagging tasks using the German corpus TIGER Treebank (Brants et al., 2002). To the best of our knowledge, the best results on this task were published in (Mue</context>
</contexts>
<marker>Santos, Zadrozny, 2014</marker>
<rawString>Cicero D. Santos and Bianca Zadrozny. 2014. Learning character-level representations for part-ofspeech tagging. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1818– 1826. JMLR Workshop and Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of conditional probabilities with decision trees and an application to fine-grained pos tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>777--784</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14599" citStr="Schmid and Laws, 2008" startWordPosition="2485" endWordPosition="2488">sults published in (Mueller et al., 2013). Results show that a character-level encoding yields better results than the conventional word-level encoding. Moreover, the structured inference allows the model to achieve accuracy reasonably close to the performance of a high-order CRF that uses handcrafted features. Finally, the model that uses the concatenation of both the character and word-level embeddings outperforms the state-of-the-art system on the more difficult task, without any feature engineering. To give an idea of how a simple model would perform on such task, the reader can refer to (Schmid and Laws, 2008) and (Mueller et al., 2013). For instance in the former, by choosing the most probable tag position-by-position, the error rate on the development set of the TIGER dataset 6Every word that appears in the training set. 235 is 32.7 for the simple POS Tagging task. We further analyze the results by looking at the error rates respectively on known and unknown words7. From table 2, we observe that the number of unknown words wrongly labeled is divided by 3 for POS and almost divided by 2 for POS+Morph tagging, showing the ability of character-level encoding to generalize to new words. Moreover, a s</context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>Helmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an application to fine-grained pos tagging. In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 777–784, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11,</booktitle>
<pages>1017--1024</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="9475" citStr="Sutskever et al., 2011" startWordPosition="1607" endWordPosition="1610">ward and bidirectional recurrent networks. For both strategies, the whole network can estimate conditional log-likelihood of a tag sequence given a sentence s and the set of parameters θ. This criterion can then be optimized using a stochastic gradient ascent with the back-propagation algorithm. 3 Related Work The choice to consider words from the character level has recently been more and more explored. While its raw application to language modeling did not achieve clear improvement over the word-based models (Mikolov et al., 2012), this approach shown impressive results for text generation (Sutskever et al., 2011; Graves, 2013). However, for this line of work, the main issue is to learn long range dependencies at the character level since the word level is not considered by the model. More recently, the character level was considered as more interpretable and convenient way to explore and understand recurrent networks (Karpathy et al., 2015). In (Zhang and LeCun, 2015), the authors build a text understanding model that does not require any knowledge and uses hierarchical feature extraction. Here the character level allows the model to ignore the definition a priori of a vocabulary and let the model bu</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey Hinton. 2011. Generating text with recurrent neural networks. In Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pages 1017–1024, New York, NY, USA, June. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Trans. Inf. Theor.,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="8702" citStr="Viterbi, 1967" startWordPosition="1481" endWordPosition="1482">ies. The first simply applies a softmax function to the output layer of the network described in the previous section. In this case, each tag prediction is made independently of the surrounding predictions. For sequence labeling, a more appropriate solution relies on the approach of (Collobert, 2011), also used in (Santos and Zadrozny, 2014). Let consider each possible tag sequence {t1, ... , t|s|} as a possible path over a sequence of hidden states. We can add a transition matrix Wtrans and then compute the score of a sequence as follows: rtrans r 1 1 Wtn−1,tn + lsnitn The Viterbi algorithm (Viterbi, 1967) offers an exact solution to infer the path that gives the maximum score. It is worth noticing that both these strategies can be applied to the feedforward and bidirectional recurrent networks. For both strategies, the whole network can estimate conditional log-likelihood of a tag sequence given a sentence s and the set of parameters θ. This criterion can then be optimized using a stochastic gradient ascent with the back-propagation algorithm. 3 Related Work The choice to consider words from the character level has recently been more and more explored. While its raw application to language mod</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Trans. Inf. Theor., 13(2):260–269, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Waibel</author>
<author>Toshiyuki Hanazawa</author>
<author>Geofrey Hinton</author>
<author>Kiyohiro Shikano</author>
<author>Kevin J Lang</author>
</authors>
<title>Readings in Speech Recognition, chapter Phoneme Recognition Using Time-delay Neural Networks,</title>
<date>1990</date>
<pages>393--404</pages>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5234" citStr="Waibel et al., 1990" startWordPosition="834" endWordPosition="837">ffer in how the global information is used. 2.1 From character to word level To obtain word embeddings, the usual approach introduced by (Bengio et al., 2003) relies on a fixed vocabulary W and each word w E W is mapped to a vector of nf real valued features by a look-up matrix W E R|W|∗nf. To avoid the use of a fixed vocabulary, we propose to derive a word representation from a sequence of character embedding: if C denotes the finite set of characters, each character is mapped on a vector of nc features gathered in the look-up matrix C. To infer a word embedding , we use a convolution layer (Waibel et al., 1990; Collobert et al., 2011), build as in (Santos and Zadrozny, 2014). As illustrated in figure 1, a word w is a character sequence {c1, .., c|w|} represented by their embeddings {Cc1, .., Cc|w|}, where Cci denotes the row in C associated to the character ci. A convolution filter Wconv E Rnf x Rdc∗nc is applied over a sliding window of dc characters, producing local features: xn = Wconv(Ccn−dc+1 :..: Ccn)T + bconv , where xn is a vector of size nf obtained for each position n in the word2. The i-th element of the embedding of w is the maximum over the i-th elements of the feature vectors : [f]i =</context>
</contexts>
<marker>Waibel, Hanazawa, Hinton, Shikano, Lang, 1990</marker>
<rawString>Alexander Waibel, Toshiyuki Hanazawa, Geofrey Hinton, Kiyohiro Shikano, and Kevin J. Lang, 1990. Readings in Speech Recognition, chapter Phoneme Recognition Using Time-delay Neural Networks, pages 393–404. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Zhang</author>
<author>Yann LeCun</author>
</authors>
<title>Text understanding from scratch.</title>
<date>2015</date>
<location>CoRR, abs/1502.01710.</location>
<contexts>
<context position="9838" citStr="Zhang and LeCun, 2015" startWordPosition="1668" endWordPosition="1672">el has recently been more and more explored. While its raw application to language modeling did not achieve clear improvement over the word-based models (Mikolov et al., 2012), this approach shown impressive results for text generation (Sutskever et al., 2011; Graves, 2013). However, for this line of work, the main issue is to learn long range dependencies at the character level since the word level is not considered by the model. More recently, the character level was considered as more interpretable and convenient way to explore and understand recurrent networks (Karpathy et al., 2015). In (Zhang and LeCun, 2015), the authors build a text understanding model that does not require any knowledge and uses hierarchical feature extraction. Here the character level allows the model to ignore the definition a priori of a vocabulary and let the model build its own representation of a sentence or a document, directly from the character level. To some extent, our work can be considered as an extension of their work, tailored for POS tagging. (Santos and Zadrozny, 2014) applies a very similar model to the POS tagging of Portuguese and English. (Luong et al., 2013) also descends lower than the word level, using a</context>
</contexts>
<marker>Zhang, LeCun, 2015</marker>
<rawString>Xiang Zhang and Yann LeCun. 2015. Text understanding from scratch. CoRR, abs/1502.01710.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
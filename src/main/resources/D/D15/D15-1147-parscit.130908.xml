<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<note confidence="0.800253">
How to Avoid Unwanted Pregnancies:
</note>
<title confidence="0.495038">
Domain Adaptation using Neural Network Models
</title>
<author confidence="0.5794055">
Shafiq Joty, Hassan Sajjad, Nadir Durrani,
Kamla Al-Mannai, Ahmed Abdelali and Stephan Vogel
</author>
<affiliation confidence="0.84645">
Qatar Computing Research Institute - Hamad Bin Khalifa University
</affiliation>
<email confidence="0.971885">
Isjoty,hsajjad,ndurrani,kmannai,aabdelali,svogell@qf.org.qa
</email>
<sectionHeader confidence="0.994271" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999990058823529">
We present novel models for domain adap-
tation based on the neural network joint
model (NNJM). Our models maximize
the cross entropy by regularizing the loss
function with respect to in-domain model.
Domain adaptation is carried out by as-
signing higher weight to out-domain se-
quences that are similar to the in-domain
data. In our alternative model we take a
more restrictive approach by additionally
penalizing sequences similar to the out-
domain data. Our models achieve better
perplexities than the baseline NNJM mod-
els and give improvements of up to 0.5
and 0.6 BLEU points in Arabic-to-English
and English-to-German language pairs, on
a standard task of translating TED talks.
</bodyText>
<sectionHeader confidence="0.998759" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999237475409836">
Rapid influx of digital data has galvanized the use
of empirical methods in many fields including Ma-
chine Translation (MT). The increasing availabil-
ity of bilingual corpora has made it possible to
automatically learn translation rules that required
years of linguistic analysis previously. While ad-
ditional data is often beneficial for a general pur-
pose Statistical Machine Translation (SMT) sys-
tem, a problem arises when translating new do-
mains such as lectures (Cettolo et al., 2014),
patents (Fujii et al., 2010) or medical text (Bojar
et al., 2014), where either the bilingual text does
not exist or is available in small quantity. All do-
mains have their own vocabulary and stylistic pref-
erences which cannot be fully encompassed by a
system trained on the general domain.
Machine translation systems trained from a sim-
ple concatenation of small in-domain and large
out-domain data often perform below par be-
cause the out-domain data is distant or over-
whelmingly larger than the in-domain data. Ad-
ditional data increases lexical ambiguity by in-
troducing new senses to the existing in-domain
vocabulary. For example, an Arabic-to-English
SMT system trained by simply concatenating in-
and out-domain data translates the Arabic phrase
“�L,�..&gt;u� ZV L�&amp;quot; I k6LIA vs” to “about the
problem of unwanted pregnancy”. This translation
is incorrect in the context of the in-domain data,
where it should be translated to “about the prob-
lem of choice overload”. The sense of the Ara-
bic phrase taken from out-domain data completely
changes the meaning of the sentence. In this paper,
we tackle this problem by proposing domain adap-
tation models that make use of all the data while
preserving the in-domain preferences.
A significant amount of research has been car-
ried out recently in domain adaptation. The com-
plexity of the SMT pipeline, starting from cor-
pus preparation to word-alignment, and then train-
ing a wide range of models opens a wide horizon
to carry out domain specific adaptations. This is
typically done using either data selection (Mat-
soukas et al., 2009) or model adaptation (Foster
and Kuhn, 2007). In this paper, we further re-
search in model adaptation using the neural net-
work framework.
In recent years, there has been a growing in-
terest in deep neural networks (NNs) and word
embeddings with application to numerous NLP
problems. A notably successful attempt on the
SMT frontier was recently made by Devlin et
al. (2014). They proposed a neural network
joint model (NNJM), which augments streams of
source with target n-grams and learns a NN model
over vector representation of such streams. The
model is then integrated into the decoder and used
as an additional language model feature.
Our aim in this paper is to advance the state-of-
the-art in SMT by extending NNJM for domain
adaptation to leverage the huge amount of out-
</bodyText>
<page confidence="0.957503">
1259
</page>
<note confidence="0.9848775">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1259–1270,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99989865625">
domain data coming from heterogeneous sources.
We hypothesize that the distributed vector rep-
resentation of NNJM helps to bridge the lexical
differences between the in-domain and the out-
domain data, and adaptation is necessary to avoid
deviation of the model from the in-domain data,
which otherwise happens because of the large out-
domain data.
To this end, we propose two novel extensions
of NNJM for domain adaptation. Our first model
minimizes the cross entropy by regularizing the
loss function with respect to the in-domain model.
The regularizer gives higher weight to the training
instances that are similar to the in-domain data.
Our second model takes a more conservative ap-
proach by additionally penalizing data instances
similar to the out-domain data.
We evaluate our models on the standard task
of translating Arabic-English and English-German
language pairs. Our adapted models achieve bet-
ter perplexities (Chen and Goodman, 1999) than
the models trained on in- and in+out-domain data.
Improvements are also reflected in BLEU scores
(Papineni et al., 2002) as we compare these mod-
els within the SMT pipeline. We obtain gains of
up to 0.5 and 0.6 on Arabic-English and English-
German pairs over a competitive baseline system.
The remainder of this paper is organized as fol-
lows: Section 2 gives an account on related work.
Section 3 revisits NNJM model and Section 4 dis-
cusses our models. Section 5 presents the experi-
mental setup and the results. Section 6 concludes.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996709666666667">
Previous work on domain adaptation in MT can
be broken down broadly into two main categories
namely data selection and model adaptation.
</bodyText>
<subsectionHeader confidence="0.988918">
2.1 Data Selection
</subsectionHeader>
<bodyText confidence="0.99975212">
Data selection has shown to be an effective way
to discard poor quality or irrelevant training in-
stances, which when included in an MT system,
hurts its performance. The idea is to score the out-
domain data using a model trained from the in-
domain data and apply a cut-off based on the re-
sulting scores. The MT system can then be trained
on a subset of the out-domain data that is closer
to in-domain. Selection based methods can be
helpful to reduce computational cost when train-
ing is expensive and also when memory is con-
strained. Data selection was done earlier for lan-
guage modeling using information retrieval tech-
niques (Hildebrand et al., 2005) and perplexity
measures (Moore and Lewis, 2010). Axelrod et
al. (2011) further extended the work of Moore and
Lewis (2010) to translation model adaptation by
using both source- and target-side language mod-
els. Duh et al. (2013) used a recurrent neural lan-
guage model instead of an ngram-based language
model to do the same. Translation model features
were used recently by (Liu et al., 2014; Hoang and
Sima’an, 2014) for data selection. Durrani et al.
(2015a) performed data selection using operation
sequence model (OSM) and NNJM models.
</bodyText>
<subsectionHeader confidence="0.99984">
2.2 Model Adaptation
</subsectionHeader>
<bodyText confidence="0.999956918918919">
The downside of data selection is that finding an
optimal cut-off threshold is a time consuming pro-
cess. An alternative to completely filtering out
less useful data is to minimize its effect by down-
weighting it. It is more robust than selection since
it takes advantage of the complete out-domain data
with intelligent weighting towards the in-domain.
Matsoukas et al. (2009) proposed a
classification-based sentence weighting method
for adaptation. Foster et al. (2010) extended this
by weighting phrases rather than sentence pairs.
Other researchers have carried out weighting by
merging phrase-tables through linear interpolation
(Finch and Sumita, 2008; Nakov and Ng, 2009)
or log-linear combination (Foster and Kuhn,
2009; Bisazza et al., 2011; Sennrich, 2012)
and through phrase training based adaptation
(Mansour and Ney, 2013). Durrani et al. (2015a)
applied EM-based mixture modeling to OSM
and NNJM models to perform model weighting.
Chen et al. (2013b) used a vector space model for
adaptation at the phrase level. Every phrase pair is
represented as a vector, where every entry in the
vector reflects its relatedness with each domain.
Chen et al. (2013a) also applied mixture model
adaptation for reordering model.
Other work on domain adaptation includes but
not limited to studies focusing on topic models
(Eidelman et al., 2012; Hasler et al., 2014), dy-
namic adaptation without in-domain data (Sen-
nrich et al., 2013; Mathur et al., 2014) and sense
disambiguation (Carpuat et al., 2013).
In this paper, we do model adaptation using a
neural network framework. In contrast to pre-
vious work, we perform it at the (bilingual) n-
gram level, where n is sufficiently large to cap-
ture long-range cross-lingual dependencies. The
</bodyText>
<page confidence="0.9784">
1260
</page>
<bodyText confidence="0.999503285714286">
generalized vector representation of the neural net-
work model reduces the data sparsity issue of tra-
ditional Markov-based models by learning better
word classes. Furthermore, our specially designed
loss functions for adaptation help the model to
avoid deviation from the in-domain data without
losing the ability to generalize.
</bodyText>
<sectionHeader confidence="0.998267" genericHeader="method">
3 Neural Network Joint Model
</sectionHeader>
<bodyText confidence="0.9978675">
In recent years, there has been a great deal of ef-
fort dedicated to neural networks (NNs) and word
embeddings with applications to SMT and other
areas in NLP (Bengio et al., 2003; Auli et al.,
2013; Kalchbrenner and Blunsom, 2013; Gao et
al., 2014; Schwenk, 2012; Collobert et al., 2011;
Mikolov et al., 2013a; Socher et al., 2013; Hin-
ton et al., 2012). Recently, Devlin et al. (2014)
proposed a neural network joint model (NNJM)
and integrated it into the decoder as an additional
feature. They showed impressive improvements
in Arabic-to-English and Chinese-to-English MT
tasks. Let us revisit the NNJM model briefly.
Given a source sentence S and its correspond-
ing target sentence T, the NNJM model computes
the conditional probability P(T|S) as follows:
</bodyText>
<equation confidence="0.9967915">
P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1)
i
</equation>
<bodyText confidence="0.999915052631579">
where, si is a q-word source window for the tar-
get word ti based on the one-to-one (non-NULL)
alignment of T to S. As exemplified in Figure 1,
this is essentially a (p + q)-gram neural network
LM (NNLM) originally proposed by Bengio et al.
(2003). Each input word i.e. source or target word
in the context is represented by a D dimensional
vector in the shared look-up layer L ∈ R|Vi|×D,
where Ui is the input vocabulary.1 The look-up
layer then creates a context vector xn representing
the context words of the (p+q)-gram sequence by
concatenating their respective vectors in L. The
concatenated vector is then passed through non-
linear hidden layers to learn a high-level represen-
tation, which is in turn fed to the output layer. The
output layer has a softmax activation over the
output vocabulary Uo of target words. Formally,
the probability of getting k-th word in the output
given the context xn can be written as:
</bodyText>
<equation confidence="0.923317333333333">
wT
P(yn = k|xn, θ) =|nt- 1 exp (w(Tntφ(xn)) (2)
1Note that L is a model parameter to be learned.
</equation>
<bodyText confidence="0.999993173913043">
where φ(xn) defines the transformations of xn
through the hidden layers, and wk are the weights
from the last hidden layer to the output layer.
For notational simplicity, henceforth we will use
(xn, yn) to represent a training sequence.
By setting p and q to be sufficiently large,
NNJM can capture long-range cross-lingual de-
pendencies between words, while still overcom-
ing the data sparseness issue by virtue of its dis-
tributed representations (i.e., word vectors). A ma-
jor bottleneck, however, is to surmount the com-
putational cost involved in training the model and
applying it for MT decoding. Devlin et al. (2014)
proposed two tricks to speed up computation in
decoding. The first one is to pre-compute the hid-
den layer computations and fetch them directly as
needed during decoding. The second technique is
to train a self-normalized NNJM to avoid compu-
tation of the softmax normalization factor (i.e., the
denominator in Equation 2) in decoding. How-
ever, self-normalization does not solve the compu-
tational cost of training the model. In the follow-
ing, we describe a method to address this issue.
</bodyText>
<subsectionHeader confidence="0.998612">
3.1 Training by Noise Contrastive Estimation
</subsectionHeader>
<bodyText confidence="0.999079">
The standard way to train NNLMs is to maximize
the log likelihood of the training data:
</bodyText>
<equation confidence="0.95363">
ynk log P(yn = k|xn, θ) (3)
</equation>
<bodyText confidence="0.9999146">
where, ynk = I(yn = k) is an indicator vari-
able (i.e., ynk=1 when yn=k, otherwise 0). Op-
timization is performed using first-order online
methods, such as stochastic gradient ascent (SGA)
with standard backpropagation algorithm. Unfor-
tunately, training NNLMs are impractically slow
because for each training instance (xn, yn), the
softmax output layer (see Equation 2) needs to
compute a summation over all words in the output
vocabulary.2 Noise contrastive estimation or NCE
(Gutmann and Hyv¨arinen, 2010) provides an effi-
cient and stable way to avoid this repetitive com-
putation as recently applied to NNLMs (Vaswani
et al., 2013; Mnih and Teh, 2012). We can re-write
Equation 2 as follows:
</bodyText>
<equation confidence="0.996831666666667">
σ(yn = k|xn, θ)
P(yn = k|xn, θ) = (4)
Z(φ(xn), W)
</equation>
<bodyText confidence="0.949134">
where Q(.) is the un-normalized score and Z(.)
is the normalization factor. In NCE, we consider
</bodyText>
<footnote confidence="0.646574">
2This would take few weeks for a modern CPU machine
to train a single NNJM model on the whole data.
</footnote>
<equation confidence="0.994387142857143">
|Vo |
E
k=1
J(θ) =
N
E
n=1
</equation>
<page confidence="0.991586">
1261
</page>
<figure confidence="0.9707751875">
Look-up Hidden Output
layer layer layer
Xn φ(Xn)
U
W
yn
ymn
C
M
ψ
7r
Source token 1
Source token 2
Source token 3
Target token 1
Target token 2
</figure>
<figureCaption confidence="0.909844">
Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target
words (i.e., 2-words history) and a source context window of size 3. For illustration, the output yn is
</figureCaption>
<bodyText confidence="0.967392866666666">
shown as a single categorical variable (scalar) as opposed to the traditional one-hot vector representation.
Z(.) as an additional model parameter along with
the regular parameters, i.e., weights, look-up vec-
tors. However, it has been shown that fixing Z(.)
to 1 instead of learning it in training does not affect
the model performance (Mnih and Teh, 2012).
For each training instance (Xn, yn), we add
M noise samples (Xn, ymn ) by sampling ymn from
a known noise distribution ψ (e.g., unigram,
uniform) M many times (i.e., m = 1... M); see
Figure 1. NCE loss is then defined to discriminate
a true instance from a noisy one. Let C E {0, 11
denote the class of an instance with C = 1 indicat-
ing true and C = 0 indicating noise. NCE maxi-
mizes the following conditional log likelihood:
</bodyText>
<equation confidence="0.999696428571429">
hlog[P(C = 1|yn, Xn, θ)]
M ilog[P(C = 0|ym n , Xn, ψ)] (5)
+ X
m=1
hlog [P(yn|C = 1, Xn, θ)P(C = 1|π)]
log [(P(ymn |C = 0, Xn, ψ)) P(C = 0|π)]
i− (M + 1) log Q (6)
</equation>
<bodyText confidence="0.99962975">
where Q = P(yn, C = 1|Xn, θ, 7r) + P(ymn , C =
0|Xn, ψ, 7r) is a normalization constant. After re-
moving the constant terms, Equation 6 can be fur-
ther simplified as:
</bodyText>
<equation confidence="0.988968333333333">
i
ynk log ψnk
m (7)
</equation>
<bodyText confidence="0.999996111111111">
where ψnk =P(ymn = k|Xn, ψ) is the noise dis-
tribution, Unk =U(yn = k|Xn, θ) is the unnormal-
ized score at the output layer (Equation 4), and ynk
and ymnk are indicator variables as defined before.
NCE reduces the number of computations needed
at the output layer from |Vo |to M + 1, where M
is a small number in comparison with |Vo|. In all
our experiments we use NCE loss with M = 100
samples as suggested by Mnih and Teh (2012).
</bodyText>
<sectionHeader confidence="0.991343" genericHeader="method">
4 Neural Domain Adaptation Models
</sectionHeader>
<bodyText confidence="0.996348066666666">
The ability to generalize and learn complex se-
mantic relationships (Mikolov et al., 2013b) and
its compelling empirical results gives a strong mo-
tivation to use the NNJM model for the problem of
domain adaptation in machine translation. How-
ever, the vanilla NNJM described above is limited
in its ability to effectively learn from a large and
diverse out-domain data in the best favor of an in-
domain data. To address this, we propose two neu-
ral domain adaptation models (NDAM) extending
the NNJM model. Our models add regularization
to its loss function either with respect to in-domain
or both in- and out-domains. In both cases, we first
present the regularized loss function for the nor-
malized output layer with the standard softmax,
</bodyText>
<equation confidence="0.996909857142857">
N
X
n=1
J(θ) =
=
N
X
n=1
M
+ X
m=1
J(θ) = N |Vo |hynk log σnk + M
X X X
n=1 k=1 m=1
</equation>
<page confidence="0.896899">
1262
</page>
<bodyText confidence="0.9988035">
followed by the corresponding un-normalized one
using the noise contrastive estimation.
</bodyText>
<subsectionHeader confidence="0.870876">
4.1 NDAMv1
</subsectionHeader>
<bodyText confidence="0.999985">
To improve the generalization of word embed-
dings, NNLMs are generally trained on very large
datasets (Mikolov et al., 2013a; Vaswani et al.,
2013). Therefore, we aim to train our neural
domain adaptation models (NDAM) on in- plus
out-domain data, while restricting it to drift away
from in-domain. In our first model NDAMv1, we
achieve this by biasing the model towards the in-
domain using a regularizer (or prior) based on the
in-domain model. Let Bi be an NNJM model al-
ready trained on the in-domain data. We train an
adapted model Ba on the whole data, but regular-
izing it with respect to Bi. We redefine the normal-
ized loss function of Equation 3 as follows:
</bodyText>
<equation confidence="0.967453">
[= λ ynk log ˆynk(θa) +
](1 − λ) ynk pnk(θi) log ˆynk(θa) (9)
</equation>
<bodyText confidence="0.999966222222222">
where ˆynk(Ba) is the softmax output and pnk(Bi)
is the probability of the training instance accord-
ing to the in-domain model Bi. Notice that the loss
function minimizes the cross entropy of the cur-
rent model Ba with respect to the gold labels yn
and the in-domain model Bi. The mixing param-
eter A ∈ [0, 1] determines the relative strength of
the two components.3 Similarly, we can re-define
the NCE loss of Equation 7 as:
</bodyText>
<subsectionHeader confidence="0.847788">
4.2 NDAMv2
</subsectionHeader>
<bodyText confidence="0.999958615384616">
The regularizer in NDAMv1 is based on an in-
domain model Bi, which puts higher weights to
the training instances (i.e., n-gram sequences) that
are similar to the in-domain ones. This might
work better when the out-domain data is similar
to the in-domain data. In cases where the out-
domain data is different, we might want to build
a more conservative model that penalizes training
instances for being similar to the out-domain ones.
Let Bi and Bo be the two NNJMs already trained
from the in- and out-domains, respectively, and Bo
is trained using the same vocabulary as Bi. We de-
fine the new normalized loss function as follows:
</bodyText>
<equation confidence="0.989827">
[λ ynk log ˆynk(θa) + (1 − λ) ynk
][pnk(θi) − pnk(θo)] log ˆynk(θa) (12)
</equation>
<bodyText confidence="0.999922909090909">
where ynk, ˆynk(Ba), pnk(Bi) and pnk(Bo) are sim-
ilarly defined as before. This loss function min-
imizes the cross entropy of the current model Ba
with respect to the gold labels yn and the differ-
ence between the in-domain model Bi and the out-
domain model Bo. Intuitively, the regularizer as-
signs higher weights to training instances that are
not only similar to the in-domain but also dissim-
ilar to the out-domain. The parameter A ∈ [0, 1]
determines the strength of the regularization. The
corresponding NCE loss can be defined as follows:
</bodyText>
<equation confidence="0.996684742857143">
[λ ynk log σnk + (1 − λ) ynk log σnk
ynk log ψnk] (13)
J(θa) = N |Vo |[λ ynk logP(yn = k|xn, θa) + (1 − λ)
E E
n=1 k=1
]ynk P(yn = k|xn, θi) logP(yn = k|xn, θa) (8)
N
E
n=1
|Vo|
E
k=1
EN
J(θa) =
n=1
|Vo|
E
k=1
|Vo|
E
k=1
J(θa) =
N
E
n=1
(pnk(θi) − pnk(θo)) +
M
E
m=1
J(θa) = N  |Vo  |[λ ynk log σnk + (1 − λ) ynk The derivatives of the above cost function with re-
E E spect to the final layer weight vectors wj are:
n=1 k=1
M
pnk(θi) log σnk + E ynklog ψnk] (10)
m=1
</equation>
<bodyText confidence="0.999933333333333">
We use SGA with backpropagation to train this
model. The derivatives of J(Ba) with respect to
the final layer weight vectors wj turn out to be:
</bodyText>
<equation confidence="0.9801391">
N
∇wj J(θa) = E [λ (ynj − σnj) + (1 − λ)
n=1
]ynk pnk(θi) σnj] (11)
3We used a balanced value λ = 0.5 for our experiments.
N
∇wj J(θa) = E [λ (ynj − σnj) + (1 − λ)[pnj(θi) −
n=1
pnj(θo) − E ]ynk σnj (pnk(θi) − pnk(θo))] (14)
k
</equation>
<bodyText confidence="0.999904166666667">
In a way, the regularizers in our loss functions
are inspired from the data selection methods of
Axelrod et al. (2011), where they use cross entropy
between the in- and the out-domain LMs to score
out-domain sentences. However, our approach is
quite different from them in several aspects. First
</bodyText>
<equation confidence="0.83177625">
E
[pnj(θi) −
k
1263
</equation>
<bodyText confidence="0.9999495">
and most importantly, we take the scoring inside
model training and use it to bias the training to-
wards the in-domain model. Both the scoring and
the training are performed at the bilingual n-gram
level rather than at the sentence level. Integrating
scoring inside the model allows us to learn a robust
model by training/tuning the relevant parameters,
while still using the complete data. Secondly, our
models are based on NNs, while theirs utilize the
traditional Markov-based generative models.
</bodyText>
<subsectionHeader confidence="0.998168">
4.3 Technical Details
</subsectionHeader>
<bodyText confidence="0.999970285714286">
In this section, we describe some implementation
details of NDAM that we found to be crucial,
such as: using gradient clipping to handle vanish-
ing/exploding gradient problem in SGA training
with backpropagation, selecting appropriate noise
distribution in NCE, and special handling of out-
domain words that are unknown to the in-domain.
</bodyText>
<subsubsectionHeader confidence="0.552038">
4.3.1 Gradient Clipping
</subsubsectionHeader>
<bodyText confidence="0.99999325">
Two common issues with training deep NNs on
large data-sets are the vanishing and the exploding
gradients problems (Pascanu et al., 2013). The er-
ror gradients propagated by the backpropagation
may sometimes become very small or very large
which can lead to undesired (nan) values in weight
matrices, causing the training to fail. We also ex-
perienced the same problem in our NDAM quite
often. One simple solution to this problem is to
truncate the gradients, known as gradient clipping
(Mikolov, 2012). In our experiments, we limit the
gradients to be in the range [−5; +5].
</bodyText>
<subsectionHeader confidence="0.637684">
4.3.2 Noise Distribution in NCE
</subsectionHeader>
<bodyText confidence="0.999980933333333">
Training with NCE relies on sampling from a
noise distribution (i.e., ψ in Equation 5), and the
performance of the NDAM models varies consid-
erably with the choice of the distribution. We ex-
plored uniform and unigram noise distributions in
this work. With uniform distribution, every word
in the output vocabulary has the same probability
to be sampled as noise. The unigram noise dis-
tribution is a multinomial distribution over words
constructed by counting their occurrences in the
output (i.e., n-th word in the n-gram sequence).
In our experiments, unigram distribution delivered
much lower perplexity and better MT results com-
pared to the uniform one. Mnih and Teh (2012)
also reported similar findings on perplexity.
</bodyText>
<subsectionHeader confidence="0.979535">
4.3.3 Handling of Unknown Words
</subsectionHeader>
<bodyText confidence="0.999992114285714">
In order to reduce the training time and to learn
better word representations, NNLMs are often
trained on most frequent vocabulary words only
and low frequency words are represented under a
class of unknown words, unk. This results in a
large number of n-gram sequences containing at
least one unk word and thereby, makes unk a
highly probable word in the model.4
Our NDAM models rely on scoring out-domain
sequences (of word Ids) using models that are
trained based on the in-domain vocabulary. To
score out-domain sequences using a model, we
need to generate the sequences using the same vo-
cabulary based on which the model was trained.
In doing so, the out-domain words that are un-
known to the in-domain data map to the same unk
class. As a result, out-domain sequences contain-
ing unks get higher probability although they are
distant from the in-domain data.
A solution to this problem is to have an in-
domain model that can differentiate between its
own unk class, resulted from the reduced in-
domain vocabulary, and actual unknown words
that come from the out-domain data. We intro-
duce a new class unk, to represent the latter.
We train the in-domain model by adding a few
dummy sequences containing unk, occurring on
both source and target sides. This enables the
model to learn unk and unk, separately, where
unk, is a less probable class according to the
model. Later, the n-gram sequences of the out-
domain data contain both unk and unk, classes
depending on whether a word is unknown to only
pruned in-domain vocabulary (i.e., unk) or is un-
known to full in-domain vocabulary (i.e., unk,).
</bodyText>
<sectionHeader confidence="0.998549" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999766">
In this section, we describe the experimental
setup (i.e., data, settings for NN models and MT
pipeline) and the results. First we evaluate our
models intrinsically by comparing the perplexities
on a held-out in-domain testset against the base-
line NNJM model. Then we carry out an extrinsic
evaluation by using the NNJM and NDAM models
as features in machine translation and compare the
BLEU scores. Initial developmental experiments
were done on the Arabic-to-English language pair.
</bodyText>
<footnote confidence="0.980857">
4For our Arabic-English in-domain data, 30% of n-gram
sequences contain at least one unk word.
</footnote>
<page confidence="0.990641">
1264
</page>
<bodyText confidence="0.999066">
We carried out further experiments on the English-
to-German pair to validate our models.
</bodyText>
<subsectionHeader confidence="0.951037">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.993540227272727">
We experimented with the data made publicly
available for the translation task of the Interna-
tional Workshop on Spoken Language Translation
(IWSLT) (Cettolo et al., 2014). We used TED
talks as our in-domain corpus. For Arabic-to-
English, we used the QCRI Educational Domain
(QED) – A bilingual collection of educational lec-
tures5 (Abdelali et al., 2014), the News, and the
multiUN (UN) (Eisele and Chen, 2010) as our
out-domain corpora. For English-to-German, we
used the News, the Europarl (EP), and the Com-
mon Crawl (CC) corpora made available for the
9th Workshop of Statistical Machine Translation.6
Table 1 shows the size of the data used.
Training NN models is expensive. We, there-
fore, randomly selected subsets of about 300K
sentences from the bigger domains (UN, CC and
EP) to train the NN models.7 The systems were
tuned on concatenation of the dev. and test2010
and evaluated on test2011-2013 datasets. The tun-
ing set was also used to measure the perplexities
of different models.
</bodyText>
<subsectionHeader confidence="0.998735">
5.2 System Settings
</subsectionHeader>
<bodyText confidence="0.99997775">
NNJM &amp; NDAM: The NNJM models were
trained using NPLM8 toolkit (Vaswani et al.,
2013) with the following settings. We used a tar-
get context of 5 words and an aligned source win-
dow of 9 words, forming a joint stream of 14-
grams for training. We restricted source and tar-
get side vocabularies to the 20K and 40K most
frequent words. The word vector size D and the
hidden layer size were set to 150 and 750, respec-
tively. Only one hidden layer is used to allow
faster decoding. Training was done by the stan-
dard stochastic gradient ascent with NCE using
</bodyText>
<footnote confidence="0.983051928571428">
5Guzm´an et al. (2013) showed that the QED corpus is
similar to IWSLT and adding it improves translation quality.
6http://www.statmt.org/wmt14/translation-task.html
7Concatenating all the data results in a corpus of ap-
proximately 4.5 million sentences which requires roughly
18 days of wall-clock time (18 hours/epoch on a Linux
Ubuntu 12.04.5 LTS running on a 16 Core Intel Xeon E5-
2650 2.00Ghz and 64Gb RAM) to train NNJM models on
our machines. We ran one baseline experiment with all the
data and did not find it better than the system trained on ran-
domly selected subset of the data. In the interest of time, we
therefore reduced the NN training to a subset (800K and 1M
sentences for AR-EN and EN-DE respectively).
8http://nlg.isi.edu/software/nplm/
</footnote>
<table confidence="0.996524">
AR-EN EN-DE
Corpus Sent. Tok. Corpus Sent. Tok.
IWSLT 150k 2.8/3.0 IWSLT 177K 3.5/3.3
QED 150k 1.4/1.5 CC 2.3M 57/53
NEWS 203k 5.6/6.3 NEWS 200K 2.8/3.4
UN 3.7M 129/125 EP 1.8M 51/48
</table>
<tableCaption confidence="0.83705425">
Table 1: Statistics of the Arabic-English and
English-German training corpora in terms of Sen-
tences and Tokens (Source/Target). Tokens are
represented in millions.
</tableCaption>
<bodyText confidence="0.998517153846154">
100 noise samples and a mini-batch size of 1000.
All models were trained for 25 epochs. We used
identical settings to train the NDAM models, ex-
cept for the special handling of unk tokens.
Machine Translation System: We trained a
Moses system (Koehn et al., 2007), with the
following settings: a maximum sentence length
of 80, Fast-Aligner for word-alignments (Dyer et
al., 2013), an interpolated Kneser-Ney smoothed
5-gram language model with KenLM (Heafield,
2011), lexicalized reordering model (Galley and
Manning, 2008), a 5-gram operation sequence
model (Durrani et al., 2015b) and other default pa-
rameters. We also used an NNJM trained with the
settings described above as an additional feature
in our baseline system. In adapted systems, we
replaced the NNJM model with the NDAM mod-
els. We used ATB segmentation using the Stanford
ATB segmenter (Green and DeNero, 2012) for
Arabic-to-English and the default tokenizer pro-
vided with the Moses toolkit (Koehn et al., 2007)
for the English-to-German pair. Arabic OOVs
were translated using an unsupervised transliter-
ation module in Moses (Durrani et al., 2014). We
used k-best batch MIRA (Cherry and Foster, 2012)
for tuning.
</bodyText>
<subsectionHeader confidence="0.96801">
5.3 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.999952916666667">
In this section, we compare the NNJM model and
our NDAM models in terms of their perplexity
numbers on the in-domain held-out dataset (i.e.,
dev+test2010). We choose Arabic-English lan-
guage pair for the development experiments and
train domain-wise models to measure the related-
ness of each domain with respect to the in-domain.
We later replicated selective experiments for the
English-German language pair.
The first part of Table 2 summarizes the results
for Arabic-English. The perplexity numbers in the
second column (NNJMb) show that NEWS is the
</bodyText>
<page confidence="0.972603">
1265
</page>
<table confidence="0.9999245">
Domain NNJMb NNJMcat NDAM„1 NDAM„2
Arabic-English
IWSLT 12.55 - - -
QED 61.34 11.72 11.14 11.15
NEWS 42.88 10.88 10.67 10.59
UN 111.11 11.25 10.83 10.74
ALL - 10.31 10.08 10.22
English-German
IWSLT 10.20 – – –
ALL - 6.71 6.21 6.37
</table>
<tableCaption confidence="0.99249">
Table 2: Comparing the perplexity of NNJM
</tableCaption>
<bodyText confidence="0.95982368">
and NDAM models. NNJMb represents the model
trained on each individual domain separately.
most related domain from the perspective of in-
domain data, whereas UN is the farthest having
the worst perplexity. The third column (NNJMcat)
shows results of the models trained from concate-
nating each domain to the in-domain data. The
perplexity numbers improved significantly in each
case showing that there is useful information avail-
able in each domain which can be utilized to im-
prove the baseline. It also shows the robustness of
neural network models. Unlike the n-gram model,
the NN-based model improves generalization with
the increase in data without completely skewing
towards the dominating part of the data.
Concatenating in-domain with the NEWS data
gave better perplexities than other domains. Best
results were obtained by concatenating all the
data together (See row ALL). The third and fourth
columns show results of our models (NDAMv*).
Both give better perplexities than NNJMcat in all
cases. However, it is unclear which of the two
is better. Similar observations were made for the
English-to-German pair, where we only did exper-
iments on the concatenation of all domains.
</bodyText>
<subsectionHeader confidence="0.990889">
5.4 Extrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.999898166666667">
Arabic-to-English: For most language pairs,
the conventional wisdom is to train the system
with all available data. However, previously re-
ported MT results on Arabic-to-English (Mansour
and Ney, 2013) show that this is not optimal and
the results are often worse than only using in-
domain data. The reason for this is that the UN
domain is found to be distant and overwhelmingly
large as compared to the in-domain IWSLT data.
We carried out domain-wise experiments and also
found this to be true.
We considered three baseline systems: (i) Bin,
</bodyText>
<table confidence="0.911036">
SYS IWSLT QED NEWS UN ALL
26.1 - - - -
- 26.2 26.7 25.8 26.5
- 26.2 26.3 25.9 26.5
</table>
<tableCaption confidence="0.998545">
Table 3: Results of the baseline Arabic-to-English
</tableCaption>
<bodyText confidence="0.97864175">
MT systems. The numbers are averaged over
tst2011-2013.
which is trained on the in-domain data, (ii) Bcat,
which is trained on the concatenation of in- and
out-domain data, and (ii) Bcat,in, where the MT
pipeline was trained on the concatenation but the
NNJM model is trained only on the in-domain
data. Table 3 reports average BLEU scores across
three test sets on all domains. Adding QED and
NEWS domains gave improvements on top of the
in-domain IWSLT baseline. Concatenation of UN
with in-domain made the results worse. Concate-
nating all out-domain and in-domain data achieves
+0.4 BLEU gain on top of the baseline in-domain
system. We will use Bcat systems as our baseline
to compare our adapted systems with.
Table 4 shows results of the MT systems Sv1
and Sv2 using our adapted models NDAMv1 and
NDAMv2. We compare them to the baseline sys-
tem Bcat, which uses the non-adapted NNJMcat
as a feature. Sv1 achieved an improvement of up
to +0.4 and Sv2 achieved an improvement of up
to +0.5 BLEU points. However, Sv2 performs
slightly worse than Sv1 on individual domains.
We speculate this is because of the nature of the
NDAMv2, which gives high weight to out-domain
sequences that are liked by the in-domain model
and disliked by the out-domain model. In the case
of individual domains, NDAMv2 might be over pe-
nalizing out-domain since the out-domain model
is only built on that particular domain and always
prefers it more than the in-domain model. In case
of ALL, the out-domain model is more diverse and
has different level of likeness for each domain.
We analyzed the output of the baseline system
(Scat) and spotted several cases of lexical ambigu-
ity caused by out-domain data. For example, the
Arabic phrase �L;�� -�yU ZI-�UI J&amp;quot;I can be trans-
lated to choice overload or unwanted pregnancy.
The latter translation is incorrect in the context of
in-domain. The bias created due to the out-domain
data caused Scat to choose the contextually incor-
rect translation unwanted pregnancy. However,
the adapted systems Sv* were able to translate it
</bodyText>
<table confidence="0.864612545454546">
Bin
Bcat
Bcat,in
1266
tst11 QED tst13 tst11 NEWS tst13 tst11 UN tst13 tst11 ALL tst13
tst12 tst12 tst12 tst12
Bcat 25.0 27.3 26.2 25.4 27.6 27.1 24.7 27.0 25.8 25.0 27.5 27.0
Sv1 25.2 27.7 26.2 25.8 27.8 27.3 24.7 27.5 26.1 25.3 27.8 27.0
A +0.2 +0.4 0.0 +0.4 +0.2 +0.2 0.0 +0.5 +0.3 +0.3 +0.2 0.0
Sv2 25.1 27.6 26.2 25.6 27.9 27.2 24.6 27.2 26.1 25.5 27.9 26.9
A +0.1 +0.3 0.0 +0.2 +0.3 +0.1 -0.1 +0.2 +0.3 +0.5 +0.4 -0.1
</table>
<tableCaption confidence="0.995984">
Table 4: Arabic-to-English MT Results
</tableCaption>
<table confidence="0.999968454545455">
SYS tst11 tst12 tst13 Avg
Baselines
Bin 25.0 22.5 23.2 23.6
Bcat 25.7 22.9 24.1 24.2
Bcat,in 26.0 22.4 23.6 24.0
Comparison against NDAM
Bcat 25.7 22.9 24.1 24.2
Sv1 26.3 23.1 24.5 24.6
A +0.6 +0.2 +0.4 +0.4
Sv2 26.2 23.0 24.6 24.6
A +0.5 +0.1 +0.5 +0.4
</table>
<tableCaption confidence="0.999733">
Table 5: English-to-German MT Results
</tableCaption>
<bodyText confidence="0.996055115384615">
correctly. In another example 4 va���1 Z;U vs 13
(How about fitness?), the word ULJ is translated
to proprietary by Scat, a translation frequently ob-
served in the out-domain data. Sv∗ translated it
correctly to fitness, as preferred by the in-domain.
English-to-German: Concatenating all training
data to train the MT pipeline has been shown to
give the best results for English-to-German (Birch
et al., 2014). Therefore, we did not do domain-
wise experiments, except for training a system on
the in-domain IWSLT data for the sake of com-
pleteness. We also tried Bcat,in variation, i.e.
training an MT system on the entire data and using
in-domain data to train the baseline NNJM. The
baseline system Bcat gave better results and was
used as our reference for comparison.
Table 5 shows the results of our systems, Sv1
and Sv2, compared to the baselines, Bin and Bcat.
Unlike Arabic-to-English, the baseline system Bin
is much worse than Bcat. Our adapted MT systems
Sv1 and Sv2 both outperformed the best baseline
system (Bcat) with an improvement of up to 0.6
points. Sv2 performed slightly better than Sv1 on
one occasion and slightly worse in others.
Comparison with Data Selection: We also
compared our results with the MML-based data
</bodyText>
<table confidence="0.999800181818182">
SYS tst11 tst12 tst13 Avg
Arabic-to-English
Bcat 25.0 27.5 27.0 26.5
Sv1 25.3 27.8 27.0 26.7
Bmml 25.5 27.8 26.8 26.7
Sv1+mml 25.5 28.2 27.2 27.0
English-to-German
Bcat 25.7 22.9 24.1 24.2
Sv1 26.3 23.1 24.5 24.6
Bmml 25.1 22.7 23.9 23.9
Sv1+mml 25.4 22.8 23.9 24.0
</table>
<tableCaption confidence="0.999833">
Table 6: Comparison with Modified Moore-Lewis
</tableCaption>
<bodyText confidence="0.999718941176471">
selection approach as shown in Table 6. The
MML-based baseline systems (Bmml) used 20%
selected data for training the MT system and the
NNJM. On Arabic-English, both MML-based se-
lection and our model (Sv1) gave similar gains on
top of the baseline system (Bcat). Further results
showed that both approaches are complementary.
We were able to obtain an average gain of +0.3
BLEU points by training an NDAMv1 model over
the selected data (see Sv1+mml).
However, on English-German, the MML-based
selection caused a drop in the performance (see
Table 6). Training an adapted NDAMv1 model
over selected data gave improvements over MML
in two test sets but could not restore the baseline
performance, probably because the useful data has
already been filtered by the selection process.
</bodyText>
<sectionHeader confidence="0.993233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9994931">
We presented two novel models for domain adap-
tation based on NNJM. Adaptation is performed
by regularizing the loss function towards the in-
domain model and away from the unrelated out-
of-domain data. Our models show better perplex-
ities than the non-adapted baseline NNJM mod-
els. When integrated into a machine translation
system, gains of up to 0.5 and 0.6 BLEU points
were obtained in Arabic-to-English and English-
to-German systems over strong baselines.
</bodyText>
<page confidence="0.993675">
1267
</page>
<sectionHeader confidence="0.913033" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9987698">
Ahmed Abdelali, Francisco Guzman, Hassan Sajjad,
and Stephan Vogel. 2014. The AMARA corpus:
Building parallel language resources for the educa-
tional domain. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), Reykjavik, Iceland, May.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, Seattle, Wash-
ington, USA, October.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, Edinburgh, United Kingdom.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.
Alexandra Birch, Matthias Huck, Nadir Durrani, Niko-
lay Bogoychev, and Philipp Koehn. 2014. Ed-
inburgh SLT and MT system description for the
IWSLT 2014 evaluation. In Proceedings of the 11th
International Workshop on Spoken Language Trans-
lation, IWSLT ’14, Lake Tahoe, CA, USA.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based SMT adaptation. In Marcello Fed-
erico, Mei-Yuh Hwang, Margit R¨odder, and Sebas-
tian St¨uker, editors, Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 136–143.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs
Tamchyna. 2014. Findings of the 2014 workshop
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, Baltimore, Maryland, USA, June.
Marine Carpuat, Hal Daume III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. Sensespotting: Never let your par-
allel data tie you to an old domain. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), Sofia, Bulgaria, August.
Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa
Bentivogli, and Marcello Federico. 2014. Report on
the 11th IWSLT Evaluation Campaign. Proceedings
of the International Workshop on Spoken Language
Translation, Lake Tahoe, US.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech &amp; Language,
13(4):359–393.
Boxing Chen, George Foster, and Roland Kuhn.
2013a. Adaptation of reordering models for statisti-
cal machine translation. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Atlanta, Georgia, June.
Boxing Chen, Roland Kuhn, and George Foster.
2013b. Vector space model for adaptation in sta-
tistical machine translation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), Sofia,
Bulgaria, August.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT ’12, Montr´eal, Canada.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. volume 12, pages 2493–2537. JMLR. org.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).
Kevin Duh, Sudoh Katsuhito Neubig, Graham, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In Proceedings of the 51th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), Sofia, Bulgaria,
August.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an Unsupervised Translit-
eration Model into Statistical Machine Translation.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the ACL (EACL 2014), Gothenburg,
Sweden, April.
Nadir Durrani, Hassan Sajjad, Shafiq Joty, Ahmed Ab-
delali, and Stephan Vogel. 2015a. Using Joint Mod-
els for Domain Adaptation in Statistical Machine
Translation. In Proceedings of the Fifteenth Ma-
chine Translation Summit (MT Summit XV), Florida,
USA, To Appear. AMTA.
Nadir Durrani, Helmut Schmid, Alexander Fraser,
Philipp Koehn, and Hinrich Sch¨utze. 2015b. The
Operation Sequence Model – Combining N-Gram-
based and Phrase-based Statistical Machine Trans-
lation. Computational Linguistics, 41(2):157–186.
</reference>
<page confidence="0.906885">
1268
</page>
<reference confidence="0.999138">
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of NAACL’13.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), Jeju Island,
Korea, July.
Andreas Eisele and Yu Chen. 2010. MultiUN: A Mul-
tilingual Corpus from United Nation Documents. In
Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation, Valleta,
Malta, May.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In Proceedings of the Third Workshop on Sta-
tistical Machine Translation, Columbus, Ohio, June.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ’07.
George Foster and Roland Kuhn. 2009. Stabilizing
minimum error rate training. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ’09, Athens, Greece.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, Cambridge, MA,
October.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2010. Overview of the patent
translation task at the ntcir-8 workshop. In In
Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pages 293–
302.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 848–856, Honolulu, Hawaii, October.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), Baltimore, Maryland, June.
Spence Green and John DeNero. 2012. A class-
based agreement model for generating accurately in-
flected translations. In Proceedings of the Associ-
ation for Computational Linguistics (ACL’12), Jeju
Island, Korea.
Michael Gutmann and Aapo Hyv¨arinen. 2010. Noise-
contrastive estimation: A new estimation principle
for unnormalized statistical models. In Y.W. Teh and
M. Titterington, editors, Proc. Int. Conf. on Artificial
Intelligence and Statistics (AISTATS), volume 9 of
JMLR W&amp;CP, pages 297–304.
Francisco Guzm´an, Hassan Sajjad, Stephan Vogel, and
Ahmed Abdelali. 2013. The AMARA corpus:
Building resources for translating the web’s educa-
tional content. In Proceedings of the 10th Interna-
tional Workshop on Spoken Language Technology
(IWSLT-13), December.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics, Gothenburg, Swe-
den, April.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187–197, Edinburgh, Scotland, United King-
dom, July.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In Proceedings of the 10th
Conference of the European Association for Ma-
chine Translation (EAMT), Budapest, May.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl,
Abdel rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara
Sainath, and Brian Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recogni-
tion. Signal Processing Magazine.
Cuong Hoang and Khalil Sima’an. 2014. La-
tent domain translation models in mix-of-domains
haystack. In COLING 2014, 25th International
Conference on Computational Linguistics, Proceed-
ings of the Conference: Technical Papers, August
23-29, 2014, Dublin, Ireland.
Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In Proceedings
of the 2013 Conference on Empirical Methods in
Natural Language Processing, Seattle, Washington,
USA, October.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Association for Computational
Linguistics (ACL’07), Prague, Czech Republic.
Le Liu, Yu Hong, Hao Liu, Xing Wang, and Jianmin
Yao. 2014. Effective selection of translation model
</reference>
<page confidence="0.863529">
1269
</page>
<reference confidence="0.999611414634146">
training data. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), Baltimore, Mary-
land, June.
Saab Mansour and Hermann Ney. 2013. Phrase train-
ing based adaptation for statistical machine trans-
lation. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, June.
Prashant Mathur, Sriram Venkatapathy, and Nicola
Cancedda. 2014. Fast domain adaptation of smt
models without in-domain parallel data. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, Dublin, Ireland, August.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2, EMNLP ’09.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. HLT-NAACL, pages 746–
751.
Tomas Mikolov, 2012. Statistical Language Models
based on Neural Networks. PhD thesis, Brno Uni-
versity of Technology.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the Interna-
tional Conference on Machine Learning.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL’10), Uppsala, Sweden.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP’09),
Singapore.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics
(ACL’02), Philadelphia, PA, USA.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks. In Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, At-
lanta, GA, USA, 16-21 June 2013.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine trans-
lation. In Proceedings of COLING 2012: Posters,
Mumbai, India, December.
Rico Sennrich, Holger Schwenk, and Walid Aransa.
2013. A multi-domain translation model frame-
work for statistical machine translation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), Sofia, Bulgaria, August.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the As-
sociation for Computational Linguistics, Avignon,
France, April.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
455–465, Sofia, Bulgaria, August.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with large-scale
neural language models improves translation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing.
</reference>
<page confidence="0.987457">
1270
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367019">
<title confidence="0.995752">How to Avoid Unwanted Domain Adaptation using Neural Network Models</title>
<author confidence="0.723708666666667">Shafiq Joty</author>
<author confidence="0.723708666666667">Hassan Sajjad</author>
<author confidence="0.723708666666667">Nadir Kamla Al-Mannai</author>
<author confidence="0.723708666666667">Ahmed Abdelali</author>
<author confidence="0.723708666666667">Stephan Qatar Computing Research Institute</author>
<abstract confidence="0.998151277777778">We present novel models for domain adaptation based on the neural network joint model (NNJM). Our models maximize the cross entropy by regularizing the loss function with respect to in-domain model. Domain adaptation is carried out by assigning higher weight to out-domain sequences that are similar to the in-domain data. In our alternative model we take a more restrictive approach by additionally penalizing sequences similar to the outdomain data. Our models achieve better perplexities than the baseline NNJM models and give improvements of up to 0.5 and 0.6 BLEU points in Arabic-to-English and English-to-German language pairs, on a standard task of translating TED talks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abdelali</author>
<author>Francisco Guzman</author>
<author>Hassan Sajjad</author>
<author>Stephan Vogel</author>
</authors>
<title>The AMARA corpus: Building parallel language resources for the educational domain.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="24368" citStr="Abdelali et al., 2014" startWordPosition="4144" endWordPosition="4147">elopmental experiments were done on the Arabic-to-English language pair. 4For our Arabic-English in-domain data, 30% of n-gram sequences contain at least one unk word. 1264 We carried out further experiments on the Englishto-German pair to validate our models. 5.1 Data We experimented with the data made publicly available for the translation task of the International Workshop on Spoken Language Translation (IWSLT) (Cettolo et al., 2014). We used TED talks as our in-domain corpus. For Arabic-toEnglish, we used the QCRI Educational Domain (QED) – A bilingual collection of educational lectures5 (Abdelali et al., 2014), the News, and the multiUN (UN) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used the News, the Europarl (EP), and the Common Crawl (CC) corpora made available for the 9th Workshop of Statistical Machine Translation.6 Table 1 shows the size of the data used. Training NN models is expensive. We, therefore, randomly selected subsets of about 300K sentences from the bigger domains (UN, CC and EP) to train the NN models.7 The systems were tuned on concatenation of the dev. and test2010 and evaluated on test2011-2013 datasets. The tuning set was also used to measure</context>
</contexts>
<marker>Abdelali, Guzman, Sajjad, Vogel, 2014</marker>
<rawString>Ahmed Abdelali, Francisco Guzman, Hassan Sajjad, and Stephan Vogel. 2014. The AMARA corpus: Building parallel language resources for the educational domain. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="9196" citStr="Auli et al., 2013" startWordPosition="1470" endWordPosition="1473">rge to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<location>Edinburgh, United Kingdom.</location>
<contexts>
<context position="6413" citStr="Axelrod et al. (2011)" startWordPosition="1027" endWordPosition="1030">nces, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An al</context>
<context position="19287" citStr="Axelrod et al. (2011)" startWordPosition="3314" endWordPosition="3317">e above cost function with reE E spect to the final layer weight vectors wj are: n=1 k=1 M pnk(θi) log σnk + E ynklog ψnk] (10) m=1 We use SGA with backpropagation to train this model. The derivatives of J(Ba) with respect to the final layer weight vectors wj turn out to be: N ∇wj J(θa) = E [λ (ynj − σnj) + (1 − λ) n=1 ]ynk pnk(θi) σnj] (11) 3We used a balanced value λ = 0.5 for our experiments. N ∇wj J(θa) = E [λ (ynj − σnj) + (1 − λ)[pnj(θi) − n=1 pnj(θo) − E ]ynk σnj (pnk(θi) − pnk(θo))] (14) k In a way, the regularizers in our loss functions are inspired from the data selection methods of Axelrod et al. (2011), where they use cross entropy between the in- and the out-domain LMs to score out-domain sentences. However, our approach is quite different from them in several aspects. First E [pnj(θi) − k 1263 and most importantly, we take the scoring inside model training and use it to bias the training towards the in-domain model. Both the scoring and the training are performed at the bilingual n-gram level rather than at the sentence level. Integrating scoring inside the model allows us to learn a robust model by training/tuning the relevant parameters, while still using the complete data. Secondly, ou</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, Edinburgh, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1137</pages>
<contexts>
<context position="9177" citStr="Bengio et al., 2003" startWordPosition="1466" endWordPosition="1469"> n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Matthias Huck</author>
<author>Nadir Durrani</author>
<author>Nikolay Bogoychev</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh SLT and MT system description for the IWSLT 2014 evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 11th International Workshop on Spoken Language Translation, IWSLT ’14,</booktitle>
<location>Lake Tahoe, CA, USA.</location>
<contexts>
<context position="33814" citStr="Birch et al., 2014" startWordPosition="5721" endWordPosition="5724">.2 Bcat,in 26.0 22.4 23.6 24.0 Comparison against NDAM Bcat 25.7 22.9 24.1 24.2 Sv1 26.3 23.1 24.5 24.6 A +0.6 +0.2 +0.4 +0.4 Sv2 26.2 23.0 24.6 24.6 A +0.5 +0.1 +0.5 +0.4 Table 5: English-to-German MT Results correctly. In another example 4 va���1 Z;U vs 13 (How about fitness?), the word ULJ is translated to proprietary by Scat, a translation frequently observed in the out-domain data. Sv∗ translated it correctly to fitness, as preferred by the in-domain. English-to-German: Concatenating all training data to train the MT pipeline has been shown to give the best results for English-to-German (Birch et al., 2014). Therefore, we did not do domainwise experiments, except for training a system on the in-domain IWSLT data for the sake of completeness. We also tried Bcat,in variation, i.e. training an MT system on the entire data and using in-domain data to train the baseline NNJM. The baseline system Bcat gave better results and was used as our reference for comparison. Table 5 shows the results of our systems, Sv1 and Sv2, compared to the baselines, Bin and Bcat. Unlike Arabic-to-English, the baseline system Bin is much worse than Bcat. Our adapted MT systems Sv1 and Sv2 both outperformed the best baseli</context>
</contexts>
<marker>Birch, Huck, Durrani, Bogoychev, Koehn, 2014</marker>
<rawString>Alexandra Birch, Matthias Huck, Nadir Durrani, Nikolay Bogoychev, and Philipp Koehn. 2014. Edinburgh SLT and MT system description for the IWSLT 2014 evaluation. In Proceedings of the 11th International Workshop on Spoken Language Translation, IWSLT ’14, Lake Tahoe, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus interpolation methods for phrase-based SMT adaptation.</title>
<date>2011</date>
<booktitle>Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>136--143</pages>
<editor>In Marcello Federico, Mei-Yuh Hwang, Margit R¨odder, and Sebastian St¨uker, editors,</editor>
<contexts>
<context position="7651" citStr="Bisazza et al., 2011" startWordPosition="1219" endWordPosition="1222">etely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasl</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus interpolation methods for phrase-based SMT adaptation. In Marcello Federico, Mei-Yuh Hwang, Margit R¨odder, and Sebastian St¨uker, editors, Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
</authors>
<title>Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna.</title>
<date>2014</date>
<journal>Findings of the</journal>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="1556" citStr="Bojar et al., 2014" startWordPosition="230" endWordPosition="233">pairs, on a standard task of translating TED talks. 1 Introduction Rapid influx of digital data has galvanized the use of empirical methods in many fields including Machine Translation (MT). The increasing availability of bilingual corpora has made it possible to automatically learn translation rules that required years of linguistic analysis previously. While additional data is often beneficial for a general purpose Statistical Machine Translation (SMT) system, a problem arises when translating new domains such as lectures (Cettolo et al., 2014), patents (Fujii et al., 2010) or medical text (Bojar et al., 2014), where either the bilingual text does not exist or is available in small quantity. All domains have their own vocabulary and stylistic preferences which cannot be fully encompassed by a system trained on the general domain. Machine translation systems trained from a simple concatenation of small in-domain and large out-domain data often perform below par because the out-domain data is distant or overwhelmingly larger than the in-domain data. Additional data increases lexical ambiguity by introducing new senses to the existing in-domain vocabulary. For example, an Arabic-to-English SMT system </context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Hal Daume Katharine Henry</author>
<author>Ann Irvine</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Rachel Rudinger</author>
</authors>
<title>Sensespotting: Never let your parallel data tie you to an old domain.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8403" citStr="Carpuat et al., 2013" startWordPosition="1340" endWordPosition="1343">ture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In r</context>
</contexts>
<marker>Carpuat, Henry, Irvine, Jagarlamudi, Rudinger, 2013</marker>
<rawString>Marine Carpuat, Hal Daume III, Katharine Henry, Ann Irvine, Jagadeesh Jagarlamudi, and Rachel Rudinger. 2013. Sensespotting: Never let your parallel data tie you to an old domain. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Jan Niehues</author>
<author>Sebastian St¨uker</author>
<author>Luisa Bentivogli</author>
<author>Marcello Federico</author>
</authors>
<date>2014</date>
<booktitle>Report on the 11th IWSLT Evaluation Campaign. Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<location>Lake Tahoe, US.</location>
<marker>Cettolo, Niehues, St¨uker, Bentivogli, Federico, 2014</marker>
<rawString>Mauro Cettolo, Jan Niehues, Sebastian St¨uker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th IWSLT Evaluation Campaign. Proceedings of the International Workshop on Spoken Language Translation, Lake Tahoe, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="4984" citStr="Chen and Goodman, 1999" startWordPosition="780" endWordPosition="783">e large outdomain data. To this end, we propose two novel extensions of NNJM for domain adaptation. Our first model minimizes the cross entropy by regularizing the loss function with respect to the in-domain model. The regularizer gives higher weight to the training instances that are similar to the in-domain data. Our second model takes a more conservative approach by additionally penalizing data instances similar to the out-domain data. We evaluate our models on the standard task of translating Arabic-English and English-German language pairs. Our adapted models achieve better perplexities (Chen and Goodman, 1999) than the models trained on in- and in+out-domain data. Improvements are also reflected in BLEU scores (Papineni et al., 2002) as we compare these models within the SMT pipeline. We obtain gains of up to 0.5 and 0.6 on Arabic-English and EnglishGerman pairs over a competitive baseline system. The remainder of this paper is organized as follows: Section 2 gives an account on related work. Section 3 revisits NNJM model and Section 4 discusses our models. Section 5 presents the experimental setup and the results. Section 6 concludes. 2 Related Work Previous work on domain adaptation in MT can be </context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech &amp; Language, 13(4):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Adaptation of reordering models for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="7864" citStr="Chen et al. (2013" startWordPosition="1252" endWordPosition="1255">domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ</context>
</contexts>
<marker>Chen, Foster, Kuhn, 2013</marker>
<rawString>Boxing Chen, George Foster, and Roland Kuhn. 2013a. Adaptation of reordering models for statistical machine translation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
</authors>
<title>Vector space model for adaptation in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7864" citStr="Chen et al. (2013" startWordPosition="1252" endWordPosition="1255">domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ</context>
</contexts>
<marker>Chen, Kuhn, Foster, 2013</marker>
<rawString>Boxing Chen, Roland Kuhn, and George Foster. 2013b. Vector space model for adaptation in statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’12,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="27848" citStr="Cherry and Foster, 2012" startWordPosition="4718" endWordPosition="4721"> sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the second column (NNJMb) show that NEWS is the 1265 Domai</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’12, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<volume>12</volume>
<pages>2493--2537</pages>
<note>JMLR. org.</note>
<contexts>
<context position="9285" citStr="Collobert et al., 2011" startWordPosition="1484" endWordPosition="1487">presentation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word source window for the target word ti based on the one-to-on</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. volume 12, pages 2493–2537. JMLR. org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</booktitle>
<volume>1</volume>
<institution>Long Papers).</institution>
<contexts>
<context position="3435" citStr="Devlin et al. (2014)" startWordPosition="540" endWordPosition="543">the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our aim in this paper is to advance the state-ofthe-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out1259 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1259–1270, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguist</context>
<context position="9383" citStr="Devlin et al. (2014)" startWordPosition="1502" endWordPosition="1505"> models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word source window for the target word ti based on the one-to-one (non-NULL) alignment of T to S. As exemplified in Figure 1, this is essentially a (p + q)-gram n</context>
<context position="11441" citStr="Devlin et al. (2014)" startWordPosition="1851" endWordPosition="1854"> where φ(xn) defines the transformations of xn through the hidden layers, and wk are the weights from the last hidden layer to the output layer. For notational simplicity, henceforth we will use (xn, yn) to represent a training sequence. By setting p and q to be sufficiently large, NNJM can capture long-range cross-lingual dependencies between words, while still overcoming the data sparseness issue by virtue of its distributed representations (i.e., word vectors). A major bottleneck, however, is to surmount the computational cost involved in training the model and applying it for MT decoding. Devlin et al. (2014) proposed two tricks to speed up computation in decoding. The first one is to pre-compute the hidden layer computations and fetch them directly as needed during decoding. The second technique is to train a self-normalized NNJM to avoid computation of the softmax normalization factor (i.e., the denominator in Equation 2) in decoding. However, self-normalization does not solve the computational cost of training the model. In the following, we describe a method to address this issue. 3.1 Training by Noise Contrastive Estimation The standard way to train NNLMs is to maximize the log likelihood of </context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Sudoh Katsuhito Neubig</author>
<author>Graham</author>
<author>Hajime Tsukada</author>
</authors>
<title>Adaptation data selection using neural language models: Experiments in machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6570" citStr="Duh et al. (2013)" startWordPosition="1053" endWordPosition="1056">a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advanta</context>
</contexts>
<marker>Duh, Neubig, Graham, Tsukada, 2013</marker>
<rawString>Kevin Duh, Sudoh Katsuhito Neubig, Graham, and Hajime Tsukada. 2013. Adaptation data selection using neural language models: Experiments in machine translation. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Hassan Sajjad</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Integrating an Unsupervised Transliteration Model into Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014),</booktitle>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="27795" citStr="Durrani et al., 2014" startWordPosition="4709" endWordPosition="4712">del (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the s</context>
</contexts>
<marker>Durrani, Sajjad, Hoang, Koehn, 2014</marker>
<rawString>Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp Koehn. 2014. Integrating an Unsupervised Transliteration Model into Statistical Machine Translation. In Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014), Gothenburg, Sweden, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Hassan Sajjad</author>
<author>Shafiq Joty</author>
<author>Ahmed Abdelali</author>
<author>Stephan Vogel</author>
</authors>
<title>Using Joint Models for Domain Adaptation in Statistical Machine Translation.</title>
<date>2015</date>
<booktitle>In Proceedings of the Fifteenth Machine Translation Summit (MT Summit XV),</booktitle>
<location>Florida, USA, To Appear. AMTA.</location>
<contexts>
<context position="6800" citStr="Durrani et al. (2015" startWordPosition="1092" endWordPosition="1095">ensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting</context>
<context position="27261" citStr="Durrani et al., 2015" startWordPosition="4623" endWordPosition="4626">et). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. </context>
</contexts>
<marker>Durrani, Sajjad, Joty, Abdelali, Vogel, 2015</marker>
<rawString>Nadir Durrani, Hassan Sajjad, Shafiq Joty, Ahmed Abdelali, and Stephan Vogel. 2015a. Using Joint Models for Domain Adaptation in Statistical Machine Translation. In Proceedings of the Fifteenth Machine Translation Summit (MT Summit XV), Florida, USA, To Appear. AMTA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
<author>Philipp Koehn</author>
<author>Hinrich Sch¨utze</author>
</authors>
<booktitle>2015b. The Operation Sequence Model – Combining N-Grambased and Phrase-based Statistical Machine Translation. Computational Linguistics,</booktitle>
<pages>41--2</pages>
<marker>Durrani, Schmid, Fraser, Koehn, Sch¨utze, </marker>
<rawString>Nadir Durrani, Helmut Schmid, Alexander Fraser, Philipp Koehn, and Hinrich Sch¨utze. 2015b. The Operation Sequence Model – Combining N-Grambased and Phrase-based Statistical Machine Translation. Computational Linguistics, 41(2):157–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of ibm model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL’13.</booktitle>
<contexts>
<context position="27060" citStr="Dyer et al., 2013" startWordPosition="4596" endWordPosition="4599">.3M 57/53 NEWS 203k 5.6/6.3 NEWS 200K 2.8/3.4 UN 3.7M 129/125 EP 1.8M 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) f</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proceedings of NAACL’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Eidelman</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Topic models for dynamic translation model adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="8245" citStr="Eidelman et al., 2012" startWordPosition="1314" endWordPosition="1317"> 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss </context>
</contexts>
<marker>Eidelman, Boyd-Graber, Resnik, 2012</marker>
<rawString>Vladimir Eidelman, Jordan Boyd-Graber, and Philip Resnik. 2012. Topic models for dynamic translation model adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Eisele</author>
<author>Yu Chen</author>
</authors>
<title>MultiUN: A Multilingual Corpus from United Nation Documents.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation,</booktitle>
<location>Valleta, Malta,</location>
<contexts>
<context position="24424" citStr="Eisele and Chen, 2010" startWordPosition="4154" endWordPosition="4157">h language pair. 4For our Arabic-English in-domain data, 30% of n-gram sequences contain at least one unk word. 1264 We carried out further experiments on the Englishto-German pair to validate our models. 5.1 Data We experimented with the data made publicly available for the translation task of the International Workshop on Spoken Language Translation (IWSLT) (Cettolo et al., 2014). We used TED talks as our in-domain corpus. For Arabic-toEnglish, we used the QCRI Educational Domain (QED) – A bilingual collection of educational lectures5 (Abdelali et al., 2014), the News, and the multiUN (UN) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used the News, the Europarl (EP), and the Common Crawl (CC) corpora made available for the 9th Workshop of Statistical Machine Translation.6 Table 1 shows the size of the data used. Training NN models is expensive. We, therefore, randomly selected subsets of about 300K sentences from the bigger domains (UN, CC and EP) to train the NN models.7 The systems were tuned on concatenation of the dev. and test2010 and evaluated on test2011-2013 datasets. The tuning set was also used to measure the perplexities of different models. 5.2 System Settin</context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>Andreas Eisele and Yu Chen. 2010. MultiUN: A Multilingual Corpus from United Nation Documents. In Proceedings of the Seventh conference on International Language Resources and Evaluation, Valleta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Dynamic model interpolation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7559" citStr="Finch and Sumita, 2008" startWordPosition="1204" endWordPosition="1207">that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptati</context>
</contexts>
<marker>Finch, Sumita, 2008</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2008. Dynamic model interpolation for statistical machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for smt.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07.</booktitle>
<contexts>
<context position="3108" citStr="Foster and Kuhn, 2007" startWordPosition="484" endWordPosition="487"> taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our a</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for smt. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Stabilizing minimum error rate training.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="7629" citStr="Foster and Kuhn, 2009" startWordPosition="1215" endWordPosition="1218">An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidel</context>
</contexts>
<marker>Foster, Kuhn, 2009</marker>
<rawString>George Foster and Roland Kuhn. 2009. Stabilizing minimum error rate training. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="7373" citStr="Foster et al. (2010)" startWordPosition="1179" endWordPosition="1182"> 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Utiyama</author>
<author>Mikio Yamamoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Overview of the patent translation task at the ntcir-8 workshop. In</title>
<date>2010</date>
<booktitle>In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access,</booktitle>
<pages>293--302</pages>
<contexts>
<context position="1519" citStr="Fujii et al., 2010" startWordPosition="223" endWordPosition="226">glish and English-to-German language pairs, on a standard task of translating TED talks. 1 Introduction Rapid influx of digital data has galvanized the use of empirical methods in many fields including Machine Translation (MT). The increasing availability of bilingual corpora has made it possible to automatically learn translation rules that required years of linguistic analysis previously. While additional data is often beneficial for a general purpose Statistical Machine Translation (SMT) system, a problem arises when translating new domains such as lectures (Cettolo et al., 2014), patents (Fujii et al., 2010) or medical text (Bojar et al., 2014), where either the bilingual text does not exist or is available in small quantity. All domains have their own vocabulary and stylistic preferences which cannot be fully encompassed by a system trained on the general domain. Machine translation systems trained from a simple concatenation of small in-domain and large out-domain data often perform below par because the out-domain data is distant or overwhelmingly larger than the in-domain data. Additional data increases lexical ambiguity by introducing new senses to the existing in-domain vocabulary. For exam</context>
</contexts>
<marker>Fujii, Utiyama, Yamamoto, Utsuro, 2010</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and Takehito Utsuro. 2010. Overview of the patent translation task at the ntcir-8 workshop. In In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access, pages 293– 302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="27204" citStr="Galley and Manning, 2008" startWordPosition="4614" endWordPosition="4617">training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848–856, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="9246" citStr="Gao et al., 2014" startWordPosition="1478" endWordPosition="1481">s. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word source window for th</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2014. Learning continuous phrase representations for translation modeling. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>John DeNero</author>
</authors>
<title>A classbased agreement model for generating accurately inflected translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL’12),</booktitle>
<location>Jeju Island,</location>
<contexts>
<context position="27557" citStr="Green and DeNero, 2012" startWordPosition="4673" endWordPosition="4676">al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the </context>
</contexts>
<marker>Green, DeNero, 2012</marker>
<rawString>Spence Green and John DeNero. 2012. A classbased agreement model for generating accurately inflected translations. In Proceedings of the Association for Computational Linguistics (ACL’12), Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gutmann</author>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>Noisecontrastive estimation: A new estimation principle for unnormalized statistical models.</title>
<date>2010</date>
<booktitle>Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS),</booktitle>
<volume>9</volume>
<pages>297--304</pages>
<editor>In Y.W. Teh and M. Titterington, editors,</editor>
<marker>Gutmann, Hyv¨arinen, 2010</marker>
<rawString>Michael Gutmann and Aapo Hyv¨arinen. 2010. Noisecontrastive estimation: A new estimation principle for unnormalized statistical models. In Y.W. Teh and M. Titterington, editors, Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS), volume 9 of JMLR W&amp;CP, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Guzm´an</author>
<author>Hassan Sajjad</author>
<author>Stephan Vogel</author>
<author>Ahmed Abdelali</author>
</authors>
<title>The AMARA corpus: Building resources for translating the web’s educational content.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13),</booktitle>
<marker>Guzm´an, Sajjad, Vogel, Abdelali, 2013</marker>
<rawString>Francisco Guzm´an, Hassan Sajjad, Stephan Vogel, and Ahmed Abdelali. 2013. The AMARA corpus: Building resources for translating the web’s educational content. In Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Phil Blunsom</author>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Dynamic topic adaptation for phrase-based mt.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="8267" citStr="Hasler et al., 2014" startWordPosition="1318" endWordPosition="1321">2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptati</context>
</contexts>
<marker>Hasler, Blunsom, Koehn, Haddow, 2014</marker>
<rawString>Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry Haddow. 2014. Dynamic topic adaptation for phrase-based mt. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, United Kingdom,</location>
<contexts>
<context position="27147" citStr="Heafield, 2011" startWordPosition="4609" endWordPosition="4610">stics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transl</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, United Kingdom, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<location>Budapest,</location>
<contexts>
<context position="6342" citStr="Hildebrand et al., 2005" startWordPosition="1016" endWordPosition="1019">o be an effective way to discard poor quality or irrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that </context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT), Budapest, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>Li Deng</author>
<author>Dong Yu</author>
<author>George Dahl</author>
<author>Abdel rahman Mohamed</author>
<author>Navdeep Jaitly</author>
</authors>
<title>Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine.</title>
<date>2012</date>
<location>Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and</location>
<contexts>
<context position="9351" citStr="Hinton et al., 2012" startWordPosition="1496" endWordPosition="1500">ssue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word source window for the target word ti based on the one-to-one (non-NULL) alignment of T to S. As exemplified in Figure 1, this</context>
</contexts>
<marker>Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, 2012</marker>
<rawString>Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. 2012. Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cuong Hoang</author>
<author>Khalil Sima’an</author>
</authors>
<title>Latent domain translation models in mix-of-domains haystack.</title>
<date>2014</date>
<booktitle>In COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers,</booktitle>
<location>Dublin, Ireland.</location>
<marker>Hoang, Sima’an, 2014</marker>
<rawString>Cuong Hoang and Khalil Sima’an. 2014. Latent domain translation models in mix-of-domains haystack. In COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, August 23-29, 2014, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="9228" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1474" endWordPosition="1477">-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word so</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL’07),</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="26944" citStr="Koehn et al., 2007" startWordPosition="4579" endWordPosition="4582">are/nplm/ AR-EN EN-DE Corpus Sent. Tok. Corpus Sent. Tok. IWSLT 150k 2.8/3.0 IWSLT 177K 3.5/3.3 QED 150k 1.4/1.5 CC 2.3M 57/53 NEWS 203k 5.6/6.3 NEWS 200K 2.8/3.4 UN 3.7M 129/125 EP 1.8M 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Association for Computational Linguistics (ACL’07), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hong Le Liu</author>
<author>Hao Liu</author>
<author>Xing Wang</author>
<author>Jianmin Yao</author>
</authors>
<title>Effective selection of translation model training data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<location>Baltimore, Maryland,</location>
<marker>Le Liu, Liu, Wang, Yao, 2014</marker>
<rawString>Le Liu, Yu Hong, Hao Liu, Xing Wang, and Jianmin Yao. 2014. Effective selection of translation model training data. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saab Mansour</author>
<author>Hermann Ney</author>
</authors>
<title>Phrase training based adaptation for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="7737" citStr="Mansour and Ney, 2013" startWordPosition="1231" endWordPosition="1234">It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Ma</context>
<context position="30125" citStr="Mansour and Ney, 2013" startWordPosition="5078" endWordPosition="5081">er domains. Best results were obtained by concatenating all the data together (See row ALL). The third and fourth columns show results of our models (NDAMv*). Both give better perplexities than NNJMcat in all cases. However, it is unclear which of the two is better. Similar observations were made for the English-to-German pair, where we only did experiments on the concatenation of all domains. 5.4 Extrinsic Evaluation Arabic-to-English: For most language pairs, the conventional wisdom is to train the system with all available data. However, previously reported MT results on Arabic-to-English (Mansour and Ney, 2013) show that this is not optimal and the results are often worse than only using indomain data. The reason for this is that the UN domain is found to be distant and overwhelmingly large as compared to the in-domain IWSLT data. We carried out domain-wise experiments and also found this to be true. We considered three baseline systems: (i) Bin, SYS IWSLT QED NEWS UN ALL 26.1 - - - - - 26.2 26.7 25.8 26.5 - 26.2 26.3 25.9 26.5 Table 3: Results of the baseline Arabic-to-English MT systems. The numbers are averaged over tst2011-2013. which is trained on the in-domain data, (ii) Bcat, which is trained</context>
</contexts>
<marker>Mansour, Ney, 2013</marker>
<rawString>Saab Mansour and Hermann Ney. 2013. Phrase training based adaptation for statistical machine translation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prashant Mathur</author>
<author>Sriram Venkatapathy</author>
<author>Nicola Cancedda</author>
</authors>
<title>Fast domain adaptation of smt models without in-domain parallel data.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="8355" citStr="Mathur et al., 2014" startWordPosition="1333" endWordPosition="1336">3). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability </context>
</contexts>
<marker>Mathur, Venkatapathy, Cancedda, 2014</marker>
<rawString>Prashant Mathur, Sriram Venkatapathy, and Nicola Cancedda. 2014. Fast domain adaptation of smt models without in-domain parallel data. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2, EMNLP ’09.</booktitle>
<contexts>
<context position="3064" citStr="Matsoukas et al., 2009" startWordPosition="476" endWordPosition="480">ice overload”. The sense of the Arabic phrase taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as</context>
<context position="7278" citStr="Matsoukas et al. (2009)" startWordPosition="1167" endWordPosition="1170">o the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vect</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2, EMNLP ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<contexts>
<context position="9307" citStr="Mikolov et al., 2013" startWordPosition="1488" endWordPosition="1491">al network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word source window for the target word ti based on the one-to-one (non-NULL) alignment</context>
<context position="15097" citStr="Mikolov et al., 2013" startWordPosition="2520" endWordPosition="2523">terms, Equation 6 can be further simplified as: i ynk log ψnk m (7) where ψnk =P(ymn = k|Xn, ψ) is the noise distribution, Unk =U(yn = k|Xn, θ) is the unnormalized score at the output layer (Equation 4), and ynk and ymnk are indicator variables as defined before. NCE reduces the number of computations needed at the output layer from |Vo |to M + 1, where M is a small number in comparison with |Vo|. In all our experiments we use NCE loss with M = 100 samples as suggested by Mnih and Teh (2012). 4 Neural Domain Adaptation Models The ability to generalize and learn complex semantic relationships (Mikolov et al., 2013b) and its compelling empirical results gives a strong motivation to use the NNJM model for the problem of domain adaptation in machine translation. However, the vanilla NNJM described above is limited in its ability to effectively learn from a large and diverse out-domain data in the best favor of an indomain data. To address this, we propose two neural domain adaptation models (NDAM) extending the NNJM model. Our models add regularization to its loss function either with respect to in-domain or both in- and out-domains. In both cases, we first present the regularized loss function for the no</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations. HLT-NAACL,</title>
<date>2013</date>
<pages>746--751</pages>
<contexts>
<context position="9307" citStr="Mikolov et al., 2013" startWordPosition="1488" endWordPosition="1491">al network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word source window for the target word ti based on the one-to-one (non-NULL) alignment</context>
<context position="15097" citStr="Mikolov et al., 2013" startWordPosition="2520" endWordPosition="2523">terms, Equation 6 can be further simplified as: i ynk log ψnk m (7) where ψnk =P(ymn = k|Xn, ψ) is the noise distribution, Unk =U(yn = k|Xn, θ) is the unnormalized score at the output layer (Equation 4), and ynk and ymnk are indicator variables as defined before. NCE reduces the number of computations needed at the output layer from |Vo |to M + 1, where M is a small number in comparison with |Vo|. In all our experiments we use NCE loss with M = 100 samples as suggested by Mnih and Teh (2012). 4 Neural Domain Adaptation Models The ability to generalize and learn complex semantic relationships (Mikolov et al., 2013b) and its compelling empirical results gives a strong motivation to use the NNJM model for the problem of domain adaptation in machine translation. However, the vanilla NNJM described above is limited in its ability to effectively learn from a large and diverse out-domain data in the best favor of an indomain data. To address this, we propose two neural domain adaptation models (NDAM) extending the NNJM model. Our models add regularization to its loss function either with respect to in-domain or both in- and out-domains. In both cases, we first present the regularized loss function for the no</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. HLT-NAACL, pages 746– 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Statistical Language Models based on Neural Networks.</title>
<date>2012</date>
<tech>PhD thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="20865" citStr="Mikolov, 2012" startWordPosition="3567" endWordPosition="3568">bution in NCE, and special handling of outdomain words that are unknown to the in-domain. 4.3.1 Gradient Clipping Two common issues with training deep NNs on large data-sets are the vanishing and the exploding gradients problems (Pascanu et al., 2013). The error gradients propagated by the backpropagation may sometimes become very small or very large which can lead to undesired (nan) values in weight matrices, causing the training to fail. We also experienced the same problem in our NDAM quite often. One simple solution to this problem is to truncate the gradients, known as gradient clipping (Mikolov, 2012). In our experiments, we limit the gradients to be in the range [−5; +5]. 4.3.2 Noise Distribution in NCE Training with NCE relies on sampling from a noise distribution (i.e., ψ in Equation 5), and the performance of the NDAM models varies considerably with the choice of the distribution. We explored uniform and unigram noise distributions in this work. With uniform distribution, every word in the output vocabulary has the same probability to be sampled as noise. The unigram noise distribution is a multinomial distribution over words constructed by counting their occurrences in the output (i.e</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov, 2012. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="12739" citStr="Mnih and Teh, 2012" startWordPosition="2063" endWordPosition="2066">icator variable (i.e., ynk=1 when yn=k, otherwise 0). Optimization is performed using first-order online methods, such as stochastic gradient ascent (SGA) with standard backpropagation algorithm. Unfortunately, training NNLMs are impractically slow because for each training instance (xn, yn), the softmax output layer (see Equation 2) needs to compute a summation over all words in the output vocabulary.2 Noise contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010) provides an efficient and stable way to avoid this repetitive computation as recently applied to NNLMs (Vaswani et al., 2013; Mnih and Teh, 2012). We can re-write Equation 2 as follows: σ(yn = k|xn, θ) P(yn = k|xn, θ) = (4) Z(φ(xn), W) where Q(.) is the un-normalized score and Z(.) is the normalization factor. In NCE, we consider 2This would take few weeks for a modern CPU machine to train a single NNJM model on the whole data. |Vo | E k=1 J(θ) = N E n=1 1261 Look-up Hidden Output layer layer layer Xn φ(Xn) U W yn ymn C M ψ 7r Source token 1 Source token 2 Source token 3 Target token 1 Target token 2 Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target words (i.e., 2-words history) a</context>
<context position="14973" citStr="Mnih and Teh (2012)" startWordPosition="2501" endWordPosition="2504">Q (6) where Q = P(yn, C = 1|Xn, θ, 7r) + P(ymn , C = 0|Xn, ψ, 7r) is a normalization constant. After removing the constant terms, Equation 6 can be further simplified as: i ynk log ψnk m (7) where ψnk =P(ymn = k|Xn, ψ) is the noise distribution, Unk =U(yn = k|Xn, θ) is the unnormalized score at the output layer (Equation 4), and ynk and ymnk are indicator variables as defined before. NCE reduces the number of computations needed at the output layer from |Vo |to M + 1, where M is a small number in comparison with |Vo|. In all our experiments we use NCE loss with M = 100 samples as suggested by Mnih and Teh (2012). 4 Neural Domain Adaptation Models The ability to generalize and learn complex semantic relationships (Mikolov et al., 2013b) and its compelling empirical results gives a strong motivation to use the NNJM model for the problem of domain adaptation in machine translation. However, the vanilla NNJM described above is limited in its ability to effectively learn from a large and diverse out-domain data in the best favor of an indomain data. To address this, we propose two neural domain adaptation models (NDAM) extending the NNJM model. Our models add regularization to its loss function either wit</context>
<context position="21646" citStr="Mnih and Teh (2012)" startWordPosition="3693" endWordPosition="3696">bution (i.e., ψ in Equation 5), and the performance of the NDAM models varies considerably with the choice of the distribution. We explored uniform and unigram noise distributions in this work. With uniform distribution, every word in the output vocabulary has the same probability to be sampled as noise. The unigram noise distribution is a multinomial distribution over words constructed by counting their occurrences in the output (i.e., n-th word in the n-gram sequence). In our experiments, unigram distribution delivered much lower perplexity and better MT results compared to the uniform one. Mnih and Teh (2012) also reported similar findings on perplexity. 4.3.3 Handling of Unknown Words In order to reduce the training time and to learn better word representations, NNLMs are often trained on most frequent vocabulary words only and low frequency words are represented under a class of unknown words, unk. This results in a large number of n-gram sequences containing at least one unk word and thereby, makes unk a highly probable word in the model.4 Our NDAM models rely on scoring out-domain sequences (of word Ids) using models that are trained based on the in-domain vocabulary. To score out-domain seque</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL’10),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="6390" citStr="Moore and Lewis, 2010" startWordPosition="1023" endWordPosition="1026">rrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time c</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the Association for Computational Linguistics (ACL’10), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Improved statistical machine translation for resource-poor languages using related resource-rich languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’09),</booktitle>
<contexts>
<context position="7580" citStr="Nakov and Ng, 2009" startWordPosition="1208" endWordPosition="1211">cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not l</context>
</contexts>
<marker>Nakov, Ng, 2009</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2009. Improved statistical machine translation for resource-poor languages using related resource-rich languages. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’09), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL’02),</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5110" citStr="Papineni et al., 2002" startWordPosition="800" endWordPosition="803">he cross entropy by regularizing the loss function with respect to the in-domain model. The regularizer gives higher weight to the training instances that are similar to the in-domain data. Our second model takes a more conservative approach by additionally penalizing data instances similar to the out-domain data. We evaluate our models on the standard task of translating Arabic-English and English-German language pairs. Our adapted models achieve better perplexities (Chen and Goodman, 1999) than the models trained on in- and in+out-domain data. Improvements are also reflected in BLEU scores (Papineni et al., 2002) as we compare these models within the SMT pipeline. We obtain gains of up to 0.5 and 0.6 on Arabic-English and EnglishGerman pairs over a competitive baseline system. The remainder of this paper is organized as follows: Section 2 gives an account on related work. Section 3 revisits NNJM model and Section 4 discusses our models. Section 5 presents the experimental setup and the results. Section 6 concludes. 2 Related Work Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation. 2.1 Data Selection Data selection has</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics (ACL’02), Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Pascanu</author>
<author>Tomas Mikolov</author>
<author>Yoshua Bengio</author>
</authors>
<title>On the difficulty of training recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning, ICML 2013,</booktitle>
<pages>16--21</pages>
<location>Atlanta, GA, USA,</location>
<contexts>
<context position="20502" citStr="Pascanu et al., 2013" startWordPosition="3505" endWordPosition="3508">ly, our models are based on NNs, while theirs utilize the traditional Markov-based generative models. 4.3 Technical Details In this section, we describe some implementation details of NDAM that we found to be crucial, such as: using gradient clipping to handle vanishing/exploding gradient problem in SGA training with backpropagation, selecting appropriate noise distribution in NCE, and special handling of outdomain words that are unknown to the in-domain. 4.3.1 Gradient Clipping Two common issues with training deep NNs on large data-sets are the vanishing and the exploding gradients problems (Pascanu et al., 2013). The error gradients propagated by the backpropagation may sometimes become very small or very large which can lead to undesired (nan) values in weight matrices, causing the training to fail. We also experienced the same problem in our NDAM quite often. One simple solution to this problem is to truncate the gradients, known as gradient clipping (Mikolov, 2012). In our experiments, we limit the gradients to be in the range [−5; +5]. 4.3.2 Noise Distribution in NCE Training with NCE relies on sampling from a noise distribution (i.e., ψ in Equation 5), and the performance of the NDAM models vari</context>
</contexts>
<marker>Pascanu, Mikolov, Bengio, 2013</marker>
<rawString>Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<location>Mumbai, India,</location>
<contexts>
<context position="9261" citStr="Schwenk, 2012" startWordPosition="1482" endWordPosition="1483">lized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word source window for the target word t</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In Proceedings of COLING 2012: Posters, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Holger Schwenk</author>
<author>Walid Aransa</author>
</authors>
<title>A multi-domain translation model framework for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8333" citStr="Sennrich et al., 2013" startWordPosition="1328" endWordPosition="1332">n (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data witho</context>
</contexts>
<marker>Sennrich, Schwenk, Aransa, 2013</marker>
<rawString>Rico Sennrich, Holger Schwenk, and Walid Aransa. 2013. A multi-domain translation model framework for statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Avignon, France,</location>
<contexts>
<context position="7668" citStr="Sennrich, 2012" startWordPosition="1223" endWordPosition="1224">ss useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014),</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, Avignon, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>455--465</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="9329" citStr="Socher et al., 2013" startWordPosition="1492" endWordPosition="1495">s the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T, the NNJM model computes the conditional probability P(T|S) as follows: P(T|S) ≈ � |T |P(ti|ti−1 ... ti−p+1, si) (1) i where, si is a q-word source window for the target word ti based on the one-to-one (non-NULL) alignment of T to S. As exempli</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455–465, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with large-scale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="12718" citStr="Vaswani et al., 2013" startWordPosition="2059" endWordPosition="2062"> = I(yn = k) is an indicator variable (i.e., ynk=1 when yn=k, otherwise 0). Optimization is performed using first-order online methods, such as stochastic gradient ascent (SGA) with standard backpropagation algorithm. Unfortunately, training NNLMs are impractically slow because for each training instance (xn, yn), the softmax output layer (see Equation 2) needs to compute a summation over all words in the output vocabulary.2 Noise contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010) provides an efficient and stable way to avoid this repetitive computation as recently applied to NNLMs (Vaswani et al., 2013; Mnih and Teh, 2012). We can re-write Equation 2 as follows: σ(yn = k|xn, θ) P(yn = k|xn, θ) = (4) Z(φ(xn), W) where Q(.) is the un-normalized score and Z(.) is the normalization factor. In NCE, we consider 2This would take few weeks for a modern CPU machine to train a single NNJM model on the whole data. |Vo | E k=1 J(θ) = N E n=1 1261 Look-up Hidden Output layer layer layer Xn φ(Xn) U W yn ymn C M ψ 7r Source token 1 Source token 2 Source token 3 Target token 1 Target token 2 Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target words (i.e</context>
<context position="16080" citStr="Vaswani et al., 2013" startWordPosition="2695" endWordPosition="2698">n adaptation models (NDAM) extending the NNJM model. Our models add regularization to its loss function either with respect to in-domain or both in- and out-domains. In both cases, we first present the regularized loss function for the normalized output layer with the standard softmax, N X n=1 J(θ) = = N X n=1 M + X m=1 J(θ) = N |Vo |hynk log σnk + M X X X n=1 k=1 m=1 1262 followed by the corresponding un-normalized one using the noise contrastive estimation. 4.1 NDAMv1 To improve the generalization of word embeddings, NNLMs are generally trained on very large datasets (Mikolov et al., 2013a; Vaswani et al., 2013). Therefore, we aim to train our neural domain adaptation models (NDAM) on in- plus out-domain data, while restricting it to drift away from in-domain. In our first model NDAMv1, we achieve this by biasing the model towards the indomain using a regularizer (or prior) based on the in-domain model. Let Bi be an NNJM model already trained on the in-domain data. We train an adapted model Ba on the whole data, but regularizing it with respect to Bi. We redefine the normalized loss function of Equation 3 as follows: [= λ ynk log ˆynk(θa) + ](1 − λ) ynk pnk(θi) log ˆynk(θa) (9) where ˆynk(Ba) is the </context>
<context position="25111" citStr="Vaswani et al., 2013" startWordPosition="4269" endWordPosition="4272">s, the Europarl (EP), and the Common Crawl (CC) corpora made available for the 9th Workshop of Statistical Machine Translation.6 Table 1 shows the size of the data used. Training NN models is expensive. We, therefore, randomly selected subsets of about 300K sentences from the bigger domains (UN, CC and EP) to train the NN models.7 The systems were tuned on concatenation of the dev. and test2010 and evaluated on test2011-2013 datasets. The tuning set was also used to measure the perplexities of different models. 5.2 System Settings NNJM &amp; NDAM: The NNJM models were trained using NPLM8 toolkit (Vaswani et al., 2013) with the following settings. We used a target context of 5 words and an aligned source window of 9 words, forming a joint stream of 14- grams for training. We restricted source and target side vocabularies to the 20K and 40K most frequent words. The word vector size D and the hidden layer size were set to 150 and 750, respectively. Only one hidden layer is used to allow faster decoding. Training was done by the standard stochastic gradient ascent with NCE using 5Guzm´an et al. (2013) showed that the QED corpus is similar to IWSLT and adding it improves translation quality. 6http://www.statmt.</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with large-scale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
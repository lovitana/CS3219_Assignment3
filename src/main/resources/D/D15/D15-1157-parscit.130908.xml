<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000580">
<title confidence="0.98942">
Foreebank: Syntactic Analysis of Customer Support Forums
</title>
<author confidence="0.975502">
Rasoul Kaljahi&apos;, Jennifer Foster&apos; Johann Roturier2
Corentin Ribeyre3, Teresa Lynn&apos;, Joseph Le Roux4
</author>
<affiliation confidence="0.967226">
&apos;ADAPT Centre, School of Computing, Dublin City University, Ireland
</affiliation>
<email confidence="0.77482">
{rkaljahi, jfoster, tlynn}@computing.dcu.ie
</email>
<affiliation confidence="0.692833">
2Symantec Research Labs, Dublin, Ireland
</affiliation>
<email confidence="0.632553">
johann roturier@symantec.com
</email>
<note confidence="0.235856">
3Alpage, INRIA, Univ Paris Diderot, Sorbonne Paris Cit´e, France
</note>
<email confidence="0.900125">
corentin.ribeyre@inria.fr
</email>
<address confidence="0.504583">
4Universit´e Paris Nord, France
</address>
<email confidence="0.997125">
joseph.le.roux@gmail.com
</email>
<sectionHeader confidence="0.993855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999934">
We present a new treebank of English and
French technical forum content which has
been annotated for grammatical errors and
phrase structure. This double annotation
allows us to empirically measure the effect
of errors on parsing performance. While
it is slightly easier to parse the corrected
versions of the forum sentences, the errors
are not the main factor in making this kind
of text hard to parse.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985025">
The last five years has seen a considerable amount
of research carried out on web and social me-
dia text parsing, with new treebanks being cre-
ated (Foster et al., 2011; Seddah et al., 2012; Mott
et al., 2012; Kong et al., 2014), and new parsing
systems developed (Petrov and McDonald, 2012;
Kong et al., 2014). In this paper we explore a par-
ticular source of user-generated text, namely, posts
from technical support forums, which are a pop-
ular means for customers to resolve their queries
about a product. An accurate parser for this kind
of text can be used to inform forum-level question-
answering, machine translation and quality esti-
mation of machine translation.
We create a 2000-sentence treebank called
Foreebank which contains sentences from the
Symantec Norton English and French technical
support forums.1 The phrase structure of the sen-
tences is annotated and any grammatical errors are
marked in the trees. Marking the grammatical er-
rors allows us to precisely measure the amount of
grammatical noise in this kind of text, and joint er-
ror and syntactic annotation allows us to determine
its effect on parsing.
</bodyText>
<footnote confidence="0.97561">
1http://community.norton.com
</footnote>
<bodyText confidence="0.999570363636364">
Foster (2010) explored the effect of spelling er-
rors on parsing performance of conversational fo-
rum text. We extend this study to include gram-
matical errors, focusing on more technical con-
tent. Foster et al. (2008) explored the effect of
artificially generated grammatical errors on Wall
Street Journal parsing. We concentrate on forum
text rather than newspaper text, and, crucially, ex-
amine the effect of real grammatical errors. We
find that the level of grammatical noise is lower
than expected, with capitalisation and punctuation
errors being the most frequent. While correcting
all the errors does result in a performance increase
of 1.5% for English and 0.8% for French, the ma-
jor challenge in parsing these sentences seems not
to be “bad language” (Eisenstein, 2013) per se.
The main contribution of the paper is the Foree-
bank data set itself2 but we also carry out prelim-
inary parsing experiments evaluating the accuracy
of a PCFG-LA parser on Foreebank, examining
the effect of grammatical errors on parsing and ex-
perimenting with different training sets.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999824583333333">
Other treebanks of English web text include the
English Web Treebank (aka Google Web Tree-
bank) (Mott et al., 2012), the small treebank of
tweets and football discussion forum posts de-
scribed in Foster et al. (2011) and the tweet de-
pendency bank described in Kong et al. (2014).
The English Web Treebank is a corpus of over
250K words, selected from blogs, newsgroups,
emails, local business reviews and Yahoo! an-
swers. It adapts the Penn Treebank (Marcus et
al., 1994) and Switchboard (Taylor, 1996) annota-
tion guidelines to address the phenomena specific
</bodyText>
<footnote confidence="0.994006">
2www.computing.dcu.ie/mt/confidentmt.
html
</footnote>
<page confidence="0.858289">
1341
</page>
<note confidence="0.68273">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999987418918919">
to this type of text. The annotation of the 1000-
sentence treebank described in Foster et al. (2011)
is based on the Penn Treebank, whereas the an-
notation of the treebank described in Kong et al.
(2014) is dependency-based. The French Social
Media Bank developed by Seddah et al. (2012) is
a treebank of 1,700 French sentences from various
type of social media including Facebook, Twit-
ter and discussion forums (video game and med-
ical). An extended version of the FTB-UC annota-
tion guidelines (Candito and Crabb´e, 2009) is em-
ployed during annotation and subcorpora contain-
ing particularly noisy utterances are identified.
The main difference between Foreebank and
other web/social media treebanks is that grammat-
ical errors in the Foreebank sentences are marked
and corrected as part of the annotation process. Er-
ror annotation not only provides more insight into
this type of text but it also enables us to directly
measure the effect of these errors on parsing accu-
racy and leaves open the possibility of performing
joint parsing and error detection by directly learn-
ing the error annotation during parser training.
A learner corpus (Granger, 2008) contains utter-
ances produced by language learners and serves as
a resource for second language acquisition, com-
putational linguistic and computer-aided language
learning research. Examples include the Interna-
tional Corpus of Learner English (Granger, 1993),
the Cambridge Learner Corpus (Nicholls, 1999;
Yannakoudakis et al., 2011), the NUS Corpus of
Learner English (Dahlmeier et al., 2013) and the
German Falko corpus (L¨udeling, 2008; Rehbein et
al., 2012). We can compare Foreebank to a learner
corpus since both contain utterances that are po-
tentially ungrammatical and because in a learner
corpus the errors are often annotated, as they are
in Foreebank. In the last five years, there have
been several shared tasks in grammatical error cor-
rection including the Helping Our Own (HOO)
shared tasks of 2011 and 2012 (Dale and Kil-
gariff, 2011; Dale et al., 2012), and the CoNLL
2013 and 2014 shared tasks (Ng et al., 2013; Ng
et al., 2014). With the exception of HOO 2011,
all shared tasks involve error-annotated sentences
from learner corpora. Annotation schemes vary
but most involve marking the span of an error,
classifying the error according to some taxonomy
designed with L2 utterances in mind, and some-
times providing the correction or “target hypothe-
sis” (Hirschmann et al., 2007).
Regarding syntactic annotation of learner data,
Dickinson and Ragheb (2009) propose a depen-
dency annotation scheme based on the CHILDES
scheme (Sagae et al., 2007) developed for first lan-
guage learners. They treat the developing lan-
guage of learners as an interlanguage, as sug-
gested by D´ıaz-Negrillo et al. (2010), and anno-
tate it as is. They use two POS tags and two de-
pendency labels for error cases: one for the sur-
face form and one for the intended form. Ros´en
and De Smedt (2010) criticise the approach of
Dickinson and Ragheb (2009) involving “annotat-
ing language text as is” arguing that interpretation
of the language is required at all annotation lev-
els. They use NorGram, a Norwegian Lexical-
Functional Grammar, to annotate a learner cor-
pus with constituency structure, functional struc-
ture and semantic structure, in order to provide a
means to search for contexts in which learner er-
rors occur. Nagata et al. (2011) describe an En-
glish learner corpus which has been manually an-
notated with shallow syntax, introducing two new
POS tags and two new chunk labels for errors.
</bodyText>
<sectionHeader confidence="0.930335" genericHeader="method">
3 Building the Foreebank
</sectionHeader>
<bodyText confidence="0.999991695652174">
The Foreebank treebank contains 1000 English
sentences and 1000 French sentences. The En-
glish sentences come from the Symantec Norton
technical support user forum. Half of the French
sentences come from the French Norton forum
and the other half are human translations of sen-
tences from the English forum. Four annotators
were involved in the annotation process. Their
main task was to correct automatically parsed
phrase structure trees using an annotation tool de-
veloped for this project.3 The English annota-
tors were guided by the Penn Treebank bracket-
ing guidelines and a Foreebank-adapted version of
the English Web Treebank bracketing guidelines.
The French annotators used the French treebank
(FTB) (Abeill´e et al., 2003) guidelines, following
the SPMRL strategy for multiword expressions
(Seddah et al., 2013; Candito and Crabb´e, 2009).
The two primary annotators, one for French and
one for English, annotated all the data for their
language. The two secondary annotators anno-
tated a 100-sentence subset. Inter-annotator agree-
ment was calculated by measuring the Parseval
</bodyText>
<footnote confidence="0.986925">
3The Stanford parser (Klein and Manning, 2003) was
used to parse the English data and the Berkeley parser (Petrov
et al., 2006) was used for the French sentences.
</footnote>
<page confidence="0.9911">
1342
</page>
<figureCaption confidence="0.87616">
Figure 1: The Foreebank annotation of i tried customize too, but i cant find them .. T T corrected as I
tried Customize too, but Ica n’t find them ... T T
</figureCaption>
<table confidence="0.9536016">
Suffix Explanation Example FBen FBfr
English French # % # %
D Deleted token It fixed [the] problem. Cela a r´esolu [le] probl`eme. 170 1.10 56 0.29
X Extraneous tokens It fixed the my problem. Cela a r´esolu le mon probl`eme. 35 0.23 17 0.09
W Wrong form error It fix the problem. Cela r´esoudre le probl`eme. 69 0.45 43 0.22
S Misspelled token It fixed my prbolem. Cela a r´esolu mon prbol`eme. 81 0.53 117 0.60
C Capitalisation error it fixed my problem. cela a r´esolu mon probl`eme. 161 1.00 194 1.00
B Broken token It fix ed my problem. Cela a r´eso lu mon probl`eme. 2 0.01 12 0.06
I Innovative initialism I have problem w/ this software. J’ai un probl`eme av. ce logiciel. 1 0.01 7 0.04
M Merged sentences It fixed the problem Thank you. Cela a r´esolu le probl`eme Merci. 3 0.30 29 2.90
</table>
<tableCaption confidence="0.999272">
Table 1: Foreebank Error Suffixes. The last two columns refer to their frequency.
</tableCaption>
<bodyText confidence="0.935178166666667">
F1 of trees produced by the secondary annotators
against those produced by the primary annotators.
For English this was 88 and for French it was 86.7.
Prior to correcting a parse tree produced by the
automatic parser, the annotators are asked to cor-
rect any errors they find in the sentence.4 The cor-
rected text is entered in a field of the annotation
tool. As part of the syntactic annotation process,
errors are marked by appending an error suffix to
the preterminals of the affected words in the tree.
The error suffixes used in Foreebank are listed in
Table 1 and an example tree from Foreebank is
shown in Figure 1. There are three kinds of substi-
tution error suffixes: C for marking problems with
capitalisation, S for marking spelling errors and W
for marking the wrong form of a word which en-
compasses inflection errors (they instead of them),
real-word spelling errors (test instead of text) and
lexical choice errors (desk instead of chair). The
POS tag of the corrected form is used in the tree
instead of the POS tag of the incorrect form.5 Al-
though this annotation scheme contains fewer er-
ror types than the taxonomies used for learner cor-
4Minimal correction is encouraged to prevent annotators
from rewriting the sentence in their preferred writing style.
Instead they are instructed to just focus on fixing the errors.
5An alternative would have been to use one POS tag for
the erroneous form and one for the corrected form, either
combined a la Nagata et al. (2011) or separate a la Dickin-
son and Ragheb (2009).
pora, its granularity increases when the error suf-
fixes are interpreted in the syntactic context in
which they occur. For example, we can distin-
guish a missing determiner (DT D) from a missing
preposition (IN D).
The “sentences” that the annotators see are the
result of passing the forum text through an auto-
matic sentence splitter (NLTK6) and tokeniser (in-
house). This is another important difference be-
tween Foreebank and the English Web Treebank
(EWT). In the EWT, sentence boundary detec-
tion and tokenisation has been carried out manu-
ally before annotation. Both approaches are valid
but ours was chosen in order to stay closer to the
more realistic scenario of less than perfect auto-
matic preprocessing tools. This means that anno-
tators have a special class of errors that result from
noisy sentence splitting and tokenisation that must
be marked during annotation.
There are two types of sentence splitting errors:
merged sentences such as (1) in which a sentence
boundary was not detected before the word When
due to the use of a comma instead of a full stop,
and split sentences such as (2).
</bodyText>
<listItem confidence="0.6992075">
(1) 7. Combofix will start, When it
is scanning don’t move the
mouse cursor inside the box,
(2) The questions to &lt;CompanyName&gt;:
</listItem>
<footnote confidence="0.984684">
6http://www.nltk.org/
</footnote>
<page confidence="0.973208">
1343
</page>
<bodyText confidence="0.999845133333333">
Merged sentences are gathered under one root
node with the error suffix M (e.g. S M) , and split
sentences are annotated as if they are standalone.
Tokenisation problems can also be categorised
as merged (3) or split (4 and 5). Merged tokens
are treated as a combination of a spelling error
(whenI instead of when) and a deleted token (I).
When the split is morphological as in (4), they are
tagged with the POS tag of the whole intended to-
ken, along with the error suffix B (for “broken”).
So in (4), the POS tag of anti would be annotated
as NN B and the POS tag of vir as NN B. When
there is no such clean morphological break (as in
(5)), the first token is treated as a spelling error and
the second as an extraneous token.
</bodyText>
<listItem confidence="0.9981375">
(3) whenI tried to use ...
(4) he should buy anti vir programs
(5) i t keeps causing &lt;ProductName&gt;
to lock up ...
</listItem>
<sectionHeader confidence="0.97115" genericHeader="method">
4 Analysing the Foreebank
</sectionHeader>
<bodyText confidence="0.999963766666667">
Table 2 presents the average and the maximum
sentence length in Foreebank, and, for compar-
ison, WSJ and FTB. It also gives the out-of-
vocabulary (OOV) rate of these data sets with re-
spect to the WSJ and FTB. The Foreebank sen-
tences are shorter on average than the WSJ and
FTB sentences. The table also shows that the
OOV rate of Foreebank with respect to WSJ/FTB
is high: 33.3% for English and 39.1% for French.
These numbers can be compared to the OOV rate
of the WSJ test set with respect to its training set
which is 13.2% and the FTB which is 20.6%. The
higher OOV rate for the French Foreebank com-
pared to the English is most likely due to the larger
size of the WSJ compared to the FTB. The OOV
rate of the English Foreebank is more than 2.5
times as large as that of the WSJ test set, while
the OOV rate of the French Foreebank is less than
2 times as large as that of the FTB test set. This
suggests that a bigger performance drop due to un-
known words should be expected in parsing the
English Foreebank sentences than the French.
The last four columns in Table 1 display the ab-
solute and relative frequency of each error suffix.
In sum, it seems that capitalisation is the major
error type in Foreebank especially in the French
data. Deleted tokens are also a major source of
problem on the English side. Most of the capital-
isation errors involve proper nouns (e.g. product
names) and most of the deleted tokens are cases
</bodyText>
<table confidence="0.99833825">
FB,n WSJ FBfr FTB
Avg. sentence length 15.4 23.8 19.6 28.4
Max. sentence length 89 141 86 260
OOV rate (%) 31.6 - 33.6 -
</table>
<tableCaption confidence="0.582415666666667">
Table 2: Characteristics of the English (FB,n) and
French (FBfr) Foreebank compared with those of
the WSJ and FTB.
</tableCaption>
<table confidence="0.9998505">
English FB,n WSJ French FB fr FTB
All Test All Test
WSJall 77.0 - FTBall 76.3 -
WSJtrain 75.4 89.6 FTBtrain 76.0 81.3
</table>
<tableCaption confidence="0.999631">
Table 3: Foreebank and WSJ/FTB test set results.
</tableCaption>
<bodyText confidence="0.999905727272727">
of missing punctuation. Overall, the errors oc-
cur on only a small fraction of the tokens in both
data sets. We also calculate the edit distance be-
tween each Foreebank sentence and its correction
by summing the number of error suffixes and di-
viding by the maximum of the original and cor-
rected sentence lengths. The average edit dis-
tance for the English section of Foreebank is 0.04
and for the French section is 0.03. Despite the
existence of some near-to-incomprehensible sen-
tences, the overall error level is very low.
</bodyText>
<sectionHeader confidence="0.966048" genericHeader="method">
5 Parsing the Foreebank
</sectionHeader>
<bodyText confidence="0.999997454545455">
We first evaluate newswire-trained parsers on
Foreebank, using our in-house PCFG-LA parser
with the max-rule parsing algorithm (Petrov and
Klein, 2007) and 6 split-merge cycles. The En-
glish model is trained on the entire WSJ and the
French model on the entire FTB. For compari-
son, we parse the WSJ/FTB and so we addition-
ally use models trained only on the training sec-
tions. We remove the error suffixes and any D-
suffixed nodes (representing deleted words) from
the gold Foreebank trees before evaluation. The
results are shown in Table 3. As expected, we see a
significant drop for both languages when we move
from in-domain data to Foreebank. Compared to
parsing the English side of Foreebank, the perfor-
mance drop for French is relatively smaller: the
former drops 14.2 points from 89.6 F1 points to
75.4 and the latter 5.3 points from 81.3 to 76. This
suggests that, either the French parsing model is
better generalisable to the forum text, or alterna-
tively, that the FTB test set is more distant from its
training set than the WSJ one. The second hypoth-
</bodyText>
<page confidence="0.98903">
1344
</page>
<bodyText confidence="0.999893392156863">
esis is more likely because 1) it is on par with the
OOV rate observed in Section 4, and 2) the perfor-
mance of the English and French parsers are close
on Foreebank but further apart on the newswire
test sets. The effect of using the entire WSJ and
FTB instead of only their training sections is also
worth noting. While adding the WSJ development
and test sets (about 5,500 sentences, a 14% in-
crease) improves the F1 of English parsing by 1.6
points, the 2,500 FTB development and test sen-
tences (a 25% increase) have little effect on the
French parsing, suggesting that either these new
sentences are still not enough or do not bring ad-
ditional information to the parsing model.
Since the annotators correct the errors made by
the forum users, we are able to parse the corrected
versions of the Foreebank sentences and examine
how accurately they are parsed compared to the
original sentences. We use the WSJall and FTBall
parsing models described above. Correcting the
user errors before parsing leads to an improved
parsing F1 of 78.6 for the English sentences, an
increase of 1.6 points (2%). A smaller impact is
observed on the French sentences where the edited
sentences receive an F1 of 77.1 (an increase of 0.8
points). Referring to the distribution of error suf-
fixes in Table 1, this suggests that the inserted and
deleted tokens may have a larger effect on parser
error than the substituted tokens, as their number
is higher for English. Many substitution errors are
capitalisation errors, typically involving a confu-
sion between proper and common nouns, which
tends not to affect the surrounding tree.
The simplest method to improve the accuracy
of parsing Foreebank is to use it as supplemen-
tary training data. We do this using a 5-fold cross
validation, in which Foreebank is randomly split
into five parts, with each part used for the evalu-
ation of the parsers trained on WSJ/FTB plus the
other four parts. The results are shown in Table 4.
Combining the larger treebank and Foreebank im-
proves the F1 by 2.6 points for English and 3.2 for
French. Considering that Foreebank is orders of
magnitude smaller than the WSJ/FTB, these gains
are encouraging. We try to overcome the small
size of Foreebank by 1) using the EWT as training
data, and 2) increasing the weight of Foreebank by
training on multiple copies of it. The EWT is not
a substitute for the WSJ but it does provide a mod-
est improvement when used in conjunction with
Foreebank and WSJ. The replication of Foreebank
</bodyText>
<table confidence="0.9994539">
English French
Training Set F1 Training Set F1
WSJall 77.0 FTBall 76.3
WSJall+FBen 79.6 FTBall+FBfr 79.5
WSJall+5FBen 80.1 FTBall+5FBfr 79.5
EWT 75.0 - -
EWT+FBen 79.0 - -
WSJall+FBen+EWT 80.3 - -
FBen 71.1 FBfr 72.4
FBen suf 70.2 FBfr suf 71.8
</table>
<tableCaption confidence="0.996045">
Table 4: Training on Foreebank/WSJ/EWT/FTB
</tableCaption>
<bodyText confidence="0.989888411764706">
and testing on Foreebank
trees has mixed results, providing a 0.5 point im-
provement for English and none for French.
In all experiments up to now, we have excluded
the error suffixes from the Foreebank trees (dur-
ing training and testing). We next try to directly
learn trees containing the error suffixes (except for
deleted tokens). That is, we use the original Foree-
bank trees containing the error suffixes for train-
ing and evaluate against Foreebank trees contain-
ing the error suffixes. The second last row of Ta-
ble 4 shows the 5-fold CV results when the version
of Foreebank without the error suffixes is used for
training and the last row the results when the er-
ror suffixes are included. Including the suffixes
decreases the accuracy, most likely due to the in-
creased data sparsity caused by the suffixed tags.
</bodyText>
<sectionHeader confidence="0.996156" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995909090909">
We have introduced a treebank of technical fo-
rum sentences for English and French, based on an
annotation strategy adapted to suit user-generated
text in a realistic NLP setting. By marking the er-
rors on the trees, we studied their prevalence as
well as their impact on parsing and found that de-
spite their low frequency, they do negatively af-
fect parser performance, while not being the most
important factor. Our next steps include learning
error suffixes during a prior tagging phase and ex-
perimenting with the French Social Media Bank.
</bodyText>
<sectionHeader confidence="0.988934" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<footnote confidence="0.9168992">
This research has been supported by the Irish
Research Council Enterprise Partnership Scheme
(EPSPG/2011/102) and the Science Foundation
Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University.
</footnote>
<page confidence="0.99318">
1345
</page>
<sectionHeader confidence="0.959177" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.970289803738318">
Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel.
2003. Building a Treebank for French. In Tree-
banks: Building and Using Syntactically Annotated
Corpora, pages 165–187. Kluwer Academic Pub-
lishers.
Marie Candito and Benoit Crabb´e. 2009. Improving
Generative Statistical Parsing with Semi-supervised
Word Clustering. In Proceedings of IWPT, pages
138–141.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
English: The NUS Corpus of Learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22–31.
Robert Dale and Adam Kilgariff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. In Pro-
ceedings of the 13th European Workshop on Natural
Language Generation (ENLG), pages 242–249.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings oftThe 7th Workshop on the Innovative
Use of NLP for Building Educational Applications,
pages 54–62.
Ana D´ıaz-Negrillo, Detmar Meurers, Salvador Valera,
and Holger Wunsch. 2010. Towards interlanguage
pos annotation for effective learner corpora in sla
and flt. In Language Forum, volume 36, pages 139–
154.
Markus Dickinson and Marwa Ragheb. 2009. Depen-
dency annotation for learner corpora. In Proceed-
ings of the Eighth Workshop on Treebanks and Lin-
guistic Theories (TLT-8), pages 59–70.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In Proceedings of NAACL,
pages 359–369.
Jennifer Foster, Joachim Wagner, and Josef Van Gen-
abith. 2008. Adapting a WSJ-trained parser to
grammatically noisy text. In Proceedings of ACL:
Short Papers, pages 221–224.
Jennifer Foster, ¨Ozlem C¸etino˘glu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011. From News to Com-
ment: Benchmarks and Resources for Parsing the
Language of Web 2.0. In Proceedings of IJCNLP,
pages 893–901.
Jennifer Foster. 2010. cba to check the spelling in-
vestigating parser performance on discussion forum
posts. In Proceedings of NAACL, pages 381–384.
Sylviane Granger. 1993. International corpus of
learner english. In J. Aarts, P. de Haan, and
N.Oostdijk, editors, English Language Corpora:
Design, Analysis and Exploitation, pages 57–71.
Rodopi, Amsterdam.
Sylviane Granger. 2008. Learner corpora. In Anke
L¨udeling and M. Kyt¨o, editors, Corpus Linguis-
tics. An International Handbook, pages 259–275.
Berlin:Mouton de Gruyter.
Hagen Hirschmann, Seanna Doolittle, and Anke
L¨udeling. 2007. Syntactic annotation of non-
canonical linguistic structures. In Proceedings of
Corpus Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL,
pages 423–430.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1001–10012.
Anke. L¨udeling. 2008. Mehrdeutigkeiten und kate-
gorisierung: Probleme bei der annotation von lern-
erkorpora. In M. Walter and P. Grommes, edi-
tors, Fortgeschrittene Lernervarietten, pages 119–
140. Niemeyer, Tbingen.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings
of the 1994 ARPA Speech and Natural Language
Workshop, pages 114–119.
Justin Mott, Ann Bies, John Laury, and Colin Warner.
2012. Bracketing Webtext: An Addendum to Penn
Treebank II Guidelines. Technical report, Linguistic
Data Consortium.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proceedings of
ACL-HLT, pages 1210–1219.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1–12.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 Shared Task
on Grammatical Error Correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 1–14.
Diane Nicholls. 1999. The cambridge learner corpus –
error coding and analysis. In Summer Workshop on
Learner Corpora, Tokyo, Japan.
</reference>
<page confidence="0.8246">
1346
</page>
<reference confidence="0.99987612244898">
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL, pages 404–411.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. Notes of
the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), 59.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact and
Interpretable Tree Annotation. In Proceedings of
COLING-ACL.
Ines Rehbein, Hagen Hirschmann, Anke L¨udeling, and
Marc Reznicek. 2012. Better tags give better trees
or do they? Linguistic Issues in Language Technol-
ogy, 7(10).
Victoria Ros´en and Koenraad De Smedt. 2010. Syn-
tactic annotation of learner corpora. Systematisk,
variert, men ikke tilfeldig, pages 120–132.
Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhin-
ney, and Shuly Wintner. 2007. High-accuracy an-
notation and parsing of childes transcripts. In Pro-
ceedings of the Workshop on Cognitive Aspects of
Computational Language Acquisition, pages 25–32.
Djam´e Seddah, Benoit Sagot, Marie Candito, Virginie
Mouilleron, and Vanessa Combet. 2012. The
French Social Media Bank: a Treebank of Noisy
User Generated Content. In Proceedings of COL-
ING, pages 2441–2458.
Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie
Candito, Jinho D. Choi, Rich´ard Farkas, Jen-
nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-
letebeitia, Yoav Goldberg, Spence Green, Nizar
Habash, Marco Kuhlmann, Wolfgang Maier, Joakim
Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang
Seeker, Yannick Versley, Veronika Vincze, Marcin
Woli´nski, Alina Wr´oblewska, and Eric Villemonte
de la Clergerie. 2013. Overview of the SPMRL
2013 shared task: A cross-framework evaluation of
parsing morphologically rich languages. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 146–
182.
Ann Taylor. 1996. Bracketing Switchboard: An Ad-
dendum to the Treebank II Guidelines. Technical
report.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In Proceedings ofACL, pages
180–189.
</reference>
<page confidence="0.99377">
1347
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.172507">
<title confidence="0.999669">Foreebank: Syntactic Analysis of Customer Support Forums</title>
<author confidence="0.9742245">Jennifer Teresa Joseph Le</author>
<affiliation confidence="0.7357315">Centre, School of Computing, Dublin City University, jfoster,</affiliation>
<address confidence="0.747563">Research Labs, Dublin,</address>
<email confidence="0.76571">johann</email>
<affiliation confidence="0.763828">INRIA, Univ Paris Diderot, Sorbonne Paris Cit´e, Paris Nord,</affiliation>
<email confidence="0.999914">joseph.le.roux@gmail.com</email>
<abstract confidence="0.997799727272727">We present a new treebank of English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Lionel Cl´ement</author>
<author>Franc¸ois Toussenel</author>
</authors>
<title>Building a Treebank for French. In Treebanks: Building and Using Syntactically Annotated Corpora,</title>
<date>2003</date>
<pages>165--187</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Abeill´e, Cl´ement, Toussenel, 2003</marker>
<rawString>Anne Abeill´e, Lionel Cl´ement, and Franc¸ois Toussenel. 2003. Building a Treebank for French. In Treebanks: Building and Using Syntactically Annotated Corpora, pages 165–187. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoit Crabb´e</author>
</authors>
<title>Improving Generative Statistical Parsing with Semi-supervised Word Clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>138--141</pages>
<marker>Candito, Crabb´e, 2009</marker>
<rawString>Marie Candito and Benoit Crabb´e. 2009. Improving Generative Statistical Parsing with Semi-supervised Word Clustering. In Proceedings of IWPT, pages 138–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a large annotated corpus of learner English: The NUS Corpus of Learner English.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>22--31</pages>
<contexts>
<context position="5491" citStr="Dahlmeier et al., 2013" startWordPosition="846" endWordPosition="849">asure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared task</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner English: The NUS Corpus of Learner English. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 22–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgariff</author>
</authors>
<title>Helping Our Own: The HOO 2011 Pilot Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>242--249</pages>
<contexts>
<context position="5945" citStr="Dale and Kilgariff, 2011" startWordPosition="922" endWordPosition="926"> of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae </context>
</contexts>
<marker>Dale, Kilgariff, 2011</marker>
<rawString>Robert Dale and Adam Kilgariff. 2011. Helping Our Own: The HOO 2011 Pilot Shared Task. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 242–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task.</title>
<date>2012</date>
<booktitle>In Proceedings oftThe 7th Workshop on the Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>54--62</pages>
<contexts>
<context position="5965" citStr="Dale et al., 2012" startWordPosition="927" endWordPosition="930">er, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) develo</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task. In Proceedings oftThe 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 54–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana D´ıaz-Negrillo</author>
<author>Detmar Meurers</author>
<author>Salvador Valera</author>
<author>Holger Wunsch</author>
</authors>
<title>Towards interlanguage pos annotation for effective learner corpora in sla and flt.</title>
<date>2010</date>
<booktitle>In Language Forum,</booktitle>
<volume>36</volume>
<pages>139--154</pages>
<marker>D´ıaz-Negrillo, Meurers, Valera, Wunsch, 2010</marker>
<rawString>Ana D´ıaz-Negrillo, Detmar Meurers, Salvador Valera, and Holger Wunsch. 2010. Towards interlanguage pos annotation for effective learner corpora in sla and flt. In Language Forum, volume 36, pages 139– 154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>Marwa Ragheb</author>
</authors>
<title>Dependency annotation for learner corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eighth Workshop on Treebanks and Linguistic Theories (TLT-8),</booktitle>
<pages>59--70</pages>
<contexts>
<context position="6470" citStr="Dickinson and Ragheb (2009)" startWordPosition="1007" endWordPosition="1010">correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) developed for first language learners. They treat the developing language of learners as an interlanguage, as suggested by D´ıaz-Negrillo et al. (2010), and annotate it as is. They use two POS tags and two dependency labels for error cases: one for the surface form and one for the intended form. Ros´en and De Smedt (2010) criticise the approach of Dickinson and Ragheb (2009) involving “annotating language text as is” arguing that interpretation of the language is required at all annotation levels. They use</context>
<context position="11315" citStr="Dickinson and Ragheb (2009)" startWordPosition="1839" endWordPosition="1843">) and lexical choice errors (desk instead of chair). The POS tag of the corrected form is used in the tree instead of the POS tag of the incorrect form.5 Although this annotation scheme contains fewer error types than the taxonomies used for learner cor4Minimal correction is encouraged to prevent annotators from rewriting the sentence in their preferred writing style. Instead they are instructed to just focus on fixing the errors. 5An alternative would have been to use one POS tag for the erroneous form and one for the corrected form, either combined a la Nagata et al. (2011) or separate a la Dickinson and Ragheb (2009). pora, its granularity increases when the error suffixes are interpreted in the syntactic context in which they occur. For example, we can distinguish a missing determiner (DT D) from a missing preposition (IN D). The “sentences” that the annotators see are the result of passing the forum text through an automatic sentence splitter (NLTK6) and tokeniser (inhouse). This is another important difference between Foreebank and the English Web Treebank (EWT). In the EWT, sentence boundary detection and tokenisation has been carried out manually before annotation. Both approaches are valid but ours </context>
</contexts>
<marker>Dickinson, Ragheb, 2009</marker>
<rawString>Markus Dickinson and Marwa Ragheb. 2009. Dependency annotation for learner corpora. In Proceedings of the Eighth Workshop on Treebanks and Linguistic Theories (TLT-8), pages 59–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>359--369</pages>
<contexts>
<context position="2841" citStr="Eisenstein, 2013" startWordPosition="434" endWordPosition="435">focusing on more technical content. Foster et al. (2008) explored the effect of artificially generated grammatical errors on Wall Street Journal parsing. We concentrate on forum text rather than newspaper text, and, crucially, examine the effect of real grammatical errors. We find that the level of grammatical noise is lower than expected, with capitalisation and punctuation errors being the most frequent. While correcting all the errors does result in a performance increase of 1.5% for English and 0.8% for French, the major challenge in parsing these sentences seems not to be “bad language” (Eisenstein, 2013) per se. The main contribution of the paper is the Foreebank data set itself2 but we also carry out preliminary parsing experiments evaluating the accuracy of a PCFG-LA parser on Foreebank, examining the effect of grammatical errors on parsing and experimenting with different training sets. 2 Related Work Other treebanks of English web text include the English Web Treebank (aka Google Web Treebank) (Mott et al., 2012), the small treebank of tweets and football discussion forum posts described in Foster et al. (2011) and the tweet dependency bank described in Kong et al. (2014). The English Web</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of NAACL, pages 359–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Josef Van Genabith</author>
</authors>
<title>Adapting a WSJ-trained parser to grammatically noisy text.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL: Short Papers,</booktitle>
<pages>221--224</pages>
<marker>Foster, Wagner, Van Genabith, 2008</marker>
<rawString>Jennifer Foster, Joachim Wagner, and Josef Van Genabith. 2008. Adapting a WSJ-trained parser to grammatically noisy text. In Proceedings of ACL: Short Papers, pages 221–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>¨Ozlem C¸etino˘glu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From News to Comment: Benchmarks and Resources for Parsing the Language of Web 2.0.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>893--901</pages>
<marker>Foster, C¸etino˘glu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, ¨Ozlem C¸etino˘glu, Joachim Wagner, Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011. From News to Comment: Benchmarks and Resources for Parsing the Language of Web 2.0. In Proceedings of IJCNLP, pages 893–901.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>cba to check the spelling investigating parser performance on discussion forum posts.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>381--384</pages>
<contexts>
<context position="2079" citStr="Foster (2010)" startWordPosition="312" endWordPosition="313"> be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French technical support forums.1 The phrase structure of the sentences is annotated and any grammatical errors are marked in the trees. Marking the grammatical errors allows us to precisely measure the amount of grammatical noise in this kind of text, and joint error and syntactic annotation allows us to determine its effect on parsing. 1http://community.norton.com Foster (2010) explored the effect of spelling errors on parsing performance of conversational forum text. We extend this study to include grammatical errors, focusing on more technical content. Foster et al. (2008) explored the effect of artificially generated grammatical errors on Wall Street Journal parsing. We concentrate on forum text rather than newspaper text, and, crucially, examine the effect of real grammatical errors. We find that the level of grammatical noise is lower than expected, with capitalisation and punctuation errors being the most frequent. While correcting all the errors does result i</context>
</contexts>
<marker>Foster, 2010</marker>
<rawString>Jennifer Foster. 2010. cba to check the spelling investigating parser performance on discussion forum posts. In Proceedings of NAACL, pages 381–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
</authors>
<title>International corpus of learner english.</title>
<date>1993</date>
<booktitle>English Language Corpora: Design, Analysis and Exploitation,</booktitle>
<pages>57--71</pages>
<editor>In J. Aarts, P. de Haan, and N.Oostdijk, editors,</editor>
<location>Rodopi, Amsterdam.</location>
<contexts>
<context position="5356" citStr="Granger, 1993" startWordPosition="828" endWordPosition="829">notation process. Error annotation not only provides more insight into this type of text but it also enables us to directly measure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et a</context>
</contexts>
<marker>Granger, 1993</marker>
<rawString>Sylviane Granger. 1993. International corpus of learner english. In J. Aarts, P. de Haan, and N.Oostdijk, editors, English Language Corpora: Design, Analysis and Exploitation, pages 57–71. Rodopi, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
</authors>
<title>Learner corpora.</title>
<date>2008</date>
<booktitle>In Anke L¨udeling</booktitle>
<pages>259--275</pages>
<editor>and M. Kyt¨o, editors,</editor>
<note>Berlin:Mouton de Gruyter.</note>
<contexts>
<context position="5099" citStr="Granger, 2008" startWordPosition="793" endWordPosition="794">ng annotation and subcorpora containing particularly noisy utterances are identified. The main difference between Foreebank and other web/social media treebanks is that grammatical errors in the Foreebank sentences are marked and corrected as part of the annotation process. Error annotation not only provides more insight into this type of text but it also enables us to directly measure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner cor</context>
</contexts>
<marker>Granger, 2008</marker>
<rawString>Sylviane Granger. 2008. Learner corpora. In Anke L¨udeling and M. Kyt¨o, editors, Corpus Linguistics. An International Handbook, pages 259–275. Berlin:Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen Hirschmann</author>
<author>Seanna Doolittle</author>
<author>Anke L¨udeling</author>
</authors>
<title>Syntactic annotation of noncanonical linguistic structures.</title>
<date>2007</date>
<booktitle>In Proceedings of Corpus Linguistics.</booktitle>
<marker>Hirschmann, Doolittle, L¨udeling, 2007</marker>
<rawString>Hagen Hirschmann, Seanna Doolittle, and Anke L¨udeling. 2007. Syntactic annotation of noncanonical linguistic structures. In Proceedings of Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="8635" citStr="Klein and Manning, 2003" startWordPosition="1357" endWordPosition="1360">nnotators were guided by the Penn Treebank bracketing guidelines and a Foreebank-adapted version of the English Web Treebank bracketing guidelines. The French annotators used the French treebank (FTB) (Abeill´e et al., 2003) guidelines, following the SPMRL strategy for multiword expressions (Seddah et al., 2013; Candito and Crabb´e, 2009). The two primary annotators, one for French and one for English, annotated all the data for their language. The two secondary annotators annotated a 100-sentence subset. Inter-annotator agreement was calculated by measuring the Parseval 3The Stanford parser (Klein and Manning, 2003) was used to parse the English data and the Berkeley parser (Petrov et al., 2006) was used for the French sentences. 1342 Figure 1: The Foreebank annotation of i tried customize too, but i cant find them .. T T corrected as I tried Customize too, but Ica n’t find them ... T T Suffix Explanation Example FBen FBfr English French # % # % D Deleted token It fixed [the] problem. Cela a r´esolu [le] probl`eme. 170 1.10 56 0.29 X Extraneous tokens It fixed the my problem. Cela a r´esolu le mon probl`eme. 35 0.23 17 0.09 W Wrong form error It fix the problem. Cela r´esoudre le probl`eme. 69 0.45 43 0.</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of ACL, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
<author>Archna Bhatia</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1001--10012</pages>
<contexts>
<context position="1142" citStr="Kong et al., 2014" startWordPosition="163" endWordPosition="166"> English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 1 Introduction The last five years has seen a considerable amount of research carried out on web and social media text parsing, with new treebanks being created (Foster et al., 2011; Seddah et al., 2012; Mott et al., 2012; Kong et al., 2014), and new parsing systems developed (Petrov and McDonald, 2012; Kong et al., 2014). In this paper we explore a particular source of user-generated text, namely, posts from technical support forums, which are a popular means for customers to resolve their queries about a product. An accurate parser for this kind of text can be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French technical support forums.1 The phrase </context>
<context position="3424" citStr="Kong et al. (2014)" startWordPosition="532" endWordPosition="535">e “bad language” (Eisenstein, 2013) per se. The main contribution of the paper is the Foreebank data set itself2 but we also carry out preliminary parsing experiments evaluating the accuracy of a PCFG-LA parser on Foreebank, examining the effect of grammatical errors on parsing and experimenting with different training sets. 2 Related Work Other treebanks of English web text include the English Web Treebank (aka Google Web Treebank) (Mott et al., 2012), the small treebank of tweets and football discussion forum posts described in Foster et al. (2011) and the tweet dependency bank described in Kong et al. (2014). The English Web Treebank is a corpus of over 250K words, selected from blogs, newsgroups, emails, local business reviews and Yahoo! answers. It adapts the Penn Treebank (Marcus et al., 1994) and Switchboard (Taylor, 1996) annotation guidelines to address the phenomena specific 2www.computing.dcu.ie/mt/confidentmt. html 1341 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. to this type of text. The annotation of the 1000- sentence treebank described</context>
</contexts>
<marker>Kong, Schneider, Swayamdipta, Bhatia, Dyer, Smith, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A. Smith. 2014. A dependency parser for tweets. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1001–10012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L¨udeling</author>
</authors>
<title>Mehrdeutigkeiten und kategorisierung: Probleme bei der annotation von lernerkorpora.</title>
<date>2008</date>
<booktitle>Fortgeschrittene Lernervarietten,</booktitle>
<pages>119--140</pages>
<editor>In M. Walter and P. Grommes, editors,</editor>
<publisher>Niemeyer, Tbingen.</publisher>
<marker>L¨udeling, 2008</marker>
<rawString>Anke. L¨udeling. 2008. Mehrdeutigkeiten und kategorisierung: Probleme bei der annotation von lernerkorpora. In M. Walter and P. Grommes, editors, Fortgeschrittene Lernervarietten, pages 119– 140. Niemeyer, Tbingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating Predicate Argument Structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 ARPA Speech and Natural Language Workshop,</booktitle>
<pages>114--119</pages>
<contexts>
<context position="3616" citStr="Marcus et al., 1994" startWordPosition="564" endWordPosition="567"> of a PCFG-LA parser on Foreebank, examining the effect of grammatical errors on parsing and experimenting with different training sets. 2 Related Work Other treebanks of English web text include the English Web Treebank (aka Google Web Treebank) (Mott et al., 2012), the small treebank of tweets and football discussion forum posts described in Foster et al. (2011) and the tweet dependency bank described in Kong et al. (2014). The English Web Treebank is a corpus of over 250K words, selected from blogs, newsgroups, emails, local business reviews and Yahoo! answers. It adapts the Penn Treebank (Marcus et al., 1994) and Switchboard (Taylor, 1996) annotation guidelines to address the phenomena specific 2www.computing.dcu.ie/mt/confidentmt. html 1341 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. to this type of text. The annotation of the 1000- sentence treebank described in Foster et al. (2011) is based on the Penn Treebank, whereas the annotation of the treebank described in Kong et al. (2014) is dependency-based. The French Social Media Bank developed by Se</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating Predicate Argument Structure. In Proceedings of the 1994 ARPA Speech and Natural Language Workshop, pages 114–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Mott</author>
<author>Ann Bies</author>
<author>John Laury</author>
<author>Colin Warner</author>
</authors>
<title>Bracketing Webtext: An Addendum to Penn Treebank II Guidelines.</title>
<date>2012</date>
<tech>Technical report, Linguistic Data Consortium.</tech>
<contexts>
<context position="1122" citStr="Mott et al., 2012" startWordPosition="159" endWordPosition="162">t a new treebank of English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 1 Introduction The last five years has seen a considerable amount of research carried out on web and social media text parsing, with new treebanks being created (Foster et al., 2011; Seddah et al., 2012; Mott et al., 2012; Kong et al., 2014), and new parsing systems developed (Petrov and McDonald, 2012; Kong et al., 2014). In this paper we explore a particular source of user-generated text, namely, posts from technical support forums, which are a popular means for customers to resolve their queries about a product. An accurate parser for this kind of text can be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French technical support </context>
<context position="3262" citStr="Mott et al., 2012" startWordPosition="503" endWordPosition="506">ecting all the errors does result in a performance increase of 1.5% for English and 0.8% for French, the major challenge in parsing these sentences seems not to be “bad language” (Eisenstein, 2013) per se. The main contribution of the paper is the Foreebank data set itself2 but we also carry out preliminary parsing experiments evaluating the accuracy of a PCFG-LA parser on Foreebank, examining the effect of grammatical errors on parsing and experimenting with different training sets. 2 Related Work Other treebanks of English web text include the English Web Treebank (aka Google Web Treebank) (Mott et al., 2012), the small treebank of tweets and football discussion forum posts described in Foster et al. (2011) and the tweet dependency bank described in Kong et al. (2014). The English Web Treebank is a corpus of over 250K words, selected from blogs, newsgroups, emails, local business reviews and Yahoo! answers. It adapts the Penn Treebank (Marcus et al., 1994) and Switchboard (Taylor, 1996) annotation guidelines to address the phenomena specific 2www.computing.dcu.ie/mt/confidentmt. html 1341 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347, Lisbo</context>
</contexts>
<marker>Mott, Bies, Laury, Warner, 2012</marker>
<rawString>Justin Mott, Ann Bies, John Laury, and Colin Warner. 2012. Bracketing Webtext: An Addendum to Penn Treebank II Guidelines. Technical report, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Nagata</author>
<author>Edward Whittaker</author>
<author>Vera Sheinman</author>
</authors>
<title>Creating a manually error-tagged and shallow-parsed learner corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>1210--1219</pages>
<contexts>
<context position="7324" citStr="Nagata et al. (2011)" startWordPosition="1154" endWordPosition="1157">(2010), and annotate it as is. They use two POS tags and two dependency labels for error cases: one for the surface form and one for the intended form. Ros´en and De Smedt (2010) criticise the approach of Dickinson and Ragheb (2009) involving “annotating language text as is” arguing that interpretation of the language is required at all annotation levels. They use NorGram, a Norwegian LexicalFunctional Grammar, to annotate a learner corpus with constituency structure, functional structure and semantic structure, in order to provide a means to search for contexts in which learner errors occur. Nagata et al. (2011) describe an English learner corpus which has been manually annotated with shallow syntax, introducing two new POS tags and two new chunk labels for errors. 3 Building the Foreebank The Foreebank treebank contains 1000 English sentences and 1000 French sentences. The English sentences come from the Symantec Norton technical support user forum. Half of the French sentences come from the French Norton forum and the other half are human translations of sentences from the English forum. Four annotators were involved in the annotation process. Their main task was to correct automatically parsed phr</context>
<context position="11270" citStr="Nagata et al. (2011)" startWordPosition="1831" endWordPosition="1834"> spelling errors (test instead of text) and lexical choice errors (desk instead of chair). The POS tag of the corrected form is used in the tree instead of the POS tag of the incorrect form.5 Although this annotation scheme contains fewer error types than the taxonomies used for learner cor4Minimal correction is encouraged to prevent annotators from rewriting the sentence in their preferred writing style. Instead they are instructed to just focus on fixing the errors. 5An alternative would have been to use one POS tag for the erroneous form and one for the corrected form, either combined a la Nagata et al. (2011) or separate a la Dickinson and Ragheb (2009). pora, its granularity increases when the error suffixes are interpreted in the syntactic context in which they occur. For example, we can distinguish a missing determiner (DT D) from a missing preposition (IN D). The “sentences” that the annotators see are the result of passing the forum text through an automatic sentence splitter (NLTK6) and tokeniser (inhouse). This is another important difference between Foreebank and the English Web Treebank (EWT). In the EWT, sentence boundary detection and tokenisation has been carried out manually before an</context>
</contexts>
<marker>Nagata, Whittaker, Sheinman, 2011</marker>
<rawString>Ryo Nagata, Edward Whittaker, and Vera Sheinman. 2011. Creating a manually error-tagged and shallow-parsed learner corpus. In Proceedings of ACL-HLT, pages 1210–1219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Yuanbin Wu</author>
<author>Christian Hadiwinoto</author>
<author>Joel Tetreault</author>
</authors>
<date>2013</date>
<booktitle>The CoNLL2013 Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="6024" citStr="Ng et al., 2013" startWordPosition="939" endWordPosition="942">koudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) developed for first language learners. They treat the developing </context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL2013 Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Ted Briscoe</author>
<author>Christian Hadiwinoto</author>
<author>Raymond Hendy Susanto</author>
<author>Christopher Bryant</author>
</authors>
<date>2014</date>
<booktitle>The CoNLL-2014 Shared Task on Grammatical Error Correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--14</pages>
<contexts>
<context position="6042" citStr="Ng et al., 2014" startWordPosition="943" endWordPosition="946">2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) developed for first language learners. They treat the developing language of learne</context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The CoNLL-2014 Shared Task on Grammatical Error Correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Nicholls</author>
</authors>
<title>The cambridge learner corpus – error coding and analysis.</title>
<date>1999</date>
<booktitle>In Summer Workshop on Learner Corpora,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="5402" citStr="Nicholls, 1999" startWordPosition="834" endWordPosition="835">ovides more insight into this type of text but it also enables us to directly measure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared </context>
</contexts>
<marker>Nicholls, 1999</marker>
<rawString>Diane Nicholls. 1999. The cambridge learner corpus – error coding and analysis. In Summer Workshop on Learner Corpora, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="15970" citStr="Petrov and Klein, 2007" startWordPosition="2676" endWordPosition="2679">n of the tokens in both data sets. We also calculate the edit distance between each Foreebank sentence and its correction by summing the number of error suffixes and dividing by the maximum of the original and corrected sentence lengths. The average edit distance for the English section of Foreebank is 0.04 and for the French section is 0.03. Despite the existence of some near-to-incomprehensible sentences, the overall error level is very low. 5 Parsing the Foreebank We first evaluate newswire-trained parsers on Foreebank, using our in-house PCFG-LA parser with the max-rule parsing algorithm (Petrov and Klein, 2007) and 6 split-merge cycles. The English model is trained on the entire WSJ and the French model on the entire FTB. For comparison, we parse the WSJ/FTB and so we additionally use models trained only on the training sections. We remove the error suffixes and any Dsuffixed nodes (representing deleted words) from the gold Foreebank trees before evaluation. The results are shown in Table 3. As expected, we see a significant drop for both languages when we move from in-domain data to Foreebank. Compared to parsing the English side of Foreebank, the performance drop for French is relatively smaller: </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proceedings of HLTNAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL),</booktitle>
<pages>59</pages>
<contexts>
<context position="1204" citStr="Petrov and McDonald, 2012" startWordPosition="172" endWordPosition="175">een annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 1 Introduction The last five years has seen a considerable amount of research carried out on web and social media text parsing, with new treebanks being created (Foster et al., 2011; Seddah et al., 2012; Mott et al., 2012; Kong et al., 2014), and new parsing systems developed (Petrov and McDonald, 2012; Kong et al., 2014). In this paper we explore a particular source of user-generated text, namely, posts from technical support forums, which are a popular means for customers to resolve their queries about a product. An accurate parser for this kind of text can be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French technical support forums.1 The phrase structure of the sentences is annotated and any grammatical er</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL), 59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="8716" citStr="Petrov et al., 2006" startWordPosition="1372" endWordPosition="1375">ted version of the English Web Treebank bracketing guidelines. The French annotators used the French treebank (FTB) (Abeill´e et al., 2003) guidelines, following the SPMRL strategy for multiword expressions (Seddah et al., 2013; Candito and Crabb´e, 2009). The two primary annotators, one for French and one for English, annotated all the data for their language. The two secondary annotators annotated a 100-sentence subset. Inter-annotator agreement was calculated by measuring the Parseval 3The Stanford parser (Klein and Manning, 2003) was used to parse the English data and the Berkeley parser (Petrov et al., 2006) was used for the French sentences. 1342 Figure 1: The Foreebank annotation of i tried customize too, but i cant find them .. T T corrected as I tried Customize too, but Ica n’t find them ... T T Suffix Explanation Example FBen FBfr English French # % # % D Deleted token It fixed [the] problem. Cela a r´esolu [le] probl`eme. 170 1.10 56 0.29 X Extraneous tokens It fixed the my problem. Cela a r´esolu le mon probl`eme. 35 0.23 17 0.09 W Wrong form error It fix the problem. Cela r´esoudre le probl`eme. 69 0.45 43 0.22 S Misspelled token It fixed my prbolem. Cela a r´esolu mon prbol`eme. 81 0.53 </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact and Interpretable Tree Annotation. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Hagen Hirschmann</author>
<author>Anke L¨udeling</author>
<author>Marc Reznicek</author>
</authors>
<title>Better tags give better trees or do they? Linguistic Issues in</title>
<date>2012</date>
<journal>Language Technology,</journal>
<volume>7</volume>
<issue>10</issue>
<marker>Rehbein, Hirschmann, L¨udeling, Reznicek, 2012</marker>
<rawString>Ines Rehbein, Hagen Hirschmann, Anke L¨udeling, and Marc Reznicek. 2012. Better tags give better trees or do they? Linguistic Issues in Language Technology, 7(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Ros´en</author>
<author>Koenraad De Smedt</author>
</authors>
<title>Syntactic annotation of learner corpora. Systematisk, variert, men ikke tilfeldig,</title>
<date>2010</date>
<pages>120--132</pages>
<marker>Ros´en, De Smedt, 2010</marker>
<rawString>Victoria Ros´en and Koenraad De Smedt. 2010. Syntactic annotation of learner corpora. Systematisk, variert, men ikke tilfeldig, pages 120–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Eric Davis</author>
<author>Alon Lavie</author>
<author>Brian MacWhinney</author>
<author>Shuly Wintner</author>
</authors>
<title>High-accuracy annotation and parsing of childes transcripts.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="6558" citStr="Sagae et al., 2007" startWordPosition="1022" endWordPosition="1025">, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) developed for first language learners. They treat the developing language of learners as an interlanguage, as suggested by D´ıaz-Negrillo et al. (2010), and annotate it as is. They use two POS tags and two dependency labels for error cases: one for the surface form and one for the intended form. Ros´en and De Smedt (2010) criticise the approach of Dickinson and Ragheb (2009) involving “annotating language text as is” arguing that interpretation of the language is required at all annotation levels. They use NorGram, a Norwegian LexicalFunctional Grammar, to annotate a learner corpus with const</context>
</contexts>
<marker>Sagae, Davis, Lavie, MacWhinney, Wintner, 2007</marker>
<rawString>Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhinney, and Shuly Wintner. 2007. High-accuracy annotation and parsing of childes transcripts. In Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Benoit Sagot</author>
<author>Marie Candito</author>
<author>Virginie Mouilleron</author>
<author>Vanessa Combet</author>
</authors>
<title>The French Social Media Bank: a Treebank of Noisy User Generated Content.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>2441--2458</pages>
<contexts>
<context position="1103" citStr="Seddah et al., 2012" startWordPosition="155" endWordPosition="158">om Abstract We present a new treebank of English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 1 Introduction The last five years has seen a considerable amount of research carried out on web and social media text parsing, with new treebanks being created (Foster et al., 2011; Seddah et al., 2012; Mott et al., 2012; Kong et al., 2014), and new parsing systems developed (Petrov and McDonald, 2012; Kong et al., 2014). In this paper we explore a particular source of user-generated text, namely, posts from technical support forums, which are a popular means for customers to resolve their queries about a product. An accurate parser for this kind of text can be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French</context>
<context position="4234" citStr="Seddah et al. (2012)" startWordPosition="653" endWordPosition="656">4) and Switchboard (Taylor, 1996) annotation guidelines to address the phenomena specific 2www.computing.dcu.ie/mt/confidentmt. html 1341 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. to this type of text. The annotation of the 1000- sentence treebank described in Foster et al. (2011) is based on the Penn Treebank, whereas the annotation of the treebank described in Kong et al. (2014) is dependency-based. The French Social Media Bank developed by Seddah et al. (2012) is a treebank of 1,700 French sentences from various type of social media including Facebook, Twitter and discussion forums (video game and medical). An extended version of the FTB-UC annotation guidelines (Candito and Crabb´e, 2009) is employed during annotation and subcorpora containing particularly noisy utterances are identified. The main difference between Foreebank and other web/social media treebanks is that grammatical errors in the Foreebank sentences are marked and corrected as part of the annotation process. Error annotation not only provides more insight into this type of text but</context>
</contexts>
<marker>Seddah, Sagot, Candito, Mouilleron, Combet, 2012</marker>
<rawString>Djam´e Seddah, Benoit Sagot, Marie Candito, Virginie Mouilleron, and Vanessa Combet. 2012. The French Social Media Bank: a Treebank of Noisy User Generated Content. In Proceedings of COLING, pages 2441–2458.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Djam´e Seddah</author>
<author>Reut Tsarfaty</author>
<author>Sandra K¨ubler</author>
<author>Marie Candito</author>
<author>Jinho D Choi</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
</authors>
<title>Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, Alina Wr´oblewska, and Eric Villemonte de la Clergerie.</title>
<date>2013</date>
<journal>Overview of the SPMRL</journal>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>146--182</pages>
<marker>Seddah, Tsarfaty, K¨ubler, Candito, Choi, Farkas, Foster, 2013</marker>
<rawString>Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie Candito, Jinho D. Choi, Rich´ard Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepi´orkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, Alina Wr´oblewska, and Eric Villemonte de la Clergerie. 2013. Overview of the SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146– 182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Taylor</author>
</authors>
<title>Bracketing Switchboard: An Addendum to the Treebank II Guidelines.</title>
<date>1996</date>
<tech>Technical report.</tech>
<contexts>
<context position="3647" citStr="Taylor, 1996" startWordPosition="570" endWordPosition="571">mining the effect of grammatical errors on parsing and experimenting with different training sets. 2 Related Work Other treebanks of English web text include the English Web Treebank (aka Google Web Treebank) (Mott et al., 2012), the small treebank of tweets and football discussion forum posts described in Foster et al. (2011) and the tweet dependency bank described in Kong et al. (2014). The English Web Treebank is a corpus of over 250K words, selected from blogs, newsgroups, emails, local business reviews and Yahoo! answers. It adapts the Penn Treebank (Marcus et al., 1994) and Switchboard (Taylor, 1996) annotation guidelines to address the phenomena specific 2www.computing.dcu.ie/mt/confidentmt. html 1341 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. to this type of text. The annotation of the 1000- sentence treebank described in Foster et al. (2011) is based on the Penn Treebank, whereas the annotation of the treebank described in Kong et al. (2014) is dependency-based. The French Social Media Bank developed by Seddah et al. (2012) is a treeban</context>
</contexts>
<marker>Taylor, 1996</marker>
<rawString>Ann Taylor. 1996. Bracketing Switchboard: An Addendum to the Treebank II Guidelines. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
</authors>
<title>A New Dataset and Method for Automatically Grading ESOL Texts.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>180--189</pages>
<contexts>
<context position="5431" citStr="Yannakoudakis et al., 2011" startWordPosition="836" endWordPosition="839">ght into this type of text but it also enables us to directly measure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et</context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A New Dataset and Method for Automatically Grading ESOL Texts. In Proceedings ofACL, pages 180–189.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
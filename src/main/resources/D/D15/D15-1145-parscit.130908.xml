<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997934">
Graph-Based Collective Lexical Selection
for Statistical Machine Translation
</title>
<author confidence="0.999241">
Jinsong Su1,2, Deyi Xiong3∗, Shujian Huang2, Xianpei Han4, Junfeng Yao1
</author>
<affiliation confidence="0.9774265">
Xiamen University, Xiamen, P.R. China1
State Key Laboratory for Novel Software Technology, Nanjing University, P.R. China2
Soochow University, Suzhou, P.R. China3
Institute of Software, Chinese Academy of Sciences, Beijing, P.R. China4
</affiliation>
<email confidence="0.9767685">
{jssu, yao0010}@xmu.edu.cn huangsj@nlp.nju.edu.cn
dyxiong@suda.edu.cn xianpei@nfs.iscas.ac.cn
</email>
<sectionHeader confidence="0.994558" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796947368421">
Lexical selection is of great importance
to statistical machine translation. In this
paper, we propose a graph-based frame-
work for collective lexical selection. The
framework is established on a translation
graph that captures not only local associ-
ations between source-side content words
and their target translations but also target-
side global dependencies in terms of relat-
edness among target items. We also in-
troduce a random walk style algorithm to
collectively identify translations of source-
side content words that are strongly related
in translation graph. We validate the ef-
fectiveness of our lexical selection frame-
work on Chinese-English translation. Ex-
periment results with large-scale training
data show that our approach significantly
improves lexical selection.
</bodyText>
<sectionHeader confidence="0.99889" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998870833333333">
Lexical selection, which selects appropriate trans-
lations for lexical items on the source side, is a cru-
cial task in statistical machine translation (SMT).
The task is closely related to two factors: 1) asso-
ciations of selected translations with lexical items
on the source side, including corresponding source
items and their neighboring words, and 2) depen-
dencies1 between selected target translations and
other items on the target side.
As translation rules and widely-used n-gram
language models can only capture local associ-
ations and dependencies, we have witnessed in-
</bodyText>
<note confidence="0.732165">
∗Corresponding author.
</note>
<footnote confidence="0.9847265">
1Please note that dependencies in this paper are not nec-
essarily syntactic dependencies.
</footnote>
<bodyText confidence="0.9996075625">
creasing efforts that attempt to incorporate non-
local associations/dependencies into lexical selec-
tion. Efforts using source-side associations mainly
focus on the exploitation of either sentence-level
context (Chan et al., 2007; Carpuat and Wu, 2007;
Hasan et al., 2008; Mauser et al., 2009; He et
al., 2008; Shen et al., 2009) or the utilization of
document-level context (Xiao et al., 2011; Ture et
al., 2012; Xiao et al., 2012; Xiong et al., 2013).
In contrast, target-side dependencies attract little
attention, although they have an important im-
pact on the accuracy of lexical selection. The
most common practice is to use language mod-
els to estimate the strength of target-side depen-
dencies (Koehn et al., 2003; Shen et al., 2008;
Xiong et al., 2011). However, conventional n-
gram language models are not good at capturing
long-distance dependencies. Consider the exam-
ple shown in Figure 1. As the translations of pol-
ysemous words “w`ent´ı”, “ch/yˇou” and “lichˇang”
are far from each other, our baseline can only
correctly translate “lichˇang” as “stance”. It in-
appropriately translates the other two words as
“problem” and null, respectively, even with the
support of an n-gram language model. If we
could model long-distance dependencies among
target translations of source words “w`ent´ı”(issue),
“ch/yˇou”(hold) and “lichˇang”(stance), these trans-
lation errors could be avoided.
In order to model target-side global dependen-
cies, we propose a novel graph-based collective
lexical selection framework for SMT. Specifically,
</bodyText>
<listItem confidence="0.8216476">
• First, we propose a translation graph to model
not only local associations between source-
side content words and their target trans-
lations but also global relatedness among
target-side items.
</listItem>
<page confidence="0.936495">
1238
</page>
<note confidence="0.9919305">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1238–1247,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.601778428571428">
Src:� T/P kk/DT jp7)&amp;/NN ,/PU 3R) /PN 4�Y/VV Mn/VA n/DEG A*/NN
duìyú zhè wèntí , shudngfdng chíyŏu xidngtóng de lìchăng
wèntí = {problem, question, issue ...}
chíyŏu = {hold, null, possess ...}
lìchăng = {stance, position ...}
Tran: For this problem , two sides of the same or similar stance .
Ref: Both sides hold the same or similar position on this issue .
</figure>
<figureCaption confidence="0.9977955">
Figure 1: A Chinese-English translation example to illustrate the importance of target-side long-distance dependencies for lex-
ical selection. Dotted lines show long-distance dependencies of source content words. Three content words “w`ent´ı”, “chiyˇou”,
“l`ıchˇang”, and their candidate translations with high translation probabilities are also presented. Src: A Chinese sentence with
part-of-speech tags. Tran: system output. Ref: reference translation.
</figureCaption>
<listItem confidence="0.973798833333333">
• Second, we introduce a collective lexical se-
lection algorithm, which can jointly identify
translations of all source-side content words
in the translation graph.
• Finally, we incorporate confidence scores
of candidate translations in the translation
</listItem>
<bodyText confidence="0.930488823529412">
graph, which are derived by the collective se-
lection algorithm, into SMT to improve lexi-
cal selection.
We validate the effectiveness of our graph-
based lexical selection framework on a hierarchi-
cal phrase-based system (Chiang, 2007). Exper-
iment results on the NIST Chinese-English test
sets show that our approach significantly improves
translation quality.
We begin in Section 2 with the construction
of translation graph for each translated sentence.
Then, we propose a graph-based collective lexical
selection framework for SMT in Section 3. Ex-
periment results are reported in Section 4. We
summarize and compare related work in Section
5. Finally, Section 6 presents conclusions and di-
rections for future research.
</bodyText>
<sectionHeader confidence="0.886153" genericHeader="method">
2 Translation Graph
</sectionHeader>
<bodyText confidence="0.9997783">
Formally, a translation graph is a weighted graph
G=(N, E). In the node set N, each node repre-
sents either a source word or a target translation
that contains one or multiple target words. In the
edge set E, an edge linking a source word to a tar-
get translation is referred to as a source-target as-
sociation edge, and an edge connecting two target
translations is called as a target-target relatedness
edge. In Section 2.1 and 2.2, we will answer the
following two questions on the translation graph:
</bodyText>
<listItem confidence="0.9526366">
• Which source words and their translations
should be included in the translation graph?
• How can we measure the strength of the
above two types of relations in the graph with
edge weights?
</listItem>
<subsectionHeader confidence="0.989327">
2.1 Graph Nodes
</subsectionHeader>
<bodyText confidence="0.999794821428572">
For a source sentence, the most ideal translation
graph is a graph that includes all source words
and their candidate translations. However, this
ideal graph has two problems: intensive compu-
tation for graph inference and difficulty in mod-
eling dependencies between function and content
words. In order to get around these two issues, we
only consider lexical selection for source content
words2.
We first identify source-side content word pairs
using statistical metrics, and then keep word pairs
with a high relatedness strength in the translation
graph. To be specific, we use pointwise mutual in-
formation (PMI) (Church and Hanks, 1990) and
co-occurrence frequency to measure the related-
ness strength of two source-side words s and s&apos;
within a window ds. Content word pairs will
be kept when their co-occurrence frequencies are
more than ccf times in our training corpus and
PMI values are larger than cpmi. In this process,
we remove noisy word pairs using the following
heuristic rules: (1) As an adjective only has rela-
tions with its head nouns or dependent adverbs,
we remove all word pairs where an adjective is
paired with words other than its head nouns or
dependent adverbs; (2) We apply a similar con-
straint to adverbs too, since the same thing hap-
pens to an adverb and its head verb or head ad-
</bodyText>
<footnote confidence="0.9995475">
2In this work, we consider nouns, verbs, adjectives and
adverbs as content words in the source/target language.
</footnote>
<page confidence="0.994364">
1239
</page>
<figure confidence="0.999489133333333">
wèntf
target translation
source word
source-target association edge
target-target relatedness edge
problem
question
issue
stance
lìchăng
position
possess
null
hold
chfyŏu
</figure>
<figureCaption confidence="0.991979">
Figure 2: Translation graph of the example shown in Figure 1. Relatedness scores on edges are shown for two group of
translations {“problem”, null, “stance”} (green) and {“issue”, “hold”, “position”} (blue), which are estimated with PMI.
Note that the null node does not have any relations with other nodes. Besides, two translation combinations {“problem”, null,
“stance”} (green) and {“issue”, “hold”, “position”} (blue) have different strengths of relatedness.
</figureCaption>
<bodyText confidence="0.909373">
jective. For example, in the Chinese sentence in
Figure 1, the adjective “xi¯angt´ong” is only related
to the noun “lichˇang” although it also frequently
co-occur with “w`ent´ı”.
After identifying source-side content word
pairs, we collect all target translations of these
content words from extracted bilingual rules ac-
cording to word alignments. These content words
and target translations are used to build a transla-
tion graph, where each node represents a source-
side content word or a candidate target transla-
tion. Note that there may be hundreds of differ-
ent translations for a source word. For simplicity,
we only consider target translations from transla-
tion options that are adopted by the decoder after
rule filtering. Let’s revisit the example in Figure 2,
we include the following target translations in the
translation graph: “problem”, “question”, “issue”,
“hold”, null, “possess”, “stance” and “position”.
</bodyText>
<subsectionHeader confidence="0.999267">
2.2 Edges and Weights
</subsectionHeader>
<bodyText confidence="0.999542333333333">
In this section, we introduce how we calculate
weights for two kinds of edges in a translation
graph.
</bodyText>
<subsectionHeader confidence="0.843815">
2.2.1 Source-Target Association Edge
</subsectionHeader>
<bodyText confidence="0.999747">
Connecting a source-side content word and its
candidate target translations, a source-target asso-
ciation edge provides a way to propagate transla-
tion association evidence from a source word to
its candidate translations. Obviously, the stronger
the association between a source word and its can-
didate translation, the more evidence the corre-
sponding edge will propagate. For each source-
side content word, we obtain its candidate trans-
lations via the kept word alignments. Following
Xiong et al. (2014), we allow a target translation
to be either a phrase of length up to 3 words or
null when s is not aligned to any word in the cor-
responding bilingual rule. We define the weight of
the edge from a source-side content word s to its
target translation t˜ as follows:
</bodyText>
<equation confidence="0.995616666666667">
TP(s, ˜t)
Weight(s → ˜t) = (1)
E˜t&apos; ∈N(s) TP(s, t&apos;)
</equation>
<bodyText confidence="0.9999267">
where N(s) denotes the set of candidate target
translations of s kept on the translation graph,
and TP(s, ˜t) measures the probability of s be-
ing translated to ˜t. It is very important to note
that there is no evidence propagated from a target
translation to a source word, as source-target asso-
ciation edges only go from a source-side content
word to its translations.
We compute TP(s, ˜t) according to the principle
of maximal likelihood as follows:
</bodyText>
<equation confidence="0.993177">
˜t)
TP(s, ˜t) = count(s, (2)
count(s)
</equation>
<bodyText confidence="0.999762666666667">
where count(s, ˜t) indicates how often s is aligned
to t˜ in the training corpus. Using this method,
we can compute the translation probabilities of
the source-target association edges in Figure 2 as
follows: TP(“w`ent´ı”, “issue”)=0.31, TP(“ch´ıyˇou”,
“hold”)=0.22 and TP(“lichˇang”, “position”)=0.47.
</bodyText>
<subsectionHeader confidence="0.683856">
2.2.2 Target-Target Relatedness Edge
</subsectionHeader>
<bodyText confidence="0.998582666666667">
Connecting two target translations of different re-
lated source content words, a target-target relat-
edness edge enables translation graph to capture
</bodyText>
<page confidence="0.935019">
1240
</page>
<bodyText confidence="0.999500923076923">
dependencies between translations of any two dif-
ferent source words.
Computing the weight of a target-target related-
ness edge is crucial for our method. Intuitively,
the stronger co-occurrence strength two transla-
tions have, the more evidence should be propa-
gated between them. Therefore we calculate the
weight of a target-target related edge based on the
co-occurrence strength of two translations linked
by the edge. Formally, given the translation t˜ of
source-side content word s and the translation ˜t&apos;
of source-side content word s&apos;, the weight of the
edge from t˜ to ˜t&apos; is defined as follows:
</bodyText>
<equation confidence="0.991017333333333">
&apos; RS(˜t, ˜t&apos;)
Weight (˜t → t)= (3)
Ei &amp;quot;EN(˜t) RS(t, t&apos;&apos;)
</equation>
<bodyText confidence="0.974526294117647">
where N(˜t) denotes the set of candidate transla-
tions that link to ˜t, and RS(˜t, ˜t&apos;) measures the
strength of relatedness between t˜ and ˜t&apos; which is
calculated as the average word-level relatedness
over all content words in these two translations t˜
and
˜t&apos;.
As for the word-level relatedness RS(t, t&apos;) for
a content word pair (t, t&apos;), we estimate it with
the following two approaches over collected co-
occurring word pairs within a window of size dt:
(1) RS(t, t&apos;) is computed as a bigram conditional
probability plm(t&apos;|t) via the language model; (2)
Following (Xiong et al., 2011) and (Liu et al.,
2014), we employ PMI to define RS(t, t&apos;) as
ln p(t,t&apos;)
p(t)p(t&apos;).
</bodyText>
<sectionHeader confidence="0.959222" genericHeader="method">
3 Collective Lexical Selection Algorithm
</sectionHeader>
<bodyText confidence="0.99906">
Based on the translation graph, we propose a col-
lective lexical selection algorithm to jointly iden-
tify translations of all source words in the graph.
</bodyText>
<subsectionHeader confidence="0.999944">
3.1 Problem Statement and Solution Method
</subsectionHeader>
<bodyText confidence="0.940247527777778">
As stated previously, the translation of a source-
side content word s should be: 1) associated with
s; 2) related to the translations of other source-side
content words. Thus, in the translation graph, the
translation of s should be a target-side node which
has: 1) an association edge with the node of s;
2) many relatedness edges with other target-side
nodes that represent translations of other source
words.
Let’s revisit Figure 2. If we know that the trans-
lation of “w`ent1” is “issue”, the relatedness be-
tween (“issue”, “hold”) and between (“issue”,
“position”) can provide evidences that “hold” and
“position” are the correct translations of “ch1yˇou”
and “l`ıchˇang”, respectively. On the other hand,
the candidate translation “problem” is less related
to “hold” and “position”, which may suggest that
it is not likely to be the correct translation of
“w`ent1”, even if it has a strong source-target as-
sociation relation with “w`ent´ı”. However, in the
translation graph, the correct target translation of
a source word depends on correct translations of
other source words in the graph, and vice versa.
So how do we find these correct translations?
We propose a Random Walk (Gobel and Jagers,
1974) style algorithm to solve this problem, aim-
ing to use both local source-target associations
and global target-target relatedness simultane-
ously during translation. In our algorithm, we as-
sign each node an evidence score in the transla-
tion graph, which indicates either the importance
of a source word (for a source word node) or the
confidence of a target translation being a correct
translation (for a target word node). Specifically,
we perform collective inference on the translation
graph as follows:
</bodyText>
<listItem confidence="0.9888072">
• First, we set initial evidence scores for nodes
in the translation graph.
• Second, evidence scores are simultaneously
updated by propagating evidences along
edges in the translation graph.
</listItem>
<bodyText confidence="0.99739">
In the following sub-section, we describe the two
steps in detail.
</bodyText>
<subsectionHeader confidence="0.999871">
3.2 Details of Our Algorithm
</subsectionHeader>
<bodyText confidence="0.999823666666667">
Using the algorithm shown in Algorithm 1, we
iteratively derive evidence scores for candidate
translations.
</bodyText>
<subsectionHeader confidence="0.763125">
3.2.1 Notations
</subsectionHeader>
<bodyText confidence="0.984098">
For a translation graph with n nodes, we assign
each node an index from 1 to n and use this index
to represent the node. We also use the following
two notations:
• The evidence vector V: an n-dimensional
vector where the ith component VZ is the ev-
idence score contained in this node (if node
i corresponds to a source word), or the evi-
dence score from the related translations (if
node i corresponds to a target translation). In
particular, we use V(0) to denote the initial
</bodyText>
<page confidence="0.973063">
1241
</page>
<tableCaption confidence="0.332688333333333">
Algorithm 1 Collective Inference in Translation Graph.
Input: S: the set of source-side content words, and S(i)
denotes the source word of node i;
</tableCaption>
<bodyText confidence="0.611517285714286">
k: the number of source-side content words ;
T: the set of all candidate target translations, and T(j)
denotes the target translation of node j;
l: the number of candidate target translations;
A: the reallocation weight;
maxIter: the maximum iteration number;
e: the difference threshold;
</bodyText>
<listItem confidence="0.717874444444444">
1: for i = 1, 2..., k
2: for j = 1, 2..., l
3: if S(i) is linked to T(j)
4: Mk+j,i ← Weight(S(i) → T(j))
5: for j1 = 1, 2..., l
6: for j2 = 1, 2..., l
7: if T(j1) is linked to T(j2)
8: Mk+j2,k+j1 → Weight(T(j1) → T(j2))
9: for i = 1, 2..., k
</listItem>
<equation confidence="0.910157">
10: V(0)
i ← Importance(S(i))
11: for j = 1, 2..., l
12: V(0)
k+j ← 0
13: 6 ← ∞
14: r ← 1
</equation>
<listItem confidence="0.827704333333333">
15: while r ≤ maxIter &amp;&amp; 6 &gt; e do
16: V(r) ← (1 − A) × M × V(r−1) + A × V(0)
17: 6 ← kV(r) − V(r−1)k2
18: r ← r + 1
19: end while
20: for i = 1, 2..., k
21: for j = 1, 2..., l
22: if S(i) is linked to T(j)
23: LexiTable(S(i), T(j)) ← normalize(V(r)
k+j)
Return: LexiTable;
evidence vector, and V(r) to represent the ev-
idence vector we obtain at the rth iteration.
• The evidence propagation matrix M: an
n x n matrix where Mij is the evidence prop-
agation ratio from node j to node i, and its
value is the weight of the edge from node j
to node i.
</listItem>
<subsectionHeader confidence="0.797484">
3.2.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.993329769230769">
In Algorithm 1, we jointly infer the evidence
scores of all candidate translations in the follow-
ing three steps.
In Step 1, we calculate the evidence propaga-
tion matrix M according to the method described
in Section 2.2 (equations (1) and (3)) (Lines 1-8).
In Step 2, we adopt different methods to set the
value of V(0) according to the node type. If the
node corresponds to a source word, we set the ini-
tial value using its importance score in the trans-
lation graph, as implemented in (Han et al. 2011)
(Lines 9-10). We calculate the importance score
of the source word s using tf.idf as follows:
</bodyText>
<equation confidence="0.960125333333333">
tf.idf(s)
Importance(s) = (4)
K&amp;quot;ENsrc tf.idf (s&apos;)
</equation>
<bodyText confidence="0.999412083333334">
where Nsrc is the set of source words in the trans-
lation graph. If the node corresponds to a target
translation, its initial evidence score is 0 (Lines
11-12).
In Step 3, evidences are simultaneously rein-
forced by propagating them among semantically
related translations (Lines 13-19). Specific to our
algorithm, we update them by propagating evi-
dences according to different types of relations in
the evidence propagation matrix M. Formally, the
recursive update of the evidence vector is defined
as follows:
</bodyText>
<equation confidence="0.999867">
V(r) = M x V(r−1) (5)
</equation>
<bodyText confidence="0.997877090909091">
where r is the number of iterations.
One problem with the above equation is that
some nodes in the translation graph do not have
evidence outgoing edges, such as translation nodes
containing only function words or the null node.
The evidence will disappear when passing through
these nodes. To solve this problem, we propagate
evidence in the form of reallocation: we reallocate
a fraction of evidence to the initial evidence vector
V(0) at each step. The new recursive update of the
evidence vector is formulated as follows:
</bodyText>
<equation confidence="0.994605">
V(r) = (1 − A) x M x V(r−1) + A x V(0) (6)
</equation>
<bodyText confidence="0.996370214285714">
where A E (0, 1) is the fraction of the reallocated
evidence. We keep updating the evidence vec-
tor according to this equation (Line 16), until the
maximal number of iteration maxIter is reached
or the Euclidean distance (Line 17) between evi-
dence vectors calculated in two consecutive itera-
tions is less than a pre-defined threshold c (Line
15).
In this way, we jointly infer the evidence scores
of all candidate target translations in the transla-
tion graph. Table 1 gives the evidence scores of
the example in Figure 2. We can find that our sys-
tem enhanced with target translation dependencies
is able to select correct translations.
</bodyText>
<subsectionHeader confidence="0.831275">
3.2.3 Integration of Derived Evidence Score
</subsectionHeader>
<bodyText confidence="0.9936555">
For each translated sentence, we may build multi-
ple translation graphs. For each translation graph,
</bodyText>
<page confidence="0.981802">
1242
</page>
<tableCaption confidence="0.433296">
scores of the source words
scores of the target translations
</tableCaption>
<table confidence="0.972990333333333">
w`ent´ı ch´ıy&amp;quot;ou licha&amp;quot;ng problem question issue hold null possess stance position
V(0) 0.2015 0.3989 0.3996 0 0 0 0 0 0 0 0
V(r) 0.0302 0.0598 0.0599 0.0533 0.0774 0.1071 0.1218 0.0244 0.0604 0.1186 0.1486
</table>
<tableCaption confidence="0.98783">
Table 1: The initial and final evidence scores of some source words and their target translations in Figure 2. Here we set the
reallocation weight A as 0.15. Note that the translations “issue”, “hold” and “position” are given high evidence scores.
</tableCaption>
<bodyText confidence="0.998952866666667">
we infer evidence scores of translations repre-
sented by graph nodes using the above-mentioned
algorithm before decoding. Then, for each can-
didate translation of a source-side content word,
we normalize its evidence score over the cor-
responding translation graph to form an addi-
tional lexical translation probability (Lines 20-
23). For instance, the normalized evidence score
of “ch´ıyˇou” translated into “hold” is calculated as
0.1218/(0.1218 + 0.0244 + 0.0604) ≈ 0.5895. In
this way, for each bilingual rule with word align-
ments, we will obtain a new lexical weight which
can be used together with the original translation
probabilities and lexical weight to improve lexical
selection in SMT.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.956671">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999945166666667">
Our bilingual training corpus is the combina-
tion of the FBIS corpus and Hansards part of
LDC2004T07 corpus (1M parallel sentences,
54.6K documents, with 25.2M Chinese words
and 29M English words). We word-aligned them
using GIZA++ (Och and Ney, 2003) with the op-
tion “grow-diag-final-and”. We chose the NIST
evaluation set of 2005 (MT05) as the development
set, and the sets of MT06/MT08 as test sets. We
used SRILM Toolkit (Stolcke, 2002) to train one
5-gram language model on the Xinhua portion of
Gigaword corpus.
To construct translation graphs, we first used the
ZPar toolkit3 and the Stanford toolkit4 to prepro-
cess (word segmentation, PoS tagging and so on)
Chinese and English sentences, respectively. We
used the Chinese part of our bilingual corpus and
an additional Chinese LDC Xinhua news corpus
(10.2M sentences with 279.9M words) as train-
ing data to collect Chinese word pairs. We set
window size ds=15, thresholds cpni=0, ccf=5 to
identify Chinese related word pairs in the NIST
translated sentences. Averagely, these three sets
contain 13.5, 10.3 and 9.5 content words used
</bodyText>
<footnote confidence="0.9999115">
3http://people.sutd.edu.sg/∼yue zhang/doc/index.html
4http://nlp.stanford.edu/software
</footnote>
<bodyText confidence="0.999812827586207">
to build translation graphs per sentence, respec-
tively. Using the English part of our bilingual cor-
pus and the Xinhua portion of Gigaword corpus
as training data, we set window size dt=20, and
used the SRILM toolkit with Witten-Bell smooth-
ing and PMI to calculate relatedness strengths for
target-side translations. To avoid data sparseness,
we build the graph using the surface forms of
words while calculating the word relatedness at
the lemma level. To achieve this, we converted
each word into its corresponding lemma with the
exception of adjectives and adverbs. In the proce-
dure of collective lexical selection, the difference
threshold c was set as 10−10, and the maximal it-
eration number maxIter 100.
We reimplemented the decoder of Hiero (Chi-
ang, 2007), a famous hierarchical phrase-based
(HPB) system. HPB system is a formally syntax-
based system and delivers good performance in
various translation evaluations. During decod-
ing, we set the ttable-limit as 20, the stack-size as
100. The translation quality is evaluated by case-
insensitive BLEU-4 metric (Papineni et al., 2002).
To alleviate the impact of the instability of MERT
(Och, 2003), we ran it three times for each exper-
iment and reported the average BLEU scores as
suggested in (Clark et al., 2011). Finally, we con-
ducted paired bootstrap sampling (Koehn, 2004)
to test the significance in BLEU score differences.
</bodyText>
<subsectionHeader confidence="0.987173">
4.2 Our Method vs Other Methods
</subsectionHeader>
<bodyText confidence="0.999666833333333">
In the first group of experiments, we investigated
the effectiveness of our model by comparing it
against the baseline as well as two additional mod-
els: (1) lexicalized rule selection model (He et
al., 2008) (LRSM), which employs local context
to improve rule selection in the HPB system; (2)
topic similarity model (Xiao et al., 2012)5 (TSM),
which explores document-level topic information
for translation rule selection in the HPB system.
Furthermore, we combined our model with the
two models to see if we could obtain further im-
provements. For this, we integrated the new lexi-
</bodyText>
<footnote confidence="0.978543">
5We used 30 topics following (Xiao et al., 2012).
</footnote>
<page confidence="0.865051">
1243
</page>
<table confidence="0.99987275">
System MT06 MT08 Avg
Baseline 30.25 21.25 25.75
LRSM 31.12 21.98 26.55
TSM 30.79 21.90 26.35
GM(LM) 30.64 21.78 26.21
GM(PMI) 31.02 21.77 26.40
LRSM +GM(PMI) 31.66 22.23 26.95
TSM +GM(PMI) 31.34 22.26 26.80
</table>
<tableCaption confidence="0.996965">
Table 2: Experiment results on the test sets with A=0.15. Avg
</tableCaption>
<bodyText confidence="0.967599073170732">
= average BLEU scores, GM(LM) and GM(PMI) denote our
model using the measure based on language model and PMI,
respectively.
cal weight learned by our model as a new feature
into the LRSM/TSM system.
Table 2 reports the results. All models outper-
form the baseline. Especially, our graph-based
lexical selection model GM(PMI) achieves an av-
erage BLEU score of 26.40 on the two test sets,
which is higher than that of the baseline by 0.65
BLEU points. This improvement is statistically
significant at p&lt;0.01. The BLEU score of our
model is close to those of LRSM and TSM, which
achieve an average BLEU score of 26.55 and
26.35 on the two test sets, respectively. As PMI
is slightly better than LM in our model, we use
PMI in experiments hereafter.
The combination of our model and LRSM is
able to further improve translation quality in terms
of BLEU. In this case, the average BLEU score
of the improved system is 26.95, with 0.4 BLEU
points higher than LRSM. When combining our
model with TSM, we obtain an average BLEU
score of 26.80, which is better than TSM by
0.45 BLEU points. The two improvements over
LRSM and TSM are also statistically significant
at p&lt;0.05. These experiment results suggest that
exploring long-distance dependencies among tar-
get translations is complementary to the previous
lexical selection methods which focus on source-
side context information.
In order to know how our approach improves
the performance of the HPB system, we compared
the best translations of the HPB system using dif-
ferent models. We find that our approach really
improves translation quality by utilizing target-
side long-distance dependencies which are, on the
contrary, ignored in previous methods.
For example, the source sentence “... if%J&apos;4J.&amp;quot;k
�� 9A +, [C] ALS 3&amp;C... *�+],To A+ �
�...” is translated as follows:
</bodyText>
<listItem confidence="0.964702538461539">
• Ref:... musharraf and some tribal leaders in
the northern region of [pakistan] last septem-
ber ... the remnant forces of the taliban ...
• Baseline:... musharraf last september and
[palestine] north of tribal leaders ... the rem-
nants of the taliban ...
• LRSM:... musharraf last september and
some tribal chiefs of the northern region of
[palestine] ... the remnants of the taliban ...
• LRSM+GM(PMI): ... last september
musharraf and some tribal chiefs of the
northern region of [pakistan] ... the remnants
of the taliban ...
</listItem>
<bodyText confidence="0.984341272727273">
Here both the baseline and LRSM fail to ob-
tain the right translation for the word “C” be-
cause “palestine” has a higher probability than
“pakistan” (0.0374 vs 0.0285). However, in our
model, the long-distance dependencies between
(“musharraf”, “pakistan”) and (“taliban”, “pak-
istan”) help the decoder correctly choose the
translation “pakistan” for “C”.
In yet another example, the source sentence “�
*V A # NO— [toiX] 4P] kirg f#L1” is
translated as follows:
</bodyText>
<listItem confidence="0.984051777777778">
• Ref: us hopes agreement on north korean nu-
clear issue be fully implemented
• Baseline: us hoped that the dprk nuclear is-
sue is the full implementation
• TSM: us hope that the full implementation of
the nuclear issue
• TSM+GM(PMI): us hope that the dprk
nuclear issue [agreement] to be fully
implemented
</listItem>
<bodyText confidence="0.9996494">
Even with TSM, the HPB system did not trans-
late “toiX” at all because translation rules “X1
  X2 implementation of X1” are used to trans-
late the source sentence by the baseline and TSM
systems respectively. However, in the combined
model TSM+GM(PMI), the differences in relat-
edness scores between (“nuclear”, “agreement”),
(“issue”, “agreement”) and (“agreement”, “im-
plemented”) encourage the enhanced system to se-
lect right translation for this word.
</bodyText>
<page confidence="0.995913">
1244
</page>
<figureCaption confidence="0.995527">
Figure 3: Experiment results on the test sets using different
reallocation weights.
</figureCaption>
<subsectionHeader confidence="0.999806">
4.3 Effect of Reallocation Weight A.
</subsectionHeader>
<bodyText confidence="0.999980210526316">
In Eq. (6), the reallocation weight A determines
which part plays a more important role in our
method. In order to investigate the effect of A
on our method, we tried different values for A:
from 0.1 to 0.5 with an increment of 0.05 each
time. The experimental setup is the same as the
previous experiments. Figure 3 shows the aver-
age BLEU scores on the two test sets. Our sys-
tem performs well when A ranges from 0.1 to 0.25.
The performance drops when A is larger than 0.25.
A small reallocation weight A reduces the impact
of initial evidences and local source-side associa-
tions in the collective lexical selection algorithm,
but increases the impact of global dependencies
of target-side translation, which are normally not
considered in previous lexical selection methods.
This performance curve on the values of A sug-
gests that target-side global dependencies are im-
portant for lexical selection.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.993101970588236">
The collective inference algorithm is partially in-
spired by Han et al. (2011) who propose a graph-
based collective entity linking (EL) method to
model global interdependences among different
EL decisions. We successfully adapt this algo-
rithm to lexical selection in SMT. Other related
work mainly includes the following two strands.
(1) Lexical selection in SMT. In order to cap-
ture source-side context for lexical selection, some
researchers propose trigger-based lexicon models
to capture long-distance dependencies (Hasan et
al., 2008; Mauser et al., 2009), and many more re-
searchers build classifiers with rich context infor-
mation to select desirable translations during de-
coding (Chan et al., 2007; Carpuat and Wu, 2007;
He et al., 2008; Liu et al., 2008). Shen et al. (2009)
introduce four new linguistic and contextual fea-
tures for HPB system. We have also witnessed in-
creasing efforts in the exploitation of document-
level context information. Xiao et al. (2011)
impose a hard constraint to guarantee the trans-
lation consistency in document-level translation.
Ture et al. (2012) soften this consistency con-
straint by integrating three counting features into
decoder. Hardmeier et al. (2012, 2013) introduce
a document-wide phrase-based decoder and inte-
grate a semantic language model that cross sen-
tence boundaries into the decoder. Based on topic
models, Xiao et al. (2012) present a topic simi-
larity model for HPB system, where each rule is
assigned with a topic distribution. Also relevant is
the work of Xiong et al. (2013), who use three
different models to capture lexical cohesion for
document-level machine translation. Compared
with the above-mentioned studies, our method fo-
cuses on the exploitation of global dependencies
among target translations, which has attracted lit-
tle attention before.
Different from exploring source-side context,
other researchers pay attention to the utilization
of target-side context information. The com-
mon practice in SMT is to use an n-gram lan-
guage model to capture local dependencies be-
tween translations (Koehn et al., 2003; Xiong et
al., 2011). Yet another approach exploring target-
side context information is proposed by Shen et al.
(2008), who use a dependency language model to
capture long-distance relations on the target side.
Moreover, Zhang et al. (2014) treat translation as
an unconstrained target sentence generation task,
using soft features to capture lexical and syntac-
tic correspondences between the source and tar-
get language. Recently, many researcher have
proposed to use deep neural networks to model
long-distance dependencies of arbitrary length for
SMT (Auli et al., 2013; Kalchbrenner and Blun-
som, 2013; Devlin et al., 2014; Hu et al., 2014;
Liu et al., 2014; Sundermeyer et al., 2014). Our
work is significantly different from these meth-
ods. We use a graph representation to capture local
and global context information, which, to the best
of our knowledge, is the first attempt to explore
graph-based representations for lexical selection.
Furthermore, our model do not resort to any syn-
tactic resources such as dependency parsers of the
target language.
(2) Random walk for SMT. Because of the
advantage of global consistency, random walk al-
</bodyText>
<page confidence="0.966853">
1245
</page>
<bodyText confidence="0.999968">
gorithm has been applied in SMT. For example,
Cui et al. (2013) develop an effective approach
to optimize phrase scoring and corpus weighting
jointly using graph-based random walk. Zhu et
al. (2013) apply a random walk method to dis-
cover implicit relations between the phrases of dif-
ferent languages. Aiming to better evaluate trans-
lation quality at the document level, Gong and Li
(2013) run PageRank algorithm to assign weights
to words in translation evaluation. Different from
these studies, the key interest of our research lies
in the lexical selection with random walk.
</bodyText>
<sectionHeader confidence="0.996046" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999918125">
This paper has presented a novel graph-based
collective lexical selection method for SMT. We
build translation graphs to capture local source-
side associations and global target-side dependen-
cies, and propose a purely collective inference al-
gorithm to jointly identify target translations of
source-side content words in translation graphs.
Our method capitalizes on capabilities of transla-
tion graphs to represent both local and global rela-
tions on the source/target side. Experiment results
demonstrate the effectiveness of our method.
In the future, we plan to further improve our
model by capturing semantic relatedness among
source words. Additionally, we also want to
jointly model different levels of context informa-
tion in a unified framework for SMT.
</bodyText>
<sectionHeader confidence="0.99758" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9753973125">
The authors were supported by National Nat-
ural Science Foundation of China (Grant Nos
61303082 and 61403269), Natural Science
Foundation of Jiangsu Province (Grant No.
BK20140355), Research Fund for the Doctoral
Program of Higher Education of China (Grant No.
20120121120046), Research fund of the State
Key Laboratory for Novel Software Technology in
Nanjing University (Grant No. KFKT2015B11),
the Special and Major Subject Project of the
Industrial Science and Technology in Fujian
Province 2013 (Grant No. 2013HZ0004-1),
and 2014 Key Project of Anhui Science and
Technology Bureau (Grant No. 1301021018).
We also thank the anonymous reviewers for their
insightful comments.
</bodyText>
<sectionHeader confidence="0.976538" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999308384615385">
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Proc.
of EMNLP 2013, pages 1044–1054.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proc. of EMNLP 2007, pages 61–72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statis-
tical machine translation. In Proc. of ACL 2007,
pages 33–40.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 201–228.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22–29.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proc. of ACL 2011, short pa-
pers, pages 176–181.
Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li, and
Ming Zhou. 2013. Bilingual data cleaning for smt
using graph-based random walk. In Proc. of ACL
2013, pages 340–345.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proc. of ACL 2014,
pages 1370–1380.
F. Gobel and A.A. Jagers. 1974. Random walks on
graphs. Stochastic Processes and Their Applica-
tions, 2(4):331–336.
Zhengxian Gong and Liangyou Li. 2013. Document-
level automatic machine translation evaluation based
on weighted lexical cohesion. In Proc. of NLPCC
2013.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: A graph-based method.
In Proc. of SIGIR 2011, pages 765–774.
Saˇsa Hasan, Juri Ganitkevitch, Hermann Ney, and
Jes´us Andr´es-Ferrer. 2008. Triplet lexicon mod-
els for statistical machine translation. In Proc. of
EMNLP 2008, pages 372–381.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proc. of COLING 2008, pages
321–328.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recurrent
neural networks. In Proc. of EACL 2014, pages 20–
29.
</reference>
<page confidence="0.790902">
1246
</page>
<reference confidence="0.999868291139241">
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proc. of EMNLP
2013, pages 1700–1709.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL-HLT 2003, pages 127–133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388–395.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proc. of EMNLP 2008, pages 89–97.
Kang Liu, Liheng Xu, and Jun Zhao. 2014. Extract-
ing opinion targets and opinionwords from online re-
views with graph co-ranking. In Proc. ofACL 2014,
pages 314–324.
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009.
Extending statistical machine translation with dis-
criminative and trigger-based lexicon models. In
Proc. of EMNLP 2009, pages 210–218.
Franz Joseph Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:19–51.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL
2003, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proc. ofACL 2002,
pages 311–318.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL 2008, pages 577–585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In Proc. of EMNLP 2009, pages
72–80.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP 2002, pages
901–904.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling
with bidirectional recurrent neural networks. In
Proc. of EMNLP 2014, pages 14–25.
Ferhan Ture, DouglasW. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proc. of NAACL-HLT 2012, pages 417–426.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Proc. of MT SUMMIT 2011,
pages 131–138.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for hi-
erarchical phrase-based translation. In Proc. of ACL
2012, pages 750–758.
Deyi Xiong and Min Zhang. 2014. A sense-based
translation model for statistical machine translation.
In Proc. of ACL 2014, pages 1459–1469.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011.
Enhancing language models in statistical machine
translation with backward n-grams and mutual infor-
mation triggers. In Proc. of ACL 2011, pages 1288–
1297.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L¨u,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proc. of IJ-
CAI 2013, pages 2183–2189.
Yue Zhang, Kai Song, Linfeng Song, Jingbo Zhu, and
Qun Liu. 2014. Syntactic smt using a discriminative
text generation model. In Proc. of EMNLP 2014,
pages 177–182.
Xiaoning Zhu, Zhongjun He, Hua Wu, Haifeng Wang,
Conghui Zhu, and Tiejun Zhao. 2013. Improving
pivot-based statistical machine translation using ran-
dom walk. In Proc. of EMNLP 2013, pages 524–
534.
</reference>
<page confidence="0.992283">
1247
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.111660">
<title confidence="0.998338">Graph-Based Collective Lexical for Statistical Machine Translation</title>
<author confidence="0.967863">Deyi Shujian Xianpei Junfeng</author>
<affiliation confidence="0.5722905">University, Xiamen, P.R. Key Laboratory for Novel Software Technology, Nanjing University, P.R.</affiliation>
<address confidence="0.3635595">University, Suzhou, P.R. of Software, Chinese Academy of Sciences, Beijing, P.R.</address>
<email confidence="0.249784">dyxiong@suda.edu.cnxianpei@nfs.iscas.ac.cn</email>
<abstract confidence="0.99925305">Lexical selection is of great importance to statistical machine translation. In this paper, we propose a graph-based framework for collective lexical selection. The is established on a captures not only local associations between source-side content words and their target translations but also targetside global dependencies in terms of relatedness among target items. We also introduce a random walk style algorithm to collectively identify translations of sourceside content words that are strongly related in translation graph. We validate the effectiveness of our lexical selection framework on Chinese-English translation. Experiment results with large-scale training data show that our approach significantly improves lexical selection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1044--1054</pages>
<contexts>
<context position="31353" citStr="Auli et al., 2013" startWordPosition="5061" endWordPosition="5064">ncies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been appl</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proc. of EMNLP 2013, pages 1044–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>61--72</pages>
<contexts>
<context position="2215" citStr="Carpuat and Wu, 2007" startWordPosition="304" endWordPosition="307">s and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed in∗Corresponding author. 1Please note that dependencies in this paper are not necessarily syntactic dependencies. creasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance depende</context>
<context position="29429" citStr="Carpuat and Wu, 2007" startWordPosition="4763" endWordPosition="4766">collective entity linking (EL) method to model global interdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries in</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proc. of EMNLP 2007, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2193" citStr="Chan et al., 2007" startWordPosition="300" endWordPosition="303">ponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed in∗Corresponding author. 1Please note that dependencies in this paper are not necessarily syntactic dependencies. creasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing</context>
<context position="29407" citStr="Chan et al., 2007" startWordPosition="4759" endWordPosition="4762">opose a graphbased collective entity linking (EL) method to model global interdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross </context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proc. of ACL 2007, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>201--228</pages>
<contexts>
<context position="5191" citStr="Chiang, 2007" startWordPosition="753" endWordPosition="754">abilities are also presented. Src: A Chinese sentence with part-of-speech tags. Tran: system output. Ref: reference translation. • Second, we introduce a collective lexical selection algorithm, which can jointly identify translations of all source-side content words in the translation graph. • Finally, we incorporate confidence scores of candidate translations in the translation graph, which are derived by the collective selection algorithm, into SMT to improve lexical selection. We validate the effectiveness of our graphbased lexical selection framework on a hierarchical phrase-based system (Chiang, 2007). Experiment results on the NIST Chinese-English test sets show that our approach significantly improves translation quality. We begin in Section 2 with the construction of translation graph for each translated sentence. Then, we propose a graph-based collective lexical selection framework for SMT in Section 3. Experiment results are reported in Section 4. We summarize and compare related work in Section 5. Finally, Section 6 presents conclusions and directions for future research. 2 Translation Graph Formally, a translation graph is a weighted graph G=(N, E). In the node set N, each node repr</context>
<context position="22548" citStr="Chiang, 2007" startWordPosition="3637" endWordPosition="3639">training data, we set window size dt=20, and used the SRILM toolkit with Witten-Bell smoothing and PMI to calculate relatedness strengths for target-side translations. To avoid data sparseness, we build the graph using the surface forms of words while calculating the word relatedness at the lemma level. To achieve this, we converted each word into its corresponding lemma with the exception of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold c was set as 10−10, and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BL</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, pages 201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="7045" citStr="Church and Hanks, 1990" startWordPosition="1054" endWordPosition="1057">tence, the most ideal translation graph is a graph that includes all source words and their candidate translations. However, this ideal graph has two problems: intensive computation for graph inference and difficulty in modeling dependencies between function and content words. In order to get around these two issues, we only consider lexical selection for source content words2. We first identify source-side content word pairs using statistical metrics, and then keep word pairs with a high relatedness strength in the translation graph. To be specific, we use pointwise mutual information (PMI) (Church and Hanks, 1990) and co-occurrence frequency to measure the relatedness strength of two source-side words s and s&apos; within a window ds. Content word pairs will be kept when their co-occurrence frequencies are more than ccf times in our training corpus and PMI values are larger than cpmi. In this process, we remove noisy word pairs using the following heuristic rules: (1) As an adjective only has relations with its head nouns or dependent adverbs, we remove all word pairs where an adjective is paired with words other than its head nouns or dependent adverbs; (2) We apply a similar constraint to adverbs too, sin</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>176--181</pages>
<contexts>
<context position="23054" citStr="Clark et al., 2011" startWordPosition="3718" endWordPosition="3721"> set as 10−10, and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermor</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. of ACL 2011, short papers, pages 176–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Dongdong Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>Bilingual data cleaning for smt using graph-based random walk.</title>
<date>2013</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>340--345</pages>
<contexts>
<context position="31995" citStr="Cui et al. (2013)" startWordPosition="5168" endWordPosition="5171">m, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phrase scoring and corpus weighting jointly using graph-based random walk. Zhu et al. (2013) apply a random walk method to discover implicit relations between the phrases of different languages. Aiming to better evaluate translation quality at the document level, Gong and Li (2013) run PageRank algorithm to assign weights to words in translation evaluation. Different from these studies, the key interest of our research lies in the lexical selection with random walk. 6 Conclusion and Future Work This paper has presented a novel graph-based collective l</context>
</contexts>
<marker>Cui, Zhang, Liu, Li, Zhou, 2013</marker>
<rawString>Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li, and Ming Zhou. 2013. Bilingual data cleaning for smt using graph-based random walk. In Proc. of ACL 2013, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>1370--1380</pages>
<contexts>
<context position="31406" citStr="Devlin et al., 2014" startWordPosition="5070" endWordPosition="5073">ng et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proc. of ACL 2014, pages 1370–1380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gobel</author>
<author>A A Jagers</author>
</authors>
<title>Random walks on graphs.</title>
<date>1974</date>
<booktitle>Stochastic Processes and Their Applications,</booktitle>
<pages>2--4</pages>
<contexts>
<context position="14113" citStr="Gobel and Jagers, 1974" startWordPosition="2191" endWordPosition="2194">ences that “hold” and “position” are the correct translations of “ch1yˇou” and “l`ıchˇang”, respectively. On the other hand, the candidate translation “problem” is less related to “hold” and “position”, which may suggest that it is not likely to be the correct translation of “w`ent1”, even if it has a strong source-target association relation with “w`ent´ı”. However, in the translation graph, the correct target translation of a source word depends on correct translations of other source words in the graph, and vice versa. So how do we find these correct translations? We propose a Random Walk (Gobel and Jagers, 1974) style algorithm to solve this problem, aiming to use both local source-target associations and global target-target relatedness simultaneously during translation. In our algorithm, we assign each node an evidence score in the translation graph, which indicates either the importance of a source word (for a source word node) or the confidence of a target translation being a correct translation (for a target word node). Specifically, we perform collective inference on the translation graph as follows: • First, we set initial evidence scores for nodes in the translation graph. • Second, evidence </context>
</contexts>
<marker>Gobel, Jagers, 1974</marker>
<rawString>F. Gobel and A.A. Jagers. 1974. Random walks on graphs. Stochastic Processes and Their Applications, 2(4):331–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Liangyou Li</author>
</authors>
<title>Documentlevel automatic machine translation evaluation based on weighted lexical cohesion.</title>
<date>2013</date>
<booktitle>In Proc. of NLPCC</booktitle>
<contexts>
<context position="32320" citStr="Gong and Li (2013)" startWordPosition="5220" endWordPosition="5223">for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phrase scoring and corpus weighting jointly using graph-based random walk. Zhu et al. (2013) apply a random walk method to discover implicit relations between the phrases of different languages. Aiming to better evaluate translation quality at the document level, Gong and Li (2013) run PageRank algorithm to assign weights to words in translation evaluation. Different from these studies, the key interest of our research lies in the lexical selection with random walk. 6 Conclusion and Future Work This paper has presented a novel graph-based collective lexical selection method for SMT. We build translation graphs to capture local sourceside associations and global target-side dependencies, and propose a purely collective inference algorithm to jointly identify target translations of source-side content words in translation graphs. Our method capitalizes on capabilities of </context>
</contexts>
<marker>Gong, Li, 2013</marker>
<rawString>Zhengxian Gong and Liangyou Li. 2013. Documentlevel automatic machine translation evaluation based on weighted lexical cohesion. In Proc. of NLPCC 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
<author>Jun Zhao</author>
</authors>
<title>Collective entity linking in web text: A graph-based method.</title>
<date>2011</date>
<booktitle>In Proc. of SIGIR</booktitle>
<pages>765--774</pages>
<marker>Han, Le Sun, Zhao, 2011</marker>
<rawString>Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective entity linking in web text: A graph-based method. In Proc. of SIGIR 2011, pages 765–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Hasan</author>
<author>Juri Ganitkevitch</author>
<author>Hermann Ney</author>
<author>Jes´us Andr´es-Ferrer</author>
</authors>
<title>Triplet lexicon models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>372--381</pages>
<marker>Hasan, Ganitkevitch, Ney, Andr´es-Ferrer, 2008</marker>
<rawString>Saˇsa Hasan, Juri Ganitkevitch, Hermann Ney, and Jes´us Andr´es-Ferrer. 2008. Triplet lexicon models for statistical machine translation. In Proc. of EMNLP 2008, pages 372–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>321--328</pages>
<contexts>
<context position="2273" citStr="He et al., 2008" startWordPosition="316" endWordPosition="319">ected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed in∗Corresponding author. 1Please note that dependencies in this paper are not necessarily syntactic dependencies. creasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the tran</context>
<context position="23409" citStr="He et al., 2008" startWordPosition="3776" endWordPosition="3779">on quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermore, we combined our model with the two models to see if we could obtain further improvements. For this, we integrated the new lexi5We used 30 topics following (Xiao et al., 2012). 1243 System MT06 MT08 Avg Baseline 30.25 21.25 25.75 LRSM 31.12 21.98 26.55 TSM 30.79 21.90 26.35 GM(LM) 30.64 21.78 26.21 GM(PMI) 31.02 21.77 26.40 LRSM +GM(PMI) 31.66 22.23 2</context>
<context position="29446" citStr="He et al., 2008" startWordPosition="4767" endWordPosition="4770">ing (EL) method to model global interdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. B</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In Proc. of COLING 2008, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuening Hu</author>
<author>Michael Auli</author>
<author>Qin Gao</author>
<author>Jianfeng Gao</author>
</authors>
<title>Minimum translation modeling with recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EACL 2014,</booktitle>
<pages>20--1246</pages>
<contexts>
<context position="31423" citStr="Hu et al., 2014" startWordPosition="5074" endWordPosition="5077"> another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approa</context>
</contexts>
<marker>Hu, Auli, Gao, Gao, 2014</marker>
<rawString>Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao. 2014. Minimum translation modeling with recurrent neural networks. In Proc. of EACL 2014, pages 20– 1246</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="31385" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="5065" endWordPosition="5069">lations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et </context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proc. of EMNLP 2013, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL-HLT</booktitle>
<pages>127--133</pages>
<contexts>
<context position="2682" citStr="Koehn et al., 2003" startWordPosition="384" endWordPosition="387">tion. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch/yˇou” and “lichˇang” are far from each other, our baseline can only correctly translate “lichˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch/yˇou”(</context>
<context position="30782" citStr="Koehn et al., 2003" startWordPosition="4974" endWordPosition="4977">with a topic distribution. Also relevant is the work of Xiong et al. (2013), who use three different models to capture lexical cohesion for document-level machine translation. Compared with the above-mentioned studies, our method focuses on the exploitation of global dependencies among target translations, which has attracted little attention before. Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The common practice in SMT is to use an n-gram language model to capture local dependencies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of NAACL-HLT 2003, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="23117" citStr="Koehn, 2004" startWordPosition="3729" endWordPosition="3730">emented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermore, we combined our model with the two models to see if we could</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qun Liu</author>
<author>Zhongjun He</author>
<author>Yang Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based rule selection model for syntax-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>89--97</pages>
<contexts>
<context position="29465" citStr="Liu et al., 2008" startWordPosition="4771" endWordPosition="4774">o model global interdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic model</context>
</contexts>
<marker>Liu, He, Liu, Lin, 2008</marker>
<rawString>Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 2008. Maximum entropy based rule selection model for syntax-based statistical machine translation. In Proc. of EMNLP 2008, pages 89–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kang Liu</author>
<author>Liheng Xu</author>
<author>Jun Zhao</author>
</authors>
<title>Extracting opinion targets and opinionwords from online reviews with graph co-ranking.</title>
<date>2014</date>
<booktitle>In Proc. ofACL 2014,</booktitle>
<pages>314--324</pages>
<contexts>
<context position="12612" citStr="Liu et al., 2014" startWordPosition="1949" endWordPosition="1952">S(t, t&apos;&apos;) where N(˜t) denotes the set of candidate translations that link to ˜t, and RS(˜t, ˜t&apos;) measures the strength of relatedness between t˜ and ˜t&apos; which is calculated as the average word-level relatedness over all content words in these two translations t˜ and ˜t&apos;. As for the word-level relatedness RS(t, t&apos;) for a content word pair (t, t&apos;), we estimate it with the following two approaches over collected cooccurring word pairs within a window of size dt: (1) RS(t, t&apos;) is computed as a bigram conditional probability plm(t&apos;|t) via the language model; (2) Following (Xiong et al., 2011) and (Liu et al., 2014), we employ PMI to define RS(t, t&apos;) as ln p(t,t&apos;) p(t)p(t&apos;). 3 Collective Lexical Selection Algorithm Based on the translation graph, we propose a collective lexical selection algorithm to jointly identify translations of all source words in the graph. 3.1 Problem Statement and Solution Method As stated previously, the translation of a sourceside content word s should be: 1) associated with s; 2) related to the translations of other source-side content words. Thus, in the translation graph, the translation of s should be a target-side node which has: 1) an association edge with the node of s; </context>
<context position="31441" citStr="Liu et al., 2014" startWordPosition="5078" endWordPosition="5081"> exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phr</context>
</contexts>
<marker>Liu, Xu, Zhao, 2014</marker>
<rawString>Kang Liu, Liheng Xu, and Jun Zhao. 2014. Extracting opinion targets and opinionwords from online reviews with graph co-ranking. In Proc. ofACL 2014, pages 314–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending statistical machine translation with discriminative and trigger-based lexicon models.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>210--218</pages>
<contexts>
<context position="2256" citStr="Mauser et al., 2009" startWordPosition="312" endWordPosition="315">ndencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed in∗Corresponding author. 1Please note that dependencies in this paper are not necessarily syntactic dependencies. creasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figu</context>
<context position="29264" citStr="Mauser et al., 2009" startWordPosition="4736" endWordPosition="4739">endencies are important for lexical selection. 5 Related Work The collective inference algorithm is partially inspired by Han et al. (2011) who propose a graphbased collective entity linking (EL) method to model global interdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Proc. of EMNLP 2009, pages 210–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="20860" citStr="Och and Ney, 2003" startWordPosition="3373" endWordPosition="3376">rmalized evidence score of “ch´ıyˇou” translated into “hold” is calculated as 0.1218/(0.1218 + 0.0244 + 0.0604) ≈ 0.5895. In this way, for each bilingual rule with word alignments, we will obtain a new lexical weight which can be used together with the original translation probabilities and lexical weight to improve lexical selection in SMT. 4 Experiments 4.1 Setup Our bilingual training corpus is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of 2005 (MT05) as the development set, and the sets of MT06/MT08 as test sets. We used SRILM Toolkit (Stolcke, 2002) to train one 5-gram language model on the Xinhua portion of Gigaword corpus. To construct translation graphs, we first used the ZPar toolkit3 and the Stanford toolkit4 to preprocess (word segmentation, PoS tagging and so on) Chinese and English sentences, respectively. We used the Chinese part of our bilingual corpus and an additional Chinese LDC Xinhua news corpus (10.2M sentences with 279.9M words) as tra</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="22937" citStr="Och, 2003" startWordPosition="3698" endWordPosition="3699"> of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold c was set as 10−10, and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="22873" citStr="Papineni et al., 2002" startWordPosition="3685" endWordPosition="3688">this, we converted each word into its corresponding lemma with the exception of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold c was set as 10−10, and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proc. ofACL 2002, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>577--585</pages>
<contexts>
<context position="2701" citStr="Shen et al., 2008" startWordPosition="388" endWordPosition="391">source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch/yˇou” and “lichˇang” are far from each other, our baseline can only correctly translate “lichˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch/yˇou”(hold) and “lichˇang</context>
<context position="30900" citStr="Shen et al. (2008)" startWordPosition="4993" endWordPosition="4996"> lexical cohesion for document-level machine translation. Compared with the above-mentioned studies, our method focuses on the exploitation of global dependencies among target translations, which has attracted little attention before. Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The common practice in SMT is to use an n-gram language model to capture local dependencies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly diff</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proc. of ACL 2008, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Ralph Weischedel</author>
</authors>
<title>Effective use of linguistic and contextual information for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>72--80</pages>
<contexts>
<context position="2293" citStr="Shen et al., 2009" startWordPosition="320" endWordPosition="323">slations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed in∗Corresponding author. 1Please note that dependencies in this paper are not necessarily syntactic dependencies. creasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemo</context>
<context position="29485" citStr="Shen et al. (2009)" startWordPosition="4775" endWordPosition="4778">rdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic models, Xiao et al. (2012</context>
</contexts>
<marker>Shen, Xu, Zhang, Matsoukas, Weischedel, 2009</marker>
<rawString>Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic and contextual information for statistical machine translation. In Proc. of EMNLP 2009, pages 72–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP</booktitle>
<pages>901--904</pages>
<contexts>
<context position="21049" citStr="Stolcke, 2002" startWordPosition="3407" endWordPosition="3408">in a new lexical weight which can be used together with the original translation probabilities and lexical weight to improve lexical selection in SMT. 4 Experiments 4.1 Setup Our bilingual training corpus is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of 2005 (MT05) as the development set, and the sets of MT06/MT08 as test sets. We used SRILM Toolkit (Stolcke, 2002) to train one 5-gram language model on the Xinhua portion of Gigaword corpus. To construct translation graphs, we first used the ZPar toolkit3 and the Stanford toolkit4 to preprocess (word segmentation, PoS tagging and so on) Chinese and English sentences, respectively. We used the Chinese part of our bilingual corpus and an additional Chinese LDC Xinhua news corpus (10.2M sentences with 279.9M words) as training data to collect Chinese word pairs. We set window size ds=15, thresholds cpni=0, ccf=5 to identify Chinese related word pairs in the NIST translated sentences. Averagely, these three </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proc. of ICSLP 2002, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation modeling with bidirectional recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>14--25</pages>
<contexts>
<context position="31468" citStr="Sundermeyer et al., 2014" startWordPosition="5082" endWordPosition="5085">ide context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phrase scoring and corpus weig</context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation modeling with bidirectional recurrent neural networks. In Proc. of EMNLP 2014, pages 14–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Encouraging consistent translation choices.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL-HLT 2012,</booktitle>
<pages>417--426</pages>
<marker>Oard, Resnik, 2012</marker>
<rawString>Ferhan Ture, DouglasW. Oard, and Philip Resnik. 2012. Encouraging consistent translation choices. In Proc. of NAACL-HLT 2012, pages 417–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Shujie Yao</author>
<author>Hao Zhang</author>
</authors>
<title>Document-level consistency verification in machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of MT SUMMIT</booktitle>
<pages>131--138</pages>
<contexts>
<context position="2357" citStr="Xiao et al., 2011" startWordPosition="330" endWordPosition="333"> and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed in∗Corresponding author. 1Please note that dependencies in this paper are not necessarily syntactic dependencies. creasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch/yˇou” and “lichˇang” are far from each o</context>
<context position="29674" citStr="Xiao et al. (2011)" startWordPosition="4805" endWordPosition="4808">ion in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic models, Xiao et al. (2012) present a topic similarity model for HPB system, where each rule is assigned with a topic distribution. Also relevant is the work of Xiong et al. (2013), who use three different models to</context>
</contexts>
<marker>Xiao, Zhu, Yao, Zhang, 2011</marker>
<rawString>Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 2011. Document-level consistency verification in machine translation. In Proc. of MT SUMMIT 2011, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>A topic similarity model for hierarchical phrase-based translation.</title>
<date>2012</date>
<booktitle>In Proc. of ACL 2012,</booktitle>
<pages>750--758</pages>
<contexts>
<context position="2395" citStr="Xiao et al., 2012" startWordPosition="338" endWordPosition="341">s can only capture local associations and dependencies, we have witnessed in∗Corresponding author. 1Please note that dependencies in this paper are not necessarily syntactic dependencies. creasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch/yˇou” and “lichˇang” are far from each other, our baseline can only correctly </context>
<context position="23537" citStr="Xiao et al., 2012" startWordPosition="3797" endWordPosition="3800">f MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermore, we combined our model with the two models to see if we could obtain further improvements. For this, we integrated the new lexi5We used 30 topics following (Xiao et al., 2012). 1243 System MT06 MT08 Avg Baseline 30.25 21.25 25.75 LRSM 31.12 21.98 26.55 TSM 30.79 21.90 26.35 GM(LM) 30.64 21.78 26.21 GM(PMI) 31.02 21.77 26.40 LRSM +GM(PMI) 31.66 22.23 26.95 TSM +GM(PMI) 31.34 22.26 26.80 Table 2: Experiment results on the test sets with A=0.15. Avg = average BLEU scores, GM(LM) </context>
<context position="30086" citStr="Xiao et al. (2012)" startWordPosition="4867" endWordPosition="4870">hen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic models, Xiao et al. (2012) present a topic similarity model for HPB system, where each rule is assigned with a topic distribution. Also relevant is the work of Xiong et al. (2013), who use three different models to capture lexical cohesion for document-level machine translation. Compared with the above-mentioned studies, our method focuses on the exploitation of global dependencies among target translations, which has attracted little attention before. Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The common practice in SMT is to use</context>
</contexts>
<marker>Xiao, Xiong, Zhang, Liu, Lin, 2012</marker>
<rawString>Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and Shouxun Lin. 2012. A topic similarity model for hierarchical phrase-based translation. In Proc. of ACL 2012, pages 750–758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
</authors>
<title>A sense-based translation model for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>1459--1469</pages>
<marker>Xiong, Zhang, 2014</marker>
<rawString>Deyi Xiong and Min Zhang. 2014. A sense-based translation model for statistical machine translation. In Proc. of ACL 2014, pages 1459–1469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Enhancing language models in statistical machine translation with backward n-grams and mutual information triggers.</title>
<date>2011</date>
<booktitle>In Proc. of ACL 2011,</booktitle>
<pages>1288--1297</pages>
<contexts>
<context position="2722" citStr="Xiong et al., 2011" startWordPosition="392" endWordPosition="395">tions mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch/yˇou” and “lichˇang” are far from each other, our baseline can only correctly translate “lichˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch/yˇou”(hold) and “lichˇang”(stance), these tran</context>
<context position="12589" citStr="Xiong et al., 2011" startWordPosition="1944" endWordPosition="1947">˜t → t)= (3) Ei &amp;quot;EN(˜t) RS(t, t&apos;&apos;) where N(˜t) denotes the set of candidate translations that link to ˜t, and RS(˜t, ˜t&apos;) measures the strength of relatedness between t˜ and ˜t&apos; which is calculated as the average word-level relatedness over all content words in these two translations t˜ and ˜t&apos;. As for the word-level relatedness RS(t, t&apos;) for a content word pair (t, t&apos;), we estimate it with the following two approaches over collected cooccurring word pairs within a window of size dt: (1) RS(t, t&apos;) is computed as a bigram conditional probability plm(t&apos;|t) via the language model; (2) Following (Xiong et al., 2011) and (Liu et al., 2014), we employ PMI to define RS(t, t&apos;) as ln p(t,t&apos;) p(t)p(t&apos;). 3 Collective Lexical Selection Algorithm Based on the translation graph, we propose a collective lexical selection algorithm to jointly identify translations of all source words in the graph. 3.1 Problem Statement and Solution Method As stated previously, the translation of a sourceside content word s should be: 1) associated with s; 2) related to the translations of other source-side content words. Thus, in the translation graph, the translation of s should be a target-side node which has: 1) an association ed</context>
<context position="30803" citStr="Xiong et al., 2011" startWordPosition="4978" endWordPosition="4981">ution. Also relevant is the work of Xiong et al. (2013), who use three different models to capture lexical cohesion for document-level machine translation. Compared with the above-mentioned studies, our method focuses on the exploitation of global dependencies among target translations, which has attracted little attention before. Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The common practice in SMT is to use an n-gram language model to capture local dependencies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2</context>
</contexts>
<marker>Xiong, Zhang, Li, 2011</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. 2011. Enhancing language models in statistical machine translation with backward n-grams and mutual information triggers. In Proc. of ACL 2011, pages 1288– 1297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Guosheng Ben</author>
<author>Min Zhang</author>
<author>Yajuan L¨u</author>
<author>Qun Liu</author>
</authors>
<title>Modeling lexical cohesion for document-level machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of IJCAI</booktitle>
<pages>2183--2189</pages>
<marker>Xiong, Ben, Zhang, L¨u, Liu, 2013</marker>
<rawString>Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L¨u, and Qun Liu. 2013. Modeling lexical cohesion for document-level machine translation. In Proc. of IJCAI 2013, pages 2183–2189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Kai Song</author>
<author>Linfeng Song</author>
<author>Jingbo Zhu</author>
<author>Qun Liu</author>
</authors>
<title>Syntactic smt using a discriminative text generation model.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>177--182</pages>
<contexts>
<context position="31022" citStr="Zhang et al. (2014)" startWordPosition="5012" endWordPosition="5015">n the exploitation of global dependencies among target translations, which has attracted little attention before. Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The common practice in SMT is to use an n-gram language model to capture local dependencies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the bes</context>
</contexts>
<marker>Zhang, Song, Song, Zhu, Liu, 2014</marker>
<rawString>Yue Zhang, Kai Song, Linfeng Song, Jingbo Zhu, and Qun Liu. 2014. Syntactic smt using a discriminative text generation model. In Proc. of EMNLP 2014, pages 177–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoning Zhu</author>
<author>Zhongjun He</author>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Conghui Zhu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Improving pivot-based statistical machine translation using random walk.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>524</pages>
<contexts>
<context position="32130" citStr="Zhu et al. (2013)" startWordPosition="5188" endWordPosition="5191">ese methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phrase scoring and corpus weighting jointly using graph-based random walk. Zhu et al. (2013) apply a random walk method to discover implicit relations between the phrases of different languages. Aiming to better evaluate translation quality at the document level, Gong and Li (2013) run PageRank algorithm to assign weights to words in translation evaluation. Different from these studies, the key interest of our research lies in the lexical selection with random walk. 6 Conclusion and Future Work This paper has presented a novel graph-based collective lexical selection method for SMT. We build translation graphs to capture local sourceside associations and global target-side dependenci</context>
</contexts>
<marker>Zhu, He, Wu, Wang, Zhu, Zhao, 2013</marker>
<rawString>Xiaoning Zhu, Zhongjun He, Hua Wu, Haifeng Wang, Conghui Zhu, and Tiejun Zhao. 2013. Improving pivot-based statistical machine translation using random walk. In Proc. of EMNLP 2013, pages 524–</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
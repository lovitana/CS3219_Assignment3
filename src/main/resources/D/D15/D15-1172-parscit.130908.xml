<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.9714515">
Do You See What I Mean?
Visual Resolution of Linguistic Ambiguities
</title>
<author confidence="0.714911">
Yevgeni Berzak Andrei Barbu Daniel Harari
</author>
<affiliation confidence="0.294989">
CSAIL MIT CSAIL MIT CSAIL MIT
</affiliation>
<email confidence="0.988793">
berzak@mit.edu andrei@0xab.com hararid@mit.edu
</email>
<author confidence="0.980679">
Boris Katz Shimon Ullman
</author>
<affiliation confidence="0.828467">
CSAIL MIT Weizmann Institute of Science
</affiliation>
<email confidence="0.998718">
boris@mit.edu shimon.ullman@weizmann.ac.il
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843136363637">
Understanding language goes hand in
hand with the ability to integrate complex
contextual information obtained via per-
ception. In this work, we present a novel
task for grounded language understanding:
disambiguating a sentence given a visual
scene which depicts one of the possible
interpretations of that sentence. To this
end, we introduce a new multimodal cor-
pus containing ambiguous sentences, rep-
resenting a wide range of syntactic, se-
mantic and discourse ambiguities, coupled
with videos that visualize the different in-
terpretations for each sentence. We ad-
dress this task by extending a vision model
which determines if a sentence is depicted
by a video. We demonstrate how such a
model can be adjusted to recognize dif-
ferent interpretations of the same under-
lying sentence, allowing to disambiguate
sentences in a unified fashion across the
different ambiguity types.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986931818182">
Ambiguity is one of the defining characteristics
of human languages, and language understand-
ing crucially relies on the ability to obtain un-
ambiguous representations of linguistic content.
While some ambiguities can be resolved using
intra-linguistic contextual cues, the disambigua-
tion of many linguistic constructions requires in-
tegration of world knowledge and perceptual in-
formation obtained from other modalities.
In this work, we focus on the problem of
grounding language in the visual modality, and in-
troduce a novel task for language understanding
which requires resolving linguistic ambiguities by
utilizing the visual context in which the linguistic
content is expressed. This type of inference is fre-
quently called for in human communication that
occurs in a visual environment, and is crucial for
language acquisition, when much of the linguis-
tic content refers to the visual surroundings of the
child (Snow, 1972).
Our task is also fundamental to the problem of
grounding vision in language, by focusing on phe-
nomena of linguistic ambiguity, which are preva-
lent in language, but typically overlooked when
using language as a medium for expressing un-
derstanding of visual content. Due to such ambi-
guities, a superficially appropriate description of
a visual scene may in fact not be sufficient for
demonstrating a correct understanding of the rel-
evant visual content. Our task addresses this issue
by introducing a deep validation protocol for vi-
sual understanding, requiring not only providing
a surface description of a visual activity but also
demonstrating structural understanding at the lev-
els of syntax, semantics and discourse.
To enable the systematic study of visually
grounded processing of ambiguous language, we
create a new corpus, LAVA (Language and Vision
Ambiguities). This corpus contains sentences with
linguistic ambiguities that can only be resolved us-
ing external information. The sentences are paired
with short videos that visualize different interpre-
tations of each sentence. Our sentences encom-
pass a wide range of syntactic, semantic and dis-
</bodyText>
<page confidence="0.947263">
1477
</page>
<note confidence="0.984857">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1477–1487,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999905727272727">
course ambiguities, including ambiguous preposi-
tional and verb phrase attachments, conjunctions,
logical forms, anaphora and ellipsis. Overall, the
corpus contains 237 sentences, with 2 to 3 inter-
pretations per sentence, and an average of 3.37
videos that depict visual variations of each sen-
tence interpretation, corresponding to a total of
1679 videos.
Using this corpus, we address the problem of
selecting the interpretation of an ambiguous sen-
tence that matches the content of a given video.
Our approach for tackling this task extends the
sentence tracker introduced in (Siddharth et al.,
2014). The sentence tracker produces a score
which determines if a sentence is depicted by a
video. This earlier work had no concept of ambi-
guities; it assumed that every sentence had a sin-
gle interpretation. We extend this approach to rep-
resent multiple interpretations of a sentence, en-
abling us to pick the interpretation that is most
compatible with the video.
To summarize, the contributions of this paper
are threefold. First, we introduce a new task for vi-
sually grounded language understanding, in which
an ambiguous sentence has to be disambiguated
using a visual depiction of the sentence’s con-
tent. Second, we release a multimodal corpus
of sentences coupled with videos which covers a
wide range of linguistic ambiguities, and enables
a systematic study of linguistic ambiguities in vi-
sual contexts. Finally, we present a computational
model which disambiguates the sentences in our
corpus with an accuracy of 75.36%.
</bodyText>
<sectionHeader confidence="0.999744" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99989656">
Previous language and vision studies focused on
the development of multimodal word and sentence
representations (Bruni et al., 2012; Socher et al.,
2013; Silberer and Lapata, 2014; Gong et al.,
2014; Lazaridou et al., 2015), as well as methods
for describing images and videos in natural lan-
guage (Farhadi et al., 2010; Kulkarni et al., 2011;
Mitchell et al., 2012; Socher et al., 2014; Thoma-
son et al., 2014; Karpathy and Fei-Fei, 2014; Sid-
dharth et al., 2014; Venugopalan et al., 2015;
Vinyals et al., 2015). While these studies handle
important challenges in multimodal processing of
language and vision, they do not provide explicit
modeling of linguistic ambiguities.
Previous work relating ambiguity in language to
the visual modality addressed the problem of word
sense disambiguation (Barnard et al., 2003). How-
ever, this work is limited to context independent
interpretation of individual words, and does not
consider structure-related ambiguities. Discourse
ambiguities were previously studied in work on
multimodal coreference resolution (Ramanathan
et al., 2014; Kong et al., 2014). Our work ex-
pands this line of research, and addresses further
discourse ambiguities in the interpretation of el-
lipsis. More importantly, to the best of our knowl-
edge our study is the first to present a systematic
treatment of syntactic and semantic sentence level
ambiguities in the context of language and vision.
The interactions between linguistic and visual
information in human sentence processing have
been extensively studied in psycholinguistics and
cognitive psychology (Tanenhaus et al., 1995). A
considerable fraction of this work focused on the
processing of ambiguous language (Spivey et al.,
2002; Coco and Keller, 2015), providing evidence
for the importance of visual information for lin-
guistic ambiguity resolution by humans. Such in-
formation is also vital during language acquisition,
when much of the linguistic content perceived by
the child refers to their immediate visual environ-
ment (Snow, 1972). Over time, children develop
mechanisms for grounded disambiguation of lan-
guage, manifested among others by the usage of
iconic gestures when communicating ambiguous
linguistic content (Kidd and Holler, 2009). Our
study leverages such insights to develop a com-
plementary framework that enables addressing the
challenge of visually grounded disambiguation of
language in the realm of artificial intelligence.
</bodyText>
<sectionHeader confidence="0.99662" genericHeader="method">
3 Task
</sectionHeader>
<bodyText confidence="0.998633333333333">
In this work we provide a concrete framework
for the study of language understanding with vi-
sual context by introducing the task of grounded
language disambiguation. This task requires to
choose the correct linguistic representation of a
sentence given a visual context depicted in a video.
Specifically, provided with a sentence, n candidate
interpretations of that sentence and a video that
depicts the content of the sentence, one needs to
choose the interpretation that corresponds to the
content of the video.
To illustrate this task, consider the example in
figure 1, where we are given the sentence “Sam
approached the chair with a bag” along with two
different linguistic interpretations. In the first in-
</bodyText>
<page confidence="0.98455">
1478
</page>
<figure confidence="0.999960369047619">
(a) First interpretation (b) Second interpretation (a) First interpretation (b) Second interpretation
S
VP
VP
NP
DT
IN
with
NN
chair
a
DT
the
V
VBD
approached
NP
NN
bag
S
NP
NNP
Sam
VP
DT
the
NN
chair
NP
PP
IN
with
a
V
VBD
approached
NP
NP
DT
NN
bag
S
NP
DT
the
NP
CC
and
NN
bag
JJ NN
green chair
NP
NNP
Bill
VBD
held
VP
NP
NP
NNP
Sam
PP
S
NP
NNP
Bill
VBD
held
VP
DT
the
NP
JJ
green
NP
NN
chair
NP
CC
and
NN
bag
(c) Visual context
</figure>
<figureCaption confidence="0.999451">
Figure 1: An example of the visually grounded
</figureCaption>
<bodyText confidence="0.970956142857143">
language disambiguation task. Given the sentence
“Sam approached the chair with a bag”, two poten-
tial parses, (a) and (b), correspond to two different
semantic interpretations. In the first interpretation
Sam has the bag, while in the second reading the
bag is on the chair. The task is to select the correct
interpretation given the visual context (c).
</bodyText>
<listItem confidence="0.501829">
(c) First visual context (d) Second visual context
</listItem>
<figureCaption confidence="0.899548">
Figure 2: Linguistic and visual interpretations of
</figureCaption>
<bodyText confidence="0.861888545454546">
the sentence “Bill held the green chair and bag”. In
the first interpretation (a,c) both the chair and bag
are green, while in the second interpretation (b,d)
only the chair is green and the bag has a different
color.
terpretation, which corresponds to parse 1(a), Sam
has the bag. In the second interpretation associ-
ated with parse 1(b), the bag is on the chair rather
than with Sam. Given the visual context from fig-
ure 1(c), the task is to choose which interpretation
is most appropriate for the sentence.
</bodyText>
<sectionHeader confidence="0.976558" genericHeader="method">
4 Approach Overview
</sectionHeader>
<bodyText confidence="0.99984788">
To address the grounded language disambiguation
task, we use a compositional approach for deter-
mining if a specific interpretation of a sentence is
depicted by a video. In this framework, described
in detail in section 6, a sentence and an accom-
panying interpretation encoded in first order logic,
give rise to a grounded model that matches a video
against the provided sentence interpretation.
The model is comprised of Hidden Markov
Models (HMMs) which encode the semantics of
words, and trackers which locate objects in video
frames. To represent an interpretation of a sen-
tence, word models are combined with trackers
through a cross-product which respects the seman-
tic representation of the sentence to create a single
model which recognizes that interpretation.
Given a sentence, we construct an HMM based
representation for each interpretation of that sen-
tence. We then detect candidate locations for ob-
jects in every frame of the video. Together the re-
forestation for the sentence and the candidate ob-
ject locations are combined to form a model which
can determine if a given interpretation is depicted
by the video. We test each interpretation and re-
port the interpretation with highest likelihood.
</bodyText>
<sectionHeader confidence="0.991845" genericHeader="method">
5 Corpus
</sectionHeader>
<bodyText confidence="0.999979952380952">
To enable a systematic study of linguistic ambi-
guities that are grounded in vision, we compiled
a corpus with ambiguous sentences describing vi-
sual actions. The sentences are formulated such
that the correct linguistic interpretation of each
sentence can only be determined using external,
non-linguistic, information about the depicted ac-
tivity. For example, in the sentence “Bill held
the green chair and bag”, the correct scope of
“green” can only be determined by integrating ad-
ditional information about the color of the bag.
This information is provided in the accompany-
ing videos, which visualize the possible interpreta-
tions of each sentence. Figure 2 presents the syn-
tactic parses for this example along with frames
from the respective videos. Although our videos
contain visual uncertainty, they are not ambiguous
with respect to the linguistic interpretation they are
presenting, and hence a video always corresponds
to a single candidate representation of a sentence.
The corpus covers a wide range of well
</bodyText>
<page confidence="0.977076">
1479
</page>
<bodyText confidence="0.99960975">
known syntactic, semantic and discourse ambigu-
ity classes. While the ambiguities are associated
with various types, different sentence interpreta-
tions always represent distinct sentence meanings,
and are hence encoded semantically using first or-
der logic. For syntactic and discourse ambiguities
we also provide an additional, ambiguity type spe-
cific encoding as described below.
</bodyText>
<listItem confidence="0.997095">
• Syntax Syntactic ambiguities include Prepo-
sitional Phrase (PP) attachments, Verb Phrase
(VP) attachments, and ambiguities in the in-
terpretation of conjunctions. In addition to
logical forms, sentences with syntactic am-
biguities are also accompanied with Context
Free Grammar (CFG) parses of the candidate
interpretations, generated from a determinis-
tic CFG parser.
• Semantics The corpus addresses several
classes of semantic quantification ambigui-
ties, in which a syntactically unambiguous
sentence may correspond to different logical
forms. For each such sentence we provide the
respective logical forms.
• Discourse The corpus contains two types
of discourse ambiguities, Pronoun Anaphora
</listItem>
<bodyText confidence="0.955615739130435">
and Ellipsis, offering examples comprising
two sentences. In anaphora ambiguity cases,
an ambiguous pronoun in the second sen-
tence is given its candidate antecedents in the
first sentence, as well as a corresponding log-
ical form for the meaning of the second sen-
tence. In ellipsis cases, a part of the second
sentence, which can constitute either the sub-
ject and the verb, or the verb and the object,
is omitted. We provide both interpretations
of the omission in the form of a single unam-
biguous sentence, and its logical form, which
combines the meanings of the first and the
second sentences.
Table 2 lists examples of the different ambiguity
classes, along with the candidate interpretations of
each example.
The corpus is generated using Part of Speech
(POS) tag sequence templates. For each template,
the POS tags are replaced with lexical items from
the corpus lexicon, described in table 3, using all
the visually applicable assignments. This gener-
ation process yields an overall of 237 sentences,
</bodyText>
<table confidence="0.987738888888889">
Ambiguity Templates #
PP NNP V DT [JJ] NN1 IN DT [JJ] NN2. 48
VP NNP1 V [IN] NNP2 V [JJ] NN. 60
Conjunction NNP1 [and NNP2] V DT JJ NN1 and NN2. 40
NNP V DT NN1 or DT NN2 and DT NN3.
Logical Form NNP1 and NNP2 V a NN. 35
Someone V the NNS.
Discourse Anaphora NNP V DT NN1 and DT NN2. It is JJ. 36
Ellipsis NNP1 V NNP2. Also NNP3. 18
</table>
<tableCaption confidence="0.99918">
Table 1: POS templates for generating the sen-
</tableCaption>
<bodyText confidence="0.992087078947369">
tences in our corpus. The rightmost column rep-
resents the number of sentences in each category.
The sentences are produced by replacing the POS
tags with all the visually applicable assignments
of lexical items from the corpus lexicon shown in
table 3.
of which 213 sentences have 2 candidate interpre-
tations, and 24 sentences have 3 interpretations.
Table 1 presents the corpus templates for each am-
biguity class, along with the number of sentences
generated from each template.
The corpus videos are filmed in an indoor
environment containing background objects and
pedestrians. To account for the manner of per-
forming actions, videos are shot twice with differ-
ent actors. Whenever applicable, we also filmed
the actions from two different directions (e.g. ap-
proach from the left, and approach from the right).
Finally, all videos were shot with two cameras
from two different view points. Taking these vari-
ations into account, the resulting video corpus
contains 7.1 videos per sentence and 3.37 videos
per sentence interpretation, corresponding to a to-
tal of 1679 videos. The average video length is
3.02 seconds (90.78 frames), with in an overall of
1.4 hours of footage (152434 frames).
A custom corpus is required for this task be-
cause no existing corpus, containing either videos
or images, systematically covers multimodal am-
biguities. Datasets such as UCF Sports (Ro-
driguez et al., 2008), YouTube (Liu et al., 2009),
and HMDB (Kuehne et al., 2011) which come out
of the activity recognition community are accom-
panied by action labels, not sentences, and do not
control for the content of the videos aside from the
principal action being performed. Datasets for im-
age and video captioning, such as MSCOCO (Lin
et al., 2014) and TACOS (Regneri et al., 2013),
</bodyText>
<table confidence="0.898121583333333">
Syntax
Semantics
1480
Ambiguity Example Linguistic interpretations Visual setups
PP Claire left the green chair with Claire [left the green chair] [with a yellow bag]. The bag is with Claire.
a yellow bag. Claire left [the green chair with a yellow bag]. Bag is on the chair.
chair(x), chair(y), x =� y, person(u), person(-), Each chair moved by a different person.
u =� -, move(u, x), move(-, y)
Discourse Anaphora Claire held the bag and the It = bag The bag is yellow.
chair. It is yellow. It = chair The chair is yellow.
Ellipsis Claire looked at Bill. Claire looked at Bill and Sam. Claire looks at Bill and Sam.
Also Sam. Claire and Sam looked at Bill. Claire and Sam look at Bill.
</table>
<tableCaption confidence="0.995848">
Table 2: An overview of the different ambiguity types, along with examples of ambiguous sentences
</tableCaption>
<bodyText confidence="0.98224375">
with their linguistic and visual interpretations. Note that similarly to semantic ambiguities, syntactic
and discourse ambiguities are also provided with first order logic formulas for the resulting sentence
interpretations. Table 4 shows additional examples for each ambiguity type, with frames from sample
videos corresponding to the different interpretations of each sentence.
</bodyText>
<subsectionHeader confidence="0.460685">
Syntactic Category Visual Category Words
</subsectionHeader>
<table confidence="0.92511975">
Nouns Objects, People chair, bag, telescope, someone, proper names
Verbs Actions pick up, put down, hold, move (transitive), look at, approach, leave
Prepositions Spacial Relations with, left of, right of, on
Adjectives Visual Properties yellow, green
</table>
<tableCaption confidence="0.997758">
Table 3: The lexicon used to instantiate the templates in figure 1 in order to generate the corpus.
</tableCaption>
<table confidence="0.969157117647059">
Claire looked at [Bill [picking up a chair]]. Bill picks up the chair.
Claire held a [[green bag] and [chair]]. The chair is not green.
Claire held [[the chair] or [the bag and the telescope]]. Claire holds the chair.
Claire held [[the chair or the bag] and [the telescope]]. Claire holds the chair and the telescope.
VP Claire looked at Bill picking
up a chair.
Claire held the chair or the
bag and the telescope.
Logical Form Claire and Bill moved a chair. chair(x), move(Claire, x), move(Bill, x) Claire and Bill move the same chair.
Semantics Someone moved the two chair(x), chair(y), move(Claire, x), Claire and Bill move different chairs.
chairs. move(Bill, y), x =� y One person moves both chairs.
chair(x), chair(y), x =� y, person(u),
move(u, x), move(u, y)
Claire [looked at Bill] [picking up a chair]. Claire picks up the chair.
Conjunction Claire held a green bag and
chair.
Syntax
</table>
<bodyText confidence="0.959994636363636">
Claire held a [green [bag and chair]]. The chair is green.
aim to control for more aspects of the videos than
just the main action being performed but they do
not provide the range of ambiguities discussed
here. The closest dataset is that of Siddharth et al.
(2014) as it controls for object appearance, color,
action, and direction of motion, making it more
likely to be suitable for evaluating disambiguation
tasks. Unfortunately, that dataset was designed to
avoid ambiguities, and therefore is not suitable for
evaluating the work described here.
</bodyText>
<sectionHeader confidence="0.997086" genericHeader="method">
6 Model
</sectionHeader>
<bodyText confidence="0.9999343">
To perform the disambiguation task, we extend
the sentence recognition model of Siddharth et
al. (2014) which represents sentences as compo-
sitions of words. Given a sentence, its first order
logic interpretation and a video, our model pro-
duces a score which determines if the sentence is
depicted by the video. It simultaneously tracks the
participants in the events described by the sentence
while recognizing the events themselves. This al-
lows it to be flexible in the presence of noise by
integrating top-down information from the sen-
tence with bottom-up information from object and
property detectors. Each word in the query sen-
tence is represented by an HMM (Baum et al.,
1970), which recognizes tracks (i.e. paths of de-
tections in a video for a specific object) that satisfy
the semantics of the given word. In essence, this
model can be described as having two layers, one
in which object tracking occurs and one in which
words observe tracks and filter tracks that do not
satisfy the word constraints.
Given a sentence interpretation, we construct
a sentence-specific model which recognizes if a
video depicts the sentence as follows. Each pred-
icate in the first order logic formula has a cor-
responding HMM, which can recognize if that
predicate is true of a video given its arguments.
Each variable has a corresponding tracker which
attempts to physically locate the bounding box
corresponding to that variable in each frame of a
</bodyText>
<page confidence="0.991531">
1481
</page>
<note confidence="0.486033">
PP Attachment Sam looked at Bill with a telescope.
</note>
<tableCaption confidence="0.971644">
Table 4: Examples of the six ambiguity classes described in table 2. The example sentences have at
least two interpretations, which are depicted by different videos. Three frames from each such video are
shown on the left and on the right below each sentence.
</tableCaption>
<figure confidence="0.988062075471699">
VP Attachment Bill approached the person holding a green chair.
Conjunction Sam and Bill picked up the yellow bag and chair.
Logical Form Someone put down the bags.
Anaphora Sam picked up the bag and the chair. It is yellow.
Ellipsis Sam left Bill. Also Clark.
1482
track 1 track L
agent,-track agent2-track patient,-track patient2-track
=6 person person moved chair
agent,-track agent2-track patient-track
=6 person person moved chair =6
predicate 1 predicate W
h a
...
...
...
. . .
. . .
. . .
. . .
...
h a
...
...
...
. . .
. . .
. . .
. . .
...
×···×
×
t = 1 t = 2 t = 3 t = T
...
...
...
. . .
. . .
. . .
. . .
...
f g
t = 1 t = 2 t = 3 t = T
...
...
...
. . .
. . .
. . .
. . .
...
f g
×···×
</figure>
<figureCaption confidence="0.799842">
Figure 3: (left) Tracker lattices for every sentence participant are combined with predicate HMMs. The
MAP estimate in the resulting cross-product lattice simultaneously finds the best tracks and the best state
</figureCaption>
<bodyText confidence="0.994403153846154">
sequences for every predicate. (right) Two interpretations of the sentence “Claire and Bill moved a chair”
having different first order logic formulas. The top interpretation corresponds to Bill and Claire moving
the same chair, while the bottom one describes them moving different chairs. Predicates are highlighted
in blue at the top and variables are highlighted in red at the bottom. Each predicate has a corresponding
HMM which recognizes its presence in a video. Each variable has a corresponding tracker which locates
it in a video. Lines connect predicates and the variables which fill their argument slots. Some predicates,
such as move and 6=, take multiple arguments. Some predicates, such as move, are applied multiple times
between different pairs of variables.
video. This creates a bipartite graph: HMMs that
represent predicates are connected to trackers that
represent variables. The trackers themselves are
similar to the HMMs, in that they comprise a lat-
tice of potential bounding boxes in every frame.
To construct a joint model for a sentence interpre-
tation, we take the cross product of HMMs and
trackers, taking only those cross products dictated
by the structure of the formula corresponding to
the desired interpretation. Given a video, we em-
ploy an object detector to generate candidate de-
tections in each frame, construct trackers which
select one of these detections in each frame, and fi-
nally construct the overall model from HMMs and
trackers.
Provided an interpretation and its correspond-
ing formula composed of P predicates and V vari-
ables, along with a collection of object detections,
</bodyText>
<equation confidence="0.5290325">
bframe
detection index, in each frame of a video of length
</equation>
<bodyText confidence="0.962800260869565">
T the model computes the score of the video-
sentence pair by finding the optimal detection for
each participant in every frame. This is in essence
the Viterbi algorithm (Viterbi, 1971), the MAP al-
gorithm for HMMs, applied to finding optimal ob-
ject detections jframe
variable for each participant, and the
optimal state
kframe
�cate for each predicate HMM, in
every frame. Each detection is scored by its con-
fidence from the object detector, f and each ob-
ject track is scored by a motion coherence metric g
which determines if the motion of the track agrees
with the underlying optical flow. Each predicate,
p, is scored by the probability of observing a par-
ticular detection in a given state hp, and by the
probability of transitioning between states ap. The
structure of the formula and the fact that multi-
ple predicates often refer to the same variables is
recorded by θ, a mapping between predicates and
their arguments. The model computes the MAP
estimate as:
</bodyText>
<table confidence="0.987574222222222">
max max V T f(btjt v) + T g(bjv i1 btjv)+
j1 1 ,...,jT 1 k1 kT E E E
.. 1,..., 1 v=1 t=1 t=2
. ...
j1 V ,..., jTV T
k1P ,..., kP
P T hp(kt p, btθ1 , btθn ) + T ap(t-1 t
E E E kp ,kp)
p=1 t=1 t=2
</table>
<bodyText confidence="0.9996875">
for sentences which have words that refer to at
most two tracks (i.e. transitive verbs or binary
predicates) but is trivially extended to arbitrary ar-
ities. Figure 3 provides a visual overview of the
model as a cross-product of tracker models and
word models.
Our model extends the approach of Siddharth et
al. (2014) in several ways. First, we depart from
the dependency based representation used in that
work, and recast the model to encode first order
logic formulas. Note that some complex first or-
der logic formulas cannot be directly encoded in
the model and require additional inference steps.
This extension enables us to represent ambiguities
in which a given sentence has multiple logical in-
terpretations for the same syntactic parse.
</bodyText>
<page confidence="0.970282">
1483
</page>
<bodyText confidence="0.999981036363636">
Second, we introduce several model compo-
nents which are not specific to disambiguation, but
are required to encode linguistic constructions that
are present in our corpus and could not be handled
by the model of Siddharth et al. (2014). These new
components are the predicate “not equal”, disjunc-
tion, and conjunction. The key addition among
these components is support for the new predicate
“not equal”, which enforces that two tracks, i.e.
objects, are distinct from each other. For example,
in the sentence “Claire and Bill moved a chair”
one would want to ensure that the two movers are
distinct entities. In earlier work, this was not re-
quired because the sentences tested in that work
were designed to distinguish objects based on con-
straints rather than identity. In other words, there
might have been two different people but they
were distinguished in the sentence by their actions
or appearance. To faithfully recognize that two ac-
tors are moving the chair in the earlier example,
we must ensure that they are disjoint from each
other. In order to do this we create a new HMM
for this predicate, which assigns low probability
to tracks that heavily overlap, forcing the model to
fit two different actors in the previous example. By
combining the new first order logic based seman-
tic representation in lieu of a syntactic represen-
tation with a more expressive model, we can en-
code the sentence interpretations required to per-
form the disambiguation task.
Figure 3(left) shows an example of two differ-
ent interpretations of the above discussed sentence
“Claire and Bill moved a chair”. Object track-
ers, which correspond to variables in the first order
logic representation of the sentence interpretation,
are shown in red. Predicates which constrain the
possible bindings of the trackers, corresponding to
predicates in the representation of the sentence,
are shown in blue. Links represent the argument
structure of the first order logic formula, and de-
termine the cross products that are taken between
the predicate HMMs and tracker lattices in order
to form the joint model which recognizes the en-
tire interpretation in a video.
The resulting model provides a single unified
formalism for representing all the ambiguities in
table 2. Moreover, this approach can be tuned to
different levels of specificity. We can create mod-
els that are specific to one interpretation of a sen-
tence or that are generic, and accept multiple inter-
pretations by eliding constraints that are not com-
mon between the different interpretations. This al-
lows the model, like humans, to defer deciding on
a particular interpretation or to infer that multiple
interpretation of the sentence are plausible.
</bodyText>
<sectionHeader confidence="0.977957" genericHeader="evaluation">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.999967955555556">
We tested the performance of the model described
in the previous section on the LAVA dataset pre-
sented in section 5. Each video in the dataset was
pre-processed with object detectors for humans,
bags, chairs, and telescopes. We employed a mix-
ture of CNN (Krizhevsky et al., 2012) and DPM
(Felzenszwalb et al., 2010) detectors, trained on
held out sections of our corpus. For each object
class we generated proposals from both the CNN
and the DPM detectors, and trained a scoring func-
tion to map both results into the same space. The
scoring function consisted of a sigmoid over the
confidence of the detectors trained on the same
held out portion of the training set. As none of the
disambiguation examples discussed here rely on
the specific identity of the actors, we did not detect
their identity. Instead, any sentence which con-
tains names was automatically converted to one
which contains arbitrary “person” labels.
The sentences in our corpus have either two or
three interpretations. Each interpretation has one
or more associated videos where the scene was
shot from a different angle, carried out either by
different actors, with different objects, or in differ-
ent directions of motion. For each sentence-video
pair, we performed a 1-out-of-2 or 1-out-of-3 clas-
sification task to determine which of the interpre-
tations of the corresponding sentence best fits that
video. Overall chance performance on our dataset
is 49.04%, slightly lower than 50% due to the 1-
out-of-3 classification examples.
The model presented here achieved an accuracy
of 75.36% over the entire corpus averaged across
all error categories. This demonstrates that the
model is largely capable of capturing the under-
lying task and that similar compositional cross-
modal models may do the same. For each of the
3 major ambiguity classes we had an accuracy of
84.26% for syntactic ambiguities, 72.28% for se-
mantic ambiguities, and 64.44% for discourse am-
biguities.
The most significant source of model failures
are poor object detections. Objects are often ro-
tated and presented at angles that are difficult to
recognize. Certain object classes like the telescope
</bodyText>
<page confidence="0.98314">
1484
</page>
<bodyText confidence="0.999976710526316">
are much more difficult to recognize due to their
small size and the fact that hands tend to largely
occlude them. This accounts for the degraded per-
formance of the semantic ambiguities relative to
the syntactic ambiguities, as many more seman-
tic ambiguities involved the telescope. Object de-
tector performance is similarly responsible for the
lower performance of the discourse ambiguities
which relied much more on the accuracy of the
person detector as many sentences involve only
people interacting with each other without any ad-
ditional objects. This degrades performance by re-
moving a helpful constraint for inference, accord-
ing to which people tend to be close to the objects
they are manipulating. In addition, these sentences
introduced more visual uncertainty as they often
involved three actors.
The remaining errors are due to the event mod-
els. HMMs can fixate on short sequences of events
which seem as if they are part of an action, but in
fact are just noise or the prefix of another action.
Ideally, one would want an event model which has
a global view of the action, if an object went up
from the beginning to the end of the video while
a person was holding it, it’s likely that the object
was being picked up. The event models used here
cannot enforce this constraint, they merely assert
that the object was moving up for some number of
frames; an event which can happen due to noise
in the object detectors. Enforcing such local con-
straints instead of the global constraint of the mo-
tion of the object over the video makes joint track-
ing and event recognition tractable in the frame-
work presented here but can lead to errors. Finding
models which strike a better balance between local
information and global constraints while maintain-
ing tractable inference remains an area of future
work.
</bodyText>
<sectionHeader confidence="0.997662" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99993088">
We present a novel framework for studying am-
biguous utterances expressed in a visual context.
In particular, we formulate a new task for resolv-
ing structural ambiguities using visual signal. This
is a fundamental task for humans, involving com-
plex cognitive processing, and is a key challenge
for language acquisition during childhood. We
release a multimodal corpus that enables to ad-
dress this task, as well as support further inves-
tigation of ambiguity related phenomena in visu-
ally grounded language processing. Finally, we
present a unified approach for resolving ambigu-
ous descriptions of videos, achieving good perfor-
mance on our corpus.
While our current investigation focuses on
structural inference, we intend to extend this line
of work to learning scenarios, in which the agent
has to deduce the meaning of words and sentences
from structurally ambiguous input. Furthermore,
our framework can be beneficial for image and
video retrieval applications in which the query is
expressed in natural language. Given an ambigu-
ous query, our approach will enable matching and
clustering the retrieved results according to the dif-
ferent query interpretations.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9990428">
This material is based upon work supported by
the Center for Brains, Minds, and Machines
(CBMM), funded by NSF STC award CCF-
1231216. SU was also supported by ERC Ad-
vanced Grant 269627 Digital Baby.
</bodyText>
<page confidence="0.989709">
1485
</page>
<sectionHeader confidence="0.989987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975308411215">
Kobus Barnard, Matthew Johnson, and David Forsyth.
2003. Word sense disambiguation with pictures. In
Proceedings of the HLT-NAACL 2003 workshop on
Learning word meaning from non-linguistic data-
Volume 6, pages 1–5. Association for Computational
Linguistics.
L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970.
A maximization technique occuring in the statistical
analysis of probabilistic functions of Markov chains.
The Annals of Mathematical Statistics, 41(1):164–
171.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012. Distributional semantics in tech-
nicolor. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 136–145. Asso-
ciation for Computational Linguistics.
Moreno I Coco and Frank Keller. 2015. The interac-
tion of visual and linguistic saliency during syntactic
ambiguity resolution. The Quarterly Journal of Ex-
perimental Psychology, 68(1):46–74.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: Generating sentences from im-
ages. In Computer Vision–ECCV 2010, pages 15–
29. Springer.
Pedro F Felzenszwalb, Ross B Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part-based
models. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 32(9):1627–1645.
Yunchao Gong, Liwei Wang, Micah Hodosh, Ju-
lia Hockenmaier, and Svetlana Lazebnik. 2014.
Improving image-sentence embeddings using large
weakly annotated photo collections. In Computer
Vision–ECCV 2014, pages 529–545. Springer.
Andrej Karpathy and Li Fei-Fei. 2014. Deep visual-
semantic alignments for generating image descrip-
tions. arXiv preprint arXiv:1412.2306.
Evan Kidd and Judith Holler. 2009. Children’s use of
gesture to resolve lexical ambiguity. Developmental
Science, 12(6):903–913.
Chen Kong, Dahua Lin, Mayank Bansal, Raquel Urta-
sun, and Sanja Fidler. 2014. What are you talking
about? text-to-image coreference. In Computer Vi-
sion and Pattern Recognition (CVPR), pages 3558–
3565. IEEE.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Advances in neural
information processing systems, pages 1097–1105.
Hildegard Kuehne, Hueihan Jhuang, Est´ıbaliz Garrote,
Tomaso Poggio, and Thomas Serre. 2011. Hmdb:
a large video database for human motion recogni-
tion. In Computer Vision (ICCV), 2011 IEEE Inter-
national Conference on, pages 2556–2563. IEEE.
G Kulkarni, V Premraj, S Dhar, Siming Li, Yejin Choi,
AC Berg, and TL Berg. 2011. Baby talk: Un-
derstanding and generating simple image descrip-
tions. In Proceedings of the 2011 IEEE Conference
on Computer Vision and Pattern Recognition, pages
1601–1608. IEEE Computer Society.
Angeliki Lazaridou, Nghia The Pham, and Marco
Baroni. 2015. Combining language and vi-
sion with a multimodal skip-gram model. CoRR,
abs/1501.02598.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In Computer Vision–
ECCV 2014, pages 740–755. Springer.
Jingen Liu, Jiebo Luo, and Mubarak Shah. 2009. Rec-
ognizing realistic actions from videos in the wild.
In Computer Vision and Pattern Recognition, 2009.
CVPR 2009. IEEE Conference on, pages 1996–
2003. IEEE.
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum´e III.
2012. Midge: Generating image descriptions from
computer vision detections. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 747–
756. Association for Computational Linguistics.
Vignesh Ramanathan, Armand Joulin, Percy Liang,
and Li Fei-Fei. 2014. Linking people in videos with
their names using coreference resolution. In Com-
puter Vision–ECCV 2014, pages 95–110. Springer.
Michaela Regneri, Marcus Rohrbach, Dominikus Wet-
zel, Stefan Thater, Bernt Schiele, and Manfred
Pinkal. 2013. Grounding action descriptions in
videos. Transactions of the Association for Com-
putational Linguistics, 1:25–36.
Mikel D. Rodriguez, Javed Ahmed, and Mubarak Shah.
2008. Action MACH A Spatio-temporal Maximum
Average Correlation Height Filter for Action Recog-
nition. In Computer Vision and Pattern Recognition,
pages 1–8.
Narayanaswamy Siddharth, Andrei Barbu, and Jef-
frey Mark Siskind. 2014. Seeing what you’re told:
Sentence-guided activity recognition in video. In
Computer Vision and Pattern Recognition (CVPR),
pages 732–739. IEEE.
Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In Proceedings of ACL, pages 721–732.
</reference>
<page confidence="0.830721">
1486
</page>
<reference confidence="0.999646931818182">
Catherine E Snow. 1972. Mothers’ speech to children
learning language. Child development, pages 549–
565.
Richard Socher, Milind Ganjoo, Christopher D Man-
ning, and Andrew Ng. 2013. Zero-shot learning
through cross-modal transfer. In Advances in Neu-
ral Information Processing Systems, pages 935–943.
Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.
Michael J Spivey, Michael K Tanenhaus, Kathleen M
Eberhard, and Julie C Sedivy. 2002. Eye move-
ments and spoken language comprehension: Effects
of visual context on syntactic ambiguity resolution.
Cognitive psychology, 45(4):447–481.
Michael K Tanenhaus, Michael J Spivey-Knowlton,
Kathleen M Eberhard, and Julie C Sedivy. 1995.
Integration of visual and linguistic information
in spoken language comprehension. Science,
268(5217):1632–1634.
Jesse Thomason, Subhashini Venugopalan, Sergio
Guadarrama, Kate Saenko, and Raymond Mooney.
2014. Integrating language and vision to generate
natural language descriptions of videos in the wild.
In Proceedings of the 25th International Conference
on Computational Linguistics (COLING), August.
Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,
Marcus Rohrbach, Raymond Mooney, and Kate
Saenko. 2015. Translating videos to natural lan-
guage using deep recurrent neural networks. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Den-
ver, Colorado.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Computer Vision and Pat-
tern Recognition (CVPR).
A. J. Viterbi. 1971. Convolutional codes and their per-
formance in communication systems. Communica-
tions of the IEEE, 19:751–772, October.
</reference>
<page confidence="0.993907">
1487
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820490">
<title confidence="0.9466485">Do You See What I Mean? Visual Resolution of Linguistic Ambiguities</title>
<author confidence="0.99993">Yevgeni Berzak Andrei Barbu Daniel Harari</author>
<affiliation confidence="0.997186">CSAIL MIT CSAIL MIT CSAIL MIT</affiliation>
<email confidence="0.975132">berzak@mit.eduandrei@0xab.comhararid@mit.edu</email>
<author confidence="0.999799">Boris Katz Shimon Ullman</author>
<affiliation confidence="0.988204">CSAIL MIT Weizmann Institute of Science</affiliation>
<email confidence="0.951991">boris@mit.edushimon.ullman@weizmann.ac.il</email>
<abstract confidence="0.999740608695652">Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>Matthew Johnson</author>
<author>David Forsyth</author>
</authors>
<title>Word sense disambiguation with pictures.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 workshop on Learning word meaning from non-linguistic dataVolume 6,</booktitle>
<pages>1--5</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5831" citStr="Barnard et al., 2003" startWordPosition="890" endWordPosition="893"> 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014). Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision. The </context>
</contexts>
<marker>Barnard, Johnson, Forsyth, 2003</marker>
<rawString>Kobus Barnard, Matthew Johnson, and David Forsyth. 2003. Word sense disambiguation with pictures. In Proceedings of the HLT-NAACL 2003 workshop on Learning word meaning from non-linguistic dataVolume 6, pages 1–5. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
<author>T Petrie</author>
<author>G Soules</author>
<author>N Weiss</author>
</authors>
<title>A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains.</title>
<date>1970</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>41</volume>
<issue>1</issue>
<pages>171</pages>
<contexts>
<context position="19787" citStr="Baum et al., 1970" startWordPosition="3169" endWordPosition="3172">nition model of Siddharth et al. (2014) which represents sentences as compositions of words. Given a sentence, its first order logic interpretation and a video, our model produces a score which determines if the sentence is depicted by the video. It simultaneously tracks the participants in the events described by the sentence while recognizing the events themselves. This allows it to be flexible in the presence of noise by integrating top-down information from the sentence with bottom-up information from object and property detectors. Each word in the query sentence is represented by an HMM (Baum et al., 1970), which recognizes tracks (i.e. paths of detections in a video for a specific object) that satisfy the semantics of the given word. In essence, this model can be described as having two layers, one in which object tracking occurs and one in which words observe tracks and filter tracks that do not satisfy the word constraints. Given a sentence interpretation, we construct a sentence-specific model which recognizes if a video depicts the sentence as follows. Each predicate in the first order logic formula has a corresponding HMM, which can recognize if that predicate is true of a video given its</context>
</contexts>
<marker>Baum, Petrie, Soules, Weiss, 1970</marker>
<rawString>L. E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970. A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains. The Annals of Mathematical Statistics, 41(1):164– 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>NamKhanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>136--145</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5148" citStr="Bruni et al., 2012" startWordPosition="780" endWordPosition="783">or visually grounded language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence’s content. Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and NamKhanh Tran. 2012. Distributional semantics in technicolor. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 136–145. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moreno I Coco</author>
<author>Frank Keller</author>
</authors>
<title>The interaction of visual and linguistic saliency during syntactic ambiguity resolution.</title>
<date>2015</date>
<journal>The Quarterly Journal of Experimental Psychology,</journal>
<volume>68</volume>
<issue>1</issue>
<contexts>
<context position="6746" citStr="Coco and Keller, 2015" startWordPosition="1025" endWordPosition="1028">ands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision. The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995). A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans. Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972). Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content (Kidd and Holler, 2009). Our study leverages such insights to develop a complementary framework that enables addressing the chall</context>
</contexts>
<marker>Coco, Keller, 2015</marker>
<rawString>Moreno I Coco and Frank Keller. 2015. The interaction of visual and linguistic saliency during syntactic ambiguity resolution. The Quarterly Journal of Experimental Psychology, 68(1):46–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: Generating sentences from images.</title>
<date>2010</date>
<booktitle>In Computer Vision–ECCV</booktitle>
<pages>15--29</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5335" citStr="Farhadi et al., 2010" startWordPosition="813" endWordPosition="816">orpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not </context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: Generating sentences from images. In Computer Vision–ECCV 2010, pages 15– 29. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>Ross B Girshick</author>
<author>David McAllester</author>
<author>Deva Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence,</title>
<date>2010</date>
<journal>IEEE Transactions on,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context position="28385" citStr="Felzenszwalb et al., 2010" startWordPosition="4663" endWordPosition="4666">are generic, and accept multiple interpretations by eliding constraints that are not common between the different interpretations. This allows the model, like humans, to defer deciding on a particular interpretation or to infer that multiple interpretation of the sentence are plausible. 7 Experimental Results We tested the performance of the model described in the previous section on the LAVA dataset presented in section 5. Each video in the dataset was pre-processed with object detectors for humans, bags, chairs, and telescopes. We employed a mixture of CNN (Krizhevsky et al., 2012) and DPM (Felzenszwalb et al., 2010) detectors, trained on held out sections of our corpus. For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring function to map both results into the same space. The scoring function consisted of a sigmoid over the confidence of the detectors trained on the same held out portion of the training set. As none of the disambiguation examples discussed here rely on the specific identity of the actors, we did not detect their identity. Instead, any sentence which contains names was automatically converted to one which contains arbitrary “person” la</context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. 2010. Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(9):1627–1645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunchao Gong</author>
<author>Liwei Wang</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
<author>Svetlana Lazebnik</author>
</authors>
<title>Improving image-sentence embeddings using large weakly annotated photo collections.</title>
<date>2014</date>
<booktitle>In Computer Vision–ECCV</booktitle>
<pages>529--545</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5215" citStr="Gong et al., 2014" startWordPosition="792" endWordPosition="795">entence has to be disambiguated using a visual depiction of the sentence’s content. Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barna</context>
</contexts>
<marker>Gong, Wang, Hodosh, Hockenmaier, Lazebnik, 2014</marker>
<rawString>Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hockenmaier, and Svetlana Lazebnik. 2014. Improving image-sentence embeddings using large weakly annotated photo collections. In Computer Vision–ECCV 2014, pages 529–545. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Li Fei-Fei</author>
</authors>
<title>Deep visualsemantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306.</title>
<date>2014</date>
<contexts>
<context position="5453" citStr="Karpathy and Fei-Fei, 2014" startWordPosition="834" endWordPosition="837">atic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreferenc</context>
</contexts>
<marker>Karpathy, Fei-Fei, 2014</marker>
<rawString>Andrej Karpathy and Li Fei-Fei. 2014. Deep visualsemantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Kidd</author>
<author>Judith Holler</author>
</authors>
<title>Children’s use of gesture to resolve lexical ambiguity.</title>
<date>2009</date>
<journal>Developmental Science,</journal>
<volume>12</volume>
<issue>6</issue>
<contexts>
<context position="7240" citStr="Kidd and Holler, 2009" startWordPosition="1097" endWordPosition="1100"> considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans. Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972). Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content (Kidd and Holler, 2009). Our study leverages such insights to develop a complementary framework that enables addressing the challenge of visually grounded disambiguation of language in the realm of artificial intelligence. 3 Task In this work we provide a concrete framework for the study of language understanding with visual context by introducing the task of grounded language disambiguation. This task requires to choose the correct linguistic representation of a sentence given a visual context depicted in a video. Specifically, provided with a sentence, n candidate interpretations of that sentence and a video that </context>
</contexts>
<marker>Kidd, Holler, 2009</marker>
<rawString>Evan Kidd and Judith Holler. 2009. Children’s use of gesture to resolve lexical ambiguity. Developmental Science, 12(6):903–913.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Kong</author>
<author>Dahua Lin</author>
<author>Mayank Bansal</author>
<author>Raquel Urtasun</author>
<author>Sanja Fidler</author>
</authors>
<title>What are you talking about? text-to-image coreference.</title>
<date>2014</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>3558--3565</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6110" citStr="Kong et al., 2014" startWordPosition="928" endWordPosition="931">t al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014). Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision. The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995). A considerable fraction of this work focused on the processing of ambiguous language (Spivey </context>
</contexts>
<marker>Kong, Lin, Bansal, Urtasun, Fidler, 2014</marker>
<rawString>Chen Kong, Dahua Lin, Mayank Bansal, Raquel Urtasun, and Sanja Fidler. 2014. What are you talking about? text-to-image coreference. In Computer Vision and Pattern Recognition (CVPR), pages 3558– 3565. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1097--1105</pages>
<contexts>
<context position="28349" citStr="Krizhevsky et al., 2012" startWordPosition="4657" endWordPosition="4660">erpretation of a sentence or that are generic, and accept multiple interpretations by eliding constraints that are not common between the different interpretations. This allows the model, like humans, to defer deciding on a particular interpretation or to infer that multiple interpretation of the sentence are plausible. 7 Experimental Results We tested the performance of the model described in the previous section on the LAVA dataset presented in section 5. Each video in the dataset was pre-processed with object detectors for humans, bags, chairs, and telescopes. We employed a mixture of CNN (Krizhevsky et al., 2012) and DPM (Felzenszwalb et al., 2010) detectors, trained on held out sections of our corpus. For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring function to map both results into the same space. The scoring function consisted of a sigmoid over the confidence of the detectors trained on the same held out portion of the training set. As none of the disambiguation examples discussed here rely on the specific identity of the actors, we did not detect their identity. Instead, any sentence which contains names was automatically converted to one </context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hildegard Kuehne</author>
<author>Hueihan Jhuang</author>
<author>Est´ıbaliz Garrote</author>
<author>Tomaso Poggio</author>
<author>Thomas Serre</author>
</authors>
<title>Hmdb: a large video database for human motion recognition.</title>
<date>2011</date>
<booktitle>In Computer Vision (ICCV), 2011 IEEE International Conference on,</booktitle>
<pages>2556--2563</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="15782" citStr="Kuehne et al., 2011" startWordPosition="2509" endWordPosition="2512"> shot with two cameras from two different view points. Taking these variations into account, the resulting video corpus contains 7.1 videos per sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos. The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage (152434 frames). A custom corpus is required for this task because no existing corpus, containing either videos or images, systematically covers multimodal ambiguities. Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed. Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013), Syntax Semantics 1480 Ambiguity Example Linguistic interpretations Visual setups PP Claire left the green chair with Claire [left the green chair] [with a yellow bag]. The bag is with Claire. a yellow bag. Claire left [the green chair with a yellow bag]. Bag is on the chair. chair(x), chai</context>
</contexts>
<marker>Kuehne, Jhuang, Garrote, Poggio, Serre, 2011</marker>
<rawString>Hildegard Kuehne, Hueihan Jhuang, Est´ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. 2011. Hmdb: a large video database for human motion recognition. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 2556–2563. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kulkarni</author>
<author>V Premraj</author>
<author>S Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>AC Berg</author>
<author>TL Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1601--1608</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="5358" citStr="Kulkarni et al., 2011" startWordPosition="817" endWordPosition="820">pled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-rela</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>G Kulkarni, V Premraj, S Dhar, Siming Li, Yejin Choi, AC Berg, and TL Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, pages 1601–1608. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
</authors>
<title>Nghia The Pham, and</title>
<date>2015</date>
<location>CoRR, abs/1501.02598.</location>
<marker>Lazaridou, 2015</marker>
<rawString>Angeliki Lazaridou, Nghia The Pham, and Marco Baroni. 2015. Combining language and vision with a multimodal skip-gram model. CoRR, abs/1501.02598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsung-Yi Lin</author>
<author>Michael Maire</author>
<author>Serge Belongie</author>
<author>James Hays</author>
<author>Pietro Perona</author>
<author>Deva Ramanan</author>
<author>Piotr Doll´ar</author>
<author>C Lawrence Zitnick</author>
</authors>
<title>Microsoft coco: Common objects in context.</title>
<date>2014</date>
<booktitle>In Computer Vision– ECCV 2014,</booktitle>
<pages>740--755</pages>
<publisher>Springer.</publisher>
<marker>Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll´ar, Zitnick, 2014</marker>
<rawString>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision– ECCV 2014, pages 740–755. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingen Liu</author>
<author>Jiebo Luo</author>
<author>Mubarak Shah</author>
</authors>
<title>Recognizing realistic actions from videos in the wild.</title>
<date>2009</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>pages</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="15750" citStr="Liu et al., 2009" startWordPosition="2503" endWordPosition="2506">ht). Finally, all videos were shot with two cameras from two different view points. Taking these variations into account, the resulting video corpus contains 7.1 videos per sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos. The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage (152434 frames). A custom corpus is required for this task because no existing corpus, containing either videos or images, systematically covers multimodal ambiguities. Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed. Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013), Syntax Semantics 1480 Ambiguity Example Linguistic interpretations Visual setups PP Claire left the green chair with Claire [left the green chair] [with a yellow bag]. The bag is with Claire. a yellow bag. Claire left [the green chair with a yellow bag]. Bag</context>
</contexts>
<marker>Liu, Luo, Shah, 2009</marker>
<rawString>Jingen Liu, Jiebo Luo, and Mubarak Shah. 2009. Recognizing realistic actions from videos in the wild. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1996– 2003. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Xufeng Han</author>
<author>Jesse Dodge</author>
<author>Alyssa Mensch</author>
<author>Amit Goyal</author>
<author>Alex Berg</author>
<author>Kota Yamaguchi</author>
<author>Tamara Berg</author>
<author>Karl Stratos</author>
<author>Hal Daum´e</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>747--756</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Mitchell, Han, Dodge, Mensch, Goyal, Berg, Yamaguchi, Berg, Stratos, Daum´e, 2012</marker>
<rawString>Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg, Karl Stratos, and Hal Daum´e III. 2012. Midge: Generating image descriptions from computer vision detections. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747– 756. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vignesh Ramanathan</author>
<author>Armand Joulin</author>
<author>Percy Liang</author>
<author>Li Fei-Fei</author>
</authors>
<title>Linking people in videos with their names using coreference resolution. In Computer Vision–ECCV</title>
<date>2014</date>
<pages>95--110</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6090" citStr="Ramanathan et al., 2014" startWordPosition="924" endWordPosition="927"> al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014). Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision. The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995). A considerable fraction of this work focused on the processing of ambiguo</context>
</contexts>
<marker>Ramanathan, Joulin, Liang, Fei-Fei, 2014</marker>
<rawString>Vignesh Ramanathan, Armand Joulin, Percy Liang, and Li Fei-Fei. 2014. Linking people in videos with their names using coreference resolution. In Computer Vision–ECCV 2014, pages 95–110. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Marcus Rohrbach</author>
<author>Dominikus Wetzel</author>
<author>Stefan Thater</author>
<author>Bernt Schiele</author>
<author>Manfred Pinkal</author>
</authors>
<title>Grounding action descriptions in videos.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--25</pages>
<contexts>
<context position="16090" citStr="Regneri et al., 2013" startWordPosition="2562" endWordPosition="2565">overall of 1.4 hours of footage (152434 frames). A custom corpus is required for this task because no existing corpus, containing either videos or images, systematically covers multimodal ambiguities. Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed. Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013), Syntax Semantics 1480 Ambiguity Example Linguistic interpretations Visual setups PP Claire left the green chair with Claire [left the green chair] [with a yellow bag]. The bag is with Claire. a yellow bag. Claire left [the green chair with a yellow bag]. Bag is on the chair. chair(x), chair(y), x =� y, person(u), person(-), Each chair moved by a different person. u =� -, move(u, x), move(-, y) Discourse Anaphora Claire held the bag and the It = bag The bag is yellow. chair. It is yellow. It = chair The chair is yellow. Ellipsis Claire looked at Bill. Claire looked at Bill and Sam. Claire loo</context>
</contexts>
<marker>Regneri, Rohrbach, Wetzel, Thater, Schiele, Pinkal, 2013</marker>
<rawString>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics, 1:25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikel D Rodriguez</author>
<author>Javed Ahmed</author>
<author>Mubarak Shah</author>
</authors>
<title>Action MACH A Spatio-temporal Maximum Average Correlation Height Filter for Action Recognition.</title>
<date>2008</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="15722" citStr="Rodriguez et al., 2008" startWordPosition="2497" endWordPosition="2501">he left, and approach from the right). Finally, all videos were shot with two cameras from two different view points. Taking these variations into account, the resulting video corpus contains 7.1 videos per sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos. The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage (152434 frames). A custom corpus is required for this task because no existing corpus, containing either videos or images, systematically covers multimodal ambiguities. Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed. Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013), Syntax Semantics 1480 Ambiguity Example Linguistic interpretations Visual setups PP Claire left the green chair with Claire [left the green chair] [with a yellow bag]. The bag is with Claire. a yellow bag. Claire left [the green c</context>
</contexts>
<marker>Rodriguez, Ahmed, Shah, 2008</marker>
<rawString>Mikel D. Rodriguez, Javed Ahmed, and Mubarak Shah. 2008. Action MACH A Spatio-temporal Maximum Average Correlation Height Filter for Action Recognition. In Computer Vision and Pattern Recognition, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narayanaswamy Siddharth</author>
<author>Andrei Barbu</author>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Seeing what you’re told: Sentence-guided activity recognition in video.</title>
<date>2014</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>732--739</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4077" citStr="Siddharth et al., 2014" startWordPosition="609" endWordPosition="612">utational Linguistics. course ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions, logical forms, anaphora and ellipsis. Overall, the corpus contains 237 sentences, with 2 to 3 interpretations per sentence, and an average of 3.37 videos that depict visual variations of each sentence interpretation, corresponding to a total of 1679 videos. Using this corpus, we address the problem of selecting the interpretation of an ambiguous sentence that matches the content of a given video. Our approach for tackling this task extends the sentence tracker introduced in (Siddharth et al., 2014). The sentence tracker produces a score which determines if a sentence is depicted by a video. This earlier work had no concept of ambiguities; it assumed that every sentence had a single interpretation. We extend this approach to represent multiple interpretations of a sentence, enabling us to pick the interpretation that is most compatible with the video. To summarize, the contributions of this paper are threefold. First, we introduce a new task for visually grounded language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence’s cont</context>
<context position="5477" citStr="Siddharth et al., 2014" startWordPosition="838" endWordPosition="842">iguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan</context>
<context position="18811" citStr="Siddharth et al. (2014)" startWordPosition="3013" endWordPosition="3016">Semantics Someone moved the two chair(x), chair(y), move(Claire, x), Claire and Bill move different chairs. chairs. move(Bill, y), x =� y One person moves both chairs. chair(x), chair(y), x =� y, person(u), move(u, x), move(u, y) Claire [looked at Bill] [picking up a chair]. Claire picks up the chair. Conjunction Claire held a green bag and chair. Syntax Claire held a [green [bag and chair]]. The chair is green. aim to control for more aspects of the videos than just the main action being performed but they do not provide the range of ambiguities discussed here. The closest dataset is that of Siddharth et al. (2014) as it controls for object appearance, color, action, and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks. Unfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable for evaluating the work described here. 6 Model To perform the disambiguation task, we extend the sentence recognition model of Siddharth et al. (2014) which represents sentences as compositions of words. Given a sentence, its first order logic interpretation and a video, our model produces a score which determines if the sentence is depicted by the vi</context>
<context position="24925" citStr="Siddharth et al. (2014)" startWordPosition="4097" endWordPosition="4100">es is recorded by θ, a mapping between predicates and their arguments. The model computes the MAP estimate as: max max V T f(btjt v) + T g(bjv i1 btjv)+ j1 1 ,...,jT 1 k1 kT E E E .. 1,..., 1 v=1 t=1 t=2 . ... j1 V ,..., jTV T k1P ,..., kP P T hp(kt p, btθ1 , btθn ) + T ap(t-1 t E E E kp ,kp) p=1 t=1 t=2 for sentences which have words that refer to at most two tracks (i.e. transitive verbs or binary predicates) but is trivially extended to arbitrary arities. Figure 3 provides a visual overview of the model as a cross-product of tracker models and word models. Our model extends the approach of Siddharth et al. (2014) in several ways. First, we depart from the dependency based representation used in that work, and recast the model to encode first order logic formulas. Note that some complex first order logic formulas cannot be directly encoded in the model and require additional inference steps. This extension enables us to represent ambiguities in which a given sentence has multiple logical interpretations for the same syntactic parse. 1483 Second, we introduce several model components which are not specific to disambiguation, but are required to encode linguistic constructions that are present in our cor</context>
</contexts>
<marker>Siddharth, Barbu, Siskind, 2014</marker>
<rawString>Narayanaswamy Siddharth, Andrei Barbu, and Jeffrey Mark Siskind. 2014. Seeing what you’re told: Sentence-guided activity recognition in video. In Computer Vision and Pattern Recognition (CVPR), pages 732–739. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning grounded meaning representations with autoencoders.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>721--732</pages>
<contexts>
<context position="5196" citStr="Silberer and Lapata, 2014" startWordPosition="788" endWordPosition="791">ng, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence’s content. Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense di</context>
</contexts>
<marker>Silberer, Lapata, 2014</marker>
<rawString>Carina Silberer and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In Proceedings of ACL, pages 721–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine E Snow</author>
</authors>
<title>Mothers’ speech to children learning language. Child development,</title>
<date>1972</date>
<pages>549--565</pages>
<contexts>
<context position="2123" citStr="Snow, 1972" startWordPosition="316" endWordPosition="317">ons requires integration of world knowledge and perceptual information obtained from other modalities. In this work, we focus on the problem of grounding language in the visual modality, and introduce a novel task for language understanding which requires resolving linguistic ambiguities by utilizing the visual context in which the linguistic content is expressed. This type of inference is frequently called for in human communication that occurs in a visual environment, and is crucial for language acquisition, when much of the linguistic content refers to the visual surroundings of the child (Snow, 1972). Our task is also fundamental to the problem of grounding vision in language, by focusing on phenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked when using language as a medium for expressing understanding of visual content. Due to such ambiguities, a superficially appropriate description of a visual scene may in fact not be sufficient for demonstrating a correct understanding of the relevant visual content. Our task addresses this issue by introducing a deep validation protocol for visual understanding, requiring not only providing a surface descripti</context>
<context position="7031" citStr="Snow, 1972" startWordPosition="1070" endWordPosition="1071">ision. The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995). A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans. Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972). Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content (Kidd and Holler, 2009). Our study leverages such insights to develop a complementary framework that enables addressing the challenge of visually grounded disambiguation of language in the realm of artificial intelligence. 3 Task In this work we provide a concrete framework for the study of language understanding with visual context by introducing the task of grounded language disambiguation. This task requires</context>
</contexts>
<marker>Snow, 1972</marker>
<rawString>Catherine E Snow. 1972. Mothers’ speech to children learning language. Child development, pages 549– 565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Milind Ganjoo</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Zero-shot learning through cross-modal transfer.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>935--943</pages>
<contexts>
<context position="5169" citStr="Socher et al., 2013" startWordPosition="784" endWordPosition="787"> language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence’s content. Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed t</context>
</contexts>
<marker>Socher, Ganjoo, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. 2013. Zero-shot learning through cross-modal transfer. In Advances in Neural Information Processing Systems, pages 935–943.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--207</pages>
<contexts>
<context position="5402" citStr="Socher et al., 2014" startWordPosition="825" endWordPosition="828">linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were </context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Spivey</author>
<author>Michael K Tanenhaus</author>
<author>Kathleen M Eberhard</author>
<author>Julie C Sedivy</author>
</authors>
<title>Eye movements and spoken language comprehension: Effects of visual context on syntactic ambiguity resolution. Cognitive psychology,</title>
<date>2002</date>
<pages>45--4</pages>
<contexts>
<context position="6722" citStr="Spivey et al., 2002" startWordPosition="1021" endWordPosition="1024">, 2014). Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision. The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995). A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans. Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972). Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content (Kidd and Holler, 2009). Our study leverages such insights to develop a complementary framework that enab</context>
</contexts>
<marker>Spivey, Tanenhaus, Eberhard, Sedivy, 2002</marker>
<rawString>Michael J Spivey, Michael K Tanenhaus, Kathleen M Eberhard, and Julie C Sedivy. 2002. Eye movements and spoken language comprehension: Effects of visual context on syntactic ambiguity resolution. Cognitive psychology, 45(4):447–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Tanenhaus</author>
<author>Michael J Spivey-Knowlton</author>
<author>Kathleen M Eberhard</author>
<author>Julie C Sedivy</author>
</authors>
<title>Integration of visual and linguistic information in spoken language comprehension.</title>
<date>1995</date>
<journal>Science,</journal>
<volume>268</volume>
<issue>5217</issue>
<contexts>
<context position="6615" citStr="Tanenhaus et al., 1995" startWordPosition="1004" endWordPosition="1007">ties were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014). Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis. More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision. The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995). A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans. Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972). Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic conten</context>
</contexts>
<marker>Tanenhaus, Spivey-Knowlton, Eberhard, Sedivy, 1995</marker>
<rawString>Michael K Tanenhaus, Michael J Spivey-Knowlton, Kathleen M Eberhard, and Julie C Sedivy. 1995. Integration of visual and linguistic information in spoken language comprehension. Science, 268(5217):1632–1634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesse Thomason</author>
<author>Subhashini Venugopalan</author>
<author>Sergio Guadarrama</author>
<author>Kate Saenko</author>
<author>Raymond Mooney</author>
</authors>
<title>Integrating language and vision to generate natural language descriptions of videos in the wild.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING),</booktitle>
<contexts>
<context position="5425" citStr="Thomason et al., 2014" startWordPosition="829" endWordPosition="833">s, and enables a systematic study of linguistic ambiguities in visual contexts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in w</context>
</contexts>
<marker>Thomason, Venugopalan, Guadarrama, Saenko, Mooney, 2014</marker>
<rawString>Jesse Thomason, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, and Raymond Mooney. 2014. Integrating language and vision to generate natural language descriptions of videos in the wild. In Proceedings of the 25th International Conference on Computational Linguistics (COLING), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhashini Venugopalan</author>
<author>Huijuan Xu</author>
<author>Jeff Donahue</author>
<author>Marcus Rohrbach</author>
<author>Raymond Mooney</author>
<author>Kate Saenko</author>
</authors>
<title>Translating videos to natural language using deep recurrent neural networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="5503" citStr="Venugopalan et al., 2015" startWordPosition="843" endWordPosition="846">xts. Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al.</context>
</contexts>
<marker>Venugopalan, Xu, Donahue, Rohrbach, Mooney, Saenko, 2015</marker>
<rawString>Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. 2015. Translating videos to natural language using deep recurrent neural networks. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Alexander Toshev</author>
<author>Samy Bengio</author>
<author>Dumitru Erhan</author>
</authors>
<title>Show and tell: A neural image caption generator.</title>
<date>2015</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="5526" citStr="Vinyals et al., 2015" startWordPosition="847" endWordPosition="850"> computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%. 2 Related Work Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015). While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities. Previous work relating ambiguity in language to the visual modality addressed the problem of word sense disambiguation (Barnard et al., 2003). However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities. Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014). Our work expan</context>
</contexts>
<marker>Vinyals, Toshev, Bengio, Erhan, 2015</marker>
<rawString>Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In Computer Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Convolutional codes and their performance in communication systems.</title>
<date>1971</date>
<journal>Communications of the IEEE,</journal>
<pages>19--751</pages>
<contexts>
<context position="23633" citStr="Viterbi, 1971" startWordPosition="3854" endWordPosition="3855">Given a video, we employ an object detector to generate candidate detections in each frame, construct trackers which select one of these detections in each frame, and finally construct the overall model from HMMs and trackers. Provided an interpretation and its corresponding formula composed of P predicates and V variables, along with a collection of object detections, bframe detection index, in each frame of a video of length T the model computes the score of the videosentence pair by finding the optimal detection for each participant in every frame. This is in essence the Viterbi algorithm (Viterbi, 1971), the MAP algorithm for HMMs, applied to finding optimal object detections jframe variable for each participant, and the optimal state kframe �cate for each predicate HMM, in every frame. Each detection is scored by its confidence from the object detector, f and each object track is scored by a motion coherence metric g which determines if the motion of the track agrees with the underlying optical flow. Each predicate, p, is scored by the probability of observing a particular detection in a given state hp, and by the probability of transitioning between states ap. The structure of the formula </context>
</contexts>
<marker>Viterbi, 1971</marker>
<rawString>A. J. Viterbi. 1971. Convolutional codes and their performance in communication systems. Communications of the IEEE, 19:751–772, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
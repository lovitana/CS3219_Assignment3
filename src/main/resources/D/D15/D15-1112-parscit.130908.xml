<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000141">
<title confidence="0.96639">
Semantic Role Labeling with Neural Network Factors
</title>
<author confidence="0.946659">
Nicholas FitzGerald$* Oscar Täckström† Kuzman Ganchev† Dipanjan Das†
</author>
<affiliation confidence="0.995699">
$Department of Computer Science and Engineering, University of Washington
</affiliation>
<address confidence="0.63075">
† Google, New York
</address>
<email confidence="0.98651">
nfitz@cs.uw.edu
{oscart,kuzman,dipanjand}@google.com
</email>
<sectionHeader confidence="0.997336" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890625">
We present a new method for semantic role
labeling in which arguments and seman-
tic roles are jointly embedded in a shared
vector space for a given predicate. These
embeddings belong to a neural network,
whose output represents the potential func-
tions of a graphical model designed for
the SRL task. We consider both local
and structured learning methods and ob-
tain strong results on standard PropBank
and FrameNet corpora with a straightfor-
ward product-of-experts model. We fur-
ther show how the model can learn jointly
from PropBank and FrameNet annotations
to obtain additional improvements on the
smaller FrameNet dataset.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998883241935484">
Semantic role labeling (SRL) is the task of iden-
tifying the semantic arguments of a predicate and
labeling them with their semantic roles. A key chal-
lenge in this task is sparsity of labeled data: a given
predicate-role instance may only occur a handful
of times in the training set. Most existing SRL
systems model each semantic role as an atomic
unit of meaning, ignoring finer-grained semantic
similarity between roles that can be leveraged to
share context between similar labels, both within
and across annotation conventions.
Low-dimensional embedding representations
have been shown to be successful in overcoming
sparsity and representing label similarity across a
wide range of tasks (Weston et al., 2011; Sriku-
mar and Manning, 2014; Hermann et al., 2014;
Lei et al., 2015). In this paper, we present a new
model for SRL that embeds candidate arguments
and semantic roles (in context of a predicate frame)
in a shared vector space. A feed-forward neural
∗Work carried out during an internship at Google.
network is learned to capture correlations of the re-
spective embedding dimensions to create argument
and role representations. The similarity of these
two representations, as measured by their dot prod-
uct, is used to score possible roles for candidate
arguments within a graphical model. This graphical
model jointly models the assignment of semantic
roles to all arguments of a predicate, subject to
structural linguistic constraints.
Our model has several advantages. Compared
to linear multiclass classifiers used in prior work,
vector embeddings of the predictions overcome the
assumption of modeling each semantic role as a
discrete label, thus capturing fine-grained label sim-
ilarity. Moreover, since predictions and inputs are
embedded in the same vector space, and features
extracted from inputs and outputs are decoupled,
our approach is amenable to joint learning of multi-
ple annotation conventions, such as PropBank and
FrameNet, in a single model. Finally, as with other
neural network approaches, our model obviates the
need to manually engineer feature conjunctions.
Our underlying inference algorithm for SRL
follows Täckström et al. (2015), who presented
a dynamic program for structured SRL; it is tar-
geted towards the prediction of full argument spans.
Hence, we present empirical results on three span-
based SRL datasets: CoNLL 2005 and 2012 data
annotated with PropBank conventions, as well as
FrameNet 1.5 data. We also evaluate our system
on the dependency-based CoNLL 2009 shared task
by assuming single word argument spans, that rep-
resent semantic dependencies, and limit our ex-
periments to English. On all datasets, our model
performs on par with a strong linear model base-
line that uses hand-engineered conjunctive features.
Due to random parameter initialization and stochas-
ticity in the online learning algorithm used to train
our models, we observed considerable variance in
performance across datasets. To resolve this vari-
ance, we adopt a product-of-experts model that
</bodyText>
<page confidence="0.953443">
960
</page>
<note confidence="0.991519">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 960–970,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.993829333333333">
Theft steal.01
steal.V steal.V
Perpetrator Goods A0 A1
Theft lift.02
Perpetrator Goods A0 A1
(a) (b)
John stole my car .
John stole my car .
lift.V
lift.V
Mary lifted a purse .
Mary lifted a purse .
</figure>
<figureCaption confidence="0.9471675">
Figure 1: FrameNet (a) and PropBank (b) annota-
tions for two sentences.
</figureCaption>
<bodyText confidence="0.995322625">
combines multiple randomly-initialized instances
of our model to achieve state-of-the-art results on
the CoNLL 2009 and FrameNet datasets, while
coming close to the previous best published results
on the other two. Finally, we present even stronger
results for FrameNet data (which is scarce) by
jointly training the model with PropBank-annotated
data.
</bodyText>
<sectionHeader confidence="0.9971" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9995015">
In this section, we briefly describe the SRL task
and discuss relevant prior work.
</bodyText>
<subsectionHeader confidence="0.980376">
2.1 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.999955636363636">
SRL annotations rely on a frame lexicon containing
frames that could be evoked by one or more lexical
units. A lexical unit consists of a word lemma con-
joined with its coarse-grained part-of-speech tag.1
Each frame is further associated with a set of pos-
sible core and non-core semantic roles which are
used to label its arguments. This description of a
frame lexicon covers both PropBank and FrameNet
conventions, but there are some differences out-
lined below. See Figure 1 for example annotations.
PropBank defines frames that are essentially
sense distinctions of a given lexical unit. The set of
PropBank roles consists of seven generic core roles
(labeled A0-A5 and AA) that assume different se-
mantics for different frames, each associating with
a subset of the core roles. In addition, there are 21
non-core roles that encapsulate further arguments
of a frame, such as temporal (AM-TMP) and locative
(AM-LOC) adjuncts. The non-core roles are shared
between all frames and assume similar meaning.
In contrast, a FrameNet frame often associates
with multiple lexical units and the frame lexicon
</bodyText>
<footnote confidence="0.8996616">
1We borrow the term “lexical unit” from the frame seman-
tics literature. The CoNLL 2005 dataset is restricted to verbal
lexical units, while the CoNLL 2009 and 2012 datasets con-
tains both verbal and nominal lexical units. FrameNet has
lexical units of several coarse syntactic categories.
</footnote>
<bodyText confidence="0.999675642857143">
contains several hundred core and non-core roles
that are shared across frames. For example, the
FrameNet frame Theft could be evoked by the verbs
steal, pickpocket, or lift, while PropBank has dis-
tinct frames for each of them. The Theft frame also
contains the core roles Goods and Perpetrator that
additionally belong to the Commercial_transaction
and Committing_crime frames respectively.
A typical SRL dataset consists of sentence-level
annotations that identify (possibly multiple) target
predicates in each sentence, a disambiguated frame
for each predicate, and the associated argument
spans (or single word argument heads) labeled with
their respective semantic roles.
</bodyText>
<subsectionHeader confidence="0.551733">
2.2 Related Work
</subsectionHeader>
<bodyText confidence="0.999888060606061">
SRL using PropBank conventions (Palmer et al.,
2005) has been widely studied. There have been
two shared tasks at CoNLL 2004-2005 to identify
the phrasal arguments of verbal predicates (Car-
reras and Màrquez, 2004; Carreras and Màrquez,
2005). The CoNLL 2008-2009 shared tasks in-
troduced a variant where semantic dependencies
are annotated rather than phrasal arguments (Sur-
deanu et al., 2008; Hajiˇc et al., 2009). Similar
approaches (Das et al., 2014; Hermann et al., 2014)
have been applied to frame-semantic parsing us-
ing FrameNet conventions (Baker et al., 1998). We
treat PropBank and FrameNet annotations in a com-
mon framework, similar to Hermann et al. (2014).
Most prior work on SRL rely on syntactic parses
provided as input and use locally estimated classi-
fiers for each span-role pair that are only combined
at prediction time.2 This is done by picking the
highest scoring role for each span, subject to a set
of structural constraints, such as avoiding overlap-
ping arguments and repeated core roles. Typically,
these constraints have been enforced by integer lin-
ear programming (ILP), as in Punyakanok et al.
(2008). Täckstr6m et al. (2015) interpreted this
as a graphical model with local factors for each
span-role pair, and global factors that encode the
structural constraints. They derived a dynamic pro-
gram (DP) that enforces most of the constraints
proposed by Punyakanok et al. and showed how
the DP can be used to take these constraints into
account during learning. Here, we use an identical
graphical model, but extend the model of Täck-
str6m et al. by replacing its linear potential func-
</bodyText>
<footnote confidence="0.993604">
2Some recent work have successfully proposed joint mod-
els for syntactic parsing and SRL instead of a pipeline ap-
proach (Lewis et al., 2015).
</footnote>
<page confidence="0.997912">
961
</page>
<bodyText confidence="0.9997554">
tions with a multi-layer neural network. A similar
use of non-linear potential functions in a structured
model was proposed by Do and Artières (2010)
for speech recognition, and by Durrett and Klein
(2015) for syntactic phrase-structure parsing.
Feature-based approaches to SRL employ hand-
engineered linguistically-motivated feature tem-
plates to represent the semantic structure. Some
recent work has focused on low-dimensional repre-
sentations that reduce the need for intensive feature
engineering and lead to better generalization in
the face of data sparsity. Lei et al. (2015) employ
low-rank tensor factorization to induce a compact
representation of the full cross-product of atomic
features; akin to this work, they represent seman-
tic roles as real-valued vectors, but use a different
scoring formulation for modeling potential argu-
ments. Moreover, they restrict their experiments
to CoNLL 2009 semantic dependencies. Roth and
Woodsend (2014) improve on the state-of-the-art
feature-based system of Björkelund et al. (2010) by
adding distributional word representations trained
on large unlabeled corpora as features.
Collobert and Weston (2007) use a neural net-
work and do not rely on syntactic parses as input.
While they use non-standard evaluation, they report
accuracy similar to the ASSERT system (Pradhan
et al., 2005), to which we compare in Table 4. Very
recently, Zhou and Xu (2015) proposed a deep bidi-
rectional LSTM model for SRL that does not rely
on syntax trees as input; their approach achieves
the best results on CoNLL 2005 and 2012 corpora
to date, but unlike this work, they do not report re-
sults on FrameNet and CoNLL 2009 dependencies
and do not investigate joint learning approaches
involving multiple annotation conventions.
For FrameNet-style SRL, Kshirsagar et al.
(2015) recently proposed the use of PropBank-
based features, but their system performance falls
short of the state of the art. Roth and Lapata (2015)
proposed another approach exploring linguistically
motivated features tuned towards the FrameNet lex-
icon, but their performance metrics are significantly
worse than our best results.
The inspiration behind our approach stems from
recent work on bilinear models (Mnih and Hin-
ton, 2007). There have been several recent studies
representing input observations and output labels
with distributed representations, for example, in the
WSABIE model for image annotation (Weston et
al., 2011), in models for embedding labels in struc-
tured graphical models (Srikumar and Manning,
2014), and in techniques to learn joint embeddings
of predicate words and their semantic frames in a
vector space (Hermann et al., 2014).
</bodyText>
<sectionHeader confidence="0.991984" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.99999032">
Our model for SRL performs inference separately
for each marked predicate in a sentence. It assumes
that the predicate has been automatically disam-
biguated to a semantic frame drawn from a frame
lexicon, and the semantic roles of the frame are
used for labeling the candidate arguments in the
sentence. Formally, we are given a sentence x in
which a predicate t, with lexical unit E, has been
marked. Assuming that the semantic frame f of the
predicate has already been identified (see §4.2 for
this step), we seek to predict the set of spans that
correspond to its overt semantic arguments and to
label each argument with its semantic role. Specif-
ically, we model the problem as that of assigning
each span s E S, from an over-generated set of can-
didate argument spans S, to a semantic role r E R.
The set of semantic roles R includes the special
null role ∅, which is used to represent non-overt
arguments. Thus, our algorithm performs the SRL
task in one step for a single predicate frame. For
the span-based SRL task, in a sentence of n words,
there could be O(n2) potential arguments. For sta-
tistical and computational reasons we prune the set
of spans S using a set of syntactically-informed
heuristics from prior work (see §4.2).
</bodyText>
<subsectionHeader confidence="0.974976">
3.1 Graphical Model
</subsectionHeader>
<bodyText confidence="0.999996944444445">
We make use of a graphical model that represents
global assignment of arguments to their semantic
roles, subject to linguistic constraints (Punyakanok
et al., 2008; Täckström et al., 2015). Under this
graphical model, we assume a parameterized po-
tential function that assigns a real-valued com-
patibility score g(s, r; θ) to each span-role pair
(s, r) E S x R, where θ denotes the model param-
eters. Below, we consider two types of potential
functions. As a baseline, similar to most prior work,
one could use a simple linear function of discrete
input features gL(s, r; θ) = θT · φ(r, s, x, t, E, f),
where φ(·) denotes a feature function. In this work,
we instead propose a multi-layer feed-forward neu-
ral network potential function, specified in §3.2.
Given these local factors, we employ the dynamic
program of Täckström et al. to enforce global con-
straints on the inferred output.
</bodyText>
<page confidence="0.970687">
962
</page>
<figure confidence="0.993517538461538">
gNN(s, r; ✓)
of
e8
e,.
VS v(f,r)
dot
product
hidden
layer
embedding
layer
input
layer
</figure>
<listItem confidence="0.996617714285714">
• first word of s • tag of the first word of s
• last word of s • tag of the last word of s
• head word of s • tag of the head word of s
• bag of words in s • bag of tags in s
• cluster of s’s head • linear distance of s from t
• t’s children words • word cluster of s’s head
• dependency path between s’s head and t
• subcategorization frame of s
• position of s w.r.t. t (before, after, overlap or same)
• predicate use voice (active, passive, or unknown)
• whether the subject of t is missing (missingsubj)
• position of s w.r.t. t (before, after, overlap or same)
• word, tag, dependency label and cluster of the words
immediately to the left and right of s
</listItem>
<figureCaption confidence="0.98986">
(s, x, t, f) if it Table 1: Span features 0(s, x, t, `) in Figure 2.
Figure 2: Neural network architecture.
</figureCaption>
<bodyText confidence="0.999897833333333">
Let R|S |denote the set of all possible assign-
ments of semantic roles to argument spans (si, ri)
for si E S that satisfy the constraints. Given a
potential function g(s, r) °= g(s, r; 9), the proba-
bility of a joint assignment r E R|S|, subject to the
constraints, is given by
</bodyText>
<equation confidence="0.973900666666667">
p(r  |x, t, `, f) = exp X g(si, ri) − A(S)
(siES ) ,
(1)
</equation>
<bodyText confidence="0.999835">
where the log-partition function A(S) sums over
all satisfying joint role assignments:
</bodyText>
<equation confidence="0.553787333333333">
X exp X g(si, rZ)⎠. (2)
A(S) =log (siES
r&apos;ETZIsI
</equation>
<subsectionHeader confidence="0.993995">
3.2 Neural Network Potentials
</subsectionHeader>
<bodyText confidence="0.99695608">
Our approach replaces the standard linear poten-
tial function gL(s, r; 9) with the real-valued output
of a feed forward neural network with non-linear
hidden units. The network structure is outlined in
Figure 2. The frame f and role r are initially en-
coded using a one-hot encoding as if and ir. In
other words, if and ir have all zeros except for one
position at f and r respectively. These are passed
through fully connected linear layers to give ef
and er. We call these linear layers the embedding
layers since if selects the embedding of the frame
f and ir for r. Next, ef and er are passed through
a fully connected rectified linear layer (Nair and
Hinton, 2010), to obtain the final frame-role repre-
sentation v(f,r). For the candidate span, the process
is similar. Atomic features 0(s, x, t, `) for the ar-
gument span s are extracted first. (These features
are the non-conjoined features used in the linear
model of Täckström et al.; see Table 1 for the list).
These are next passed through a fully-connected
linear embedding layer to get the span embedding
es, which is subsequently passed through a fully
connected rectified linear layer to obtain vs, the
final span representation. The final output is the
dot product of vs and v(f,r):
</bodyText>
<equation confidence="0.98959">
gNN(s, r; 9) = vs · v(f,r) . (3)
</equation>
<bodyText confidence="0.999987705882353">
The weights of all the layers constitute the param-
eters 9 of the neural network. We initialize 9 ran-
domly, with the exception of embedding parame-
ters corresponding to words, which are initialized
from pre-trained word embeddings (see §4.4 for
details). We train the network as described in §3.3.3
Note that unlike typical linear models, the atomic
span features are not explicitly conjoined with each
other, the frame or the role. Instead the hidden
layers learn to emulate span feature conjunctions
and frame and role feature conjunctions in paral-
lel.4 Moreover, note that span vs and frame-role
v(f,r) representations are decoupled in this model.
This decoupling is important as it allows us to train
a single model in a multitask setting. We demon-
strate this by successfully combining PropBank
and FrameNet training data, as described in §5.
</bodyText>
<subsectionHeader confidence="0.997717">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.759444714285714">
We consider two methods for parameter estimation.
3Various other network structures are worth investigating,
such as concatenating the span, frame and role representa-
tions and passing them through fully connected layers. This
treatment, for example, has been used by Chen and Manning
(2014) for syntactic parsing. We leave these explorations to
future work.
</bodyText>
<footnote confidence="0.9065685">
4We found that adding feature conjunctions to the net-
work’s input layer did not improve performance in practice.
</footnote>
<page confidence="0.997016">
963
</page>
<bodyText confidence="0.998255125">
Local Estimation In local estimation, we treat
each span-role assignment pair (s, r) E SxR as an
individual binary decision problem and maximize
the corresponding log-likelihood of the training
set.5 Denote by zs,r E 10, 11 the decision vari-
able, such that zs,r = 1 iff span s is assigned role
r. By passing the potential gNN(s, r; θ) through
the logistic function, we obtain the log-likelihood
</bodyText>
<equation confidence="0.918283">
l(zs,r; θ) °= log p(zs,r  |x, t, `, f) of an individual
training example. Here,
1 if z = 1
1+e−9NN(s,r;θ) s,r
e−9NN(s,r;θ) if z = 0
1+e−9NN(s,r;θ) s,r
</equation>
<bodyText confidence="0.983421210526316">
Thus, the gold role for a given span according to
the training data serves as the positive example,
while all the other potential roles serve as negatives.
To maximize the log-likelihood, we use Adagrad
(Duchi et al., 2011). This requires the gradient of
the log-likelihood with respect to the parameters θ,
which can be derived using the chain rule.
Structured Estimation In structured estimation,
we instead learn a globally normalized probabilis-
tic model that takes the structural constraints into
account during training. This method is closely
related to the linear approach of Täckström et al.
(2015), as well as to the fine-tuning of a neural
CRF described by Do and Artières (2010).
We train the model by maximizing the log-
likelihood of the training data, again using Adagrad.
From Equation (1), we have that the log-likelihood
l(r; θ) °= log p(r  |x, t, `, f) of a single (struc-
tured) training example (r, S, x) is given by
</bodyText>
<equation confidence="0.837358">
l(r; θ) = � g(si, ri) − A(S) . (4)
si∈S
By application of the chain rule, the gradient of the
log-likelihood factorizes as
∂gNN
∂θ , (5)
</equation>
<bodyText confidence="0.999945">
where we have used the shorthand gNN for brevity.
It is easy to show that the first term ∂l(r; θ)/∂gNN
factors into the marginals over edges in the DP
lattice, which can be computed with the forward-
backward algorithm (recall that the potentials are in
</bodyText>
<footnote confidence="0.7122288">
5An alternate way to locally train the neural network would
be to treat the scores as potentials in a multiclass logistic
regression model and optimize log-likelihood, as is done with
the locally-trained linear model from Täckström et al. (2015),
but we did not investigate this alternative in this work.
</footnote>
<bodyText confidence="0.999858">
simple correspondence with the edge scores in the
DP lattice, see Täckström et al. (2015, §4) for de-
tails). Again, the chain rule can be used to compute
the gradient ∂gNN/∂θ with respect to the parame-
ters of each layer in the network.
</bodyText>
<subsectionHeader confidence="0.996572">
3.4 Product of Experts
</subsectionHeader>
<bodyText confidence="0.9997499">
As we will observe in Tables 2 to 5, random initial-
ization of the neural network parameters θ causes
variance in the performance over different runs.
We found that using a straightforward product-of-
experts (PoE) model (Hinton, 2002) at inference
time reduces this variance and results in signifi-
cantly higher performance. This PoE model is a
very simple ensemble, being the factor-wise sum
of the potential functions from K independently
trained neural networks:
</bodyText>
<equation confidence="0.83713225">
gNN(s, r, θ) . (6)
(j)
where g(j)
NN(s, r, θ) is the score from model j.
</equation>
<sectionHeader confidence="0.999417" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999875">
In this section we describe the datasets used, the re-
quired preprocessing steps, the baselines compared
to and the details of our experimental setup.
</bodyText>
<subsectionHeader confidence="0.988745">
4.1 Datasets and Significance Testing
</subsectionHeader>
<bodyText confidence="0.999950045454545">
We evaluate our approach on four standard datasets.
For span-based SRL using PropBank conventions
(Palmer et al., 2005), we evaluate on both the
CoNLL 2005 shared task dataset (Carreras and
Màrquez, 2005), and the larger CoNLL 2012
dataset derived from the OntoNotes 5.0 corpus
(Weischedel et al., 2011). We also evaluate our
model on the CoNLL 2009 shared task dataset (Ha-
jiˇc et al., 2009), that annotates roles for semantic
dependencies, rather than full argument spans. For
the CoNLL datasets, we use the standard training,
development and test sets. For frame-semantic
parsing using FrameNet conventions (Baker et al.,
1998), we follow Das et al. (2014) and Hermann et
al. (2014) in using the full-text annotations of the
FrameNet 1.5 release and follow their data splits.
We use the standard evaluation scripts for each
task and use a paired bootstrap test (Efron and Tib-
shirani, 1994) to assess the statistical significance
of the results. For brevity, we only give the p-values
for the observed differences between our best and
second best models on each of the test sets.
</bodyText>
<equation confidence="0.997585222222222">
p(zs,r  |x, t, `, f) =
⎧
⎨
⎩
∂l(r; θ) ∂l(r; θ)
∂θ = ∂gNN
K
gPoE(s, r; θ) =
j=1
</equation>
<page confidence="0.986343">
964
</page>
<subsectionHeader confidence="0.977429">
4.2 Preprocessing and Frame Identification
</subsectionHeader>
<bodyText confidence="0.999990951219512">
All datasets are preprocessed with a part-of-speech
tagger and a syntactic dependency parser, both
trained on the CoNLL 2012 training split, after
converting the constituency trees to Stanford-style
dependencies (De Marneffe and Manning, 2013).
The tagger is based on a second-order conditional
random field (Lafferty et al., 2001) with standard
emission and transition features; for parsing, we
use a graph-based parser with structural diversity
and cube-pruning (Zhang and McDonald, 2014).
On the WSJ development set (section 22), the la-
beled attachment score of the parser is 90.9% while
the part-of-speech tagger achieves an accuracy of
97.2%. On the CoNLL 2012 development set, the
corresponding scores are 90.2% and 97.3%. Both
the tagger and the parser, as well as the SRL mod-
els use word cluster features (see Table 1). Specif-
ically, we use the clusters with 1000 classes from
Turian et al. (2010), which are induced with the
Brown algorithm (Brown et al., 1992). To gener-
ate the candidate arguments S (see §3.2) for the
CoNLL 2005 and 2012 span-based datasets, we
follow Täckström et al. (2015) and adapt the algo-
rithm of Xue and Palmer (2004) to use dependency
syntax. For the dependency-based CoNLL 2009
experiments, we modify our approach to assume
single length spans and treat every word of the sen-
tence as a candidate argument. For FrameNet, we
follow the heuristic of Hermann et al. (2014).
As mentioned in §3, we automatically disam-
biguate the predicate frames. For FrameNet, we
use an embedding-based model described by Her-
mann et al. (2014). For PropBank, we use a multi-
class log-linear model, since Hermann et al. did not
observe better results with the embedding model.
To ensure a fair comparison with the closest lin-
ear model baseline, we ensured that the prepro-
cessing steps, the argument candidate generation
algorithm for the span-based datasets and the frame
identification methods are identical to Täckström
et al. (2015, §3.2, §6.2-§6.3).
</bodyText>
<subsectionHeader confidence="0.998375">
4.3 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.989375516129032">
In addition to comparing to Täckström et al. (2015),
whose setup is closest to ours, we also compare to
prior state-of-the-art systems from the literature.
For CoNLL 2005, we compare to the best non-
ensemble and ensemble systems of Surdeanu et al.
(2007), Punyakanok et al. (2008) and Toutanova et
al. (2008). The ensemble variants of these systems
use multiple parses and multiple SRL systems to
leverage diversity. In contrast to these ensemble
systems, our product-of-experts model uses only a
single architecture and one syntactic parse; the con-
stituent models differ only in random initialization.
We also compare with the recent deep bidirectional
LSTM model of Zhou and Xu (2015).
For CoNLL 2012, we compare to Pradhan et al.
(2013), who report results with the (non-ensemble)
ASSERT system (Pradhan et al., 2005), and to the
model of Zhou and Xu (2015).
For CoNLL 2009, we compare to the top
system from the shared task (Zhao et al.,
2009), two state-of-the-art systems that employ a
reranker (Björkelund et al., 2010; Roth and Wood-
send, 2014), and the recent tensor-based model
of Lei et al. (2015). We also trained the linear
model of Täckström et al. on this dataset (their
work omitted this experiment), as a baseline.
Finally, for the FrameNet experiments, we com-
pare to the state-of-the-art system of Hermann et
al. (2014), which combines a frame-identification
model based on WSABIE (Weston et al., 2011) with
a log-linear role labeling model.
</bodyText>
<subsectionHeader confidence="0.997871">
4.4 Hyperparameters and Initialization
</subsectionHeader>
<bodyText confidence="0.9999934">
There are several hyperparameters in our model
(§3.2). First, the span embedding dimension of es
was fixed to 300 to match the dimension of the pre-
trained GloVe word embeddings from Pennington
et al. (2014) that we use to initialize the embed-
dings of the word-based features in φ(s, x, t, E).
Preliminary experiments showed random initial-
ization of the word-based embeddings to be in-
ferior to pre-trained embeddings. The remain-
ing model parameters were randomly initialized.
The frame embedding dimension was chosen from
1100, 200, 300, 5001, while the hidden layer di-
mension was chosen from 1300, 5001. For Prop-
Bank, we fixed the role embedding dimension
to 27, which is the number of semantic roles in
PropBank datasets (ignoring the AA role, that ap-
pears with negligible frequency). For FrameNet,
the role embedding dimension was chosen from
1100, 200, 300, 500}. In the Adagrad algorithm,
the mini-batch size was fixed to 100 for local esti-
mation (§3.3). For structured estimation (§3.3), a
batch size of one was used, since each structured in-
stance contains multiple local factors. The learning
rate was chosen from 10.1, 0.2,0.5, 1.01 for local
learning and from 10.01, 0.02, 0.05, 0.11 for struc-
</bodyText>
<page confidence="0.996763">
965
</page>
<table confidence="0.9978692">
Method WSJ Dev WSJ Test Brown Test
P R F1 Comp. P R F1 Comp. P R F1 Comp.
Surdeanu (Single) – – – – 79.7 74.9 77.2 52.0 – – – –
Surdeanu (Ensemble) – – – – 87.5 74.7 80.6 51.7 81.8 61.3 70.1 34.3
Toutanova (Single) – – 77.9 57.2 – – 79.7 58.7 – – 67.8 39.4
Toutanova (Ensemble) – – 78.6 58.7 81.9 78.8 80.3 60.1 – – 68.8 40.8
Punyakanok (Single) – – – – 77.1 75.5 76.3 – – – – –
Punyakanok (Ensemble) 80.1 74.8 77.4 50.7 82.3 76.8 79.4 53.8 73.4 62.9 67.8 32.3
Täckström (Local) 81.3 74.8 77.9 52.4 82.6 76.4 79.3 54.3 74.0 66.8 70.2 38.4
Täckström (Struct.) 81.2 76.2 78.6 54.4 82.3 77.6 79.9 56.0 74.3 68.6 71.3 39.8
Zhou 79.7 79.4 79.6 – 82.9 82.8 82.8 – 70.7 68.2 69.4 –
This work (Local) 81.4 75.6 78.4 53.7 82.3±0.4 76.8±0.5 79.4±0.1 55.1±0.6 74.1±0.6 68.0±0.7 70.9±0.3 39.1±0.8
This work (Struct.) 80.7 76.1 78.3 54.1 81.8±0.5 77.3±0.3 79.4±0.2 55.6±0.5 73.8±0.7 68.8±0.6 71.2±0.3 40.5±0.8
This work (Local, PoE) 82.0 76.6 79.2 55.2 82.9 77.8 80.3* 56.7 75.2 69.1 72.0 40.8
This work (Struct., PoE) 81.2 76.7 78.9 55.1 82.5 78.2 80.3* 57.3* 74.5 70.0 72.2** 41.3
</table>
<tableCaption confidence="0.7724414">
Table 2: PropBank-style SRL results on CoNLL 2005 data. Bold font indicates the best system using a
single or no syntactic parse, while the best scores among all systems are underlined. Results from prior
work are taken from the respective papers, and ‘–’ indicates performance metrics missing in the original
publication. Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with
p &lt; 0.01 (*) and p &lt; 0.05 (**).
</tableCaption>
<table confidence="0.999826916666667">
Excluding predicate senses Including predicate senses
WSJ Dev WSJ Test Brown Test WSJ Test Brown Test
CoNLL-2009 1st place – 82.1 69.8 86.2 74.6
Björkelund et al., 2010 + reranking 80.5 82.9 70.9 86.9 75.7
Roth and Woodsend, 2014 + reranking – 82.1 71.1 86.3 75.9
Lei et al. 2015 81.0 82.5 70.8 86.6 75.6
Täckström et al. 2015 (Local) 81.4 83.0 71.2 86.9 74.8
Täckström et al. 2015 (Struct.) 82.4 83.7 72.3 87.3 75.5
This work (Local) 81.2±0.2 82.7±0.3 71.9±0.4 86.7±0.2 75.2±0.3
This work (Struct) 82.3±0.1 83.6±0.1 71.9±0.3 87.3±0.1 75.2±0.2
This work (Local, PoE) 82.4 83.8 72.8 87.5 75.9
This work (Struct., PoE) 83.0* 84.3* 72.4 87.8* 75.5
</table>
<tableCaption confidence="0.8865805">
Table 3: PropBank-style semantic dependency SRL results (labeled F1) on the CoNLL 2009 data set.
Bold font indicates the best system. Statistical significance was assessed with p &lt; 0.01 (*).
</tableCaption>
<bodyText confidence="0.999236166666667">
tured learning.6 All hyperparameters were tuned
on the respective development sets for each dataset
with a straightforward grid-search procedure. In
the product-of-experts setup, we train K = 10
models, each with a different random seed, and
combine them at inference time (see Equation (6)).
</bodyText>
<sectionHeader confidence="0.977081" genericHeader="method">
5 Empirical Results
</sectionHeader>
<bodyText confidence="0.9998595">
Table 2 shows results on the CoNLL 2005 devel-
opment set and the WSJ and Brown test sets. Our
individual neural network models are on par with
the best linear single-system baselines that use care-
fully chosen feature combinations, but has variance
across reruns. On the WSJ test set, the product-
</bodyText>
<footnote confidence="0.9989145">
6We observed a strong interaction between learning rate
and mini-batch size. Since the number of factors per frame
structure is much larger than 100, lower learning rates are
better suited for structured estimation.
</footnote>
<bodyText confidence="0.999601705882353">
of-experts model featuring neural networks trained
with structured learning achieves higher F,-score
than all non-ensemble baselines, except the LSTM
model of Zhou and Xu. It is on par and at times
better than ensemble baselines that use diverse syn-
tactic parses. The PoE model outperforms all base-
lines on the Brown test set, exhibiting its gener-
alization power on out-of-domain text. Overall,
using structured learning improves recall at a slight
expense of precision when compared to local learn-
ing, leading to an increase in the complete argu-
ment structure accuracy (Comp. in the tables).
Table 3 shows results on the CoNLL 2009 task.
Following Lei et al. (2015), we present results us-
ing the official evaluation script, along with addi-
tional metrics that do not count frame predictions.
Note that the linear baseline of Täckström et al.
</bodyText>
<page confidence="0.995311">
966
</page>
<table confidence="0.9999595">
Method CoNLL 2012 Development
P R F1 Comp.
Täckstr6m (Local) 80.6 77.1 78.8 59.0
Täckstr6m (Struct.) 80.5 77.8 79.1 60.1
Zhou – – 81.1 –
This work (Local) 80.4 77.3 78.8 59.0
This work (Struct) 80.6 77.8 79.2 59.8
This work (Local, PoE) 81.0 78.3 79.6 60.6
This work (Struct., PoE) 81.0 78.5 79.7 60.9
CoNLL 2012 Test
Method P R F1 Comp.
Pradhan 81.3 70.5 75.5 51.7
Pradhan, revised 78.5 76.6 77.5 55.8
Täckstr6m (Local) 80.9 77.7 79.2 60.9
Täckstr6m (Struct.) 80.6 78.2 79.4 61.8
Zhou – – 81.3 –
This work (Local) 80.6±0.3 77.8±0.2 79.2±0.1 60.8±0.3
This work (Struct.) 80.9±0.2 78.4±0.2 79.6±0.1 61.7±0.2
This work (Local, PoE) 81.3 78.8 80.0 62.4
This work (Struct., PoE) 81.2 79.0 80.1* 62.6*
</table>
<tableCaption confidence="0.992449">
Table 4: PropBank-style SRL results on the
</tableCaption>
<bodyText confidence="0.9870032">
CoNLL 2012 development and test sets. Results
from prior work are taken from the respective pa-
pers, and ‘–’ indicates performance metrics miss-
ing in the original publication. Significance was
assessed for F1 and Comp. on the test set with
p &lt; 0.01 (*).
outperforms most prior work, including ones that
employs rerankers, except on the Brown test set.
Our neural network model performs even better,
achieving state-of-the-art results on all metrics.
Table 4 shows the results on the span-based
CoNLL 2012 data. The trends observed on the
CoNLL 2005 data hold here as well, with struc-
tured training yielding an increase in precision at
the cost of a small drop in recall. This leads to im-
provements in both F1 score and complete structure
accuracy. The product-of-experts model trained
with structured learning here yields results better
than the ASSERT system (Pradhan et al., 2013),
but akin to CoNLL 2005, our system falls short in
comparison to Zhou and Xu’s F1-score. In contrast
to the smaller CoNLL 2005 data, even our sin-
gle (non-PoE) model outperforms the linear model
of Täckstr6m et al. (2015) on the CoNLL 2012
data. We hypothesize that the relative abundance
of the latter counteracts the risk for overfitting of
the larger number of parameters in our model.
Finally, Table 5 shows the results on FrameNet
data, which is very small in size. Here, structured
learning does not help and in fact leads to a small
</bodyText>
<table confidence="0.998914681818182">
FrameNet Development
Method P R F1 Comp.
Hermann 78.3 64.5 70.8 –
Täckstr6m (Local) 80.7 62.9 70.7 31.2
Täckstr6m (Struct.) 79.6 64.1 71.0 33.3
This work (Local) 78.6 64.6 70.9 32.0
This work (Struct.) 79.6 63.9 70.9 31.8
This work (Local, PoE) 79.0 65.0 71.3 33.1
This work (Struct., PoE) 79.0 64.4 71.0 32.3
This work (Local, PoE, Joint) 79.4 65.8 72.0 34.5
This work (Struct., PoE, Joint) 78.8 65.4 71.5 33.5
FrameNet Test
Method P R F1 Comp.
Hermann 74.3 66.0 69.9 –
Täckstr6m (Local) 76.1 64.9 70.1 33.0
Täckstr6m (Struct.) 75.4 65.8 70.3 33.8
This work (Local) 73.9 ±0.6 66.4 ±0.4 69.9 ±0.3 33.4±0.6
This work (Struct.) 74.8 ±0.2 65.5 ±0.2 69.9 ±0.1 33.0±0.3
This work (Local, PoE) 74.3 66.9 70.4 33.9
This work (Struct., PoE) 74.6 66.3 70.2 33.3
This work (Local, PoE, Joint) 75.0 67.3 70.9 ** 35.4*
This work (Struct., PoE, Joint) 74.2 67.2 70.5 34.2
</table>
<tableCaption confidence="0.908091">
Table 5: Joint frame and argument identification
</tableCaption>
<bodyText confidence="0.97272532">
results for FrameNet. Statistical significance was
assessed for F1 and Comp. on the test set with
p &lt; 0.01 (*) and p &lt; 0.05 (**).
drop in performance. Our locally-trained neural
network model performs comparably to the linear
model of Täckstr6m et al. (2015). However we
achieve significant improvements in both F1-score
and full structure accuracy by training our model
with a dataset composed of both FrameNet and
CoNLL 2005 data.7 The ability to train in this mul-
titask setting is a unique capability of our approach,
and yields state-of-the-art results for FrameNet.
Figure 4 shows the effect of adding increasing
amount of CoNLL 2005 data to supplement the
FrameNet training corpus in this multitask setting.
The Y-axis plots F1-score on the development data
averaged across runs for the local non-PoE model.
With increasing amount of PropBank data, perfor-
mance increases in small steps, and peaks when
all the data is added. This shows that with more
PropBank data we could further improve perfor-
mance on the FrameNet task; we leave further ex-
ploration of multitask learning of predicate argu-
ment structures, including multilingual settings, to
future work.
</bodyText>
<footnote confidence="0.99646225">
7The joint model does not improve results for PropBank.
This is likely due to the much larger CoNLL 2005 training set,
compared to the FrameNet data (39,832 training sentences in
the former as opposed to 3,256 sentences in the latter).
</footnote>
<page confidence="0.987961">
967
</page>
<figure confidence="0.99898868">
lower.01
compose.02
Cause_change_position_on_a_scale
stash.02
reduce.01
Behind the scenes
verify.01
Verification
Þlm.01
substantiate.01
abandon.01 Abandonment
raise.01
increase.01
grow.01
commute.01::ARG0
tour.01::ARG0
travel.01::ARG0
commute.01::ARG1
Travel::Traveler
tour.01::ARG1
travel.01::ARG1
Travel::Goal
commute.01::ARG2
Travel::Path
Travel::Source
</figure>
<figureCaption confidence="0.971916">
Figure 3: Two-
</figureCaption>
<bodyText confidence="0.629568875">
dimensional t-SNE
projections (Van der
Maaten and Hinton,
2008) of joint Prop-
Bank and FrameNet
(boxed) embeddings
of frames (a) and
frame-role pairs (b).
</bodyText>
<figure confidence="0.992867">
(a)
</figure>
<figureCaption confidence="0.814535">
Figure 4: F1 score on the FrameNet development
data averaged over runs versus the percentage of
CoNLL 2005 data used to append the FrameNet
training corpus. For this plot, we used the locally
trained non-PoE model.
</figureCaption>
<subsectionHeader confidence="0.986971">
5.1 Qualitative Analysis of Embeddings
</subsectionHeader>
<bodyText confidence="0.99985125">
Figure 3 shows example embeddings from the
model trained jointly on FrameNet and PropBank
annotations. Figure 3a shows the proximity of
the learned embeddings ef of frames from both
FrameNet and PropBank. Figure 3b shows the em-
beddings for frame-role pairs v(f,r) (the output of
the hidden rectified linear layer). Here, we fix the
FrameNet frame Travel and the similar PropBank
frames commute.01, tour.01 and travel.01 are visual-
ized along with their semantic roles. We observe
that the model learns very similar embeddings for
the semantically related roles across both datasets.
Note that there is a clear separation of the agentive
roles from the others for both conventions and how
the FrameNet and PropBank counterparts of each
type of role are proximate in vector space.
</bodyText>
<figure confidence="0.681193">
(b)
</figure>
<sectionHeader confidence="0.975162" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999116">
We presented a neural network model for seman-
tic role labeling that learns to embed both inputs
and outputs in the same vector space. We consid-
ered both local and structured training methods for
the network parameters from supervised SRL data.
Empirically, our approach achieves state-of-the-art
results on two standard datasets with a product of
experts model, while approaching the performance
of a recent deep recurrent neural network model on
two other datasets. By training the model jointly
on both FrameNet and PropBank data, we achieve
the best result to date on the FrameNet test set. Fi-
nally, qualitative analysis indicates that the model
represents semantically similar annotations with
proximate vector-space embeddings.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999970833333333">
We thank Tom Kwiatkowski, Slav Petrov and Fer-
nando Pereira for comments on early drafts. We
are also thankful to Mike Lewis, Mark Yatskar and
Luke Zettlemoyer for valuable feedback. Finally,
we thank the three anonymous reviewers for sug-
gestions that enriched the final version of the paper.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997248181818182">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of ACL.
Anders Björkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syntactic
and semantic dependency parser. In Proceedings of
ICCL: Demonstrations.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Compu-
tational Linguistics, 18(4):467–479.
</reference>
<figure confidence="0.9971237">
71.5 71.5
71.4
71.3 71.3
F1
71.2 71.1
71.0
70.9
70.8
0% 25% 50% 75% 100%
Amount of CoNLL 2005 Data
</figure>
<page confidence="0.974207">
968
</page>
<reference confidence="0.999521213592233">
Xavier Carreras and Lluís Màrquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proceedings of CoNLL.
Xavier Carreras and Lluís Màrquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of CoNLL.
Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In Proceedings of EMNLP.
Ronan Collobert and Jason Weston. 2007. Fast seman-
tic extraction using a novel neural network architec-
ture. In Proceedings of ACL.
Dipanjan Das, Desai Chen, Andre F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguis-
tics, 40(1):9–56.
Marie-Catherine De Marneffe and Christopher D Man-
ning, 2013. Stanford typed dependencies manual.
Trinh-Minh-Tri Do and Thierry Artières. 2010. Neu-
ral conditional random fields. In Proceedings ofAIS-
TATS.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Greg Durrett and Dan Klein. 2015. Neural CRF pars-
ing. In Proceedings ofACL.
Bradley Efron and Robert J Tibshirani. 1994. An intro-
duction to the bootstrap. CRC press.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martí, Lluís
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štˇepánek, et al. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of CoNLL.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Proceedings of ACL.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Computation, 14(8):1771–1800.
Meghana Kshirsagar, Sam Thomson, Nathan Schnei-
der, Jaime Carbonell, Noah A. Smith, and Chris
Dyer. 2015. Frame-semantic role labeling with het-
erogeneous annotations. In Proceedings of ACL.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Tao Lei, Yuan Zhang, Lluís Márquez, Alessandro Mos-
chitti, and Regina Barzilay. 2015. High-order low-
rank tensors for semantic role labeling. In Proceed-
ings of NAACL.
Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015.
Joint A* CCG parsing and semantic role labelling.
In Proceedings of EMNLP.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted Boltzmann machines.
In Proceedings of ICML.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational linguistics,
31(1):71–106.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H Martin, and Daniel Jurafsky.
2005. Support vector learning for semantic argu-
ment classification. Machine Learning, 60(1-3):11–
39.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Björkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards
robust linguistic analysis using OntoNotes. In Pro-
ceedings of CoNLL.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.
Michael Roth and Mirella Lapata. 2015. Efficient in-
ference and structured learning for semantic role la-
beling. Transactions of the Association for Compu-
tational Linguistics, 3:449–460.
Michael Roth and Kristian Woodsend. 2014. Composi-
tion of word representations improves semantic role
labelling. In Proceedings of EMNLP.
Vivek Srikumar and Christopher D Manning. 2014.
Learning distributed representations for structured
output prediction. In Proceedings of NIPS.
Mihai Surdeanu, Lluís Màrquez, Xavier Carreras, and
Pere Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelli-
gence Research, 29:105–151.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluís Màrquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
CoNLL.
</reference>
<page confidence="0.986572">
969
</page>
<reference confidence="0.999780105263158">
Oscar Täckström, Kuzman Ganchev, and Dipanjan Das.
2015. Efficient inference and structured learning for
semantic role labeling. Transactions of the Associa-
tion for Computational Linguistics, 3:29–41.
Kristina Toutanova, Aria Haghighi, and Christopher D
Manning. 2008. A global joint model for se-
mantic role labeling. Computational Linguistics,
34(2):161–191.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(85).
Ralph Weischedel, Eduard Hovy, Martha Palmer,
Mitch Marcus, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. 2011.
OntoNotes: A large training corpus for enhanced
processing. In J. Olive, C. Christianson, and J. Mc-
Cary, editors, Handbook of Natural Language Pro-
cessing and Machine Translation. Springer.
Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. WSABIE: Scaling up to large vocabulary im-
age annotation. In Proceedings of IJCAI.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP.
Hao Zhang and Ryan McDonald. 2014. Enforcing
structural diversity in cube-pruned dependency pars-
ing. In Proceedings of ACL.
Hai Zhao, Wenliang Chen, Chunyu Kity, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of CoNLL.
Jie Zhou and Wei Xu. 2015. End-to-end learning of se-
mantic role labeling using recurrent neural networks.
In Proceedings ofACL.
</reference>
<page confidence="0.99778">
970
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.355214">
<title confidence="0.999538">Semantic Role Labeling with Neural Network Factors</title>
<author confidence="0.762979">Oscar</author>
<affiliation confidence="0.533618">of Computer Science and Engineering, University of New</affiliation>
<email confidence="0.998705">oscart@google.com</email>
<email confidence="0.998705">kuzman@google.com</email>
<email confidence="0.998705">dipanjand@google.com</email>
<abstract confidence="0.997643176470588">We present a new method for semantic role labeling in which arguments and semantic roles are jointly embedded in a shared vector space for a given predicate. These embeddings belong to a neural network, whose output represents the potential functions of a graphical model designed for the SRL task. We consider both local and structured learning methods and obtain strong results on standard PropBank and FrameNet corpora with a straightforward product-of-experts model. We further show how the model can learn jointly from PropBank and FrameNet annotations to obtain additional improvements on the smaller FrameNet dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7517" citStr="Baker et al., 1998" startWordPosition="1156" endWordPosition="1159">th their respective semantic roles. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and repeated core roles. Typically, these constraints have been enforced by integer linear programming (ILP), as in Punyakanok et al. (2008). Täckstr6m et al. (2015) interpreted t</context>
<context position="21294" citStr="Baker et al., 1998" startWordPosition="3493" endWordPosition="3496">e our approach on four standard datasets. For span-based SRL using PropBank conventions (Palmer et al., 2005), we evaluate on both the CoNLL 2005 shared task dataset (Carreras and Màrquez, 2005), and the larger CoNLL 2012 dataset derived from the OntoNotes 5.0 corpus (Weischedel et al., 2011). We also evaluate our model on the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009), that annotates roles for semantic dependencies, rather than full argument spans. For the CoNLL datasets, we use the standard training, development and test sets. For frame-semantic parsing using FrameNet conventions (Baker et al., 1998), we follow Das et al. (2014) and Hermann et al. (2014) in using the full-text annotations of the FrameNet 1.5 release and follow their data splits. We use the standard evaluation scripts for each task and use a paired bootstrap test (Efron and Tibshirani, 1994) to assess the statistical significance of the results. For brevity, we only give the p-values for the observed differences between our best and second best models on each of the test sets. p(zs,r |x, t, `, f) = ⎧ ⎨ ⎩ ∂l(r; θ) ∂l(r; θ) ∂θ = ∂gNN K gPoE(s, r; θ) = j=1 964 4.2 Preprocessing and Frame Identification All datasets are prepro</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Björkelund</author>
<author>Bernd Bohnet</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>A high-performance syntactic and semantic dependency parser.</title>
<date>2010</date>
<booktitle>In Proceedings of ICCL:</booktitle>
<publisher>Demonstrations.</publisher>
<contexts>
<context position="9736" citStr="Björkelund et al. (2010)" startWordPosition="1504" endWordPosition="1507">ed on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on Fra</context>
<context position="24884" citStr="Björkelund et al., 2010" startWordPosition="4089" endWordPosition="4092">age diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model (§3.2). First, the span embedding dimension of es was fixed to 300 to match the </context>
<context position="28247" citStr="Björkelund et al., 2010" startWordPosition="4674" endWordPosition="4677">2** 41.3 Table 2: PropBank-style SRL results on CoNLL 2005 data. Bold font indicates the best system using a single or no syntactic parse, while the best scores among all systems are underlined. Results from prior work are taken from the respective papers, and ‘–’ indicates performance metrics missing in the original publication. Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p &lt; 0.01 (*) and p &lt; 0.05 (**). Excluding predicate senses Including predicate senses WSJ Dev WSJ Test Brown Test WSJ Test Brown Test CoNLL-2009 1st place – 82.1 69.8 86.2 74.6 Björkelund et al., 2010 + reranking 80.5 82.9 70.9 86.9 75.7 Roth and Woodsend, 2014 + reranking – 82.1 71.1 86.3 75.9 Lei et al. 2015 81.0 82.5 70.8 86.6 75.6 Täckström et al. 2015 (Local) 81.4 83.0 71.2 86.9 74.8 Täckström et al. 2015 (Struct.) 82.4 83.7 72.3 87.3 75.5 This work (Local) 81.2±0.2 82.7±0.3 71.9±0.4 86.7±0.2 75.2±0.3 This work (Struct) 82.3±0.1 83.6±0.1 71.9±0.3 87.3±0.1 75.2±0.2 This work (Local, PoE) 82.4 83.8 72.8 87.5 75.9 This work (Struct., PoE) 83.0* 84.3* 72.4 87.8* 75.5 Table 3: PropBank-style semantic dependency SRL results (labeled F1) on the CoNLL 2009 data set. Bold font indicates the be</context>
</contexts>
<marker>Björkelund, Bohnet, Hafdell, Nugues, 2010</marker>
<rawString>Anders Björkelund, Bernd Bohnet, Love Hafdell, and Pierre Nugues. 2010. A high-performance syntactic and semantic dependency parser. In Proceedings of ICCL: Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="22841" citStr="Brown et al., 1992" startWordPosition="3753" endWordPosition="3756">ion and transition features; for parsing, we use a graph-based parser with structural diversity and cube-pruning (Zhang and McDonald, 2014). On the WSJ development set (section 22), the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2%. On the CoNLL 2012 development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the SRL models use word cluster features (see Table 1). Specifically, we use the clusters with 1000 classes from Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). To generate the candidate arguments S (see §3.2) for the CoNLL 2005 and 2012 span-based datasets, we follow Täckström et al. (2015) and adapt the algorithm of Xue and Palmer (2004) to use dependency syntax. For the dependency-based CoNLL 2009 experiments, we modify our approach to assume single length spans and treat every word of the sentence as a candidate argument. For FrameNet, we follow the heuristic of Hermann et al. (2014). As mentioned in §3, we automatically disambiguate the predicate frames. For FrameNet, we use an embedding-based model described by Hermann et al. (2014). For PropB</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluís Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="7163" citStr="Carreras and Màrquez, 2004" startWordPosition="1100" endWordPosition="1104">nd Perpetrator that additionally belong to the Commercial_transaction and Committing_crime frames respectively. A typical SRL dataset consists of sentence-level annotations that identify (possibly multiple) target predicates in each sentence, a disambiguated frame for each predicate, and the associated argument spans (or single word argument heads) labeled with their respective semantic roles. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only co</context>
</contexts>
<marker>Carreras, Màrquez, 2004</marker>
<rawString>Xavier Carreras and Lluís Màrquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluís Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="7192" citStr="Carreras and Màrquez, 2005" startWordPosition="1105" endWordPosition="1108">ally belong to the Commercial_transaction and Committing_crime frames respectively. A typical SRL dataset consists of sentence-level annotations that identify (possibly multiple) target predicates in each sentence, a disambiguated frame for each predicate, and the associated argument spans (or single word argument heads) labeled with their respective semantic roles. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 T</context>
<context position="20869" citStr="Carreras and Màrquez, 2005" startWordPosition="3426" endWordPosition="3429">This PoE model is a very simple ensemble, being the factor-wise sum of the potential functions from K independently trained neural networks: gNN(s, r, θ) . (6) (j) where g(j) NN(s, r, θ) is the score from model j. 4 Experimental Setup In this section we describe the datasets used, the required preprocessing steps, the baselines compared to and the details of our experimental setup. 4.1 Datasets and Significance Testing We evaluate our approach on four standard datasets. For span-based SRL using PropBank conventions (Palmer et al., 2005), we evaluate on both the CoNLL 2005 shared task dataset (Carreras and Màrquez, 2005), and the larger CoNLL 2012 dataset derived from the OntoNotes 5.0 corpus (Weischedel et al., 2011). We also evaluate our model on the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009), that annotates roles for semantic dependencies, rather than full argument spans. For the CoNLL datasets, we use the standard training, development and test sets. For frame-semantic parsing using FrameNet conventions (Baker et al., 1998), we follow Das et al. (2014) and Hermann et al. (2014) in using the full-text annotations of the FrameNet 1.5 release and follow their data splits. We use the standard evalua</context>
</contexts>
<marker>Carreras, Màrquez, 2005</marker>
<rawString>Xavier Carreras and Lluís Màrquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17306" citStr="Chen and Manning (2014)" startWordPosition="2820" endWordPosition="2823">ns in parallel.4 Moreover, note that span vs and frame-role v(f,r) representations are decoupled in this model. This decoupling is important as it allows us to train a single model in a multitask setting. We demonstrate this by successfully combining PropBank and FrameNet training data, as described in §5. 3.3 Parameter Estimation We consider two methods for parameter estimation. 3Various other network structures are worth investigating, such as concatenating the span, frame and role representations and passing them through fully connected layers. This treatment, for example, has been used by Chen and Manning (2014) for syntactic parsing. We leave these explorations to future work. 4We found that adding feature conjunctions to the network’s input layer did not improve performance in practice. 963 Local Estimation In local estimation, we treat each span-role assignment pair (s, r) E SxR as an individual binary decision problem and maximize the corresponding log-likelihood of the training set.5 Denote by zs,r E 10, 11 the decision variable, such that zs,r = 1 iff span s is assigned role r. By passing the potential gNN(s, r; θ) through the logistic function, we obtain the log-likelihood l(zs,r; θ) °= log p(</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>Fast semantic extraction using a novel neural network architecture.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9858" citStr="Collobert and Weston (2007)" startWordPosition="1520" endWordPosition="1523">ization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventio</context>
</contexts>
<marker>Collobert, Weston, 2007</marker>
<rawString>Ronan Collobert and Jason Weston. 2007. Fast semantic extraction using a novel neural network architecture. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Desai Chen</author>
<author>Andre F T Martins</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Frame-semantic parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="7402" citStr="Das et al., 2014" startWordPosition="1138" endWordPosition="1141">mbiguated frame for each predicate, and the associated argument spans (or single word argument heads) labeled with their respective semantic roles. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and repeated core roles. Typically, these constraints have been </context>
<context position="21323" citStr="Das et al. (2014)" startWordPosition="3499" endWordPosition="3502"> datasets. For span-based SRL using PropBank conventions (Palmer et al., 2005), we evaluate on both the CoNLL 2005 shared task dataset (Carreras and Màrquez, 2005), and the larger CoNLL 2012 dataset derived from the OntoNotes 5.0 corpus (Weischedel et al., 2011). We also evaluate our model on the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009), that annotates roles for semantic dependencies, rather than full argument spans. For the CoNLL datasets, we use the standard training, development and test sets. For frame-semantic parsing using FrameNet conventions (Baker et al., 1998), we follow Das et al. (2014) and Hermann et al. (2014) in using the full-text annotations of the FrameNet 1.5 release and follow their data splits. We use the standard evaluation scripts for each task and use a paired bootstrap test (Efron and Tibshirani, 1994) to assess the statistical significance of the results. For brevity, we only give the p-values for the observed differences between our best and second best models on each of the test sets. p(zs,r |x, t, `, f) = ⎧ ⎨ ⎩ ∂l(r; θ) ∂l(r; θ) ∂θ = ∂gNN K gPoE(s, r; θ) = j=1 964 4.2 Preprocessing and Frame Identification All datasets are preprocessed with a part-of-speech </context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>Dipanjan Das, Desai Chen, Andre F. T. Martins, Nathan Schneider, and Noah A. Smith. 2014. Frame-semantic parsing. Computational Linguistics, 40(1):9–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2013</date>
<note>Stanford typed dependencies manual.</note>
<marker>De Marneffe, Manning, 2013</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning, 2013. Stanford typed dependencies manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trinh-Minh-Tri Do</author>
<author>Thierry Artières</author>
</authors>
<title>Neural conditional random fields.</title>
<date>2010</date>
<booktitle>In Proceedings ofAISTATS.</booktitle>
<contexts>
<context position="8855" citStr="Do and Artières (2010)" startWordPosition="1379" endWordPosition="1382">onstraints. They derived a dynamic program (DP) that enforces most of the constraints proposed by Punyakanok et al. and showed how the DP can be used to take these constraints into account during learning. Here, we use an identical graphical model, but extend the model of Täckstr6m et al. by replacing its linear potential func2Some recent work have successfully proposed joint models for syntactic parsing and SRL instead of a pipeline approach (Lewis et al., 2015). 961 tions with a multi-layer neural network. A similar use of non-linear potential functions in a structured model was proposed by Do and Artières (2010) for speech recognition, and by Durrett and Klein (2015) for syntactic phrase-structure parsing. Feature-based approaches to SRL employ handengineered linguistically-motivated feature templates to represent the semantic structure. Some recent work has focused on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles a</context>
<context position="18725" citStr="Do and Artières (2010)" startWordPosition="3057" endWordPosition="3060"> serves as the positive example, while all the other potential roles serve as negatives. To maximize the log-likelihood, we use Adagrad (Duchi et al., 2011). This requires the gradient of the log-likelihood with respect to the parameters θ, which can be derived using the chain rule. Structured Estimation In structured estimation, we instead learn a globally normalized probabilistic model that takes the structural constraints into account during training. This method is closely related to the linear approach of Täckström et al. (2015), as well as to the fine-tuning of a neural CRF described by Do and Artières (2010). We train the model by maximizing the loglikelihood of the training data, again using Adagrad. From Equation (1), we have that the log-likelihood l(r; θ) °= log p(r |x, t, `, f) of a single (structured) training example (r, S, x) is given by l(r; θ) = � g(si, ri) − A(S) . (4) si∈S By application of the chain rule, the gradient of the log-likelihood factorizes as ∂gNN ∂θ , (5) where we have used the shorthand gNN for brevity. It is easy to show that the first term ∂l(r; θ)/∂gNN factors into the marginals over edges in the DP lattice, which can be computed with the forwardbackward algorithm (re</context>
</contexts>
<marker>Do, Artières, 2010</marker>
<rawString>Trinh-Minh-Tri Do and Thierry Artières. 2010. Neural conditional random fields. In Proceedings ofAISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="18259" citStr="Duchi et al., 2011" startWordPosition="2983" endWordPosition="2986">corresponding log-likelihood of the training set.5 Denote by zs,r E 10, 11 the decision variable, such that zs,r = 1 iff span s is assigned role r. By passing the potential gNN(s, r; θ) through the logistic function, we obtain the log-likelihood l(zs,r; θ) °= log p(zs,r |x, t, `, f) of an individual training example. Here, 1 if z = 1 1+e−9NN(s,r;θ) s,r e−9NN(s,r;θ) if z = 0 1+e−9NN(s,r;θ) s,r Thus, the gold role for a given span according to the training data serves as the positive example, while all the other potential roles serve as negatives. To maximize the log-likelihood, we use Adagrad (Duchi et al., 2011). This requires the gradient of the log-likelihood with respect to the parameters θ, which can be derived using the chain rule. Structured Estimation In structured estimation, we instead learn a globally normalized probabilistic model that takes the structural constraints into account during training. This method is closely related to the linear approach of Täckström et al. (2015), as well as to the fine-tuning of a neural CRF described by Do and Artières (2010). We train the model by maximizing the loglikelihood of the training data, again using Adagrad. From Equation (1), we have that the lo</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Neural CRF parsing.</title>
<date>2015</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8911" citStr="Durrett and Klein (2015)" startWordPosition="1388" endWordPosition="1391">nforces most of the constraints proposed by Punyakanok et al. and showed how the DP can be used to take these constraints into account during learning. Here, we use an identical graphical model, but extend the model of Täckstr6m et al. by replacing its linear potential func2Some recent work have successfully proposed joint models for syntactic parsing and SRL instead of a pipeline approach (Lewis et al., 2015). 961 tions with a multi-layer neural network. A similar use of non-linear potential functions in a structured model was proposed by Do and Artières (2010) for speech recognition, and by Durrett and Klein (2015) for syntactic phrase-structure parsing. Feature-based approaches to SRL employ handengineered linguistically-motivated feature templates to represent the semantic structure. Some recent work has focused on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formu</context>
</contexts>
<marker>Durrett, Klein, 2015</marker>
<rawString>Greg Durrett and Dan Klein. 2015. Neural CRF parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An introduction to the bootstrap.</title>
<date>1994</date>
<publisher>CRC press.</publisher>
<contexts>
<context position="21556" citStr="Efron and Tibshirani, 1994" startWordPosition="3538" endWordPosition="3542">es 5.0 corpus (Weischedel et al., 2011). We also evaluate our model on the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009), that annotates roles for semantic dependencies, rather than full argument spans. For the CoNLL datasets, we use the standard training, development and test sets. For frame-semantic parsing using FrameNet conventions (Baker et al., 1998), we follow Das et al. (2014) and Hermann et al. (2014) in using the full-text annotations of the FrameNet 1.5 release and follow their data splits. We use the standard evaluation scripts for each task and use a paired bootstrap test (Efron and Tibshirani, 1994) to assess the statistical significance of the results. For brevity, we only give the p-values for the observed differences between our best and second best models on each of the test sets. p(zs,r |x, t, `, f) = ⎧ ⎨ ⎩ ∂l(r; θ) ∂l(r; θ) ∂θ = ∂gNN K gPoE(s, r; θ) = j=1 964 4.2 Preprocessing and Frame Identification All datasets are preprocessed with a part-of-speech tagger and a syntactic dependency parser, both trained on the CoNLL 2012 training split, after converting the constituency trees to Stanford-style dependencies (De Marneffe and Manning, 2013). The tagger is based on a second-order co</context>
</contexts>
<marker>Efron, Tibshirani, 1994</marker>
<rawString>Bradley Efron and Robert J Tibshirani. 1994. An introduction to the bootstrap. CRC press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Maria Antònia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre,</title>
<date></date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<location>Sebastian Padó,</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, </marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antònia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Štˇepánek, et al. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Dipanjan Das</author>
<author>Jason Weston</author>
<author>Kuzman Ganchev</author>
</authors>
<title>Semantic frame identification with distributed word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1678" citStr="Hermann et al., 2014" startWordPosition="250" endWordPosition="253">y challenge in this task is sparsity of labeled data: a given predicate-role instance may only occur a handful of times in the training set. Most existing SRL systems model each semantic role as an atomic unit of meaning, ignoring finer-grained semantic similarity between roles that can be leveraged to share context between similar labels, both within and across annotation conventions. Low-dimensional embedding representations have been shown to be successful in overcoming sparsity and representing label similarity across a wide range of tasks (Weston et al., 2011; Srikumar and Manning, 2014; Hermann et al., 2014; Lei et al., 2015). In this paper, we present a new model for SRL that embeds candidate arguments and semantic roles (in context of a predicate frame) in a shared vector space. A feed-forward neural ∗Work carried out during an internship at Google. network is learned to capture correlations of the respective embedding dimensions to create argument and role representations. The similarity of these two representations, as measured by their dot product, is used to score possible roles for candidate arguments within a graphical model. This graphical model jointly models the assignment of semantic</context>
<context position="7425" citStr="Hermann et al., 2014" startWordPosition="1142" endWordPosition="1145">r each predicate, and the associated argument spans (or single word argument heads) labeled with their respective semantic roles. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and repeated core roles. Typically, these constraints have been enforced by integer lin</context>
<context position="11360" citStr="Hermann et al., 2014" startWordPosition="1759" endWordPosition="1762">e FrameNet lexicon, but their performance metrics are significantly worse than our best results. The inspiration behind our approach stems from recent work on bilinear models (Mnih and Hinton, 2007). There have been several recent studies representing input observations and output labels with distributed representations, for example, in the WSABIE model for image annotation (Weston et al., 2011), in models for embedding labels in structured graphical models (Srikumar and Manning, 2014), and in techniques to learn joint embeddings of predicate words and their semantic frames in a vector space (Hermann et al., 2014). 3 Model Our model for SRL performs inference separately for each marked predicate in a sentence. It assumes that the predicate has been automatically disambiguated to a semantic frame drawn from a frame lexicon, and the semantic roles of the frame are used for labeling the candidate arguments in the sentence. Formally, we are given a sentence x in which a predicate t, with lexical unit E, has been marked. Assuming that the semantic frame f of the predicate has already been identified (see §4.2 for this step), we seek to predict the set of spans that correspond to its overt semantic arguments</context>
<context position="21349" citStr="Hermann et al. (2014)" startWordPosition="3504" endWordPosition="3507">sed SRL using PropBank conventions (Palmer et al., 2005), we evaluate on both the CoNLL 2005 shared task dataset (Carreras and Màrquez, 2005), and the larger CoNLL 2012 dataset derived from the OntoNotes 5.0 corpus (Weischedel et al., 2011). We also evaluate our model on the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009), that annotates roles for semantic dependencies, rather than full argument spans. For the CoNLL datasets, we use the standard training, development and test sets. For frame-semantic parsing using FrameNet conventions (Baker et al., 1998), we follow Das et al. (2014) and Hermann et al. (2014) in using the full-text annotations of the FrameNet 1.5 release and follow their data splits. We use the standard evaluation scripts for each task and use a paired bootstrap test (Efron and Tibshirani, 1994) to assess the statistical significance of the results. For brevity, we only give the p-values for the observed differences between our best and second best models on each of the test sets. p(zs,r |x, t, `, f) = ⎧ ⎨ ⎩ ∂l(r; θ) ∂l(r; θ) ∂θ = ∂gNN K gPoE(s, r; θ) = j=1 964 4.2 Preprocessing and Frame Identification All datasets are preprocessed with a part-of-speech tagger and a syntactic dep</context>
<context position="23276" citStr="Hermann et al. (2014)" startWordPosition="3828" endWordPosition="3831">ls use word cluster features (see Table 1). Specifically, we use the clusters with 1000 classes from Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). To generate the candidate arguments S (see §3.2) for the CoNLL 2005 and 2012 span-based datasets, we follow Täckström et al. (2015) and adapt the algorithm of Xue and Palmer (2004) to use dependency syntax. For the dependency-based CoNLL 2009 experiments, we modify our approach to assume single length spans and treat every word of the sentence as a candidate argument. For FrameNet, we follow the heuristic of Hermann et al. (2014). As mentioned in §3, we automatically disambiguate the predicate frames. For FrameNet, we use an embedding-based model described by Hermann et al. (2014). For PropBank, we use a multiclass log-linear model, since Hermann et al. did not observe better results with the embedding model. To ensure a fair comparison with the closest linear model baseline, we ensured that the preprocessing steps, the argument candidate generation algorithm for the span-based datasets and the frame identification methods are identical to Täckström et al. (2015, §3.2, §6.2-§6.3). 4.3 Baseline Systems In addition to c</context>
<context position="25195" citStr="Hermann et al. (2014)" startWordPosition="4142" endWordPosition="4145">to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model (§3.2). First, the span embedding dimension of es was fixed to 300 to match the dimension of the pretrained GloVe word embeddings from Pennington et al. (2014) that we use to initialize the embeddings of the word-based features in φ(s, x, t, E). Preliminary experiments showed random initialization of the word-based embeddings to be inferior to pre-trained embeddings. The remaining model p</context>
</contexts>
<marker>Hermann, Das, Weston, Ganchev, 2014</marker>
<rawString>Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. 2014. Semantic frame identification with distributed word representations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>8</issue>
<contexts>
<context position="20152" citStr="Hinton, 2002" startWordPosition="3312" endWordPosition="3313">h the locally-trained linear model from Täckström et al. (2015), but we did not investigate this alternative in this work. simple correspondence with the edge scores in the DP lattice, see Täckström et al. (2015, §4) for details). Again, the chain rule can be used to compute the gradient ∂gNN/∂θ with respect to the parameters of each layer in the network. 3.4 Product of Experts As we will observe in Tables 2 to 5, random initialization of the neural network parameters θ causes variance in the performance over different runs. We found that using a straightforward product-ofexperts (PoE) model (Hinton, 2002) at inference time reduces this variance and results in significantly higher performance. This PoE model is a very simple ensemble, being the factor-wise sum of the potential functions from K independently trained neural networks: gNN(s, r, θ) . (6) (j) where g(j) NN(s, r, θ) is the score from model j. 4 Experimental Setup In this section we describe the datasets used, the required preprocessing steps, the baselines compared to and the details of our experimental setup. 4.1 Datasets and Significance Testing We evaluate our approach on four standard datasets. For span-based SRL using PropBank c</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey E. Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meghana Kshirsagar</author>
<author>Sam Thomson</author>
<author>Nathan Schneider</author>
<author>Jaime Carbonell</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
</authors>
<title>Frame-semantic role labeling with heterogeneous annotations.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10510" citStr="Kshirsagar et al. (2015)" startWordPosition="1630" endWordPosition="1633"> not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventions. For FrameNet-style SRL, Kshirsagar et al. (2015) recently proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically motivated features tuned towards the FrameNet lexicon, but their performance metrics are significantly worse than our best results. The inspiration behind our approach stems from recent work on bilinear models (Mnih and Hinton, 2007). There have been several recent studies representing input observations and output labels with distributed representations, for example, in the WSABIE model for image annot</context>
</contexts>
<marker>Kshirsagar, Thomson, Schneider, Carbonell, Smith, Dyer, 2015</marker>
<rawString>Meghana Kshirsagar, Sam Thomson, Nathan Schneider, Jaime Carbonell, Noah A. Smith, and Chris Dyer. 2015. Frame-semantic role labeling with heterogeneous annotations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="22202" citStr="Lafferty et al., 2001" startWordPosition="3647" endWordPosition="3650">al significance of the results. For brevity, we only give the p-values for the observed differences between our best and second best models on each of the test sets. p(zs,r |x, t, `, f) = ⎧ ⎨ ⎩ ∂l(r; θ) ∂l(r; θ) ∂θ = ∂gNN K gPoE(s, r; θ) = j=1 964 4.2 Preprocessing and Frame Identification All datasets are preprocessed with a part-of-speech tagger and a syntactic dependency parser, both trained on the CoNLL 2012 training split, after converting the constituency trees to Stanford-style dependencies (De Marneffe and Manning, 2013). The tagger is based on a second-order conditional random field (Lafferty et al., 2001) with standard emission and transition features; for parsing, we use a graph-based parser with structural diversity and cube-pruning (Zhang and McDonald, 2014). On the WSJ development set (section 22), the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2%. On the CoNLL 2012 development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the SRL models use word cluster features (see Table 1). Specifically, we use the clusters with 1000 classes from Turian et al. (2010), which are induced with t</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yuan Zhang</author>
<author>Lluís Márquez</author>
<author>Alessandro Moschitti</author>
<author>Regina Barzilay</author>
</authors>
<title>High-order lowrank tensors for semantic role labeling.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="1697" citStr="Lei et al., 2015" startWordPosition="254" endWordPosition="257">sk is sparsity of labeled data: a given predicate-role instance may only occur a handful of times in the training set. Most existing SRL systems model each semantic role as an atomic unit of meaning, ignoring finer-grained semantic similarity between roles that can be leveraged to share context between similar labels, both within and across annotation conventions. Low-dimensional embedding representations have been shown to be successful in overcoming sparsity and representing label similarity across a wide range of tasks (Weston et al., 2011; Srikumar and Manning, 2014; Hermann et al., 2014; Lei et al., 2015). In this paper, we present a new model for SRL that embeds candidate arguments and semantic roles (in context of a predicate frame) in a shared vector space. A feed-forward neural ∗Work carried out during an internship at Google. network is learned to capture correlations of the respective embedding dimensions to create argument and role representations. The similarity of these two representations, as measured by their dot product, is used to score possible roles for candidate arguments within a graphical model. This graphical model jointly models the assignment of semantic roles to all argum</context>
<context position="9286" citStr="Lei et al. (2015)" startWordPosition="1440" endWordPosition="1443">e approach (Lewis et al., 2015). 961 tions with a multi-layer neural network. A similar use of non-linear potential functions in a structured model was proposed by Do and Artières (2010) for speech recognition, and by Durrett and Klein (2015) for syntactic phrase-structure parsing. Feature-based approaches to SRL employ handengineered linguistically-motivated feature templates to represent the semantic structure. Some recent work has focused on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do</context>
<context position="24966" citStr="Lei et al. (2015)" startWordPosition="4104" endWordPosition="4107">only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model (§3.2). First, the span embedding dimension of es was fixed to 300 to match the dimension of the pretrained GloVe word embeddings from Pennington et al. (2014) th</context>
<context position="28358" citStr="Lei et al. 2015" startWordPosition="4696" endWordPosition="4699">no syntactic parse, while the best scores among all systems are underlined. Results from prior work are taken from the respective papers, and ‘–’ indicates performance metrics missing in the original publication. Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p &lt; 0.01 (*) and p &lt; 0.05 (**). Excluding predicate senses Including predicate senses WSJ Dev WSJ Test Brown Test WSJ Test Brown Test CoNLL-2009 1st place – 82.1 69.8 86.2 74.6 Björkelund et al., 2010 + reranking 80.5 82.9 70.9 86.9 75.7 Roth and Woodsend, 2014 + reranking – 82.1 71.1 86.3 75.9 Lei et al. 2015 81.0 82.5 70.8 86.6 75.6 Täckström et al. 2015 (Local) 81.4 83.0 71.2 86.9 74.8 Täckström et al. 2015 (Struct.) 82.4 83.7 72.3 87.3 75.5 This work (Local) 81.2±0.2 82.7±0.3 71.9±0.4 86.7±0.2 75.2±0.3 This work (Struct) 82.3±0.1 83.6±0.1 71.9±0.3 87.3±0.1 75.2±0.2 This work (Local, PoE) 82.4 83.8 72.8 87.5 75.9 This work (Struct., PoE) 83.0* 84.3* 72.4 87.8* 75.5 Table 3: PropBank-style semantic dependency SRL results (labeled F1) on the CoNLL 2009 data set. Bold font indicates the best system. Statistical significance was assessed with p &lt; 0.01 (*). tured learning.6 All hyperparameters were t</context>
<context position="30404" citStr="Lei et al. (2015)" startWordPosition="5024" endWordPosition="5027">d with structured learning achieves higher F,-score than all non-ensemble baselines, except the LSTM model of Zhou and Xu. It is on par and at times better than ensemble baselines that use diverse syntactic parses. The PoE model outperforms all baselines on the Brown test set, exhibiting its generalization power on out-of-domain text. Overall, using structured learning improves recall at a slight expense of precision when compared to local learning, leading to an increase in the complete argument structure accuracy (Comp. in the tables). Table 3 shows results on the CoNLL 2009 task. Following Lei et al. (2015), we present results using the official evaluation script, along with additional metrics that do not count frame predictions. Note that the linear baseline of Täckström et al. 966 Method CoNLL 2012 Development P R F1 Comp. Täckstr6m (Local) 80.6 77.1 78.8 59.0 Täckstr6m (Struct.) 80.5 77.8 79.1 60.1 Zhou – – 81.1 – This work (Local) 80.4 77.3 78.8 59.0 This work (Struct) 80.6 77.8 79.2 59.8 This work (Local, PoE) 81.0 78.3 79.6 60.6 This work (Struct., PoE) 81.0 78.5 79.7 60.9 CoNLL 2012 Test Method P R F1 Comp. Pradhan 81.3 70.5 75.5 51.7 Pradhan, revised 78.5 76.6 77.5 55.8 Täckstr6m (Local)</context>
</contexts>
<marker>Lei, Zhang, Márquez, Moschitti, Barzilay, 2015</marker>
<rawString>Tao Lei, Yuan Zhang, Lluís Márquez, Alessandro Moschitti, and Regina Barzilay. 2015. High-order lowrank tensors for semantic role labeling. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Luheng He</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Joint A* CCG parsing and semantic role labelling.</title>
<date>2015</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8700" citStr="Lewis et al., 2015" startWordPosition="1354" endWordPosition="1357">äckstr6m et al. (2015) interpreted this as a graphical model with local factors for each span-role pair, and global factors that encode the structural constraints. They derived a dynamic program (DP) that enforces most of the constraints proposed by Punyakanok et al. and showed how the DP can be used to take these constraints into account during learning. Here, we use an identical graphical model, but extend the model of Täckstr6m et al. by replacing its linear potential func2Some recent work have successfully proposed joint models for syntactic parsing and SRL instead of a pipeline approach (Lewis et al., 2015). 961 tions with a multi-layer neural network. A similar use of non-linear potential functions in a structured model was proposed by Do and Artières (2010) for speech recognition, and by Durrett and Klein (2015) for syntactic phrase-structure parsing. Feature-based approaches to SRL employ handengineered linguistically-motivated feature templates to represent the semantic structure. Some recent work has focused on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-ra</context>
</contexts>
<marker>Lewis, He, Zettlemoyer, 2015</marker>
<rawString>Mike Lewis, Luheng He, and Luke Zettlemoyer. 2015. Joint A* CCG parsing and semantic role labelling. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="10937" citStr="Mnih and Hinton, 2007" startWordPosition="1694" endWordPosition="1698">report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventions. For FrameNet-style SRL, Kshirsagar et al. (2015) recently proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically motivated features tuned towards the FrameNet lexicon, but their performance metrics are significantly worse than our best results. The inspiration behind our approach stems from recent work on bilinear models (Mnih and Hinton, 2007). There have been several recent studies representing input observations and output labels with distributed representations, for example, in the WSABIE model for image annotation (Weston et al., 2011), in models for embedding labels in structured graphical models (Srikumar and Manning, 2014), and in techniques to learn joint embeddings of predicate words and their semantic frames in a vector space (Hermann et al., 2014). 3 Model Our model for SRL performs inference separately for each marked predicate in a sentence. It assumes that the predicate has been automatically disambiguated to a semant</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve restricted Boltzmann machines.</title>
<date>2010</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="15536" citStr="Nair and Hinton, 2010" startWordPosition="2531" endWordPosition="2534">ential function gL(s, r; 9) with the real-valued output of a feed forward neural network with non-linear hidden units. The network structure is outlined in Figure 2. The frame f and role r are initially encoded using a one-hot encoding as if and ir. In other words, if and ir have all zeros except for one position at f and r respectively. These are passed through fully connected linear layers to give ef and er. We call these linear layers the embedding layers since if selects the embedding of the frame f and ir for r. Next, ef and er are passed through a fully connected rectified linear layer (Nair and Hinton, 2010), to obtain the final frame-role representation v(f,r). For the candidate span, the process is similar. Atomic features 0(s, x, t, `) for the argument span s are extracted first. (These features are the non-conjoined features used in the linear model of Täckström et al.; see Table 1 for the list). These are next passed through a fully-connected linear embedding layer to get the span embedding es, which is subsequently passed through a fully connected rectified linear layer to obtain vs, the final span representation. The final output is the dot product of vs and v(f,r): gNN(s, r; 9) = vs · v(f</context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted Boltzmann machines. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational linguistics,</journal>
<pages>31--1</pages>
<contexts>
<context position="7003" citStr="Palmer et al., 2005" startWordPosition="1075" endWordPosition="1078"> evoked by the verbs steal, pickpocket, or lift, while PropBank has distinct frames for each of them. The Theft frame also contains the core roles Goods and Perpetrator that additionally belong to the Commercial_transaction and Committing_crime frames respectively. A typical SRL dataset consists of sentence-level annotations that identify (possibly multiple) target predicates in each sentence, a disambiguated frame for each predicate, and the associated argument spans (or single word argument heads) labeled with their respective semantic roles. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann</context>
<context position="20784" citStr="Palmer et al., 2005" startWordPosition="3412" endWordPosition="3415">e time reduces this variance and results in significantly higher performance. This PoE model is a very simple ensemble, being the factor-wise sum of the potential functions from K independently trained neural networks: gNN(s, r, θ) . (6) (j) where g(j) NN(s, r, θ) is the score from model j. 4 Experimental Setup In this section we describe the datasets used, the required preprocessing steps, the baselines compared to and the details of our experimental setup. 4.1 Datasets and Significance Testing We evaluate our approach on four standard datasets. For span-based SRL using PropBank conventions (Palmer et al., 2005), we evaluate on both the CoNLL 2005 shared task dataset (Carreras and Màrquez, 2005), and the larger CoNLL 2012 dataset derived from the OntoNotes 5.0 corpus (Weischedel et al., 2011). We also evaluate our model on the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009), that annotates roles for semantic dependencies, rather than full argument spans. For the CoNLL datasets, we use the standard training, development and test sets. For frame-semantic parsing using FrameNet conventions (Baker et al., 1998), we follow Das et al. (2014) and Hermann et al. (2014) in using the full-text annotations</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="25563" citStr="Pennington et al. (2014)" startWordPosition="4199" endWordPosition="4202"> model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model (§3.2). First, the span embedding dimension of es was fixed to 300 to match the dimension of the pretrained GloVe word embeddings from Pennington et al. (2014) that we use to initialize the embeddings of the word-based features in φ(s, x, t, E). Preliminary experiments showed random initialization of the word-based embeddings to be inferior to pre-trained embeddings. The remaining model parameters were randomly initialized. The frame embedding dimension was chosen from 1100, 200, 300, 5001, while the hidden layer dimension was chosen from 1300, 5001. For PropBank, we fixed the role embedding dimension to 27, which is the number of semantic roles in PropBank datasets (ignoring the AA role, that appears with negligible frequency). For FrameNet, the ro</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="10038" citStr="Pradhan et al., 2005" startWordPosition="1551" endWordPosition="1554"> work, they represent semantic roles as real-valued vectors, but use a different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventions. For FrameNet-style SRL, Kshirsagar et al. (2015) recently proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and</context>
<context position="24679" citStr="Pradhan et al., 2005" startWordPosition="4052" endWordPosition="4055">ensemble and ensemble systems of Surdeanu et al. (2007), Punyakanok et al. (2008) and Toutanova et al. (2008). The ensemble variants of these systems use multiple parses and multiple SRL systems to leverage diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) </context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H Martin, and Daniel Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning, 60(1-3):11– 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
</authors>
<title>Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Björkelund,</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<location>Olga Uryupina, Yuchen Zhang, and</location>
<marker>Pradhan, 2013</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="8078" citStr="Punyakanok et al. (2008)" startWordPosition="1249" endWordPosition="1252">emantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and repeated core roles. Typically, these constraints have been enforced by integer linear programming (ILP), as in Punyakanok et al. (2008). Täckstr6m et al. (2015) interpreted this as a graphical model with local factors for each span-role pair, and global factors that encode the structural constraints. They derived a dynamic program (DP) that enforces most of the constraints proposed by Punyakanok et al. and showed how the DP can be used to take these constraints into account during learning. Here, we use an identical graphical model, but extend the model of Täckstr6m et al. by replacing its linear potential func2Some recent work have successfully proposed joint models for syntactic parsing and SRL instead of a pipeline approac</context>
<context position="12790" citStr="Punyakanok et al., 2008" startWordPosition="2007" endWordPosition="2010">. The set of semantic roles R includes the special null role ∅, which is used to represent non-overt arguments. Thus, our algorithm performs the SRL task in one step for a single predicate frame. For the span-based SRL task, in a sentence of n words, there could be O(n2) potential arguments. For statistical and computational reasons we prune the set of spans S using a set of syntactically-informed heuristics from prior work (see §4.2). 3.1 Graphical Model We make use of a graphical model that represents global assignment of arguments to their semantic roles, subject to linguistic constraints (Punyakanok et al., 2008; Täckström et al., 2015). Under this graphical model, we assume a parameterized potential function that assigns a real-valued compatibility score g(s, r; θ) to each span-role pair (s, r) E S x R, where θ denotes the model parameters. Below, we consider two types of potential functions. As a baseline, similar to most prior work, one could use a simple linear function of discrete input features gL(s, r; θ) = θT · φ(r, s, x, t, E, f), where φ(·) denotes a feature function. In this work, we instead propose a multi-layer feed-forward neural network potential function, specified in §3.2. Given thes</context>
<context position="24139" citStr="Punyakanok et al. (2008)" startWordPosition="3967" endWordPosition="3970">serve better results with the embedding model. To ensure a fair comparison with the closest linear model baseline, we ensured that the preprocessing steps, the argument candidate generation algorithm for the span-based datasets and the frame identification methods are identical to Täckström et al. (2015, §3.2, §6.2-§6.3). 4.3 Baseline Systems In addition to comparing to Täckström et al. (2015), whose setup is closest to ours, we also compare to prior state-of-the-art systems from the literature. For CoNLL 2005, we compare to the best nonensemble and ensemble systems of Surdeanu et al. (2007), Punyakanok et al. (2008) and Toutanova et al. (2008). The ensemble variants of these systems use multiple parses and multiple SRL systems to leverage diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Mirella Lapata</author>
</authors>
<title>Efficient inference and structured learning for semantic role labeling.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--449</pages>
<contexts>
<context position="10652" citStr="Roth and Lapata (2015)" startWordPosition="1654" endWordPosition="1657">., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventions. For FrameNet-style SRL, Kshirsagar et al. (2015) recently proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically motivated features tuned towards the FrameNet lexicon, but their performance metrics are significantly worse than our best results. The inspiration behind our approach stems from recent work on bilinear models (Mnih and Hinton, 2007). There have been several recent studies representing input observations and output labels with distributed representations, for example, in the WSABIE model for image annotation (Weston et al., 2011), in models for embedding labels in structured graphical models (Srikumar and Manning, 2014), and in techniques to </context>
</contexts>
<marker>Roth, Lapata, 2015</marker>
<rawString>Michael Roth and Mirella Lapata. 2015. Efficient inference and structured learning for semantic role labeling. Transactions of the Association for Computational Linguistics, 3:449–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Roth</author>
<author>Kristian Woodsend</author>
</authors>
<title>Composition of word representations improves semantic role labelling.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="9655" citStr="Roth and Woodsend (2014)" startWordPosition="1493" endWordPosition="1496">feature templates to represent the semantic structure. Some recent work has focused on low-dimensional representations that reduce the need for intensive feature engineering and lead to better generalization in the face of data sparsity. Lei et al. (2015) employ low-rank tensor factorization to induce a compact representation of the full cross-product of atomic features; akin to this work, they represent semantic roles as real-valued vectors, but use a different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 </context>
<context position="24910" citStr="Roth and Woodsend, 2014" startWordPosition="4093" endWordPosition="4097">t to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model (§3.2). First, the span embedding dimension of es was fixed to 300 to match the dimension of the pretraine</context>
<context position="28308" citStr="Roth and Woodsend, 2014" startWordPosition="4685" endWordPosition="4688">ta. Bold font indicates the best system using a single or no syntactic parse, while the best scores among all systems are underlined. Results from prior work are taken from the respective papers, and ‘–’ indicates performance metrics missing in the original publication. Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p &lt; 0.01 (*) and p &lt; 0.05 (**). Excluding predicate senses Including predicate senses WSJ Dev WSJ Test Brown Test WSJ Test Brown Test CoNLL-2009 1st place – 82.1 69.8 86.2 74.6 Björkelund et al., 2010 + reranking 80.5 82.9 70.9 86.9 75.7 Roth and Woodsend, 2014 + reranking – 82.1 71.1 86.3 75.9 Lei et al. 2015 81.0 82.5 70.8 86.6 75.6 Täckström et al. 2015 (Local) 81.4 83.0 71.2 86.9 74.8 Täckström et al. 2015 (Struct.) 82.4 83.7 72.3 87.3 75.5 This work (Local) 81.2±0.2 82.7±0.3 71.9±0.4 86.7±0.2 75.2±0.3 This work (Struct) 82.3±0.1 83.6±0.1 71.9±0.3 87.3±0.1 75.2±0.2 This work (Local, PoE) 82.4 83.8 72.8 87.5 75.9 This work (Struct., PoE) 83.0* 84.3* 72.4 87.8* 75.5 Table 3: PropBank-style semantic dependency SRL results (labeled F1) on the CoNLL 2009 data set. Bold font indicates the best system. Statistical significance was assessed with p &lt; 0.0</context>
</contexts>
<marker>Roth, Woodsend, 2014</marker>
<rawString>Michael Roth and Kristian Woodsend. 2014. Composition of word representations improves semantic role labelling. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning distributed representations for structured output prediction.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1656" citStr="Srikumar and Manning, 2014" startWordPosition="245" endWordPosition="249">h their semantic roles. A key challenge in this task is sparsity of labeled data: a given predicate-role instance may only occur a handful of times in the training set. Most existing SRL systems model each semantic role as an atomic unit of meaning, ignoring finer-grained semantic similarity between roles that can be leveraged to share context between similar labels, both within and across annotation conventions. Low-dimensional embedding representations have been shown to be successful in overcoming sparsity and representing label similarity across a wide range of tasks (Weston et al., 2011; Srikumar and Manning, 2014; Hermann et al., 2014; Lei et al., 2015). In this paper, we present a new model for SRL that embeds candidate arguments and semantic roles (in context of a predicate frame) in a shared vector space. A feed-forward neural ∗Work carried out during an internship at Google. network is learned to capture correlations of the respective embedding dimensions to create argument and role representations. The similarity of these two representations, as measured by their dot product, is used to score possible roles for candidate arguments within a graphical model. This graphical model jointly models the </context>
<context position="11229" citStr="Srikumar and Manning, 2014" startWordPosition="1737" endWordPosition="1740">rt of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically motivated features tuned towards the FrameNet lexicon, but their performance metrics are significantly worse than our best results. The inspiration behind our approach stems from recent work on bilinear models (Mnih and Hinton, 2007). There have been several recent studies representing input observations and output labels with distributed representations, for example, in the WSABIE model for image annotation (Weston et al., 2011), in models for embedding labels in structured graphical models (Srikumar and Manning, 2014), and in techniques to learn joint embeddings of predicate words and their semantic frames in a vector space (Hermann et al., 2014). 3 Model Our model for SRL performs inference separately for each marked predicate in a sentence. It assumes that the predicate has been automatically disambiguated to a semantic frame drawn from a frame lexicon, and the semantic roles of the frame are used for labeling the candidate arguments in the sentence. Formally, we are given a sentence x in which a predicate t, with lexical unit E, has been marked. Assuming that the semantic frame f of the predicate has al</context>
</contexts>
<marker>Srikumar, Manning, 2014</marker>
<rawString>Vivek Srikumar and Christopher D Manning. 2014. Learning distributed representations for structured output prediction. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Lluís Màrquez</author>
<author>Xavier Carreras</author>
<author>Pere Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>29--105</pages>
<contexts>
<context position="24113" citStr="Surdeanu et al. (2007)" startWordPosition="3963" endWordPosition="3966">ermann et al. did not observe better results with the embedding model. To ensure a fair comparison with the closest linear model baseline, we ensured that the preprocessing steps, the argument candidate generation algorithm for the span-based datasets and the frame identification methods are identical to Täckström et al. (2015, §3.2, §6.2-§6.3). 4.3 Baseline Systems In addition to comparing to Täckström et al. (2015), whose setup is closest to ours, we also compare to prior state-of-the-art systems from the literature. For CoNLL 2005, we compare to the best nonensemble and ensemble systems of Surdeanu et al. (2007), Punyakanok et al. (2008) and Toutanova et al. (2008). The ensemble variants of these systems use multiple parses and multiple SRL systems to leverage diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu </context>
</contexts>
<marker>Surdeanu, Màrquez, Carreras, Comas, 2007</marker>
<rawString>Mihai Surdeanu, Lluís Màrquez, Xavier Carreras, and Pere Comas. 2007. Combination strategies for semantic role labeling. Journal of Artificial Intelligence Research, 29:105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluís Màrquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="7342" citStr="Surdeanu et al., 2008" startWordPosition="1127" endWordPosition="1131">fy (possibly multiple) target predicates in each sentence, a disambiguated frame for each predicate, and the associated argument spans (or single word argument heads) labeled with their respective semantic roles. 2.2 Related Work SRL using PropBank conventions (Palmer et al., 2005) has been widely studied. There have been two shared tasks at CoNLL 2004-2005 to identify the phrasal arguments of verbal predicates (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). The CoNLL 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Similar approaches (Das et al., 2014; Hermann et al., 2014) have been applied to frame-semantic parsing using FrameNet conventions (Baker et al., 1998). We treat PropBank and FrameNet annotations in a common framework, similar to Hermann et al. (2014). Most prior work on SRL rely on syntactic parses provided as input and use locally estimated classifiers for each span-role pair that are only combined at prediction time.2 This is done by picking the highest scoring role for each span, subject to a set of structural constraints, such as avoiding overlapping arguments and </context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Màrquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar Täckström</author>
<author>Kuzman Ganchev</author>
<author>Dipanjan Das</author>
</authors>
<title>Efficient inference and structured learning for semantic role labeling.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--29</pages>
<contexts>
<context position="3083" citStr="Täckström et al. (2015)" startWordPosition="465" endWordPosition="468">vector embeddings of the predictions overcome the assumption of modeling each semantic role as a discrete label, thus capturing fine-grained label similarity. Moreover, since predictions and inputs are embedded in the same vector space, and features extracted from inputs and outputs are decoupled, our approach is amenable to joint learning of multiple annotation conventions, such as PropBank and FrameNet, in a single model. Finally, as with other neural network approaches, our model obviates the need to manually engineer feature conjunctions. Our underlying inference algorithm for SRL follows Täckström et al. (2015), who presented a dynamic program for structured SRL; it is targeted towards the prediction of full argument spans. Hence, we present empirical results on three spanbased SRL datasets: CoNLL 2005 and 2012 data annotated with PropBank conventions, as well as FrameNet 1.5 data. We also evaluate our system on the dependency-based CoNLL 2009 shared task by assuming single word argument spans, that represent semantic dependencies, and limit our experiments to English. On all datasets, our model performs on par with a strong linear model baseline that uses hand-engineered conjunctive features. Due t</context>
<context position="12815" citStr="Täckström et al., 2015" startWordPosition="2011" endWordPosition="2014">es R includes the special null role ∅, which is used to represent non-overt arguments. Thus, our algorithm performs the SRL task in one step for a single predicate frame. For the span-based SRL task, in a sentence of n words, there could be O(n2) potential arguments. For statistical and computational reasons we prune the set of spans S using a set of syntactically-informed heuristics from prior work (see §4.2). 3.1 Graphical Model We make use of a graphical model that represents global assignment of arguments to their semantic roles, subject to linguistic constraints (Punyakanok et al., 2008; Täckström et al., 2015). Under this graphical model, we assume a parameterized potential function that assigns a real-valued compatibility score g(s, r; θ) to each span-role pair (s, r) E S x R, where θ denotes the model parameters. Below, we consider two types of potential functions. As a baseline, similar to most prior work, one could use a simple linear function of discrete input features gL(s, r; θ) = θT · φ(r, s, x, t, E, f), where φ(·) denotes a feature function. In this work, we instead propose a multi-layer feed-forward neural network potential function, specified in §3.2. Given these local factors, we emplo</context>
<context position="18642" citStr="Täckström et al. (2015)" startWordPosition="3041" endWordPosition="3044">e−9NN(s,r;θ) s,r Thus, the gold role for a given span according to the training data serves as the positive example, while all the other potential roles serve as negatives. To maximize the log-likelihood, we use Adagrad (Duchi et al., 2011). This requires the gradient of the log-likelihood with respect to the parameters θ, which can be derived using the chain rule. Structured Estimation In structured estimation, we instead learn a globally normalized probabilistic model that takes the structural constraints into account during training. This method is closely related to the linear approach of Täckström et al. (2015), as well as to the fine-tuning of a neural CRF described by Do and Artières (2010). We train the model by maximizing the loglikelihood of the training data, again using Adagrad. From Equation (1), we have that the log-likelihood l(r; θ) °= log p(r |x, t, `, f) of a single (structured) training example (r, S, x) is given by l(r; θ) = � g(si, ri) − A(S) . (4) si∈S By application of the chain rule, the gradient of the log-likelihood factorizes as ∂gNN ∂θ , (5) where we have used the shorthand gNN for brevity. It is easy to show that the first term ∂l(r; θ)/∂gNN factors into the marginals over ed</context>
<context position="22974" citStr="Täckström et al. (2015)" startWordPosition="3776" endWordPosition="3779">ald, 2014). On the WSJ development set (section 22), the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2%. On the CoNLL 2012 development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the SRL models use word cluster features (see Table 1). Specifically, we use the clusters with 1000 classes from Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). To generate the candidate arguments S (see §3.2) for the CoNLL 2005 and 2012 span-based datasets, we follow Täckström et al. (2015) and adapt the algorithm of Xue and Palmer (2004) to use dependency syntax. For the dependency-based CoNLL 2009 experiments, we modify our approach to assume single length spans and treat every word of the sentence as a candidate argument. For FrameNet, we follow the heuristic of Hermann et al. (2014). As mentioned in §3, we automatically disambiguate the predicate frames. For FrameNet, we use an embedding-based model described by Hermann et al. (2014). For PropBank, we use a multiclass log-linear model, since Hermann et al. did not observe better results with the embedding model. To ensure a </context>
<context position="28405" citStr="Täckström et al. 2015" startWordPosition="4705" endWordPosition="4708"> among all systems are underlined. Results from prior work are taken from the respective papers, and ‘–’ indicates performance metrics missing in the original publication. Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p &lt; 0.01 (*) and p &lt; 0.05 (**). Excluding predicate senses Including predicate senses WSJ Dev WSJ Test Brown Test WSJ Test Brown Test CoNLL-2009 1st place – 82.1 69.8 86.2 74.6 Björkelund et al., 2010 + reranking 80.5 82.9 70.9 86.9 75.7 Roth and Woodsend, 2014 + reranking – 82.1 71.1 86.3 75.9 Lei et al. 2015 81.0 82.5 70.8 86.6 75.6 Täckström et al. 2015 (Local) 81.4 83.0 71.2 86.9 74.8 Täckström et al. 2015 (Struct.) 82.4 83.7 72.3 87.3 75.5 This work (Local) 81.2±0.2 82.7±0.3 71.9±0.4 86.7±0.2 75.2±0.3 This work (Struct) 82.3±0.1 83.6±0.1 71.9±0.3 87.3±0.1 75.2±0.2 This work (Local, PoE) 82.4 83.8 72.8 87.5 75.9 This work (Struct., PoE) 83.0* 84.3* 72.4 87.8* 75.5 Table 3: PropBank-style semantic dependency SRL results (labeled F1) on the CoNLL 2009 data set. Bold font indicates the best system. Statistical significance was assessed with p &lt; 0.01 (*). tured learning.6 All hyperparameters were tuned on the respective development sets for eac</context>
</contexts>
<marker>Täckström, Ganchev, Das, 2015</marker>
<rawString>Oscar Täckström, Kuzman Ganchev, and Dipanjan Das. 2015. Efficient inference and structured learning for semantic role labeling. Transactions of the Association for Computational Linguistics, 3:29–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="24167" citStr="Toutanova et al. (2008)" startWordPosition="3972" endWordPosition="3975"> embedding model. To ensure a fair comparison with the closest linear model baseline, we ensured that the preprocessing steps, the argument candidate generation algorithm for the span-based datasets and the frame identification methods are identical to Täckström et al. (2015, §3.2, §6.2-§6.3). 4.3 Baseline Systems In addition to comparing to Täckström et al. (2015), whose setup is closest to ours, we also compare to prior state-of-the-art systems from the literature. For CoNLL 2005, we compare to the best nonensemble and ensemble systems of Surdeanu et al. (2007), Punyakanok et al. (2008) and Toutanova et al. (2008). The ensemble variants of these systems use multiple parses and multiple SRL systems to leverage diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system f</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34(2):161–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22776" citStr="Turian et al. (2010)" startWordPosition="3742" endWordPosition="3745">nditional random field (Lafferty et al., 2001) with standard emission and transition features; for parsing, we use a graph-based parser with structural diversity and cube-pruning (Zhang and McDonald, 2014). On the WSJ development set (section 22), the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2%. On the CoNLL 2012 development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the SRL models use word cluster features (see Table 1). Specifically, we use the clusters with 1000 classes from Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). To generate the candidate arguments S (see §3.2) for the CoNLL 2005 and 2012 span-based datasets, we follow Täckström et al. (2015) and adapt the algorithm of Xue and Palmer (2004) to use dependency syntax. For the dependency-based CoNLL 2009 experiments, we modify our approach to assume single length spans and treat every word of the sentence as a candidate argument. For FrameNet, we follow the heuristic of Hermann et al. (2014). As mentioned in §3, we automatically disambiguate the predicate frames. For FrameNet, we use an em</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens Van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing data using t-sne.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<issue>85</issue>
<marker>Van der Maaten, Hinton, 2008</marker>
<rawString>Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research, 9(85).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Eduard Hovy</author>
<author>Martha Palmer</author>
<author>Mitch Marcus</author>
<author>Robert Belvin</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Nianwen Xue</author>
</authors>
<title>OntoNotes: A large training corpus for enhanced processing. In</title>
<date>2011</date>
<contexts>
<context position="20968" citStr="Weischedel et al., 2011" startWordPosition="3442" endWordPosition="3445">independently trained neural networks: gNN(s, r, θ) . (6) (j) where g(j) NN(s, r, θ) is the score from model j. 4 Experimental Setup In this section we describe the datasets used, the required preprocessing steps, the baselines compared to and the details of our experimental setup. 4.1 Datasets and Significance Testing We evaluate our approach on four standard datasets. For span-based SRL using PropBank conventions (Palmer et al., 2005), we evaluate on both the CoNLL 2005 shared task dataset (Carreras and Màrquez, 2005), and the larger CoNLL 2012 dataset derived from the OntoNotes 5.0 corpus (Weischedel et al., 2011). We also evaluate our model on the CoNLL 2009 shared task dataset (Hajiˇc et al., 2009), that annotates roles for semantic dependencies, rather than full argument spans. For the CoNLL datasets, we use the standard training, development and test sets. For frame-semantic parsing using FrameNet conventions (Baker et al., 1998), we follow Das et al. (2014) and Hermann et al. (2014) in using the full-text annotations of the FrameNet 1.5 release and follow their data splits. We use the standard evaluation scripts for each task and use a paired bootstrap test (Efron and Tibshirani, 1994) to assess t</context>
</contexts>
<marker>Weischedel, Hovy, Palmer, Marcus, Belvin, Pradhan, Ramshaw, Xue, 2011</marker>
<rawString>Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch Marcus, Robert Belvin, Sameer Pradhan, Lance Ramshaw, and Nianwen Xue. 2011. OntoNotes: A large training corpus for enhanced processing. In J. Olive, C. Christianson, and J. Mc-</rawString>
</citation>
<citation valid="false">
<booktitle>Handbook of Natural Language Processing and Machine Translation.</booktitle>
<editor>Cary, editors,</editor>
<publisher>Springer.</publisher>
<marker></marker>
<rawString>Cary, editors, Handbook of Natural Language Processing and Machine Translation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>WSABIE: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="1628" citStr="Weston et al., 2011" startWordPosition="241" endWordPosition="244">and labeling them with their semantic roles. A key challenge in this task is sparsity of labeled data: a given predicate-role instance may only occur a handful of times in the training set. Most existing SRL systems model each semantic role as an atomic unit of meaning, ignoring finer-grained semantic similarity between roles that can be leveraged to share context between similar labels, both within and across annotation conventions. Low-dimensional embedding representations have been shown to be successful in overcoming sparsity and representing label similarity across a wide range of tasks (Weston et al., 2011; Srikumar and Manning, 2014; Hermann et al., 2014; Lei et al., 2015). In this paper, we present a new model for SRL that embeds candidate arguments and semantic roles (in context of a predicate frame) in a shared vector space. A feed-forward neural ∗Work carried out during an internship at Google. network is learned to capture correlations of the respective embedding dimensions to create argument and role representations. The similarity of these two representations, as measured by their dot product, is used to score possible roles for candidate arguments within a graphical model. This graphic</context>
<context position="11137" citStr="Weston et al., 2011" startWordPosition="1723" endWordPosition="1726">ly proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically motivated features tuned towards the FrameNet lexicon, but their performance metrics are significantly worse than our best results. The inspiration behind our approach stems from recent work on bilinear models (Mnih and Hinton, 2007). There have been several recent studies representing input observations and output labels with distributed representations, for example, in the WSABIE model for image annotation (Weston et al., 2011), in models for embedding labels in structured graphical models (Srikumar and Manning, 2014), and in techniques to learn joint embeddings of predicate words and their semantic frames in a vector space (Hermann et al., 2014). 3 Model Our model for SRL performs inference separately for each marked predicate in a sentence. It assumes that the predicate has been automatically disambiguated to a semantic frame drawn from a frame lexicon, and the semantic roles of the frame are used for labeling the candidate arguments in the sentence. Formally, we are given a sentence x in which a predicate t, with</context>
<context position="25278" citStr="Weston et al., 2011" startWordPosition="4154" endWordPosition="4157">Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model (§3.2). First, the span embedding dimension of es was fixed to 300 to match the dimension of the pretrained GloVe word embeddings from Pennington et al. (2014) that we use to initialize the embeddings of the word-based features in φ(s, x, t, E). Preliminary experiments showed random initialization of the word-based embeddings to be inferior to pre-trained embeddings. The remaining model parameters were randomly initialized. The frame embedding dimension was chosen from </context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. WSABIE: Scaling up to large vocabulary image annotation. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="23023" citStr="Xue and Palmer (2004)" startWordPosition="3786" endWordPosition="3789">, the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2%. On the CoNLL 2012 development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the SRL models use word cluster features (see Table 1). Specifically, we use the clusters with 1000 classes from Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). To generate the candidate arguments S (see §3.2) for the CoNLL 2005 and 2012 span-based datasets, we follow Täckström et al. (2015) and adapt the algorithm of Xue and Palmer (2004) to use dependency syntax. For the dependency-based CoNLL 2009 experiments, we modify our approach to assume single length spans and treat every word of the sentence as a candidate argument. For FrameNet, we follow the heuristic of Hermann et al. (2014). As mentioned in §3, we automatically disambiguate the predicate frames. For FrameNet, we use an embedding-based model described by Hermann et al. (2014). For PropBank, we use a multiclass log-linear model, since Hermann et al. did not observe better results with the embedding model. To ensure a fair comparison with the closest linear model bas</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Enforcing structural diversity in cube-pruned dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22361" citStr="Zhang and McDonald, 2014" startWordPosition="3669" endWordPosition="3672">est sets. p(zs,r |x, t, `, f) = ⎧ ⎨ ⎩ ∂l(r; θ) ∂l(r; θ) ∂θ = ∂gNN K gPoE(s, r; θ) = j=1 964 4.2 Preprocessing and Frame Identification All datasets are preprocessed with a part-of-speech tagger and a syntactic dependency parser, both trained on the CoNLL 2012 training split, after converting the constituency trees to Stanford-style dependencies (De Marneffe and Manning, 2013). The tagger is based on a second-order conditional random field (Lafferty et al., 2001) with standard emission and transition features; for parsing, we use a graph-based parser with structural diversity and cube-pruning (Zhang and McDonald, 2014). On the WSJ development set (section 22), the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2%. On the CoNLL 2012 development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the SRL models use word cluster features (see Table 1). Specifically, we use the clusters with 1000 classes from Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). To generate the candidate arguments S (see §3.2) for the CoNLL 2005 and 2012 span-based datasets, we follow Täckström </context>
</contexts>
<marker>Zhang, McDonald, 2014</marker>
<rawString>Hao Zhang and Ryan McDonald. 2014. Enforcing structural diversity in cube-pruned dependency parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Chunyu Kity</author>
<author>Guodong Zhou</author>
</authors>
<title>Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="24806" citStr="Zhao et al., 2009" startWordPosition="4078" endWordPosition="4081">ts of these systems use multiple parses and multiple SRL systems to leverage diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a log-linear role labeling model. 4.4 Hyperparameters and Initialization There are several hyperparameters in our model (§</context>
</contexts>
<marker>Zhao, Chen, Kity, Zhou, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Chunyu Kity, and Guodong Zhou. 2009. Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Zhou</author>
<author>Wei Xu</author>
</authors>
<title>End-to-end learning of semantic role labeling using recurrent neural networks.</title>
<date>2015</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10105" citStr="Zhou and Xu (2015)" startWordPosition="1564" endWordPosition="1567"> different scoring formulation for modeling potential arguments. Moreover, they restrict their experiments to CoNLL 2009 semantic dependencies. Roth and Woodsend (2014) improve on the state-of-the-art feature-based system of Björkelund et al. (2010) by adding distributional word representations trained on large unlabeled corpora as features. Collobert and Weston (2007) use a neural network and do not rely on syntactic parses as input. While they use non-standard evaluation, they report accuracy similar to the ASSERT system (Pradhan et al., 2005), to which we compare in Table 4. Very recently, Zhou and Xu (2015) proposed a deep bidirectional LSTM model for SRL that does not rely on syntax trees as input; their approach achieves the best results on CoNLL 2005 and 2012 corpora to date, but unlike this work, they do not report results on FrameNet and CoNLL 2009 dependencies and do not investigate joint learning approaches involving multiple annotation conventions. For FrameNet-style SRL, Kshirsagar et al. (2015) recently proposed the use of PropBankbased features, but their system performance falls short of the state of the art. Roth and Lapata (2015) proposed another approach exploring linguistically m</context>
<context position="24545" citStr="Zhou and Xu (2015)" startWordPosition="4030" endWordPosition="4033"> closest to ours, we also compare to prior state-of-the-art systems from the literature. For CoNLL 2005, we compare to the best nonensemble and ensemble systems of Surdeanu et al. (2007), Punyakanok et al. (2008) and Toutanova et al. (2008). The ensemble variants of these systems use multiple parses and multiple SRL systems to leverage diversity. In contrast to these ensemble systems, our product-of-experts model uses only a single architecture and one syntactic parse; the constituent models differ only in random initialization. We also compare with the recent deep bidirectional LSTM model of Zhou and Xu (2015). For CoNLL 2012, we compare to Pradhan et al. (2013), who report results with the (non-ensemble) ASSERT system (Pradhan et al., 2005), and to the model of Zhou and Xu (2015). For CoNLL 2009, we compare to the top system from the shared task (Zhao et al., 2009), two state-of-the-art systems that employ a reranker (Björkelund et al., 2010; Roth and Woodsend, 2014), and the recent tensor-based model of Lei et al. (2015). We also trained the linear model of Täckström et al. on this dataset (their work omitted this experiment), as a baseline. Finally, for the FrameNet experiments, we compare to th</context>
</contexts>
<marker>Zhou, Xu, 2015</marker>
<rawString>Jie Zhou and Wei Xu. 2015. End-to-end learning of semantic role labeling using recurrent neural networks. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
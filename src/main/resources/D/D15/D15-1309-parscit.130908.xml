<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005363">
<title confidence="0.988643">
An Analysis of Domestic Abuse Discourse on Reddit
</title>
<author confidence="0.997971">
Nicolas Schrading1 Cecilia O. Alm&apos; Ray Ptucha1 Christopher M. Homan3
</author>
<affiliation confidence="0.935009666666667">
1 Kate Gleason College of Engineering, Rochester Institute of Technology
&apos; College of Liberal Arts, Rochester Institute of Technology
3 Golisano College of Computing and Information Sciences, Rochester Institute of Technology
</affiliation>
<email confidence="0.85882">
{jxs8172§|coagla§|rwpeec§|cmh†}@{§rit.edu|†cs.rit.edu}
</email>
<sectionHeader confidence="0.990129" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999225">
Domestic abuse affects people of every
race, class, age, and nation. There is sig-
nificant research on the prevalence and ef-
fects of domestic abuse; however, such re-
search typically involves population-based
surveys that have high financial costs. This
work provides a qualitative analysis of do-
mestic abuse using data collected from the
social and news-aggregation website red-
dit.com. We develop classifiers to detect
submissions discussing domestic abuse,
achieving accuracies of up to 92%, a sub-
stantial error reduction over its baseline.
Analysis of the top features used in detect-
ing abuse discourse provides insight into
the dynamics of abusive relationships.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999784491803279">
Globally, 30% of women fifteen and older have ex-
perienced physical and/or sexual intimate partner
violence at some point in their life (Devries et al.,
2013). While domestic abuse tends to have greater
prevalence in low-income and non-western coun-
tries, it is still endemic in regions like North Amer-
ica and Western Europe. In the United States, by
an intimate partner, 9.4% of women have been
raped, 16.9% of women and 8% of men have expe-
rienced sexual violence other than rape, and 24.3%
of women and 13.8% of men have experienced se-
vere physical violence (Black et al., 2011). This
translates to an estimated economic cost of $5.8
billion for direct medical and mental health care
services, along with lost productivity and reduced
lifetime earnings (Craft, 2003). Economic costs
are calculable and provide concrete metrics for
policy makers, but the physical and psychologi-
cal effects felt by victims of domestic abuse are
the true costs. Domestic abuse is the 12th leading
cause of years of life lost (Murray et al., 2013),
and it contributes to health issues including fre-
quent headaches, chronic pain, difficulty sleeping,
anxiety, and depression (Black et al., 2011).
The data used to calculate such statistics are
often derived from costly and time-consuming
population-based surveys that primarily seek to
obtain insight into the prevalence and conse-
quences of domestic abuse. Due to safety con-
cerns for victims and researchers, these surveys
follow strict guidelines set by the World Health
Organization (Garcia-Moreno et al., 2001). Great
care must be taken by the researchers to ensure the
safety of the participants, and therefore the num-
ber of participants is often quite small (Burge et
al., 2014). One way to avoid the cost of wide
scale surveys while still maintaining appropriate
research conditions is to leverage the abundance
of data publicly available on the web. Of particular
interest are moderated forums that allow discourse
between users.
Reddit1 is one such website, and the chosen
source of data for this paper. This site has a
wide range of forums dedicated to various topics,
called subreddits, each of which are moderated by
community volunteers. For subreddits dedicated
to sensitive topics such as depression, domestic
abuse, and suicide, the moderators tend to ensure
that the anonymous submitter has access to local
help hotlines if a life-threatening situation is de-
scribed. They also enforce respectful behavior and
ensure that the submissions are on topic by delet-
ing disrespectful or off-topic posts. Finally, they
ensure that site rules are followed, including the
strict disallowal of doxing, the practice of using
submission details to reveal user identities.
Reddit allows lengthy submissions, unlike Twit-
ter, and therefore the use of standard English is
more common. This allows natural language pro-
cessing tools like semantic role labelers trained on
standard English to function better. Finally, Red-
</bodyText>
<footnote confidence="0.998455">
1See www.reddit.com.
</footnote>
<page confidence="0.849891">
2577
</page>
<note confidence="0.6836545">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2577–2583,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999822888888889">
dit allows users to comment on submissions, pro-
viding them with the ability to ask questions, give
advice, and provide support. This makes its data
ideal for sensitive subjects not typically discussed
in social media.
This work makes two contributions: classifiers
for identifying texts discussing domestic abuse
and an analysis of discussions of domestic abuse
in several subreddits.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997396263157895">
Social media sites are an emerging source of data
for public health studies, such as mental health,
bullying, and disease tracking. These sites provide
less intimidating and more accessible channels
for reporting, collectively processing, and making
sense of traumatic and stigmatizing experiences
(Homan et al., 2014; Walther, 1996). Many re-
searchers have focused on Twitter data, due to its
prominent presence, accessibility, and the charac-
teristics of tweets. For instance, De Choudhury et
al. (2013) predicted the onset of depression from
user tweets, while other studies have modeled dis-
tress (Homan et al., 2014; Lehrman et al., 2012).
Most relevantly, Schrading et al. (2015) used the
#WhyIStayed trend to predict whether a tweet was
about staying in an abusive relationship or leaving,
analyzing the lexical structures victims of abuse
give for staying or leaving.
Reddit has been studied less in this area, with
work mainly focusing on mental health. In
Pavalanathan and De Choudhury (2015), a large
number of subreddits on the topic of mental health
were identified and used to determine the differ-
ences in discourse between throwaway2 and regu-
lar accounts. They observed almost 6 times more
throwaway submissions in mental health subred-
dits over control subreddits, and found that throw-
away accounts exhibit considerable disinhibition
in discussing sensitive aspects of the self. This
motivates our work in analyzing Reddit submis-
sions on domestic abuse, which can be assumed
to have similar levels of throwaway accounts and
discussion. Additionally, Balani and De Choud-
hury (2015) used standard ngram features, along
with submission and author attributes to classify a
submission as high or low self-disclosure.
2Anonymous, one-time accounts to submit a single (often
personal or sensitive) submission or comment.
</bodyText>
<sectionHeader confidence="0.936656" genericHeader="method">
3 Dataset3 and Data Analysis
</sectionHeader>
<bodyText confidence="0.999444111111111">
Following the procedure in Balani and De Choud-
hury (2015) for subreddit discovery, we identified
several subreddits that focus on domestic abuse.
Additionally, we determined subreddits unrelated
to domestic abuse, to be used as a control set. Ta-
ble 1 shows the subreddits, the total number of
unique posts (called submissions4), and total num-
ber of replies to those submissions (called com-
ments) collected.
</bodyText>
<table confidence="0.998341666666667">
Domestic Abuse # Submissions # Comments
abuseinterrupted 1653 1069
domesticviolence 749 2145
survivorsofabuse 512 2172
Control # Submissions # Comments
casualconversation 7286 285575
advice 5913 31323
anxiety 4183 23300
anger 837 3693
</table>
<tableCaption confidence="0.997703">
Table 1: The domestic abuse subreddits and con-
</tableCaption>
<bodyText confidence="0.977926619047619">
trol subreddits with the total number of submis-
sions and comments collected.
The anger and anxiety subreddits were chosen
as control subreddits in order to help the classi-
fier discriminate between the dynamics of abusive
relationships and the potential effects of abuse on
victims. For example, anxiety and anger may be
affect caused by domestic abuse, but they are also
caused by a wide variety of other factors. By in-
cluding these subreddits in the control set, a classi-
fier should utilize the situations, causes, and stake-
holders in abusive relationships as features, not
the affect particularly associated with abusive rela-
tionships. Similarly, the advice subreddit was cho-
sen as a way to help the classifier understand that
advice-seeking behavior is not indicative of abuse.
The casualconversation subreddit allows discus-
sion of anything, providing an excellent sample
of general written discourse. The domestic abuse
subreddits have far fewer active users, submis-
sions, and comments in total.
</bodyText>
<subsectionHeader confidence="0.998639">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999668">
All experiments used the same preprocessing
steps. From the collected subreddits, only sub-
missions with at least one comment were chosen
to be included for study. We then ran the sub-
mission text through the Illinois Curator (Clarke et
</bodyText>
<footnote confidence="0.987718333333333">
3Released on nicschrading.com/data/.
4Submission text is its title and selftext (an optional text
body) concatenated together.
</footnote>
<page confidence="0.990256">
2578
</page>
<bodyText confidence="0.9972833">
al., 2012) to provide semantic role labeling (SRL)
(Punyakanok et al., 2008). A total of 552 domestic
abuse submissions were parsed, and we randomly
chose an even distribution of the control subred-
dits (138 each), yielding a total sample size of
1104. All submissions were normalized by low-
ercasing, lemmatizing, and stoplisting. External
links and URLs were replaced with url and refer-
ences to subreddits, e.g. /r/domesticviolence, were
replaced with subreddit link.
</bodyText>
<subsectionHeader confidence="0.999631">
3.2 Descriptive Statistics
</subsectionHeader>
<bodyText confidence="0.997578333333333">
We present basic descriptive statistics on the set
of 552 abuse submissions and 552 non-abuse sub-
missions in Table 2.
</bodyText>
<table confidence="0.994757">
Abuse Non-Abuse
Avg comments/post 5.4 f 6.1 13.2 f 25.3
Avg score/post 6.1 f 5.1 7.5 f 16.4
Avg tokens/post 278 f 170 208 f 164
# unique submitters 482 535
Avg comment depth 0.96 f 1.5 1.5 f 1.9
Avg comment score 2.2 f 2.7 2.1 f 2.9
Avg tokens/comment 107 f 128 53.4 f 79.9
# comments 2989 6964
# unique commenters 1022 2519
</table>
<tableCaption confidence="0.997971">
Table 2: Basic descriptive statistics. The
</tableCaption>
<bodyText confidence="0.999779428571429">
score is provided by users voting on submis-
sions/comments they feel are informative. The
depth of a comment indicates where in a reply
chain it falls. A depth of 0 means it is in reply
to the submission, a depth of 1 means it is in re-
ply to a depth 0 comment, etc. The ± values are
standard deviation metrics.
In general, the non-abuse subreddits have more
discourse between commenters, as indicated by a
larger comment depth, however, the abuse subred-
dits tend to have longer submissions and replies.
The abuse subreddits perhaps also have a smaller
more tight-knit, community as indicated by fewer
numbers of unique submitters and commenters.
</bodyText>
<subsectionHeader confidence="0.99654">
3.3 Ngram Attributes
</subsectionHeader>
<bodyText confidence="0.999122529411765">
To get a sense of the language used between the
two sets of subreddits, the most frequent 1-, 2-
, and 3-grams were examined. While there are
many common and overlapping ngrams in the
two sets, each set does have distinct ngrams. In
the abuse set, distinct ngrams include the obvi-
ous abuse (1595 occurences), domestic violence
(202), and abusive relationship (166). Addition-
ally, unique 3-grams related to the agents and situ-
ations in abusive relationships like local dv agency
(12) and make feel bad (11) appear. Also included
are unique empathetic and helping discourse from
comments, including let know (121), and feel free
pm5 (27). This indicates that comment data could
improve classification results, as support and em-
pathy may be more prevalent in the abuse set than
in the control set.
</bodyText>
<subsectionHeader confidence="0.995781">
3.4 Semantic Role Attributes
</subsectionHeader>
<bodyText confidence="0.999439666666667">
From the SRL tool, our dataset was tagged with
various arguments of predicates. This data is par-
ticularly useful in our study, as we are interested
in examining the semantic actions and stakehold-
ers within an abusive relationship. By perform-
ing a lookup in Proposition Bank (Martha et al.,
2005) with a given argument number, predicate,
and sense, we retrieved unique role labels for each
argument.
We determined the top 100 most frequent roles
and predicates in the two sets, and took only the
unique roles and predicates within each set to
see what frequently occurring but unique roles
and predicates exist within the abuse and control
group.
</bodyText>
<table confidence="0.991355083333333">
Role Label Predicate
caller, 175 abuse, 433
thing hit, 174 share, 167
agent, hitter - animate only!, 164 believe, 164
abuser, agent, 162 call, 151
entity abused, 139 remember, 149
utterance, 115 cry, 147
patient, entity experiencing !tell, 142
hurt/damage, 113
utterance, sound, 104 send, 127
belief, 104 thank, 127
benefactive, 103 realize, 124
</table>
<tableCaption confidence="0.994147">
Table 3: Top 10 unique roles and predicates with
</tableCaption>
<bodyText confidence="0.861409181818182">
their frequency for the abuse data. An exclamation
point on a predicate indicates negation.
Table 3 contains roles and predicates that are
powerful indicators of an abusive relationship, in-
cluding a caller, hitter, thing hit, abuser, and entity
experiencing hurt/damage. Importantly, several
predicates that appear in this data also appear in a
study on discussions of domestic abuse in Twitter
data, including believe and realize, which indicate
cognitive manipulation in the victims of domestic
abuse (Schrading et al., 2015).
</bodyText>
<footnote confidence="0.992604">
5The initials pm stand for private message.
</footnote>
<page confidence="0.892815">
2579
</page>
<table confidence="0.9803505">
Classifer N P R N+P
Linear SVM 90 ± 3 72 ± 5 73 ± 4 88 ± 3
N+P+R
87 ± 3
N+R P+R
88 ± 3 73 ± 4
</table>
<tableCaption confidence="0.99887">
Table 4: Classification accuracies of Linear SVM. N=Ngrams, P=Predicates, R=Roles.
</tableCaption>
<sectionHeader confidence="0.983449" genericHeader="method">
4 Classification Experiments
</sectionHeader>
<bodyText confidence="0.999951363636363">
In order to discover the semantic and lexical fea-
tures salient to abusive relationships, we designed
several classifiers. The subreddit category to
which a submission was posted was used as the
gold standard label of abuse or non-abuse. The la-
bels were validated by examining the top ngrams,
roles, and predicates in Section 3, and taking into
account that these subreddits are moderated for
on-topic content. We ran several experiments to
study classifiers, the impact of features, and the
effect of comments on prediction performance.
</bodyText>
<subsectionHeader confidence="0.999328">
4.1 Combinations of Features
</subsectionHeader>
<bodyText confidence="0.999993647058824">
We used the 1-, 2-, and 3-grams in the submis-
sion text, the predicates, and the semantic role
labels as features, using TF-IDF vectorization6.
Perceptron, naive Bayes, logistic regression, ran-
dom forest, radial basis function SVM, and lin-
ear SVM classifiers were parameter optimized us-
ing 10-fold cross validation. Table 4 contains the
results for the best classifier. The best features
are the ngrams, achieving the highest accuracies
alone. Predicates and semantic roles perform ad-
mirably, but bring the classifier accuracies down
slightly when added to ngrams. To determine
the top features for prediction, we examined the
weights of the top performing classifier, Scikit-
learn’s (Pedregosa et al., 2011) Linear SVM with
C=0.1, as in Guyon et al. (2002). These, along
with their weights, are shown in Table 5.
</bodyText>
<subsectionHeader confidence="0.995532">
4.2 Comment Data Only
</subsectionHeader>
<bodyText confidence="0.999994727272727">
We experimented with only comment data to pre-
dict if they were posted in an abuse or non-abuse
subreddit. Because ngram features performed best
in the previous experiment, a larger set of sub-
missions (1336 per class) was used. A final held
out testset was created from 10% of these submis-
sions, giving 1202 submissions per class for the
devset and 134 per class for the testset. Taking the
comments from these submissions yielded 4712
abuse and 19349 non-abuse comments for the de-
vset and 642 abuse and 2264 non-abuse comments
</bodyText>
<footnote confidence="0.919334">
6Binary features and only unigrams were tried but these
did not improve results.
</footnote>
<table confidence="0.999807818181818">
Abuse Non Abuse
abusive, 1.3 anxiety, 1.1
child, 0.93 anger, 1.1
abuser, 0.86 job, 0.52
relationship, 0.84 school, 0.46
therapy, 0.83 hour, 0.45
survivor, 0.83 week, 0.45
domestic, 0.73 fuck, 0.44
happen, 0.72 class, 0.42
violence, 0.68 college, 0.41
father, 0.67 fun, 0.40
</table>
<tableCaption confidence="0.983814">
Table 5: Top 10 features based on Linear SVM
</tableCaption>
<bodyText confidence="0.952698428571429">
weights using only ngrams from submissions. The
classifier may be relying heavily on the anxi-
ety and anger subreddits to discriminate between
abuse and non-abuse, as indicated by the sharp
drop in SVM weight from anger to job. Abuse
word weights are more evenly distributed.
for the testset. 10-fold cross-validation was used
on the devset to tune the classifier. Using a Linear
SVM with C=1 achieved an F1 score of 0.70±.02
on the devset. On the held out testset, it achieved
a precision of 0.68, recall of 0.62, and F1 score of
0.65. Examining its weights gives features similar
to those in Table 5, with additional empathetic dis-
course like thank, hug, and safe in the abuse class.
</bodyText>
<subsectionHeader confidence="0.9686215">
4.3 Comment and Submission Text
Combined
</subsectionHeader>
<bodyText confidence="0.999984055555556">
Concatenating the comments to their respective
submissions may improve results, but because
comments can be completely off-topic or in re-
ply to other comments, we experimented with only
the top-scoring comments and those most simi-
lar to the submission text. To compute similar-
ity we used a sum of the word vector representa-
tions from Levy and Goldberg (2014) as included
in spaCy (Honnibal, 2015) and used cosine simi-
larity. Taking only the top 90th percentile in user
voting score and text similarity, we had 2688 abuse
comments and 7852 non-abuse comments con-
catenated to the 1336 submissions of their class.
This method achieves extremely high accuracy of
94%±2% on the devset and 92% on the testset us-
ing a Linear SVM with C=1. A classifier trained
only on the submission text data from the same
devset/testset split obtains the lower accuracies of
</bodyText>
<page confidence="0.955399">
2580
</page>
<bodyText confidence="0.996562">
90%±2% on the devset and 86% on the testset, in-
dicating that comments can add predictive power.
The top features are similar to those in Table 5.
</bodyText>
<subsectionHeader confidence="0.998766">
4.4 Uneven Set of Submissions
</subsectionHeader>
<bodyText confidence="0.964902">
Using the method in Section 4.3 to train the clas-
sifier, a larger, uneven set of data was examined
(still using only ngrams). This set contained 1336
abuse and 17020 non-abuse instances. From this
set, 15% were held out for final examination as a
testset and the rest was used as a devset with 5-
fold cross-validation. On the devset, an F1 score
of 0.81 ± 0.01 was achieved while on the testset it
had a precision of 0.84, recall of 0.74, and F1 score
of 0.79. The best classifier was a Linear SVM with
C=100. The confusion matrix of the testset is in
</bodyText>
<tableCaption confidence="0.790261">
Table 4.4.
</tableCaption>
<table confidence="0.999161">
Predicted Class
Abuse Non-Abuse
Actual Abuse 152 53
Class
Non-Abuse 29 2520
</table>
<tableCaption confidence="0.927345">
Table 6: Confusion matrix on the testset of the
Abuse/Non-Abuse classifier.
</tableCaption>
<bodyText confidence="0.99947475">
This classifier has good precision for the abuse
class, and decent recall, meaning that there can be
confidence that submissions flagged as abuse are
indeed about abuse. By applying this classifier to
a large held out set of data, these results suggest
that many potentially relevant submissions would
be flagged for examination, and they would mostly
be about abuse.
</bodyText>
<subsectionHeader confidence="0.9986515">
4.5 Testing on Completely Held Out
Subreddits
</subsectionHeader>
<bodyText confidence="0.999989243902439">
To get a sense of efficacy in the wild in detecting
submissions about abuse, the best classifier from
Section 4.4 was taken (trained on the devset data)
and run on a large set of submissions from the
relationships and relationship advice subreddits.
These subreddits are general forums for discussion
and advice on any relationship (not necessarily in-
timate). Their submissions tend to be long, de-
scriptive, and extremely personal.
After running the abuse classifier on the submis-
sions from these subreddits with at least 1 com-
ment (13623 in total, with their 90th percentile
comments concatenated), 423 submissions were
flagged as being about abuse. 101 of these 423
were annotated by 3 annotators (co-authors), us-
ing the labels A (the submission discusses an abu-
sive relationship), M (off-hand mention of abuse),
N (not about abuse), and O (off-topic submission
or other).
From the three annotators’ annotations, on av-
erage 59% are A, 16% are M, 23% are N, and
2% are O. The percentage of overall agreement
was 72% and Randolph’s free-marginal multirater
kappa (Warrens, 2010) score was 0.63. Annota-
tors occasionally had a hard time distinguishing
between A and M, as context may have been miss-
ing. Combining the two by considering all M as
A, the average percent of A increases to 75%, the
percentage of overall agreement improves to 86%
and the kappa score improves to 0.79. Taking the
statistic that on average 75% of the flagged sub-
missions in the annotated subset are about abuse
or have a mention of abuse indicates that this clas-
sifier should hopefully have a precision of around
0.75 on unseen Reddit data at large. Understand-
ably, the precision drops by about .1 compared to
its use on the subreddits it was trained and tested
on. A precision of 0.75 on this set of data would
mean that any statistics from this set may include
some noise, but overall, the trends should reveal
important results about abuse.
</bodyText>
<sectionHeader confidence="0.998835" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999686875">
This work provides an analysis of domestic abuse
using the online social site Reddit. Language anal-
ysis reveals interesting patterns used in discussing
abuse, as well as initial data about the semantic
actions and stakeholders involved in abusive rela-
tionships. Multiple classifiers were implemented
to determine the top semantic and linguistic fea-
tures in detecting abusive relationships. Simpler
features such as ngrams performed above the more
complex predicate and role labels extracted from
a semantic role labeler, though the more complex
structures contribute to interesting insights in data
analysis. Future work could use a larger training
set from multiple online sites to analyze the pat-
terns of online abuse discourse across varied fo-
rums.
</bodyText>
<sectionHeader confidence="0.972513" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.8818785">
This work was supported in part by a GCCIS
Kodak Endowed Chair Fund Health Information
Technology Strategic Initiative Grant and NSF
Award #SES-1111016.
</bodyText>
<page confidence="0.979802">
2581
</page>
<sectionHeader confidence="0.98905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996028504504505">
Sairam Balani and Munmun De Choudhury. 2015. De-
tecting and characterizing mental health related self-
disclosure in social media. In Proceedings of the
33rd Annual Association for Computing Machinery
Conference Extended Abstracts on Human Factors
in Computing Systems, CHI EA ’15, pages 1373–
1378, New York, NY, USA. ACM.
Michele C Black, Kathleen C Basile, Matthew J Brei-
ding, Sharon G Smith, Mikel L Walters, Melissa T
Merrick, and MR Stevens. 2011. National intimate
partner and sexual violence survey. Atlanta, GA:
Centers for Disease Control and Prevention, 75.
Sandra K. Burge, Johanna Becho, Robert L. Ferrer,
Robert C. Wood, Melissa Talamantes, and David A.
Katerndahl. 2014. Safely examining complex dy-
namics of intimate partner violence. Families, Sys-
tems, &amp; Health, 32(3):259 – 270.
Munmun De Choudhury, Scott Counts, Eric Horvitz,
and Michael Gamon. 2013. Predicting depression
via social media. In Proceedings of the Seventh An-
nual International Association for the Advancement
ofArtificial Intelligence Conference on Weblogs and
Social Media, pages 128–137. AAAI, July.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP curator (or: How I
learned to stop worrying and love NLP pipelines). In
Proceedings of the Eighth International Conference
on Language Resources and Evaluation (LREC’12),
pages 3276–3283, Istanbul, Turkey, may. European
Language Resources Association (ELRA).
Carole Craft. 2003. Costs of intimate partner violence
against women in the United States. Technical re-
port, National Center for Injury Prevention and Con-
trol, Centers for Disease Control and Prevention, At-
lanta, GA.
K.M. Devries, Joelle Y.T. Mak, C. Garc´ıa-Moreno,
M. Petzold, J.C. Child, G. Falder, S. Lim, L.J. Bac-
chus, R.E. Engell, L. Rosenfeld, C. Pallitto, T. Voss,
and C.H. Watts. 2013. The global prevalence of
intimate partner violence against women. Science,
340(6140):1527–1528.
Claudia Garcia-Moreno, C Watts, and L Heise.
2001. Putting women first: Ethical and safety rec-
ommendations for research on domestic violence
against women. Department of Gender and Wom-
ens Health, World Health Organization. Geneva,
Switzerland.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389–422, March.
Christopher Homan, Ravdeep Johar, Tong Liu, Megan
Lytle, Vincent Silenzio, and Cecilia Ovesdotter Alm.
2014. Toward macro-insights for suicide preven-
tion: Analyzing fine-grained distress at scale. In
Proceedings of the Workshop on Computational Lin-
guistics and Clinical Psychology: From Linguistic
Signal to Clinical Reality, pages 107–117, Balti-
more, Maryland, USA, June. Association for Com-
putational Linguistics.
Matthew Honnibal. 2015. SpaCy: Industrial
strength NLP with Python and Cython. https:
//github.com/honnibal/spaCy.
Michael Thaul Lehrman, Cecilia Ovesdotter Alm, and
Rub´en A. Proa˜no. 2012. Detecting distressed and
non-distressed affect states in short forum texts. In
Proceedings of the Second Workshop on Language
in Social Media, LSM ’12, pages 9–18, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, volume 2, pages 302–308.
Palmer Martha, Gildea Dan, and Kingsbury Paul.
2005. The Proposition Bank: A corpus annotated
with semantic roles. Computational Linguistics
Journal, 31:71–106.
Christopher JL Murray, Jerry Abraham, Mohammed K
Ali, Miriam Alvarado, Charles Atkinson, Larry M
Baddour, David H Bartels, Emelia J Benjamin, Kavi
Bhalla, Gretchen Birbeck, et al. 2013. The state of
US health, 1990-2010: Burden of diseases, injuries,
and risk factors. JAMA, 310(6):591–606.
Umashanthi Pavalanathan and Munmun De Choud-
hury. 2015. Identity management and mental
health discourse in social media. In Proceedings
of WWW’15 Companion: 24th International World
Wide Web Conference, Web Science Track, Florence,
Italy, May. WWW’15 Companion.
Fabian Pedregosa, Ga¨el Varoquaux., Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.
Nicolas Schrading, Cecilia Ovesdotter Alm, Raymond
Ptucha, and Christopher Homan. 2015. #WhyIS-
tayed, #WhyILeft: Microblogging to make sense of
domestic abuse. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1281–1286, Denver, Col-
orado, May–June. Association for Computational
Linguistics.
</reference>
<page confidence="0.761574">
2582
</page>
<reference confidence="0.997874285714286">
Joseph Walther. 1996. Computer-mediated com-
munication: Impersonal, interpersonal, and hyper-
personal interaction. Communication Research,
23(1):3–43, Feb.
Matthijs J. Warrens. 2010. Inequalities between multi-
rater kappas. Advances in Data Analysis and Clas-
sification, 4(4):271–286.
</reference>
<page confidence="0.964074">
2583
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.563477">
<title confidence="0.833418">An Analysis of Domestic Abuse Discourse on Reddit</title>
<author confidence="0.753086">Cecilia O Christopher M</author>
<note confidence="0.838329333333333">1Kate Gleason College of Engineering, Rochester Institute of of Liberal Arts, Rochester Institute of 3Golisano College of Computing and Information Sciences, Rochester Institute of</note>
<abstract confidence="0.998625470588235">Domestic abuse affects people of every race, class, age, and nation. There is significant research on the prevalence and effects of domestic abuse; however, such research typically involves population-based surveys that have high financial costs. This work provides a qualitative analysis of domestic abuse using data collected from the social and news-aggregation website reddit.com. We develop classifiers to detect submissions discussing domestic abuse, achieving accuracies of up to 92%, a substantial error reduction over its baseline. Analysis of the top features used in detecting abuse discourse provides insight into the dynamics of abusive relationships.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sairam Balani</author>
<author>Munmun De Choudhury</author>
</authors>
<title>Detecting and characterizing mental health related selfdisclosure in social media.</title>
<date>2015</date>
<booktitle>In Proceedings of the 33rd Annual Association for Computing Machinery Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA ’15,</booktitle>
<pages>1373--1378</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Balani, De Choudhury, 2015</marker>
<rawString>Sairam Balani and Munmun De Choudhury. 2015. Detecting and characterizing mental health related selfdisclosure in social media. In Proceedings of the 33rd Annual Association for Computing Machinery Conference Extended Abstracts on Human Factors in Computing Systems, CHI EA ’15, pages 1373– 1378, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele C Black</author>
<author>Kathleen C Basile</author>
<author>Matthew J Breiding</author>
<author>Sharon G Smith</author>
<author>Mikel L Walters</author>
<author>Melissa T Merrick</author>
<author>MR Stevens</author>
</authors>
<title>National intimate partner and sexual violence survey.</title>
<date>2011</date>
<booktitle>Centers for Disease Control and Prevention,</booktitle>
<volume>75</volume>
<location>Atlanta, GA:</location>
<contexts>
<context position="1667" citStr="Black et al., 2011" startWordPosition="250" endWordPosition="253">of abusive relationships. 1 Introduction Globally, 30% of women fifteen and older have experienced physical and/or sexual intimate partner violence at some point in their life (Devries et al., 2013). While domestic abuse tends to have greater prevalence in low-income and non-western countries, it is still endemic in regions like North America and Western Europe. In the United States, by an intimate partner, 9.4% of women have been raped, 16.9% of women and 8% of men have experienced sexual violence other than rape, and 24.3% of women and 13.8% of men have experienced severe physical violence (Black et al., 2011). This translates to an estimated economic cost of $5.8 billion for direct medical and mental health care services, along with lost productivity and reduced lifetime earnings (Craft, 2003). Economic costs are calculable and provide concrete metrics for policy makers, but the physical and psychological effects felt by victims of domestic abuse are the true costs. Domestic abuse is the 12th leading cause of years of life lost (Murray et al., 2013), and it contributes to health issues including frequent headaches, chronic pain, difficulty sleeping, anxiety, and depression (Black et al., 2011). Th</context>
</contexts>
<marker>Black, Basile, Breiding, Smith, Walters, Merrick, Stevens, 2011</marker>
<rawString>Michele C Black, Kathleen C Basile, Matthew J Breiding, Sharon G Smith, Mikel L Walters, Melissa T Merrick, and MR Stevens. 2011. National intimate partner and sexual violence survey. Atlanta, GA: Centers for Disease Control and Prevention, 75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K Burge</author>
<author>Johanna Becho</author>
<author>Robert L Ferrer</author>
<author>Robert C Wood</author>
<author>Melissa Talamantes</author>
<author>David A Katerndahl</author>
</authors>
<title>Safely examining complex dynamics of intimate partner violence.</title>
<date>2014</date>
<journal>Families, Systems, &amp; Health,</journal>
<volume>32</volume>
<issue>3</issue>
<pages>270</pages>
<contexts>
<context position="2802" citStr="Burge et al., 2014" startWordPosition="428" endWordPosition="431"> chronic pain, difficulty sleeping, anxiety, and depression (Black et al., 2011). The data used to calculate such statistics are often derived from costly and time-consuming population-based surveys that primarily seek to obtain insight into the prevalence and consequences of domestic abuse. Due to safety concerns for victims and researchers, these surveys follow strict guidelines set by the World Health Organization (Garcia-Moreno et al., 2001). Great care must be taken by the researchers to ensure the safety of the participants, and therefore the number of participants is often quite small (Burge et al., 2014). One way to avoid the cost of wide scale surveys while still maintaining appropriate research conditions is to leverage the abundance of data publicly available on the web. Of particular interest are moderated forums that allow discourse between users. Reddit1 is one such website, and the chosen source of data for this paper. This site has a wide range of forums dedicated to various topics, called subreddits, each of which are moderated by community volunteers. For subreddits dedicated to sensitive topics such as depression, domestic abuse, and suicide, the moderators tend to ensure that the </context>
</contexts>
<marker>Burge, Becho, Ferrer, Wood, Talamantes, Katerndahl, 2014</marker>
<rawString>Sandra K. Burge, Johanna Becho, Robert L. Ferrer, Robert C. Wood, Melissa Talamantes, and David A. Katerndahl. 2014. Safely examining complex dynamics of intimate partner violence. Families, Systems, &amp; Health, 32(3):259 – 270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Scott Counts</author>
<author>Eric Horvitz</author>
<author>Michael Gamon</author>
</authors>
<title>Predicting depression via social media.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh Annual International Association for the Advancement ofArtificial Intelligence Conference on Weblogs and Social Media,</booktitle>
<pages>128--137</pages>
<publisher>AAAI,</publisher>
<marker>De Choudhury, Counts, Horvitz, Gamon, 2013</marker>
<rawString>Munmun De Choudhury, Scott Counts, Eric Horvitz, and Michael Gamon. 2013. Predicting depression via social media. In Proceedings of the Seventh Annual International Association for the Advancement ofArtificial Intelligence Conference on Weblogs and Social Media, pages 128–137. AAAI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Vivek Srikumar</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>An NLP curator (or: How I learned to stop worrying and love NLP pipelines).</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>3276--3283</pages>
<location>Istanbul, Turkey,</location>
<marker>Clarke, Srikumar, Sammons, Roth, 2012</marker>
<rawString>James Clarke, Vivek Srikumar, Mark Sammons, and Dan Roth. 2012. An NLP curator (or: How I learned to stop worrying and love NLP pipelines). In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), pages 3276–3283, Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carole Craft</author>
</authors>
<title>Costs of intimate partner violence against women in the United States.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>National Center for Injury Prevention and Control, Centers for Disease Control and Prevention,</institution>
<location>Atlanta, GA.</location>
<contexts>
<context position="1855" citStr="Craft, 2003" startWordPosition="280" endWordPosition="281">13). While domestic abuse tends to have greater prevalence in low-income and non-western countries, it is still endemic in regions like North America and Western Europe. In the United States, by an intimate partner, 9.4% of women have been raped, 16.9% of women and 8% of men have experienced sexual violence other than rape, and 24.3% of women and 13.8% of men have experienced severe physical violence (Black et al., 2011). This translates to an estimated economic cost of $5.8 billion for direct medical and mental health care services, along with lost productivity and reduced lifetime earnings (Craft, 2003). Economic costs are calculable and provide concrete metrics for policy makers, but the physical and psychological effects felt by victims of domestic abuse are the true costs. Domestic abuse is the 12th leading cause of years of life lost (Murray et al., 2013), and it contributes to health issues including frequent headaches, chronic pain, difficulty sleeping, anxiety, and depression (Black et al., 2011). The data used to calculate such statistics are often derived from costly and time-consuming population-based surveys that primarily seek to obtain insight into the prevalence and consequence</context>
</contexts>
<marker>Craft, 2003</marker>
<rawString>Carole Craft. 2003. Costs of intimate partner violence against women in the United States. Technical report, National Center for Injury Prevention and Control, Centers for Disease Control and Prevention, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Devries</author>
<author>Joelle Y T Mak</author>
<author>C Garc´ıa-Moreno</author>
<author>M Petzold</author>
<author>J C Child</author>
<author>G Falder</author>
<author>S Lim</author>
<author>L J Bacchus</author>
<author>R E Engell</author>
<author>L Rosenfeld</author>
<author>C Pallitto</author>
<author>T Voss</author>
<author>C H Watts</author>
</authors>
<title>The global prevalence of intimate partner violence against women.</title>
<date>2013</date>
<journal>Science,</journal>
<volume>340</volume>
<issue>6140</issue>
<marker>Devries, Mak, Garc´ıa-Moreno, Petzold, Child, Falder, Lim, Bacchus, Engell, Rosenfeld, Pallitto, Voss, Watts, 2013</marker>
<rawString>K.M. Devries, Joelle Y.T. Mak, C. Garc´ıa-Moreno, M. Petzold, J.C. Child, G. Falder, S. Lim, L.J. Bacchus, R.E. Engell, L. Rosenfeld, C. Pallitto, T. Voss, and C.H. Watts. 2013. The global prevalence of intimate partner violence against women. Science, 340(6140):1527–1528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Garcia-Moreno</author>
<author>C Watts</author>
<author>L Heise</author>
</authors>
<title>Putting women first: Ethical and safety recommendations for research on domestic violence against women. Department of Gender and Womens Health, World Health Organization.</title>
<date>2001</date>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2632" citStr="Garcia-Moreno et al., 2001" startWordPosition="398" endWordPosition="401"> abuse are the true costs. Domestic abuse is the 12th leading cause of years of life lost (Murray et al., 2013), and it contributes to health issues including frequent headaches, chronic pain, difficulty sleeping, anxiety, and depression (Black et al., 2011). The data used to calculate such statistics are often derived from costly and time-consuming population-based surveys that primarily seek to obtain insight into the prevalence and consequences of domestic abuse. Due to safety concerns for victims and researchers, these surveys follow strict guidelines set by the World Health Organization (Garcia-Moreno et al., 2001). Great care must be taken by the researchers to ensure the safety of the participants, and therefore the number of participants is often quite small (Burge et al., 2014). One way to avoid the cost of wide scale surveys while still maintaining appropriate research conditions is to leverage the abundance of data publicly available on the web. Of particular interest are moderated forums that allow discourse between users. Reddit1 is one such website, and the chosen source of data for this paper. This site has a wide range of forums dedicated to various topics, called subreddits, each of which ar</context>
</contexts>
<marker>Garcia-Moreno, Watts, Heise, 2001</marker>
<rawString>Claudia Garcia-Moreno, C Watts, and L Heise. 2001. Putting women first: Ethical and safety recommendations for research on domestic violence against women. Department of Gender and Womens Health, World Health Organization. Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Jason Weston</author>
<author>Stephen Barnhill</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Gene selection for cancer classification using support vector machines.</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<pages>46--1</pages>
<contexts>
<context position="14202" citStr="Guyon et al. (2002)" startWordPosition="2238" endWordPosition="2241">tion6. Perceptron, naive Bayes, logistic regression, random forest, radial basis function SVM, and linear SVM classifiers were parameter optimized using 10-fold cross validation. Table 4 contains the results for the best classifier. The best features are the ngrams, achieving the highest accuracies alone. Predicates and semantic roles perform admirably, but bring the classifier accuracies down slightly when added to ngrams. To determine the top features for prediction, we examined the weights of the top performing classifier, Scikitlearn’s (Pedregosa et al., 2011) Linear SVM with C=0.1, as in Guyon et al. (2002). These, along with their weights, are shown in Table 5. 4.2 Comment Data Only We experimented with only comment data to predict if they were posted in an abuse or non-abuse subreddit. Because ngram features performed best in the previous experiment, a larger set of submissions (1336 per class) was used. A final held out testset was created from 10% of these submissions, giving 1202 submissions per class for the devset and 134 per class for the testset. Taking the comments from these submissions yielded 4712 abuse and 19349 non-abuse comments for the devset and 642 abuse and 2264 non-abuse com</context>
</contexts>
<marker>Guyon, Weston, Barnhill, Vapnik, 2002</marker>
<rawString>Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002. Gene selection for cancer classification using support vector machines. Machine Learning, 46(1-3):389–422, March.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christopher Homan</author>
<author>Ravdeep Johar</author>
<author>Tong Liu</author>
<author>Megan Lytle</author>
<author>Vincent Silenzio</author>
<author>Cecilia Ovesdotter Alm</author>
</authors>
<title>Toward macro-insights for suicide prevention: Analyzing fine-grained distress at scale.</title>
<date>2014</date>
<booktitle>In Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,</booktitle>
<pages>107--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="4974" citStr="Homan et al., 2014" startWordPosition="753" endWordPosition="756">advice, and provide support. This makes its data ideal for sensitive subjects not typically discussed in social media. This work makes two contributions: classifiers for identifying texts discussing domestic abuse and an analysis of discussions of domestic abuse in several subreddits. 2 Related Work Social media sites are an emerging source of data for public health studies, such as mental health, bullying, and disease tracking. These sites provide less intimidating and more accessible channels for reporting, collectively processing, and making sense of traumatic and stigmatizing experiences (Homan et al., 2014; Walther, 1996). Many researchers have focused on Twitter data, due to its prominent presence, accessibility, and the characteristics of tweets. For instance, De Choudhury et al. (2013) predicted the onset of depression from user tweets, while other studies have modeled distress (Homan et al., 2014; Lehrman et al., 2012). Most relevantly, Schrading et al. (2015) used the #WhyIStayed trend to predict whether a tweet was about staying in an abusive relationship or leaving, analyzing the lexical structures victims of abuse give for staying or leaving. Reddit has been studied less in this area, w</context>
</contexts>
<marker>Homan, Johar, Liu, Lytle, Silenzio, Alm, 2014</marker>
<rawString>Christopher Homan, Ravdeep Johar, Tong Liu, Megan Lytle, Vincent Silenzio, and Cecilia Ovesdotter Alm. 2014. Toward macro-insights for suicide prevention: Analyzing fine-grained distress at scale. In Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 107–117, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
</authors>
<date>2015</date>
<booktitle>SpaCy: Industrial strength NLP with Python and Cython. https:</booktitle>
<pages>//github.com/honnibal/spaCy.</pages>
<contexts>
<context position="16330" citStr="Honnibal, 2015" startWordPosition="2599" endWordPosition="2600"> 0.62, and F1 score of 0.65. Examining its weights gives features similar to those in Table 5, with additional empathetic discourse like thank, hug, and safe in the abuse class. 4.3 Comment and Submission Text Combined Concatenating the comments to their respective submissions may improve results, but because comments can be completely off-topic or in reply to other comments, we experimented with only the top-scoring comments and those most similar to the submission text. To compute similarity we used a sum of the word vector representations from Levy and Goldberg (2014) as included in spaCy (Honnibal, 2015) and used cosine similarity. Taking only the top 90th percentile in user voting score and text similarity, we had 2688 abuse comments and 7852 non-abuse comments concatenated to the 1336 submissions of their class. This method achieves extremely high accuracy of 94%±2% on the devset and 92% on the testset using a Linear SVM with C=1. A classifier trained only on the submission text data from the same devset/testset split obtains the lower accuracies of 2580 90%±2% on the devset and 86% on the testset, indicating that comments can add predictive power. The top features are similar to those in T</context>
</contexts>
<marker>Honnibal, 2015</marker>
<rawString>Matthew Honnibal. 2015. SpaCy: Industrial strength NLP with Python and Cython. https: //github.com/honnibal/spaCy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Thaul Lehrman</author>
<author>Cecilia Ovesdotter Alm</author>
<author>Rub´en A Proa˜no</author>
</authors>
<title>Detecting distressed and non-distressed affect states in short forum texts.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media, LSM ’12,</booktitle>
<pages>9--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Lehrman, Alm, Proa˜no, 2012</marker>
<rawString>Michael Thaul Lehrman, Cecilia Ovesdotter Alm, and Rub´en A. Proa˜no. 2012. Detecting distressed and non-distressed affect states in short forum texts. In Proceedings of the Second Workshop on Language in Social Media, LSM ’12, pages 9–18, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>302--308</pages>
<contexts>
<context position="16292" citStr="Levy and Goldberg (2014)" startWordPosition="2591" endWordPosition="2594">set, it achieved a precision of 0.68, recall of 0.62, and F1 score of 0.65. Examining its weights gives features similar to those in Table 5, with additional empathetic discourse like thank, hug, and safe in the abuse class. 4.3 Comment and Submission Text Combined Concatenating the comments to their respective submissions may improve results, but because comments can be completely off-topic or in reply to other comments, we experimented with only the top-scoring comments and those most similar to the submission text. To compute similarity we used a sum of the word vector representations from Levy and Goldberg (2014) as included in spaCy (Honnibal, 2015) and used cosine similarity. Taking only the top 90th percentile in user voting score and text similarity, we had 2688 abuse comments and 7852 non-abuse comments concatenated to the 1336 submissions of their class. This method achieves extremely high accuracy of 94%±2% on the devset and 92% on the testset using a Linear SVM with C=1. A classifier trained only on the submission text data from the same devset/testset split obtains the lower accuracies of 2580 90%±2% on the devset and 86% on the testset, indicating that comments can add predictive power. The </context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Dependencybased word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 2, pages 302–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Palmer Martha</author>
<author>Gildea Dan</author>
<author>Kingsbury Paul</author>
</authors>
<title>The Proposition Bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics Journal,</journal>
<pages>31--71</pages>
<contexts>
<context position="11351" citStr="Martha et al., 2005" startWordPosition="1778" endWordPosition="1781">d (11) appear. Also included are unique empathetic and helping discourse from comments, including let know (121), and feel free pm5 (27). This indicates that comment data could improve classification results, as support and empathy may be more prevalent in the abuse set than in the control set. 3.4 Semantic Role Attributes From the SRL tool, our dataset was tagged with various arguments of predicates. This data is particularly useful in our study, as we are interested in examining the semantic actions and stakeholders within an abusive relationship. By performing a lookup in Proposition Bank (Martha et al., 2005) with a given argument number, predicate, and sense, we retrieved unique role labels for each argument. We determined the top 100 most frequent roles and predicates in the two sets, and took only the unique roles and predicates within each set to see what frequently occurring but unique roles and predicates exist within the abuse and control group. Role Label Predicate caller, 175 abuse, 433 thing hit, 174 share, 167 agent, hitter - animate only!, 164 believe, 164 abuser, agent, 162 call, 151 entity abused, 139 remember, 149 utterance, 115 cry, 147 patient, entity experiencing !tell, 142 hurt/</context>
</contexts>
<marker>Martha, Dan, Paul, 2005</marker>
<rawString>Palmer Martha, Gildea Dan, and Kingsbury Paul. 2005. The Proposition Bank: A corpus annotated with semantic roles. Computational Linguistics Journal, 31:71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher JL Murray</author>
<author>Jerry Abraham</author>
<author>Mohammed K Ali</author>
<author>Miriam Alvarado</author>
<author>Charles Atkinson</author>
<author>Larry M Baddour</author>
<author>David H Bartels</author>
<author>Emelia J Benjamin</author>
<author>Kavi Bhalla</author>
<author>Gretchen Birbeck</author>
</authors>
<title>The state of US health, 1990-2010: Burden of diseases, injuries, and risk factors.</title>
<date>2013</date>
<journal>JAMA,</journal>
<volume>310</volume>
<issue>6</issue>
<contexts>
<context position="2116" citStr="Murray et al., 2013" startWordPosition="322" endWordPosition="325">of women and 8% of men have experienced sexual violence other than rape, and 24.3% of women and 13.8% of men have experienced severe physical violence (Black et al., 2011). This translates to an estimated economic cost of $5.8 billion for direct medical and mental health care services, along with lost productivity and reduced lifetime earnings (Craft, 2003). Economic costs are calculable and provide concrete metrics for policy makers, but the physical and psychological effects felt by victims of domestic abuse are the true costs. Domestic abuse is the 12th leading cause of years of life lost (Murray et al., 2013), and it contributes to health issues including frequent headaches, chronic pain, difficulty sleeping, anxiety, and depression (Black et al., 2011). The data used to calculate such statistics are often derived from costly and time-consuming population-based surveys that primarily seek to obtain insight into the prevalence and consequences of domestic abuse. Due to safety concerns for victims and researchers, these surveys follow strict guidelines set by the World Health Organization (Garcia-Moreno et al., 2001). Great care must be taken by the researchers to ensure the safety of the participan</context>
</contexts>
<marker>Murray, Abraham, Ali, Alvarado, Atkinson, Baddour, Bartels, Benjamin, Bhalla, Birbeck, 2013</marker>
<rawString>Christopher JL Murray, Jerry Abraham, Mohammed K Ali, Miriam Alvarado, Charles Atkinson, Larry M Baddour, David H Bartels, Emelia J Benjamin, Kavi Bhalla, Gretchen Birbeck, et al. 2013. The state of US health, 1990-2010: Burden of diseases, injuries, and risk factors. JAMA, 310(6):591–606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umashanthi Pavalanathan</author>
<author>Munmun De Choudhury</author>
</authors>
<title>Identity management and mental health discourse in social media.</title>
<date>2015</date>
<booktitle>In Proceedings of WWW’15 Companion: 24th International World Wide Web Conference, Web Science Track,</booktitle>
<location>Florence, Italy,</location>
<note>WWW’15 Companion.</note>
<marker>Pavalanathan, De Choudhury, 2015</marker>
<rawString>Umashanthi Pavalanathan and Munmun De Choudhury. 2015. Identity management and mental health discourse in social media. In Proceedings of WWW’15 Companion: 24th International World Wide Web Conference, Web Science Track, Florence, Italy, May. WWW’15 Companion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux., Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="8661" citStr="Punyakanok et al., 2008" startWordPosition="1322" endWordPosition="1325">ything, providing an excellent sample of general written discourse. The domestic abuse subreddits have far fewer active users, submissions, and comments in total. 3.1 Preprocessing All experiments used the same preprocessing steps. From the collected subreddits, only submissions with at least one comment were chosen to be included for study. We then ran the submission text through the Illinois Curator (Clarke et 3Released on nicschrading.com/data/. 4Submission text is its title and selftext (an optional text body) concatenated together. 2578 al., 2012) to provide semantic role labeling (SRL) (Punyakanok et al., 2008). A total of 552 domestic abuse submissions were parsed, and we randomly chose an even distribution of the control subreddits (138 each), yielding a total sample size of 1104. All submissions were normalized by lowercasing, lemmatizing, and stoplisting. External links and URLs were replaced with url and references to subreddits, e.g. /r/domesticviolence, were replaced with subreddit link. 3.2 Descriptive Statistics We present basic descriptive statistics on the set of 552 abuse submissions and 552 non-abuse submissions in Table 2. Abuse Non-Abuse Avg comments/post 5.4 f 6.1 13.2 f 25.3 Avg sco</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Schrading</author>
<author>Cecilia Ovesdotter Alm</author>
<author>Raymond Ptucha</author>
<author>Christopher Homan</author>
</authors>
<title>WhyIStayed, #WhyILeft: Microblogging to make sense of domestic abuse.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1281--1286</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="5339" citStr="Schrading et al. (2015)" startWordPosition="811" endWordPosition="814">c health studies, such as mental health, bullying, and disease tracking. These sites provide less intimidating and more accessible channels for reporting, collectively processing, and making sense of traumatic and stigmatizing experiences (Homan et al., 2014; Walther, 1996). Many researchers have focused on Twitter data, due to its prominent presence, accessibility, and the characteristics of tweets. For instance, De Choudhury et al. (2013) predicted the onset of depression from user tweets, while other studies have modeled distress (Homan et al., 2014; Lehrman et al., 2012). Most relevantly, Schrading et al. (2015) used the #WhyIStayed trend to predict whether a tweet was about staying in an abusive relationship or leaving, analyzing the lexical structures victims of abuse give for staying or leaving. Reddit has been studied less in this area, with work mainly focusing on mental health. In Pavalanathan and De Choudhury (2015), a large number of subreddits on the topic of mental health were identified and used to determine the differences in discourse between throwaway2 and regular accounts. They observed almost 6 times more throwaway submissions in mental health subreddits over control subreddits, and f</context>
<context position="12624" citStr="Schrading et al., 2015" startWordPosition="1978" endWordPosition="1981">, 104 thank, 127 benefactive, 103 realize, 124 Table 3: Top 10 unique roles and predicates with their frequency for the abuse data. An exclamation point on a predicate indicates negation. Table 3 contains roles and predicates that are powerful indicators of an abusive relationship, including a caller, hitter, thing hit, abuser, and entity experiencing hurt/damage. Importantly, several predicates that appear in this data also appear in a study on discussions of domestic abuse in Twitter data, including believe and realize, which indicate cognitive manipulation in the victims of domestic abuse (Schrading et al., 2015). 5The initials pm stand for private message. 2579 Classifer N P R N+P Linear SVM 90 ± 3 72 ± 5 73 ± 4 88 ± 3 N+P+R 87 ± 3 N+R P+R 88 ± 3 73 ± 4 Table 4: Classification accuracies of Linear SVM. N=Ngrams, P=Predicates, R=Roles. 4 Classification Experiments In order to discover the semantic and lexical features salient to abusive relationships, we designed several classifiers. The subreddit category to which a submission was posted was used as the gold standard label of abuse or non-abuse. The labels were validated by examining the top ngrams, roles, and predicates in Section 3, and taking into</context>
</contexts>
<marker>Schrading, Alm, Ptucha, Homan, 2015</marker>
<rawString>Nicolas Schrading, Cecilia Ovesdotter Alm, Raymond Ptucha, and Christopher Homan. 2015. #WhyIStayed, #WhyILeft: Microblogging to make sense of domestic abuse. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1281–1286, Denver, Colorado, May–June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Walther</author>
</authors>
<title>Computer-mediated communication: Impersonal, interpersonal, and hyperpersonal interaction.</title>
<date>1996</date>
<journal>Communication Research,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="4990" citStr="Walther, 1996" startWordPosition="757" endWordPosition="758">support. This makes its data ideal for sensitive subjects not typically discussed in social media. This work makes two contributions: classifiers for identifying texts discussing domestic abuse and an analysis of discussions of domestic abuse in several subreddits. 2 Related Work Social media sites are an emerging source of data for public health studies, such as mental health, bullying, and disease tracking. These sites provide less intimidating and more accessible channels for reporting, collectively processing, and making sense of traumatic and stigmatizing experiences (Homan et al., 2014; Walther, 1996). Many researchers have focused on Twitter data, due to its prominent presence, accessibility, and the characteristics of tweets. For instance, De Choudhury et al. (2013) predicted the onset of depression from user tweets, while other studies have modeled distress (Homan et al., 2014; Lehrman et al., 2012). Most relevantly, Schrading et al. (2015) used the #WhyIStayed trend to predict whether a tweet was about staying in an abusive relationship or leaving, analyzing the lexical structures victims of abuse give for staying or leaving. Reddit has been studied less in this area, with work mainly </context>
</contexts>
<marker>Walther, 1996</marker>
<rawString>Joseph Walther. 1996. Computer-mediated communication: Impersonal, interpersonal, and hyperpersonal interaction. Communication Research, 23(1):3–43, Feb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthijs J Warrens</author>
</authors>
<title>Inequalities between multirater kappas.</title>
<date>2010</date>
<booktitle>Advances in Data Analysis and Classification,</booktitle>
<pages>4--4</pages>
<contexts>
<context position="19176" citStr="Warrens, 2010" startWordPosition="3081" endWordPosition="3082">er on the submissions from these subreddits with at least 1 comment (13623 in total, with their 90th percentile comments concatenated), 423 submissions were flagged as being about abuse. 101 of these 423 were annotated by 3 annotators (co-authors), using the labels A (the submission discusses an abusive relationship), M (off-hand mention of abuse), N (not about abuse), and O (off-topic submission or other). From the three annotators’ annotations, on average 59% are A, 16% are M, 23% are N, and 2% are O. The percentage of overall agreement was 72% and Randolph’s free-marginal multirater kappa (Warrens, 2010) score was 0.63. Annotators occasionally had a hard time distinguishing between A and M, as context may have been missing. Combining the two by considering all M as A, the average percent of A increases to 75%, the percentage of overall agreement improves to 86% and the kappa score improves to 0.79. Taking the statistic that on average 75% of the flagged submissions in the annotated subset are about abuse or have a mention of abuse indicates that this classifier should hopefully have a precision of around 0.75 on unseen Reddit data at large. Understandably, the precision drops by about .1 comp</context>
</contexts>
<marker>Warrens, 2010</marker>
<rawString>Matthijs J. Warrens. 2010. Inequalities between multirater kappas. Advances in Data Analysis and Classification, 4(4):271–286.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
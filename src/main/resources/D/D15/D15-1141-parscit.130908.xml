<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001179">
<title confidence="0.8591295">
Long Short-Term Memory Neural Networks
for Chinese Word Segmentation
</title>
<author confidence="0.995308">
Xinchi Chen, Xipeng Qiu; Chenxi Zhu, Pengfei Liu, Xuanjing Huang
</author>
<affiliation confidence="0.999259">
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
</affiliation>
<address confidence="0.920482">
825 Zhangheng Road, Shanghai, China
</address>
<email confidence="0.997175">
{xinchichen13,xpqiu,czhu13,pfliu14,xjhuang}@fudan.edu.cn
</email>
<sectionHeader confidence="0.997351" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999525235294118">
Currently most of state-of-the-art meth-
ods for Chinese word segmentation are
based on supervised learning, whose fea-
tures are mostly extracted from a local con-
text. These methods cannot utilize the long
distance information which is also crucial
for word segmentation. In this paper, we
propose a novel neural network model for
Chinese word segmentation, which adopts
the long short-term memory (LSTM) neu-
ral network to keep the previous impor-
tant information in memory cell and avoids
the limit of window size of local context.
Experiments on PKU, MSRA and CTB6
benchmark datasets show that our model
outperforms the previous neural network
models and state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991165808510638">
Word segmentation is a fundamental task for Chi-
nese language processing. In recent years, Chi-
nese word segmentation (CWS) has undergone
great development. The popular method is to re-
gard word segmentation task as a sequence label-
ing problem (Xue, 2003; Peng et al., 2004). The
goal of sequence labeling is to assign labels to all
elements in a sequence, which can be handled with
supervised learning algorithms such as Maximum
Entropy (ME) (Berger et al., 1996) and Condi-
tional Random Fields (CRF) (Lafferty et al., 2001).
However, the ability of these models is restricted
by the design of features, and the number of fea-
tures could be so large that the result models are
too large for practical use and prone to overfit on
training corpus.
Recently, neural network models have increas-
ingly used for NLP tasks for their ability to min-
imize the effort in feature engineering (Collobert
*Corresponding author.
et al., 2011; Socher et al., 2013; Turian et al.,
2010; Mikolov et al., 2013b; Bengio et al., 2003).
Collobert et al. (2011) developed the SENNA sys-
tem that approaches or surpasses the state-of-the-
art systems on a variety of sequence labeling tasks
for English. Zheng et al. (2013) applied the archi-
tecture of Collobert et al. (2011) to Chinese word
segmentation and POS tagging, also he proposed a
perceptron style algorithm to speed up the train-
ing process with negligible loss in performance.
Pei et al. (2014) models tag-tag interactions, tag-
character interactions and character-character in-
teractions based on Zheng et al. (2013). Chen et al.
(2015) proposed a gated recursive neural network
(GRNN) to explicitly model the combinations of
the characters for Chinese word segmentation task.
Each neuron in GRNN can be regarded as a differ-
ent combination of the input characters. Thus, the
whole GRNN has an ability to simulate the design
of the sophisticated features in traditional methods.
Despite of their success, a limitation of them
is that their performances are easily affected by
the size of the context window. Intuitively, many
words are difficult to segment based on the local
information only. For example, the segmentation
of the following sentence needs the information of
the long distance collocation.
</bodyText>
<equation confidence="0.998072">
I&apos; _f, (winter), &amp; (can) 3F (wear) -:; %
(amount) 3F (wear) -:; % (amount); X �.
(summer),&amp; (can) 3F (wear) -:; (more) %
(little) 3F (wear) -:; (more) % (little).
</equation>
<bodyText confidence="0.9996586">
Without the word “夏天 (summer)” or “冬天
(winter)”, it is difficult to segment the phrase “能
V多i`V多i`”. Therefore, we usually need uti-
lize the non-local information for more accurate
word segmentation. However, it does not work
by simply increasing the context window size. As
reported in (Zheng et al., 2013), the performance
drops smoothly when the window size is larger
than 3. The reason is that the number of its pa-
rameters is so large that the trained network has
</bodyText>
<page confidence="0.954896">
1197
</page>
<note confidence="0.9849065">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1197–1206,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999090869565217">
overfitted on training data. Therefore, it is neces-
sary to capture the potential long-distance depen-
dencies without increasing the size of the context
window.
In order to address this problem, we propose a
neural model based on Long Short-Term Memory
Neural Network (LSTM) (Hochreiter and Schmid-
huber, 1997) that explicitly model the previous
information by exploiting input, output and for-
get gates to decide how to utilize and update the
memory of pervious information. Intuitively, if
the LSTM unit detects an important feature from
an input sequence at early stage, it easily carries
this information (the existence of the feature) over
a long distance, hence, capturing the potential use-
ful long-distance information. We evaluate our
model on three popular benchmark datasets (PKU,
MSRA and CTB6), and the experimental results
show that our model achieves the state-of-the-art
performance with the smaller context window size
(0,2).
The contributions of this paper can be summa-
rized as follows.
</bodyText>
<listItem confidence="0.9940574">
• We first introduce the LSTM neural network
for Chinese word segmentation. The LSTM
can capture potential long-distance depen-
dencies and keep the previous useful informa-
tion in memory, which avoids the limit of the
size of context window.
• Although there are relatively few researches
of applying dropout method to the LSTM,
we investigate several dropout strategies and
find that dropout is also effective to avoid the
overfitting of the LSTM.
• Despite Chinese word segmentation being a
specific case, our model can be easily gener-
alized and applied to the other sequence la-
beling tasks.
</listItem>
<sectionHeader confidence="0.9448935" genericHeader="introduction">
2 Neural Model for Chinese Word
Segmentation
</sectionHeader>
<bodyText confidence="0.99670028125">
Chinese word segmentation is usually regarded as
character-based sequence labeling. Each character
is labeled as one of {B, M, E, S} to indicate the
segmentation. {B, M, E} represent Begin, Mid-
dle, End of a multi-character segmentation respec-
tively, and S represents a Single character segmen-
tation.
The neural model is usually characterized by
three specialized layers: (1) a character embedding
layer; (2) a series of classical neural network lay-
ers and (3) tag inference layer. An illustration is
shown in Figure 1.
The most common tagging approach is based
on a local window. The window approach as-
sumes that the tag of a character largely depends
on its neighboring characters. Given an input sen-
tence c(1:&apos;), a window of size k slides over the
sentence from character c(1) to c(&apos;), where n is
the length of the sentence. As shown in Figure
1, for each character c(t)(1 &lt; t &lt; n), the con-
text characters (c(t−2),c(t−1),c(t),c(t+1),c(t+2)) are
fed into the lookup table layer when the window
size k is 5. The characters exceeding the sen-
tence boundaries are mapped to one of two spe-
cial symbols, namely “start” and “end” symbols.
The character embeddings extracted by the lookup
table layer are then concatenated into a single vec-
tor x(t) E RH1, where H1 = k x d is the size
of layer 1. Then x(t) is fed into the next layer
which performs linear transformation followed by
an element-wise activation function g such as sig-
moid function u(x) = (1+e−x)−1 and hyperbolic
</bodyText>
<equation confidence="0.973460666666667">
tangent function O(x) = ��−�−�
��+�−� here.
h(t) = g(W1x(t) + b1), (1)
</equation>
<bodyText confidence="0.9908422">
where W1 E RH2×H1, b1 E RH2, h(t) E RH2. H2
is a hyper-parameter which indicates the number of
hidden units in layer 2. Given a set oftags T of size
|T |, a similar linear transformation is performed
except that no non-linear function is followed:
</bodyText>
<equation confidence="0.999498">
y(t) = W2h(t) + b2, (2)
</equation>
<bodyText confidence="0.999543416666667">
where W2 E R|T |×H2, b2 E R|T |. y(t) E R|T  |is
the score vector for each possible tag. In Chinese
word segmentation, the most prevalent tag set T C
T is {B, M, E, S} as mentioned above.
To model the tag dependency, a transition score
AZA is introduced to measure the probability of
jumping from tag i E T to tag j E T (Collobert
et al., 2011). Although this model works well for
Chinese word segmentation and other sequence la-
beling tasks, it just utilizes the information of con-
text of a limited-length window. Some useful long
distance information is neglected.
</bodyText>
<sectionHeader confidence="0.868664666666667" genericHeader="method">
3 Long Short-Term Memory Neural
Network for Chinese Word
Segmentation
</sectionHeader>
<bodyText confidence="0.8833975">
In this section, we introduce the LSTM neural net-
work for Chinese word segmentation.
</bodyText>
<page confidence="0.997086">
1198
</page>
<figureCaption confidence="0.9780765">
Figure 1: General architecture of neural model for
Chinese word segmentation.
</figureCaption>
<subsectionHeader confidence="0.99879">
3.1 Character Embeddings
</subsectionHeader>
<bodyText confidence="0.99979695">
The first step of using neural network to process
symbolic data is to represent them into distributed
vectors, also called embeddings (Bengio et al.,
2003; Collobert and Weston, 2008).
Formally, in Chinese word segmentation task,
we have a character dictionary C of size JCJ. Un-
less otherwise specified, the character dictionary is
extracted from the training set and unknown char-
acters are mapped to a special symbol that is not
used elsewhere. Each character c E C is repre-
sented as a real-valued vector (character embed-
ding) v, E Rd where d is the dimensionality of the
vector space. The character embeddings are then
stacked into an embedding matrix M E Rd×|C|. For
a character c E C, the corresponding character em-
bedding v, E Rd is retrieved by the lookup table
layer. And the lookup table layer can be regarded
as a simple projection layer where the character
embedding for each context character is achieved
by table lookup operation according to its index.
</bodyText>
<subsectionHeader confidence="0.990019">
3.2 LSTM
</subsectionHeader>
<bodyText confidence="0.999831">
The long short term memory neural network
(LSTM) (Hochreiter and Schmidhuber,1997) is an
extension of the recurrent neural network (RNN).
The RNN has recurrent hidden states whose
output at each time is dependent on that of the
previous time. More formally, given a sequence
</bodyText>
<equation confidence="0.987492">
x(1:n) = (x(1), x(2), ... , x(t), ... , x(n)), the RNN
updates its recurrent hidden state h(t) by
h(t) = g(Uh(t−1) + Wx(t) + b), (3)
</equation>
<bodyText confidence="0.999869783783784">
where g is a nonlinear function as mentioned
above.
Though RNN has been proven successful on
many tasks such as speech recognition (Vinyals
et al., 2012), language modeling (Mikolov et al.,
2010) and text generation (Sutskever et al., 2011),
it can be difficult to train them to learn long-
term dynamics, likely due in part to the vanishing
and exploding gradient problem (Hochreiter and
Schmidhuber, 1997).
The LSTM provides a solution by incorporating
memory units that allow the network to learn when
to forget previous information and when to update
the memory cells given new information. Thus, it
is a natural choice to apply LSTM neural network
to word segmentation task since the LSTM neural
network can learn from data with long range tem-
poral dependencies (memory) due to the consider-
able time lag between the inputs and their corre-
sponding outputs. In addition, the LSTM has been
applied successfully in many NLP tasks, such as
text classification (Liu et al., 2015) and machine
translation (Sutskever et al., 2014).
The core of the LSTM model is a memory cell
c encoding memory at every time step of what in-
puts have been observed up to this step (see Figure
2) . The behavior of the cell is controlled by three
“gates”, namely input gate i, forget gate f and out-
put gate o. The operations on gates are defined as
element-wise multiplications, thus gate can either
scale the input value if the gate is non-zero vector
or omit input if the gate is zero vector. The output
of output gate will be fed into the next time step
t + 1 as previous hidden state and input of upper
layer of neural network at current time step t. The
definitions of the gates, cell update and output are
as follows:
</bodyText>
<equation confidence="0.999824666666667">
i(t) = Q(Wixx(t) + Wihh(t−1) + Wicc(t−1)), (4)
f(t) = Q(Wfxx(t) + Wfhh(t−1) + Wfcc(t−1)), (5)
c(t) = f(t) ⊙ c(t−1) + i(t) ⊙ O(Wcxx(t) + Wchh(t−1)),
(6)
o(t) = Q(Woxx(t) + Wohh(t−1) + Wocc(t)), (7)
h(t) = o(t) ⊙ O(c(t)), (8)
</equation>
<figure confidence="0.999258540540541">
Input Window
Characters
1
Lookup Table
2
3
4
5
6
d-1
d
Concatenate
Linear
x(t)
z(t) = W1 × x(t) + b1
...
h(t) =σ(z(t))
Sigmoid
h(t)
...
...
...
...
...
...
...
Linear
it) = W2 × h(t) + b2
c(t-2) c(t-1) c(t) c(t+1) c(t+2)
Tag Inference
B
M
S
E
J1) J2) it) in-1) in)
it)
A
</figure>
<page confidence="0.842864">
1199
</page>
<figureCaption confidence="0.871912">
Figure 2: LSTM Memory Unit. The memory unit
</figureCaption>
<bodyText confidence="0.98779415">
contains a cell c which is controlled by three gates.
The green links show the signals at time t − 1,
while the black links show the current signals. The
dashed links represent the weight matrices from
beginning to end are diagonal. Moreover, the solid
pointers mean there are weight matrices on the
connections, and hollow pointers mean none. The
current output signal, h(t), will fed back to the next
time t + 1 via three gates, and is the input of the
higher layer of the neural network as well.
where Q and ϕ are the logistic sigmoid function and
hyperbolic tangent function respectively; i(t), f(t),
o(t) and c(t) are respectively the input gate, forget
gate, output gate, and memory cell activation vec-
tor at time step t, all of which have the same size as
the hidden vector h(t) E RH2; the parameter ma-
trices Ws with different subscripts are all square
matrices; O denotes the element-wise product of
the vectors. Note that Wic, Wfc and Woc are di-
agonal matrices.
</bodyText>
<subsectionHeader confidence="0.985078">
3.3 LSTM Architectures for Chinese Word
Segmentation
</subsectionHeader>
<bodyText confidence="0.9970368">
To fully utilize the LSTM, we propose four differ-
ent structures of neural network to select the effec-
tive features via memory units. Figure 3 illustrates
our proposed architectures.
LSTM-1 The LSTM-1 simply replace the hid-
den neurons in Eq. (1) with LSTM units (See Fig-
ure 3a).
The input of the LSTM unit is from a window of
context characters. For each character, c(t), (1 &lt;
t &lt; n), the input of the LSTM unit x(t),
</bodyText>
<equation confidence="0.9968715">
x(t) = v(t−k1) c® · · ·® v(t+k2)
c , (9)
</equation>
<bodyText confidence="0.999090857142857">
is concatenated from character embeddings of
c(t−k1):(t+k2), where k1 and k2 represent the num-
bers of characters from left and right contexts re-
spectively. The output of the LSTM unit is used
in final inference function (Eq. (11) ) after a linear
transformation.
LSTM-2 The LSTM-2 can be created by stack-
ing multiple LSTM hidden layers on top of each
other, with the output sequence of one layer form-
ing the input sequence for the next (See Figure 3b).
Here we use two LSTM layers. Specifically, input
of the upper LSTM layer takes h(t) from the lower
LSTM layer without any transformation. The in-
put of the first layer is same to LSTM-1, and the
output of the second layer is as same operation as
LSTM-1.
LSTM-3 The LSTM-3 is a extension of LSTM-
1, which adopts a local context of LSTM layer as
input of the last layer (See Figure 3c). For each
time step t, we concatenate the outputs of a win-
dow of the LSTM layer into a vector h(t),
</bodyText>
<equation confidence="0.972072">
h(t) = h(t−m1) ® ··· ® h(t+m2), (10)
</equation>
<bodyText confidence="0.850493">
where m1 and m2 represent the lengths of time lags
</bodyText>
<equation confidence="0.703912">
�h(t) is
</equation>
<bodyText confidence="0.995243777777778">
before and after current time step.Finally,
used in final inference function (Eq. (11) ) after a
linear transformation.
LSTM-4 The LSTM-4 (see Figure 3d) is a mix-
ture of the LSTM-2 and LSTM-3, which consists
of two LSTM layers. The output sequence of the
lower LSTM layer forms the input sequence of the
upper LSTM layer. The final layer adopts a local
context of upper LSTM layer as input.
</bodyText>
<subsectionHeader confidence="0.876411">
3.4 Inference at Sentence Level
</subsectionHeader>
<bodyText confidence="0.999955625">
To model the tag dependency, previous neural net-
work models (Collobert et al., 2011; Zheng et al.,
2013; Pei et al., 2014) introduced the transition
score Aij for measuring the probability of jump-
ing from tag i E T to tag j E T. For a input sen-
tence c(1:n) with a tag sequence y(1:n), a sentence-
level score is then given by the sum of tag transi-
tion scores and network tagging scores:
</bodyText>
<figure confidence="0.999621017857143">
Output
Gate o
Input
Gate i
σ
σ
h(t-1)
h(t-1)
c(t)
c(t-1)
o(t)
i(t)
x(t)
φ
φ
c
h(t-1)
h(t)
c(t)
c(t-1)
c(t-1)
LSTM
Memory Unit
f(t)
h(t-1)
Forget
Gate f
σ
1200
LSTM
y(t-1) y(t) y(t+1)
x(t-1) x(t) x(t+1)
LSTM
LSTM
LSTM
LSTM
y(o-1) y(o) y(o+1)
x(o-1) x(o) x(o+1)
LSTM
LSTM
LSTM
LSTM
LSTM
y(t-1) y(t) y(t+1)
x(t-1) x(t) x(t+1)
LSTM
LSTM
LSTM
LSTM
y(t-1) y(t) y(t+1)
x(t-1) x(t) x(t+1)
LSTM
LSTM
LSTM
LSTM
(a) LSTM-1 (b) LSTM-2 (c) LSTM-3 (d) LSTM-4
</figure>
<figureCaption confidence="0.96834">
Figure 3: Our proposed LSTM architectures for Chinese word segmentation.
</figureCaption>
<figure confidence="0.7735895">
a l2-norm term:
( )
M )
Ay(t−1)y(t) + y,(t) (11
</figure>
<bodyText confidence="0.897346666666667">
where y(t)
y(�) indicates the score of tag y(t),
and y(t) is computed by the network as in
Eq. (2). The parameter set of our model θ =
JM, A, Wic, Wfc, Woc, Wix, Wfx, Wox, Wih, Wfh,
Woh, Wcx, Wch}.
</bodyText>
<sectionHeader confidence="0.999562" genericHeader="method">
4 Training
</sectionHeader>
<subsectionHeader confidence="0.993526">
4.1 Max-Margin criterion
</subsectionHeader>
<bodyText confidence="0.999956727272727">
We use the Max-Margin criterion to train our
model. Intuitively, the Max-Margin criterion pro-
vides an alternative to probabilistic, likelihood
based estimation methods by concentrating di-
rectly on the robustness of the decision boundary
of a model (Taskar et al., 2005). We use Y (xi) to
denote the set of all possible tag sequences for a
given sentence xi and the correct tag sequence for
xi is yi. The parameter set of our model is θ. We
first define a structured margin loss ∆(yi, y) for
predicted tag sequence y:
</bodyText>
<equation confidence="0.997808">
η1Jy(t)
i =� y(t)}, (12)
</equation>
<bodyText confidence="0.999891166666667">
where n is the length of sentence xi and η is a dis-
count parameter. The loss is proportional to the
number of characters with incorrect tags in the pro-
posed tag sequence. For a given training instance
(xi, yi),the predicted tag sequence yi E Y (xi) is
the one with the highest score:
</bodyText>
<equation confidence="0.9771265">
yi = arg max s(xi, y, θ), (13)
yEY(xi)
</equation>
<bodyText confidence="0.99979275">
where the function s(·) is sentence-level score and
defined in equation (11).
Given a set of training set D, the regularized ob-
jective function is the loss function J(θ) including
</bodyText>
<equation confidence="0.996502">
1 J(θ) =IDI 1:li(θ) + 211θ1122, (14)
(xi,yi)ED
</equation>
<bodyText confidence="0.980182727272727">
where li(θ) = max(0, s(xi, yi, θ) + ∆(yi, yi) −
s(xi, yi, θ)).
To minimize J(θ), we use a generalization
of gradient descent called subgradient method
(Ratliff et al., 2007) which computes a gradient-
like direction.
Following (Socher et al., 2013), we also use the
diagonal variant of AdaGrad (Duchi et al., 2011)
with minibatchs to minimize the objective. The
parameter update for the i-th parameter θt,i at time
step t is as follows:
</bodyText>
<equation confidence="0.9295045">
α
θt,i = θt−1,i − t 2 gt,i, ( 15
)
�Eτ=1 gτ,i
</equation>
<bodyText confidence="0.99911225">
where α is the initial learning rate and gτ E R|θi|
is the subgradient at time step τ for parameter θi.
In addition, the process of back propagation is fol-
lowd Hochreiter and Schmidhuber (1997).
</bodyText>
<subsectionHeader confidence="0.88173">
4.2 Dropout
</subsectionHeader>
<bodyText confidence="0.999924125">
Dropout is one of prevalent methods to avoid over-
fitting in neural networks (Srivastava et al., 2014).
When dropping a unit out, we temporarily remove
it from the network, along with all its incoming and
outgoing connections. In the simplest case, each
unit is omitted with a fixed probability p indepen-
dent of other units, namely dropout rate, where p
is also chosen on development set.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.964953">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9999435">
We use three popular datasets, PKU, MSRA
and CTB6, to evaluate our model. The PKU
</bodyText>
<equation confidence="0.996118">
n
s(c(1:n), y(1:n), B) _ ∑
�=1
n
∆(yi, y) =
t
</equation>
<page confidence="0.990027">
1201
</page>
<figure confidence="0.998922361111111">
F-value(%)
F-value(%)
F-value(%)
epoches
(a) LSTM-1(2,2)
epoches
(b) LSTM-1 (1,2)
epoches
(c) LSTM-1(0,2)
0 10 20 30
96
94
92
90
88
Dropout Rate=20%
Dropout Rate=50%
without Dropout
0 10 20 30
96
94
92
90
88
Dropout Rate=20%
Dropout Rate=50%
without Dropout
0 10 20 30
96
94
92
90
88
Dropout Rate=20%
Dropout Rate=50%
without Dropout
</figure>
<figureCaption confidence="0.997262">
Figure 4: Performances of LSTM-1 with the different context lengths and dropout rates on PKU devel-
opment set.
</figureCaption>
<table confidence="0.81847375">
Context length (k1, k2) = (0, 2)
Character embedding size d = 100
Hidden unit number H2 = 150
Initial learning rate α = 0.2
Margin loss discount � = 0.2
Regularization A = 10−4
Dropout rate on input layer p = 0.2
epoches
</table>
<figureCaption confidence="0.974267">
Figure 5: Performances of LSTM-1 (0,2) with
20% dropout on PKU development set.
</figureCaption>
<bodyText confidence="0.99975505">
and MSRA data are provided by the second In-
ternational Chinese Word Segmentation Bakeoff
(Emerson, 2005), and CTB6 is from Chinese Tree-
Bank 6.0 (LDC2007T36) (Xue et al., 2005), which
is a segmented, part-of-speech tagged and fully
bracketed corpus in the constituency formalism.
These datasets are commonly used by previous
state-of-the-art models and neural network mod-
els. In addition, we use the first 90% sentences of
the training data as training set and the rest 10%
sentences as development set for PKU and MSRA
datasets. For CTB6 dataset, we divide the training,
development and test sets according to (Yang and
Xue, 2012)
All datasets are preprocessed by replacing the
Chinese idioms and the continuous English char-
acters and digits with a unique flag.
For evaluation, we use the standard bake-off
scoring program to calculate precision, recall, F1-
score and out-of-vocabulary (OOV) word recall.
</bodyText>
<subsectionHeader confidence="0.999337">
5.2 Hyper-parameters
</subsectionHeader>
<bodyText confidence="0.999983133333333">
Hyper-parameters of neural model impact the per-
formance of the algorithm significantly. Accord-
ing to experiment results, we choose the hyper-
parameters of our model as showing in Figure
1. The minibatch size is set to 20. Generally,
the number of hidden units has a limited impact
on the performance as long as it is large enough.
We found that 150 is a good trade-off between
speed and model performance. The dimension-
ality of character embedding is set to 100 which
achieved the best performance. All these hyper-
parameters are chosen according to their average
performances on three development sets.
For the context lengths (k1, k2) and dropout
strategy, we give detailed analysis in next section.
</bodyText>
<subsectionHeader confidence="0.996076">
5.3 Dropout and Context Length
</subsectionHeader>
<bodyText confidence="0.99876625">
We first investigate the different dropout strate-
gies, including dropout at different layers and with
different dropout rate p. As a result, we found that
it is a good trade-off between speed and model per-
formance to drop the input layer only with dropout
rate ping,,t = 0.2. However, it does not show
any significant improvement to dropout on hidden
LSTM layers.
</bodyText>
<tableCaption confidence="0.933">
Table 1: Settings of the hyper-parameters.
</tableCaption>
<figure confidence="0.979055">
0 20 40 60
F-value(%) 95
90
85
80
LSTM-1 LSTM-2
LSTM-3 LSTM-4
</figure>
<page confidence="0.953981">
1202
</page>
<table confidence="0.9998626">
Context Length Dropout rate=20% Dropout rate=50% without Dropout
P R F P R F P R F
LSTM-1 (2,2) 95.8 95.3 95.6 94.8 94.4 94.6 95.2 94.9 95.1
LSTM-1 (1,2) 95.7 95.3 95.5 94.8 94.4 94.6 95.4 94.9 95.2
LSTM-1 (0,2) 95.8 95.5 95.7 94.6 94.2 94.4 95.4 95.0 95.2
</table>
<tableCaption confidence="0.954502">
Table 2: Performances of LSTM-1 with the different context lengths and dropout rates on PKU test set.
</tableCaption>
<table confidence="0.998991833333333">
models Contextr Length = (0,2)
P R F
LSTM-1 95.8 95.5 95.7
LSTM-2 95.1 94.5 94.8
LSTM-3 89.1 90.4 89.8
LSTM-4 92.1 91.7 91.9
</table>
<tableCaption confidence="0.996958">
Table 3: Performance on our four proposed models
on PKU test set.
</tableCaption>
<bodyText confidence="0.999587333333333">
Due to space constraints, we just give the per-
formances of LSTM-1 model on PKU dataset with
different context lengths (k1, k2) and dropout rates
in Figure 4 and Table 2. From Figure 4, we can see
that 20% dropout converges slightly slower than
the one without dropout, but avoids overfitting.
50% or higher dropout rate seems to be underfit-
ting since its training error is also high.
Table 2 shows that the LSTM-1 model performs
consistently well with the different context length,
but the LSTM-1 model with short context length
saves computational resource, and gets more ef-
ficiency. At the meanwhile, the LSTM-1 model
with context length (0,2) can receive the same or
better performance than that with context length
(2,2), which shows that the LSTM model can well
model the pervious information, and it is more ro-
bust for its insensitivity of window size variation.
We employ context length (0,2) with the 20%
dropout rate in the following experiments to bal-
ance the tradeoff between accuracy and efficiency.
</bodyText>
<subsectionHeader confidence="0.998995">
5.4 Model Selection
</subsectionHeader>
<bodyText confidence="0.999989380952381">
We also evaluate the our four proposed models
with the hyper-parameter settings in Table 1. For
LSTM-3 and LSTM-4 models, the context win-
dow length of top LSTM layer is set to (2,0). For
LSTM-2 and LSTM-4,the number of upper hidden
LSTM layer is set to 100. We use PKU dataset to
select the best model. Figure 5 shows the results of
the four models on PKU development set from first
epoch to 60-th epoch. We see that the LSTM-1 is
the fastest one to converge and achieves the best
performance. The LSTM-2 (two LSTM layers)
get worse, which shows the performance seems
not to benefit from deep model. The LSTM-3 and
LSTM-4 models do not converge, which could be
caused by the complexity of models.
The results on PKU test set are also shown in Ta-
ble 3, which again show that the LSTM-1 achieves
the best performance. Therefore, in the rest of
the paper we will give more analysis based on the
LSTM-1 with hyper-parameter settings as showing
in Table 1.
</bodyText>
<subsectionHeader confidence="0.991521">
5.5 Experiment Results
</subsectionHeader>
<bodyText confidence="0.999894">
In this section, we give comparisons of the LSTM-
1 with pervious neural models and state-of-the-art
methods on the PKU, MSRA and CTB6 datasets.
We first compare our model with two neural
models (Zheng et al., 2013; Pei et al., 2014) on
Chinese word segmentation task with random ini-
tialized character embeddings. As showing in Ta-
ble 4, the performance is boosted significantly by
utilizing LSTM unit. And more notably, our win-
dow size of the context characters is set to (0,2),
while the size of the other models is (2,2).
Previous works found that the performance can
be improved by pre-training the character embed-
dings on large unlabeled data. We use word2vec
1 (Mikolov et al., 2013a) toolkit to pre-train the
character embeddings on the Chinese Wikipedia
corpus. The obtained embeddings are used to ini-
tialize the character lookup table instead of random
initialization. Inspired by (Pei et al., 2014), we also
utilize bigram character embeddings which is sim-
ply initialized as the average of embeddings of two
consecutive characters.
Table 5 shows the performances with addi-
tional pre-trained and bigram character embed-
dings. Again, the performances boost significantly
as a result. Moreover, when we use bigram embed-
dings only, which means we do close test without
pre-training the embeddings on other extra corpus,
our model still perform competitively compared
</bodyText>
<footnote confidence="0.99048">
1http://code.google.com/p/word2vec/
</footnote>
<page confidence="0.73178">
1203
</page>
<table confidence="0.999704">
models PKU MSRA CTB6
P R F P R F P R F
(Zheng et al., 2013) 92.8 92.0 92.4 92.9 93.6 93.3 94.0* 93.1* 93.6*
(Pei et al., 2014) 93.7 93.4 93.5 94.6 94.2 94.4 94.4* 93.4* 93.9*
LSTM 95.8 95.5 95.7 96.7 96.2 96.4 95.0 94.8 94.9
</table>
<tableCaption confidence="0.9334855">
Table 4: Performances on three test sets with random initialized character embeddings. The results with
* symbol are from our implementations of their methods.
</tableCaption>
<table confidence="0.999917090909091">
models PKU MSRA CTB6
P R F P R F P R F
+Pre-train
(Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7*
(Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0*
LSTM 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7
+bigram
LSTM 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5
+Pre-train+bigram
(Pei et al., 2014) - - 95.2 - - 97.2 - - -
LSTM 96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0
</table>
<tableCaption confidence="0.988112">
Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results
with * symbol are from our implementations of their methods.
</tableCaption>
<table confidence="0.999895166666667">
Models PKU MSRA CTB6
(Tseng et al., 2005) 95.0 96.4 -
(Zhang and Clark, 2007) 95.1 97.2 -
(Sun and Xu, 2011) - - 95.7
(Zhang et al., 2013) 96.1 97.4 -
This work 96.5 97.4 96.0
</table>
<tableCaption confidence="0.9614875">
Table 6: Comparison of our model with state-of-
the-art methods on three test sets.
</tableCaption>
<bodyText confidence="0.990760714285714">
with previous neural models with pre-trained em-
bedding and bigram embeddings.
Table 6 lists the performances of our model as
well as previous state-of-the-art systems. (Zhang
and Clark, 2007) is a word-based segmentation
algorithm, which exploit features of complete
words, while the rest of the list are character-based
word segmenters, whose features are mostly ex-
tracted from a window of characters. Moreover,
some systems (such as Sun and Xu (2011) and
Zhang et al. (2013)) also exploit kinds of extra in-
formation such as unlabeled data or other knowl-
edge. Despite our model only uses simple bigram
features, it outperforms previous state-of-the-art
models which use more complex features.
Since that we do not focus on the speed of the al-
gorithm in this paper, we do not optimize the speed
a lot. On PKU dataset, it takes about 3 days to train
the model (last row of Table 5) using CPU (Intel(R)
Xeon(R) CPU E5-2665 @ 2.40GHz) only. All im-
plementation is based on Python.
</bodyText>
<sectionHeader confidence="0.999871" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999693125">
Chinese word segmentation has been studied with
considerable efforts in the NLP community. The
most popular word segmentation methods is based
on sequence labeling (Xue, 2003). Recently, re-
searchers have tended to explore neural network
based approaches (Collobert et al., 2011) to re-
duce efforts of feature engineering (Zheng et al.,
2013; Pei et al., 2014; Qi et al., 2014; Chen et al.,
2015). The features of all these methods are ex-
tracted from a local context and neglect the long
distance information. However, previous informa-
tion is also crucial for word segmentation. Our
model adopts the LSTM to keep the previous im-
portant information in memory and avoids the lim-
itation of ambiguity caused by limit of the size of
context window.
</bodyText>
<sectionHeader confidence="0.993542" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999757666666667">
In this paper, we use LSTM to explicitly model the
previous information for Chinese word segmen-
tation, which can well model the potential long-
</bodyText>
<page confidence="0.986184">
1204
</page>
<bodyText confidence="0.999924916666667">
distance features. Though our model use smaller
context window size (0,2), it still outperforms the
previous neural models with context window size
(2,2). Besides, our model can also be easily gener-
alized and applied to other sequence labeling tasks.
Although our model achieves state-of-the-art
performance, it only makes use of previous con-
text. The future context is also useful for Chi-
nese word segmentation. In future work, we would
like to adopt the bidirectional recurrent neural net-
work (Schuster and Paliwal, 1997) to process the
sequence in both directions.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999936375">
We would like to thank the anonymous review-
ers for their valuable comments. This work was
partially funded by the National Natural Science
Foundation of China (61472088, 61473092), Na-
tional High Technology Research and Develop-
ment Program of China (2015AA015408), Shang-
hai Science and Technology Development Funds
(14ZR1403200).
</bodyText>
<sectionHeader confidence="0.998922" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999193341772152">
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal ofMachine Learning Re-
search, 3:1137–1155.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39–71.
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015. Gated recursive neural network for
Chinese word segmentation. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings ofICML.
Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal ofMachine Learning Research,
12:2493–2537.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal ofMachine
Learning Research, 12:2121–2159.
T. Emerson. 2005. The second international Chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, pages 123–133. Jeju Island, Korea.
Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning.
PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu,
and Xuanjing Huang. 2015. Multi-timescale long
short-term memory neural network for modelling
sentences and documents. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.
Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings ofACL.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. Proceedings of the 20th inter-
national conference on Computational Linguistics.
Yanjun Qi, Sujatha G Das, Ronan Collobert, and Jason
Weston. 2014. Deep learning for character-based
information extraction. In Advances in Information
Retrieval, pages 668–674. Springer.
Nathan D Ratliff, J Andrew Bagnell, and Martin A
Zinkevich. 2007. (online) subgradient methods
for structured prediction. In Eleventh International
Conference on Artificial Intelligence and Statistics
(AIStats).
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
</reference>
<page confidence="0.776069">
1205
</page>
<reference confidence="0.999799466666667">
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970–979.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017–1024.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems, pages 3104–3112.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing, volume
171.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
Oriol Vinyals, Suman V Ravuri, and Daniel Povey.
2012. Revisiting recurrent neural networks for ro-
bust asr. In Acoustics, Speech and Signal Processing
(ICASSP), 2012 IEEE International Conference on,
pages 4085–4088. IEEE.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural lan-
guage engineering, 11(2):207–238.
N. Xue. 2003. Chinese word segmentation as charac-
ter tagging. Computational Linguistics and Chinese
Language Processing, 8(1):29–48.
Yaqin Yang and Nianwen Xue. 2012. Chinese comma
disambiguation for discourse analysis. In Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-
Volume 1, pages 786–794. Association for Compu-
tational Linguistics.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
ACL.
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.
Deep learning for chinese word segmentation and
pos tagging. In EMNLP, pages 647–657.
</reference>
<page confidence="0.990325">
1206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.699740">
<title confidence="0.998473">Long Short-Term Memory Neural for Chinese Word Segmentation</title>
<author confidence="0.8688545">Xipeng Zhu Chen</author>
<author confidence="0.8688545">Pengfei Liu</author>
<author confidence="0.8688545">Xuanjing Shanghai Key Laboratory of Intelligent Information Processing</author>
<author confidence="0.8688545">Fudan</author>
<affiliation confidence="0.997788">School of Computer Science, Fudan</affiliation>
<address confidence="0.997857">825 Zhangheng Road, Shanghai,</address>
<email confidence="0.97058">xinchichen13@fudan.edu.cn</email>
<email confidence="0.97058">xpqiu@fudan.edu.cn</email>
<email confidence="0.97058">czhu13@fudan.edu.cn</email>
<email confidence="0.97058">pfliu14@fudan.edu.cn</email>
<email confidence="0.97058">xjhuang@fudan.edu.cn</email>
<abstract confidence="0.998941444444445">Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features are mostly extracted from a local context. These methods cannot utilize the long distance information which is also crucial for word segmentation. In this paper, we propose a novel neural network model for Chinese word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Réjean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal ofMachine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2063" citStr="Bengio et al., 2003" startWordPosition="316" endWordPosition="319">ed learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (G</context>
<context position="8550" citStr="Bengio et al., 2003" startWordPosition="1392" endWordPosition="1395">l works well for Chinese word segmentation and other sequence labeling tasks, it just utilizes the information of context of a limited-length window. Some useful long distance information is neglected. 3 Long Short-Term Memory Neural Network for Chinese Word Segmentation In this section, we introduce the LSTM neural network for Chinese word segmentation. 1198 Figure 1: General architecture of neural model for Chinese word segmentation. 3.1 Character Embeddings The first step of using neural network to process symbolic data is to represent them into distributed vectors, also called embeddings (Bengio et al., 2003; Collobert and Weston, 2008). Formally, in Chinese word segmentation task, we have a character dictionary C of size JCJ. Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each character c E C is represented as a real-valued vector (character embedding) v, E Rd where d is the dimensionality of the vector space. The character embeddings are then stacked into an embedding matrix M E Rd×|C|. For a character c E C, the corresponding character embedding v, E Rd is retrieved by the</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal ofMachine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="1516" citStr="Berger et al., 1996" startWordPosition="222" endWordPosition="225">ments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinchi Chen</author>
<author>Xipeng Qiu</author>
<author>Chenxi Zhu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Gated recursive neural network for Chinese word segmentation.</title>
<date>2015</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2618" citStr="Chen et al. (2015)" startWordPosition="405" endWordPosition="408">urian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the context window. Intuitively, many words are difficult to segment based on the local information only. For example, the segmentation of the follow</context>
<context position="27915" citStr="Chen et al., 2015" startWordPosition="4816" endWordPosition="4819"> do not optimize the speed a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation methods is based on sequence labeling (Xue, 2003). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015). The features of all these methods are extracted from a local context and neglect the long distance information. However, previous information is also crucial for word segmentation. Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window. 7 Conclusion In this paper, we use LSTM to explicitly model the previous information for Chinese word segmentation, which can well model the potential long1204 distance features. Though our model use smaller context window size (0,2), it still outperfo</context>
</contexts>
<marker>Chen, Qiu, Zhu, Huang, 2015</marker>
<rawString>Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing Huang. 2015. Gated recursive neural network for Chinese word segmentation. In Proceedings of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="8579" citStr="Collobert and Weston, 2008" startWordPosition="1396" endWordPosition="1399">ese word segmentation and other sequence labeling tasks, it just utilizes the information of context of a limited-length window. Some useful long distance information is neglected. 3 Long Short-Term Memory Neural Network for Chinese Word Segmentation In this section, we introduce the LSTM neural network for Chinese word segmentation. 1198 Figure 1: General architecture of neural model for Chinese word segmentation. 3.1 Character Embeddings The first step of using neural network to process symbolic data is to represent them into distributed vectors, also called embeddings (Bengio et al., 2003; Collobert and Weston, 2008). Formally, in Chinese word segmentation task, we have a character dictionary C of size JCJ. Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each character c E C is represented as a real-valued vector (character embedding) v, E Rd where d is the dimensionality of the vector space. The character embeddings are then stacked into an embedding matrix M E Rd×|C|. For a character c E C, the corresponding character embedding v, E Rd is retrieved by the lookup table layer. And the </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Léon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<booktitle>The Journal ofMachine Learning Research,</booktitle>
<pages>12--2493</pages>
<contexts>
<context position="2088" citStr="Collobert et al. (2011)" startWordPosition="320" endWordPosition="323"> such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model </context>
<context position="7911" citStr="Collobert et al., 2011" startWordPosition="1294" endWordPosition="1297">1), (1) where W1 E RH2×H1, b1 E RH2, h(t) E RH2. H2 is a hyper-parameter which indicates the number of hidden units in layer 2. Given a set oftags T of size |T |, a similar linear transformation is performed except that no non-linear function is followed: y(t) = W2h(t) + b2, (2) where W2 E R|T |×H2, b2 E R|T |. y(t) E R|T |is the score vector for each possible tag. In Chinese word segmentation, the most prevalent tag set T C T is {B, M, E, S} as mentioned above. To model the tag dependency, a transition score AZA is introduced to measure the probability of jumping from tag i E T to tag j E T (Collobert et al., 2011). Although this model works well for Chinese word segmentation and other sequence labeling tasks, it just utilizes the information of context of a limited-length window. Some useful long distance information is neglected. 3 Long Short-Term Memory Neural Network for Chinese Word Segmentation In this section, we introduce the LSTM neural network for Chinese word segmentation. 1198 Figure 1: General architecture of neural model for Chinese word segmentation. 3.1 Character Embeddings The first step of using neural network to process symbolic data is to represent them into distributed vectors, also</context>
<context position="15043" citStr="Collobert et al., 2011" startWordPosition="2565" endWordPosition="2568">nto a vector h(t), h(t) = h(t−m1) ® ··· ® h(t+m2), (10) where m1 and m2 represent the lengths of time lags �h(t) is before and after current time step.Finally, used in final inference function (Eq. (11) ) after a linear transformation. LSTM-4 The LSTM-4 (see Figure 3d) is a mixture of the LSTM-2 and LSTM-3, which consists of two LSTM layers. The output sequence of the lower LSTM layer forms the input sequence of the upper LSTM layer. The final layer adopts a local context of upper LSTM layer as input. 3.4 Inference at Sentence Level To model the tag dependency, previous neural network models (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014) introduced the transition score Aij for measuring the probability of jumping from tag i E T to tag j E T. For a input sentence c(1:n) with a tag sequence y(1:n), a sentencelevel score is then given by the sum of tag transition scores and network tagging scores: Output Gate o Input Gate i σ σ h(t-1) h(t-1) c(t) c(t-1) o(t) i(t) x(t) φ φ c h(t-1) h(t) c(t) c(t-1) c(t-1) LSTM Memory Unit f(t) h(t-1) Forget Gate f σ 1200 LSTM y(t-1) y(t) y(t+1) x(t-1) x(t) x(t+1) LSTM LSTM LSTM LSTM y(o-1) y(o) y(o+1) x(o-1) x(o) x(o+1) LSTM LSTM LSTM LSTM LSTM y(t-1) y(t) y</context>
<context position="27799" citStr="Collobert et al., 2011" startWordPosition="4793" endWordPosition="4796">he-art models which use more complex features. Since that we do not focus on the speed of the algorithm in this paper, we do not optimize the speed a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation methods is based on sequence labeling (Xue, 2003). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015). The features of all these methods are extracted from a local context and neglect the long distance information. However, previous information is also crucial for word segmentation. Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window. 7 Conclusion In this paper, we use LSTM to explicitly model the previous information for Chinese word segmentation, which can well model</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal ofMachine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<booktitle>The Journal ofMachine Learning Research,</booktitle>
<pages>12--2121</pages>
<contexts>
<context position="17549" citStr="Duchi et al., 2011" startWordPosition="3025" endWordPosition="3028"> (xi) is the one with the highest score: yi = arg max s(xi, y, θ), (13) yEY(xi) where the function s(·) is sentence-level score and defined in equation (11). Given a set of training set D, the regularized objective function is the loss function J(θ) including 1 J(θ) =IDI 1:li(θ) + 211θ1122, (14) (xi,yi)ED where li(θ) = max(0, s(xi, yi, θ) + ∆(yi, yi) − s(xi, yi, θ)). To minimize J(θ), we use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradientlike direction. Following (Socher et al., 2013), we also use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. The parameter update for the i-th parameter θt,i at time step t is as follows: α θt,i = θt−1,i − t 2 gt,i, ( 15 ) �Eτ=1 gτ,i where α is the initial learning rate and gτ E R|θi| is the subgradient at time step τ for parameter θi. In addition, the process of back propagation is followd Hochreiter and Schmidhuber (1997). 4.2 Dropout Dropout is one of prevalent methods to avoid overfitting in neural networks (Srivastava et al., 2014). When dropping a unit out, we temporarily remove it from the network, along with all its incoming and outgoing connections</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal ofMachine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Emerson</author>
</authors>
<title>The second international Chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>123--133</pages>
<location>Jeju Island,</location>
<contexts>
<context position="19322" citStr="Emerson, 2005" startWordPosition="3345" endWordPosition="3346">te=50% without Dropout 0 10 20 30 96 94 92 90 88 Dropout Rate=20% Dropout Rate=50% without Dropout Figure 4: Performances of LSTM-1 with the different context lengths and dropout rates on PKU development set. Context length (k1, k2) = (0, 2) Character embedding size d = 100 Hidden unit number H2 = 150 Initial learning rate α = 0.2 Margin loss discount � = 0.2 Regularization A = 10−4 Dropout rate on input layer p = 0.2 epoches Figure 5: Performances of LSTM-1 (0,2) with 20% dropout on PKU development set. and MSRA data are provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36) (Xue et al., 2005), which is a segmented, part-of-speech tagged and fully bracketed corpus in the constituency formalism. These datasets are commonly used by previous state-of-the-art models and neural network models. In addition, we use the first 90% sentences of the training data as training set and the rest 10% sentences as development set for PKU and MSRA datasets. For CTB6 dataset, we divide the training, development and test sets according to (Yang and Xue, 2012) All datasets are preprocessed by replacing the Chinese idioms and the con</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>T. Emerson. 2005. The second international Chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 123–133. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>Jürgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="4424" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="693" endWordPosition="697">en the window size is larger than 3. The reason is that the number of its parameters is so large that the trained network has 1197 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1197–1206, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. overfitted on training data. Therefore, it is necessary to capture the potential long-distance dependencies without increasing the size of the context window. In order to address this problem, we propose a neural model based on Long Short-Term Memory Neural Network (LSTM) (Hochreiter and Schmidhuber, 1997) that explicitly model the previous information by exploiting input, output and forget gates to decide how to utilize and update the memory of pervious information. Intuitively, if the LSTM unit detects an important feature from an input sequence at early stage, it easily carries this information (the existence of the feature) over a long distance, hence, capturing the potential useful long-distance information. We evaluate our model on three popular benchmark datasets (PKU, MSRA and CTB6), and the experimental results show that our model achieves the state-of-the-art performance with the smal</context>
<context position="10184" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1670" endWordPosition="1673">output at each time is dependent on that of the previous time. More formally, given a sequence x(1:n) = (x(1), x(2), ... , x(t), ... , x(n)), the RNN updates its recurrent hidden state h(t) by h(t) = g(Uh(t−1) + Wx(t) + b), (3) where g is a nonlinear function as mentioned above. Though RNN has been proven successful on many tasks such as speech recognition (Vinyals et al., 2012), language modeling (Mikolov et al., 2010) and text generation (Sutskever et al., 2011), it can be difficult to train them to learn longterm dynamics, likely due in part to the vanishing and exploding gradient problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs. In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification (Liu et al., 2015) and machine translation (Sut</context>
<context position="17911" citStr="Hochreiter and Schmidhuber (1997)" startWordPosition="3094" endWordPosition="3097"> yi) − s(xi, yi, θ)). To minimize J(θ), we use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradientlike direction. Following (Socher et al., 2013), we also use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. The parameter update for the i-th parameter θt,i at time step t is as follows: α θt,i = θt−1,i − t 2 gt,i, ( 15 ) �Eτ=1 gτ,i where α is the initial learning rate and gτ E R|θi| is the subgradient at time step τ for parameter θi. In addition, the process of back propagation is followd Hochreiter and Schmidhuber (1997). 4.2 Dropout Dropout is one of prevalent methods to avoid overfitting in neural networks (Srivastava et al., 2014). When dropping a unit out, we temporarily remove it from the network, along with all its incoming and outgoing connections. In the simplest case, each unit is omitted with a fixed probability p independent of other units, namely dropout rate, where p is also chosen on development set. 5 Experiments 5.1 Datasets We use three popular datasets, PKU, MSRA and CTB6, to evaluate our model. The PKU n s(c(1:n), y(1:n), B) _ ∑ �=1 n ∆(yi, y) = t 1201 F-value(%) F-value(%) F-value(%) epoch</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1576" citStr="Lafferty et al., 2001" startWordPosition="232" endWordPosition="235">ur model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PengFei Liu</author>
<author>Xipeng Qiu</author>
<author>Xinchi Chen</author>
<author>Shiyu Wu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Multi-timescale long short-term memory neural network for modelling sentences and documents.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="10755" citStr="Liu et al., 2015" startWordPosition="1766" endWordPosition="1769">ent problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs. In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification (Liu et al., 2015) and machine translation (Sutskever et al., 2014). The core of the LSTM model is a memory cell c encoding memory at every time step of what inputs have been observed up to this step (see Figure 2) . The behavior of the cell is controlled by three “gates”, namely input gate i, forget gate f and output gate o. The operations on gates are defined as element-wise multiplications, thus gate can either scale the input value if the gate is non-zero vector or omit input if the gate is zero vector. The output of output gate will be fed into the next time step t + 1 as previous hidden state and input of</context>
</contexts>
<marker>Liu, Qiu, Chen, Wu, Huang, 2015</marker>
<rawString>PengFei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu, and Xuanjing Huang. 2015. Multi-timescale long short-term memory neural network for modelling sentences and documents. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafiát</author>
<author>Lukas Burget</author>
<author>Jan Cernockỳ</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="9974" citStr="Mikolov et al., 2010" startWordPosition="1636" endWordPosition="1639">s index. 3.2 LSTM The long short term memory neural network (LSTM) (Hochreiter and Schmidhuber,1997) is an extension of the recurrent neural network (RNN). The RNN has recurrent hidden states whose output at each time is dependent on that of the previous time. More formally, given a sequence x(1:n) = (x(1), x(2), ... , x(t), ... , x(n)), the RNN updates its recurrent hidden state h(t) by h(t) = g(Uh(t−1) + Wx(t) + b), (3) where g is a nonlinear function as mentioned above. Though RNN has been proven successful on many tasks such as speech recognition (Vinyals et al., 2012), language modeling (Mikolov et al., 2010) and text generation (Sutskever et al., 2011), it can be difficult to train them to learn longterm dynamics, likely due in part to the vanishing and exploding gradient problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerab</context>
</contexts>
<marker>Mikolov, Karafiát, Burget, Cernockỳ, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="2040" citStr="Mikolov et al., 2013" startWordPosition="312" endWordPosition="315">e handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recu</context>
<context position="24592" citStr="Mikolov et al., 2013" startWordPosition="4245" endWordPosition="4248">eural models and state-of-the-art methods on the PKU, MSRA and CTB6 datasets. We first compare our model with two neural models (Zheng et al., 2013; Pei et al., 2014) on Chinese word segmentation task with random initialized character embeddings. As showing in Table 4, the performance is boosted significantly by utilizing LSTM unit. And more notably, our window size of the context characters is set to (0,2), while the size of the other models is (2,2). Previous works found that the performance can be improved by pre-training the character embeddings on large unlabeled data. We use word2vec 1 (Mikolov et al., 2013a) toolkit to pre-train the character embeddings on the Chinese Wikipedia corpus. The obtained embeddings are used to initialize the character lookup table instead of random initialization. Inspired by (Pei et al., 2014), we also utilize bigram character embeddings which is simply initialized as the average of embeddings of two consecutive characters. Table 5 shows the performances with additional pre-trained and bigram character embeddings. Again, the performances boost significantly as a result. Moreover, when we use bigram embeddings only, which means we do close test without pre-training t</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2040" citStr="Mikolov et al., 2013" startWordPosition="312" endWordPosition="315">e handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recu</context>
<context position="24592" citStr="Mikolov et al., 2013" startWordPosition="4245" endWordPosition="4248">eural models and state-of-the-art methods on the PKU, MSRA and CTB6 datasets. We first compare our model with two neural models (Zheng et al., 2013; Pei et al., 2014) on Chinese word segmentation task with random initialized character embeddings. As showing in Table 4, the performance is boosted significantly by utilizing LSTM unit. And more notably, our window size of the context characters is set to (0,2), while the size of the other models is (2,2). Previous works found that the performance can be improved by pre-training the character embeddings on large unlabeled data. We use word2vec 1 (Mikolov et al., 2013a) toolkit to pre-train the character embeddings on the Chinese Wikipedia corpus. The obtained embeddings are used to initialize the character lookup table instead of random initialization. Inspired by (Pei et al., 2014), we also utilize bigram character embeddings which is simply initialized as the average of embeddings of two consecutive characters. Table 5 shows the performances with additional pre-trained and bigram character embeddings. Again, the performances boost significantly as a result. Moreover, when we use bigram embeddings only, which means we do close test without pre-training t</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenzhe Pei</author>
<author>Tao Ge</author>
<author>Chang Baobao</author>
</authors>
<title>Maxmargin tensor neural network for chinese word segmentation.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2477" citStr="Pei et al. (2014)" startWordPosition="385" endWordPosition="388">tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the cont</context>
<context position="15082" citStr="Pei et al., 2014" startWordPosition="2573" endWordPosition="2576">t+m2), (10) where m1 and m2 represent the lengths of time lags �h(t) is before and after current time step.Finally, used in final inference function (Eq. (11) ) after a linear transformation. LSTM-4 The LSTM-4 (see Figure 3d) is a mixture of the LSTM-2 and LSTM-3, which consists of two LSTM layers. The output sequence of the lower LSTM layer forms the input sequence of the upper LSTM layer. The final layer adopts a local context of upper LSTM layer as input. 3.4 Inference at Sentence Level To model the tag dependency, previous neural network models (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014) introduced the transition score Aij for measuring the probability of jumping from tag i E T to tag j E T. For a input sentence c(1:n) with a tag sequence y(1:n), a sentencelevel score is then given by the sum of tag transition scores and network tagging scores: Output Gate o Input Gate i σ σ h(t-1) h(t-1) c(t) c(t-1) o(t) i(t) x(t) φ φ c h(t-1) h(t) c(t) c(t-1) c(t-1) LSTM Memory Unit f(t) h(t-1) Forget Gate f σ 1200 LSTM y(t-1) y(t) y(t+1) x(t-1) x(t) x(t+1) LSTM LSTM LSTM LSTM y(o-1) y(o) y(o+1) x(o-1) x(o) x(o+1) LSTM LSTM LSTM LSTM LSTM y(t-1) y(t) y(t+1) x(t-1) x(t) x(t+1) LSTM LSTM LSTM</context>
<context position="24138" citStr="Pei et al., 2014" startWordPosition="4168" endWordPosition="4171">eep model. The LSTM-3 and LSTM-4 models do not converge, which could be caused by the complexity of models. The results on PKU test set are also shown in Table 3, which again show that the LSTM-1 achieves the best performance. Therefore, in the rest of the paper we will give more analysis based on the LSTM-1 with hyper-parameter settings as showing in Table 1. 5.5 Experiment Results In this section, we give comparisons of the LSTM1 with pervious neural models and state-of-the-art methods on the PKU, MSRA and CTB6 datasets. We first compare our model with two neural models (Zheng et al., 2013; Pei et al., 2014) on Chinese word segmentation task with random initialized character embeddings. As showing in Table 4, the performance is boosted significantly by utilizing LSTM unit. And more notably, our window size of the context characters is set to (0,2), while the size of the other models is (2,2). Previous works found that the performance can be improved by pre-training the character embeddings on large unlabeled data. We use word2vec 1 (Mikolov et al., 2013a) toolkit to pre-train the character embeddings on the Chinese Wikipedia corpus. The obtained embeddings are used to initialize the character loo</context>
<context position="25443" citStr="Pei et al., 2014" startWordPosition="4380" endWordPosition="4383">ize bigram character embeddings which is simply initialized as the average of embeddings of two consecutive characters. Table 5 shows the performances with additional pre-trained and bigram character embeddings. Again, the performances boost significantly as a result. Moreover, when we use bigram embeddings only, which means we do close test without pre-training the embeddings on other extra corpus, our model still perform competitively compared 1http://code.google.com/p/word2vec/ 1203 models PKU MSRA CTB6 P R F P R F P R F (Zheng et al., 2013) 92.8 92.0 92.4 92.9 93.6 93.3 94.0* 93.1* 93.6* (Pei et al., 2014) 93.7 93.4 93.5 94.6 94.2 94.4 94.4* 93.4* 93.9* LSTM 95.8 95.5 95.7 96.7 96.2 96.4 95.0 94.8 94.9 Table 4: Performances on three test sets with random initialized character embeddings. The results with * symbol are from our implementations of their methods. models PKU MSRA CTB6 P R F P R F P R F +Pre-train (Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* (Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* LSTM 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 +bigram LSTM 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 +Pre-train+bigram (Pei et al., 2014) - - 95.2 -</context>
<context position="27878" citStr="Pei et al., 2014" startWordPosition="4808" endWordPosition="4811"> of the algorithm in this paper, we do not optimize the speed a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation methods is based on sequence labeling (Xue, 2003). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015). The features of all these methods are extracted from a local context and neglect the long distance information. However, previous information is also crucial for word segmentation. Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window. 7 Conclusion In this paper, we use LSTM to explicitly model the previous information for Chinese word segmentation, which can well model the potential long1204 distance features. Though our model use smaller context</context>
</contexts>
<marker>Pei, Ge, Baobao, 2014</marker>
<rawString>Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Maxmargin tensor neural network for chinese word segmentation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>F Feng</author>
<author>A McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th international conference on Computational Linguistics.</booktitle>
<contexts>
<context position="1326" citStr="Peng et al., 2004" startWordPosition="190" endWordPosition="193">tation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineer</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. Proceedings of the 20th international conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Qi</author>
<author>Sujatha G Das</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>Deep learning for character-based information extraction.</title>
<date>2014</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>668--674</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="27895" citStr="Qi et al., 2014" startWordPosition="4812" endWordPosition="4815">in this paper, we do not optimize the speed a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation methods is based on sequence labeling (Xue, 2003). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015). The features of all these methods are extracted from a local context and neglect the long distance information. However, previous information is also crucial for word segmentation. Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window. 7 Conclusion In this paper, we use LSTM to explicitly model the previous information for Chinese word segmentation, which can well model the potential long1204 distance features. Though our model use smaller context window size (0,2</context>
</contexts>
<marker>Qi, Das, Collobert, Weston, 2014</marker>
<rawString>Yanjun Qi, Sujatha G Das, Ronan Collobert, and Jason Weston. 2014. Deep learning for character-based information extraction. In Advances in Information Retrieval, pages 668–674. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan D Ratliff</author>
<author>J Andrew Bagnell</author>
<author>Martin A Zinkevich</author>
</authors>
<title>(online) subgradient methods for structured prediction.</title>
<date>2007</date>
<booktitle>In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).</booktitle>
<contexts>
<context position="17410" citStr="Ratliff et al., 2007" startWordPosition="3002" endWordPosition="3005">mber of characters with incorrect tags in the proposed tag sequence. For a given training instance (xi, yi),the predicted tag sequence yi E Y (xi) is the one with the highest score: yi = arg max s(xi, y, θ), (13) yEY(xi) where the function s(·) is sentence-level score and defined in equation (11). Given a set of training set D, the regularized objective function is the loss function J(θ) including 1 J(θ) =IDI 1:li(θ) + 211θ1122, (14) (xi,yi)ED where li(θ) = max(0, s(xi, yi, θ) + ∆(yi, yi) − s(xi, yi, θ)). To minimize J(θ), we use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradientlike direction. Following (Socher et al., 2013), we also use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. The parameter update for the i-th parameter θt,i at time step t is as follows: α θt,i = θt−1,i − t 2 gt,i, ( 15 ) �Eτ=1 gτ,i where α is the initial learning rate and gτ E R|θi| is the subgradient at time step τ for parameter θi. In addition, the process of back propagation is followd Hochreiter and Schmidhuber (1997). 4.2 Dropout Dropout is one of prevalent methods to avoid overfitting in neural networks (Srivasta</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2007</marker>
<rawString>Nathan D Ratliff, J Andrew Bagnell, and Martin A Zinkevich. 2007. (online) subgradient methods for structured prediction. In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Schuster</author>
<author>Kuldip K Paliwal</author>
</authors>
<title>Bidirectional recurrent neural networks.</title>
<date>1997</date>
<journal>Signal Processing, IEEE Transactions on,</journal>
<volume>45</volume>
<issue>11</issue>
<marker>Schuster, Paliwal, 1997</marker>
<rawString>Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. Signal Processing, IEEE Transactions on, 45(11):2673–2681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL conference. Citeseer.</booktitle>
<contexts>
<context position="1997" citStr="Socher et al., 2013" startWordPosition="304" endWordPosition="307">to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013</context>
<context position="17483" citStr="Socher et al., 2013" startWordPosition="3013" endWordPosition="3016"> given training instance (xi, yi),the predicted tag sequence yi E Y (xi) is the one with the highest score: yi = arg max s(xi, y, θ), (13) yEY(xi) where the function s(·) is sentence-level score and defined in equation (11). Given a set of training set D, the regularized objective function is the loss function J(θ) including 1 J(θ) =IDI 1:li(θ) + 211θ1122, (14) (xi,yi)ED where li(θ) = max(0, s(xi, yi, θ) + ∆(yi, yi) − s(xi, yi, θ)). To minimize J(θ), we use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradientlike direction. Following (Socher et al., 2013), we also use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective. The parameter update for the i-th parameter θt,i at time step t is as follows: α θt,i = θt−1,i − t 2 gt,i, ( 15 ) �Eτ=1 gτ,i where α is the initial learning rate and gτ E R|θi| is the subgradient at time step τ for parameter θi. In addition, the process of back propagation is followd Hochreiter and Schmidhuber (1997). 4.2 Dropout Dropout is one of prevalent methods to avoid overfitting in neural networks (Srivastava et al., 2014). When dropping a unit out, we temporarily remove it from</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In In Proceedings of the ACL conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>970--979</pages>
<contexts>
<context position="26379" citStr="Sun and Xu, 2011" startWordPosition="4555" endWordPosition="4558">al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* (Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* LSTM 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 +bigram LSTM 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 +Pre-train+bigram (Pei et al., 2014) - - 95.2 - - 97.2 - - - LSTM 96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0 Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results with * symbol are from our implementations of their methods. Models PKU MSRA CTB6 (Tseng et al., 2005) 95.0 96.4 - (Zhang and Clark, 2007) 95.1 97.2 - (Sun and Xu, 2011) - - 95.7 (Zhang et al., 2013) 96.1 97.4 - This work 96.5 97.4 96.0 Table 6: Comparison of our model with state-ofthe-art methods on three test sets. with previous neural models with pre-trained embedding and bigram embeddings. Table 6 lists the performances of our model as well as previous state-of-the-art systems. (Zhang and Clark, 2007) is a word-based segmentation algorithm, which exploit features of complete words, while the rest of the list are character-based word segmenters, whose features are mostly extracted from a window of characters. Moreover, some systems (such as Sun and Xu (201</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 970–979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Generating text with recurrent neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>1017--1024</pages>
<contexts>
<context position="10019" citStr="Sutskever et al., 2011" startWordPosition="1643" endWordPosition="1646">y neural network (LSTM) (Hochreiter and Schmidhuber,1997) is an extension of the recurrent neural network (RNN). The RNN has recurrent hidden states whose output at each time is dependent on that of the previous time. More formally, given a sequence x(1:n) = (x(1), x(2), ... , x(t), ... , x(n)), the RNN updates its recurrent hidden state h(t) by h(t) = g(Uh(t−1) + Wx(t) + b), (3) where g is a nonlinear function as mentioned above. Though RNN has been proven successful on many tasks such as speech recognition (Vinyals et al., 2012), language modeling (Mikolov et al., 2010) and text generation (Sutskever et al., 2011), it can be difficult to train them to learn longterm dynamics, likely due in part to the vanishing and exploding gradient problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corr</context>
</contexts>
<marker>Sutskever, Martens, Hinton, 2011</marker>
<rawString>Ilya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="10804" citStr="Sutskever et al., 2014" startWordPosition="1773" endWordPosition="1776">97). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs. In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification (Liu et al., 2015) and machine translation (Sutskever et al., 2014). The core of the LSTM model is a memory cell c encoding memory at every time step of what inputs have been observed up to this step (see Figure 2) . The behavior of the cell is controlled by three “gates”, namely input gate i, forget gate f and output gate o. The operations on gates are defined as element-wise multiplications, thus gate can either scale the input value if the gate is non-zero vector or omit input if the gate is zero vector. The output of output gate will be fed into the next time step t + 1 as previous hidden state and input of upper layer of neural network at current time st</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the fourth SIGHAN workshop on Chinese language Processing,</booktitle>
<volume>171</volume>
<contexts>
<context position="26312" citStr="Tseng et al., 2005" startWordPosition="4541" endWordPosition="4544">methods. models PKU MSRA CTB6 P R F P R F P R F +Pre-train (Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* (Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* LSTM 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 +bigram LSTM 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 +Pre-train+bigram (Pei et al., 2014) - - 95.2 - - 97.2 - - - LSTM 96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0 Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results with * symbol are from our implementations of their methods. Models PKU MSRA CTB6 (Tseng et al., 2005) 95.0 96.4 - (Zhang and Clark, 2007) 95.1 97.2 - (Sun and Xu, 2011) - - 95.7 (Zhang et al., 2013) 96.1 97.4 - This work 96.5 97.4 96.0 Table 6: Comparison of our model with state-ofthe-art methods on three test sets. with previous neural models with pre-trained embedding and bigram embeddings. Table 6 lists the performances of our model as well as previous state-of-the-art systems. (Zhang and Clark, 2007) is a word-based segmentation algorithm, which exploit features of complete words, while the rest of the list are character-based word segmenters, whose features are mostly extracted from a wi</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proceedings of the fourth SIGHAN workshop on Chinese language Processing, volume 171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2018" citStr="Turian et al., 2010" startWordPosition="308" endWordPosition="311">sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015)</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Suman V Ravuri</author>
<author>Daniel Povey</author>
</authors>
<title>Revisiting recurrent neural networks for robust asr.</title>
<date>2012</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on,</booktitle>
<pages>4085--4088</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="9932" citStr="Vinyals et al., 2012" startWordPosition="1630" endWordPosition="1633"> by table lookup operation according to its index. 3.2 LSTM The long short term memory neural network (LSTM) (Hochreiter and Schmidhuber,1997) is an extension of the recurrent neural network (RNN). The RNN has recurrent hidden states whose output at each time is dependent on that of the previous time. More formally, given a sequence x(1:n) = (x(1), x(2), ... , x(t), ... , x(n)), the RNN updates its recurrent hidden state h(t) by h(t) = g(Uh(t−1) + Wx(t) + b), (3) where g is a nonlinear function as mentioned above. Though RNN has been proven successful on many tasks such as speech recognition (Vinyals et al., 2012), language modeling (Mikolov et al., 2010) and text generation (Sutskever et al., 2011), it can be difficult to train them to learn longterm dynamics, likely due in part to the vanishing and exploding gradient problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal d</context>
</contexts>
<marker>Vinyals, Ravuri, Povey, 2012</marker>
<rawString>Oriol Vinyals, Suman V Ravuri, and Daniel Povey. 2012. Revisiting recurrent neural networks for robust asr. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 4085–4088. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naiwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural language engineering,</title>
<date>2005</date>
<pages>11--2</pages>
<contexts>
<context position="19393" citStr="Xue et al., 2005" startWordPosition="3356" endWordPosition="3359">opout Rate=50% without Dropout Figure 4: Performances of LSTM-1 with the different context lengths and dropout rates on PKU development set. Context length (k1, k2) = (0, 2) Character embedding size d = 100 Hidden unit number H2 = 150 Initial learning rate α = 0.2 Margin loss discount � = 0.2 Regularization A = 10−4 Dropout rate on input layer p = 0.2 epoches Figure 5: Performances of LSTM-1 (0,2) with 20% dropout on PKU development set. and MSRA data are provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36) (Xue et al., 2005), which is a segmented, part-of-speech tagged and fully bracketed corpus in the constituency formalism. These datasets are commonly used by previous state-of-the-art models and neural network models. In addition, we use the first 90% sentences of the training data as training set and the rest 10% sentences as development set for PKU and MSRA datasets. For CTB6 dataset, we divide the training, development and test sets according to (Yang and Xue, 2012) All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag. For evaluatio</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural language engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1306" citStr="Xue, 2003" startWordPosition="188" endWordPosition="189">word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort</context>
<context position="27696" citStr="Xue, 2003" startWordPosition="4780" endWordPosition="4781">ge. Despite our model only uses simple bigram features, it outperforms previous state-of-the-art models which use more complex features. Since that we do not focus on the speed of the algorithm in this paper, we do not optimize the speed a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation methods is based on sequence labeling (Xue, 2003). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015). The features of all these methods are extracted from a local context and neglect the long distance information. However, previous information is also crucial for word segmentation. Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window. 7 Conclusion In this paper, we us</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>N. Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaqin Yang</author>
<author>Nianwen Xue</author>
</authors>
<title>Chinese comma disambiguation for discourse analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersVolume 1,</booktitle>
<pages>786--794</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19848" citStr="Yang and Xue, 2012" startWordPosition="3429" endWordPosition="3432">ta are provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36) (Xue et al., 2005), which is a segmented, part-of-speech tagged and fully bracketed corpus in the constituency formalism. These datasets are commonly used by previous state-of-the-art models and neural network models. In addition, we use the first 90% sentences of the training data as training set and the rest 10% sentences as development set for PKU and MSRA datasets. For CTB6 dataset, we divide the training, development and test sets according to (Yang and Xue, 2012) All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag. For evaluation, we use the standard bake-off scoring program to calculate precision, recall, F1- score and out-of-vocabulary (OOV) word recall. 5.2 Hyper-parameters Hyper-parameters of neural model impact the performance of the algorithm significantly. According to experiment results, we choose the hyperparameters of our model as showing in Figure 1. The minibatch size is set to 20. Generally, the number of hidden units has a limited impact on the performance as l</context>
</contexts>
<marker>Yang, Xue, 2012</marker>
<rawString>Yaqin Yang and Nianwen Xue. 2012. Chinese comma disambiguation for discourse analysis. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersVolume 1, pages 786–794. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="26348" citStr="Zhang and Clark, 2007" startWordPosition="4548" endWordPosition="4551"> F P R F P R F +Pre-train (Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* (Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* LSTM 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 +bigram LSTM 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 +Pre-train+bigram (Pei et al., 2014) - - 95.2 - - 97.2 - - - LSTM 96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0 Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results with * symbol are from our implementations of their methods. Models PKU MSRA CTB6 (Tseng et al., 2005) 95.0 96.4 - (Zhang and Clark, 2007) 95.1 97.2 - (Sun and Xu, 2011) - - 95.7 (Zhang et al., 2013) 96.1 97.4 - This work 96.5 97.4 96.0 Table 6: Comparison of our model with state-ofthe-art methods on three test sets. with previous neural models with pre-trained embedding and bigram embeddings. Table 6 lists the performances of our model as well as previous state-of-the-art systems. (Zhang and Clark, 2007) is a word-based segmentation algorithm, which exploit features of complete words, while the rest of the list are character-based word segmenters, whose features are mostly extracted from a window of characters. Moreover, some s</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longkai Zhang</author>
<author>Houfeng Wang</author>
<author>Xu Sun</author>
<author>Mairgup Mansur</author>
</authors>
<title>Exploring representations from unlabeled data with co-training for Chinese word segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="26409" citStr="Zhang et al., 2013" startWordPosition="4562" endWordPosition="4565">.2 93.7 93.9 93.9* 93.4* 93.7* (Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* LSTM 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 +bigram LSTM 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 +Pre-train+bigram (Pei et al., 2014) - - 95.2 - - 97.2 - - - LSTM 96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0 Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results with * symbol are from our implementations of their methods. Models PKU MSRA CTB6 (Tseng et al., 2005) 95.0 96.4 - (Zhang and Clark, 2007) 95.1 97.2 - (Sun and Xu, 2011) - - 95.7 (Zhang et al., 2013) 96.1 97.4 - This work 96.5 97.4 96.0 Table 6: Comparison of our model with state-ofthe-art methods on three test sets. with previous neural models with pre-trained embedding and bigram embeddings. Table 6 lists the performances of our model as well as previous state-of-the-art systems. (Zhang and Clark, 2007) is a word-based segmentation algorithm, which exploit features of complete words, while the rest of the list are character-based word segmenters, whose features are mostly extracted from a window of characters. Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) al</context>
</contexts>
<marker>Zhang, Wang, Sun, Mansur, 2013</marker>
<rawString>Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup Mansur. 2013. Exploring representations from unlabeled data with co-training for Chinese word segmentation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqing Zheng</author>
<author>Hanyang Chen</author>
<author>Tianyu Xu</author>
</authors>
<title>Deep learning for chinese word segmentation and pos tagging.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>647--657</pages>
<contexts>
<context position="2245" citStr="Zheng et al. (2013)" startWordPosition="346" endWordPosition="349">ed by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert *Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters</context>
<context position="3756" citStr="Zheng et al., 2013" startWordPosition="591" endWordPosition="594"> based on the local information only. For example, the segmentation of the following sentence needs the information of the long distance collocation. I&apos; _f, (winter), &amp; (can) 3F (wear) -:; % (amount) 3F (wear) -:; % (amount); X �. (summer),&amp; (can) 3F (wear) -:; (more) % (little) 3F (wear) -:; (more) % (little). Without the word “夏天 (summer)” or “冬天 (winter)”, it is difficult to segment the phrase “能 V多i`V多i`”. Therefore, we usually need utilize the non-local information for more accurate word segmentation. However, it does not work by simply increasing the context window size. As reported in (Zheng et al., 2013), the performance drops smoothly when the window size is larger than 3. The reason is that the number of its parameters is so large that the trained network has 1197 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1197–1206, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. overfitted on training data. Therefore, it is necessary to capture the potential long-distance dependencies without increasing the size of the context window. In order to address this problem, we propose a neural model based on Long Short-</context>
<context position="15063" citStr="Zheng et al., 2013" startWordPosition="2569" endWordPosition="2572">= h(t−m1) ® ··· ® h(t+m2), (10) where m1 and m2 represent the lengths of time lags �h(t) is before and after current time step.Finally, used in final inference function (Eq. (11) ) after a linear transformation. LSTM-4 The LSTM-4 (see Figure 3d) is a mixture of the LSTM-2 and LSTM-3, which consists of two LSTM layers. The output sequence of the lower LSTM layer forms the input sequence of the upper LSTM layer. The final layer adopts a local context of upper LSTM layer as input. 3.4 Inference at Sentence Level To model the tag dependency, previous neural network models (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014) introduced the transition score Aij for measuring the probability of jumping from tag i E T to tag j E T. For a input sentence c(1:n) with a tag sequence y(1:n), a sentencelevel score is then given by the sum of tag transition scores and network tagging scores: Output Gate o Input Gate i σ σ h(t-1) h(t-1) c(t) c(t-1) o(t) i(t) x(t) φ φ c h(t-1) h(t) c(t) c(t-1) c(t-1) LSTM Memory Unit f(t) h(t-1) Forget Gate f σ 1200 LSTM y(t-1) y(t) y(t+1) x(t-1) x(t) x(t+1) LSTM LSTM LSTM LSTM y(o-1) y(o) y(o+1) x(o-1) x(o) x(o+1) LSTM LSTM LSTM LSTM LSTM y(t-1) y(t) y(t+1) x(t-1) x(t) x(</context>
<context position="24119" citStr="Zheng et al., 2013" startWordPosition="4164" endWordPosition="4167">ot to benefit from deep model. The LSTM-3 and LSTM-4 models do not converge, which could be caused by the complexity of models. The results on PKU test set are also shown in Table 3, which again show that the LSTM-1 achieves the best performance. Therefore, in the rest of the paper we will give more analysis based on the LSTM-1 with hyper-parameter settings as showing in Table 1. 5.5 Experiment Results In this section, we give comparisons of the LSTM1 with pervious neural models and state-of-the-art methods on the PKU, MSRA and CTB6 datasets. We first compare our model with two neural models (Zheng et al., 2013; Pei et al., 2014) on Chinese word segmentation task with random initialized character embeddings. As showing in Table 4, the performance is boosted significantly by utilizing LSTM unit. And more notably, our window size of the context characters is set to (0,2), while the size of the other models is (2,2). Previous works found that the performance can be improved by pre-training the character embeddings on large unlabeled data. We use word2vec 1 (Mikolov et al., 2013a) toolkit to pre-train the character embeddings on the Chinese Wikipedia corpus. The obtained embeddings are used to initializ</context>
<context position="25376" citStr="Zheng et al., 2013" startWordPosition="4367" endWordPosition="4370">f random initialization. Inspired by (Pei et al., 2014), we also utilize bigram character embeddings which is simply initialized as the average of embeddings of two consecutive characters. Table 5 shows the performances with additional pre-trained and bigram character embeddings. Again, the performances boost significantly as a result. Moreover, when we use bigram embeddings only, which means we do close test without pre-training the embeddings on other extra corpus, our model still perform competitively compared 1http://code.google.com/p/word2vec/ 1203 models PKU MSRA CTB6 P R F P R F P R F (Zheng et al., 2013) 92.8 92.0 92.4 92.9 93.6 93.3 94.0* 93.1* 93.6* (Pei et al., 2014) 93.7 93.4 93.5 94.6 94.2 94.4 94.4* 93.4* 93.9* LSTM 95.8 95.5 95.7 96.7 96.2 96.4 95.0 94.8 94.9 Table 4: Performances on three test sets with random initialized character embeddings. The results with * symbol are from our implementations of their methods. models PKU MSRA CTB6 P R F P R F P R F +Pre-train (Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* (Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* LSTM 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 +bigram LSTM 96.3 95.9 96.1 97.1 97.1 </context>
<context position="27860" citStr="Zheng et al., 2013" startWordPosition="4804" endWordPosition="4807">t focus on the speed of the algorithm in this paper, we do not optimize the speed a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation methods is based on sequence labeling (Xue, 2003). Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015). The features of all these methods are extracted from a local context and neglect the long distance information. However, previous information is also crucial for word segmentation. Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window. 7 Conclusion In this paper, we use LSTM to explicitly model the previous information for Chinese word segmentation, which can well model the potential long1204 distance features. Though our model u</context>
</contexts>
<marker>Zheng, Chen, Xu, 2013</marker>
<rawString>Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for chinese word segmentation and pos tagging. In EMNLP, pages 647–657.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
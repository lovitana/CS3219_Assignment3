<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002780">
<title confidence="0.9805625">
Noise or additional information? Leveraging crowdsource annotation
item agreement for natural language tasks.
</title>
<author confidence="0.993089">
Emily K. Jamison ‡ and Iryna Gurevych †‡‡Ubiquitous Knowledge Processing Lab (UKP-TUDA),
</author>
<affiliation confidence="0.949143333333333">
Department of Computer Science, Technische Universit¨at Darmstadt
† Ubiquitous Knowledge Processing Lab (UKP-DIPF),
German Institute for Educational Research
</affiliation>
<email confidence="0.913679">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.992039" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884">
In order to reduce noise in training data,
most natural language crowdsourcing an-
notation tasks gather redundant labels and
aggregate them into an integrated label,
which is provided to the classifier. How-
ever, aggregation discards potentially use-
ful information from linguistically am-
biguous instances.
For five natural language tasks, we pass
item agreement on to the task classifier
via soft labeling and low-agreement filter-
ing of the training dataset. We find a sta-
tistically significant benefit from low item
agreement training filtering in four of our
five tasks, and no systematic benefit from
soft labeling.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999823785714286">
Crowdsourcing is a cheap and increasingly-
utilized source of annotation labels. In a typical
annotation task, five or ten labels are collected for
an instance, and are aggregated together into an
integrated label. The high number of labels is
used to compensate for worker bias, task misun-
derstanding, lack of interest, incompetance, and
malicious intent (Wauthier and Jordan, 2011).
Majority voting for label aggregation has been
found effective in filtering noisy labels (Nowak
and R¨uger, 2010). Labels can be aggregated un-
der weighted conditions reflecting the reliability
of the annotator (Whitehill et al., 2009; Welinder
et al., 2010). Certain classifiers are also robust
to random (unbiased) label noise (Tibshirani and
Manning, 2014; Beigman and Beigman Klebanov,
2009). However, minority label information is dis-
carded by aggregation, and when the labels were
gathered under controlled circumstances, these la-
bels may reflect linguistic intuition and contain
useful information (Plank et al., 2014b). Two al-
ternative strategies that allow the classifier to learn
from the item agreement include training instance
filtering and soft labeling. Filtering training in-
stances by item agreement removes low agree-
ment instances from the training set. Soft labeling
assigns a classifier weight to a training instance
based on the item agreement.
</bodyText>
<figure confidence="0.744188">
Consider two Affect Recognition instances and
their Krippendorff (1970)’s α item agreement:
Text: India’s Taj Mahal gets facelift
Sadness Rating (0-100): 8.0
α Agreement (-1.0 – 1.0): 0.7
</figure>
<figureCaption confidence="0.999059">
Figure 1: Affect Recognition Easy Case.
</figureCaption>
<figure confidence="0.806374">
Text: After Iraq trip, Clinton proposes war limits
Sadness Rating (0-100): 12.5
α Agreement (-1.0 – 1.0): -0.1
</figure>
<figureCaption confidence="0.999669">
Figure 2: Affect Recognition Hard Case.
</figureCaption>
<bodyText confidence="0.999964444444444">
In Figure 1, annotators mostly agreed that the
headline expresses little sadness. But in Figure 2,
the low item agreement may be caused by instance
difficulty (i.e., Is a war zone sad or just bad?):
a Hard Case (Zeman, 2010). Previous work
(Beigman Klebanov and Beigman, 2014; Beigman
and Beigman Klebanov, 2009) has shown that
training strategy may affect Hard and Easy Case
test instances differently.
In this work, for five natural language tasks,
we examine the impact of passing crowdsource
item agreement on to the task classifier, by means
of training instance filtering and soft labeling.
We construct classifiers for Biased Text Detec-
tion, Stemming Classification, Recognizing Tex-
tual Entailment, Twitter POS Tagging, and Affect
Recognition, and evaluate the effect of our dif-
ferent training strategies on the accuracy of each
</bodyText>
<page confidence="0.967403">
291
</page>
<note confidence="0.9850025">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 291–297,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99980365">
task. These tasks represent a wide range of ma-
chine learning tasks typical in NLP: sentence-level
SVM regression using n-grams; word pairs with
character-based features and binary SVM classi-
fication; pairwise sentence binary SVM classi-
fication with similarity score features; CRF se-
quence word classification with a range of fea-
ture types; and sentence-level regression using a
token-weight averaging, respectively. We use pre-
existing, freely-available crowdsourced datasets
and post all our experiment code on GitHub1.
Contributions This is the first work (1) to ap-
ply item-agreement-weighted soft labeling from
crowdsourced labels to multiple real natural lan-
guage tasks; (2) to filter training instances by item
agreement from crowdsourced labels, for multiple
natural language tasks; (3) to evaluate classifier
performance on high item agreement (Easy Case)
instances and low item agreement (Hard Case) in-
stances across multiple natural language tasks.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.989782563636364">
Dekel and Shamir (2009) calculated integrated
labels for an information retrieval crowdsourced
dataset, and identified low-quality workers by de-
viation from the integrated label. Removal of
these workers’ labels improved classifier perfor-
mance on data that was not similarly filtered.
While much work (Dawid and Skene, 1979;
Ipeirotis et al., 2010; Dalvi et al., 2013) has ex-
plored techniques to model worker ability, bias,
and instance difficulty while aggregating labels,
there is no evaluation comparing classifiers trained
on the new integrated labels with other options, on
their respective NLP tasks.
Training instance filtering aims to remove mis-
labeled instances from the training dataset. Scul-
ley and Cormack (2008) learned a logistic regres-
sion classifier to identify and filter noisy labels in
a spam email filtering task. They also proposed
a label correcting technique that replaces identi-
fied noisy labels with “corrected” labels, at the risk
of introducing noise into the corpus. Rebbapra-
gada et al. (2009) developed a label noise detection
technique to cluster training instances and remove
label outliers. Raykar et al. (2010) jointly learned
a classifier/regressor, annotator accuracy, and the
integrated label on datasets with multiple noisy la-
bels, outperforming Smyth et al. (1995)’s model
1github.com/EmilyKJamison/crowdsourcing
of estimating ground truth labels.
Soft labeling, or the association of one train-
ing instance with multiple, weighted, conflict-
ing labels, is a technique to model noisy training
data. Thiel (2008) found that soft labeled train-
ing data produced more accurate classifiers than
hard labeled training data, with both Radial Ba-
sis Function Networks and Fuzzy-Input Fuzzy-
Output SVMs. Shen and Lapata (2007) used soft
labeling to model their semantic frame structures
in a question answering task, to represent that the
semantic frames can bear multiple sematic roles.
Previous research has found that, for a few in-
dividual NLP tasks, training while incorporating
label noise weight may produce a better model.
Martinez Alonso et al. (2015) show that inform-
ing a parser of annotator disagreement via loss
function reduced error in labeled attachments by
6.4%. Plank et al. (2014a) incorporate annota-
tor disagreement in POS tags into the loss func-
tion of a POS-tag machine learner, resulting in
improved performance on downstream chunking.
Beigman Klebanov and Beigman (2014) observed
that, on a task classifying text as semantically old
or new, the inclusion of Hard Cases in training
data resulted in reduced classifier performance on
Easy Cases.
</bodyText>
<sectionHeader confidence="0.93764" genericHeader="method">
3 Overview of Experiments
</sectionHeader>
<bodyText confidence="0.999977157894737">
We built systems for the five NLP tasks, and
trained them using aggregation, soft labeling, and
instance screening strategies. When labels were
numeric, the integrated label was the average2.
When labels were nominal, the integrated label
was majority vote. Krippendorff (1970)’s α item
agreement was used to filter ambiguous train-
ing instances. For soft labeling, percentage item
agreement was used to assign instance weights.
We followed Sheng et al. (2008)’s suggested Mul-
tiplied Examples procedure: for each unlabeled in-
stance xi and each existing label yi ∈ Li = {yid}
(as annotated by worker j), we create one replica
of xi, assign it yi, and weight the instance accord-
ing to the count of yi in Li (i.e., the percentage
item agrement). For each training strategy (Soft-
Label, etc), the training instances were changed
by the strategy, but the test instances were unaf-
fected. For the division of test instances into Hard
</bodyText>
<footnote confidence="0.54902625">
2We followed Yano et al. (2010) and Strapparava and Mi-
halcea (2007) in using mean as gold standard. Although an-
other aggregation such as as median might be more represen-
tative, such discussion is beyond the scope of this paper.
</footnote>
<page confidence="0.988354">
292
</page>
<bodyText confidence="0.998656710526316">
and Easy Cases, the training instances were un-
affected, but the test instances were filtered by α
item agreement. Hard/Easy Case parameters were
chosen to divide the corpus by item agreement into
roughly equal portions3, relative to the corpus, for
post-hoc error analysis.
All systems except Affect Recognition were
constructed using DKPro Text Classification
(Daxenberger et al., 2014), and used Weka’s SMO
(Platt, 1999) or SMOreg (Shevade et al., 2000) im-
plementations with default parameters, with 10-
fold (or 5-fold, for computationally-intensive POS
Tagging) cross-validation. More details are avail-
able in the Supplemental Notes document.
Agreement Parameters Training strategies
HighAgree and VeryHigh utilize agreement cutoff
parameters that vary per corpus. These strategies
are a discretized approximation of the gradual
effect of filtering low agreement instances from
the training data. For any given corpus, we could
not use a cutoff value equal to no filtering, or
that eliminated a class. If there were only 2
remaining cutoffs, we used these. If there were
more candidate cutoff values, we trained and
evaluated a classifier on a development set and
chose the value for HighAgree that maximized
Hard Case performance on the development set.
Percentage Agreement In this paper, we follow
Beigman Klebanov and Beigman (2014) in us-
ing the nominal agreement categories Hard Cases
and Easy Cases to separate instances by item
agreement. However, unlike Beigman Klebanov
and Beigman (2014) who use simple percentage
agreement, we calculate item-specific agreement
via Krippendorff (1970)’s α item agreement4, with
Nominal, Ordinal, or Ratio distance metrics as ap-
propriate. The agreement is expressed in the range
(-1.0 – 1.0); 1.0 is perfect agreement.
</bodyText>
<subsectionHeader confidence="0.999601">
3.1 Biased Language Detection
</subsectionHeader>
<bodyText confidence="0.999831285714286">
This task detects the use of bias in political text.
The corpus (Yano et al., 2010)5 consists of 1,041
sentences from American political blogs. For each
sentence, five crowdsource annotators chose a la-
bel no bias, some bias, and very biased. We follow
Yano et al. (2010) in representing the amount of
bias on a numerical scale (1-3). Hard/Easy Case
</bodyText>
<footnote confidence="0.975244">
3Limited by the discrete nature of our agreement.
4From the DKPro Statistics library (Meyer et al., 2014)
5Available at sites.google.com/site/
amtworkshop2010/data-1
</footnote>
<bodyText confidence="0.999894294117647">
cutoffs were &lt;-.21 and &gt;.20. Of 1041 total in-
stances, 161 were Hard Cases (&lt;-.21) and 499
were Easy Cases (&gt;.20).
We built an SVM regression task using uni-
grams, to predict the numerical amount of bias.
The gold standard was the integrated labels. Item-
specific agreement was calculated with Ordinal
Distance Function (Krippendorff, 1980).
We used the following training strategies:
VeryHigh Filtered for agreement &gt;0.4.
HighAgree Filtered for agreement &gt;-0.2.
SoftLabel One training instance is generated for
each label from a text, and weighted by how many
times that label occurred with the text.
SLLimited SoftLabel, except that training in-
stances with a label distance &gt;1.0 from the origi-
nal text label average are discarded.
</bodyText>
<subsectionHeader confidence="0.999791">
3.2 Morphological Stemming
</subsectionHeader>
<bodyText confidence="0.983745233333333">
The goal of this binary classification task is to pre-
dict, given an original word and a stemmed ver-
sion of the word, whether the stemmed version
has been correctly stemmed. The word pair was
correct if: the stemmed word contained one less
affix; or if the original word was a compound,
the stemmed word had a space inserted between
the components; or if the original word was mis-
spelled, the stemmed word was deleted; or if the
original word had no affixes and was not a com-
pound and was not misspelled, then the stemmed
word had no changes.
This dataset was compiled by Carpenter et al.
(2009)6. The dataset contains 6679 word pairs;
most pairs have 5 labels each. In the cross-
validation division, no pairs with the same original
word could be split across training and test data.
The gold standard was the integrated label, with
4898 positive and 1781 negative pairs. Hard/Easy
Case cutoffs were &lt;-.5 and &gt;.5. Of 6679 total
instances, 822 were Hard Cases (&lt;-.5) and 3615
were Easy Cases (&gt;.5). Features used are combi-
nations of the characters after the removal of the
longest common substring between the word pair,
including 0-2 additional characters from the sub-
string; word boundaries are marked.
Stemming-new training strategies include:
HighAgree Filtered for agreement &gt;-0.1.
SLLimited MajVote with instances weighted by
the frequency of the label for the text pair.
</bodyText>
<footnote confidence="0.990023">
6Available at github.com/bob-carpenter/anno
</footnote>
<page confidence="0.994186">
293
</page>
<subsectionHeader confidence="0.993287">
3.3 Recognising Textual Entailment
</subsectionHeader>
<bodyText confidence="0.999635">
Recognizing textual entailment is the process of
determining if, given two sentences text and hy-
pothesis, the meaning of the hypothesis can be in-
ferred from the text.
We used the dataset from the PASCAL RTE-1,
which contains 800 sentence pairs. The crowd-
source annotations of 10 labels per pair were ob-
tained by Snow et al. (2008)7. We reproduced the
basic system described in (Dagan et al., 2006) of
TF-IDF weighted Cosine Similarity between lem-
mas of the text and hypothesis. The weight of each
wordi in documentj, n total documents, is the
log-plus-one termi frequency normalized by raw
termi document frequency, with Euclidean nor-
malization.
</bodyText>
<equation confidence="0.99721475">
� =
(1 + log(tfi,j)) N if tfi,j ≥ 1
weight(i, j)dfi
0 if tfi,j = 0
</equation>
<bodyText confidence="0.999912454545455">
Additionally, we used features including the dif-
ference in noun chunk character and token length,
the difference in number of tokens, shared named
entities, and subtask names. The gold standard
was the original labels from Dagan et al. (2006).
Hard/Easy Case cutoffs were &lt;0.0 and &gt;.3. Train-
ing strategies are from Biased Language (Very-
High) and Stem (others) experiments, except the
HighAgree cutoff was 0.0 and the VeryHigh cutoff
was 0.3. Of 800 total instances, 230 were Hard
Cases (&lt;0.0) and 207 were Easy Cases (&gt;.30).
</bodyText>
<subsectionHeader confidence="0.883323">
3.4 POS tagging
</subsectionHeader>
<bodyText confidence="0.999965058823529">
We built a POS-tagger for Twitter posts. We used
the training section of the dataset from Gimpel et
al. (2011). The POS tagset was the universal tag
set (Petrov et al., 2012); we converted Gimpel et
al. (2011)’s tags to the universal tagset using Hovy
et al. (2014)’s mapping. Crowdsource labels for
this data came from Hovy et al. (2014)8, who ob-
tained 5 labels for each tweet. After aligning and
cleaning, our dataset consisted of 953 tweets of
14,439 tokens.
We followed Hovy et al. (2014) in constructing
a CRF classifier (Lafferty et al., 2001), using a list
of English affixes, Hovy et al. (2014)’s set of or-
thographic features, and word clusters (Owoputi
et al., 2013). In the cross-validation division, in-
dividual tweets were assigned to folds. The gold
standard was the integrated label. Hard/Easy Case
</bodyText>
<footnote confidence="0.998564333333333">
7Available at sites.google.com/site/
nlpannotations/
8Available at lowlands.ku.dk/results/
</footnote>
<bodyText confidence="0.995082733333333">
cutoffs were &lt;0.0 and &gt;.49. Of 14,439 tokens,
649 were Hard Cases (&lt;0.0) and 10830 were Easy
Cases (&gt;.49).
We used the following strategies:
VeryHigh For each token t in sequence s where
agreement(t) &lt;0.5, s is broken into two separate
sequences s1 and s2 and t is deleted (i.e. filtered).
HighAgree VeryHigh with agreement &lt;0.2.
SoftLabel For each proto-sequence s, we generate
5 sequences {s0, s1,..., sil, in which each token t
is assigned a crowdsource label drawn at random:
lt,si E Lt.
SLLimited, Each token t in sequence s is assigned
its MajVote label. Then s is given a weight repre-
senting the average item agreement for all t E s.
</bodyText>
<subsectionHeader confidence="0.906498">
3.5 Affect Recognition
</subsectionHeader>
<bodyText confidence="0.999226909090909">
Our Affect Recognition experiments are based on
the affective text annotation task in Strapparava
and Mihalcea (2007), using the Sadness dataset.
Each headline is rated for “sadness” using a scale
of 0-100. Examples are in Figures 1 and 2.
We use the crowdsourced annotation for a 100-
headline sample of this dataset provided by Snow
et al. (2008)9, with 10 annotations per emotion per
headline. Of 100 total instances, 20 were Hard
Cases (&lt;0.0) and 49 were Easy Cases (&gt;.30).
Our system design is identical to Snow et al.
(2008), which is similar to the SWAT system (Katz
et al., 2007), a top-performing system on the Se-
mEval Affective Text task. Hard/Easy Case cut-
offs were &lt;0.0 and &gt;.3.
Training strategies are the same as for the Biased
Language experiments, except:
VeryHigh Filtered for agreement &gt;0.3.
HighAgree Filtered for agreement &gt;0.
SLLimited SoftLabel, except that instances with
a label distance &gt;20.0 from the original label av-
erage are discarded.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99996425">
Our results on all five tasks, using each of the
training strategies and variously evaluating on all,
Easy, or Hard Cases, can be seen in Table 1.
Systems outputing numeric values show results in
Pearson correlation, and systems outputing nomi-
nal labels show micro F1. Soft labeling (SoftLa-
bel) failed to outperform integrated labels for 4
of the 5 complete test sets. Likewise, SLLimited
</bodyText>
<footnote confidence="0.970889">
9Available at sites.google.com/site/
nlpannotations/
</footnote>
<page confidence="0.991862">
294
</page>
<table confidence="0.931230142857143">
Biased Lang Stemming RTE POS Affective Text
Training All Hard Easy All Hard Easy All Hard Easy All Hard Easy All Hard Easy
Integrated .236 .144 .221 .797 .568 .927 .513 .330 .831 .790 .370 .878 .446 .115 .476
VeryHigh .140 .010 .158 – – – .499 .304 .836 .771 .310 .869 .326 .059 .376
HighAgree .231 .210 .222 .796 .569 .924 .543 .361 .831 .810 .382 .901 .453 .265 .505
SoftLabel .223 .131 .210 .766 .539 .957 .499 .304 .836 .789 .353 .880 .450 .112 .477
SLLimited .235 .158 .208 .799 .569 .930 .493 .291 .831 .797 .376 .882 .450 .139 .472
</table>
<tableCaption confidence="0.999834">
Table 1: Results (Pearson or micro F1) with different training strategies and all, Hard, and Easy Cases.
</tableCaption>
<figure confidence="0.998958454545455">
0.8
micro Fl
0.6
0.4
0.2
0.4
0.2
0
Pearson correlation
−0.5 0 0.5
Agreement cutoff
</figure>
<figureCaption confidence="0.998528">
Figure 3: Biased Language.
</figureCaption>
<figure confidence="0.952095">
−0.2 0 0.2 0.4
Agreement cutoff
</figure>
<figureCaption confidence="0.989165">
Figure 4: RTE.
</figureCaption>
<figure confidence="0.9523955">
−0.2 0 0.2 0.4 0.6
Agreement cutoff
</figure>
<figureCaption confidence="0.998887">
Figure 5: POS Tags.
</figureCaption>
<figure confidence="0.98331">
−0.5 0 0.5
Agreement cutoff
</figure>
<figureCaption confidence="0.998978">
Figure 6: Affective Text.
</figureCaption>
<figure confidence="0.998927">
Hard Cases
All Cases
Easy Cases
Easy Cases
Hard Cases
All Cases
Hard Cases
Easy Cases
All Cases
micro Fl 0.8
0.6
0.4
0.2
0.2
0.1
0
Pearson correlation
Hard Cases
Easy Cases
All Cases
</figure>
<bodyText confidence="0.985409382978724">
did not significantly outperform Integrated. How-
ever, HighAgree does outperform Integrated on 4
or the 5 tasks, especially for Hard Cases: Hard
Case improvements for Biased Language and POS
Tagging, and Affective Text, and overall improve-
ments for RTE, POS Tagging, and Affective Text
were significant (Paired TTest, p &lt; 0.05, for nu-
merical output, or McNemar’s Test10 (McNemar,
1947), p &lt; 0.05, for nominal classes). The fifth
task, Stemming, had the lowest number of item
agreement categories of the five tasks, preventing
fine-grained agreement training filtering, which
explains why filtering shows no benefit.
All training strategies used the same amount of
annotated data as input, and for filtering strategies
such as HighAgree, a reduced number of strategy-
output instances are used to train the model. As a
higher cutoff is used for HighAgree, the lack of
training data results in a worse model; this can
be seen in the downward curves of Figures 3 – 6,
where the curved line is HighAgree and the match-
ing pattern straight line is Integrated. (Due to the
low number of item agreement categories, Stem-
ming results are not displayed in an item agree-
ment cutoff table.) However, Figures 4 – 6 show
the overall performance boost, and Figure 3 shows
the Hard Case performance boost, right before the
downward curves from too little training data, us-
ing HighAgree.
Comparability We found the accuracy of our
systems was similar to that reported in previous lit-
erature. Dagan et al. (2006) report performance of
the RTE system, on a different data division, with
accuracy=0.568. Hovy et al. (2014) report major-
ity vote results (from acc=0.805 to acc=0.837 on
a different data section) similar to our results of
10See Japkowicz and Shah (2011) for usage description.
0.790 micro-Fl. For Affective Text, Snow et al.
(2008) report results on a different data section of
r=0.174, a merged result from systems trained on
combinations of crowdsource labels and evaluated
against expert-trained systems. The SWAT sys-
tem (Katz et al., 2007), which also used lexical
resources and additional training data, acheived
r=0.3898 on a different section of data. These re-
sults are comparable with ours, which range from
r=0.326 to r=0.453.
</bodyText>
<sectionHeader confidence="0.98893" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999985733333333">
In this work, for five natural langauge tasks, we
have examined the impact of informing the classi-
fier of crowdsource item agreement, by means of
soft labeling and removal of low-agreement train-
ing instances. We found a statistically significant
benefit from low-agreement training filtering in
four of our five tasks, and strongest improvements
for Hard Cases. Previous work (Beigman Kle-
banov and Beigman, 2014) found a similar effect,
but only evaluated a single task, so generalizabil-
ity was unknown. We also found that soft labeling
was not beneficial compared to aggregation. Our
findings suggest that the best crowdsource label
training strategy is to remove low item agreement
instances from the training set.
</bodyText>
<sectionHeader confidence="0.989774" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9980254">
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the Center for Advanced Security Research
(www.cased.de).
</bodyText>
<page confidence="0.997999">
295
</page>
<sectionHeader confidence="0.947543" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989055409090909">
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 280–287, Suntec, Singapore.
Beata Beigman Klebanov and Eyal Beigman. 2014.
Difficult cases: From data to learning, and back. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 390–
396, Baltimore, Maryland.
Bob Carpenter, Emily Jamison, and Breck Baldwin.
2009. Building a stemming corpus: Coding stan-
dards. http://lingpipe-blog.com/2009/
02/25/stemming-morphology-corpus-
coding-standards/.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual En-
tailment Challenge. In Machine learning chal-
lenges. Evaluating predictive uncertainty, visual ob-
ject classification, and recognising textual entail-
ment, pages 177–190. Springer.
Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vib-
hor Rastogi. 2013. Aggregating crowdsourced bi-
nary ratings. In Proceedings of the 22nd Interna-
tional Conference on World Wide Web, pages 285–
294, Rio de Janeiro, Brazil.
Alexander Philip Dawid and Allan M. Skene. 1979.
Maximum likelihood estimation of observer error-
rates using the EM algorithm. Journal of the
Royal Statistical Society. Series C (Applied Statis-
tics), 28(1):20–28.
Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. DKPro TC:
A Java-based Framework for Supervised Learning
Experiments on Textual Data. In Proceedings
of 52nd Annual Meeting of the Association for
Computational Linguistics, pages 61–66, Baltimore,
Maryland.
Ofer Dekel and Ohad Shamir. 2009. Vox populi: Col-
lecting high-quality labels from a crowd. In Pro-
ceedings of the Twenty-Second Annual Conference
on Learning Theory, Montreal, Canada. Online pro-
ceedings.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tag-
ging for Twitter: Annotation, features, and exper-
iments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 42–47,
Portland, Oregon.
Dirk Hovy, Barbara Plank, and Anders Søgaard. 2014.
Experiments with crowdsourced re-annotation of a
pos tagging data set. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, pages 377–382, Baltimore, Maryland.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on Amazon Mechanical
Turk. In Proceedings of the ACM SIGKDD Work-
shop on Human Computation, pages 64–67, Wash-
ington DC, USA.
Nathalie Japkowicz and Mohak Shah. 2011. Evalu-
ating learning algorithms: a classification perspec-
tive. Cambridge University Press.
Phil Katz, Matt Singleton, and Richard Wicentowski.
2007. SWAT-MP:The SemEval-2007 Systems for
Task 5 and Task 14. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 308–313, Prague, Czech Re-
public.
Klaus Krippendorff. 1970. Estimating the reliabil-
ity, systematic error and random error of interval
data. Educational and Psychological Measurement,
30(1):61–70.
Klaus Krippendorff. 1980. Content analysis: An in-
troduction to its methodology. Sage, Beverly Hills,
California.
John Lafferty, Andrew McCallum, and Fernando C.N.
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282–
289, Williamstown, Massachusetts.
H´ector Martinez Alonso, Barbara Plank, Arne
Skjærholt, and Anders Søgaard. 2015. Learning to
parse with IAA-weighted loss. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1357–1361,
Denver, Colorado.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153–157.
Christian M. Meyer, Margot Mieskes, Christian Stab,
and Iryna Gurevych. 2014. DKPro Agreement:
An open-source java library for measuring inter-
rater agreement. In Proceedings of the 25th Inter-
national Conference on Computational Linguistics
(COLING), pages 105–109, Dublin, Ireland.
Stefanie Nowak and Stefan R¨uger. 2010. How reliable
are annotations via crowdsourcing: A study about
inter-annotator agreement for multi-label image an-
notation. In Proceedings of the International Con-
ference on Multimedia Information Retrieval, pages
557–566, Philadelphia, Pennsylvania.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
</reference>
<page confidence="0.992322">
296
</page>
<reference confidence="0.991518618181819">
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380–390, Atlanta, Georgia.
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012. A universal part-of-speech tagset. In Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2089–2096, Istanbul, Turkey.
Barbara Plank, Dirk Hovy, and Anders Søgaard.
2014a. Learning part-of-speech taggers with inter-
annotator agreement loss. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 742–
751, Gothenburg, Sweden.
Barbara Plank, Dirk Hovy, and Anders Søgaard.
2014b. Linguistically debatable or just plain wrong?
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics, pages
507–511, Baltimore, Maryland.
John Platt. 1999. Fast training of support vector ma-
chines using sequential minimal optimization. In
Advances in kernel methods – support vector learn-
ing, pages 185–208. MIT Press.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from
crowds. The Journal of Machine Learning Re-
search, 11:1297–1322.
Umaa Rebbapragada, Lukas Mandrake, Kiri L.
Wagstaff, Damhnait Gleeson, Rebecca Castano,
Steve Chien, and Carla E. Brodley. 2009. Improv-
ing onboard analysis of hyperion images by filtering
mislabeled training data examples. In Proceedings
of the 2009 IEEE Aerospace Conference, pages 1–9,
Big Sky, Montana.
D. Sculley and Gordon V. Cormack. 2008. Filtering
email spam in the presence of noisy user feedback.
In Proceedings of the Conference on Email andAnti-
spam (CEAS), Mountain View, CA, USA. Online
proceedings.
Dan Shen and Mirella Lapata. 2007. Using seman-
tic roles to improve question answering. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 12–21, Prague, Czech Republic.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? Improving data
quality and data mining using multiple, noisy label-
ers. In Proceedings of the 14th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 614–622, Las Vegas, Nevada.
Shirish Krishnaj Shevade, S. Sathiya Keerthi, Chiranjib
Bhattacharyya, and Karaturi Radha Krishna Murthy.
2000. Improvements to the SMO algorithm for
SVM regression. IEEE Transactions on Neural Net-
works, 11(5):1188–1193.
Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro
Perona, and Pierre Baldi. 1995. Inferring ground
truth from subjective labelling of Venus images. Ad-
vances in Neural Information Processing Systems,
pages 1085–1092.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast – but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 254–263, Honolulu, Hawaii.
Carlo Strapparava and Rada Mihalcea. 2007.
SemEval-2007 Task 14: Affective Text. In Proceed-
ings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 70–74,
Prague, Czech Republic.
Christian Thiel. 2008. Classification on soft labels
is robust against label noise. In Knowledge-Based
Intelligent Information and Engineering Systems,
pages 65–73, Wellington, New Zealand.
Julie Tibshirani and Christopher D. Manning. 2014.
Robust logistic regression using shift parameters. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 124–
129, Baltimore, Maryland.
Fabian L. Wauthier and Michael I. Jordan. 2011.
Bayesian bias mitigation for crowdsourcing. In Ad-
vances in Neural Information Processing Systems,
pages 1800–1808.
Peter Welinder, Steve Branson, Pietro Perona, and
Serge J. Belongie. 2010. The multidimensional wis-
dom of crowds. In Advances in Neural Information
Processing Systems, pages 2424–2432.
Jacob Whitehill, Ting fan Wu, Jacob Bergsma,
Javier R. Movellan, and Paul L. Ruvolo. 2009.
Whose vote should count more: Optimal integra-
tion of labels from labelers of unknown expertise.
In Y. Bengio, D. Schuurmans, J.D. Lafferty, C.K.I.
Williams, and A. Culotta, editors, Advances in Neu-
ral Information Processing Systems, pages 2035–
2043. Curran Associates, Inc.
Tae Yano, Philip Resnik, and Noah A. Smith. 2010.
Shedding (a thousand points of) light on biased lan-
guage. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon’s Mechanical Turk, pages 152–158,
Los Angeles, California.
Daniel Zeman. 2010. Hard problems of tagset con-
version. In Proceedings of the Second International
Conference on Global Interoperability for Language
Resources, pages 181–185, Hong Kong, China.
</reference>
<page confidence="0.997444">
297
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.714467">
<title confidence="0.87303">Noise or additional information? Leveraging crowdsource item agreement for natural language tasks.</title>
<author confidence="0.969726">K Jamison Iryna Gurevych Knowledge Processing Lab</author>
<affiliation confidence="0.998826666666667">Department of Computer Science, Technische Universit¨at Knowledge Processing Lab German Institute for Educational</affiliation>
<web confidence="0.979032">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.99805594117647">In order to reduce noise in training data, most natural language crowdsourcing annotation tasks gather redundant labels and aggregate them into an integrated label, which is provided to the classifier. However, aggregation discards potentially useful information from linguistically ambiguous instances. For five natural language tasks, we pass item agreement on to the task classifier via soft labeling and low-agreement filtering of the training dataset. We find a statistically significant benefit from low item agreement training filtering in four of our five tasks, and no systematic benefit from soft labeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eyal Beigman</author>
<author>Beata Beigman Klebanov</author>
</authors>
<title>Learning with annotation noise.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>280--287</pages>
<location>Suntec, Singapore.</location>
<marker>Beigman, Klebanov, 2009</marker>
<rawString>Eyal Beigman and Beata Beigman Klebanov. 2009. Learning with annotation noise. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 280–287, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>Difficult cases: From data to learning, and back.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>390--396</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="3030" citStr="Klebanov and Beigman, 2014" startWordPosition="445" endWordPosition="448">ion instances and their Krippendorff (1970)’s α item agreement: Text: India’s Taj Mahal gets facelift Sadness Rating (0-100): 8.0 α Agreement (-1.0 – 1.0): 0.7 Figure 1: Affect Recognition Easy Case. Text: After Iraq trip, Clinton proposes war limits Sadness Rating (0-100): 12.5 α Agreement (-1.0 – 1.0): -0.1 Figure 2: Affect Recognition Hard Case. In Figure 1, annotators mostly agreed that the headline expresses little sadness. But in Figure 2, the low item agreement may be caused by instance difficulty (i.e., Is a war zone sad or just bad?): a Hard Case (Zeman, 2010). Previous work (Beigman Klebanov and Beigman, 2014; Beigman and Beigman Klebanov, 2009) has shown that training strategy may affect Hard and Easy Case test instances differently. In this work, for five natural language tasks, we examine the impact of passing crowdsource item agreement on to the task classifier, by means of training instance filtering and soft labeling. We construct classifiers for Biased Text Detection, Stemming Classification, Recognizing Textual Entailment, Twitter POS Tagging, and Affect Recognition, and evaluate the effect of our different training strategies on the accuracy of each 291 Proceedings of the 2015 Conference </context>
<context position="7178" citStr="Klebanov and Beigman (2014)" startWordPosition="1063" endWordPosition="1066">tic frame structures in a question answering task, to represent that the semantic frames can bear multiple sematic roles. Previous research has found that, for a few individual NLP tasks, training while incorporating label noise weight may produce a better model. Martinez Alonso et al. (2015) show that informing a parser of annotator disagreement via loss function reduced error in labeled attachments by 6.4%. Plank et al. (2014a) incorporate annotator disagreement in POS tags into the loss function of a POS-tag machine learner, resulting in improved performance on downstream chunking. Beigman Klebanov and Beigman (2014) observed that, on a task classifying text as semantically old or new, the inclusion of Hard Cases in training data resulted in reduced classifier performance on Easy Cases. 3 Overview of Experiments We built systems for the five NLP tasks, and trained them using aggregation, soft labeling, and instance screening strategies. When labels were numeric, the integrated label was the average2. When labels were nominal, the integrated label was majority vote. Krippendorff (1970)’s α item agreement was used to filter ambiguous training instances. For soft labeling, percentage item agreement was used </context>
<context position="9875" citStr="Klebanov and Beigman (2014)" startWordPosition="1491" endWordPosition="1494">ilize agreement cutoff parameters that vary per corpus. These strategies are a discretized approximation of the gradual effect of filtering low agreement instances from the training data. For any given corpus, we could not use a cutoff value equal to no filtering, or that eliminated a class. If there were only 2 remaining cutoffs, we used these. If there were more candidate cutoff values, we trained and evaluated a classifier on a development set and chose the value for HighAgree that maximized Hard Case performance on the development set. Percentage Agreement In this paper, we follow Beigman Klebanov and Beigman (2014) in using the nominal agreement categories Hard Cases and Easy Cases to separate instances by item agreement. However, unlike Beigman Klebanov and Beigman (2014) who use simple percentage agreement, we calculate item-specific agreement via Krippendorff (1970)’s α item agreement4, with Nominal, Ordinal, or Ratio distance metrics as appropriate. The agreement is expressed in the range (-1.0 – 1.0); 1.0 is perfect agreement. 3.1 Biased Language Detection This task detects the use of bias in political text. The corpus (Yano et al., 2010)5 consists of 1,041 sentences from American political blogs. </context>
</contexts>
<marker>Klebanov, Beigman, 2014</marker>
<rawString>Beata Beigman Klebanov and Eyal Beigman. 2014. Difficult cases: From data to learning, and back. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 390– 396, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
<author>Emily Jamison</author>
<author>Breck Baldwin</author>
</authors>
<title>Building a stemming corpus: Coding standards.</title>
<date>2009</date>
<note>http://lingpipe-blog.com/2009/ 02/25/stemming-morphology-corpuscoding-standards/.</note>
<contexts>
<context position="12197" citStr="Carpenter et al. (2009)" startWordPosition="1868" endWordPosition="1871">. 3.2 Morphological Stemming The goal of this binary classification task is to predict, given an original word and a stemmed version of the word, whether the stemmed version has been correctly stemmed. The word pair was correct if: the stemmed word contained one less affix; or if the original word was a compound, the stemmed word had a space inserted between the components; or if the original word was misspelled, the stemmed word was deleted; or if the original word had no affixes and was not a compound and was not misspelled, then the stemmed word had no changes. This dataset was compiled by Carpenter et al. (2009)6. The dataset contains 6679 word pairs; most pairs have 5 labels each. In the crossvalidation division, no pairs with the same original word could be split across training and test data. The gold standard was the integrated label, with 4898 positive and 1781 negative pairs. Hard/Easy Case cutoffs were &lt;-.5 and &gt;.5. Of 6679 total instances, 822 were Hard Cases (&lt;-.5) and 3615 were Easy Cases (&gt;.5). Features used are combinations of the characters after the removal of the longest common substring between the word pair, including 0-2 additional characters from the substring; word boundaries are </context>
</contexts>
<marker>Carpenter, Jamison, Baldwin, 2009</marker>
<rawString>Bob Carpenter, Emily Jamison, and Breck Baldwin. 2009. Building a stemming corpus: Coding standards. http://lingpipe-blog.com/2009/ 02/25/stemming-morphology-corpuscoding-standards/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. In Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising textual entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13458" citStr="Dagan et al., 2006" startWordPosition="2069" endWordPosition="2072">nclude: HighAgree Filtered for agreement &gt;-0.1. SLLimited MajVote with instances weighted by the frequency of the label for the text pair. 6Available at github.com/bob-carpenter/anno 293 3.3 Recognising Textual Entailment Recognizing textual entailment is the process of determining if, given two sentences text and hypothesis, the meaning of the hypothesis can be inferred from the text. We used the dataset from the PASCAL RTE-1, which contains 800 sentence pairs. The crowdsource annotations of 10 labels per pair were obtained by Snow et al. (2008)7. We reproduced the basic system described in (Dagan et al., 2006) of TF-IDF weighted Cosine Similarity between lemmas of the text and hypothesis. The weight of each wordi in documentj, n total documents, is the log-plus-one termi frequency normalized by raw termi document frequency, with Euclidean normalization. � = (1 + log(tfi,j)) N if tfi,j ≥ 1 weight(i, j)dfi 0 if tfi,j = 0 Additionally, we used features including the difference in noun chunk character and token length, the difference in number of tokens, shared named entities, and subtask names. The gold standard was the original labels from Dagan et al. (2006). Hard/Easy Case cutoffs were &lt;0.0 and &gt;.3</context>
<context position="19887" citStr="Dagan et al. (2006)" startWordPosition="3155" endWordPosition="3158">g data results in a worse model; this can be seen in the downward curves of Figures 3 – 6, where the curved line is HighAgree and the matching pattern straight line is Integrated. (Due to the low number of item agreement categories, Stemming results are not displayed in an item agreement cutoff table.) However, Figures 4 – 6 show the overall performance boost, and Figure 3 shows the Hard Case performance boost, right before the downward curves from too little training data, using HighAgree. Comparability We found the accuracy of our systems was similar to that reported in previous literature. Dagan et al. (2006) report performance of the RTE system, on a different data division, with accuracy=0.568. Hovy et al. (2014) report majority vote results (from acc=0.805 to acc=0.837 on a different data section) similar to our results of 10See Japkowicz and Shah (2011) for usage description. 0.790 micro-Fl. For Affective Text, Snow et al. (2008) report results on a different data section of r=0.174, a merged result from systems trained on combinations of crowdsource labels and evaluated against expert-trained systems. The SWAT system (Katz et al., 2007), which also used lexical resources and additional traini</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Machine learning challenges. Evaluating predictive uncertainty, visual object classification, and recognising textual entailment, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nilesh Dalvi</author>
<author>Anirban Dasgupta</author>
<author>Ravi Kumar</author>
<author>Vibhor Rastogi</author>
</authors>
<title>Aggregating crowdsourced binary ratings.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd International Conference on World Wide Web,</booktitle>
<pages>285--294</pages>
<location>Rio de Janeiro, Brazil.</location>
<contexts>
<context position="5128" citStr="Dalvi et al., 2013" startWordPosition="752" endWordPosition="755">ent from crowdsourced labels, for multiple natural language tasks; (3) to evaluate classifier performance on high item agreement (Easy Case) instances and low item agreement (Hard Case) instances across multiple natural language tasks. 2 Related Work Dekel and Shamir (2009) calculated integrated labels for an information retrieval crowdsourced dataset, and identified low-quality workers by deviation from the integrated label. Removal of these workers’ labels improved classifier performance on data that was not similarly filtered. While much work (Dawid and Skene, 1979; Ipeirotis et al., 2010; Dalvi et al., 2013) has explored techniques to model worker ability, bias, and instance difficulty while aggregating labels, there is no evaluation comparing classifiers trained on the new integrated labels with other options, on their respective NLP tasks. Training instance filtering aims to remove mislabeled instances from the training dataset. Sculley and Cormack (2008) learned a logistic regression classifier to identify and filter noisy labels in a spam email filtering task. They also proposed a label correcting technique that replaces identified noisy labels with “corrected” labels, at the risk of introduc</context>
</contexts>
<marker>Dalvi, Dasgupta, Kumar, Rastogi, 2013</marker>
<rawString>Nilesh Dalvi, Anirban Dasgupta, Ravi Kumar, and Vibhor Rastogi. 2013. Aggregating crowdsourced binary ratings. In Proceedings of the 22nd International Conference on World Wide Web, pages 285– 294, Rio de Janeiro, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Philip Dawid</author>
<author>Allan M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer errorrates using the EM algorithm.</title>
<date>1979</date>
<journal>Journal of the Royal Statistical Society. Series C (Applied Statistics),</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="5083" citStr="Dawid and Skene, 1979" startWordPosition="744" endWordPosition="747">(2) to filter training instances by item agreement from crowdsourced labels, for multiple natural language tasks; (3) to evaluate classifier performance on high item agreement (Easy Case) instances and low item agreement (Hard Case) instances across multiple natural language tasks. 2 Related Work Dekel and Shamir (2009) calculated integrated labels for an information retrieval crowdsourced dataset, and identified low-quality workers by deviation from the integrated label. Removal of these workers’ labels improved classifier performance on data that was not similarly filtered. While much work (Dawid and Skene, 1979; Ipeirotis et al., 2010; Dalvi et al., 2013) has explored techniques to model worker ability, bias, and instance difficulty while aggregating labels, there is no evaluation comparing classifiers trained on the new integrated labels with other options, on their respective NLP tasks. Training instance filtering aims to remove mislabeled instances from the training dataset. Sculley and Cormack (2008) learned a logistic regression classifier to identify and filter noisy labels in a spam email filtering task. They also proposed a label correcting technique that replaces identified noisy labels wit</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>Alexander Philip Dawid and Allan M. Skene. 1979. Maximum likelihood estimation of observer errorrates using the EM algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1):20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Oliver Ferschke</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>DKPro TC: A Java-based Framework for Supervised Learning Experiments on Textual Data.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>61--66</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="8922" citStr="Daxenberger et al., 2014" startWordPosition="1346" endWordPosition="1349"> Hard 2We followed Yano et al. (2010) and Strapparava and Mihalcea (2007) in using mean as gold standard. Although another aggregation such as as median might be more representative, such discussion is beyond the scope of this paper. 292 and Easy Cases, the training instances were unaffected, but the test instances were filtered by α item agreement. Hard/Easy Case parameters were chosen to divide the corpus by item agreement into roughly equal portions3, relative to the corpus, for post-hoc error analysis. All systems except Affect Recognition were constructed using DKPro Text Classification (Daxenberger et al., 2014), and used Weka’s SMO (Platt, 1999) or SMOreg (Shevade et al., 2000) implementations with default parameters, with 10- fold (or 5-fold, for computationally-intensive POS Tagging) cross-validation. More details are available in the Supplemental Notes document. Agreement Parameters Training strategies HighAgree and VeryHigh utilize agreement cutoff parameters that vary per corpus. These strategies are a discretized approximation of the gradual effect of filtering low agreement instances from the training data. For any given corpus, we could not use a cutoff value equal to no filtering, or that e</context>
</contexts>
<marker>Daxenberger, Ferschke, Gurevych, Zesch, 2014</marker>
<rawString>Johannes Daxenberger, Oliver Ferschke, Iryna Gurevych, and Torsten Zesch. 2014. DKPro TC: A Java-based Framework for Supervised Learning Experiments on Textual Data. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, pages 61–66, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ofer Dekel</author>
<author>Ohad Shamir</author>
</authors>
<title>Vox populi: Collecting high-quality labels from a crowd.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-Second Annual Conference on Learning Theory,</booktitle>
<location>Montreal, Canada.</location>
<note>Online proceedings.</note>
<contexts>
<context position="4783" citStr="Dekel and Shamir (2009)" startWordPosition="701" endWordPosition="704">ing a token-weight averaging, respectively. We use preexisting, freely-available crowdsourced datasets and post all our experiment code on GitHub1. Contributions This is the first work (1) to apply item-agreement-weighted soft labeling from crowdsourced labels to multiple real natural language tasks; (2) to filter training instances by item agreement from crowdsourced labels, for multiple natural language tasks; (3) to evaluate classifier performance on high item agreement (Easy Case) instances and low item agreement (Hard Case) instances across multiple natural language tasks. 2 Related Work Dekel and Shamir (2009) calculated integrated labels for an information retrieval crowdsourced dataset, and identified low-quality workers by deviation from the integrated label. Removal of these workers’ labels improved classifier performance on data that was not similarly filtered. While much work (Dawid and Skene, 1979; Ipeirotis et al., 2010; Dalvi et al., 2013) has explored techniques to model worker ability, bias, and instance difficulty while aggregating labels, there is no evaluation comparing classifiers trained on the new integrated labels with other options, on their respective NLP tasks. Training instanc</context>
</contexts>
<marker>Dekel, Shamir, 2009</marker>
<rawString>Ofer Dekel and Ohad Shamir. 2009. Vox populi: Collecting high-quality labels from a crowd. In Proceedings of the Twenty-Second Annual Conference on Learning Theory, Montreal, Canada. Online proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>42--47</pages>
<location>Portland, Oregon.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 42–47, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Barbara Plank</author>
<author>Anders Søgaard</author>
</authors>
<title>Experiments with crowdsourced re-annotation of a pos tagging data set.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>377--382</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="14580" citStr="Hovy et al. (2014)" startWordPosition="2261" endWordPosition="2264">ndard was the original labels from Dagan et al. (2006). Hard/Easy Case cutoffs were &lt;0.0 and &gt;.3. Training strategies are from Biased Language (VeryHigh) and Stem (others) experiments, except the HighAgree cutoff was 0.0 and the VeryHigh cutoff was 0.3. Of 800 total instances, 230 were Hard Cases (&lt;0.0) and 207 were Easy Cases (&gt;.30). 3.4 POS tagging We built a POS-tagger for Twitter posts. We used the training section of the dataset from Gimpel et al. (2011). The POS tagset was the universal tag set (Petrov et al., 2012); we converted Gimpel et al. (2011)’s tags to the universal tagset using Hovy et al. (2014)’s mapping. Crowdsource labels for this data came from Hovy et al. (2014)8, who obtained 5 labels for each tweet. After aligning and cleaning, our dataset consisted of 953 tweets of 14,439 tokens. We followed Hovy et al. (2014) in constructing a CRF classifier (Lafferty et al., 2001), using a list of English affixes, Hovy et al. (2014)’s set of orthographic features, and word clusters (Owoputi et al., 2013). In the cross-validation division, individual tweets were assigned to folds. The gold standard was the integrated label. Hard/Easy Case 7Available at sites.google.com/site/ nlpannotations/ </context>
<context position="19995" citStr="Hovy et al. (2014)" startWordPosition="3172" endWordPosition="3175">ine is HighAgree and the matching pattern straight line is Integrated. (Due to the low number of item agreement categories, Stemming results are not displayed in an item agreement cutoff table.) However, Figures 4 – 6 show the overall performance boost, and Figure 3 shows the Hard Case performance boost, right before the downward curves from too little training data, using HighAgree. Comparability We found the accuracy of our systems was similar to that reported in previous literature. Dagan et al. (2006) report performance of the RTE system, on a different data division, with accuracy=0.568. Hovy et al. (2014) report majority vote results (from acc=0.805 to acc=0.837 on a different data section) similar to our results of 10See Japkowicz and Shah (2011) for usage description. 0.790 micro-Fl. For Affective Text, Snow et al. (2008) report results on a different data section of r=0.174, a merged result from systems trained on combinations of crowdsource labels and evaluated against expert-trained systems. The SWAT system (Katz et al., 2007), which also used lexical resources and additional training data, acheived r=0.3898 on a different section of data. These results are comparable with ours, which ran</context>
</contexts>
<marker>Hovy, Plank, Søgaard, 2014</marker>
<rawString>Dirk Hovy, Barbara Plank, and Anders Søgaard. 2014. Experiments with crowdsourced re-annotation of a pos tagging data set. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 377–382, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panagiotis G Ipeirotis</author>
<author>Foster Provost</author>
<author>Jing Wang</author>
</authors>
<title>Quality management on Amazon Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM SIGKDD Workshop on Human Computation,</booktitle>
<pages>64--67</pages>
<location>Washington DC, USA.</location>
<contexts>
<context position="5107" citStr="Ipeirotis et al., 2010" startWordPosition="748" endWordPosition="751">instances by item agreement from crowdsourced labels, for multiple natural language tasks; (3) to evaluate classifier performance on high item agreement (Easy Case) instances and low item agreement (Hard Case) instances across multiple natural language tasks. 2 Related Work Dekel and Shamir (2009) calculated integrated labels for an information retrieval crowdsourced dataset, and identified low-quality workers by deviation from the integrated label. Removal of these workers’ labels improved classifier performance on data that was not similarly filtered. While much work (Dawid and Skene, 1979; Ipeirotis et al., 2010; Dalvi et al., 2013) has explored techniques to model worker ability, bias, and instance difficulty while aggregating labels, there is no evaluation comparing classifiers trained on the new integrated labels with other options, on their respective NLP tasks. Training instance filtering aims to remove mislabeled instances from the training dataset. Sculley and Cormack (2008) learned a logistic regression classifier to identify and filter noisy labels in a spam email filtering task. They also proposed a label correcting technique that replaces identified noisy labels with “corrected” labels, at</context>
</contexts>
<marker>Ipeirotis, Provost, Wang, 2010</marker>
<rawString>Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang. 2010. Quality management on Amazon Mechanical Turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation, pages 64–67, Washington DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathalie Japkowicz</author>
<author>Mohak Shah</author>
</authors>
<title>Evaluating learning algorithms: a classification perspective.</title>
<date>2011</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="20140" citStr="Japkowicz and Shah (2011)" startWordPosition="3196" endWordPosition="3199">ts are not displayed in an item agreement cutoff table.) However, Figures 4 – 6 show the overall performance boost, and Figure 3 shows the Hard Case performance boost, right before the downward curves from too little training data, using HighAgree. Comparability We found the accuracy of our systems was similar to that reported in previous literature. Dagan et al. (2006) report performance of the RTE system, on a different data division, with accuracy=0.568. Hovy et al. (2014) report majority vote results (from acc=0.805 to acc=0.837 on a different data section) similar to our results of 10See Japkowicz and Shah (2011) for usage description. 0.790 micro-Fl. For Affective Text, Snow et al. (2008) report results on a different data section of r=0.174, a merged result from systems trained on combinations of crowdsource labels and evaluated against expert-trained systems. The SWAT system (Katz et al., 2007), which also used lexical resources and additional training data, acheived r=0.3898 on a different section of data. These results are comparable with ours, which range from r=0.326 to r=0.453. 5 Conclusions and Future Work In this work, for five natural langauge tasks, we have examined the impact of informing</context>
</contexts>
<marker>Japkowicz, Shah, 2011</marker>
<rawString>Nathalie Japkowicz and Mohak Shah. 2011. Evaluating learning algorithms: a classification perspective. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Katz</author>
<author>Matt Singleton</author>
<author>Richard Wicentowski</author>
</authors>
<date>2007</date>
<booktitle>SWAT-MP:The SemEval-2007 Systems for Task 5 and Task 14. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>308--313</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="16469" citStr="Katz et al., 2007" startWordPosition="2575" endWordPosition="2578"> t E s. 3.5 Affect Recognition Our Affect Recognition experiments are based on the affective text annotation task in Strapparava and Mihalcea (2007), using the Sadness dataset. Each headline is rated for “sadness” using a scale of 0-100. Examples are in Figures 1 and 2. We use the crowdsourced annotation for a 100- headline sample of this dataset provided by Snow et al. (2008)9, with 10 annotations per emotion per headline. Of 100 total instances, 20 were Hard Cases (&lt;0.0) and 49 were Easy Cases (&gt;.30). Our system design is identical to Snow et al. (2008), which is similar to the SWAT system (Katz et al., 2007), a top-performing system on the SemEval Affective Text task. Hard/Easy Case cutoffs were &lt;0.0 and &gt;.3. Training strategies are the same as for the Biased Language experiments, except: VeryHigh Filtered for agreement &gt;0.3. HighAgree Filtered for agreement &gt;0. SLLimited SoftLabel, except that instances with a label distance &gt;20.0 from the original label average are discarded. 4 Results Our results on all five tasks, using each of the training strategies and variously evaluating on all, Easy, or Hard Cases, can be seen in Table 1. Systems outputing numeric values show results in Pearson correlat</context>
<context position="20430" citStr="Katz et al., 2007" startWordPosition="3241" endWordPosition="3244">ms was similar to that reported in previous literature. Dagan et al. (2006) report performance of the RTE system, on a different data division, with accuracy=0.568. Hovy et al. (2014) report majority vote results (from acc=0.805 to acc=0.837 on a different data section) similar to our results of 10See Japkowicz and Shah (2011) for usage description. 0.790 micro-Fl. For Affective Text, Snow et al. (2008) report results on a different data section of r=0.174, a merged result from systems trained on combinations of crowdsource labels and evaluated against expert-trained systems. The SWAT system (Katz et al., 2007), which also used lexical resources and additional training data, acheived r=0.3898 on a different section of data. These results are comparable with ours, which range from r=0.326 to r=0.453. 5 Conclusions and Future Work In this work, for five natural langauge tasks, we have examined the impact of informing the classifier of crowdsource item agreement, by means of soft labeling and removal of low-agreement training instances. We found a statistically significant benefit from low-agreement training filtering in four of our five tasks, and strongest improvements for Hard Cases. Previous work (</context>
</contexts>
<marker>Katz, Singleton, Wicentowski, 2007</marker>
<rawString>Phil Katz, Matt Singleton, and Richard Wicentowski. 2007. SWAT-MP:The SemEval-2007 Systems for Task 5 and Task 14. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 308–313, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Estimating the reliability, systematic error and random error of interval data.</title>
<date>1970</date>
<journal>Educational and Psychological Measurement,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="2447" citStr="Krippendorff (1970)" startWordPosition="351" endWordPosition="352">bel information is discarded by aggregation, and when the labels were gathered under controlled circumstances, these labels may reflect linguistic intuition and contain useful information (Plank et al., 2014b). Two alternative strategies that allow the classifier to learn from the item agreement include training instance filtering and soft labeling. Filtering training instances by item agreement removes low agreement instances from the training set. Soft labeling assigns a classifier weight to a training instance based on the item agreement. Consider two Affect Recognition instances and their Krippendorff (1970)’s α item agreement: Text: India’s Taj Mahal gets facelift Sadness Rating (0-100): 8.0 α Agreement (-1.0 – 1.0): 0.7 Figure 1: Affect Recognition Easy Case. Text: After Iraq trip, Clinton proposes war limits Sadness Rating (0-100): 12.5 α Agreement (-1.0 – 1.0): -0.1 Figure 2: Affect Recognition Hard Case. In Figure 1, annotators mostly agreed that the headline expresses little sadness. But in Figure 2, the low item agreement may be caused by instance difficulty (i.e., Is a war zone sad or just bad?): a Hard Case (Zeman, 2010). Previous work (Beigman Klebanov and Beigman, 2014; Beigman and Bei</context>
<context position="7655" citStr="Krippendorff (1970)" startWordPosition="1138" endWordPosition="1139"> the loss function of a POS-tag machine learner, resulting in improved performance on downstream chunking. Beigman Klebanov and Beigman (2014) observed that, on a task classifying text as semantically old or new, the inclusion of Hard Cases in training data resulted in reduced classifier performance on Easy Cases. 3 Overview of Experiments We built systems for the five NLP tasks, and trained them using aggregation, soft labeling, and instance screening strategies. When labels were numeric, the integrated label was the average2. When labels were nominal, the integrated label was majority vote. Krippendorff (1970)’s α item agreement was used to filter ambiguous training instances. For soft labeling, percentage item agreement was used to assign instance weights. We followed Sheng et al. (2008)’s suggested Multiplied Examples procedure: for each unlabeled instance xi and each existing label yi ∈ Li = {yid} (as annotated by worker j), we create one replica of xi, assign it yi, and weight the instance according to the count of yi in Li (i.e., the percentage item agrement). For each training strategy (SoftLabel, etc), the training instances were changed by the strategy, but the test instances were unaffecte</context>
<context position="10134" citStr="Krippendorff (1970)" startWordPosition="1530" endWordPosition="1531"> that eliminated a class. If there were only 2 remaining cutoffs, we used these. If there were more candidate cutoff values, we trained and evaluated a classifier on a development set and chose the value for HighAgree that maximized Hard Case performance on the development set. Percentage Agreement In this paper, we follow Beigman Klebanov and Beigman (2014) in using the nominal agreement categories Hard Cases and Easy Cases to separate instances by item agreement. However, unlike Beigman Klebanov and Beigman (2014) who use simple percentage agreement, we calculate item-specific agreement via Krippendorff (1970)’s α item agreement4, with Nominal, Ordinal, or Ratio distance metrics as appropriate. The agreement is expressed in the range (-1.0 – 1.0); 1.0 is perfect agreement. 3.1 Biased Language Detection This task detects the use of bias in political text. The corpus (Yano et al., 2010)5 consists of 1,041 sentences from American political blogs. For each sentence, five crowdsource annotators chose a label no bias, some bias, and very biased. We follow Yano et al. (2010) in representing the amount of bias on a numerical scale (1-3). Hard/Easy Case 3Limited by the discrete nature of our agreement. 4Fro</context>
</contexts>
<marker>Krippendorff, 1970</marker>
<rawString>Klaus Krippendorff. 1970. Estimating the reliability, systematic error and random error of interval data. Educational and Psychological Measurement, 30(1):61–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content analysis: An introduction to its methodology. Sage,</title>
<date>1980</date>
<location>Beverly Hills, California.</location>
<contexts>
<context position="11183" citStr="Krippendorff, 1980" startWordPosition="1697" endWordPosition="1698"> biased. We follow Yano et al. (2010) in representing the amount of bias on a numerical scale (1-3). Hard/Easy Case 3Limited by the discrete nature of our agreement. 4From the DKPro Statistics library (Meyer et al., 2014) 5Available at sites.google.com/site/ amtworkshop2010/data-1 cutoffs were &lt;-.21 and &gt;.20. Of 1041 total instances, 161 were Hard Cases (&lt;-.21) and 499 were Easy Cases (&gt;.20). We built an SVM regression task using unigrams, to predict the numerical amount of bias. The gold standard was the integrated labels. Itemspecific agreement was calculated with Ordinal Distance Function (Krippendorff, 1980). We used the following training strategies: VeryHigh Filtered for agreement &gt;0.4. HighAgree Filtered for agreement &gt;-0.2. SoftLabel One training instance is generated for each label from a text, and weighted by how many times that label occurred with the text. SLLimited SoftLabel, except that training instances with a label distance &gt;1.0 from the original text label average are discarded. 3.2 Morphological Stemming The goal of this binary classification task is to predict, given an original word and a stemmed version of the word, whether the stemmed version has been correctly stemmed. The wor</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content analysis: An introduction to its methodology. Sage, Beverly Hills, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>Williamstown, Massachusetts.</location>
<contexts>
<context position="14864" citStr="Lafferty et al., 2001" startWordPosition="2309" endWordPosition="2312">0 were Hard Cases (&lt;0.0) and 207 were Easy Cases (&gt;.30). 3.4 POS tagging We built a POS-tagger for Twitter posts. We used the training section of the dataset from Gimpel et al. (2011). The POS tagset was the universal tag set (Petrov et al., 2012); we converted Gimpel et al. (2011)’s tags to the universal tagset using Hovy et al. (2014)’s mapping. Crowdsource labels for this data came from Hovy et al. (2014)8, who obtained 5 labels for each tweet. After aligning and cleaning, our dataset consisted of 953 tweets of 14,439 tokens. We followed Hovy et al. (2014) in constructing a CRF classifier (Lafferty et al., 2001), using a list of English affixes, Hovy et al. (2014)’s set of orthographic features, and word clusters (Owoputi et al., 2013). In the cross-validation division, individual tweets were assigned to folds. The gold standard was the integrated label. Hard/Easy Case 7Available at sites.google.com/site/ nlpannotations/ 8Available at lowlands.ku.dk/results/ cutoffs were &lt;0.0 and &gt;.49. Of 14,439 tokens, 649 were Hard Cases (&lt;0.0) and 10830 were Easy Cases (&gt;.49). We used the following strategies: VeryHigh For each token t in sequence s where agreement(t) &lt;0.5, s is broken into two separate sequences </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282– 289, Williamstown, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H´ector Martinez Alonso</author>
<author>Barbara Plank</author>
<author>Arne Skjærholt</author>
<author>Anders Søgaard</author>
</authors>
<title>Learning to parse with IAA-weighted loss.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1357--1361</pages>
<location>Denver, Colorado.</location>
<contexts>
<context position="6844" citStr="Alonso et al. (2015)" startWordPosition="1011" endWordPosition="1014">hted, conflicting labels, is a technique to model noisy training data. Thiel (2008) found that soft labeled training data produced more accurate classifiers than hard labeled training data, with both Radial Basis Function Networks and Fuzzy-Input FuzzyOutput SVMs. Shen and Lapata (2007) used soft labeling to model their semantic frame structures in a question answering task, to represent that the semantic frames can bear multiple sematic roles. Previous research has found that, for a few individual NLP tasks, training while incorporating label noise weight may produce a better model. Martinez Alonso et al. (2015) show that informing a parser of annotator disagreement via loss function reduced error in labeled attachments by 6.4%. Plank et al. (2014a) incorporate annotator disagreement in POS tags into the loss function of a POS-tag machine learner, resulting in improved performance on downstream chunking. Beigman Klebanov and Beigman (2014) observed that, on a task classifying text as semantically old or new, the inclusion of Hard Cases in training data resulted in reduced classifier performance on Easy Cases. 3 Overview of Experiments We built systems for the five NLP tasks, and trained them using ag</context>
</contexts>
<marker>Alonso, Plank, Skjærholt, Søgaard, 2015</marker>
<rawString>H´ector Martinez Alonso, Barbara Plank, Arne Skjærholt, and Anders Søgaard. 2015. Learning to parse with IAA-weighted loss. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1357–1361, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the sampling error of the difference between correlated proportions or percentages.</title>
<date>1947</date>
<journal>Psychometrika,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="18781" citStr="McNemar, 1947" startWordPosition="2970" endWordPosition="2971">0.5 Agreement cutoff Figure 6: Affective Text. Hard Cases All Cases Easy Cases Easy Cases Hard Cases All Cases Hard Cases Easy Cases All Cases micro Fl 0.8 0.6 0.4 0.2 0.2 0.1 0 Pearson correlation Hard Cases Easy Cases All Cases did not significantly outperform Integrated. However, HighAgree does outperform Integrated on 4 or the 5 tasks, especially for Hard Cases: Hard Case improvements for Biased Language and POS Tagging, and Affective Text, and overall improvements for RTE, POS Tagging, and Affective Text were significant (Paired TTest, p &lt; 0.05, for numerical output, or McNemar’s Test10 (McNemar, 1947), p &lt; 0.05, for nominal classes). The fifth task, Stemming, had the lowest number of item agreement categories of the five tasks, preventing fine-grained agreement training filtering, which explains why filtering shows no benefit. All training strategies used the same amount of annotated data as input, and for filtering strategies such as HighAgree, a reduced number of strategyoutput instances are used to train the model. As a higher cutoff is used for HighAgree, the lack of training data results in a worse model; this can be seen in the downward curves of Figures 3 – 6, where the curved line </context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12(2):153–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Margot Mieskes</author>
<author>Christian Stab</author>
<author>Iryna Gurevych</author>
</authors>
<title>DKPro Agreement: An open-source java library for measuring interrater agreement.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>105--109</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="10785" citStr="Meyer et al., 2014" startWordPosition="1636" endWordPosition="1639">al, Ordinal, or Ratio distance metrics as appropriate. The agreement is expressed in the range (-1.0 – 1.0); 1.0 is perfect agreement. 3.1 Biased Language Detection This task detects the use of bias in political text. The corpus (Yano et al., 2010)5 consists of 1,041 sentences from American political blogs. For each sentence, five crowdsource annotators chose a label no bias, some bias, and very biased. We follow Yano et al. (2010) in representing the amount of bias on a numerical scale (1-3). Hard/Easy Case 3Limited by the discrete nature of our agreement. 4From the DKPro Statistics library (Meyer et al., 2014) 5Available at sites.google.com/site/ amtworkshop2010/data-1 cutoffs were &lt;-.21 and &gt;.20. Of 1041 total instances, 161 were Hard Cases (&lt;-.21) and 499 were Easy Cases (&gt;.20). We built an SVM regression task using unigrams, to predict the numerical amount of bias. The gold standard was the integrated labels. Itemspecific agreement was calculated with Ordinal Distance Function (Krippendorff, 1980). We used the following training strategies: VeryHigh Filtered for agreement &gt;0.4. HighAgree Filtered for agreement &gt;-0.2. SoftLabel One training instance is generated for each label from a text, and we</context>
</contexts>
<marker>Meyer, Mieskes, Stab, Gurevych, 2014</marker>
<rawString>Christian M. Meyer, Margot Mieskes, Christian Stab, and Iryna Gurevych. 2014. DKPro Agreement: An open-source java library for measuring interrater agreement. In Proceedings of the 25th International Conference on Computational Linguistics (COLING), pages 105–109, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Nowak</author>
<author>Stefan R¨uger</author>
</authors>
<title>How reliable are annotations via crowdsourcing: A study about inter-annotator agreement for multi-label image annotation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Multimedia Information Retrieval,</booktitle>
<pages>557--566</pages>
<location>Philadelphia, Pennsylvania.</location>
<marker>Nowak, R¨uger, 2010</marker>
<rawString>Stefanie Nowak and Stefan R¨uger. 2010. How reliable are annotations via crowdsourcing: A study about inter-annotator agreement for multi-label image annotation. In Proceedings of the International Conference on Multimedia Information Retrieval, pages 557–566, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>380--390</pages>
<location>Atlanta,</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 380–390, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),</booktitle>
<pages>2089--2096</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="14489" citStr="Petrov et al., 2012" startWordPosition="2245" endWordPosition="2248">h, the difference in number of tokens, shared named entities, and subtask names. The gold standard was the original labels from Dagan et al. (2006). Hard/Easy Case cutoffs were &lt;0.0 and &gt;.3. Training strategies are from Biased Language (VeryHigh) and Stem (others) experiments, except the HighAgree cutoff was 0.0 and the VeryHigh cutoff was 0.3. Of 800 total instances, 230 were Hard Cases (&lt;0.0) and 207 were Easy Cases (&gt;.30). 3.4 POS tagging We built a POS-tagger for Twitter posts. We used the training section of the dataset from Gimpel et al. (2011). The POS tagset was the universal tag set (Petrov et al., 2012); we converted Gimpel et al. (2011)’s tags to the universal tagset using Hovy et al. (2014)’s mapping. Crowdsource labels for this data came from Hovy et al. (2014)8, who obtained 5 labels for each tweet. After aligning and cleaning, our dataset consisted of 953 tweets of 14,439 tokens. We followed Hovy et al. (2014) in constructing a CRF classifier (Lafferty et al., 2001), using a list of English affixes, Hovy et al. (2014)’s set of orthographic features, and word clusters (Owoputi et al., 2013). In the cross-validation division, individual tweets were assigned to folds. The gold standard was</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pages 2089–2096, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Learning part-of-speech taggers with interannotator agreement loss.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--751</pages>
<location>Gothenburg,</location>
<contexts>
<context position="2035" citStr="Plank et al., 2014" startWordPosition="288" endWordPosition="291">ity voting for label aggregation has been found effective in filtering noisy labels (Nowak and R¨uger, 2010). Labels can be aggregated under weighted conditions reflecting the reliability of the annotator (Whitehill et al., 2009; Welinder et al., 2010). Certain classifiers are also robust to random (unbiased) label noise (Tibshirani and Manning, 2014; Beigman and Beigman Klebanov, 2009). However, minority label information is discarded by aggregation, and when the labels were gathered under controlled circumstances, these labels may reflect linguistic intuition and contain useful information (Plank et al., 2014b). Two alternative strategies that allow the classifier to learn from the item agreement include training instance filtering and soft labeling. Filtering training instances by item agreement removes low agreement instances from the training set. Soft labeling assigns a classifier weight to a training instance based on the item agreement. Consider two Affect Recognition instances and their Krippendorff (1970)’s α item agreement: Text: India’s Taj Mahal gets facelift Sadness Rating (0-100): 8.0 α Agreement (-1.0 – 1.0): 0.7 Figure 1: Affect Recognition Easy Case. Text: After Iraq trip, Clinton </context>
<context position="6982" citStr="Plank et al. (2014" startWordPosition="1034" endWordPosition="1037">rate classifiers than hard labeled training data, with both Radial Basis Function Networks and Fuzzy-Input FuzzyOutput SVMs. Shen and Lapata (2007) used soft labeling to model their semantic frame structures in a question answering task, to represent that the semantic frames can bear multiple sematic roles. Previous research has found that, for a few individual NLP tasks, training while incorporating label noise weight may produce a better model. Martinez Alonso et al. (2015) show that informing a parser of annotator disagreement via loss function reduced error in labeled attachments by 6.4%. Plank et al. (2014a) incorporate annotator disagreement in POS tags into the loss function of a POS-tag machine learner, resulting in improved performance on downstream chunking. Beigman Klebanov and Beigman (2014) observed that, on a task classifying text as semantically old or new, the inclusion of Hard Cases in training data resulted in reduced classifier performance on Easy Cases. 3 Overview of Experiments We built systems for the five NLP tasks, and trained them using aggregation, soft labeling, and instance screening strategies. When labels were numeric, the integrated label was the average2. When labels </context>
</contexts>
<marker>Plank, Hovy, Søgaard, 2014</marker>
<rawString>Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014a. Learning part-of-speech taggers with interannotator agreement loss. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742– 751, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Linguistically debatable or just plain wrong?</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>507--511</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="2035" citStr="Plank et al., 2014" startWordPosition="288" endWordPosition="291">ity voting for label aggregation has been found effective in filtering noisy labels (Nowak and R¨uger, 2010). Labels can be aggregated under weighted conditions reflecting the reliability of the annotator (Whitehill et al., 2009; Welinder et al., 2010). Certain classifiers are also robust to random (unbiased) label noise (Tibshirani and Manning, 2014; Beigman and Beigman Klebanov, 2009). However, minority label information is discarded by aggregation, and when the labels were gathered under controlled circumstances, these labels may reflect linguistic intuition and contain useful information (Plank et al., 2014b). Two alternative strategies that allow the classifier to learn from the item agreement include training instance filtering and soft labeling. Filtering training instances by item agreement removes low agreement instances from the training set. Soft labeling assigns a classifier weight to a training instance based on the item agreement. Consider two Affect Recognition instances and their Krippendorff (1970)’s α item agreement: Text: India’s Taj Mahal gets facelift Sadness Rating (0-100): 8.0 α Agreement (-1.0 – 1.0): 0.7 Figure 1: Affect Recognition Easy Case. Text: After Iraq trip, Clinton </context>
<context position="6982" citStr="Plank et al. (2014" startWordPosition="1034" endWordPosition="1037">rate classifiers than hard labeled training data, with both Radial Basis Function Networks and Fuzzy-Input FuzzyOutput SVMs. Shen and Lapata (2007) used soft labeling to model their semantic frame structures in a question answering task, to represent that the semantic frames can bear multiple sematic roles. Previous research has found that, for a few individual NLP tasks, training while incorporating label noise weight may produce a better model. Martinez Alonso et al. (2015) show that informing a parser of annotator disagreement via loss function reduced error in labeled attachments by 6.4%. Plank et al. (2014a) incorporate annotator disagreement in POS tags into the loss function of a POS-tag machine learner, resulting in improved performance on downstream chunking. Beigman Klebanov and Beigman (2014) observed that, on a task classifying text as semantically old or new, the inclusion of Hard Cases in training data resulted in reduced classifier performance on Easy Cases. 3 Overview of Experiments We built systems for the five NLP tasks, and trained them using aggregation, soft labeling, and instance screening strategies. When labels were numeric, the integrated label was the average2. When labels </context>
</contexts>
<marker>Plank, Hovy, Søgaard, 2014</marker>
<rawString>Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014b. Linguistically debatable or just plain wrong? In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 507–511, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1999</date>
<booktitle>In Advances in kernel methods – support vector learning,</booktitle>
<pages>185--208</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8957" citStr="Platt, 1999" startWordPosition="1354" endWordPosition="1355">arava and Mihalcea (2007) in using mean as gold standard. Although another aggregation such as as median might be more representative, such discussion is beyond the scope of this paper. 292 and Easy Cases, the training instances were unaffected, but the test instances were filtered by α item agreement. Hard/Easy Case parameters were chosen to divide the corpus by item agreement into roughly equal portions3, relative to the corpus, for post-hoc error analysis. All systems except Affect Recognition were constructed using DKPro Text Classification (Daxenberger et al., 2014), and used Weka’s SMO (Platt, 1999) or SMOreg (Shevade et al., 2000) implementations with default parameters, with 10- fold (or 5-fold, for computationally-intensive POS Tagging) cross-validation. More details are available in the Supplemental Notes document. Agreement Parameters Training strategies HighAgree and VeryHigh utilize agreement cutoff parameters that vary per corpus. These strategies are a discretized approximation of the gradual effect of filtering low agreement instances from the training data. For any given corpus, we could not use a cutoff value equal to no filtering, or that eliminated a class. If there were on</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John Platt. 1999. Fast training of support vector machines using sequential minimal optimization. In Advances in kernel methods – support vector learning, pages 185–208. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas C Raykar</author>
<author>Shipeng Yu</author>
<author>Linda H Zhao</author>
<author>Gerardo Hermosillo Valadez</author>
<author>Charles Florin</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
</authors>
<title>Learning from crowds.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--1297</pages>
<contexts>
<context position="5903" citStr="Raykar et al. (2010)" startWordPosition="871" endWordPosition="874">ned on the new integrated labels with other options, on their respective NLP tasks. Training instance filtering aims to remove mislabeled instances from the training dataset. Sculley and Cormack (2008) learned a logistic regression classifier to identify and filter noisy labels in a spam email filtering task. They also proposed a label correcting technique that replaces identified noisy labels with “corrected” labels, at the risk of introducing noise into the corpus. Rebbapragada et al. (2009) developed a label noise detection technique to cluster training instances and remove label outliers. Raykar et al. (2010) jointly learned a classifier/regressor, annotator accuracy, and the integrated label on datasets with multiple noisy labels, outperforming Smyth et al. (1995)’s model 1github.com/EmilyKJamison/crowdsourcing of estimating ground truth labels. Soft labeling, or the association of one training instance with multiple, weighted, conflicting labels, is a technique to model noisy training data. Thiel (2008) found that soft labeled training data produced more accurate classifiers than hard labeled training data, with both Radial Basis Function Networks and Fuzzy-Input FuzzyOutput SVMs. Shen and Lapat</context>
</contexts>
<marker>Raykar, Yu, Zhao, Valadez, Florin, Bogoni, Moy, 2010</marker>
<rawString>Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. 2010. Learning from crowds. The Journal of Machine Learning Research, 11:1297–1322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umaa Rebbapragada</author>
<author>Lukas Mandrake</author>
<author>Kiri L Wagstaff</author>
<author>Damhnait Gleeson</author>
<author>Rebecca Castano</author>
<author>Steve Chien</author>
<author>Carla E Brodley</author>
</authors>
<title>Improving onboard analysis of hyperion images by filtering mislabeled training data examples.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 IEEE Aerospace Conference,</booktitle>
<pages>1--9</pages>
<location>Big Sky, Montana.</location>
<contexts>
<context position="5781" citStr="Rebbapragada et al. (2009)" startWordPosition="852" endWordPosition="856"> model worker ability, bias, and instance difficulty while aggregating labels, there is no evaluation comparing classifiers trained on the new integrated labels with other options, on their respective NLP tasks. Training instance filtering aims to remove mislabeled instances from the training dataset. Sculley and Cormack (2008) learned a logistic regression classifier to identify and filter noisy labels in a spam email filtering task. They also proposed a label correcting technique that replaces identified noisy labels with “corrected” labels, at the risk of introducing noise into the corpus. Rebbapragada et al. (2009) developed a label noise detection technique to cluster training instances and remove label outliers. Raykar et al. (2010) jointly learned a classifier/regressor, annotator accuracy, and the integrated label on datasets with multiple noisy labels, outperforming Smyth et al. (1995)’s model 1github.com/EmilyKJamison/crowdsourcing of estimating ground truth labels. Soft labeling, or the association of one training instance with multiple, weighted, conflicting labels, is a technique to model noisy training data. Thiel (2008) found that soft labeled training data produced more accurate classifiers </context>
</contexts>
<marker>Rebbapragada, Mandrake, Wagstaff, Gleeson, Castano, Chien, Brodley, 2009</marker>
<rawString>Umaa Rebbapragada, Lukas Mandrake, Kiri L. Wagstaff, Damhnait Gleeson, Rebecca Castano, Steve Chien, and Carla E. Brodley. 2009. Improving onboard analysis of hyperion images by filtering mislabeled training data examples. In Proceedings of the 2009 IEEE Aerospace Conference, pages 1–9, Big Sky, Montana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sculley</author>
<author>Gordon V Cormack</author>
</authors>
<title>Filtering email spam in the presence of noisy user feedback.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Email andAntispam (CEAS),</booktitle>
<location>Mountain View, CA, USA.</location>
<note>Online proceedings.</note>
<contexts>
<context position="5484" citStr="Sculley and Cormack (2008)" startWordPosition="804" endWordPosition="808">aset, and identified low-quality workers by deviation from the integrated label. Removal of these workers’ labels improved classifier performance on data that was not similarly filtered. While much work (Dawid and Skene, 1979; Ipeirotis et al., 2010; Dalvi et al., 2013) has explored techniques to model worker ability, bias, and instance difficulty while aggregating labels, there is no evaluation comparing classifiers trained on the new integrated labels with other options, on their respective NLP tasks. Training instance filtering aims to remove mislabeled instances from the training dataset. Sculley and Cormack (2008) learned a logistic regression classifier to identify and filter noisy labels in a spam email filtering task. They also proposed a label correcting technique that replaces identified noisy labels with “corrected” labels, at the risk of introducing noise into the corpus. Rebbapragada et al. (2009) developed a label noise detection technique to cluster training instances and remove label outliers. Raykar et al. (2010) jointly learned a classifier/regressor, annotator accuracy, and the integrated label on datasets with multiple noisy labels, outperforming Smyth et al. (1995)’s model 1github.com/E</context>
</contexts>
<marker>Sculley, Cormack, 2008</marker>
<rawString>D. Sculley and Gordon V. Cormack. 2008. Filtering email spam in the presence of noisy user feedback. In Proceedings of the Conference on Email andAntispam (CEAS), Mountain View, CA, USA. Online proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>12--21</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6511" citStr="Shen and Lapata (2007)" startWordPosition="958" endWordPosition="961"> et al. (2010) jointly learned a classifier/regressor, annotator accuracy, and the integrated label on datasets with multiple noisy labels, outperforming Smyth et al. (1995)’s model 1github.com/EmilyKJamison/crowdsourcing of estimating ground truth labels. Soft labeling, or the association of one training instance with multiple, weighted, conflicting labels, is a technique to model noisy training data. Thiel (2008) found that soft labeled training data produced more accurate classifiers than hard labeled training data, with both Radial Basis Function Networks and Fuzzy-Input FuzzyOutput SVMs. Shen and Lapata (2007) used soft labeling to model their semantic frame structures in a question answering task, to represent that the semantic frames can bear multiple sematic roles. Previous research has found that, for a few individual NLP tasks, training while incorporating label noise weight may produce a better model. Martinez Alonso et al. (2015) show that informing a parser of annotator disagreement via loss function reduced error in labeled attachments by 6.4%. Plank et al. (2014a) incorporate annotator disagreement in POS tags into the loss function of a POS-tag machine learner, resulting in improved perf</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 12–21, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S Sheng</author>
<author>Foster Provost</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Get another label? Improving data quality and data mining using multiple, noisy labelers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>614--622</pages>
<location>Las Vegas, Nevada.</location>
<contexts>
<context position="7837" citStr="Sheng et al. (2008)" startWordPosition="1165" endWordPosition="1168">xt as semantically old or new, the inclusion of Hard Cases in training data resulted in reduced classifier performance on Easy Cases. 3 Overview of Experiments We built systems for the five NLP tasks, and trained them using aggregation, soft labeling, and instance screening strategies. When labels were numeric, the integrated label was the average2. When labels were nominal, the integrated label was majority vote. Krippendorff (1970)’s α item agreement was used to filter ambiguous training instances. For soft labeling, percentage item agreement was used to assign instance weights. We followed Sheng et al. (2008)’s suggested Multiplied Examples procedure: for each unlabeled instance xi and each existing label yi ∈ Li = {yid} (as annotated by worker j), we create one replica of xi, assign it yi, and weight the instance according to the count of yi in Li (i.e., the percentage item agrement). For each training strategy (SoftLabel, etc), the training instances were changed by the strategy, but the test instances were unaffected. For the division of test instances into Hard 2We followed Yano et al. (2010) and Strapparava and Mihalcea (2007) in using mean as gold standard. Although another aggregation such </context>
</contexts>
<marker>Sheng, Provost, Ipeirotis, 2008</marker>
<rawString>Victor S. Sheng, Foster Provost, and Panagiotis G. Ipeirotis. 2008. Get another label? Improving data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 614–622, Las Vegas, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shirish Krishnaj Shevade</author>
<author>S Sathiya Keerthi</author>
<author>Chiranjib Bhattacharyya</author>
<author>Karaturi Radha Krishna Murthy</author>
</authors>
<title>Improvements to the SMO algorithm for SVM regression.</title>
<date>2000</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>11</volume>
<issue>5</issue>
<contexts>
<context position="8990" citStr="Shevade et al., 2000" startWordPosition="1358" endWordPosition="1361">) in using mean as gold standard. Although another aggregation such as as median might be more representative, such discussion is beyond the scope of this paper. 292 and Easy Cases, the training instances were unaffected, but the test instances were filtered by α item agreement. Hard/Easy Case parameters were chosen to divide the corpus by item agreement into roughly equal portions3, relative to the corpus, for post-hoc error analysis. All systems except Affect Recognition were constructed using DKPro Text Classification (Daxenberger et al., 2014), and used Weka’s SMO (Platt, 1999) or SMOreg (Shevade et al., 2000) implementations with default parameters, with 10- fold (or 5-fold, for computationally-intensive POS Tagging) cross-validation. More details are available in the Supplemental Notes document. Agreement Parameters Training strategies HighAgree and VeryHigh utilize agreement cutoff parameters that vary per corpus. These strategies are a discretized approximation of the gradual effect of filtering low agreement instances from the training data. For any given corpus, we could not use a cutoff value equal to no filtering, or that eliminated a class. If there were only 2 remaining cutoffs, we used t</context>
</contexts>
<marker>Shevade, Keerthi, Bhattacharyya, Murthy, 2000</marker>
<rawString>Shirish Krishnaj Shevade, S. Sathiya Keerthi, Chiranjib Bhattacharyya, and Karaturi Radha Krishna Murthy. 2000. Improvements to the SMO algorithm for SVM regression. IEEE Transactions on Neural Networks, 11(5):1188–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
<author>Usama Fayyad</author>
<author>Michael Burl</author>
<author>Pietro Perona</author>
<author>Pierre Baldi</author>
</authors>
<title>Inferring ground truth from subjective labelling of Venus images.</title>
<date>1995</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>1085--1092</pages>
<contexts>
<context position="6062" citStr="Smyth et al. (1995)" startWordPosition="893" endWordPosition="896">ining dataset. Sculley and Cormack (2008) learned a logistic regression classifier to identify and filter noisy labels in a spam email filtering task. They also proposed a label correcting technique that replaces identified noisy labels with “corrected” labels, at the risk of introducing noise into the corpus. Rebbapragada et al. (2009) developed a label noise detection technique to cluster training instances and remove label outliers. Raykar et al. (2010) jointly learned a classifier/regressor, annotator accuracy, and the integrated label on datasets with multiple noisy labels, outperforming Smyth et al. (1995)’s model 1github.com/EmilyKJamison/crowdsourcing of estimating ground truth labels. Soft labeling, or the association of one training instance with multiple, weighted, conflicting labels, is a technique to model noisy training data. Thiel (2008) found that soft labeled training data produced more accurate classifiers than hard labeled training data, with both Radial Basis Function Networks and Fuzzy-Input FuzzyOutput SVMs. Shen and Lapata (2007) used soft labeling to model their semantic frame structures in a question answering task, to represent that the semantic frames can bear multiple sema</context>
</contexts>
<marker>Smyth, Fayyad, Burl, Perona, Baldi, 1995</marker>
<rawString>Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. 1995. Inferring ground truth from subjective labelling of Venus images. Advances in Neural Information Processing Systems, pages 1085–1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254–263, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Rada Mihalcea</author>
</authors>
<title>SemEval-2007 Task 14: Affective Text.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>70--74</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8370" citStr="Strapparava and Mihalcea (2007)" startWordPosition="1258" endWordPosition="1262">ng, percentage item agreement was used to assign instance weights. We followed Sheng et al. (2008)’s suggested Multiplied Examples procedure: for each unlabeled instance xi and each existing label yi ∈ Li = {yid} (as annotated by worker j), we create one replica of xi, assign it yi, and weight the instance according to the count of yi in Li (i.e., the percentage item agrement). For each training strategy (SoftLabel, etc), the training instances were changed by the strategy, but the test instances were unaffected. For the division of test instances into Hard 2We followed Yano et al. (2010) and Strapparava and Mihalcea (2007) in using mean as gold standard. Although another aggregation such as as median might be more representative, such discussion is beyond the scope of this paper. 292 and Easy Cases, the training instances were unaffected, but the test instances were filtered by α item agreement. Hard/Easy Case parameters were chosen to divide the corpus by item agreement into roughly equal portions3, relative to the corpus, for post-hoc error analysis. All systems except Affect Recognition were constructed using DKPro Text Classification (Daxenberger et al., 2014), and used Weka’s SMO (Platt, 1999) or SMOreg (S</context>
<context position="15999" citStr="Strapparava and Mihalcea (2007)" startWordPosition="2491" endWordPosition="2494">For each token t in sequence s where agreement(t) &lt;0.5, s is broken into two separate sequences s1 and s2 and t is deleted (i.e. filtered). HighAgree VeryHigh with agreement &lt;0.2. SoftLabel For each proto-sequence s, we generate 5 sequences {s0, s1,..., sil, in which each token t is assigned a crowdsource label drawn at random: lt,si E Lt. SLLimited, Each token t in sequence s is assigned its MajVote label. Then s is given a weight representing the average item agreement for all t E s. 3.5 Affect Recognition Our Affect Recognition experiments are based on the affective text annotation task in Strapparava and Mihalcea (2007), using the Sadness dataset. Each headline is rated for “sadness” using a scale of 0-100. Examples are in Figures 1 and 2. We use the crowdsourced annotation for a 100- headline sample of this dataset provided by Snow et al. (2008)9, with 10 annotations per emotion per headline. Of 100 total instances, 20 were Hard Cases (&lt;0.0) and 49 were Easy Cases (&gt;.30). Our system design is identical to Snow et al. (2008), which is similar to the SWAT system (Katz et al., 2007), a top-performing system on the SemEval Affective Text task. Hard/Easy Case cutoffs were &lt;0.0 and &gt;.3. Training strategies are th</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>Carlo Strapparava and Rada Mihalcea. 2007. SemEval-2007 Task 14: Affective Text. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 70–74, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Thiel</author>
</authors>
<title>Classification on soft labels is robust against label noise.</title>
<date>2008</date>
<booktitle>In Knowledge-Based Intelligent Information and Engineering Systems,</booktitle>
<pages>65--73</pages>
<location>Wellington, New Zealand.</location>
<contexts>
<context position="6307" citStr="Thiel (2008)" startWordPosition="928" endWordPosition="929">” labels, at the risk of introducing noise into the corpus. Rebbapragada et al. (2009) developed a label noise detection technique to cluster training instances and remove label outliers. Raykar et al. (2010) jointly learned a classifier/regressor, annotator accuracy, and the integrated label on datasets with multiple noisy labels, outperforming Smyth et al. (1995)’s model 1github.com/EmilyKJamison/crowdsourcing of estimating ground truth labels. Soft labeling, or the association of one training instance with multiple, weighted, conflicting labels, is a technique to model noisy training data. Thiel (2008) found that soft labeled training data produced more accurate classifiers than hard labeled training data, with both Radial Basis Function Networks and Fuzzy-Input FuzzyOutput SVMs. Shen and Lapata (2007) used soft labeling to model their semantic frame structures in a question answering task, to represent that the semantic frames can bear multiple sematic roles. Previous research has found that, for a few individual NLP tasks, training while incorporating label noise weight may produce a better model. Martinez Alonso et al. (2015) show that informing a parser of annotator disagreement via los</context>
</contexts>
<marker>Thiel, 2008</marker>
<rawString>Christian Thiel. 2008. Classification on soft labels is robust against label noise. In Knowledge-Based Intelligent Information and Engineering Systems, pages 65–73, Wellington, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Tibshirani</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust logistic regression using shift parameters.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>124--129</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="1769" citStr="Tibshirani and Manning, 2014" startWordPosition="250" endWordPosition="253">e or ten labels are collected for an instance, and are aggregated together into an integrated label. The high number of labels is used to compensate for worker bias, task misunderstanding, lack of interest, incompetance, and malicious intent (Wauthier and Jordan, 2011). Majority voting for label aggregation has been found effective in filtering noisy labels (Nowak and R¨uger, 2010). Labels can be aggregated under weighted conditions reflecting the reliability of the annotator (Whitehill et al., 2009; Welinder et al., 2010). Certain classifiers are also robust to random (unbiased) label noise (Tibshirani and Manning, 2014; Beigman and Beigman Klebanov, 2009). However, minority label information is discarded by aggregation, and when the labels were gathered under controlled circumstances, these labels may reflect linguistic intuition and contain useful information (Plank et al., 2014b). Two alternative strategies that allow the classifier to learn from the item agreement include training instance filtering and soft labeling. Filtering training instances by item agreement removes low agreement instances from the training set. Soft labeling assigns a classifier weight to a training instance based on the item agre</context>
</contexts>
<marker>Tibshirani, Manning, 2014</marker>
<rawString>Julie Tibshirani and Christopher D. Manning. 2014. Robust logistic regression using shift parameters. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 124– 129, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian L Wauthier</author>
<author>Michael I Jordan</author>
</authors>
<title>Bayesian bias mitigation for crowdsourcing.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1800--1808</pages>
<contexts>
<context position="1410" citStr="Wauthier and Jordan, 2011" startWordPosition="197" endWordPosition="200">ia soft labeling and low-agreement filtering of the training dataset. We find a statistically significant benefit from low item agreement training filtering in four of our five tasks, and no systematic benefit from soft labeling. 1 Introduction Crowdsourcing is a cheap and increasinglyutilized source of annotation labels. In a typical annotation task, five or ten labels are collected for an instance, and are aggregated together into an integrated label. The high number of labels is used to compensate for worker bias, task misunderstanding, lack of interest, incompetance, and malicious intent (Wauthier and Jordan, 2011). Majority voting for label aggregation has been found effective in filtering noisy labels (Nowak and R¨uger, 2010). Labels can be aggregated under weighted conditions reflecting the reliability of the annotator (Whitehill et al., 2009; Welinder et al., 2010). Certain classifiers are also robust to random (unbiased) label noise (Tibshirani and Manning, 2014; Beigman and Beigman Klebanov, 2009). However, minority label information is discarded by aggregation, and when the labels were gathered under controlled circumstances, these labels may reflect linguistic intuition and contain useful inform</context>
</contexts>
<marker>Wauthier, Jordan, 2011</marker>
<rawString>Fabian L. Wauthier and Michael I. Jordan. 2011. Bayesian bias mitigation for crowdsourcing. In Advances in Neural Information Processing Systems, pages 1800–1808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Welinder</author>
<author>Steve Branson</author>
<author>Pietro Perona</author>
<author>Serge J Belongie</author>
</authors>
<title>The multidimensional wisdom of crowds.</title>
<date>2010</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2424--2432</pages>
<contexts>
<context position="1669" citStr="Welinder et al., 2010" startWordPosition="236" endWordPosition="239"> cheap and increasinglyutilized source of annotation labels. In a typical annotation task, five or ten labels are collected for an instance, and are aggregated together into an integrated label. The high number of labels is used to compensate for worker bias, task misunderstanding, lack of interest, incompetance, and malicious intent (Wauthier and Jordan, 2011). Majority voting for label aggregation has been found effective in filtering noisy labels (Nowak and R¨uger, 2010). Labels can be aggregated under weighted conditions reflecting the reliability of the annotator (Whitehill et al., 2009; Welinder et al., 2010). Certain classifiers are also robust to random (unbiased) label noise (Tibshirani and Manning, 2014; Beigman and Beigman Klebanov, 2009). However, minority label information is discarded by aggregation, and when the labels were gathered under controlled circumstances, these labels may reflect linguistic intuition and contain useful information (Plank et al., 2014b). Two alternative strategies that allow the classifier to learn from the item agreement include training instance filtering and soft labeling. Filtering training instances by item agreement removes low agreement instances from the t</context>
</contexts>
<marker>Welinder, Branson, Perona, Belongie, 2010</marker>
<rawString>Peter Welinder, Steve Branson, Pietro Perona, and Serge J. Belongie. 2010. The multidimensional wisdom of crowds. In Advances in Neural Information Processing Systems, pages 2424–2432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Whitehill</author>
<author>Ting fan Wu</author>
<author>Jacob Bergsma</author>
<author>Javier R Movellan</author>
<author>Paul L Ruvolo</author>
</authors>
<title>Whose vote should count more: Optimal integration of labels from labelers of unknown expertise.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>2035--2043</pages>
<editor>In Y. Bengio, D. Schuurmans, J.D. Lafferty, C.K.I. Williams, and A. Culotta, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="1645" citStr="Whitehill et al., 2009" startWordPosition="232" endWordPosition="235">ction Crowdsourcing is a cheap and increasinglyutilized source of annotation labels. In a typical annotation task, five or ten labels are collected for an instance, and are aggregated together into an integrated label. The high number of labels is used to compensate for worker bias, task misunderstanding, lack of interest, incompetance, and malicious intent (Wauthier and Jordan, 2011). Majority voting for label aggregation has been found effective in filtering noisy labels (Nowak and R¨uger, 2010). Labels can be aggregated under weighted conditions reflecting the reliability of the annotator (Whitehill et al., 2009; Welinder et al., 2010). Certain classifiers are also robust to random (unbiased) label noise (Tibshirani and Manning, 2014; Beigman and Beigman Klebanov, 2009). However, minority label information is discarded by aggregation, and when the labels were gathered under controlled circumstances, these labels may reflect linguistic intuition and contain useful information (Plank et al., 2014b). Two alternative strategies that allow the classifier to learn from the item agreement include training instance filtering and soft labeling. Filtering training instances by item agreement removes low agreem</context>
</contexts>
<marker>Whitehill, Wu, Bergsma, Movellan, Ruvolo, 2009</marker>
<rawString>Jacob Whitehill, Ting fan Wu, Jacob Bergsma, Javier R. Movellan, and Paul L. Ruvolo. 2009. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In Y. Bengio, D. Schuurmans, J.D. Lafferty, C.K.I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems, pages 2035– 2043. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tae Yano</author>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>Shedding (a thousand points of) light on biased language.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>152--158</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="8334" citStr="Yano et al. (2010)" startWordPosition="1253" endWordPosition="1256">tances. For soft labeling, percentage item agreement was used to assign instance weights. We followed Sheng et al. (2008)’s suggested Multiplied Examples procedure: for each unlabeled instance xi and each existing label yi ∈ Li = {yid} (as annotated by worker j), we create one replica of xi, assign it yi, and weight the instance according to the count of yi in Li (i.e., the percentage item agrement). For each training strategy (SoftLabel, etc), the training instances were changed by the strategy, but the test instances were unaffected. For the division of test instances into Hard 2We followed Yano et al. (2010) and Strapparava and Mihalcea (2007) in using mean as gold standard. Although another aggregation such as as median might be more representative, such discussion is beyond the scope of this paper. 292 and Easy Cases, the training instances were unaffected, but the test instances were filtered by α item agreement. Hard/Easy Case parameters were chosen to divide the corpus by item agreement into roughly equal portions3, relative to the corpus, for post-hoc error analysis. All systems except Affect Recognition were constructed using DKPro Text Classification (Daxenberger et al., 2014), and used W</context>
<context position="10414" citStr="Yano et al., 2010" startWordPosition="1575" endWordPosition="1578">rcentage Agreement In this paper, we follow Beigman Klebanov and Beigman (2014) in using the nominal agreement categories Hard Cases and Easy Cases to separate instances by item agreement. However, unlike Beigman Klebanov and Beigman (2014) who use simple percentage agreement, we calculate item-specific agreement via Krippendorff (1970)’s α item agreement4, with Nominal, Ordinal, or Ratio distance metrics as appropriate. The agreement is expressed in the range (-1.0 – 1.0); 1.0 is perfect agreement. 3.1 Biased Language Detection This task detects the use of bias in political text. The corpus (Yano et al., 2010)5 consists of 1,041 sentences from American political blogs. For each sentence, five crowdsource annotators chose a label no bias, some bias, and very biased. We follow Yano et al. (2010) in representing the amount of bias on a numerical scale (1-3). Hard/Easy Case 3Limited by the discrete nature of our agreement. 4From the DKPro Statistics library (Meyer et al., 2014) 5Available at sites.google.com/site/ amtworkshop2010/data-1 cutoffs were &lt;-.21 and &gt;.20. Of 1041 total instances, 161 were Hard Cases (&lt;-.21) and 499 were Easy Cases (&gt;.20). We built an SVM regression task using unigrams, to pre</context>
</contexts>
<marker>Yano, Resnik, Smith, 2010</marker>
<rawString>Tae Yano, Philip Resnik, and Noah A. Smith. 2010. Shedding (a thousand points of) light on biased language. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 152–158, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Hard problems of tagset conversion.</title>
<date>2010</date>
<booktitle>In Proceedings of the Second International Conference on Global Interoperability for Language Resources,</booktitle>
<pages>181--185</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="2979" citStr="Zeman, 2010" startWordPosition="440" endWordPosition="441">reement. Consider two Affect Recognition instances and their Krippendorff (1970)’s α item agreement: Text: India’s Taj Mahal gets facelift Sadness Rating (0-100): 8.0 α Agreement (-1.0 – 1.0): 0.7 Figure 1: Affect Recognition Easy Case. Text: After Iraq trip, Clinton proposes war limits Sadness Rating (0-100): 12.5 α Agreement (-1.0 – 1.0): -0.1 Figure 2: Affect Recognition Hard Case. In Figure 1, annotators mostly agreed that the headline expresses little sadness. But in Figure 2, the low item agreement may be caused by instance difficulty (i.e., Is a war zone sad or just bad?): a Hard Case (Zeman, 2010). Previous work (Beigman Klebanov and Beigman, 2014; Beigman and Beigman Klebanov, 2009) has shown that training strategy may affect Hard and Easy Case test instances differently. In this work, for five natural language tasks, we examine the impact of passing crowdsource item agreement on to the task classifier, by means of training instance filtering and soft labeling. We construct classifiers for Biased Text Detection, Stemming Classification, Recognizing Textual Entailment, Twitter POS Tagging, and Affect Recognition, and evaluate the effect of our different training strategies on the accur</context>
</contexts>
<marker>Zeman, 2010</marker>
<rawString>Daniel Zeman. 2010. Hard problems of tagset conversion. In Proceedings of the Second International Conference on Global Interoperability for Language Resources, pages 181–185, Hong Kong, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
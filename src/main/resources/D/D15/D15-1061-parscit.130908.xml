<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019068">
<title confidence="0.957418">
An Entity-centric Approach for Overcoming Knowledge Graph
Sparsity
</title>
<author confidence="0.976754">
Manjunath Hegde Partha Talukdar
</author>
<affiliation confidence="0.99346">
Indian Institute of Science Indian Institute of Science
</affiliation>
<email confidence="0.988418">
manjunath@ssl.serc.iisc.in ppt@serc.iisc.in
</email>
<sectionHeader confidence="0.979329" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999692259259259">
Automatic construction of knowledge
graphs (KGs) from unstructured text
has received considerable attention in
recent research, resulting in the con-
struction of several KGs with millions
of entities (nodes) and facts (edges)
among them. Unfortunately, such KGs
tend to be severely sparse in terms of
number of facts known for a given en-
tity, i.e., have low knowledge density.
For example, the NELL KG consists
of only 1.34 facts per entity. Unfor-
tunately, such low knowledge density
makes it challenging to use such KGs
in real-world applications. In contrast
to best-effort extraction paradigms fol-
lowed in the construction of such KGs,
in this paper we argue in favor of
ENTIty Centric Expansion (ENTICE),
an entity-centric KG population frame-
work, to alleviate the low knowledge
density problem in existing KGs. By
using ENTICE, we are able to increase
NELL’s knowledge density by a factor
of 7.7 at 75.5% accuracy. Additionally,
we are also able to extend the ontology
discovering new relations and entities.
</bodyText>
<sectionHeader confidence="0.997335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960363636364">
Over the last few years, automatic construc-
tion of knowledge graphs (KGs) from web-
scale text data has received considerable at-
tention, resulting in the construction of sev-
eral large KGs such as NELL (Mitchell et al.,
2015), Google’s Knowledge Vault (Dong et al.,
2014). These KGs consist of millions of en-
tities and facts involving them. While mea-
suring size of the KGs in terms of number of
entities and facts is helpful, they don’t read-
ily capture the volume of knowledge needed in
</bodyText>
<table confidence="0.983661666666667">
Known Target New Target
Entity Entity
Known KR-KE KR-NE
Relation
New NR-KE NR-NE
Relation
</table>
<tableCaption confidence="0.997328">
Table 1: Any new fact involving a source en-
</tableCaption>
<bodyText confidence="0.997917575757576">
tity from a Knowledge Graph (i.e., facts of the
form entity1-relation-entity2 where entity1 is
already in the KG) can be classified into one of
the four extraction classes shown above. Most
KG population techniques tend to focus on ex-
tracting facts of the KR-KE class. ENTICE,
the entity-centric approach proposed in this
paper, is able to extract facts of all four classes.
real-world applications. When such a KG is
used in an application, one is often interested
in known facts for a given entity, and not nec-
essarily the overall size of the KG. In particu-
lar, knowing the average number of facts per
entity is quite informative. We shall refer to
this as the knowledge density of the KG.
Low knowledge density (or high sparsity) in
automatically constructed KGs has been rec-
ognized in recent research (West et al., 2014).
For example, NELL KG has a knowledge den-
sity of 1.34. Such low knowledge density puts
significant limitations on the utility of these
KGs. Construction of such KGs tend to follow
a batch paradigm: the knowledge extraction
system makes a full pass over the text corpus
extracting whatever knowledge it finds, and fi-
nally aggregating all extractions into a graph.
Clearly, such best-effort extraction paradigm
has proved to be inadequate to address the low
knowledge density issue mentioned above. We
refer to such paradigm as best-effort since its
attention is divided equally among all possible
entities.
Recently, a few entity-centric methods have
</bodyText>
<page confidence="0.949862">
530
</page>
<note confidence="0.9340105">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 530–535,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figureCaption confidence="0.999867">
Figure 1: Dataflow and architecture and of ENTICE. See Section 3 for details.
</figureCaption>
<bodyText confidence="0.997424666666667">
been proposed to increase knowledge density
in KGs (Gardner et al., 2013; Gardner et
al., 2014). In contrast to the best-effort ap-
proaches mentioned above, these entity-centric
approaches aim at increasing knowledge den-
sity for a given entity. A new fact involving
the given entity can belong to one of the four
types shown in Table 1. Unfortunately, these
densifying techniques only aim at identifying
instances of known relations among entities al-
ready present in the KG, i.e., they fall in the
KR-KE type of Table 1.
In this paper we propose ENTIty Centric
Expansion (ENTICE), an entity-centric
knowledge densifying framework which, given
an entity, is capable of extracting facts be-
longing to all the four types shown in Table 1.
By using ENTICE, we are able to increase
NELL’s knowledge density by a factor of 7.71,
while achieving 75.4% accuracy. Our goal
here is to draw attention to the effectiveness
of entity-centric approaches with bigger scope
(i.e., covering all four extraction classes in
Table 1) towards improving knowledge den-
sity, and that even relatively straightforward
techniques can go a long way in alleviating
low knowledge density in existing state-of-
the-art KGs. ENTICE code is available at:
https://github.com/malllabiisc/entity-centric-
kb-pop
</bodyText>
<sectionHeader confidence="0.991552" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.841608666666667">
Open Information Extraction (OIE) systems
(Yates et al., 2007; Fader et al., 2011; Schmitz
et al., 2012) aim at extracting textual triples of
</bodyText>
<footnote confidence="0.823533">
&apos;Measured with respect to the five categories exper-
imented with in the paper. See Section 4 for details.
</footnote>
<bodyText confidence="0.999930486486486">
the form noun phrase-predicate-noun phrase.
While such systems aim for extraction cover-
age, and because they operate in an ontology-
free setting, they don’t directly address the
problem of improving knowledge density in on-
tological KGs such as NELL. However, OIE
extractions provide a suitable starting point
which is exploited by ENTICE.
(Gal´arraga et al., 2014) addresses the prob-
lem of normalizing (or canonicalizing) OIE ex-
tractions which can be considered as one of the
components of ENTICE (see Section 3.3).
As previously mentioned, recent proposals
for improving density of KGs such as those re-
ported in (Gardner et al., 2013; Gardner et al.,
2014) focus on extracting facts of one of the
four extraction classes mentioned in Table 1,
viz., KR-KE. The KBP challenge (Surdeanu,
2013) also focuses on extracting facts while
keeping the relation set fixed, i.e., it addresses
the KR-KE and KR-NE extraction classes.
A method to improve knowledge density in
KGs by using search engine query logs and
a question answering system is presented in
(West et al., 2014). The proprietary nature of
datasets and tools used in this approach limits
its applicability in our setting.
ENTICE aims to improve knowledge den-
sity by extracting facts from all four extrac-
tion classes, i.e., for a given entity, it extracts
facts involving known relations, identifies po-
tentially new relations that might be relevant
for this entity, establishes such relations be-
tween the given entity and other known as
well as new entities – all in a single system.
While various parts of this problem have been
studied in isolation in the past, ENTICE is
</bodyText>
<page confidence="0.986908">
531
</page>
<bodyText confidence="0.999029333333333">
the first system to the best of our knowledge
that addresses the complete problem as a sin-
gle framework.
</bodyText>
<sectionHeader confidence="0.4180055" genericHeader="method">
3 ENTIty Centric Expansion
(ENTICE)
</sectionHeader>
<bodyText confidence="0.998198666666667">
Overall architecture and dataflow within EN-
TICE is shown in Figure 1. We describe each
of the components in the sections below.
</bodyText>
<subsectionHeader confidence="0.997055">
3.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.999995357142857">
Given the source entity, documents relevant
to it are downloaded by issues queries against
Google. In order to make the query specific,
especially in case of ambiguous entities, a few
keywords are also added to the query. For the
experiments in this paper, the category is used
as the keyword. For example, for the entity Al-
bert Einstein from the scientist category, the
query will be ”Albert Einstein scientist”. Top
20 documents returned by the search engine
are downloaded and processed further. Text
is extracted from the raw downloaded docu-
ments using regex patters, HTML tag match-
ing, and by using the Boilerpipe tool2.
</bodyText>
<subsectionHeader confidence="0.999145">
3.2 Triple Extraction
</subsectionHeader>
<bodyText confidence="0.999992928571429">
Text of each document obtained in the pre-
vious step is processed through the Stanford
CoreNLP toolkit (Manning et al., 2014) for
tokenization, coreference resolution, and de-
pendency parsing. Tokenized and coreference-
resolved sentences are then passes through
OpenIEv4 system 3 to extract (noun phrase,
predicate, noun phrase) triples. Multiple and
overlapping triples from the sentence was per-
mitted. Length filter is applied on the noun
phrase and the predicate of the triple ex-
tracted. This eliminates triples whose predi-
cate is more than 6 tokens and noun phrase
more than 7 tokens.
</bodyText>
<subsectionHeader confidence="0.884629">
3.3 Noun and Relation Phrase
Normalization
</subsectionHeader>
<bodyText confidence="0.999844333333333">
Noun phrases (NPs) and relation phrases ob-
tained from the previous step are normalized
(or canonicalized) in this step. Canopy clus-
tering technique as proposed in (Gal´arraga et
al., 2014) was used for noun phrase as well re-
lation phrase clustering. Initial clustering is
</bodyText>
<footnote confidence="0.9991255">
2Boilerpipe: http://code.google.com/p/boilerpipe
3OpenIEv4: http://knowitall.github.io/openie/
</footnote>
<bodyText confidence="0.999831086956522">
done over the unlinked noun phrases in the
triples. Please note that since we are working
in an entity-centric manner, one of the two
NPs present in the triple is already connected
to the knowledge graph, and hence is consid-
ered linked. To cluster noun phrases, we first
construct canopies corresponding to each word
in the noun phrase. For example, for noun
phrase Albert Einstein, we create two canopies,
viz., a canopy for Albert and another canopy
for Einstein, and add Albert Einstein to both
canopies. Grouping of noun phrases inside the
canopy is the next step of clustering phase.
Noun phrase similarity is calculated based on
similarity of words in the noun phrases. Word
similarity is either direct string matching or
Gensim similarity score4, which internally uses
word2vec embeddings (Mikolov et al., 2013).
After calculating pairwise similarity of noun
phrases, hierarchical clustering is carried out
to group noun phrases inside each canopy. A
threshold score is used to stop hierarchical
clustering. At the end of this process, we have
canopies and groups of noun phrases inside
them. A noun phrase can be in more than one
canopy, hence those groups across canopies are
merged if the similarity is greater than certain
threshold. After this, each group will contain
facts which have similar noun phrases and dif-
ferent (or same) relation phrase. Again the
facts are clustered based on the similarity of
the relation phrase. Relation phrase similar-
ity calculation step resembles the one used for
noun phrases as described above.
After this triple clustering step, the best
representative triple from each cluster is se-
lected based on a few rules. We consider
the structure of POS tags in noun phrases of
a triple as one of the criteria. Secondly, if
both noun phrases in the triple are linked to
the knowledge graph, then it makes the triple
more likely to become a representative tuple
of the cluster. Also, if the NPs present in the
triple are frequent in the cluster, then it makes
the corresponding triple more like to become
a representative.
</bodyText>
<footnote confidence="0.970254">
4https://github.com/piskvorky/gensim/
</footnote>
<page confidence="0.986613">
532
</page>
<table confidence="0.999682222222222">
Category Knowledge Knowledge # Facts # Correct Accuracy
Density in Density after Evaluated Facts
NELL ENTICE
Scientist 1.27 18.5 164 141 85.97
Universities 1.17 9 197 141 71.57
Books 1.34 4.49 202 165 81.68
Birds 1.27 6.69 194 136 70.10
Cars 1.5 11.61 201 140 69.65
Overall 1.3 10.05 958 723 75.46
</table>
<tableCaption confidence="0.895320333333333">
Table 2: Knowledge densities of five categories in NELL and after application of ENTICE, along
with resulting accuracy. We observe that overall, ENTICE is able to increase knowledge density
by a factor of 7.7 at 75.5% accuracy. This is our main result.
</tableCaption>
<table confidence="0.99970225">
Entity Name All facts in NELL Sample facts extracted by EN- Extraction
TICE Class
George Paget (George Paget Thomson, isIn- (Sir George Thomson, isFellowOf, NR-KE
Thomson stanceOf, scientist) Royal Society) KR-NE
(George Thomson, hasSpouse, Kath- KR-KE
leen Buchanan Smith)
(George Paget Thomson, diedOn,
September 10)
</table>
<tableCaption confidence="0.862940333333333">
Table 3: Facts corresponding to an entity from the scientists domain in NELL as well as those
extracted by ENTICE. While NELL contained only one fact for this entity, ENTICE was able
to extract 15 facts for this entity, only 3 of which are shown above.
</tableCaption>
<table confidence="0.999476666666667">
Category KR - KE KR - NE NR - KE NR - NE
correct wrong acc. correct wrong acc. correct wrong acc. correct wrong acc.
facts facts facts facts facts facts facts facts
Scientists 57 10 85.07 61 8 88.40 14 3 82.35 9 2 81.81
Cars 68 35 66.01 58 21 73.41 9 5 64.28 5 0 100
Universities 52 30 63.41 68 20 77.27 9 2 81.81 12 4 75
Books 78 24 76.47 79 12 86.81 2 0 100 6 1 85.71
Birds 67 29 69.79 46 19 70.76 15 4 78.94 8 6 57.14
Overall 322 128 71.55 312 80 79.59 49 14 77.77 40 13 75.47
</table>
<tableCaption confidence="0.997698">
Table 4: Accuracy breakdown over ENTICE extractions for each of the four extraction classes
</tableCaption>
<bodyText confidence="0.955363">
in Table 1. For each category, approximately 200 extractions were evaluated using Mechanical
Turk.
</bodyText>
<subsectionHeader confidence="0.9443955">
3.4 Integrating with Knowledge
Graph
</subsectionHeader>
<bodyText confidence="0.999946821428571">
The set of normalized triples from the pre-
vious step are linked with the Knowledge
Graph, whenever possible, in this step. For
a given normalized triple, following steps are
performed as part of linking. First, category
of each noun phrase in the triple is obtained
based on string matching. In case of no match,
refinements like dropping of adjectives, con-
sidering only noun phrases are done to for re-
matching. Now, the relation phrase is mapped
to an existing predicate in the KG based on
the extraction patterns in the metadata of the
target relation (e.g., NELL and many other
KGs have such metadata available). Can-
didate predicates are chosen from the above
mapped predicates based on category signa-
ture of the two noun phrases (i.e. entity1 and
entity2). This is possible since the all the pred-
icates in NELL have the type signature defined
in the metadata. Frequency of the relation
phrase in the metadata is used as a criteria to
select a candidate from multiple predicates. If
such category-signature based mapping is not
possible, then the predicate is listed as a new
relation, and the corresponding triple marked
to belong to either NR-KE or NE-NE extrac-
tion class, depending on whether the target
entity is already present in the KG or not.
</bodyText>
<page confidence="0.998657">
533
</page>
<sectionHeader confidence="0.99747" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.980465581081081">
In order to evaluate effectiveness of ENTICE,
we apply it to increase knowledge density for
100 randomly selected entities from each of the
following five NELL categories: Scientist, Uni-
versities, Books, Birds, and Cars. For each
category, a random subset of extractions in
that category was evaluated using Mechanical
Turk. To get a better accuracy of the eval-
uation, each fact was evaluated by 3 workers.
Workers were made to classify each fact as cor-
rect, incorrect or can’t say. Only those facts
classified as correct by 2 or more evaluators
were considered as correct facts.
Main Result: Experimental results com-
paring knowledge densities in NELL and after
application of ENTICE, along with the accu-
racy of extractions, are presented in Table 2.
From this, we observe that ENTICE is able to
improve knowledge density in NELL by a fac-
tor of 7.7 while maintaining 75.5% accuracy.
Sample extraction examples and accuracy per-
extraction class are presented in Table 3 and
Table 4, respectively.
Noun and Relation Phrase Normaliza-
tion: We didn’t perform any intrinsic eval-
uation of the entity and relation normaliza-
tion step. However, in this section, we pro-
vide a few anecdotal examples to give a sense
of the output quality from this step. We ob-
serve that the canopy clustering algorithm for
entity and normalization is able to cluster to-
gether facts with somewhat different surface
representations. For example, the algorithm
came up with the following cluster with two
facts: {(J. Willard Milnor, was awarded,
2011 Abel Prize); (John Milnor, received, Abel
Prize)}. It is encouraging to see that the sys-
tem is able to put J. Willard Milnor and John
Milnor together, even though they have some-
what different surface forms (only one word
overlap). Similarly, the relation phrases was
awarded and received are also considered to
be equivalent in the context of these beliefs.
Integrating with Knowledge Graph:
Based on evaluation over a random-sampling,
we find that entity linking in ENTICE is 92%
accurate, while relation linking is about 70%
accurate.
In the entity linking stage, adjectives
present in a noun phrase (NP) were ignored
while matching the noun phrase to entities in
the knowledge graph (NELL KB in this case).
In case the whole NP didn’t find any match,
part of the NP was used to retrieve its cat-
egory, if any. For example, in (Georg Walde-
mar Cantor, was born in, 1854), the NP Georg
Waldemar Cantor was mapped to category
person using his last name and 1854 to cat-
egory date. The relation phrase ”was born
in” maps to many predicates in NELL rela-
tional metadata. NELL predicate AtDate was
selected based on the rule that category sig-
nature of the predicate matches the category
of the noun phrases present in the triple. It
also has the highest frequency count for the
relational phrase in the metadata.
We observed that relation mapping has
lesser accuracy due to two reasons. Firstly,
error in determining right categories of NPs
present in a triple; and secondly, due to
higher ambiguity involving relation phrases in
general, i.e., a single relation phrase usually
matches many relation predicates in the on-
tology.
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999992904761905">
This paper presents ENTICE, a simple but
effective entity-centric framework for increas-
ing knowledge densities in automatically con-
structed knowledge graphs. We find that EN-
TICE is able to significantly increase NELL’s
knowledge density by a factor of 7.7 at 75.5%
accuracy. In addition to extracting new facts,
ENTICE is also able to extend the ontol-
ogy. Our goal in this paper is twofold: (1)
to draw attention to the effectiveness of entity-
centric approaches with bigger scope (i.e., cov-
ering all four extraction classes in Table 1) to-
wards improving knowledge density; and (2)
to demonstrate that even relatively straight-
forward techniques can go a long way in allevi-
ating low knowledge density in existing state-
of- the-art KGs. While these initial results are
encouraging, we hope to apply ENTICE on
other knowledge graphs, and also experiment
with other normalization and entity linking al-
gorithms as part of future work.
</bodyText>
<sectionHeader confidence="0.981052" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.983312666666667">
This work is supported in part by a gift from
Google. Thanks to Uday Saini for carefully
reading a draft of the paper.
</bodyText>
<page confidence="0.997012">
534
</page>
<sectionHeader confidence="0.975783" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999627943661972">
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz,
Wilko Horn, Ni Lao, Kevin Murphy, Thomas
Strohmann, Shaohua Sun, and Wei Zhang.
2014. Knowledge vault: A web-scale approach
to probabilistic knowledge fusion. In Proceed-
ings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data
mining.
Anthony Fader, Stephen Soderland, and Oren Et-
zioni. 2011. Identifying relations for open infor-
mation extraction. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1535–1545. Association
for Computational Linguistics.
Luis Gal´arraga, Geremy Heitz, Kevin Murphy, and
Fabian M Suchanek. 2014. Canonicalizing open
knowledge bases. In Proceedings of the 23rd
ACM International Conference on Conference
on Information and Knowledge Management,
pages 1679–1688. ACM.
Matt Gardner, Partha Pratim Talukdar, Bryan
Kisiel, and Tom Mitchell. 2013. Improving
learning and inference in a large knowledge-base
using latent syntactic cues.
Matt Gardner, Partha Pratim Talukdar, Jayant
Krishnamurthy, and Tom Mitchell. 2014. Incor-
porating vector space similarity in random walk
inference over knowledge bases.
Christopher D Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J Bethard, and
David McClosky. 2014. The stanford corenlp
natural language processing toolkit. In Proceed-
ings of 52nd Annual Meeting of the Association
for Computational Linguistics: System Demon-
strations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013. Distributed rep-
resentations of words and phrases and their com-
positionality. In Advances in neural information
processing systems, pages 3111–3119.
T Mitchell, W Cohen, E Hruschka, P Talukdar,
J Betteridge, A Carlson, B Dalvi, M Gardner,
B Kisiel, J Krishnamurthy, et al. 2015. Never-
ending learning. In Proceedings of AAAI.
Michael Schmitz, Robert Bart, Stephen Soderland,
Oren Etzioni, et al. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 523–
534. Association for Computational Linguistics.
Mihai Surdeanu. 2013. Overview of the tac2013
knowledge base population evaluation: English
slot filling and temporal slot filling. In Proceed-
ings of the Sixth Text Analysis Conference (TAC
2013).
Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin.
2014. Knowledge base completion via search-
based question answering. In Proceedings of
the 23rd international conference on World wide
web.
Alexander Yates, Michael Cafarella, Michele
Banko, Oren Etzioni, Matthew Broadhead, and
Stephen Soderland. 2007. Textrunner: open
information extraction on the web. In Proceed-
ings of Human Language Technologies: The An-
nual Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Demonstrations, pages 25–26. Association
for Computational Linguistics.
</reference>
<page confidence="0.998532">
535
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.266107">
<title confidence="0.982528">An Entity-centric Approach for Overcoming Knowledge Graph Sparsity</title>
<author confidence="0.811412">Manjunath Hegde Partha Talukdar</author>
<affiliation confidence="0.961582">Indian Institute of Science Indian Institute of Science</affiliation>
<abstract confidence="0.97704875862069">manjunath@ssl.serc.iisc.in ppt@serc.iisc.in Abstract Automatic construction of knowledge graphs (KGs) from unstructured text has received considerable attention in recent research, resulting in the construction of several KGs with millions of entities (nodes) and facts (edges) among them. Unfortunately, such KGs tend to be severely sparse in terms of of facts known for a eni.e., have low For example, the NELL KG consists of only 1.34 facts per entity. Unfortunately, such low knowledge density makes it challenging to use such KGs in real-world applications. In contrast paradigms followed in the construction of such KGs, in this paper we argue in favor of ENTIty Centric Expansion (ENTICE), population framework, to alleviate the low knowledge density problem in existing KGs. By using ENTICE, we are able to increase NELL’s knowledge density by a factor of 7.7 at 75.5% accuracy. Additionally, we are also able to extend the ontology discovering new relations and entities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xin Dong</author>
<author>Evgeniy Gabrilovich</author>
<author>Geremy Heitz</author>
<author>Wilko Horn</author>
<author>Ni Lao</author>
<author>Kevin Murphy</author>
<author>Thomas Strohmann</author>
<author>Shaohua Sun</author>
<author>Wei Zhang</author>
</authors>
<title>Knowledge vault: A web-scale approach to probabilistic knowledge fusion.</title>
<date>2014</date>
<booktitle>In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<contexts>
<context position="1504" citStr="Dong et al., 2014" startWordPosition="228" endWordPosition="231">y Centric Expansion (ENTICE), an entity-centric KG population framework, to alleviate the low knowledge density problem in existing KGs. By using ENTICE, we are able to increase NELL’s knowledge density by a factor of 7.7 at 75.5% accuracy. Additionally, we are also able to extend the ontology discovering new relations and entities. 1 Introduction Over the last few years, automatic construction of knowledge graphs (KGs) from webscale text data has received considerable attention, resulting in the construction of several large KGs such as NELL (Mitchell et al., 2015), Google’s Knowledge Vault (Dong et al., 2014). These KGs consist of millions of entities and facts involving them. While measuring size of the KGs in terms of number of entities and facts is helpful, they don’t readily capture the volume of knowledge needed in Known Target New Target Entity Entity Known KR-KE KR-NE Relation New NR-KE NR-NE Relation Table 1: Any new fact involving a source entity from a Knowledge Graph (i.e., facts of the form entity1-relation-entity2 where entity1 is already in the KG) can be classified into one of the four extraction classes shown above. Most KG population techniques tend to focus on extracting facts of</context>
</contexts>
<marker>Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, Zhang, 2014</marker>
<rawString>Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4969" citStr="Fader et al., 2011" startWordPosition="787" endWordPosition="790">E, we are able to increase NELL’s knowledge density by a factor of 7.71, while achieving 75.4% accuracy. Our goal here is to draw attention to the effectiveness of entity-centric approaches with bigger scope (i.e., covering all four extraction classes in Table 1) towards improving knowledge density, and that even relatively straightforward techniques can go a long way in alleviating low knowledge density in existing state-ofthe-art KGs. ENTICE code is available at: https://github.com/malllabiisc/entity-centrickb-pop 2 Related Work Open Information Extraction (OIE) systems (Yates et al., 2007; Fader et al., 2011; Schmitz et al., 2012) aim at extracting textual triples of &apos;Measured with respect to the five categories experimented with in the paper. See Section 4 for details. the form noun phrase-predicate-noun phrase. While such systems aim for extraction coverage, and because they operate in an ontologyfree setting, they don’t directly address the problem of improving knowledge density in ontological KGs such as NELL. However, OIE extractions provide a suitable starting point which is exploited by ENTICE. (Gal´arraga et al., 2014) addresses the problem of normalizing (or canonicalizing) OIE extractio</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1535–1545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Gal´arraga</author>
<author>Geremy Heitz</author>
<author>Kevin Murphy</author>
<author>Fabian M Suchanek</author>
</authors>
<title>Canonicalizing open knowledge bases.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management,</booktitle>
<pages>1679--1688</pages>
<publisher>ACM.</publisher>
<marker>Gal´arraga, Heitz, Murphy, Suchanek, 2014</marker>
<rawString>Luis Gal´arraga, Geremy Heitz, Kevin Murphy, and Fabian M Suchanek. 2014. Canonicalizing open knowledge bases. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 1679–1688. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
<author>Partha Pratim Talukdar</author>
<author>Bryan Kisiel</author>
<author>Tom Mitchell</author>
</authors>
<title>Improving learning and inference in a large knowledge-base using latent syntactic cues.</title>
<date>2013</date>
<contexts>
<context position="3675" citStr="Gardner et al., 2013" startWordPosition="584" endWordPosition="587">best-effort extraction paradigm has proved to be inadequate to address the low knowledge density issue mentioned above. We refer to such paradigm as best-effort since its attention is divided equally among all possible entities. Recently, a few entity-centric methods have 530 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 530–535, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Dataflow and architecture and of ENTICE. See Section 3 for details. been proposed to increase knowledge density in KGs (Gardner et al., 2013; Gardner et al., 2014). In contrast to the best-effort approaches mentioned above, these entity-centric approaches aim at increasing knowledge density for a given entity. A new fact involving the given entity can belong to one of the four types shown in Table 1. Unfortunately, these densifying techniques only aim at identifying instances of known relations among entities already present in the KG, i.e., they fall in the KR-KE type of Table 1. In this paper we propose ENTIty Centric Expansion (ENTICE), an entity-centric knowledge densifying framework which, given an entity, is capable of extra</context>
<context position="5768" citStr="Gardner et al., 2013" startWordPosition="916" endWordPosition="919"> phrase-predicate-noun phrase. While such systems aim for extraction coverage, and because they operate in an ontologyfree setting, they don’t directly address the problem of improving knowledge density in ontological KGs such as NELL. However, OIE extractions provide a suitable starting point which is exploited by ENTICE. (Gal´arraga et al., 2014) addresses the problem of normalizing (or canonicalizing) OIE extractions which can be considered as one of the components of ENTICE (see Section 3.3). As previously mentioned, recent proposals for improving density of KGs such as those reported in (Gardner et al., 2013; Gardner et al., 2014) focus on extracting facts of one of the four extraction classes mentioned in Table 1, viz., KR-KE. The KBP challenge (Surdeanu, 2013) also focuses on extracting facts while keeping the relation set fixed, i.e., it addresses the KR-KE and KR-NE extraction classes. A method to improve knowledge density in KGs by using search engine query logs and a question answering system is presented in (West et al., 2014). The proprietary nature of datasets and tools used in this approach limits its applicability in our setting. ENTICE aims to improve knowledge density by extracting f</context>
</contexts>
<marker>Gardner, Talukdar, Kisiel, Mitchell, 2013</marker>
<rawString>Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom Mitchell. 2013. Improving learning and inference in a large knowledge-base using latent syntactic cues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
<author>Partha Pratim Talukdar</author>
<author>Jayant Krishnamurthy</author>
<author>Tom Mitchell</author>
</authors>
<title>Incorporating vector space similarity in random walk inference over knowledge bases.</title>
<date>2014</date>
<contexts>
<context position="3698" citStr="Gardner et al., 2014" startWordPosition="588" endWordPosition="591"> paradigm has proved to be inadequate to address the low knowledge density issue mentioned above. We refer to such paradigm as best-effort since its attention is divided equally among all possible entities. Recently, a few entity-centric methods have 530 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 530–535, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Figure 1: Dataflow and architecture and of ENTICE. See Section 3 for details. been proposed to increase knowledge density in KGs (Gardner et al., 2013; Gardner et al., 2014). In contrast to the best-effort approaches mentioned above, these entity-centric approaches aim at increasing knowledge density for a given entity. A new fact involving the given entity can belong to one of the four types shown in Table 1. Unfortunately, these densifying techniques only aim at identifying instances of known relations among entities already present in the KG, i.e., they fall in the KR-KE type of Table 1. In this paper we propose ENTIty Centric Expansion (ENTICE), an entity-centric knowledge densifying framework which, given an entity, is capable of extracting facts belonging t</context>
<context position="5791" citStr="Gardner et al., 2014" startWordPosition="920" endWordPosition="923"> phrase. While such systems aim for extraction coverage, and because they operate in an ontologyfree setting, they don’t directly address the problem of improving knowledge density in ontological KGs such as NELL. However, OIE extractions provide a suitable starting point which is exploited by ENTICE. (Gal´arraga et al., 2014) addresses the problem of normalizing (or canonicalizing) OIE extractions which can be considered as one of the components of ENTICE (see Section 3.3). As previously mentioned, recent proposals for improving density of KGs such as those reported in (Gardner et al., 2013; Gardner et al., 2014) focus on extracting facts of one of the four extraction classes mentioned in Table 1, viz., KR-KE. The KBP challenge (Surdeanu, 2013) also focuses on extracting facts while keeping the relation set fixed, i.e., it addresses the KR-KE and KR-NE extraction classes. A method to improve knowledge density in KGs by using search engine query logs and a question answering system is presented in (West et al., 2014). The proprietary nature of datasets and tools used in this approach limits its applicability in our setting. ENTICE aims to improve knowledge density by extracting facts from all four extr</context>
</contexts>
<marker>Gardner, Talukdar, Krishnamurthy, Mitchell, 2014</marker>
<rawString>Matt Gardner, Partha Pratim Talukdar, Jayant Krishnamurthy, and Tom Mitchell. 2014. Incorporating vector space similarity in random walk inference over knowledge bases.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The stanford corenlp natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</booktitle>
<contexts>
<context position="7831" citStr="Manning et al., 2014" startWordPosition="1260" endWordPosition="1263">ase of ambiguous entities, a few keywords are also added to the query. For the experiments in this paper, the category is used as the keyword. For example, for the entity Albert Einstein from the scientist category, the query will be ”Albert Einstein scientist”. Top 20 documents returned by the search engine are downloaded and processed further. Text is extracted from the raw downloaded documents using regex patters, HTML tag matching, and by using the Boilerpipe tool2. 3.2 Triple Extraction Text of each document obtained in the previous step is processed through the Stanford CoreNLP toolkit (Manning et al., 2014) for tokenization, coreference resolution, and dependency parsing. Tokenized and coreferenceresolved sentences are then passes through OpenIEv4 system 3 to extract (noun phrase, predicate, noun phrase) triples. Multiple and overlapping triples from the sentence was permitted. Length filter is applied on the noun phrase and the predicate of the triple extracted. This eliminates triples whose predicate is more than 6 tokens and noun phrase more than 7 tokens. 3.3 Noun and Relation Phrase Normalization Noun phrases (NPs) and relation phrases obtained from the previous step are normalized (or cano</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems,</title>
<date>2013</date>
<pages>3111--3119</pages>
<contexts>
<context position="9521" citStr="Mikolov et al., 2013" startWordPosition="1520" endWordPosition="1523"> graph, and hence is considered linked. To cluster noun phrases, we first construct canopies corresponding to each word in the noun phrase. For example, for noun phrase Albert Einstein, we create two canopies, viz., a canopy for Albert and another canopy for Einstein, and add Albert Einstein to both canopies. Grouping of noun phrases inside the canopy is the next step of clustering phase. Noun phrase similarity is calculated based on similarity of words in the noun phrases. Word similarity is either direct string matching or Gensim similarity score4, which internally uses word2vec embeddings (Mikolov et al., 2013). After calculating pairwise similarity of noun phrases, hierarchical clustering is carried out to group noun phrases inside each canopy. A threshold score is used to stop hierarchical clustering. At the end of this process, we have canopies and groups of noun phrases inside them. A noun phrase can be in more than one canopy, hence those groups across canopies are merged if the similarity is greater than certain threshold. After this, each group will contain facts which have similar noun phrases and different (or same) relation phrase. Again the facts are clustered based on the similarity of t</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
<author>W Cohen</author>
<author>E Hruschka</author>
<author>P Talukdar</author>
<author>J Betteridge</author>
<author>A Carlson</author>
<author>B Dalvi</author>
<author>M Gardner</author>
<author>B Kisiel</author>
<author>J Krishnamurthy</author>
</authors>
<title>Neverending learning.</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="1458" citStr="Mitchell et al., 2015" startWordPosition="221" endWordPosition="224">such KGs, in this paper we argue in favor of ENTIty Centric Expansion (ENTICE), an entity-centric KG population framework, to alleviate the low knowledge density problem in existing KGs. By using ENTICE, we are able to increase NELL’s knowledge density by a factor of 7.7 at 75.5% accuracy. Additionally, we are also able to extend the ontology discovering new relations and entities. 1 Introduction Over the last few years, automatic construction of knowledge graphs (KGs) from webscale text data has received considerable attention, resulting in the construction of several large KGs such as NELL (Mitchell et al., 2015), Google’s Knowledge Vault (Dong et al., 2014). These KGs consist of millions of entities and facts involving them. While measuring size of the KGs in terms of number of entities and facts is helpful, they don’t readily capture the volume of knowledge needed in Known Target New Target Entity Entity Known KR-KE KR-NE Relation New NR-KE NR-NE Relation Table 1: Any new fact involving a source entity from a Knowledge Graph (i.e., facts of the form entity1-relation-entity2 where entity1 is already in the KG) can be classified into one of the four extraction classes shown above. Most KG population t</context>
</contexts>
<marker>Mitchell, Cohen, Hruschka, Talukdar, Betteridge, Carlson, Dalvi, Gardner, Kisiel, Krishnamurthy, 2015</marker>
<rawString>T Mitchell, W Cohen, E Hruschka, P Talukdar, J Betteridge, A Carlson, B Dalvi, M Gardner, B Kisiel, J Krishnamurthy, et al. 2015. Neverending learning. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schmitz</author>
<author>Robert Bart</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>523--534</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4992" citStr="Schmitz et al., 2012" startWordPosition="791" endWordPosition="794">crease NELL’s knowledge density by a factor of 7.71, while achieving 75.4% accuracy. Our goal here is to draw attention to the effectiveness of entity-centric approaches with bigger scope (i.e., covering all four extraction classes in Table 1) towards improving knowledge density, and that even relatively straightforward techniques can go a long way in alleviating low knowledge density in existing state-ofthe-art KGs. ENTICE code is available at: https://github.com/malllabiisc/entity-centrickb-pop 2 Related Work Open Information Extraction (OIE) systems (Yates et al., 2007; Fader et al., 2011; Schmitz et al., 2012) aim at extracting textual triples of &apos;Measured with respect to the five categories experimented with in the paper. See Section 4 for details. the form noun phrase-predicate-noun phrase. While such systems aim for extraction coverage, and because they operate in an ontologyfree setting, they don’t directly address the problem of improving knowledge density in ontological KGs such as NELL. However, OIE extractions provide a suitable starting point which is exploited by ENTICE. (Gal´arraga et al., 2014) addresses the problem of normalizing (or canonicalizing) OIE extractions which can be conside</context>
</contexts>
<marker>Schmitz, Bart, Soderland, Etzioni, 2012</marker>
<rawString>Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, et al. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 523– 534. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
</authors>
<title>Overview of the tac2013 knowledge base population evaluation: English slot filling and temporal slot filling.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="5925" citStr="Surdeanu, 2013" startWordPosition="944" endWordPosition="945">e problem of improving knowledge density in ontological KGs such as NELL. However, OIE extractions provide a suitable starting point which is exploited by ENTICE. (Gal´arraga et al., 2014) addresses the problem of normalizing (or canonicalizing) OIE extractions which can be considered as one of the components of ENTICE (see Section 3.3). As previously mentioned, recent proposals for improving density of KGs such as those reported in (Gardner et al., 2013; Gardner et al., 2014) focus on extracting facts of one of the four extraction classes mentioned in Table 1, viz., KR-KE. The KBP challenge (Surdeanu, 2013) also focuses on extracting facts while keeping the relation set fixed, i.e., it addresses the KR-KE and KR-NE extraction classes. A method to improve knowledge density in KGs by using search engine query logs and a question answering system is presented in (West et al., 2014). The proprietary nature of datasets and tools used in this approach limits its applicability in our setting. ENTICE aims to improve knowledge density by extracting facts from all four extraction classes, i.e., for a given entity, it extracts facts involving known relations, identifies potentially new relations that might</context>
</contexts>
<marker>Surdeanu, 2013</marker>
<rawString>Mihai Surdeanu. 2013. Overview of the tac2013 knowledge base population evaluation: English slot filling and temporal slot filling. In Proceedings of the Sixth Text Analysis Conference (TAC 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert West</author>
<author>Evgeniy Gabrilovich</author>
<author>Kevin Murphy</author>
<author>Shaohua Sun</author>
<author>Rahul Gupta</author>
<author>Dekang Lin</author>
</authors>
<title>Knowledge base completion via searchbased question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of the 23rd international conference on World wide web.</booktitle>
<contexts>
<context position="2677" citStr="West et al., 2014" startWordPosition="432" endWordPosition="435">chniques tend to focus on extracting facts of the KR-KE class. ENTICE, the entity-centric approach proposed in this paper, is able to extract facts of all four classes. real-world applications. When such a KG is used in an application, one is often interested in known facts for a given entity, and not necessarily the overall size of the KG. In particular, knowing the average number of facts per entity is quite informative. We shall refer to this as the knowledge density of the KG. Low knowledge density (or high sparsity) in automatically constructed KGs has been recognized in recent research (West et al., 2014). For example, NELL KG has a knowledge density of 1.34. Such low knowledge density puts significant limitations on the utility of these KGs. Construction of such KGs tend to follow a batch paradigm: the knowledge extraction system makes a full pass over the text corpus extracting whatever knowledge it finds, and finally aggregating all extractions into a graph. Clearly, such best-effort extraction paradigm has proved to be inadequate to address the low knowledge density issue mentioned above. We refer to such paradigm as best-effort since its attention is divided equally among all possible ent</context>
<context position="6202" citStr="West et al., 2014" startWordPosition="988" endWordPosition="991"> considered as one of the components of ENTICE (see Section 3.3). As previously mentioned, recent proposals for improving density of KGs such as those reported in (Gardner et al., 2013; Gardner et al., 2014) focus on extracting facts of one of the four extraction classes mentioned in Table 1, viz., KR-KE. The KBP challenge (Surdeanu, 2013) also focuses on extracting facts while keeping the relation set fixed, i.e., it addresses the KR-KE and KR-NE extraction classes. A method to improve knowledge density in KGs by using search engine query logs and a question answering system is presented in (West et al., 2014). The proprietary nature of datasets and tools used in this approach limits its applicability in our setting. ENTICE aims to improve knowledge density by extracting facts from all four extraction classes, i.e., for a given entity, it extracts facts involving known relations, identifies potentially new relations that might be relevant for this entity, establishes such relations between the given entity and other known as well as new entities – all in a single system. While various parts of this problem have been studied in isolation in the past, ENTICE is 531 the first system to the best of our</context>
</contexts>
<marker>West, Gabrilovich, Murphy, Sun, Gupta, Lin, 2014</marker>
<rawString>Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014. Knowledge base completion via searchbased question answering. In Proceedings of the 23rd international conference on World wide web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Michael Cafarella</author>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
<author>Matthew Broadhead</author>
<author>Stephen Soderland</author>
</authors>
<title>Textrunner: open information extraction on the web.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations,</booktitle>
<pages>25--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4949" citStr="Yates et al., 2007" startWordPosition="783" endWordPosition="786">le 1. By using ENTICE, we are able to increase NELL’s knowledge density by a factor of 7.71, while achieving 75.4% accuracy. Our goal here is to draw attention to the effectiveness of entity-centric approaches with bigger scope (i.e., covering all four extraction classes in Table 1) towards improving knowledge density, and that even relatively straightforward techniques can go a long way in alleviating low knowledge density in existing state-ofthe-art KGs. ENTICE code is available at: https://github.com/malllabiisc/entity-centrickb-pop 2 Related Work Open Information Extraction (OIE) systems (Yates et al., 2007; Fader et al., 2011; Schmitz et al., 2012) aim at extracting textual triples of &apos;Measured with respect to the five categories experimented with in the paper. See Section 4 for details. the form noun phrase-predicate-noun phrase. While such systems aim for extraction coverage, and because they operate in an ontologyfree setting, they don’t directly address the problem of improving knowledge density in ontological KGs such as NELL. However, OIE extractions provide a suitable starting point which is exploited by ENTICE. (Gal´arraga et al., 2014) addresses the problem of normalizing (or canonical</context>
</contexts>
<marker>Yates, Cafarella, Banko, Etzioni, Broadhead, Soderland, 2007</marker>
<rawString>Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. Textrunner: open information extraction on the web. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, pages 25–26. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.971368">
Modeling Relation Paths for Representation Learning of Knowledge Bases
</title>
<author confidence="0.998427">
Yankai Lin1, Zhiyuan Liu1 ∗, Huanbo Luan1, Maosong Sun1, Siwei Rao2, Song Liu2
</author>
<affiliation confidence="0.895064666666667">
1 Department of Computer Science and Technology, State Key Lab on Intelligent Technology and Systems,
National Lab for Information Science and Technology, Tsinghua University, Beijing, China
2 Samsung R&amp;D Institute of China, Beijing, China
</affiliation>
<sectionHeader confidence="0.972369" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999511269230769">
Representation learning of knowledge
bases aims to embed both entities and
relations into a low-dimensional space.
Most existing methods only consider
direct relations in representation learning.
We argue that multiple-step relation paths
also contain rich inference patterns be-
tween entities, and propose a path-based
representation learning model. This model
considers relation paths as translations
between entities for representation learn-
ing, and addresses two key challenges: (1)
Since not all relation paths are reliable,
we design a path-constraint resource allo-
cation algorithm to measure the reliability
of relation paths. (2) We represent relation
paths via semantic composition of relation
embeddings. Experimental results on
real-world datasets show that, as com-
pared with baselines, our model achieves
significant and consistent improvements
on knowledge base completion and re-
lation extraction from text. The source
code of this paper can be obtained from
https://github.com/mrlyk423/
relation_extraction.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998077">
People have recently built many large-scale
knowledge bases (KBs) such as Freebase, DBpe-
dia and YAGO. These KBs consist of facts about
the real world, mostly in the form of triples, e.g.,
(Steve Jobs, FounderOf, Apple Inc.). KBs are
important resources for many applications such as
question answering and Web search. Although
typical KBs are large in size, usually containing
thousands of relation types, millions of entities
and billions of facts (triples), they are far from
</bodyText>
<note confidence="0.523717">
∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn)
</note>
<bodyText confidence="0.999132357142857">
complete. Hence, many efforts have been invested
in relation extraction to enrich KBs.
Recent studies reveal that, neural-based repre-
sentation learning methods are scalable and ef-
fective to encode relational knowledge with low-
dimensional representations of both entities and
relations, which can be further used to extract
unknown relational facts. TransE (Bordes et al.,
2013) is a typical method in the neural-based ap-
proach, which learns vectors (i.e., embeddings) for
both entities and relations. The basic idea behind
TransE is that, the relationship between two enti-
ties corresponds to a translation between the em-
beddings of the entities, that is, h + r ≈ t when
the triple (h, r, t) holds. Since TransE has issues
when modeling 1-to-N, N-to-1 and N-to-N rela-
tions, various methods such as TransH (Wang et
al., 2014) and TransR (Lin et al., 2015) are pro-
posed to assign an entity with different represen-
tations when involved in various relations.
Despite their success in modeling relational
facts, TransE and its extensions only take di-
rect relations between entities into considera-
tion. It is known that there are also substan-
tial multiple-step relation paths between entities
indicating their semantic relationships. The re-
lation paths reflect complicated inference pat-
terns among relations in KBs. For example, the
</bodyText>
<subsectionHeader confidence="0.90305">
BornInCity CityInState
</subsectionHeader>
<bodyText confidence="0.999417">
relation path h−−−−−−−−→ e1 −−−−−−−−→
e2 StateInCountry −−−−−−−−−−−→t indicates the relation
Nationality between h and t, i.e., (h,
Nationality, t).
In this paper, we aim at extending TransE to
model relation paths for representation learning of
KBs, and propose path-based TransE (PTransE).
In PTransE, in addition to direct connected rela-
tional facts, we also build triples from KBs us-
ing entity pairs connected with relation paths.
As shown in Figure 1, TransE only considers
direct relations between entities, e.g., h →−r t,
builds a triple (h, r, t), and optimizes the objec-
</bodyText>
<page confidence="0.976809">
705
</page>
<note confidence="0.9933675">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 705–714,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<page confidence="0.523933">
e1
</page>
<bodyText confidence="0.991445">
tive h + r = t. PTransE generalizes TransE
by regarding multiple-step relation paths as con-
nections between entities. Take the 2-step path
h r1 −→ e1 −→ t for example as shown in Figure 1.
</bodyText>
<note confidence="0.378253">
r2
</note>
<bodyText confidence="0.981339833333334">
Besides building triples (h, r1, e1) and (e1, r2, t)
for learning as in TransE, PTransE also builds a
triple (h, r1 ◦ r2, t), and optimizes the objective
h + (r1 ◦ r2) = t, where ◦ is an operation to join
the relations r1 and r2 together into a unified rela-
tion path representation.
</bodyText>
<figure confidence="0.96320825">
TransE PTransE
KB
Triples
Objectives
</figure>
<figureCaption confidence="0.999984">
Figure 1: TransE and PTransE.
</figureCaption>
<bodyText confidence="0.984128428571429">
As compared with TransE, PTransE takes rich
relation paths in KBs for learning. There are two
critical challenges that make PTransE nontrivial to
learn from relation paths:
Relation Path Reliability. Not all relation
paths are meaningful and reliable for learning. For
example, there is often a relation path h Friend
</bodyText>
<equation confidence="0.2561695">
−−−−−→
Profession
</equation>
<bodyText confidence="0.999482266666667">
→ t, but actually it does not indicate
any semantic relationship between h and t. Hence,
it is inappropriate to consider all relation paths in
our model. In experiments, we find that those re-
lation paths that lead to lots of possible tail enti-
ties are mostly unreliable for the entity pair. In
this paper, we propose a path-constraint resource
allocation algorithm to measure the reliability of
relation paths. Afterwards, we select the reliable
relation paths for representation learning.
Relation Path Representation. In order to take
relation paths into consideration, relation paths
should also be represented in a low-dimensional
space. It is straightforward that the semantic
meaning of a relation path depends on all relations
in this path. Given a relation path p = (r1, ... , rl)
, we will define and learn a binary operation func-
tion (◦) to obtain the path embedding p by re-
cursively composing multiple relations, i.e., p =
r1 ◦ ... ◦ rl.
With relation path selection and representation,
PTransE learns entity and relation embeddings by
regarding relation paths as translations between
the corresponding entities. In experiments, we
select a typical KB, Freebase, to build datasets
and carry out evaluation on three tasks, including
entity prediction, relation prediction and relation
extraction from text. Experimental results show
that, PTransE significantly outperforms TransE
and other baseline methods on all three tasks.
</bodyText>
<sectionHeader confidence="0.996207" genericHeader="introduction">
2 Our Model
</sectionHeader>
<bodyText confidence="0.9998993">
In this section, we introduce path-based TransE
(PTransE) that learns representations of entities
and relations considering relation paths. In TransE
and PTransE, we have entity set E and relation
set R, and learn to encode both entities and re-
lations in Rk. Given a KB represented by a set of
triples S = {(h, r, t)} with each triple composed
of two entities h, t ∈ E and their relation r ∈ R.
Our model is expected to return a low energy score
when the relation holds, and a high one otherwise.
</bodyText>
<subsectionHeader confidence="0.96717">
2.1 TransE and PTransE
</subsectionHeader>
<bodyText confidence="0.998194666666667">
For each triple (h, r, t), TransE regards the relation
as a translation vector r between two entity vectors
h and t. The energy function is defined as
</bodyText>
<equation confidence="0.999751">
E(h,r,t) = ||h + r − t||, (1)
</equation>
<bodyText confidence="0.997963916666667">
which is expected to get a low score when (h, r, t)
holds, and high otherwise.
TransE only learns from direct relations be-
tween entities but ignores multiple-step relation
paths, which also contain rich inference patterns
between entities. PTransE take relation paths into
consideration for representation learning.
Suppose there are multiple relation paths
P(h, t) = {p1, ... , pnr} connecting two entities
h and t, where relation path p = (r1, ... , rl) indi-
cates h r1−→ ... rl−→ t. For each triple (h, r, t), the
energy function is defined as
</bodyText>
<equation confidence="0.985031">
G(h, r, t) = E(h, r, t) + E(h, P, t), (2)
</equation>
<bodyText confidence="0.9999796">
where E(h, r, t) models correlations between rela-
tions and entities with direct relation triples, as de-
fined in Equation (1). E(h, P, t) models the infer-
ence correlations between relations with multiple-
step relation path triples, which is defined as
</bodyText>
<equation confidence="0.999438">
1 �
E(h, P, t) =
Z p∈P(h,t)
</equation>
<bodyText confidence="0.9569015">
where R(p|h, t) indicates the reliability of the re-
lation path p given the entity pair (h, t), Z =
</bodyText>
<equation confidence="0.9390675">
R(p|h, t)E(h, p, t), (3)
706
E
pEP(h,t) R(p|h, t) is a normalization factor, and
E(h, p, t) is the energy function of the triple
(h, p, t).
</equation>
<bodyText confidence="0.9999244">
For the energy of each triple (h, p, t), the com-
ponent R(p|h, t) concerns about relation path reli-
ability, and E(h, p, t) concerns about relation path
representation. We introduce the two components
in detail as follows.
</bodyText>
<subsectionHeader confidence="0.997793">
2.2 Relation Path Reliability
</subsectionHeader>
<bodyText confidence="0.999492">
We propose a path-constraint resource allocation
(PCRA) algorithm to measure the reliability of a
relation path. Resource allocation over networks
was originally proposed for personalized recom-
mendation (Zhou et al., 2007), and has been suc-
cessfully used in information retrieval for measur-
ing relatedness between two objects (L¨u and Zhou,
2011). Here we extend it to PCRA to measure the
reliability of relation paths. The basic idea is, we
assume that a certain amount of resource is associ-
ated with the head entity h, and will flow following
the given path p. We use the resource amount that
eventually flows to the tail entity t to measure the
reliability of the path p as a meaningful connection
between h and t.
Formally, for a path triple (h, p, t), we compute
the resource amount flowing from h to t given the
path p = (r1, ... , rl) as follows. Starting from h
and following the relation path p, we can write the
flowing path as S0 −→ S1 r1−→ ... rl
</bodyText>
<equation confidence="0.995863">
r2 −→ Sl, where
S0 = hand t ∈ Sl.
</equation>
<bodyText confidence="0.999739285714286">
For any entity m ∈ Si, we denote its direct pre-
decessors along relation ri in Si−1 as Si−1(·, m).
The resource flowing to m is defined as
where Si(n, ·) is the direct successors of n ∈ Si−1
following the relation ri, and Rp(n) is the resource
obtained from the entity n.
For each relation path p, we set the initial re-
source in h as Rp(h) = 1. By performing re-
source allocation recursively from h through the
path p, the tail entity t eventually obtains the re-
source Rp(t) which indicates how much informa-
tion of the head entity h can be well translated. We
use Rp(t) to measure the reliability of the path p
given (h, t), i.e., R(p|h, t) = Rp(t).
</bodyText>
<subsectionHeader confidence="0.99605">
2.3 Relation Path Representation
</subsectionHeader>
<bodyText confidence="0.976196045454545">
Besides relation path reliability, we also need to
define energy function E(h, p, t) for the path triple
(h, p, t) in Equation (2). Similar with the en-
ergy function of TransE in Equation (1), we will
also represent the relation path p in the embedding
space.
Figure 2: Path representations are computed by se-
mantic composition of relation embeddings.
The semantic meaning of a relation path con-
siderably relies on its involved relations. It is thus
reasonable for us to build path embeddings via se-
mantic composition of relation embeddings. As
illustrated in Figure 2, the path embedding p is
composed by the embeddings of BorninCity,
CityTnState and StateTnCountry.
Formally, for a path p = (r1, ... , rl), we define
a composition operation ◦ and obtain path embed-
ding as p = r1 ◦...◦rl. In this paper, we consider
three types of composition operation:
Addition (ADD). The addition operation ob-
tains the vector of a path by summing up the vec-
tors of all relations, which is formalized as
</bodyText>
<equation confidence="0.996351">
p = r1 + ... + rl. (5)
</equation>
<bodyText confidence="0.9995">
Multiplication (MUL). The multiplication op-
eration obtains the vector of a path as the cumula-
tive product of the vectors of all relations, which
is formalized as
</bodyText>
<equation confidence="0.969891">
p = r1 ·... · rl. (6)
</equation>
<bodyText confidence="0.99977575">
Both addition and multiplication operations are
simple and have been extensively investigated in
semantic composition of phrases and sentences
(Mitchell and Lapata, 2008).
Recurrent Neural Network (RNN). RNN is a
recent neural-based model for semantic composi-
tion (Mikolov et al., 2010). The composition op-
eration is realized using a matrix W:
</bodyText>
<equation confidence="0.966244">
ci = f(W [ci−1; ri]), (7)
</equation>
<bodyText confidence="0.9991035">
where f is a non-linearity or identical function,
and [a; b] represents the concatenation of two vec-
</bodyText>
<equation confidence="0.876227666666667">
�Rp(m) =
nESi−1(·,m)
|Si(n, ·)|Rp(n), (4)
</equation>
<figure confidence="0.968685">
1
Composition
Steve
Jobs
BornInCity San CityInState Californi StateInCountry
Francisco a
United
State
</figure>
<page confidence="0.975454">
707
</page>
<bodyText confidence="0.9990335">
tors. By setting c1 = r1 and recursively perform-
ing RNN following the relation path, we will fi-
nally obtain p = cn. RNN has also been used
for representation learning of relation paths in KBs
(Neelakantan et al., 2015).
For a multiple-step relation path triple (h, p, t),
we could have followed TransE and define the
energy function as E(h, p, t) = ||h + p − t||.
However, since we have minimized ||h + r − t||
with the direct relation triple (h, r, t) to make sure
r Pz� t−h, we may directly define the energy func-
tion of (h, p, t) as
</bodyText>
<equation confidence="0.99831">
E(h,p,t) = ||p−(t−h) ||= ||p−r ||= E(p,r),
</equation>
<bodyText confidence="0.9117284">
(8)
which is expected to be a low score when the
multiple-relation path p is consistent with the di-
rect relation r, and high otherwise, without using
entity embeddings.
</bodyText>
<subsectionHeader confidence="0.982976">
2.4 Objective Formalization
</subsectionHeader>
<bodyText confidence="0.989623">
We formalize the optimization objective of
PTransE as
</bodyText>
<equation confidence="0.999248666666667">
E [L(h, r, t)+ 1 R(p|h, t)L(p, r)].
L(S) = Z E
(h,r,t)ES pEP(h,t)
</equation>
<bodyText confidence="0.692315666666667">
Following TransE, L(h, r, t) and L(p, r) are
margin-based loss functions with respect to the
triple (h, r, t) and the pair (p, r):
</bodyText>
<equation confidence="0.913068">
L(h, r, t) = E [y + E(h, r, t) − E(h&apos;, r&apos;, t&apos;)]+,
(h0,r0,t0)ES−
and
L(p, r) = E [y + E(p, r) − E(p, r&apos;)]+, (11)
(h,r0,t)ES−
</equation>
<bodyText confidence="0.999940142857143">
where [x]+ = max(0, x) returns the maximum be-
tween 0 and x, γ is the margin, S is the set of valid
triples existing in a KB and S− is the set of invalid
triples. The objective will favor lower scores for
valid triples as compared with invalid triples.
The invalid triple set with respect to (h, r, t) is
defined as
</bodyText>
<equation confidence="0.9956">
S− = {(h0,r,t)}U{(h,r0, t)}U{(h,r, t0)}. (12)
</equation>
<bodyText confidence="0.999718666666667">
That is, the set of invalid triples is composed of
the original valid triple (h, r, t) with one of three
components replaced.
</bodyText>
<subsectionHeader confidence="0.993719">
2.5 Optimization and Implementation Details
</subsectionHeader>
<bodyText confidence="0.999272">
For optimization, we employ stochastic gradient
descent (SGD) to minimize the loss function. We
randomly select a valid triple from the training set
iteratively for learning. In the implementation, we
also enforce constraints on the norms of the em-
beddings h, r, t. That is, we set
</bodyText>
<equation confidence="0.6632965">
IIhII2c1, IIrII2c1, IItII2c1. bh,r,t.
(13)
</equation>
<bodyText confidence="0.970345125">
There are also some implementation details that
will significantly influence the performance of
representation learning, which are introduced as
follows.
Reverse Relation Addition. In some cases, we
are interested in the reverse version of a relation,
which may not be presented in KBs. For exam-
ple, according to the relation path e1 Borns
CityOfCountry
e2 e3 we expect to infer the fact
that (e1, Nationality, e3). In this paper, how-
ever, we only consider the relation paths follow-
ing one direction. Hence, we add reverse relations
for each relation in KBs. That is, for each triple
(h, r, t) we build another (t, r−1, h). In this way,
our method can consider the above-mentioned
</bodyText>
<subsectionHeader confidence="0.735882">
BornInCity CityOfCountry−1
</subsectionHeader>
<bodyText confidence="0.999282533333334">
path as e1 −−−−−−−−, e2 −−−−−−−−−−−−, e3
for learning.
Path Selection Limitation. There are usually
large amount of relations and facts about each en-
tity pair. It will be impractical to enumerate all
possible relation paths between head and tail en-
tities. For example, if each entity refers to more
than 100 relations on average, which is common
in Freebase, there will be billions of 4-step paths.
Even for 2-step or 3-step paths, it will be time-
consuming to consider all of them without limita-
tion. For computational efficiency, in this paper
we restrict the length of paths to at most 3-steps
and consider those relation paths with the reliabil-
ity score larger than 0.01.
</bodyText>
<subsectionHeader confidence="0.993174">
2.6 Complexity Analysis
</subsectionHeader>
<bodyText confidence="0.999953555555556">
We denote Ne as the number of entities, Nr as
the number of relations and K as the vector di-
mension. The model parameter size of PTransE
is (NeK + NrK), which is the same as TransE.
PTransE follows the optimization procedure in-
troduced by (Bordes et al., 2013) to solve Equa-
tion (9). We denote S as the number of triples
for learning, P as the expected number of relation
paths between two entities, and L as the expected
</bodyText>
<page confidence="0.990348">
708
</page>
<bodyText confidence="0.99989">
length of relation paths. For each iteration in opti-
mization, the complexity of TransE is O(SK) and
the complexity of PTransE is O(SKPL) for ADD
and MUL, and O(SK2PL) for RNN.
</bodyText>
<sectionHeader confidence="0.993587" genericHeader="background">
3 Experiments and Analysis
</sectionHeader>
<subsectionHeader confidence="0.999949">
3.1 Data Sets and Experimental Setting
</subsectionHeader>
<bodyText confidence="0.9997472">
We evaluate our method on a typical large-scale
KB Freebase (Bollacker et al., 2008). In this pa-
per, we adopt two datasets extracted from Free-
base, i.e., FB15K and FB40K. The statistics of the
datasets are listed in Table 1.
</bodyText>
<tableCaption confidence="0.999364">
Table 1: Statistics of data sets.
</tableCaption>
<table confidence="0.993950333333333">
Dataset #Rel #Ent #Train #Valid # Test
FB15K 1,345 14,951 483,142 50,000 59,071
FB40K 1,336 39,528 370,648 67,946 96,678
</table>
<bodyText confidence="0.999945111111111">
We evaluate the performance of PTransE and
other baselines by predicting whether testing
triples hold. We consider two scenarios: (1)
Knowledge base completion, aiming to predict the
missing entities or relations in given triples only
based on existing KBs. (2) Relation extraction
from texts, aiming to extract relations between en-
tities based on information from both plain texts
and KBs.
</bodyText>
<subsectionHeader confidence="0.996979">
3.2 Knowledge Base Completion
</subsectionHeader>
<bodyText confidence="0.999416909090909">
The task of knowledge base completion is to com-
plete the triple (h, r, t) when one of h, t, r is miss-
ing. The task has been used for evaluation in (Bor-
des et al., 2011; Bordes et al., 2012; Bordes et
al., 2013). We conduct the evaluation on FB15K,
which has 483,142 relational triples and 1, 345 re-
lation types, among which there are rich inference
and reasoning patterns.
In the testing phase, for each testing triple
(h, r, t), we define the following score function for
prediction
</bodyText>
<equation confidence="0.953297">
S(h, r, t) = G(h, r, t) + G(t, r−1, h), (14)
</equation>
<bodyText confidence="0.998791">
and the score function G(h, r, t) is further defined
as
</bodyText>
<equation confidence="0.9917885">
G(h,r,t) =||h + r − t||+
1 Z Pr(r|p)R(p|h, t)||p − r||.
p∈P(h,t)
(15)
</equation>
<bodyText confidence="0.999839125">
The score function is similar to the energy func-
tion defined in Section 2.1. The difference is that,
here we consider the reliability of a path p is also
related to the inference strength given r, which is
quantified as Pr(r|p) = Pr(r, p)/ Pr(p) obtained
from the training data.
We divide the stage into two sub-tasks, i.e., en-
tity prediction and relation prediction.
</bodyText>
<subsubsectionHeader confidence="0.805979">
3.2.1 Entity Prediction
</subsubsectionHeader>
<bodyText confidence="0.999834431818182">
In the sub-task of entity prediction, we follow the
setting in (Bordes et al., 2013). For each test-
ing triple with missing head or tail entity, vari-
ous methods are asked to compute the scores of
G(h, r, t) for all candidate entities and rank them
in descending order.
We use two measures as our evaluation metrics:
the mean of correct entity ranks and the proportion
of valid entities ranked in top-10 (Hits@10). As
mentioned in (Bordes et al., 2013), the measures
are desirable but flawed when an invalid triple
ends up being valid in KBs. For example, when
the testing triple is (Obama, PresidentOf,
USA) with the head entity Obama is missing, the
head entity Lincoln may be regarded invalid for
prediction, but in fact it is valid in KBs. The eval-
uation metrics will under-estimate those methods
that rank these triples high. Hence, we can filter
out all these valid triples in KBs before ranking.
The first evaluation setting was named as “Raw”
and the second one as “Filter”.
For comparison, we select all methods in (Bor-
des et al., 2013; Wang et al., 2014) as our base-
lines and use their reported results directly since
the evaluation dataset is identical.
Ideally, PTransE has to find all possible relation
paths between the given entity and each candidate
entity. However, it is time consuming and imprac-
tical, because we need to iterate all candidate en-
tities in |E |for each testing triple. Here we adopt
a re-ranking method: we first rank all candidate
entities according to the scores from TransE, and
then re-rank top-500 candidates according to the
scores from PTransE.
For PTransE, we find the best hyperparameters
according to the mean rank in validation set. The
optimal configurations of PTransE we used are
A = 0.001, ry = 1, k = 100 and taking L1 as
dissimilarity. For training, we limit the number of
epochs over all the training triples to 500.
Evaluation results of entity prediction are
shown in Table 2. The baselines include RESCAL
(Nickel et al., 2011), SE (Bordes et al., 2011),
SME (linear) (Bordes et al., 2012), SME (bilinear)
</bodyText>
<page confidence="0.999369">
709
</page>
<tableCaption confidence="0.99854">
Table 2: Evaluation results on entity prediction.
</tableCaption>
<table confidence="0.9994358">
Metric Mean Rank Hits@10 (%)
Raw Filter Raw Filter
RESCAL 828 683 28.4 44.1
SE 273 162 28.8 39.8
SME (linear) 274 154 30.7 40.8
SME (bilinear) 284 158 31.3 41.3
LFM 283 164 26.0 33.1
TransE 243 125 34.9 47.1
TransH 212 87 45.7 64.4
TransR 198 77 48.2 68.7
TransE (Our) 205 63 47.9 70.2
PTransE (ADD, 2-step) 200 54 51.8 83.4
PTransE (MUL, 2-step) 216 67 47.4 77.7
PTransE (RNN, 2-step) 242 92 50.6 82.2
PTransE (ADD, 3-step) 207 58 51.4 84.6
</table>
<bodyText confidence="0.980867489795918">
(Bordes et al., 2012), LFM (Jenatton et al., 2012),
TransE (Bordes et al., 2013) (original version and
our implementation considering reverse relations),
TransH (Wang et al., 2014), and TransR (Lin et al.,
2015).
For PTransE, we consider three composition op-
erations for relation path representation: addition
(ADD), multiplication (MUL) and recurrent neu-
ral networks (RNN). We also consider relation
paths with at most 2-steps and 3-steps. With the
same configurations of PTransE, our TransE im-
plementation achieves much better performance
than that reported in (Bordes et al., 2013).
From Table 2 we observe that: (1) PTransE
significantly and consistently outperforms other
baselines including TransE. It indicates that rela-
tion paths provide a good supplement for repre-
sentation learning of KBs, which have been suc-
cessfully encoded by PTransE. For example, since
both George W. Bush and Abraham Lincoln were
presidents of the United States, they exhibit simi-
lar embeddings in TransE. This may lead to con-
fusion for TransE to predict the spouse of Laura
Bush. In contrast, since PTransE models rela-
tion paths, it can take advantage of the relation
paths between George W. Bush and Laura Bush,
and leads to more accurate prediction. (2) For
PTransE, the addition operation outperforms other
composition operations in both Mean Rank and
Hits@10. The reason is that, the addition opera-
tion is compatible with the learning objectives of
both TransE and PTransE. Take h r1 ) e1 �) t for
r2
example. The optimization objectives of two di-
rect relations h + r1 = e1 and e1 + r2 = t can
easily derive the path objective h + r1 + r2 = t.
(3) PTransE of considering relation paths with at
most 2-step and 3-step achieve comparable results.
This indicates that it may be unnecessary to con-
sider those relation paths that are too long.
As defined in (Bordes et al., 2013), relations in
KBs can be divided into various types according
to their mapping properties such as 1-to-1, 1-to-
N, N-to-1 and N-to-N. Here we demonstrate the
performance of PTransE and some baselines with
respect to different types of relations in Table 3.
It is observed that, on all mapping types of re-
lations, PTransE consistently achieves significant
improvement as compared with TransE.
</bodyText>
<subsubsectionHeader confidence="0.791857">
3.2.2 Relation Prediction
</subsubsectionHeader>
<bodyText confidence="0.999989051282051">
Relation prediction aims to predict relations given
two entities. We also use FB15K for evaluation.
In this sub-task, we can use the score function of
PTransE to rank candidate relations instead of re-
ranking like in entity prediction.
Since our implementation of TransE has
achieved the best performance among all base-
lines for entity prediction, here we only com-
pare PTransE with TransE due to limited space.
Evaluation results are shown in Table 4, where
we report Hits@1 instead of Hits@10 for com-
parison, because Hits@10 for both TransE and
PTransE exceeds 95%. In this table, we report
the performance of TransE without reverse rela-
tions (TransE), with reverse relations (+Rev) and
considering relation paths for testing like that in
PTransE (+Rev+Path). We report the performance
of PTransE with only considering relation paths (-
TransE), only considering the part in Equation (1)
(-Path) and considering both (PTransE).
The optimal configurations of PTransE for re-
lation prediction are identical to those for entity
prediction: A = 0.001, γ = 1, k = 100 and taking
L1 as dissimilarity.
From Table 4 we observe that: (1) PTransE out-
performs TransE+Rev+Path significantly for rela-
tion prediction by reducing 41.8% prediction er-
rors. (2) Even for TransE itself, considering re-
lation paths for testing can reduce 17.3% errors
as compared with TransE+Rev. It indicates that
encoding relation paths will benefit for predict-
ing relations. (3) PTransE with only considering
relation paths (PTransE-TransE) gets surprisingly
high mean rank. The reason is that, not all entity
pairs in testing triples have relation paths, which
will lead to random guess and the expectation of
rank of these entity pairs is |R|/2. Meanwhile,
Hits@1 of PTransE-TransE is relatively reason-
able, which indicates the worth of modeling rela-
</bodyText>
<page confidence="0.998723">
710
</page>
<tableCaption confidence="0.999693">
Table 3: Evaluation results on FB15K by mapping properties of relations. (%)
</tableCaption>
<table confidence="0.999767076923077">
Tasks Predicting Head Entities (Hits@10) Predicting Tail Entities (Hits@10)
Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N
SE 35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3
SME (linear) 35.1 53.7 19.0 40.3 32.7 14.9 61.6 43.3
SME (bilinear) 30.9 69.6 19.9 38.6 28.2 13.1 76.0 41.8
TransE 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0
TransH 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
TransR 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
TransE (Our) 74.6 86.6 43.7 70.6 71.5 49.0 85.0 72.9
PTransE (ADD, 2-step) 91.0 92.8 60.9 83.8 91.2 74.0 88.9 86.4
PTransE (MUL, 2-step) 89.0 86.8 57.6 79.8 87.8 71.4 72.2 80.4
PTransE (RNN, 2-step) 88.9 84.0 56.3 84.5 88.8 68.4 81.5 86.7
PTrasnE (ADD, 3-step) 90.1 92.0 58.7 86.1 90.7 70.7 87.5 88.7
</table>
<tableCaption confidence="0.7167868">
tion paths. As compared with TransE, the inferior
of PTransE-TransE also indicates that entity repre-
sentations are informative and crucial for relation
prediction.
Table 4: Evaluation results on relation prediction.
</tableCaption>
<table confidence="0.999856636363636">
Metric Mean Rank Hits@1 (%)
Raw Filter Raw Filter
TransE (Our) 2.8 2.5 65.1 84.3
+Rev 2.6 2.3 67.1 86.7
+Rev+Path 2.4 1.9 65.2 89.0
PTransE (ADD, 2-step) 1.7 1.2 69.5 93.6
-TransE 135.8 135.3 51.4 78.0
-Path 2.0 1.6 69.7 89.0
PTransE (MUL, 2-step) 2.5 2.0 66.3 89.0
PTransE (RNN, 2-step) 1.9 1.4 68.3 93.2
PTransE (ADD, 3-step) 1.8 1.4 68.5 94.0
</table>
<subsectionHeader confidence="0.999076">
3.3 Relation Extraction from Text
</subsectionHeader>
<bodyText confidence="0.999941432432432">
Relation extraction from text aims to extract re-
lational facts from plain text to enrich existing
KBs. Many works regard large-scale KBs as dis-
tant supervision to annotate sentences as training
instances and build relation classifiers according to
features extracted from the sentences (Mintz et al.,
2009; Riedel et al., 2010; Hoffmann et al., 2011;
Surdeanu et al., 2012). All these methods reason
new facts only based on plain text. TransE was
used to enrich a text-based model and achieved a
significant improvement (Weston et al., 2013), and
so do TransH (Wang et al., 2014) and TransR (Lin
et al., 2015). In this task, we explore the effective-
ness of PTransE for relation extraction from text.
We use New York Times corpus (NYT) released
by (Riedel et al., 2010) as training and testing data.
NYT aligns Freebase with the articles in New York
Times, and extracts sentence-level features such
as part-of-speech tags, dependency tree paths for
each mention. There are 53 relations (including
non-relation denoted as NA) and 121,034 training
mentions. We use FB40K as the KB, consisting all
entities mentioned in NYT and 1, 336 relations.
In the experiments, we implemented the text-
based model Sm2r presented in (Weston et al.,
2013). We combine the ranking scores from
the text-based model with those from KB rep-
resentations to rank testing triples, and gener-
ate precision-recall curves for both TransE and
PTransE. For learning of TransE and PTransE,
we set the dimensions of entities/relations embed-
dings k = 50, the learning rate A = 0.001, the
margin γ = 1.0 and dissimilarity metric as L1.
We also compare with MIMLRE (Surdeanu et al.,
2012) which is the state-of-art method using dis-
tant supervision. The evaluation curves are shown
in Figure 3.
</bodyText>
<figure confidence="0.929406">
0 0.05 0.1 0.15 0.2
Recall
</figure>
<figureCaption confidence="0.9988225">
Figure 3: Precision-recall curves of Sm2r, TransE
and PTransE combine with Sm2r.
</figureCaption>
<figure confidence="0.9984925">
1
0.9
Sm2r
TransE
PTransE
MIMLRE
0.8
0.7
0.6
0.5
0.4
Precision
</figure>
<page confidence="0.993393">
711
</page>
<bodyText confidence="0.999945260869565">
From Figure 3 we can observe that, by combin-
ing with the text-based model Sm2r, the precision
of PTransE significantly outperforms TransE espe-
cially for the top-ranked triples. This indicates that
encoding relation paths is also useful for relation
extraction from text.
Note that TransE used here does not consider
reverse relations and relation paths because the
performance does not change much. We analyze
the reason as follows. In the task of knowledge
base completion, each testing triple has at least
one valid relation. In contrast, many testing triples
in this task correspond to non-relation (NA), and
there are usually several relation paths between
two entities in these non-relation triples. TransE
does not encode relation paths during the training
phase like PTransE, which results in worse perfor-
mance for predicting non-relation when consider-
ing relation paths in the testing phase, and com-
pensates the improvement on those triples that do
have relations. This indicates it is non-trivial to
encode relation paths, and also confirms the effec-
tiveness of PTransE.
</bodyText>
<subsectionHeader confidence="0.999321">
3.4 Case Study of Relation Inference
</subsectionHeader>
<bodyText confidence="0.999185181818182">
We have shown that PTransE can achieve high per-
formance for knowledge base completion and re-
lation extraction from text. In this section, we
present some examples of relation inference ac-
cording to relation paths.
According to the learning results of PTransE,
we can find new facts from KBs. As shown in
Figure 4, two entities Forrest Gump and English
are connected by three relation paths, which give
us more confidence to predict the relation between
the two entities to LanguageOfFilm.
</bodyText>
<figureCaption confidence="0.971341">
Figure 4: An inference example in Freebase.
</figureCaption>
<sectionHeader confidence="0.998881" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999982955555555">
Recent years have witnessed great advances of
modeling multi-relational data such as social net-
works and KBs. Many works cope with rela-
tional learning as a multi-relational representation
learning problem, encoding both entities and re-
lations in a low-dimensional latent space, based
on Bayesian clustering (Kemp et al., 2006; Miller
et al., 2009; Sutskever et al., 2009; Zhu, 2012),
energy-based models (Bordes et al., 2011; Chen et
al., 2013; Socher et al., 2013; Bordes et al., 2013;
Bordes et al., 2014), matrix factorization (Singh
and Gordon, 2008; Nickel et al., 2011; Nickel et
al., 2012) . Among existing representation mod-
els, TransE (Bordes et al., 2013) regards a relation
as translation between head and tail entities for
optimization, which achieves a good trade-off be-
tween prediction accuracy and computational effi-
ciency. All existing representation learning meth-
ods of knowledge bases only use direct relations
between entities, ignoring rich information in re-
lation paths.
Relation paths have already been widely con-
sidered in social networks and recommender sys-
tems. Most of these works regard each relation and
path as discrete symbols, and deal with them us-
ing graph-based algorithms, such as random walks
with restart (Tong et al., 2006). Relation paths
have also been used for inference on large-scale
KBs, such as Path Ranking algorithm (PRA) (Lao
and Cohen, 2010), which has been adopted for ex-
pert finding (Lao and Cohen, 2010) and informa-
tion retrieval (Lao et al., 2012). PRA has also been
used for relation extraction based on KB structure
(Lao et al., 2011; Gardner et al., 2013). (Nee-
lakantan et al., 2015) further learns a recurrent
neural network (RNN) to represent unseen rela-
tion paths according to involved relations. We
note that, these methods focus on modeling rela-
tion paths for relation extraction without consid-
ering any information of entities. In contrast, by
successfully integrating the merits of modeling en-
tities and relation paths, PTransE can learn supe-
rior representations of both entities and relations
for knowledge graph completion and relation ex-
traction as shown in our experiments.
</bodyText>
<sectionHeader confidence="0.995644" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999322333333333">
This paper presents PTransE, a novel representa-
tion learning method for KBs, which encodes re-
lation paths to embed both entities and relations
</bodyText>
<figure confidence="0.999068">
Robert
Zemeckis
Language
Director
Country
Language of Film
Forrest
Gump
English
Paramount
Pictures
United
States
Official Language
Company
Release Region
Official Language
Norway
</figure>
<page confidence="0.988508">
712
</page>
<bodyText confidence="0.99975324">
in a low-dimensional space. To take advantages
of relation paths, we propose path-constraint re-
source allocation to measure relation path reliabil-
ity, and employ semantic composition of relations
to represent paths for optimization. We evaluate
PTransE on knowledge base completion and re-
lation extraction from text. Experimental results
show that PTransE achieves consistent and signif-
icant improvements as compared with TransE and
other baselines.
In future, we will explore the following research
directions: (1) This paper only considers the infer-
ence patterns between direct relations and relation
paths between two entities for learning. There are
much complicated patterns among relations. For
example, the inference form Queen(e) Inference
=====⇒
Female(e) cannot be handled by PTransE. We
may take advantages of first-order logic to encode
these inference patterns for representation learn-
ing. (2) There are some extensions for TransE,
e.g., TransH and TransR. It is non-trivial for them
to adopt the idea of PTransE, and we will explore
to extend PTransE to these models to better deal
with complicated scenarios of KBs.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9971119">
Zhiyuan Liu and Maosong Sun are supported by
the 973 Program (No. 2014CB340501) and the
National Natural Science Foundation of China
(NSFC No. 61133012) and Tsinghua-Samsung
Joint Lab. Huanbo Luan is supported by the
National Natural Science Foundation of China
(NSFC No. 61303075). We sincerely thank Yan-
song Feng for insightful discussions, and thank all
anonymous reviewers for their constructive com-
ments.
</bodyText>
<sectionHeader confidence="0.997848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999753808823529">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of KDD, pages
1247–1250.
Antoine Bordes, Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured
embeddings of knowledge bases. In Proceedings of
AAAI, pages 301–306.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In Proceedings of AISTATS, pages 127–
135.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Proceedings of NIPS, pages
2787–2795.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2014. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, 94(2):233–259.
Danqi Chen, Richard Socher, Christopher D Manning,
and Andrew Y Ng. 2013. Learning new facts from
knowledge bases with neural tensor networks and
semantic word vectors. Proceedings of ICLR.
Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom M Mitchell. 2013. Improving learning
and inference in a large knowledge-base using latent
syntactic cues. In Proceedings of EMNLP, pages
833–838.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of ACL-
HLT, pages 541–550.
Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes,
and Guillaume R Obozinski. 2012. A latent factor
model for highly multi-relational data. In Proceed-
ings of NIPS, pages 3167–3175.
Charles Kemp, Joshua B Tenenbaum, Thomas L Grif-
fiths, Takeshi Yamada, and Naonori Ueda. 2006.
Learning systems of concepts with an infinite rela-
tional model. In Proceedings of AAAI, volume 3,
page 5.
Ni Lao and William W Cohen. 2010. Relational re-
trieval using a combination of path-constrained ran-
dom walks. Machine learning, 81(1):53–67.
Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large scale
knowledge base. In Proceedings of EMNLP, pages
529–539.
Ni Lao, Amarnag Subramanya, Fernando Pereira, and
William W Cohen. 2012. Reading the web with
learned syntactic-semantic inference rules. In Pro-
ceedings of EMNLP-CoNLL, pages 1017–1026.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In Pro-
ceedings of AAAI, pages 2181–2187.
Linyuan L¨u and Tao Zhou. 2011. Link prediction in
complex networks: A survey. Physica A: Statistical
Mechanics and its Applications, 390(6):1150–1170.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of Interspeech, pages 1045–1048.
</reference>
<page confidence="0.985929">
713
</page>
<reference confidence="0.999896216666667">
Kurt Miller, Michael I Jordan, and Thomas L Griffiths.
2009. Nonparametric latent feature models for link
prediction. In Proceedings of NIPS, pages 1276–
1284.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236–244.
Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space models
for knowledge base inference. In 2015 AAAI Spring
Symposium Series.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings of
ICML, pages 809–816.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: scalable machine
learning for linked data. In Proceedings of WWW,
pages 271–280.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.
Ajit P Singh and Geoffrey J Gordon. 2008. Relational
learning via collective matrix factorization. In Pro-
ceedings of KDD, pages 650–658.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Proceedings of NIPS, pages 926–934.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.
Ilya Sutskever, Joshua B Tenenbaum, and Ruslan
Salakhutdinov. 2009. Modelling relational data us-
ing bayesian clustered tensor factorization. In Pro-
ceedings of NIPS, pages 1821–1828.
Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan.
2006. Fast random walk with restart and its applica-
tions. In Proceedings of ICDM, pages 613–622.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of AAAI,
pages 1112–1119.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of EMNLP, pages
1366–1371.
Tao Zhou, Jie Ren, Mat´uˇs Medo, and Yi-Cheng Zhang.
2007. Bipartite network projection and personal rec-
ommendation. Physical Review E, 76(4):046115.
Jun Zhu. 2012. Max-margin nonparametric latent fea-
ture models for link prediction. In Proceedings of
ICML, pages 719–726.
</reference>
<page confidence="0.998177">
714
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.472642">
<title confidence="0.999996">Modeling Relation Paths for Representation Learning of Knowledge Bases</title>
<author confidence="0.999174">Zhiyuan Huanbo Maosong Siwei Song</author>
<affiliation confidence="0.9972515">of Computer Science and Technology, State Key Lab on Intelligent Technology and National Lab for Information Science and Technology, Tsinghua University, Beijing,</affiliation>
<address confidence="0.50746">R&amp;D Institute of China, Beijing, China</address>
<abstract confidence="0.99879392">Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves significant and consistent improvements on knowledge base completion and relation extraction from text. The source code of this paper can be obtained from</abstract>
<web confidence="0.965443">https://github.com/mrlyk423/</web>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="16133" citStr="Bollacker et al., 2008" startWordPosition="2726" endWordPosition="2729">PTransE is (NeK + NrK), which is the same as TransE. PTransE follows the optimization procedure introduced by (Bordes et al., 2013) to solve Equation (9). We denote S as the number of triples for learning, P as the expected number of relation paths between two entities, and L as the expected 708 length of relation paths. For each iteration in optimization, the complexity of TransE is O(SK) and the complexity of PTransE is O(SKPL) for ADD and MUL, and O(SK2PL) for RNN. 3 Experiments and Analysis 3.1 Data Sets and Experimental Setting We evaluate our method on a typical large-scale KB Freebase (Bollacker et al., 2008). In this paper, we adopt two datasets extracted from Freebase, i.e., FB15K and FB40K. The statistics of the datasets are listed in Table 1. Table 1: Statistics of data sets. Dataset #Rel #Ent #Train #Valid # Test FB15K 1,345 14,951 483,142 50,000 59,071 FB40K 1,336 39,528 370,648 67,946 96,678 We evaluate the performance of PTransE and other baselines by predicting whether testing triples hold. We consider two scenarios: (1) Knowledge base completion, aiming to predict the missing entities or relations in given triples only based on existing KBs. (2) Relation extraction from texts, aiming to </context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of KDD, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Jason Weston</author>
<author>Ronan Collobert</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning structured embeddings of knowledge bases.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>301--306</pages>
<contexts>
<context position="17017" citStr="Bordes et al., 2011" startWordPosition="2877" endWordPosition="2881">6 39,528 370,648 67,946 96,678 We evaluate the performance of PTransE and other baselines by predicting whether testing triples hold. We consider two scenarios: (1) Knowledge base completion, aiming to predict the missing entities or relations in given triples only based on existing KBs. (2) Relation extraction from texts, aiming to extract relations between entities based on information from both plain texts and KBs. 3.2 Knowledge Base Completion The task of knowledge base completion is to complete the triple (h, r, t) when one of h, t, r is missing. The task has been used for evaluation in (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013). We conduct the evaluation on FB15K, which has 483,142 relational triples and 1, 345 relation types, among which there are rich inference and reasoning patterns. In the testing phase, for each testing triple (h, r, t), we define the following score function for prediction S(h, r, t) = G(h, r, t) + G(t, r−1, h), (14) and the score function G(h, r, t) is further defined as G(h,r,t) =||h + r − t||+ 1 Z Pr(r|p)R(p|h, t)||p − r||. p∈P(h,t) (15) The score function is similar to the energy function defined in Section 2.1. The difference is that, here we con</context>
<context position="19902" citStr="Bordes et al., 2011" startWordPosition="3382" endWordPosition="3385">testing triple. Here we adopt a re-ranking method: we first rank all candidate entities according to the scores from TransE, and then re-rank top-500 candidates according to the scores from PTransE. For PTransE, we find the best hyperparameters according to the mean rank in validation set. The optimal configurations of PTransE we used are A = 0.001, ry = 1, k = 100 and taking L1 as dissimilarity. For training, we limit the number of epochs over all the training triples to 500. Evaluation results of entity prediction are shown in Table 2. The baselines include RESCAL (Nickel et al., 2011), SE (Bordes et al., 2011), SME (linear) (Bordes et al., 2012), SME (bilinear) 709 Table 2: Evaluation results on entity prediction. Metric Mean Rank Hits@10 (%) Raw Filter Raw Filter RESCAL 828 683 28.4 44.1 SE 273 162 28.8 39.8 SME (linear) 274 154 30.7 40.8 SME (bilinear) 284 158 31.3 41.3 LFM 283 164 26.0 33.1 TransE 243 125 34.9 47.1 TransH 212 87 45.7 64.4 TransR 198 77 48.2 68.7 TransE (Our) 205 63 47.9 70.2 PTransE (ADD, 2-step) 200 54 51.8 83.4 PTransE (MUL, 2-step) 216 67 47.4 77.7 PTransE (RNN, 2-step) 242 92 50.6 82.2 PTransE (ADD, 3-step) 207 58 51.4 84.6 (Bordes et al., 2012), LFM (Jenatton et al., 2012),</context>
<context position="29987" citStr="Bordes et al., 2011" startWordPosition="5041" endWordPosition="5044">re connected by three relation paths, which give us more confidence to predict the relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been</context>
</contexts>
<marker>Bordes, Weston, Collobert, Bengio, 2011</marker>
<rawString>Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. 2011. Learning structured embeddings of knowledge bases. In Proceedings of AAAI, pages 301–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of AISTATS,</booktitle>
<pages>127--135</pages>
<contexts>
<context position="17038" citStr="Bordes et al., 2012" startWordPosition="2882" endWordPosition="2885">46 96,678 We evaluate the performance of PTransE and other baselines by predicting whether testing triples hold. We consider two scenarios: (1) Knowledge base completion, aiming to predict the missing entities or relations in given triples only based on existing KBs. (2) Relation extraction from texts, aiming to extract relations between entities based on information from both plain texts and KBs. 3.2 Knowledge Base Completion The task of knowledge base completion is to complete the triple (h, r, t) when one of h, t, r is missing. The task has been used for evaluation in (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013). We conduct the evaluation on FB15K, which has 483,142 relational triples and 1, 345 relation types, among which there are rich inference and reasoning patterns. In the testing phase, for each testing triple (h, r, t), we define the following score function for prediction S(h, r, t) = G(h, r, t) + G(t, r−1, h), (14) and the score function G(h, r, t) is further defined as G(h,r,t) =||h + r − t||+ 1 Z Pr(r|p)R(p|h, t)||p − r||. p∈P(h,t) (15) The score function is similar to the energy function defined in Section 2.1. The difference is that, here we consider the reliability</context>
<context position="19938" citStr="Bordes et al., 2012" startWordPosition="3388" endWordPosition="3391">anking method: we first rank all candidate entities according to the scores from TransE, and then re-rank top-500 candidates according to the scores from PTransE. For PTransE, we find the best hyperparameters according to the mean rank in validation set. The optimal configurations of PTransE we used are A = 0.001, ry = 1, k = 100 and taking L1 as dissimilarity. For training, we limit the number of epochs over all the training triples to 500. Evaluation results of entity prediction are shown in Table 2. The baselines include RESCAL (Nickel et al., 2011), SE (Bordes et al., 2011), SME (linear) (Bordes et al., 2012), SME (bilinear) 709 Table 2: Evaluation results on entity prediction. Metric Mean Rank Hits@10 (%) Raw Filter Raw Filter RESCAL 828 683 28.4 44.1 SE 273 162 28.8 39.8 SME (linear) 274 154 30.7 40.8 SME (bilinear) 284 158 31.3 41.3 LFM 283 164 26.0 33.1 TransE 243 125 34.9 47.1 TransH 212 87 45.7 64.4 TransR 198 77 48.2 68.7 TransE (Our) 205 63 47.9 70.2 PTransE (ADD, 2-step) 200 54 51.8 83.4 PTransE (MUL, 2-step) 216 67 47.4 77.7 PTransE (RNN, 2-step) 242 92 50.6 82.2 PTransE (ADD, 3-step) 207 58 51.4 84.6 (Bordes et al., 2012), LFM (Jenatton et al., 2012), TransE (Bordes et al., 2013) (origi</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In Proceedings of AISTATS, pages 127– 135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Alberto GarciaDuran</author>
<author>Jason Weston</author>
<author>Oksana Yakhnenko</author>
</authors>
<title>Translating embeddings for modeling multirelational data.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2787--2795</pages>
<contexts>
<context position="2344" citStr="Bordes et al., 2013" startWordPosition="331" endWordPosition="334">on answering and Web search. Although typical KBs are large in size, usually containing thousands of relation types, millions of entities and billions of facts (triples), they are far from ∗Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) complete. Hence, many efforts have been invested in relation extraction to enrich KBs. Recent studies reveal that, neural-based representation learning methods are scalable and effective to encode relational knowledge with lowdimensional representations of both entities and relations, which can be further used to extract unknown relational facts. TransE (Bordes et al., 2013) is a typical method in the neural-based approach, which learns vectors (i.e., embeddings) for both entities and relations. The basic idea behind TransE is that, the relationship between two entities corresponds to a translation between the embeddings of the entities, that is, h + r ≈ t when the triple (h, r, t) holds. Since TransE has issues when modeling 1-to-N, N-to-1 and N-to-N relations, various methods such as TransH (Wang et al., 2014) and TransR (Lin et al., 2015) are proposed to assign an entity with different representations when involved in various relations. Despite their success i</context>
<context position="15641" citStr="Bordes et al., 2013" startWordPosition="2639" endWordPosition="2642">common in Freebase, there will be billions of 4-step paths. Even for 2-step or 3-step paths, it will be timeconsuming to consider all of them without limitation. For computational efficiency, in this paper we restrict the length of paths to at most 3-steps and consider those relation paths with the reliability score larger than 0.01. 2.6 Complexity Analysis We denote Ne as the number of entities, Nr as the number of relations and K as the vector dimension. The model parameter size of PTransE is (NeK + NrK), which is the same as TransE. PTransE follows the optimization procedure introduced by (Bordes et al., 2013) to solve Equation (9). We denote S as the number of triples for learning, P as the expected number of relation paths between two entities, and L as the expected 708 length of relation paths. For each iteration in optimization, the complexity of TransE is O(SK) and the complexity of PTransE is O(SKPL) for ADD and MUL, and O(SK2PL) for RNN. 3 Experiments and Analysis 3.1 Data Sets and Experimental Setting We evaluate our method on a typical large-scale KB Freebase (Bollacker et al., 2008). In this paper, we adopt two datasets extracted from Freebase, i.e., FB15K and FB40K. The statistics of the</context>
<context position="17060" citStr="Bordes et al., 2013" startWordPosition="2886" endWordPosition="2889"> the performance of PTransE and other baselines by predicting whether testing triples hold. We consider two scenarios: (1) Knowledge base completion, aiming to predict the missing entities or relations in given triples only based on existing KBs. (2) Relation extraction from texts, aiming to extract relations between entities based on information from both plain texts and KBs. 3.2 Knowledge Base Completion The task of knowledge base completion is to complete the triple (h, r, t) when one of h, t, r is missing. The task has been used for evaluation in (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013). We conduct the evaluation on FB15K, which has 483,142 relational triples and 1, 345 relation types, among which there are rich inference and reasoning patterns. In the testing phase, for each testing triple (h, r, t), we define the following score function for prediction S(h, r, t) = G(h, r, t) + G(t, r−1, h), (14) and the score function G(h, r, t) is further defined as G(h,r,t) =||h + r − t||+ 1 Z Pr(r|p)R(p|h, t)||p − r||. p∈P(h,t) (15) The score function is similar to the energy function defined in Section 2.1. The difference is that, here we consider the reliability of a path p is also r</context>
<context position="18347" citStr="Bordes et al., 2013" startWordPosition="3115" endWordPosition="3118">(r|p) = Pr(r, p)/ Pr(p) obtained from the training data. We divide the stage into two sub-tasks, i.e., entity prediction and relation prediction. 3.2.1 Entity Prediction In the sub-task of entity prediction, we follow the setting in (Bordes et al., 2013). For each testing triple with missing head or tail entity, various methods are asked to compute the scores of G(h, r, t) for all candidate entities and rank them in descending order. We use two measures as our evaluation metrics: the mean of correct entity ranks and the proportion of valid entities ranked in top-10 (Hits@10). As mentioned in (Bordes et al., 2013), the measures are desirable but flawed when an invalid triple ends up being valid in KBs. For example, when the testing triple is (Obama, PresidentOf, USA) with the head entity Obama is missing, the head entity Lincoln may be regarded invalid for prediction, but in fact it is valid in KBs. The evaluation metrics will under-estimate those methods that rank these triples high. Hence, we can filter out all these valid triples in KBs before ranking. The first evaluation setting was named as “Raw” and the second one as “Filter”. For comparison, we select all methods in (Bordes et al., 2013; Wang e</context>
<context position="20531" citStr="Bordes et al., 2013" startWordPosition="3497" endWordPosition="3500">near) (Bordes et al., 2012), SME (bilinear) 709 Table 2: Evaluation results on entity prediction. Metric Mean Rank Hits@10 (%) Raw Filter Raw Filter RESCAL 828 683 28.4 44.1 SE 273 162 28.8 39.8 SME (linear) 274 154 30.7 40.8 SME (bilinear) 284 158 31.3 41.3 LFM 283 164 26.0 33.1 TransE 243 125 34.9 47.1 TransH 212 87 45.7 64.4 TransR 198 77 48.2 68.7 TransE (Our) 205 63 47.9 70.2 PTransE (ADD, 2-step) 200 54 51.8 83.4 PTransE (MUL, 2-step) 216 67 47.4 77.7 PTransE (RNN, 2-step) 242 92 50.6 82.2 PTransE (ADD, 3-step) 207 58 51.4 84.6 (Bordes et al., 2012), LFM (Jenatton et al., 2012), TransE (Bordes et al., 2013) (original version and our implementation considering reverse relations), TransH (Wang et al., 2014), and TransR (Lin et al., 2015). For PTransE, we consider three composition operations for relation path representation: addition (ADD), multiplication (MUL) and recurrent neural networks (RNN). We also consider relation paths with at most 2-steps and 3-steps. With the same configurations of PTransE, our TransE implementation achieves much better performance than that reported in (Bordes et al., 2013). From Table 2 we observe that: (1) PTransE significantly and consistently outperforms other bas</context>
<context position="22313" citStr="Bordes et al., 2013" startWordPosition="3794" endWordPosition="3797">r PTransE, the addition operation outperforms other composition operations in both Mean Rank and Hits@10. The reason is that, the addition operation is compatible with the learning objectives of both TransE and PTransE. Take h r1 ) e1 �) t for r2 example. The optimization objectives of two direct relations h + r1 = e1 and e1 + r2 = t can easily derive the path objective h + r1 + r2 = t. (3) PTransE of considering relation paths with at most 2-step and 3-step achieve comparable results. This indicates that it may be unnecessary to consider those relation paths that are too long. As defined in (Bordes et al., 2013), relations in KBs can be divided into various types according to their mapping properties such as 1-to-1, 1-toN, N-to-1 and N-to-N. Here we demonstrate the performance of PTransE and some baselines with respect to different types of relations in Table 3. It is observed that, on all mapping types of relations, PTransE consistently achieves significant improvement as compared with TransE. 3.2.2 Relation Prediction Relation prediction aims to predict relations given two entities. We also use FB15K for evaluation. In this sub-task, we can use the score function of PTransE to rank candidate relati</context>
<context position="30048" citStr="Bordes et al., 2013" startWordPosition="5053" endWordPosition="5056">idence to predict the relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems</context>
</contexts>
<marker>Bordes, Usunier, GarciaDuran, Weston, Yakhnenko, 2013</marker>
<rawString>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Proceedings of NIPS, pages 2787–2795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>A semantic matching energy function for learning with multi-relational data.</title>
<date>2014</date>
<booktitle>Machine Learning,</booktitle>
<volume>94</volume>
<issue>2</issue>
<contexts>
<context position="30070" citStr="Bordes et al., 2014" startWordPosition="5057" endWordPosition="5060"> relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works </context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2014</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014. A semantic matching energy function for learning with multi-relational data. Machine Learning, 94(2):233–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning new facts from knowledge bases with neural tensor networks and semantic word vectors.</title>
<date>2013</date>
<booktitle>Proceedings of ICLR.</booktitle>
<contexts>
<context position="30006" citStr="Chen et al., 2013" startWordPosition="5045" endWordPosition="5048"> relation paths, which give us more confidence to predict the relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered </context>
</contexts>
<marker>Chen, Socher, Manning, Ng, 2013</marker>
<rawString>Danqi Chen, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2013. Learning new facts from knowledge bases with neural tensor networks and semantic word vectors. Proceedings of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gardner</author>
<author>Partha Pratim Talukdar</author>
<author>Bryan Kisiel</author>
<author>Tom M Mitchell</author>
</authors>
<title>Improving learning and inference in a large knowledge-base using latent syntactic cues.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>833--838</pages>
<contexts>
<context position="31173" citStr="Gardner et al., 2013" startWordPosition="5235" endWordPosition="5238">s. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. 5 Conclusion and Future Work This paper presents PTransE, a novel repr</context>
</contexts>
<marker>Gardner, Talukdar, Kisiel, Mitchell, 2013</marker>
<rawString>Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel, and Tom M Mitchell. 2013. Improving learning and inference in a large knowledge-base using latent syntactic cues. In Proceedings of EMNLP, pages 833–838.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of ACLHLT,</booktitle>
<pages>541--550</pages>
<contexts>
<context position="26316" citStr="Hoffmann et al., 2011" startWordPosition="4443" endWordPosition="4446">+Path 2.4 1.9 65.2 89.0 PTransE (ADD, 2-step) 1.7 1.2 69.5 93.6 -TransE 135.8 135.3 51.4 78.0 -Path 2.0 1.6 69.7 89.0 PTransE (MUL, 2-step) 2.5 2.0 66.3 89.0 PTransE (RNN, 2-step) 1.9 1.4 68.3 93.2 PTransE (ADD, 3-step) 1.8 1.4 68.5 94.0 3.3 Relation Extraction from Text Relation extraction from text aims to extract relational facts from plain text to enrich existing KBs. Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these methods reason new facts only based on plain text. TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al., 2015). In this task, we explore the effectiveness of PTransE for relation extraction from text. We use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data. NYT aligns Freebase with the articles in New York Times, and extracts sentence-level features such as part-of-speech tags, dependency tree paths for e</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of ACLHLT, pages 541–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodolphe Jenatton</author>
<author>Nicolas L Roux</author>
<author>Antoine Bordes</author>
<author>Guillaume R Obozinski</author>
</authors>
<title>A latent factor model for highly multi-relational data.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3167--3175</pages>
<contexts>
<context position="20501" citStr="Jenatton et al., 2012" startWordPosition="3492" endWordPosition="3495">E (Bordes et al., 2011), SME (linear) (Bordes et al., 2012), SME (bilinear) 709 Table 2: Evaluation results on entity prediction. Metric Mean Rank Hits@10 (%) Raw Filter Raw Filter RESCAL 828 683 28.4 44.1 SE 273 162 28.8 39.8 SME (linear) 274 154 30.7 40.8 SME (bilinear) 284 158 31.3 41.3 LFM 283 164 26.0 33.1 TransE 243 125 34.9 47.1 TransH 212 87 45.7 64.4 TransR 198 77 48.2 68.7 TransE (Our) 205 63 47.9 70.2 PTransE (ADD, 2-step) 200 54 51.8 83.4 PTransE (MUL, 2-step) 216 67 47.4 77.7 PTransE (RNN, 2-step) 242 92 50.6 82.2 PTransE (ADD, 3-step) 207 58 51.4 84.6 (Bordes et al., 2012), LFM (Jenatton et al., 2012), TransE (Bordes et al., 2013) (original version and our implementation considering reverse relations), TransH (Wang et al., 2014), and TransR (Lin et al., 2015). For PTransE, we consider three composition operations for relation path representation: addition (ADD), multiplication (MUL) and recurrent neural networks (RNN). We also consider relation paths with at most 2-steps and 3-steps. With the same configurations of PTransE, our TransE implementation achieves much better performance than that reported in (Bordes et al., 2013). From Table 2 we observe that: (1) PTransE significantly and cons</context>
</contexts>
<marker>Jenatton, Roux, Bordes, Obozinski, 2012</marker>
<rawString>Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes, and Guillaume R Obozinski. 2012. A latent factor model for highly multi-relational data. In Proceedings of NIPS, pages 3167–3175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Kemp</author>
<author>Joshua B Tenenbaum</author>
<author>Thomas L Griffiths</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Learning systems of concepts with an infinite relational model.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<volume>3</volume>
<pages>5</pages>
<contexts>
<context position="29888" citStr="Kemp et al., 2006" startWordPosition="5025" endWordPosition="5028">sE, we can find new facts from KBs. As shown in Figure 4, two entities Forrest Gump and English are connected by three relation paths, which give us more confidence to predict the relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relati</context>
</contexts>
<marker>Kemp, Tenenbaum, Griffiths, Yamada, Ueda, 2006</marker>
<rawString>Charles Kemp, Joshua B Tenenbaum, Thomas L Griffiths, Takeshi Yamada, and Naonori Ueda. 2006. Learning systems of concepts with an infinite relational model. In Proceedings of AAAI, volume 3, page 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>William W Cohen</author>
</authors>
<title>Relational retrieval using a combination of path-constrained random walks.</title>
<date>2010</date>
<booktitle>Machine learning,</booktitle>
<pages>81--1</pages>
<contexts>
<context position="30952" citStr="Lao and Cohen, 2010" startWordPosition="5195" endWordPosition="5198">d trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE</context>
</contexts>
<marker>Lao, Cohen, 2010</marker>
<rawString>Ni Lao and William W Cohen. 2010. Relational retrieval using a combination of path-constrained random walks. Machine learning, 81(1):53–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Tom Mitchell</author>
<author>William W Cohen</author>
</authors>
<title>Random walk inference and learning in a large scale knowledge base.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>529--539</pages>
<contexts>
<context position="31150" citStr="Lao et al., 2011" startWordPosition="5231" endWordPosition="5234">n in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. 5 Conclusion and Future Work This paper present</context>
</contexts>
<marker>Lao, Mitchell, Cohen, 2011</marker>
<rawString>Ni Lao, Tom Mitchell, and William W Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In Proceedings of EMNLP, pages 529–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ni Lao</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>William W Cohen</author>
</authors>
<title>Reading the web with learned syntactic-semantic inference rules.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1017--1026</pages>
<contexts>
<context position="31062" citStr="Lao et al., 2012" startWordPosition="5215" endWordPosition="5218">s of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation</context>
</contexts>
<marker>Lao, Subramanya, Pereira, Cohen, 2012</marker>
<rawString>Ni Lao, Amarnag Subramanya, Fernando Pereira, and William W Cohen. 2012. Reading the web with learned syntactic-semantic inference rules. In Proceedings of EMNLP-CoNLL, pages 1017–1026.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yankai Lin</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
<author>Yang Liu</author>
<author>Xuan Zhu</author>
</authors>
<title>Learning entity and relation embeddings for knowledge graph completion.</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>2181--2187</pages>
<contexts>
<context position="2820" citStr="Lin et al., 2015" startWordPosition="415" endWordPosition="418">l representations of both entities and relations, which can be further used to extract unknown relational facts. TransE (Bordes et al., 2013) is a typical method in the neural-based approach, which learns vectors (i.e., embeddings) for both entities and relations. The basic idea behind TransE is that, the relationship between two entities corresponds to a translation between the embeddings of the entities, that is, h + r ≈ t when the triple (h, r, t) holds. Since TransE has issues when modeling 1-to-N, N-to-1 and N-to-N relations, various methods such as TransH (Wang et al., 2014) and TransR (Lin et al., 2015) are proposed to assign an entity with different representations when involved in various relations. Despite their success in modeling relational facts, TransE and its extensions only take direct relations between entities into consideration. It is known that there are also substantial multiple-step relation paths between entities indicating their semantic relationships. The relation paths reflect complicated inference patterns among relations in KBs. For example, the BornInCity CityInState relation path h−−−−−−−−→ e1 −−−−−−−−→ e2 StateInCountry −−−−−−−−−−−→t indicates the relation Nationality</context>
<context position="20662" citStr="Lin et al., 2015" startWordPosition="3516" endWordPosition="3519">ter Raw Filter RESCAL 828 683 28.4 44.1 SE 273 162 28.8 39.8 SME (linear) 274 154 30.7 40.8 SME (bilinear) 284 158 31.3 41.3 LFM 283 164 26.0 33.1 TransE 243 125 34.9 47.1 TransH 212 87 45.7 64.4 TransR 198 77 48.2 68.7 TransE (Our) 205 63 47.9 70.2 PTransE (ADD, 2-step) 200 54 51.8 83.4 PTransE (MUL, 2-step) 216 67 47.4 77.7 PTransE (RNN, 2-step) 242 92 50.6 82.2 PTransE (ADD, 3-step) 207 58 51.4 84.6 (Bordes et al., 2012), LFM (Jenatton et al., 2012), TransE (Bordes et al., 2013) (original version and our implementation considering reverse relations), TransH (Wang et al., 2014), and TransR (Lin et al., 2015). For PTransE, we consider three composition operations for relation path representation: addition (ADD), multiplication (MUL) and recurrent neural networks (RNN). We also consider relation paths with at most 2-steps and 3-steps. With the same configurations of PTransE, our TransE implementation achieves much better performance than that reported in (Bordes et al., 2013). From Table 2 we observe that: (1) PTransE significantly and consistently outperforms other baselines including TransE. It indicates that relation paths provide a good supplement for representation learning of KBs, which have </context>
<context position="26576" citStr="Lin et al., 2015" startWordPosition="4489" endWordPosition="4492">om Text Relation extraction from text aims to extract relational facts from plain text to enrich existing KBs. Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these methods reason new facts only based on plain text. TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al., 2015). In this task, we explore the effectiveness of PTransE for relation extraction from text. We use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data. NYT aligns Freebase with the articles in New York Times, and extracts sentence-level features such as part-of-speech tags, dependency tree paths for each mention. There are 53 relations (including non-relation denoted as NA) and 121,034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented the textbased model Sm2r present</context>
</contexts>
<marker>Lin, Liu, Sun, Liu, Zhu, 2015</marker>
<rawString>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of AAAI, pages 2181–2187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linyuan L¨u</author>
<author>Tao Zhou</author>
</authors>
<title>Link prediction in complex networks: A survey. Physica A: Statistical Mechanics and its Applications,</title>
<date>2011</date>
<volume>390</volume>
<issue>6</issue>
<marker>L¨u, Zhou, 2011</marker>
<rawString>Linyuan L¨u and Tao Zhou. 2011. Link prediction in complex networks: A survey. Physica A: Statistical Mechanics and its Applications, 390(6):1150–1170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of Interspeech, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Miller</author>
<author>Michael I Jordan</author>
<author>Thomas L Griffiths</author>
</authors>
<title>Nonparametric latent feature models for link prediction.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1276--1284</pages>
<contexts>
<context position="29909" citStr="Miller et al., 2009" startWordPosition="5029" endWordPosition="5032"> facts from KBs. As shown in Figure 4, two entities Forrest Gump and English are connected by three relation paths, which give us more confidence to predict the relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities,</context>
</contexts>
<marker>Miller, Jordan, Griffiths, 2009</marker>
<rawString>Kurt Miller, Michael I Jordan, and Thomas L Griffiths. 2009. Nonparametric latent feature models for link prediction. In Proceedings of NIPS, pages 1276– 1284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="26272" citStr="Mintz et al., 2009" startWordPosition="4435" endWordPosition="4438">2.5 65.1 84.3 +Rev 2.6 2.3 67.1 86.7 +Rev+Path 2.4 1.9 65.2 89.0 PTransE (ADD, 2-step) 1.7 1.2 69.5 93.6 -TransE 135.8 135.3 51.4 78.0 -Path 2.0 1.6 69.7 89.0 PTransE (MUL, 2-step) 2.5 2.0 66.3 89.0 PTransE (RNN, 2-step) 1.9 1.4 68.3 93.2 PTransE (ADD, 3-step) 1.8 1.4 68.5 94.0 3.3 Relation Extraction from Text Relation extraction from text aims to extract relational facts from plain text to enrich existing KBs. Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these methods reason new facts only based on plain text. TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al., 2015). In this task, we explore the effectiveness of PTransE for relation extraction from text. We use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data. NYT aligns Freebase with the articles in New York Times, and extracts sentence-level features such as part</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of ACLIJCNLP, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="11519" citStr="Mitchell and Lapata, 2008" startWordPosition="1915" endWordPosition="1918">n ◦ and obtain path embedding as p = r1 ◦...◦rl. In this paper, we consider three types of composition operation: Addition (ADD). The addition operation obtains the vector of a path by summing up the vectors of all relations, which is formalized as p = r1 + ... + rl. (5) Multiplication (MUL). The multiplication operation obtains the vector of a path as the cumulative product of the vectors of all relations, which is formalized as p = r1 ·... · rl. (6) Both addition and multiplication operations are simple and have been extensively investigated in semantic composition of phrases and sentences (Mitchell and Lapata, 2008). Recurrent Neural Network (RNN). RNN is a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W: ci = f(W [ci−1; ri]), (7) where f is a non-linearity or identical function, and [a; b] represents the concatenation of two vec�Rp(m) = nESi−1(·,m) |Si(n, ·)|Rp(n), (4) 1 Composition Steve Jobs BornInCity San CityInState Californi StateInCountry Francisco a United State 707 tors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn. RNN has also been used for represent</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Benjamin Roth</author>
<author>Andrew McCallum</author>
</authors>
<title>Compositional vector space models for knowledge base inference.</title>
<date>2015</date>
<booktitle>In 2015 AAAI Spring Symposium Series.</booktitle>
<contexts>
<context position="12185" citStr="Neelakantan et al., 2015" startWordPosition="2026" endWordPosition="2029">a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W: ci = f(W [ci−1; ri]), (7) where f is a non-linearity or identical function, and [a; b] represents the concatenation of two vec�Rp(m) = nESi−1(·,m) |Si(n, ·)|Rp(n), (4) 1 Composition Steve Jobs BornInCity San CityInState Californi StateInCountry Francisco a United State 707 tors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn. RNN has also been used for representation learning of relation paths in KBs (Neelakantan et al., 2015). For a multiple-step relation path triple (h, p, t), we could have followed TransE and define the energy function as E(h, p, t) = ||h + p − t||. However, since we have minimized ||h + r − t|| with the direct relation triple (h, r, t) to make sure r Pz� t−h, we may directly define the energy function of (h, p, t) as E(h,p,t) = ||p−(t−h) ||= ||p−r ||= E(p,r), (8) which is expected to be a low score when the multiple-relation path p is consistent with the direct relation r, and high otherwise, without using entity embeddings. 2.4 Objective Formalization We formalize the optimization objective of</context>
<context position="31201" citStr="Neelakantan et al., 2015" startWordPosition="5239" endWordPosition="5243">lready been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments. 5 Conclusion and Future Work This paper presents PTransE, a novel representation learning method f</context>
</contexts>
<marker>Neelakantan, Roth, McCallum, 2015</marker>
<rawString>Arvind Neelakantan, Benjamin Roth, and Andrew McCallum. 2015. Compositional vector space models for knowledge base inference. In 2015 AAAI Spring Symposium Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Nickel</author>
<author>Volker Tresp</author>
<author>Hans-Peter Kriegel</author>
</authors>
<title>A three-way model for collective learning on multi-relational data.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>809--816</pages>
<contexts>
<context position="19876" citStr="Nickel et al., 2011" startWordPosition="3377" endWordPosition="3380"> entities in |E |for each testing triple. Here we adopt a re-ranking method: we first rank all candidate entities according to the scores from TransE, and then re-rank top-500 candidates according to the scores from PTransE. For PTransE, we find the best hyperparameters according to the mean rank in validation set. The optimal configurations of PTransE we used are A = 0.001, ry = 1, k = 100 and taking L1 as dissimilarity. For training, we limit the number of epochs over all the training triples to 500. Evaluation results of entity prediction are shown in Table 2. The baselines include RESCAL (Nickel et al., 2011), SE (Bordes et al., 2011), SME (linear) (Bordes et al., 2012), SME (bilinear) 709 Table 2: Evaluation results on entity prediction. Metric Mean Rank Hits@10 (%) Raw Filter Raw Filter RESCAL 828 683 28.4 44.1 SE 273 162 28.8 39.8 SME (linear) 274 154 30.7 40.8 SME (bilinear) 284 158 31.3 41.3 LFM 283 164 26.0 33.1 TransE 243 125 34.9 47.1 TransH 212 87 45.7 64.4 TransR 198 77 48.2 68.7 TransE (Our) 205 63 47.9 70.2 PTransE (ADD, 2-step) 200 54 51.8 83.4 PTransE (MUL, 2-step) 216 67 47.4 77.7 PTransE (RNN, 2-step) 242 92 50.6 82.2 PTransE (ADD, 3-step) 207 58 51.4 84.6 (Bordes et al., 2012), LF</context>
<context position="30137" citStr="Nickel et al., 2011" startWordPosition="5067" endWordPosition="5070">nference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with th</context>
</contexts>
<marker>Nickel, Tresp, Kriegel, 2011</marker>
<rawString>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of ICML, pages 809–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Nickel</author>
<author>Volker Tresp</author>
<author>Hans-Peter Kriegel</author>
</authors>
<title>Factorizing yago: scalable machine learning for linked data.</title>
<date>2012</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>271--280</pages>
<contexts>
<context position="30159" citStr="Nickel et al., 2012" startWordPosition="5071" endWordPosition="5074">reebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based a</context>
</contexts>
<marker>Nickel, Tresp, Kriegel, 2012</marker>
<rawString>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2012. Factorizing yago: scalable machine learning for linked data. In Proceedings of WWW, pages 271–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of ECML-PKDD,</booktitle>
<pages>148--163</pages>
<contexts>
<context position="26293" citStr="Riedel et al., 2010" startWordPosition="4439" endWordPosition="4442">.6 2.3 67.1 86.7 +Rev+Path 2.4 1.9 65.2 89.0 PTransE (ADD, 2-step) 1.7 1.2 69.5 93.6 -TransE 135.8 135.3 51.4 78.0 -Path 2.0 1.6 69.7 89.0 PTransE (MUL, 2-step) 2.5 2.0 66.3 89.0 PTransE (RNN, 2-step) 1.9 1.4 68.3 93.2 PTransE (ADD, 3-step) 1.8 1.4 68.5 94.0 3.3 Relation Extraction from Text Relation extraction from text aims to extract relational facts from plain text to enrich existing KBs. Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these methods reason new facts only based on plain text. TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al., 2015). In this task, we explore the effectiveness of PTransE for relation extraction from text. We use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data. NYT aligns Freebase with the articles in New York Times, and extracts sentence-level features such as part-of-speech tags, depe</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of ECML-PKDD, pages 148–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ajit P Singh</author>
<author>Geoffrey J Gordon</author>
</authors>
<title>Relational learning via collective matrix factorization.</title>
<date>2008</date>
<booktitle>In Proceedings of KDD,</booktitle>
<pages>650--658</pages>
<contexts>
<context position="30116" citStr="Singh and Gordon, 2008" startWordPosition="5063" endWordPosition="5066">geOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symb</context>
</contexts>
<marker>Singh, Gordon, 2008</marker>
<rawString>Ajit P Singh and Geoffrey J Gordon. 2008. Relational learning via collective matrix factorization. In Proceedings of KDD, pages 650–658.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Reasoning with neural tensor networks for knowledge base completion.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>926--934</pages>
<contexts>
<context position="30027" citStr="Socher et al., 2013" startWordPosition="5049" endWordPosition="5052">ich give us more confidence to predict the relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks an</context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Proceedings of NIPS, pages 926–934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>455--465</pages>
<contexts>
<context position="26340" citStr="Surdeanu et al., 2012" startWordPosition="4447" endWordPosition="4450"> PTransE (ADD, 2-step) 1.7 1.2 69.5 93.6 -TransE 135.8 135.3 51.4 78.0 -Path 2.0 1.6 69.7 89.0 PTransE (MUL, 2-step) 2.5 2.0 66.3 89.0 PTransE (RNN, 2-step) 1.9 1.4 68.3 93.2 PTransE (ADD, 3-step) 1.8 1.4 68.5 94.0 3.3 Relation Extraction from Text Relation extraction from text aims to extract relational facts from plain text to enrich existing KBs. Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these methods reason new facts only based on plain text. TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al., 2015). In this task, we explore the effectiveness of PTransE for relation extraction from text. We use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data. NYT aligns Freebase with the articles in New York Times, and extracts sentence-level features such as part-of-speech tags, dependency tree paths for each mention. There are 5</context>
<context position="27617" citStr="Surdeanu et al., 2012" startWordPosition="4662" endWordPosition="4665">121,034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013). We combine the ranking scores from the text-based model with those from KB representations to rank testing triples, and generate precision-recall curves for both TransE and PTransE. For learning of TransE and PTransE, we set the dimensions of entities/relations embeddings k = 50, the learning rate A = 0.001, the margin γ = 1.0 and dissimilarity metric as L1. We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision. The evaluation curves are shown in Figure 3. 0 0.05 0.1 0.15 0.2 Recall Figure 3: Precision-recall curves of Sm2r, TransE and PTransE combine with Sm2r. 1 0.9 Sm2r TransE PTransE MIMLRE 0.8 0.7 0.6 0.5 0.4 Precision 711 From Figure 3 we can observe that, by combining with the text-based model Sm2r, the precision of PTransE significantly outperforms TransE especially for the top-ranked triples. This indicates that encoding relation paths is also useful for relation extraction from text. Note that TransE used here does not consider rev</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of EMNLP, pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Joshua B Tenenbaum</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Modelling relational data using bayesian clustered tensor factorization.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1821--1828</pages>
<contexts>
<context position="29933" citStr="Sutskever et al., 2009" startWordPosition="5033" endWordPosition="5036">hown in Figure 4, two entities Forrest Gump and English are connected by three relation paths, which give us more confidence to predict the relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich informati</context>
</contexts>
<marker>Sutskever, Tenenbaum, Salakhutdinov, 2009</marker>
<rawString>Ilya Sutskever, Joshua B Tenenbaum, and Ruslan Salakhutdinov. 2009. Modelling relational data using bayesian clustered tensor factorization. In Proceedings of NIPS, pages 1821–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanghang Tong</author>
<author>Christos Faloutsos</author>
<author>Jia-Yu Pan</author>
</authors>
<title>Fast random walk with restart and its applications.</title>
<date>2006</date>
<booktitle>In Proceedings of ICDM,</booktitle>
<pages>613--622</pages>
<contexts>
<context position="30823" citStr="Tong et al., 2006" startWordPosition="5174" endWordPosition="5177">E (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths. Relation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering </context>
</contexts>
<marker>Tong, Faloutsos, Pan, 2006</marker>
<rawString>Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. 2006. Fast random walk with restart and its applications. In Proceedings of ICDM, pages 613–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Wang</author>
<author>Jianwen Zhang</author>
<author>Jianlin Feng</author>
<author>Zheng Chen</author>
</authors>
<title>Knowledge graph embedding by translating on hyperplanes.</title>
<date>2014</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>1112--1119</pages>
<contexts>
<context position="2790" citStr="Wang et al., 2014" startWordPosition="409" endWordPosition="412">al knowledge with lowdimensional representations of both entities and relations, which can be further used to extract unknown relational facts. TransE (Bordes et al., 2013) is a typical method in the neural-based approach, which learns vectors (i.e., embeddings) for both entities and relations. The basic idea behind TransE is that, the relationship between two entities corresponds to a translation between the embeddings of the entities, that is, h + r ≈ t when the triple (h, r, t) holds. Since TransE has issues when modeling 1-to-N, N-to-1 and N-to-N relations, various methods such as TransH (Wang et al., 2014) and TransR (Lin et al., 2015) are proposed to assign an entity with different representations when involved in various relations. Despite their success in modeling relational facts, TransE and its extensions only take direct relations between entities into consideration. It is known that there are also substantial multiple-step relation paths between entities indicating their semantic relationships. The relation paths reflect complicated inference patterns among relations in KBs. For example, the BornInCity CityInState relation path h−−−−−−−−→ e1 −−−−−−−−→ e2 StateInCountry −−−−−−−−−−−→t indi</context>
<context position="18959" citStr="Wang et al., 2014" startWordPosition="3222" endWordPosition="3225"> 2013), the measures are desirable but flawed when an invalid triple ends up being valid in KBs. For example, when the testing triple is (Obama, PresidentOf, USA) with the head entity Obama is missing, the head entity Lincoln may be regarded invalid for prediction, but in fact it is valid in KBs. The evaluation metrics will under-estimate those methods that rank these triples high. Hence, we can filter out all these valid triples in KBs before ranking. The first evaluation setting was named as “Raw” and the second one as “Filter”. For comparison, we select all methods in (Bordes et al., 2013; Wang et al., 2014) as our baselines and use their reported results directly since the evaluation dataset is identical. Ideally, PTransE has to find all possible relation paths between the given entity and each candidate entity. However, it is time consuming and impractical, because we need to iterate all candidate entities in |E |for each testing triple. Here we adopt a re-ranking method: we first rank all candidate entities according to the scores from TransE, and then re-rank top-500 candidates according to the scores from PTransE. For PTransE, we find the best hyperparameters according to the mean rank in va</context>
<context position="20631" citStr="Wang et al., 2014" startWordPosition="3510" endWordPosition="3513">ic Mean Rank Hits@10 (%) Raw Filter Raw Filter RESCAL 828 683 28.4 44.1 SE 273 162 28.8 39.8 SME (linear) 274 154 30.7 40.8 SME (bilinear) 284 158 31.3 41.3 LFM 283 164 26.0 33.1 TransE 243 125 34.9 47.1 TransH 212 87 45.7 64.4 TransR 198 77 48.2 68.7 TransE (Our) 205 63 47.9 70.2 PTransE (ADD, 2-step) 200 54 51.8 83.4 PTransE (MUL, 2-step) 216 67 47.4 77.7 PTransE (RNN, 2-step) 242 92 50.6 82.2 PTransE (ADD, 3-step) 207 58 51.4 84.6 (Bordes et al., 2012), LFM (Jenatton et al., 2012), TransE (Bordes et al., 2013) (original version and our implementation considering reverse relations), TransH (Wang et al., 2014), and TransR (Lin et al., 2015). For PTransE, we consider three composition operations for relation path representation: addition (ADD), multiplication (MUL) and recurrent neural networks (RNN). We also consider relation paths with at most 2-steps and 3-steps. With the same configurations of PTransE, our TransE implementation achieves much better performance than that reported in (Bordes et al., 2013). From Table 2 we observe that: (1) PTransE significantly and consistently outperforms other baselines including TransE. It indicates that relation paths provide a good supplement for representati</context>
<context position="26546" citStr="Wang et al., 2014" startWordPosition="4483" endWordPosition="4486">94.0 3.3 Relation Extraction from Text Relation extraction from text aims to extract relational facts from plain text to enrich existing KBs. Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these methods reason new facts only based on plain text. TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al., 2015). In this task, we explore the effectiveness of PTransE for relation extraction from text. We use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data. NYT aligns Freebase with the articles in New York Times, and extracts sentence-level features such as part-of-speech tags, dependency tree paths for each mention. There are 53 relations (including non-relation denoted as NA) and 121,034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations. In the experiments, we implemented th</context>
</contexts>
<marker>Wang, Zhang, Feng, Chen, 2014</marker>
<rawString>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of AAAI, pages 1112–1119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Oksana Yakhnenko</author>
<author>Nicolas Usunier</author>
</authors>
<title>Connecting language and knowledge bases with embedding models for relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1366--1371</pages>
<contexts>
<context position="26508" citStr="Weston et al., 2013" startWordPosition="4475" endWordPosition="4478">93.2 PTransE (ADD, 3-step) 1.8 1.4 68.5 94.0 3.3 Relation Extraction from Text Relation extraction from text aims to extract relational facts from plain text to enrich existing KBs. Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these methods reason new facts only based on plain text. TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al., 2015). In this task, we explore the effectiveness of PTransE for relation extraction from text. We use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data. NYT aligns Freebase with the articles in New York Times, and extracts sentence-level features such as part-of-speech tags, dependency tree paths for each mention. There are 53 relations (including non-relation denoted as NA) and 121,034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations.</context>
</contexts>
<marker>Weston, Bordes, Yakhnenko, Usunier, 2013</marker>
<rawString>Jason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of EMNLP, pages 1366–1371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zhou</author>
<author>Jie Ren</author>
<author>Mat´uˇs Medo</author>
<author>Yi-Cheng Zhang</author>
</authors>
<title>Bipartite network projection and personal recommendation. Physical Review E,</title>
<date>2007</date>
<contexts>
<context position="8701" citStr="Zhou et al., 2007" startWordPosition="1398" endWordPosition="1401"> Z = R(p|h, t)E(h, p, t), (3) 706 E pEP(h,t) R(p|h, t) is a normalization factor, and E(h, p, t) is the energy function of the triple (h, p, t). For the energy of each triple (h, p, t), the component R(p|h, t) concerns about relation path reliability, and E(h, p, t) concerns about relation path representation. We introduce the two components in detail as follows. 2.2 Relation Path Reliability We propose a path-constraint resource allocation (PCRA) algorithm to measure the reliability of a relation path. Resource allocation over networks was originally proposed for personalized recommendation (Zhou et al., 2007), and has been successfully used in information retrieval for measuring relatedness between two objects (L¨u and Zhou, 2011). Here we extend it to PCRA to measure the reliability of relation paths. The basic idea is, we assume that a certain amount of resource is associated with the head entity h, and will flow following the given path p. We use the resource amount that eventually flows to the tail entity t to measure the reliability of the path p as a meaningful connection between h and t. Formally, for a path triple (h, p, t), we compute the resource amount flowing from h to t given the path</context>
</contexts>
<marker>Zhou, Ren, Medo, Zhang, 2007</marker>
<rawString>Tao Zhou, Jie Ren, Mat´uˇs Medo, and Yi-Cheng Zhang. 2007. Bipartite network projection and personal recommendation. Physical Review E, 76(4):046115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
</authors>
<title>Max-margin nonparametric latent feature models for link prediction.</title>
<date>2012</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>719--726</pages>
<contexts>
<context position="29945" citStr="Zhu, 2012" startWordPosition="5037" endWordPosition="5038">tities Forrest Gump and English are connected by three relation paths, which give us more confidence to predict the relation between the two entities to LanguageOfFilm. Figure 4: An inference example in Freebase. 4 Related Work Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relati</context>
</contexts>
<marker>Zhu, 2012</marker>
<rawString>Jun Zhu. 2012. Max-margin nonparametric latent feature models for link prediction. In Proceedings of ICML, pages 719–726.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
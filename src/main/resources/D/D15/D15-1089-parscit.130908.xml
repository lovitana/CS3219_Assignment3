<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.067958">
<title confidence="0.898531">
Convolutional Sentence Kernel from
Word Embeddings for Short Text Categorization
</title>
<author confidence="0.966567">
Jonghoon Kim François Rousseau Michalis Vazirgiannis
</author>
<affiliation confidence="0.947147">
LIX, École Polytechnique, France
</affiliation>
<email confidence="0.992062">
john.jonghoon.kim@gmail.com
</email>
<sectionHeader confidence="0.997325" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889666666667">
This paper introduces a convolutional sen-
tence kernel based on word embeddings.
Our kernel overcomes the sparsity issue
that arises when classifying short docu-
ments or in case of little training data. Ex-
periments on six sentence datasets showed
statistically significant higher accuracy
over the standard linear kernel with n-
gram features and other proposed models.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935379310345">
With the proliferation of text data available on-
line, text categorization emerged as a prominent
research topic. Traditionally, words (unigrams)
and phrases (n-grams) have been considered as
document features and subsequently fed to a clas-
sifier such as an SVM (Joachims, 1998). In the
SVM dual formulation that relies on kernels, i. e.
similarity measures between documents, a linear
kernel can be interpreted as the number of ex-
act matching n-grams between two documents.
Consequently, for short documents or when lit-
tle training data is available, sparsity issues due
to word synonymy arise, e. g., the sentences ‘John
likes hot beverages’ and ‘John loves warm drinks’
have little overlap and therefore low linear kernel
value (only 1) in the n-gram feature space, even
with dependency tree representations and down-
ward paths for n-grams as illustrated in Figure 1.
We propose to relax the exact matching between
words by capitalizing on distances in word embed-
dings. We smooth the implicit delta word kernel,
i. e. a Dirac similarity function between unigrams,
behind the traditional linear document kernel to
capture the similarity between words that are dif-
ferent, yet semantically close. We then aggregate
these word and phrase kernels into sentence and
documents kernels through convolution resulting
in higher kernel values between semantically re-
lated sentences (e.g., close to 7 compared to 1
</bodyText>
<figureCaption confidence="0.8570015">
Figure 1: Dependency tree representations of se-
mantically related sentences yet with little overlap.
</figureCaption>
<bodyText confidence="0.99939375">
with bigram downward paths in Figure 1). Ex-
periments on six standard datasets for sentiment
analysis, subjectivity detection and topic spotting
showed statistically significant higher accuracy for
our proposed kernel over the bigram approaches.
Our main goal is to demonstrate empirically that
word distances from a given word vector space can
easily be incorporated in the standard kernel be-
tween documents for higher effectiveness and lit-
tle additional cost in efficiency.
The rest of this paper is structured as follows.
Section 2 reviews the related work. Section 3 gives
the detailed formulation of our kernel. Section 4
describes the experimental settings and the results
we obtained on several datasets. Finally, Section 5
concludes our paper and mentions future work.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999632454545455">
Siolas and d’Alché Buc (2000) pioneered the idea
of semantic kernels for text categorization, cap-
italizing on WordNet (Miller, 1995) to propose
continuous word kernels based on the inverse of
the path lengths in the tree rather than the com-
mon delta word kernel used so far, i.e. exact
matching between unigrams. Bloehdorn et al.
(2006) extended it later to other tree-based simi-
larity measures from WordNet while Mavroeidis
et al. (2005) exploited its hierarchical structure to
define a Generalized Vector Space Model kernel.
</bodyText>
<figure confidence="0.998422">
likes
loves
John
beverages
≈ John
drinks
hot
(a) ‘John likes hot beverages’
warm
(b) ‘John loves warm drinks’
</figure>
<page confidence="0.973949">
775
</page>
<note confidence="0.6590975">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 775–780,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999957028571428">
In parallel, Collins and Duffy (2001) devel-
oped the first tree kernels to compare trees based
on their topology (e. g., shared subtrees) rather
than the similarity between their nodes. Culotta
and Sorensen (2004) used them as Dependency
Tree Kernels (DTK) to capture syntactic similar-
ities while Bloehdorn and Moschitti (2007) and
Croce et al. (2011) used them on parse trees
with respectively Semantic Syntactic Tree Ker-
nels (SSTK) and Smoothing Partial Tree Kernels
(SPTK), adding node similarity based on Word-
Net to capture semantic similarities but limiting to
comparisons between words of the same POS tag.
Similarly, Gärtner et al. (2003) developed graph
kernels based on random walks and Srivastava et
al. (2013) used them on dependency trees with
Vector Tree Kernels (VTK), adding node simi-
larity based on word embeddings from SENNA
(Collobert et al., 2011) and reporting improve-
ments over SSTK. The change from WordNet to
SENNA was supported by the recent progress in
low-dimension Euclidean vector space representa-
tions of words that are better suited for computing
distances between words. Actually, in our exper-
iments, word2vec by Mikolov et al. (2013a) led
to better results than with SENNA for both VTK
and our kernels. Moreover, it possesses an addi-
tional additive compositionality property obtained
from the Skip-gram training setting (Mikolov et
al., 2013b), e. g., the closest word to ‘Germany’ +
‘capital’ in the vector space is found to be ‘Berlin’.
More recently, for short text similarity, Song
and Roth (2015) and Kenter and de Rijke (2015)
proposed additional semantic meta-features based
on word embeddings to enhance classification.
</bodyText>
<sectionHeader confidence="0.998239" genericHeader="method">
3 Formulation
</sectionHeader>
<bodyText confidence="0.974598">
We denote the embedding of a word w by w.
</bodyText>
<subsectionHeader confidence="0.997052">
3.1 Word Kernel (WK)
</subsectionHeader>
<bodyText confidence="0.998126333333333">
We define a kernel between two words as a poly-
nomial kernel over a cosine similarity in the word
embedding space:
</bodyText>
<equation confidence="0.991884">
�1 (��α
1 + �w1, w2�
WK(w1, w2) = (1)
2 �w1��w2�
</equation>
<bodyText confidence="0.999868">
where α is a scaling factor. We also tried Gaus-
sian, Laplacian and sigmoid kernels but they led
to poorer results in our experiments. Note that a
delta word kernel, i. e. the Dirac function ✶,,,1=,,,2,
leads to a document kernel corresponding to the
standard linear kernel over n-grams.
</bodyText>
<subsectionHeader confidence="0.999177">
3.2 Phrase Kernel (PhK)
</subsectionHeader>
<bodyText confidence="0.999995333333333">
Next we define a kernel between phrases consist-
ing of several words. In our work, we considered
two types of phrases: (1) co-occurrence phrases
defined as contiguous sequences of words in the
text; and (2) syntactic phrases defined as down-
ward paths in the dependency tree representation,
e. g., respectively ‘hot beverages’ and ‘beverages
hot’ in Figure 1. With this dependency tree in-
volved, we expect to have phrases that are syntac-
tically more meaningful. Note that VTK consid-
ers random walks in dependency trees instead of
downward paths, i. e. potentially taking into ac-
count same nodes multiple times for phrase length
greater than two, phenomenon known as tottering.
Once we have phrases to compare, we may con-
struct a kernel between them as the product of
word kernels if they are of the same length l. That
is, we define the Product Kernel (PK) as:
</bodyText>
<equation confidence="0.997789">
PK(p1,p2) = �l WK(w1Z , w2Z ) (2)
Z=1
</equation>
<bodyText confidence="0.999943">
where wjZ is the i-th word in phrase pj of length l.
Alternatively, in particular for phrases of different
lengths, we may embed phrases into the embed-
ding space by taking a composition operation on
the constituent word embeddings. We considered
two common forms of composition (Blacoe and
Lapata, 2012): vector addition (+) and element-
wise multiplication (0). Then we define the Com-
position Kernel (CK) between phrases as:
</bodyText>
<equation confidence="0.998646">
CK(p1,p2) = WK(p1,p2) (3)
</equation>
<bodyText confidence="0.999985">
where pj, the embedding of the phrase pj, can be
obtained either by addition (pj _ ElZ=1 wjZ) or by
element-wise multiplication (pj _ C)lZ=1 wjZ) of its
word embeddings. For CK, we do not require the
two phrases to be of the same length so the kernel
has a desirable property of being able to compare
‘Berlin’ with ‘capital of Germany’ for instance.
</bodyText>
<subsectionHeader confidence="0.998574">
3.3 Sentence Kernel (SK)
</subsectionHeader>
<bodyText confidence="0.9999548">
We can then formulate a sentence kernel in a sim-
ilar way to Zelenko et al. (2003). It is defined
through convolution as the sum of all local phrasal
similarities, i. e. kernel values between phrases
contained in the sentences:
</bodyText>
<equation confidence="0.990746666666667">
�SK(s1, s2) = λ1E λ2&apos;7 PhK(p1,p2) (4)
p1∈O(s1),
p2∈O(s2)
</equation>
<page confidence="0.962764">
776
</page>
<bodyText confidence="0.99965475">
where O(sk) is the set of either statistical or syn-
tactic phrases (or set of random walks for VTK)
in sentence sk, A1 is a decaying factor penaliz-
ing longer phrases, c = max{|p1|, |p2|} is the max-
imum length of the two phrases, A2 is a distortion
parameter controlling the length difference q be-
tween the two phrases (q = ||p1 |− |p2||) and PhK
is a phrase kernel, either PK, CK+ or CKO.
Since the composition methods we consider are
associative, we employed a dynamic programming
approach in a similar fashion to Zelenko et al.
(2003) to avoid duplicate computations.
</bodyText>
<subsectionHeader confidence="0.995701">
3.4 Document Kernel
</subsectionHeader>
<bodyText confidence="0.999983">
Finally, we sum sentence kernel values for all pairs
of sentences between two documents to get the
document kernel. Once we have obtained all doc-
ument kernel values Kij between documents i and
j, we may normalize them by VlKiiKjj as the
length of input documents might not be uniform.
</bodyText>
<sectionHeader confidence="0.999496" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999863">
We evaluated our kernel with co-occurrence and
syntactic phrases on several standard text catego-
rization tasks.
</bodyText>
<subsectionHeader confidence="0.985421">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9999905625">
We considered four tasks: (1) binary sentiment
analysis with a movie review dataset of 10,662
sentences (PL05) (Pang and Lee, 2005) and a
product review dataset (Amazon) of 2,000 multi-
line documents for 4 different product groups
(Blitzer et al., 2007) (we will report the average ef-
fectiveness over the 4 sub-collections); (2) ternary
sentiment analysis with the SemEval 2013 Task
B dataset (Twitter) containing 12,348 tweets clas-
sified as positive, neutral or negative (Nakov et
al., 2013); (3) binary subjectivity detection with a
dataset of 10,000 sentences (PL04) (Pang and Lee,
2004) and another of 11,640 sentences (MPQA)
(Wiebe et al., 2005); and (4) seven-class topic
spotting with a news dataset (News) of 32,602
one-line news summaries (Vitale et al., 2012).
</bodyText>
<subsectionHeader confidence="0.980827">
4.2 Experimental settings
</subsectionHeader>
<bodyText confidence="0.9961768">
In all our experiments, we used the FANSE parser
(Tratz and Hovy, 2011) to generate dependency
trees and the pre-trained version of word2vec1, a
300 dimensional representation of 3 million En-
glish words trained over a Google News dataset
</bodyText>
<footnote confidence="0.908295">
1https://code.google.com/p/word2vec
</footnote>
<bodyText confidence="0.999778568627451">
of 100 billion words using the Skip-gram model
and a context size of 5. While fine-tuning the em-
beddings to a specific task or on a given dataset
may improve the result for that particular task or
dataset (Levy et al., 2015), it makes the expected
results less generalizable and the method harder
to use as an off-the-shelf solution – re-training the
neural network to obtain task-specific embeddings
requires a certain amount of training data, admit-
tedly unlabeled, but still not optimal under our sce-
nario with short documents and little task-specific
training data available. Moreover, tuning the hy-
perparameters to maximize the classification accu-
racy needs to be carried out on a validation set and
therefore requires additional labeled data. Here,
we are more interested in showing that distances
in a given word vector space can enhance classi-
fication in general. As for the dependency-based
word embeddings proposed by Levy and Goldberg
(2014), we do not think they are better suited for
the problem we are tackling. As we will see in the
results, we do benefit from the dependency tree
structure in the phrase kernel but we still want the
word kernel to be based on topical similarity rather
than functional similarity.
To train and test the SVM classifier, we used
the LibSVM library (Chang and Lin, 2011) and
employed the one-vs-one strategy for multi-class
tasks. To prevent overfitting, we tuned the pa-
rameters using cross-validation on 80% of PL05
dataset (α = 5, A1 = 1 for PK since there is no
need for distortion as the phrases are of the same
length by definition, and A1 = A2 = 0.5 for CK)
and used the same set of parameters on the remain-
ing datasets. We performed normalization for our
kernel and baselines only when it led to perfor-
mance improvements on the training set (PL05,
News, PL04 and MPQA).
We report accuracy on the remaining 20% for
PL05, on the standard test split for Twitter (25%)
and News (50%) and from 5-fold cross-validation
for the other datasets (Amazon, PL04 and MPQA).
We only report accuracy as the macro-average F1-
scores led to similar conclusions (and except for
Twitter and News, the class label distributions
are balanced). Results for phrase lengths longer
than two were omitted since they were marginally
different at best. Statistical significance of im-
provement over the bigram baseline with the same
phrase definition was assessed using the micro
sign test (p &lt; 0.01) (Yang and Liu, 1999).
</bodyText>
<page confidence="0.998583">
777
</page>
<tableCaption confidence="0.764454">
Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News
(50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best
performance in the column. * indicates statistical significance at p &lt; 0.01 using micro sign test against
the bigram baseline (delta word kernel) of the same column and with the same phrase definition.
</tableCaption>
<table confidence="0.999357615384615">
phrase phrase phrase word PL05 Amazon Twitter News PL04 MPQA
definition kernel length kernel
co-occurrence PK 1 delta 0.742 0.768 0.623 0.769 0.904 0.754
co-occurrence PK 2 delta 0.739 0.765 0.611 0.766 0.907 0.754
syntactic PK 2 delta 0.748 0.791 0.646 0.767 0.910 0.757
random walk PK 2 poly 0.799 0.810 0.698 0.802 0.927 0.797
co-occurrence PK 1 poly 0.789* 0.797 0.776* 0.806* 0.923* 0.793*
co-occurrence PK 2 poly 0.784* 0.798 0.762* 0.801* 0.926* 0.794*
co-occurrence CK+ 2 poly 0.796* 0.778 0.613 0.792* 0.917* 0.796*
co-occurrence CKG 2 poly 0.801* 0.783 0.757* 0.793* 0.918* 0.794*
syntactic PK 2 poly 0.796* 0.813* 0.808* 0.805* 0.927* 0.796*
syntactic CK+ 2 poly 0.794* 0.780 0.741* 0.788* 0.918* 0.794*
syntactic CKG 2 poly 0.797* 0.774 0.744* 0.792* 0.918* 0.794*
</table>
<subsectionHeader confidence="0.743161">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999905">
Table 1 presents results from our convolutional
sentence kernel and the baseline approaches. Note
again that a delta word kernel leads to the typi-
cal unigram and bigram baseline approaches (first
three rows). The 3rd row corresponds to DTK (Cu-
lotta and Sorensen, 2004) and the 4th one to VTK
(Srivastava et al., 2013) – the difference with our
model on the 9th row lies in the function φ(·) that
enumerates all random walks in the dependency
tree representation following Gärtner et al. (2003)
whereas we only consider the downward paths.
Overall, we obtained better results than the n-
gram baselines, DTK and VTK, especially with
syntactic phrases. VTK shows good performance
across all datasets but its computation was more
than 700% slower than with our kernel. Regarding
the phrase kernels, PK generally produced better
results than CK, implying that the semantic lin-
earity and ontological relation encoded in the em-
bedding is not sufficient enough and treating them
separately is more beneficial. However, we be-
lieve CK has more room for improvement with
the use of more accurate phrase embeddings such
as the ones from Le and Mikolov (2014), Yin and
SchUtze (2014) and Yu and Dredze (2015).
There was little contribution to the accuracy
from non-unigram features, indicating that large
part of the performance improvement is credited to
the word embedding resolving the sparsity issue.
</bodyText>
<figure confidence="0.998636545454546">
Accuracy
0.94
0.86
0.74
0 1000 2000 3000 4000 5000 6000 7000 8000
Number of training examples
0.90
0.82
0.78
PK (poly)
PK (delta)
</figure>
<figureCaption confidence="0.864124">
Figure 2: Test accuracy vs. number of training
examples for our kernel and the bigram baseline.
</figureCaption>
<bodyText confidence="0.997546090909091">
This can be well observed with the following ex-
periment on the number of training examples. Fig-
ure 2 shows the accuracy on the same test set (20%
of the dataset) when the learning was done on 1%
to 100% of the training set (80% of the dataset)
for the bigram baseline and our bigram PK phrase
kernel, both with dependency tree representation,
on PL04. We see that our kernel starts to plateau
earlier in the learning curve than the baseline and
also reaches the maximum baseline accuracy with
only about 1,500 training examples.
</bodyText>
<subsectionHeader confidence="0.992103">
4.4 Computational complexity
</subsectionHeader>
<bodyText confidence="0.999748166666667">
Solving the SVM in the primal for the baselines
requires O(NnL) time where N is the number
of training documents, n is the number of words
in the document and L is the maximum phrase
length considered. The computation of VTK re-
duces down to power series computation of the
</bodyText>
<page confidence="0.994841">
778
</page>
<bodyText confidence="0.99952852631579">
adjacency matrix of the product graph, and since
we require kernel values between all documents, it
requires O(N2(n2d + n4L)) time where d is the
dimension of the word embedding space.
Our kernel is the sum of phrase kernels (PhK)
starting from every pair of nodes between two sen-
tences, for all phrase lengths (l) and distortions
(a2) under consideration. By storing intermedi-
ate values of composite vectors, a phrase kernel
can be computed in O(d) time regardless of the
phrase length, therefore the whole computation
process has O(N2n2L2d) complexity. Although
our kernel has the squared terms of the baseline’s
complexity, we are tackling the sparsity issue that
arises with short text (small n) or when little train-
ing data is available (small N). Moreover, we
were able to get better results with only bigrams
(small L). Hence, the loss in efficiency is accept-
able considering significant gains in effectiveness.
</bodyText>
<sectionHeader confidence="0.997771" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999967388888889">
In this paper, we proposed a novel convolutional
sentence kernel based on word embeddings that
overcomes the sparsity issue, which arises when
classifying short documents or when little training
data is available. We described a general frame-
work that can encompass the standard n-gram
baseline approach as well as more relaxed ver-
sions with smoother word and phrase kernels. It
achieved significant improvements over the base-
lines across all datasets when taking into account
the additional information from the latent word
similarity (word embeddings) and the syntactic
structure (dependency tree).
Future work might involve designing new ker-
nels for syntactic parse trees with appropriate sim-
ilarity measures between non-terminal nodes as
well as exploring recently proposed phrase em-
beddings for more accurate phrase kernels.
</bodyText>
<sectionHeader confidence="0.992171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991724151515151">
William Blacoe and Mirella Lapata. 2012. A Com-
parison of Vector-based Representations for Seman-
tic Composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ’12, pages 546–
556. ACL.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boomboxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ’07, pages 440–447. ACL.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Structure and Semantics for Expressive Text Ker-
nels. In Proceedings of the 16th ACM international
conference on Information and knowledge manage-
ment, CIKM ’07, pages 861–864. ACM.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining, ICDM
’06, pages 808–812. IEEE Computer Society.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1–27:27.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Advances in Neu-
ral Information Processing Systems 14, NIPS ’01,
pages 625–632. The MIT Press.
Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493–2537, November.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured Lexical Similarity via Con-
volution Kernels on Dependency Trees. In Proceed-
ings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 1034–1046. ACL.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics, ACL ’04, pages 423–
429. ACL.
Thomas Gärtner, Peter Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and
efficient alternatives. In Proceedings of the An-
nual Conference on Computational Learning The-
ory, COLT ’03, pages 129–143.
Thorsten Joachims. 1998. Text categorization with
Support Vector Machines: Learning with many rel-
evant features. In Proceedings of the 10th European
Conference on Machine Learning, ECML ’98, pages
137–142.
Tom Kenter and Maarten de Rijke. 2015. Short text
similarity with word embeddings. In Proceedings of
the 24th ACM international conference on Informa-
tion and knowledge management, CIKM ’15. ACM.
Quoc Le and Tomas Mikolov. 2014. Distributed Rep-
resentations of Sentences and Documents. In Pro-
ceedings of the 31st International Conference on
Machine Learning, volume 32 of ICML ’14, pages
</reference>
<page confidence="0.993181">
779
</page>
<reference confidence="0.998902112244897">
1188–1196. JMLR Workshop and Conference Pro-
ceedings.
Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, volume 2 of ACL ’14, pages
302–308. ACL.
Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.
Dimitrios Mavroeidis, George Tsatsaronis, Michalis
Vazirgiannis, Martin Theobald, and Gerhard
Weikum. 2005. Word Sense Disambiguation for
Exploiting Hierarchical Thesauri in Text Classifica-
tion. In Proceedings of the 9th European conference
on Principles and Practice of Knowledge Discovery
in Databases, ECML PKDD ’05, pages 181–192.
Springer-Verlag Berlin.
Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. In Proceedings of
Workshop at International Conference on Learning
Representations, ICLR ’13.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Advances in Neural Information Pro-
cessing Systems 26, NIPS ’13, pages 3111–3119.
Neural Information Processing Systems.
George A. Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38(11):39–41, November.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis
in twitter. In Proceedings of the 7th International
Workshop on Semantic Evaluation, SemEval-2013.
Bo Pang and Lilian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ’04, pages 271–278. ACL.
Bo Pang and Lilian Lee. 2005. Seeing stars: exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, ACL ’05, pages 115–124. ACL.
Georges Siolas and Florence d’Alché Buc. 2000. Sup-
port Vector Machines Based on a Semantic Ker-
nel for Text Categorization. In Proceedings of the
IEEE-INNS-ENNS International Joint Conference
on Neural Networks, volume 5 of IJCNN ’00, pages
205–209. IEEE Computer Society.
Yangqiu Song and Dan Roth. 2015. Unsupervised
Sparse Vector Densification for Short Text Sim-
ilarity. In Proceedings of the 2015 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-Short ’15, pages 1275–1280.
ACL.
Shashank Srivastava, Dirk Hovy, and Eduard H. Hovy.
2013. A Walk-Based Semantically Enriched Tree
Kernel Over Distributed Word Representations. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’13, pages 1411–1416. ACL.
Stephen Tratz and Eduard H. Hovy. 2011. A Fast,
Accurate, Non-projective, Semantically-enriched
Parser. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 1257–1268. ACL.
Daniele Vitale, Paolo Ferragina, and Ugo Scaiella.
2012. Classification of Short Texts by Deploying
Topical Annotations. In Proceedings of the 34th
European Conference on Information Retrieval,
ECIR’12, pages 376–387. Springer-Verlag.
Janyce M. Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165–210.
Yiming Yang and Xin Liu. 1999. A Re-examination
of Text Categorization Methods. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ’99, pages 42–49. ACM.
Wenpeng Yin and Hinrich Schütze. 2014. An Explo-
ration of Embeddings for Generalized Phrases. In
Proceedings of the ACL Student Research Workshop,
ACLstudent ’14, pages 41–47. ACL.
Mo Yu and Mark Dredze. 2015. Learning Composi-
tion Models for Phrase Embeddings. Transactions
of the Association for Computational Linguistics,
3:227–242.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel Methods for Relation
Extraction. Journal of Machine Learning Research,
3:1083–1106.
</reference>
<page confidence="0.997129">
780
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666174">
<title confidence="0.994302">Convolutional Sentence Kernel Word Embeddings for Short Text Categorization</title>
<author confidence="0.890754">Jonghoon Kim François Rousseau Michalis Vazirgiannis</author>
<affiliation confidence="0.720364">LIX, École Polytechnique,</affiliation>
<email confidence="0.999355">john.jonghoon.kim@gmail.com</email>
<abstract confidence="0.9945762">This paper introduces a convolutional sentence kernel based on word embeddings. Our kernel overcomes the sparsity issue that arises when classifying short documents or in case of little training data. Experiments on six sentence datasets showed statistically significant higher accuracy over the standard linear kernel with ngram features and other proposed models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A Comparison of Vector-based Representations for Semantic Composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>546--556</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="7125" citStr="Blacoe and Lapata, 2012" startWordPosition="1131" endWordPosition="1134">count same nodes multiple times for phrase length greater than two, phenomenon known as tottering. Once we have phrases to compare, we may construct a kernel between them as the product of word kernels if they are of the same length l. That is, we define the Product Kernel (PK) as: PK(p1,p2) = �l WK(w1Z , w2Z ) (2) Z=1 where wjZ is the i-th word in phrase pj of length l. Alternatively, in particular for phrases of different lengths, we may embed phrases into the embedding space by taking a composition operation on the constituent word embeddings. We considered two common forms of composition (Blacoe and Lapata, 2012): vector addition (+) and elementwise multiplication (0). Then we define the Composition Kernel (CK) between phrases as: CK(p1,p2) = WK(p1,p2) (3) where pj, the embedding of the phrase pj, can be obtained either by addition (pj _ ElZ=1 wjZ) or by element-wise multiplication (pj _ C)lZ=1 wjZ) of its word embeddings. For CK, we do not require the two phrases to be of the same length so the kernel has a desirable property of being able to compare ‘Berlin’ with ‘capital of Germany’ for instance. 3.3 Sentence Kernel (SK) We can then formulate a sentence kernel in a similar way to Zelenko et al. (20</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A Comparison of Vector-based Representations for Semantic Composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 546– 556. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07,</booktitle>
<pages>440--447</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9200" citStr="Blitzer et al., 2007" startWordPosition="1488" endWordPosition="1491"> sentences between two documents to get the document kernel. Once we have obtained all document kernel values Kij between documents i and j, we may normalize them by VlKiiKjj as the length of input documents might not be uniform. 4 Experiments We evaluated our kernel with co-occurrence and syntactic phrases on several standard text categorization tasks. 4.1 Datasets We considered four tasks: (1) binary sentiment analysis with a movie review dataset of 10,662 sentences (PL05) (Pang and Lee, 2005) and a product review dataset (Amazon) of 2,000 multiline documents for 4 different product groups (Blitzer et al., 2007) (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) (Pang and Lee, 2004) and another of 11,640 sentences (MPQA) (Wiebe et al., 2005); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012). 4.2 Experimental settings In all our experiments, we used the FANSE parser (Tratz </context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07, pages 440–447. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structure and Semantics for Expressive Text Kernels.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th ACM international conference on Information and knowledge management, CIKM ’07,</booktitle>
<pages>861--864</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4041" citStr="Bloehdorn and Moschitti (2007)" startWordPosition="612" endWordPosition="615">es loves John beverages ≈ John drinks hot (a) ‘John likes hot beverages’ warm (b) ‘John loves warm drinks’ 775 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 775–780, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. In parallel, Collins and Duffy (2001) developed the first tree kernels to compare trees based on their topology (e. g., shared subtrees) rather than the similarity between their nodes. Culotta and Sorensen (2004) used them as Dependency Tree Kernels (DTK) to capture syntactic similarities while Bloehdorn and Moschitti (2007) and Croce et al. (2011) used them on parse trees with respectively Semantic Syntactic Tree Kernels (SSTK) and Smoothing Partial Tree Kernels (SPTK), adding node similarity based on WordNet to capture semantic similarities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al. (2003) developed graph kernels based on random walks and Srivastava et al. (2013) used them on dependency trees with Vector Tree Kernels (VTK), adding node similarity based on word embeddings from SENNA (Collobert et al., 2011) and reporting improvements over SSTK. The change from WordNe</context>
</contexts>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Stephan Bloehdorn and Alessandro Moschitti. 2007. Structure and Semantics for Expressive Text Kernels. In Proceedings of the 16th ACM international conference on Information and knowledge management, CIKM ’07, pages 861–864. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic kernels for text classification based on topological measures of feature similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th IEEE International Conference on Data Mining, ICDM ’06,</booktitle>
<pages>808--812</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="3217" citStr="Bloehdorn et al. (2006)" startWordPosition="489" endWordPosition="492">uctured as follows. Section 2 reviews the related work. Section 3 gives the detailed formulation of our kernel. Section 4 describes the experimental settings and the results we obtained on several datasets. Finally, Section 5 concludes our paper and mentions future work. 2 Related work Siolas and d’Alché Buc (2000) pioneered the idea of semantic kernels for text categorization, capitalizing on WordNet (Miller, 1995) to propose continuous word kernels based on the inverse of the path lengths in the tree rather than the common delta word kernel used so far, i.e. exact matching between unigrams. Bloehdorn et al. (2006) extended it later to other tree-based similarity measures from WordNet while Mavroeidis et al. (2005) exploited its hierarchical structure to define a Generalized Vector Space Model kernel. likes loves John beverages ≈ John drinks hot (a) ‘John likes hot beverages’ warm (b) ‘John loves warm drinks’ 775 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 775–780, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. In parallel, Collins and Duffy (2001) developed the first tree kernels to compare trees based on their</context>
</contexts>
<marker>Bloehdorn, Basili, Cammisa, Moschitti, 2006</marker>
<rawString>Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2006. Semantic kernels for text classification based on topological measures of feature similarity. In Proceedings of the 6th IEEE International Conference on Data Mining, ICDM ’06, pages 808–812. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="11333" citStr="Chang and Lin, 2011" startWordPosition="1832" endWordPosition="1835"> set and therefore requires additional labeled data. Here, we are more interested in showing that distances in a given word vector space can enhance classification in general. As for the dependency-based word embeddings proposed by Levy and Goldberg (2014), we do not think they are better suited for the problem we are tackling. As we will see in the results, we do benefit from the dependency tree structure in the phrase kernel but we still want the word kernel to be based on topical similarity rather than functional similarity. To train and test the SVM classifier, we used the LibSVM library (Chang and Lin, 2011) and employed the one-vs-one strategy for multi-class tasks. To prevent overfitting, we tuned the parameters using cross-validation on 80% of PL05 dataset (α = 5, A1 = 1 for PK since there is no need for distortion as the phrases are of the same length by definition, and A1 = A2 = 0.5 for CK) and used the same set of parameters on the remaining datasets. We performed normalization for our kernel and baselines only when it led to performance improvements on the training set (PL05, News, PL04 and MPQA). We report accuracy on the remaining 20% for PL05, on the standard test split for Twitter (25%</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14, NIPS ’01,</booktitle>
<pages>625--632</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3752" citStr="Collins and Duffy (2001)" startWordPosition="567" endWordPosition="570">ta word kernel used so far, i.e. exact matching between unigrams. Bloehdorn et al. (2006) extended it later to other tree-based similarity measures from WordNet while Mavroeidis et al. (2005) exploited its hierarchical structure to define a Generalized Vector Space Model kernel. likes loves John beverages ≈ John drinks hot (a) ‘John likes hot beverages’ warm (b) ‘John loves warm drinks’ 775 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 775–780, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. In parallel, Collins and Duffy (2001) developed the first tree kernels to compare trees based on their topology (e. g., shared subtrees) rather than the similarity between their nodes. Culotta and Sorensen (2004) used them as Dependency Tree Kernels (DTK) to capture syntactic similarities while Bloehdorn and Moschitti (2007) and Croce et al. (2011) used them on parse trees with respectively Semantic Syntactic Tree Kernels (SSTK) and Smoothing Partial Tree Kernels (SPTK), adding node similarity based on WordNet to capture semantic similarities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al.</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Advances in Neural Information Processing Systems 14, NIPS ’01, pages 625–632. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Léon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (Almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="4580" citStr="Collobert et al., 2011" startWordPosition="699" endWordPosition="702">Kernels (DTK) to capture syntactic similarities while Bloehdorn and Moschitti (2007) and Croce et al. (2011) used them on parse trees with respectively Semantic Syntactic Tree Kernels (SSTK) and Smoothing Partial Tree Kernels (SPTK), adding node similarity based on WordNet to capture semantic similarities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al. (2003) developed graph kernels based on random walks and Srivastava et al. (2013) used them on dependency trees with Vector Tree Kernels (VTK), adding node similarity based on word embeddings from SENNA (Collobert et al., 2011) and reporting improvements over SSTK. The change from WordNet to SENNA was supported by the recent progress in low-dimension Euclidean vector space representations of words that are better suited for computing distances between words. Actually, in our experiments, word2vec by Mikolov et al. (2013a) led to better results than with SENNA for both VTK and our kernels. Moreover, it possesses an additional additive compositionality property obtained from the Skip-gram training setting (Mikolov et al., 2013b), e. g., the closest word to ‘Germany’ + ‘capital’ in the vector space is found to be ‘Berl</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured Lexical Similarity via Convolution Kernels on Dependency Trees.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1034--1046</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4065" citStr="Croce et al. (2011)" startWordPosition="617" endWordPosition="620">ks hot (a) ‘John likes hot beverages’ warm (b) ‘John loves warm drinks’ 775 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 775–780, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. In parallel, Collins and Duffy (2001) developed the first tree kernels to compare trees based on their topology (e. g., shared subtrees) rather than the similarity between their nodes. Culotta and Sorensen (2004) used them as Dependency Tree Kernels (DTK) to capture syntactic similarities while Bloehdorn and Moschitti (2007) and Croce et al. (2011) used them on parse trees with respectively Semantic Syntactic Tree Kernels (SSTK) and Smoothing Partial Tree Kernels (SPTK), adding node similarity based on WordNet to capture semantic similarities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al. (2003) developed graph kernels based on random walks and Srivastava et al. (2013) used them on dependency trees with Vector Tree Kernels (VTK), adding node similarity based on word embeddings from SENNA (Collobert et al., 2011) and reporting improvements over SSTK. The change from WordNet to SENNA was supported</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured Lexical Similarity via Convolution Kernels on Dependency Trees. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1034–1046. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency Tree Kernels for Relation Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL ’04,</booktitle>
<pages>423--429</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3927" citStr="Culotta and Sorensen (2004)" startWordPosition="595" endWordPosition="598">eidis et al. (2005) exploited its hierarchical structure to define a Generalized Vector Space Model kernel. likes loves John beverages ≈ John drinks hot (a) ‘John likes hot beverages’ warm (b) ‘John loves warm drinks’ 775 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 775–780, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. In parallel, Collins and Duffy (2001) developed the first tree kernels to compare trees based on their topology (e. g., shared subtrees) rather than the similarity between their nodes. Culotta and Sorensen (2004) used them as Dependency Tree Kernels (DTK) to capture syntactic similarities while Bloehdorn and Moschitti (2007) and Croce et al. (2011) used them on parse trees with respectively Semantic Syntactic Tree Kernels (SSTK) and Smoothing Partial Tree Kernels (SPTK), adding node similarity based on WordNet to capture semantic similarities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al. (2003) developed graph kernels based on random walks and Srivastava et al. (2013) used them on dependency trees with Vector Tree Kernels (VTK), adding node similarity based o</context>
<context position="13930" citStr="Culotta and Sorensen, 2004" startWordPosition="2267" endWordPosition="2271">8 0.762* 0.801* 0.926* 0.794* co-occurrence CK+ 2 poly 0.796* 0.778 0.613 0.792* 0.917* 0.796* co-occurrence CKG 2 poly 0.801* 0.783 0.757* 0.793* 0.918* 0.794* syntactic PK 2 poly 0.796* 0.813* 0.808* 0.805* 0.927* 0.796* syntactic CK+ 2 poly 0.794* 0.780 0.741* 0.788* 0.918* 0.794* syntactic CKG 2 poly 0.797* 0.774 0.744* 0.792* 0.918* 0.794* 4.3 Results Table 1 presents results from our convolutional sentence kernel and the baseline approaches. Note again that a delta word kernel leads to the typical unigram and bigram baseline approaches (first three rows). The 3rd row corresponds to DTK (Culotta and Sorensen, 2004) and the 4th one to VTK (Srivastava et al., 2013) – the difference with our model on the 9th row lies in the function φ(·) that enumerates all random walks in the dependency tree representation following Gärtner et al. (2003) whereas we only consider the downward paths. Overall, we obtained better results than the ngram baselines, DTK and VTK, especially with syntactic phrases. VTK shows good performance across all datasets but its computation was more than 700% slower than with our kernel. Regarding the phrase kernels, PK generally produced better results than CK, implying that the semantic l</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree Kernels for Relation Extraction. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL ’04, pages 423– 429. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Gärtner</author>
<author>Peter Flach</author>
<author>Stefan Wrobel</author>
</authors>
<title>On graph kernels: Hardness results and efficient alternatives.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Conference on Computational Learning Theory, COLT ’03,</booktitle>
<pages>129--143</pages>
<contexts>
<context position="4359" citStr="Gärtner et al. (2003)" startWordPosition="663" endWordPosition="666">d Duffy (2001) developed the first tree kernels to compare trees based on their topology (e. g., shared subtrees) rather than the similarity between their nodes. Culotta and Sorensen (2004) used them as Dependency Tree Kernels (DTK) to capture syntactic similarities while Bloehdorn and Moschitti (2007) and Croce et al. (2011) used them on parse trees with respectively Semantic Syntactic Tree Kernels (SSTK) and Smoothing Partial Tree Kernels (SPTK), adding node similarity based on WordNet to capture semantic similarities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al. (2003) developed graph kernels based on random walks and Srivastava et al. (2013) used them on dependency trees with Vector Tree Kernels (VTK), adding node similarity based on word embeddings from SENNA (Collobert et al., 2011) and reporting improvements over SSTK. The change from WordNet to SENNA was supported by the recent progress in low-dimension Euclidean vector space representations of words that are better suited for computing distances between words. Actually, in our experiments, word2vec by Mikolov et al. (2013a) led to better results than with SENNA for both VTK and our kernels. Moreover, </context>
<context position="14155" citStr="Gärtner et al. (2003)" startWordPosition="2308" endWordPosition="2311">c CK+ 2 poly 0.794* 0.780 0.741* 0.788* 0.918* 0.794* syntactic CKG 2 poly 0.797* 0.774 0.744* 0.792* 0.918* 0.794* 4.3 Results Table 1 presents results from our convolutional sentence kernel and the baseline approaches. Note again that a delta word kernel leads to the typical unigram and bigram baseline approaches (first three rows). The 3rd row corresponds to DTK (Culotta and Sorensen, 2004) and the 4th one to VTK (Srivastava et al., 2013) – the difference with our model on the 9th row lies in the function φ(·) that enumerates all random walks in the dependency tree representation following Gärtner et al. (2003) whereas we only consider the downward paths. Overall, we obtained better results than the ngram baselines, DTK and VTK, especially with syntactic phrases. VTK shows good performance across all datasets but its computation was more than 700% slower than with our kernel. Regarding the phrase kernels, PK generally produced better results than CK, implying that the semantic linearity and ontological relation encoded in the embedding is not sufficient enough and treating them separately is more beneficial. However, we believe CK has more room for improvement with the use of more accurate phrase em</context>
</contexts>
<marker>Gärtner, Flach, Wrobel, 2003</marker>
<rawString>Thomas Gärtner, Peter Flach, and Stefan Wrobel. 2003. On graph kernels: Hardness results and efficient alternatives. In Proceedings of the Annual Conference on Computational Learning Theory, COLT ’03, pages 129–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with Support Vector Machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning, ECML ’98,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="862" citStr="Joachims, 1998" startWordPosition="120" endWordPosition="121">nce kernel based on word embeddings. Our kernel overcomes the sparsity issue that arises when classifying short documents or in case of little training data. Experiments on six sentence datasets showed statistically significant higher accuracy over the standard linear kernel with ngram features and other proposed models. 1 Introduction With the proliferation of text data available online, text categorization emerged as a prominent research topic. Traditionally, words (unigrams) and phrases (n-grams) have been considered as document features and subsequently fed to a classifier such as an SVM (Joachims, 1998). In the SVM dual formulation that relies on kernels, i. e. similarity measures between documents, a linear kernel can be interpreted as the number of exact matching n-grams between two documents. Consequently, for short documents or when little training data is available, sparsity issues due to word synonymy arise, e. g., the sentences ‘John likes hot beverages’ and ‘John loves warm drinks’ have little overlap and therefore low linear kernel value (only 1) in the n-gram feature space, even with dependency tree representations and downward paths for n-grams as illustrated in Figure 1. We propo</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with Support Vector Machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, ECML ’98, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kenter</author>
<author>Maarten de Rijke</author>
</authors>
<title>Short text similarity with word embeddings.</title>
<date>2015</date>
<booktitle>In Proceedings of the 24th ACM international conference on Information and knowledge management, CIKM ’15.</booktitle>
<publisher>ACM.</publisher>
<marker>Kenter, de Rijke, 2015</marker>
<rawString>Tom Kenter and Maarten de Rijke. 2015. Short text similarity with word embeddings. In Proceedings of the 24th ACM international conference on Information and knowledge management, CIKM ’15. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le and Tomas Mikolov</author>
</authors>
<title>Distributed Representations of Sentences and Documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning,</booktitle>
<volume>32</volume>
<pages>1188--1196</pages>
<contexts>
<context position="14807" citStr="Mikolov (2014)" startWordPosition="2416" endWordPosition="2417"> paths. Overall, we obtained better results than the ngram baselines, DTK and VTK, especially with syntactic phrases. VTK shows good performance across all datasets but its computation was more than 700% slower than with our kernel. Regarding the phrase kernels, PK generally produced better results than CK, implying that the semantic linearity and ontological relation encoded in the embedding is not sufficient enough and treating them separately is more beneficial. However, we believe CK has more room for improvement with the use of more accurate phrase embeddings such as the ones from Le and Mikolov (2014), Yin and SchUtze (2014) and Yu and Dredze (2015). There was little contribution to the accuracy from non-unigram features, indicating that large part of the performance improvement is credited to the word embedding resolving the sparsity issue. Accuracy 0.94 0.86 0.74 0 1000 2000 3000 4000 5000 6000 7000 8000 Number of training examples 0.90 0.82 0.78 PK (poly) PK (delta) Figure 2: Test accuracy vs. number of training examples for our kernel and the bigram baseline. This can be well observed with the following experiment on the number of training examples. Figure 2 shows the accuracy on the s</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of ICML ’14, pages 1188–1196. JMLR Workshop and Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>DependencyBased Word Embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>302--308</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="10969" citStr="Levy and Goldberg (2014)" startWordPosition="1766" endWordPosition="1769">ution – re-training the neural network to obtain task-specific embeddings requires a certain amount of training data, admittedly unlabeled, but still not optimal under our scenario with short documents and little task-specific training data available. Moreover, tuning the hyperparameters to maximize the classification accuracy needs to be carried out on a validation set and therefore requires additional labeled data. Here, we are more interested in showing that distances in a given word vector space can enhance classification in general. As for the dependency-based word embeddings proposed by Levy and Goldberg (2014), we do not think they are better suited for the problem we are tackling. As we will see in the results, we do benefit from the dependency tree structure in the phrase kernel but we still want the word kernel to be based on topical similarity rather than functional similarity. To train and test the SVM classifier, we used the LibSVM library (Chang and Lin, 2011) and employed the one-vs-one strategy for multi-class tasks. To prevent overfitting, we tuned the parameters using cross-validation on 80% of PL05 dataset (α = 5, A1 = 1 for PK since there is no need for distortion as the phrases are of</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. DependencyBased Word Embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 2 of ACL ’14, pages 302–308. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
<author>Ido Dagan</author>
</authors>
<title>Improving distributional similarity with lessons learned from word embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--211</pages>
<contexts>
<context position="10242" citStr="Levy et al., 2015" startWordPosition="1654" endWordPosition="1657"> spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012). 4.2 Experimental settings In all our experiments, we used the FANSE parser (Tratz and Hovy, 2011) to generate dependency trees and the pre-trained version of word2vec1, a 300 dimensional representation of 3 million English words trained over a Google News dataset 1https://code.google.com/p/word2vec of 100 billion words using the Skip-gram model and a context size of 5. While fine-tuning the embeddings to a specific task or on a given dataset may improve the result for that particular task or dataset (Levy et al., 2015), it makes the expected results less generalizable and the method harder to use as an off-the-shelf solution – re-training the neural network to obtain task-specific embeddings requires a certain amount of training data, admittedly unlabeled, but still not optimal under our scenario with short documents and little task-specific training data available. Moreover, tuning the hyperparameters to maximize the classification accuracy needs to be carried out on a validation set and therefore requires additional labeled data. Here, we are more interested in showing that distances in a given word vecto</context>
</contexts>
<marker>Levy, Goldberg, Dagan, 2015</marker>
<rawString>Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Mavroeidis</author>
<author>George Tsatsaronis</author>
<author>Michalis Vazirgiannis</author>
<author>Martin Theobald</author>
<author>Gerhard Weikum</author>
</authors>
<title>Word Sense Disambiguation for Exploiting Hierarchical Thesauri in Text Classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th European conference on Principles and Practice of Knowledge Discovery in Databases, ECML PKDD ’05,</booktitle>
<pages>181--192</pages>
<publisher>Springer-Verlag</publisher>
<location>Berlin.</location>
<contexts>
<context position="3319" citStr="Mavroeidis et al. (2005)" startWordPosition="505" endWordPosition="508">ur kernel. Section 4 describes the experimental settings and the results we obtained on several datasets. Finally, Section 5 concludes our paper and mentions future work. 2 Related work Siolas and d’Alché Buc (2000) pioneered the idea of semantic kernels for text categorization, capitalizing on WordNet (Miller, 1995) to propose continuous word kernels based on the inverse of the path lengths in the tree rather than the common delta word kernel used so far, i.e. exact matching between unigrams. Bloehdorn et al. (2006) extended it later to other tree-based similarity measures from WordNet while Mavroeidis et al. (2005) exploited its hierarchical structure to define a Generalized Vector Space Model kernel. likes loves John beverages ≈ John drinks hot (a) ‘John likes hot beverages’ warm (b) ‘John loves warm drinks’ 775 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 775–780, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. In parallel, Collins and Duffy (2001) developed the first tree kernels to compare trees based on their topology (e. g., shared subtrees) rather than the similarity between their nodes. Culotta and Sorense</context>
</contexts>
<marker>Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald, Weikum, 2005</marker>
<rawString>Dimitrios Mavroeidis, George Tsatsaronis, Michalis Vazirgiannis, Martin Theobald, and Gerhard Weikum. 2005. Word Sense Disambiguation for Exploiting Hierarchical Thesauri in Text Classification. In Proceedings of the 9th European conference on Principles and Practice of Knowledge Discovery in Databases, ECML PKDD ’05, pages 181–192. Springer-Verlag Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at International Conference on Learning Representations, ICLR ’13.</booktitle>
<contexts>
<context position="4878" citStr="Mikolov et al. (2013" startWordPosition="746" endWordPosition="749">ities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al. (2003) developed graph kernels based on random walks and Srivastava et al. (2013) used them on dependency trees with Vector Tree Kernels (VTK), adding node similarity based on word embeddings from SENNA (Collobert et al., 2011) and reporting improvements over SSTK. The change from WordNet to SENNA was supported by the recent progress in low-dimension Euclidean vector space representations of words that are better suited for computing distances between words. Actually, in our experiments, word2vec by Mikolov et al. (2013a) led to better results than with SENNA for both VTK and our kernels. Moreover, it possesses an additional additive compositionality property obtained from the Skip-gram training setting (Mikolov et al., 2013b), e. g., the closest word to ‘Germany’ + ‘capital’ in the vector space is found to be ‘Berlin’. More recently, for short text similarity, Song and Roth (2015) and Kenter and de Rijke (2015) proposed additional semantic meta-features based on word embeddings to enhance classification. 3 Formulation We denote the embedding of a word w by w. 3.1 Word Kernel (WK) We define a kernel between </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at International Conference on Learning Representations, ICLR ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26, NIPS ’13,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="4878" citStr="Mikolov et al. (2013" startWordPosition="746" endWordPosition="749">ities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al. (2003) developed graph kernels based on random walks and Srivastava et al. (2013) used them on dependency trees with Vector Tree Kernels (VTK), adding node similarity based on word embeddings from SENNA (Collobert et al., 2011) and reporting improvements over SSTK. The change from WordNet to SENNA was supported by the recent progress in low-dimension Euclidean vector space representations of words that are better suited for computing distances between words. Actually, in our experiments, word2vec by Mikolov et al. (2013a) led to better results than with SENNA for both VTK and our kernels. Moreover, it possesses an additional additive compositionality property obtained from the Skip-gram training setting (Mikolov et al., 2013b), e. g., the closest word to ‘Germany’ + ‘capital’ in the vector space is found to be ‘Berlin’. More recently, for short text similarity, Song and Roth (2015) and Kenter and de Rijke (2015) proposed additional semantic meta-features based on word embeddings to enhance classification. 3 Formulation We denote the embedding of a word w by w. 3.1 Word Kernel (WK) We define a kernel between </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, NIPS ’13, pages 3111–3119. Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3013" citStr="Miller, 1995" startWordPosition="455" endWordPosition="456">rom a given word vector space can easily be incorporated in the standard kernel between documents for higher effectiveness and little additional cost in efficiency. The rest of this paper is structured as follows. Section 2 reviews the related work. Section 3 gives the detailed formulation of our kernel. Section 4 describes the experimental settings and the results we obtained on several datasets. Finally, Section 5 concludes our paper and mentions future work. 2 Related work Siolas and d’Alché Buc (2000) pioneered the idea of semantic kernels for text categorization, capitalizing on WordNet (Miller, 1995) to propose continuous word kernels based on the inverse of the path lengths in the tree rather than the common delta word kernel used so far, i.e. exact matching between unigrams. Bloehdorn et al. (2006) extended it later to other tree-based similarity measures from WordNet while Mavroeidis et al. (2005) exploited its hierarchical structure to define a Generalized Vector Space Model kernel. likes loves John beverages ≈ John drinks hot (a) ‘John likes hot beverages’ warm (b) ‘John loves warm drinks’ 775 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pag</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Zornitsa Kozareva</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation, SemEval-2013.</booktitle>
<contexts>
<context position="9439" citStr="Nakov et al., 2013" startWordPosition="1525" endWordPosition="1528">ts We evaluated our kernel with co-occurrence and syntactic phrases on several standard text categorization tasks. 4.1 Datasets We considered four tasks: (1) binary sentiment analysis with a movie review dataset of 10,662 sentences (PL05) (Pang and Lee, 2005) and a product review dataset (Amazon) of 2,000 multiline documents for 4 different product groups (Blitzer et al., 2007) (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) (Pang and Lee, 2004) and another of 11,640 sentences (MPQA) (Wiebe et al., 2005); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012). 4.2 Experimental settings In all our experiments, we used the FANSE parser (Tratz and Hovy, 2011) to generate dependency trees and the pre-trained version of word2vec1, a 300 dimensional representation of 3 million English words trained over a Google News dataset 1https://code.google.com/p/word2vec of 100 billion words </context>
</contexts>
<marker>Nakov, Kozareva, Ritter, Rosenthal, Stoyanov, Wilson, 2013</marker>
<rawString>Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation, SemEval-2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<pages>271--278</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9537" citStr="Pang and Lee, 2004" startWordPosition="1540" endWordPosition="1543">orization tasks. 4.1 Datasets We considered four tasks: (1) binary sentiment analysis with a movie review dataset of 10,662 sentences (PL05) (Pang and Lee, 2005) and a product review dataset (Amazon) of 2,000 multiline documents for 4 different product groups (Blitzer et al., 2007) (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) (Pang and Lee, 2004) and another of 11,640 sentences (MPQA) (Wiebe et al., 2005); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012). 4.2 Experimental settings In all our experiments, we used the FANSE parser (Tratz and Hovy, 2011) to generate dependency trees and the pre-trained version of word2vec1, a 300 dimensional representation of 3 million English words trained over a Google News dataset 1https://code.google.com/p/word2vec of 100 billion words using the Skip-gram model and a context size of 5. While fine-tuning the embeddings to a specific </context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lilian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, pages 271–278. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>115--124</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9079" citStr="Pang and Lee, 2005" startWordPosition="1468" endWordPosition="1471">al. (2003) to avoid duplicate computations. 3.4 Document Kernel Finally, we sum sentence kernel values for all pairs of sentences between two documents to get the document kernel. Once we have obtained all document kernel values Kij between documents i and j, we may normalize them by VlKiiKjj as the length of input documents might not be uniform. 4 Experiments We evaluated our kernel with co-occurrence and syntactic phrases on several standard text categorization tasks. 4.1 Datasets We considered four tasks: (1) binary sentiment analysis with a movie review dataset of 10,662 sentences (PL05) (Pang and Lee, 2005) and a product review dataset (Amazon) of 2,000 multiline documents for 4 different product groups (Blitzer et al., 2007) (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) (Pang and Lee, 2004) and another of 11,640 sentences (MPQA) (Wiebe et al., 2005); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lilian Lee. 2005. Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 115–124. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georges Siolas</author>
<author>Florence d’Alché Buc</author>
</authors>
<title>Support Vector Machines Based on a Semantic Kernel for Text Categorization.</title>
<date>2000</date>
<booktitle>In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks,</booktitle>
<volume>5</volume>
<pages>205--209</pages>
<publisher>IEEE Computer Society.</publisher>
<marker>Siolas, Buc, 2000</marker>
<rawString>Georges Siolas and Florence d’Alché Buc. 2000. Support Vector Machines Based on a Semantic Kernel for Text Categorization. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks, volume 5 of IJCNN ’00, pages 205–209. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangqiu Song</author>
<author>Dan Roth</author>
</authors>
<title>Unsupervised Sparse Vector Densification for Short Text Similarity.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-Short ’15,</booktitle>
<pages>1275--1280</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="5247" citStr="Song and Roth (2015)" startWordPosition="806" endWordPosition="809">ge from WordNet to SENNA was supported by the recent progress in low-dimension Euclidean vector space representations of words that are better suited for computing distances between words. Actually, in our experiments, word2vec by Mikolov et al. (2013a) led to better results than with SENNA for both VTK and our kernels. Moreover, it possesses an additional additive compositionality property obtained from the Skip-gram training setting (Mikolov et al., 2013b), e. g., the closest word to ‘Germany’ + ‘capital’ in the vector space is found to be ‘Berlin’. More recently, for short text similarity, Song and Roth (2015) and Kenter and de Rijke (2015) proposed additional semantic meta-features based on word embeddings to enhance classification. 3 Formulation We denote the embedding of a word w by w. 3.1 Word Kernel (WK) We define a kernel between two words as a polynomial kernel over a cosine similarity in the word embedding space: �1 (��α 1 + �w1, w2� WK(w1, w2) = (1) 2 �w1��w2� where α is a scaling factor. We also tried Gaussian, Laplacian and sigmoid kernels but they led to poorer results in our experiments. Note that a delta word kernel, i. e. the Dirac function ✶,,,1=,,,2, leads to a document kernel corr</context>
</contexts>
<marker>Song, Roth, 2015</marker>
<rawString>Yangqiu Song and Dan Roth. 2015. Unsupervised Sparse Vector Densification for Short Text Similarity. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-Short ’15, pages 1275–1280. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shashank Srivastava</author>
<author>Dirk Hovy</author>
<author>Eduard H Hovy</author>
</authors>
<title>A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP ’13,</booktitle>
<pages>1411--1416</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4434" citStr="Srivastava et al. (2013)" startWordPosition="675" endWordPosition="678">n their topology (e. g., shared subtrees) rather than the similarity between their nodes. Culotta and Sorensen (2004) used them as Dependency Tree Kernels (DTK) to capture syntactic similarities while Bloehdorn and Moschitti (2007) and Croce et al. (2011) used them on parse trees with respectively Semantic Syntactic Tree Kernels (SSTK) and Smoothing Partial Tree Kernels (SPTK), adding node similarity based on WordNet to capture semantic similarities but limiting to comparisons between words of the same POS tag. Similarly, Gärtner et al. (2003) developed graph kernels based on random walks and Srivastava et al. (2013) used them on dependency trees with Vector Tree Kernels (VTK), adding node similarity based on word embeddings from SENNA (Collobert et al., 2011) and reporting improvements over SSTK. The change from WordNet to SENNA was supported by the recent progress in low-dimension Euclidean vector space representations of words that are better suited for computing distances between words. Actually, in our experiments, word2vec by Mikolov et al. (2013a) led to better results than with SENNA for both VTK and our kernels. Moreover, it possesses an additional additive compositionality property obtained from</context>
<context position="13979" citStr="Srivastava et al., 2013" startWordPosition="2278" endWordPosition="2281">ly 0.796* 0.778 0.613 0.792* 0.917* 0.796* co-occurrence CKG 2 poly 0.801* 0.783 0.757* 0.793* 0.918* 0.794* syntactic PK 2 poly 0.796* 0.813* 0.808* 0.805* 0.927* 0.796* syntactic CK+ 2 poly 0.794* 0.780 0.741* 0.788* 0.918* 0.794* syntactic CKG 2 poly 0.797* 0.774 0.744* 0.792* 0.918* 0.794* 4.3 Results Table 1 presents results from our convolutional sentence kernel and the baseline approaches. Note again that a delta word kernel leads to the typical unigram and bigram baseline approaches (first three rows). The 3rd row corresponds to DTK (Culotta and Sorensen, 2004) and the 4th one to VTK (Srivastava et al., 2013) – the difference with our model on the 9th row lies in the function φ(·) that enumerates all random walks in the dependency tree representation following Gärtner et al. (2003) whereas we only consider the downward paths. Overall, we obtained better results than the ngram baselines, DTK and VTK, especially with syntactic phrases. VTK shows good performance across all datasets but its computation was more than 700% slower than with our kernel. Regarding the phrase kernels, PK generally produced better results than CK, implying that the semantic linearity and ontological relation encoded in the </context>
</contexts>
<marker>Srivastava, Hovy, Hovy, 2013</marker>
<rawString>Shashank Srivastava, Dirk Hovy, and Eduard H. Hovy. 2013. A Walk-Based Semantically Enriched Tree Kernel Over Distributed Word Representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP ’13, pages 1411–1416. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard H Hovy</author>
</authors>
<title>A Fast, Accurate, Non-projective, Semantically-enriched Parser.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1257--1268</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="9815" citStr="Tratz and Hovy, 2011" startWordPosition="1585" endWordPosition="1588"> 2007) (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) (Pang and Lee, 2004) and another of 11,640 sentences (MPQA) (Wiebe et al., 2005); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012). 4.2 Experimental settings In all our experiments, we used the FANSE parser (Tratz and Hovy, 2011) to generate dependency trees and the pre-trained version of word2vec1, a 300 dimensional representation of 3 million English words trained over a Google News dataset 1https://code.google.com/p/word2vec of 100 billion words using the Skip-gram model and a context size of 5. While fine-tuning the embeddings to a specific task or on a given dataset may improve the result for that particular task or dataset (Levy et al., 2015), it makes the expected results less generalizable and the method harder to use as an off-the-shelf solution – re-training the neural network to obtain task-specific embeddi</context>
</contexts>
<marker>Tratz, Hovy, 2011</marker>
<rawString>Stephen Tratz and Eduard H. Hovy. 2011. A Fast, Accurate, Non-projective, Semantically-enriched Parser. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1257–1268. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Vitale</author>
<author>Paolo Ferragina</author>
<author>Ugo Scaiella</author>
</authors>
<title>Classification of Short Texts by Deploying Topical Annotations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 34th European Conference on Information Retrieval, ECIR’12,</booktitle>
<pages>376--387</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="9716" citStr="Vitale et al., 2012" startWordPosition="1569" endWordPosition="1572">view dataset (Amazon) of 2,000 multiline documents for 4 different product groups (Blitzer et al., 2007) (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) (Pang and Lee, 2004) and another of 11,640 sentences (MPQA) (Wiebe et al., 2005); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012). 4.2 Experimental settings In all our experiments, we used the FANSE parser (Tratz and Hovy, 2011) to generate dependency trees and the pre-trained version of word2vec1, a 300 dimensional representation of 3 million English words trained over a Google News dataset 1https://code.google.com/p/word2vec of 100 billion words using the Skip-gram model and a context size of 5. While fine-tuning the embeddings to a specific task or on a given dataset may improve the result for that particular task or dataset (Levy et al., 2015), it makes the expected results less generalizable and the method harder t</context>
</contexts>
<marker>Vitale, Ferragina, Scaiella, 2012</marker>
<rawString>Daniele Vitale, Paolo Ferragina, and Ugo Scaiella. 2012. Classification of Short Texts by Deploying Topical Annotations. In Proceedings of the 34th European Conference on Information Retrieval, ECIR’12, pages 376–387. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="9597" citStr="Wiebe et al., 2005" startWordPosition="1550" endWordPosition="1553">binary sentiment analysis with a movie review dataset of 10,662 sentences (PL05) (Pang and Lee, 2005) and a product review dataset (Amazon) of 2,000 multiline documents for 4 different product groups (Blitzer et al., 2007) (we will report the average effectiveness over the 4 sub-collections); (2) ternary sentiment analysis with the SemEval 2013 Task B dataset (Twitter) containing 12,348 tweets classified as positive, neutral or negative (Nakov et al., 2013); (3) binary subjectivity detection with a dataset of 10,000 sentences (PL04) (Pang and Lee, 2004) and another of 11,640 sentences (MPQA) (Wiebe et al., 2005); and (4) seven-class topic spotting with a news dataset (News) of 32,602 one-line news summaries (Vitale et al., 2012). 4.2 Experimental settings In all our experiments, we used the FANSE parser (Tratz and Hovy, 2011) to generate dependency trees and the pre-trained version of word2vec1, a 300 dimensional representation of 3 million English words trained over a Google News dataset 1https://code.google.com/p/word2vec of 100 billion words using the Skip-gram model and a context size of 5. While fine-tuning the embeddings to a specific task or on a given dataset may improve the result for that p</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce M. Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A Re-examination of Text Categorization Methods.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99,</booktitle>
<pages>42--49</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12462" citStr="Yang and Liu, 1999" startWordPosition="2026" endWordPosition="2029">report accuracy on the remaining 20% for PL05, on the standard test split for Twitter (25%) and News (50%) and from 5-fold cross-validation for the other datasets (Amazon, PL04 and MPQA). We only report accuracy as the macro-average F1- scores led to similar conclusions (and except for Twitter and News, the class label distributions are balanced). Results for phrase lengths longer than two were omitted since they were marginally different at best. Statistical significance of improvement over the bigram baseline with the same phrase definition was assessed using the micro sign test (p &lt; 0.01) (Yang and Liu, 1999). 777 Table 1: Accuracy results on the test set for PL05 (20%), standard test split for Twitter (25%) and News (50%) and from 5-fold CV for the other datasets (Amazon, PL04 and MPQA). Bold font marks the best performance in the column. * indicates statistical significance at p &lt; 0.01 using micro sign test against the bigram baseline (delta word kernel) of the same column and with the same phrase definition. phrase phrase phrase word PL05 Amazon Twitter News PL04 MPQA definition kernel length kernel co-occurrence PK 1 delta 0.742 0.768 0.623 0.769 0.904 0.754 co-occurrence PK 2 delta 0.739 0.76</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A Re-examination of Text Categorization Methods. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’99, pages 42–49. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenpeng Yin</author>
<author>Hinrich Schütze</author>
</authors>
<title>An Exploration of Embeddings for Generalized Phrases.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL Student Research Workshop, ACLstudent ’14,</booktitle>
<pages>41--47</pages>
<publisher>ACL.</publisher>
<marker>Yin, Schütze, 2014</marker>
<rawString>Wenpeng Yin and Hinrich Schütze. 2014. An Exploration of Embeddings for Generalized Phrases. In Proceedings of the ACL Student Research Workshop, ACLstudent ’14, pages 41–47. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mo Yu</author>
<author>Mark Dredze</author>
</authors>
<title>Learning Composition Models for Phrase Embeddings.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--227</pages>
<contexts>
<context position="14856" citStr="Yu and Dredze (2015)" startWordPosition="2423" endWordPosition="2426"> than the ngram baselines, DTK and VTK, especially with syntactic phrases. VTK shows good performance across all datasets but its computation was more than 700% slower than with our kernel. Regarding the phrase kernels, PK generally produced better results than CK, implying that the semantic linearity and ontological relation encoded in the embedding is not sufficient enough and treating them separately is more beneficial. However, we believe CK has more room for improvement with the use of more accurate phrase embeddings such as the ones from Le and Mikolov (2014), Yin and SchUtze (2014) and Yu and Dredze (2015). There was little contribution to the accuracy from non-unigram features, indicating that large part of the performance improvement is credited to the word embedding resolving the sparsity issue. Accuracy 0.94 0.86 0.74 0 1000 2000 3000 4000 5000 6000 7000 8000 Number of training examples 0.90 0.82 0.78 PK (poly) PK (delta) Figure 2: Test accuracy vs. number of training examples for our kernel and the bigram baseline. This can be well observed with the following experiment on the number of training examples. Figure 2 shows the accuracy on the same test set (20% of the dataset) when the learni</context>
</contexts>
<marker>Yu, Dredze, 2015</marker>
<rawString>Mo Yu and Mark Dredze. 2015. Learning Composition Models for Phrase Embeddings. Transactions of the Association for Computational Linguistics, 3:227–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel Methods for Relation Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="7728" citStr="Zelenko et al. (2003)" startWordPosition="1239" endWordPosition="1242"> and Lapata, 2012): vector addition (+) and elementwise multiplication (0). Then we define the Composition Kernel (CK) between phrases as: CK(p1,p2) = WK(p1,p2) (3) where pj, the embedding of the phrase pj, can be obtained either by addition (pj _ ElZ=1 wjZ) or by element-wise multiplication (pj _ C)lZ=1 wjZ) of its word embeddings. For CK, we do not require the two phrases to be of the same length so the kernel has a desirable property of being able to compare ‘Berlin’ with ‘capital of Germany’ for instance. 3.3 Sentence Kernel (SK) We can then formulate a sentence kernel in a similar way to Zelenko et al. (2003). It is defined through convolution as the sum of all local phrasal similarities, i. e. kernel values between phrases contained in the sentences: �SK(s1, s2) = λ1E λ2&apos;7 PhK(p1,p2) (4) p1∈O(s1), p2∈O(s2) 776 where O(sk) is the set of either statistical or syntactic phrases (or set of random walks for VTK) in sentence sk, A1 is a decaying factor penalizing longer phrases, c = max{|p1|, |p2|} is the maximum length of the two phrases, A2 is a distortion parameter controlling the length difference q between the two phrases (q = ||p1 |− |p2||) and PhK is a phrase kernel, either PK, CK+ or CKO. Since</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel Methods for Relation Extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
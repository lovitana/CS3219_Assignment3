<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002177">
<title confidence="0.9553535">
LDTM: A Latent Document Type Model for Cumulative Citation
Recommendation
</title>
<author confidence="0.999391">
Jingang Wang1∗, Dandan Song1†, Zhiwei Zhang2, Lejian Liao1, Luo Si2, Chin-Yew Lin3
</author>
<affiliation confidence="0.960562">
1School of Computer Science, Beijing Institute of Technology
2Dept. of Computer Science, Purdue University
3Knowledge Mining Group, Microsoft Research
</affiliation>
<email confidence="0.977715">
{bitwjg, sdd, liaolj}@bit.edu.cn
{zhan1187, lsi}@purdue.edu
cyl@microsoft.com
</email>
<sectionHeader confidence="0.993799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936608695652">
This paper studies Cumulative Citation
Recommendation (CCR) - given an entity
in Knowledge Bases, how to effectively
detect its potential citations from volume
text streams. Most previous approaches
treated all kinds of features indifferently to
build a global relevance model, in which
the prior knowledge embedded in docu-
ments cannot be exploited adequately. To
address this problem, we propose a la-
tent document type discriminative model
by introducing a latent layer to capture the
correlations between documents and their
underlying types. The model can better
adjust to different types of documents and
yield flexible performance when dealing
with a broad range of document types. An
extensive set of experiments has been con-
ducted on TREC-KBA-2013 dataset, and
the results demonstrate that this model can
yield a significant performance gain in rec-
ommendation quality as compared to the
state-of-the-art.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9991515">
Knowledge Bases (KBs), like Wikipedia, are
playing increasingly important roles in numerous
entity-based information retrieval tasks. Neverthe-
less, most KBs are hard to be up-to-date due to
their manual maintenances by human editors. As
reported in (Frank et al., 2012), there exists a me-
dian time lag of 356 days between the day a news
article is published and the time that the news is
cited in a Wikipedia article dedicated to the entity
concerned by the news. The time lag would be
reduced if relevant documents could be automati-
cally detected as soon as they are published online
</bodyText>
<footnote confidence="0.46268">
∗This work was partially performed when the first author
was visiting Purdue University and Microsoft Research Asia.
† Corresponding Author
</footnote>
<bodyText confidence="0.999668024390244">
and then recommended to the editors. This task is
studied as Cumulative Citation Recommendation
(CCR). Formally, given a set of KB entities, CCR
is to filter relevant documents from a stream cor-
pus and evaluate their citation-worthiness to the
target entities.
A variety of supervised approaches (e.g., clas-
sification, learning to rank) have been employed
and achieved promising results (Wang et al., 2013;
Balog and Ramampiaro, 2013; Balog et al., 2013).
Nevertheless, most of them leverage all features
indiscriminately to build a global relevance model,
which leads to unsatisfactory performance. The
documents can offer some prior knowledge, which
is named as type in this paper. The type is the prior
knowledge embedded in the document that im-
pacts on the probability of its being recommended
to KBs. For instance, when dealing with a docu-
ment on “music” topic, we would like to have less
weights put on a politician entity because this doc-
ument is not likely to related to it, but more often
related to musicians or musical bands. Besides,
the source of a document impacts on the recom-
mendation strategies too. A document from news
agencies is more reliable and citable than the one
from social websites even if they state an identi-
cal story about the target KB entity. Hence we
consider two kinds of document features to model
the prior type knowledge: (1) topic-based features,
and (2) source-based features.
This paper proposes a latent document type dis-
criminative mixture model for CCR. We introduce
an intermediate latent layer to model latent docu-
ment types and define a joint distribution over the
document-entity pairs and latent document-types
on the observation data. The aim is to achieve a
discriminative mixture model that is expected to
outperform the global relevance model.
To the best of our knowledge, this is the first
research work that leverages prior knowledge em-
bedded in documents to improve CCR perfor-
</bodyText>
<page confidence="0.968412">
561
</page>
<note confidence="0.6583355">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 561–566,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999158666666667">
mance. An extensive set of experiments conducted
on TREC-KBA-2013 dataset has demonstrated the
effectiveness of the proposed mixture model.
</bodyText>
<sectionHeader confidence="0.967298" genericHeader="method">
2 Discriminative Models for CCR
</sectionHeader>
<bodyText confidence="0.956903384615385">
Given a set of KB entities E = {eu|u =
1, · · · , M} and a document collection D =
{dv|v = 1, · · · , N}, our objective is to es-
timate the conditional probability of relevance
P (r|e, d) with respect to an entity-document pair
(e, d). Each (e, d) is represented as a feature
vector f(e, d) = (f1(e, d), · · · , fK(e, d)), where
K is the dimension of the entity-document fea-
ture vector. Moreover, to model the hidden
document type, each document is represented
as an document-type feature vector g(d) =
(g1(d), · · · , gL(d)), where L indicates the dimen-
sion of the document-type feature vector.
</bodyText>
<subsectionHeader confidence="0.691942">
2.1 Global Model
</subsectionHeader>
<bodyText confidence="0.998156526315789">
This paper utilizes logistic regression to estimate
the conditional probability P(r|e, d), where r(r ∈
{1, −1}) is a binary label indicating the relevance
of an entity-document pair (e, d). The value of r is
1 if the document d is related to the entity e, oth-
erwise r = −1. Formally, the parametric form
of P(r=1|e,d) is expressed as P(r=1|e,d) =
S(PKi=1wifi(e, d)), where S(x) is the standard lo-
gistic function, wi is the combination parame-
ter for the ith feature. It is easy to derive that
for different values of r, the only difference in
P (r|e, d) is the sign within the logistic function.
Therefore, we adopt the general representation of
P(r|e, d)=S(r PKi=1 wifi(e, d)). This model is
denoted as GM in this paper. Several previous ap-
proaches can be deemed as global models adopt-
ing different classification functions such as deci-
sion trees (Wang et al., 2013) and Support Vector
Machine (SVM) (Bonnefoy et al., 2013).
</bodyText>
<subsectionHeader confidence="0.999031">
2.2 Latent Document Type Model
</subsectionHeader>
<bodyText confidence="0.999623193548387">
In GM, a fixed set of combination weights (i.e.,
w) are learned to optimize the overall performance
for all entity-document pairs. However, the best
combination strategy for a given pair is not al-
ways the best for the others since both the docu-
ments and entities are heterogeneous. Therefore,
we may benefit from developing a document type
dependent model in which we choose the com-
bination strategy individually for each document
type to optimize the performance for specific doc-
ument types. Since it is not feasible to determine
the proper combination strategy for each docu-
ment type, we need to classify documents into one
of several types. The combination strategy is then
tuned to optimize average performance for docu-
ments within the same type.
We propose a latent document type model
(LDTM) by introducing an intermediate layer
to capture the underlying type information in
documents. A latent variable z is utilized to
indicate which type the combination weights wz
are drawn from. The choice of z is determined
by the document d. The joint probability of
relevance r and the latent variable z is represented
as P(r, z|e, d; α, w)=P(z|d; α)P(r|e, d, z; w),
where P(z|d; α) is the mixing coefficient, denot-
ing the probability of choosing the hidden type
z given document d, and α is the corresponding
parameter. P(r|e, d, z; w) denotes the discrimi-
native component which takes a logistic function.
By marginalizing out z, we obtain
</bodyText>
<equation confidence="0.9977355">
P(r|e, d; α, w) =
/ K (1)
P(z|d; α)S I r X wzifi(e, d)�
i=1
</equation>
<bodyText confidence="0.999974222222222">
where wzi is the weight for the ith entry in the
feature vector under the hidden variable z. We
adopt a soft-max function Zd1exp(PL j=1 αzjgj(d))
to model P(z|d; α), and Zd is the normalization
factor that scaled the exponential function to be
a probability distribution. In this representation,
each document d is denoted by a bag of document
type features (g1(d), · · · , gL(d)). By plugging the
soft-max function into Equation (1), we have
</bodyText>
<equation confidence="0.9983875">
P(r|e, d; α, w)=
2 z
exp Xαzjgj (d) S rXwzifi(e, d) ( )
j=1 i=1
</equation>
<bodyText confidence="0.999507285714286">
Suppose entity-document pairs in training set are
represented as T ={(du, ev)}, and R={ruv} de-
notes the corresponding relevance judgment of
(du, ev), where u = 1,··· ,M and v = 1,··· ,N.
Assume training instances in T are independently
generated, the conditional likelihood of training
data is written as
</bodyText>
<equation confidence="0.970540428571429">
P(ruv|eu, dv) (3)
XNz
z
1
Zd
XNz
z=1
N
Y
v=1
P(R|T ) =
M
Y
u=1
</equation>
<page confidence="0.983858">
562
</page>
<subsectionHeader confidence="0.992446">
2.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.997624">
The parameters (i.e., ω and α) can be estimated
by maximizing the data log-likelihood L(ω, α),
which is the form of logarithm of Equation (3).
A typical parameter estimation method is to use
Expectation-Maximization (EM) algorithm by it-
erating E-step and M-step continuously until con-
vergence. The E-step is derived by computing the
posterior probability of z given du and ev, which
is denoted as P(z|du, ev).
</bodyText>
<equation confidence="0.991213714285714">
P(z|du, ev) =
�PLz � � �
PK
exp j=1αzjgj(du) δ ruv i=1ωzifi(du, ev)
� �
P �PLz �
PK
</equation>
<bodyText confidence="0.854578333333333">
zexp j=1αzjgj(du) δ ruv i=1ωzifi(du, ev) (4)
In M-step, we can obtain the following parame-
ter update rules.
</bodyText>
<equation confidence="0.994039272727273">
ω∗ =
z
� � XK ��
P(z|du, ev)log δ ωzifi(du, ev)
i=1
α∗z = arg max
αz
X�X � � 1 ��
� XLz
P(z|du, ev) log exp αzjgj(du)
Zdu u vj=1
</equation>
<bodyText confidence="0.995443538461538">
(5)
To optimize Equation (5), we employ the
minFunc toolkit1 using Quasi-Newton strategy.
We adopt Akaike Information Criteria (AIC) to
determine the number of latent variables (Fang et
al., 2010), which is calculated as 2m − 2L(ω, α),
where m is the number of parameters in the model.
LDTM holds two advantages over GM. (1) The
combination parameters vary across various docu-
ment types and hence lead to a gain of flexibility;
(2) It offers probabilistic semantics for the latent
document types and thus documents can be asso-
ciated with multiple types.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999317">
This section presents the two types of fea-
tures used in the discriminative models. Entity-
document features (i.e., f(e, d)) are used in the
discriminative components of GM and LDTM. In
</bodyText>
<footnote confidence="0.92349">
1http://www.cs.ubc.ca/˜schmidtm/
Software/minFunc.html
</footnote>
<bodyText confidence="0.999817161290322">
addition, LDTM requires document-type features
(i.e., g(e)) to learn the mixing coefficients in the
mixture component.
Since our goal is not to develop new entity-
document features, we adopt the identical entity-
document feature set proposed in our previous
work (Wang et al., 2013; Wang et al., 2015a; Wang
et al., 2015b), which has been proved effective.
In terms of document-type features, we consider
two kinds of prior knowledge embedded in doc-
uments to model the correlations between docu-
ments and their latent types.
Topic-based features One prior knowledge to
model a document’s latent type is its intrinsic top-
ics. As we have claimed, documents with one
or more obvious topics are more likely to be rec-
ommended to KB than those without any explicit
topic. We capture the underlying topics in docu-
ments with word co-occurrences. After removing
stop words, we represent each document as a fea-
ture vector with the bag-of-words model, where
word weights are determined by TF-IDF scheme.
Source-based features The source of a docu-
ment is another prior knowledge to evaluate the
probability of the document’s being recommended
to KBs. We leverage a “bag-of-sources” model to
represent each document as source-based feature
vector, and term weights are determined by binary
occurrence scheme. Please note that the sources
are organized hierarchically. For example, main-
stream news is a sub-source of news.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.898698">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999321153846154">
We utilize TREC-KBA-2013 dataset2 as our ex-
perimental dataset. The dataset is composed
of a temporally stream corpus and a target KB
entity set. The stream corpus contains nearly
1 billion documents crawled from 10 sources:
news, mainstream news, social, weblog, link-
ing, arxiv, classified, reviews, forum and meme-
tracker3. The corpus has been split with doc-
uments from October 2011 to February 2012 as
training instances and the remainder for evalua-
tion. We adopt the same training/test range setting
in our experiments. The entity set is composed of
121 Wikipedia entities and 20 Twitter entities.
</bodyText>
<footnote confidence="0.999175">
2http://trec-kba.org/
kba-stream-corpus-2013.shtml
3http://www.memetracker.org/
</footnote>
<figure confidence="0.957882333333333">
Xarg max
ωz
uv
</figure>
<page confidence="0.997611">
563
</page>
<bodyText confidence="0.999733833333333">
Each entity-document pair is labeled as one of
the 4 relevance levels: (i) Vital, timely informa-
tion about the entity’s current state, actions, or sit-
uation. This would motivate a change to an al-
ready up-to-date KB article. (ii) Useful, possibly
citable but not timely, e.g., background biography,
secondary source information. (iii) Neural, in-
formative but not citable, e.g., tertiary source like
Wikipedia article itself. and (iv) Garbage, no in-
formation about the target entity could be learned
from the document, e.g., spam. Annotation details
of the dataset are presented in Table 1.
</bodyText>
<table confidence="0.997564666666667">
Range Vital Useful Neutral Garbage
Train 2011.10 ∼ 2012.02 1696 2121 1030 1702
Test 2012.03 ∼ 2013.02 5630 11579 3379 10543
</table>
<tableCaption confidence="0.860635">
Table 1: Annotation details of TREC-KBA-2013
dataset.
</tableCaption>
<subsectionHeader confidence="0.98392">
4.2 Evaluation Scenarios
</subsectionHeader>
<bodyText confidence="0.9968475">
According to different granularity settings, we
evaluate the proposed models in two scenarios:
(i) Vital Only. Only vital entity-document pairs
are treated as positive instances. (ii) Vital + Use-
ful. Both vital and useful entity-document pairs
are treated as positive instances.
</bodyText>
<subsectionHeader confidence="0.953334">
4.3 Comparison Methods
</subsectionHeader>
<bodyText confidence="0.999511">
We conduct extensive comparisons with the fol-
lowing methods.
</bodyText>
<listItem confidence="0.993726083333333">
• Global Model (GM). The global discrimina-
tive model introduced in section 2.1.
• Source-based Latent Document Type Model
(src LDTM). A variant of LDTM that uti-
lizes source-based features as document-type
features.
• Topic-based Latent Document Type Model
(topic LDTM). A variant of LDTM that uti-
lizes topic-based features as document-type
features.
• Combination Latent Document Type Model
(combine LDTM). This approach utilizes
</listItem>
<bodyText confidence="0.977284571428571">
source-based and topic-based features to-
gether as document-type features. In our ex-
perimental setting, we simply union the two
feature vectors together into an integral fea-
ture vector.
For reference, we also include three top-ranked ap-
proaches in TREC-KBA-2013 track.
</bodyText>
<listItem confidence="0.9860971">
• BIT-MSRA (Wang et al., 2013). A global
random forests classification method, the first
place approach in TREC-KBA-2013 track.
• UDEL (Liu et al., 2013). An entity-centric
query expansion approach, the second place
approach in TREC-KBA-2013 track.
• Official Baseline (Frank et al., 2013). A
strong baseline in which human annotators
go through target entities and came up with a
list of keywords for filtering vital documents.
</listItem>
<subsectionHeader confidence="0.969049">
4.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99992575">
Improving precision is harder than improving re-
call for CCR (Frank et al., 2013). Therefore,
we care more about recommendation quality of
CCR. Precision and overall accuracy are adopted
as metrics to evaluate different approaches. All
the metrics are computed in the test pool of all
entity-document pairs. The results are reported
in Table 2. In comparison to the baselines listed
</bodyText>
<table confidence="0.998192444444444">
Methods Vital Only Vital + Useful
P Accu P Accu
Official Baseline .171 .175 .540 .532
BIT-MSRA .214 .445 .589 .615
UDEL .169 .259 .573 .579
GM .218 .587 .604 .565
src LDTM .273 .763 .626 .607
topic LDTM .293 .755 .643 .609
combine LDTM .299 .751 .633 .611
</table>
<tableCaption confidence="0.7819515">
Table 2: Overall results of evaluated methods.
Best scores are typeset boldface.
</tableCaption>
<bodyText confidence="0.999947277777778">
in the 2nd block of Table 2, our mixture models
achieve higher or competitive precision and ac-
curacy in both scenarios considerably. Compared
with the official baseline, our best mixture model
improves precision about 28%. In both scenarios,
the variants of LDTM outperform GM on preci-
sion and accuracy, which validates our motivations
that (i) introducing document latent types in mix-
ture model can enhance the recommendation qual-
ity, and (ii) source-based and topic-based features
can capture the hidden type information of docu-
ments.
Moreover, topic LDTM generally performs bet-
ter than src LDTM in both scenarios, which meets
our expectation because topic-based features have
far more dimensions than source-based features.
However, even if source-based feature vector
holds a few dimensions (10 in our experiments),
</bodyText>
<page confidence="0.995303">
564
</page>
<bodyText confidence="0.99990304">
src LDT improves the precision on the basis of
GM. Thus, the precision can be enhanced further
if we can develop more valuable features to repre-
sent the underlying document types. The combi-
nation variant of LDTM achieve the best precision
in Vital Only scenario and the best accuracy in
Vital + Useful scenario. The naive combination
strategy of two types of features can improve the
performance but not stable, so we need find better
combination strategies.
For all variant of the LDTM, the number of
latent types determined by AIC are reported in
Table 3. The optimal number of latent types in
Vital + Useful is more than that in Vital Only.
This reveals that the types of Vital documents for
entities have more restrictions than Useful docu-
ments, either by topics or by sources. In addition,
the optimal number of latent topics is more than
that of latent sources, which also follows our in-
tuition that topic-based features holding more di-
mensions than source-based features. Since we
employ a naive combination strategy for the two
types of features, the number of latent types of
combine LDTM is more close to topic LDTM,
which possesses more features than src LDTM.
</bodyText>
<table confidence="0.93370875">
Model Vital Vital + Useful
src LDTM 6 7
topic LDTM 9 15
combine LDTM 14 15
</table>
<tableCaption confidence="0.9026705">
Table 3: Number of latent types determined by
AIC.
</tableCaption>
<sectionHeader confidence="0.999869" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99996292">
There are three kinds of approaches developed
for CCR in previous work: query expansion (Liu
et al., 2013; Wang et al., 2013), classification
such as SVM (Kjersten and McNamee, 2012)
and Random Forest classifier (Bonnefoy et al.,
2013; Balog et al., 2013), and learning to rank
approaches (Wang et al., 2013; Balog and Ra-
mampiaro, 2013). Transfer learning is utilized
to transfer the keyword importance learned from
training pairs to query pairs (Zhou and Chang,
2013).
However, some highly supervised methods re-
quire training instances for each entity to build
a relevance model, limiting their scalabilities. A
compromised solution is to build a global dis-
criminative model with all features indifferently.
We spotlight document-type features and study the
impacts of them in discriminative mixture models.
Mixture model has been applied and proved effec-
tive in multiple information retrieval tasks, such
as expert search (Fang et al., 2010) and federated
search (Hong and Si, 2012). By learning flexible
combination weights for different types of training
instances, mixture model can outperform global
models with fixed weights for all instances.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999379952380952">
Cumulative Citation Recommendation (CCR) is
an important task to automatically detect citation-
worthy documents from volume text streams for
knowledge base entities. We study CCR as a
classification problem and propose a latent docu-
ment type model (LDTM) through introducing a
latent layer in a discriminative model to capture
the correlations between documents and their in-
trinsic types. Two variants of LDTM are imple-
mented by modeling the latent types with doc-
ument source-based and topic-based features re-
spectively. Experimental results on TREC-KBA-
2013 dataset demonstrate that our mixture model
can improve CCR performance significantly, espe-
cially on precision and accuracy, revealing the ad-
vantage of LDTM in enhancing recommendation
quality of citation-worthy documents.
For future work, we wish to explore more use-
ful document-type features and apply more proper
combination strategies to improve the latent docu-
ment type model.
</bodyText>
<sectionHeader confidence="0.980093" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.97905225">
The authors would like to thank Fei Sun, Qifan
Wang and Chen Shao for their valuable sugges-
tions and the anonymous reviewers for their help-
ful comments. This work is funded by the Na-
</bodyText>
<reference confidence="0.7589">
tional Program on Key Basic Research Project
(973 Program, Grant No. 2013CB329600),
National Natural Science Foundation of China
(NSFC, Grant Nos. 61472040 and 60873237), and
Beijing Higher Education Young Elite Teacher
Project (Grant No. YETP1198).
</reference>
<sectionHeader confidence="0.958172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.977701333333333">
Krisztian Balog and Heri Ramampiaro. 2013. Cu-
mulative citation recommendation: classification vs.
ranking. In SIGIR, pages 941–944. ACM.
</reference>
<page confidence="0.981754">
565
</page>
<reference confidence="0.999874104166667">
Krisztian Balog, Heri Ramampiaro, Naimdjon Takhi-
rov, and Kjetil Nørv˚ag. 2013. Multi-step classifica-
tion approaches to cumulative citation recommenda-
tion. In OAIR, pages 121–128.
Ludovic Bonnefoy, Vincent Bouvier, and Patrice Bel-
lot. 2013. A weakly-supervised detection of entity
central documents in a stream. In SIGIR, pages 769–
772.
Yi Fang, Luo Si, and Aditya P. Mathur. 2010. Discrim-
inative models of integrating document evidence and
document-candidate associations for expert search.
In SIGIR, pages 683–690. ACM.
J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, F. Niu,
C. Zhang, C. Re, and I. Soboroff. 2012. Building an
Entity-Centric Stream Filtering Test Collection for
TREC 2012. In TREC.
John Frank, Steve J. Bauer, Max Kleiman-Weiner,
Daniel A. Roberts, Nilesh Triouraneni, Ce Zhang,
and Christopher R`e. 2013. Evaluating stream filter-
ing for entity profile updates for trec 2013. In TREC.
Dzung Hong and Luo Si. 2012. Mixture model with
multiple centralized retrieval algorithms for result
merging in federated search. In SIGIR, pages 821–
830. ACM.
Brain Kjersten and Paul McNamee. 2012. The hltcoe
approach to the trec 2012 kba track. In TREC.
Xitong Liu, Jeffrey Darko, and Hui Fang. 2013. A re-
lated entity based approach for knowledge base ac-
celeration. In TREC.
Jingang Wang, Dandan Song, Chin-Yew Lin, and
Lejian Liao. 2013. Bit and msra at trec kba ccr
track 2013. TREC.
Jingang Wang, Lejian Liao, Dandan Song, Lerong Ma,
Chin-Yew Lin, and Yong Rui. 2015a. Resort-
ing relevance evidences to cumulative citation rec-
ommendation for knowledge base acceleration. In
Web-Age Information Management, volume 9098 of
Lecture Notes in Computer Science, pages 169–180.
Springer International Publishing.
Jingang Wang, Dandan Song, Qifan Wang, Zhiwei
Zhang, Luo Si, Lejian Liao, and Chin-Yew Lin.
2015b. An entity class-depedent discriminative mix-
ture model for cumulative citation recommendation.
In SIGIR. ACM.
Mianwei Zhou and Kevin Chen-Chuan Chang. 2013.
Entity-centric document filtering: boosting feature
mapping through meta-features. In CIKM, pages
119–128. ACM.
</reference>
<page confidence="0.998503">
566
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.424289">
<title confidence="0.9992915">LDTM: A Latent Document Type Model for Cumulative Recommendation</title>
<author confidence="0.967736">Dandan Zhiwei Lejian Luo Chin-Yew</author>
<degree confidence="0.654957">of Computer Science, Beijing Institute of of Computer Science, Purdue Mining Group, Microsoft</degree>
<email confidence="0.882854">sdd,cyl@microsoft.com</email>
<abstract confidence="0.996403333333333">This paper studies Cumulative Citation Recommendation (CCR) given an entity in Knowledge Bases, how to effectively detect its potential citations from volume text streams. Most previous approaches treated all kinds of features indifferently to build a global relevance model, in which the prior knowledge embedded in documents cannot be exploited adequately. To address this problem, we propose a latent document type discriminative model by introducing a latent layer to capture the correlations between documents and their underlying types. The model can better adjust to different types of documents and yield flexible performance when dealing with a broad range of document types. An extensive set of experiments has been conducted on TREC-KBA-2013 dataset, and the results demonstrate that this model can yield a significant performance gain in recommendation quality as compared to the state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>tional Program on Key Basic Research Project (973 Program, Grant No. 2013CB329600), National Natural Science Foundation of China (NSFC, Grant Nos. 61472040 and 60873237), and Beijing Higher Education Young Elite Teacher Project</booktitle>
<location>(Grant No. YETP1198).</location>
<marker></marker>
<rawString>tional Program on Key Basic Research Project (973 Program, Grant No. 2013CB329600), National Natural Science Foundation of China (NSFC, Grant Nos. 61472040 and 60873237), and Beijing Higher Education Young Elite Teacher Project (Grant No. YETP1198).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krisztian Balog</author>
<author>Heri Ramampiaro</author>
</authors>
<title>Cumulative citation recommendation: classification vs. ranking.</title>
<date>2013</date>
<booktitle>In SIGIR,</booktitle>
<pages>941--944</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2480" citStr="Balog and Ramampiaro, 2013" startWordPosition="370" endWordPosition="373">ically detected as soon as they are published online ∗This work was partially performed when the first author was visiting Purdue University and Microsoft Research Asia. † Corresponding Author and then recommended to the editors. This task is studied as Cumulative Citation Recommendation (CCR). Formally, given a set of KB entities, CCR is to filter relevant documents from a stream corpus and evaluate their citation-worthiness to the target entities. A variety of supervised approaches (e.g., classification, learning to rank) have been employed and achieved promising results (Wang et al., 2013; Balog and Ramampiaro, 2013; Balog et al., 2013). Nevertheless, most of them leverage all features indiscriminately to build a global relevance model, which leads to unsatisfactory performance. The documents can offer some prior knowledge, which is named as type in this paper. The type is the prior knowledge embedded in the document that impacts on the probability of its being recommended to KBs. For instance, when dealing with a document on “music” topic, we would like to have less weights put on a politician entity because this document is not likely to related to it, but more often related to musicians or musical ban</context>
<context position="17487" citStr="Balog and Ramampiaro, 2013" startWordPosition="2860" endWordPosition="2864">y for the two types of features, the number of latent types of combine LDTM is more close to topic LDTM, which possesses more features than src LDTM. Model Vital Vital + Useful src LDTM 6 7 topic LDTM 9 15 combine LDTM 14 15 Table 3: Number of latent types determined by AIC. 5 Related Work There are three kinds of approaches developed for CCR in previous work: query expansion (Liu et al., 2013; Wang et al., 2013), classification such as SVM (Kjersten and McNamee, 2012) and Random Forest classifier (Bonnefoy et al., 2013; Balog et al., 2013), and learning to rank approaches (Wang et al., 2013; Balog and Ramampiaro, 2013). Transfer learning is utilized to transfer the keyword importance learned from training pairs to query pairs (Zhou and Chang, 2013). However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global discriminative model with all features indifferently. We spotlight document-type features and study the impacts of them in discriminative mixture models. Mixture model has been applied and proved effective in multiple information retrieval tasks, such as expert search (Fang et al.</context>
</contexts>
<marker>Balog, Ramampiaro, 2013</marker>
<rawString>Krisztian Balog and Heri Ramampiaro. 2013. Cumulative citation recommendation: classification vs. ranking. In SIGIR, pages 941–944. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krisztian Balog</author>
<author>Heri Ramampiaro</author>
<author>Naimdjon Takhirov</author>
<author>Kjetil Nørv˚ag</author>
</authors>
<title>Multi-step classification approaches to cumulative citation recommendation.</title>
<date>2013</date>
<booktitle>In OAIR,</booktitle>
<pages>121--128</pages>
<marker>Balog, Ramampiaro, Takhirov, Nørv˚ag, 2013</marker>
<rawString>Krisztian Balog, Heri Ramampiaro, Naimdjon Takhirov, and Kjetil Nørv˚ag. 2013. Multi-step classification approaches to cumulative citation recommendation. In OAIR, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Bonnefoy</author>
<author>Vincent Bouvier</author>
<author>Patrice Bellot</author>
</authors>
<title>A weakly-supervised detection of entity central documents in a stream.</title>
<date>2013</date>
<booktitle>In SIGIR,</booktitle>
<pages>769--772</pages>
<contexts>
<context position="5884" citStr="Bonnefoy et al., 2013" startWordPosition="950" endWordPosition="953">rm of P(r=1|e,d) is expressed as P(r=1|e,d) = S(PKi=1wifi(e, d)), where S(x) is the standard logistic function, wi is the combination parameter for the ith feature. It is easy to derive that for different values of r, the only difference in P (r|e, d) is the sign within the logistic function. Therefore, we adopt the general representation of P(r|e, d)=S(r PKi=1 wifi(e, d)). This model is denoted as GM in this paper. Several previous approaches can be deemed as global models adopting different classification functions such as decision trees (Wang et al., 2013) and Support Vector Machine (SVM) (Bonnefoy et al., 2013). 2.2 Latent Document Type Model In GM, a fixed set of combination weights (i.e., w) are learned to optimize the overall performance for all entity-document pairs. However, the best combination strategy for a given pair is not always the best for the others since both the documents and entities are heterogeneous. Therefore, we may benefit from developing a document type dependent model in which we choose the combination strategy individually for each document type to optimize the performance for specific document types. Since it is not feasible to determine the proper combination strategy for </context>
<context position="17385" citStr="Bonnefoy et al., 2013" startWordPosition="2843" endWordPosition="2846"> holding more dimensions than source-based features. Since we employ a naive combination strategy for the two types of features, the number of latent types of combine LDTM is more close to topic LDTM, which possesses more features than src LDTM. Model Vital Vital + Useful src LDTM 6 7 topic LDTM 9 15 combine LDTM 14 15 Table 3: Number of latent types determined by AIC. 5 Related Work There are three kinds of approaches developed for CCR in previous work: query expansion (Liu et al., 2013; Wang et al., 2013), classification such as SVM (Kjersten and McNamee, 2012) and Random Forest classifier (Bonnefoy et al., 2013; Balog et al., 2013), and learning to rank approaches (Wang et al., 2013; Balog and Ramampiaro, 2013). Transfer learning is utilized to transfer the keyword importance learned from training pairs to query pairs (Zhou and Chang, 2013). However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global discriminative model with all features indifferently. We spotlight document-type features and study the impacts of them in discriminative mixture models. Mixture model has been ap</context>
</contexts>
<marker>Bonnefoy, Bouvier, Bellot, 2013</marker>
<rawString>Ludovic Bonnefoy, Vincent Bouvier, and Patrice Bellot. 2013. A weakly-supervised detection of entity central documents in a stream. In SIGIR, pages 769– 772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Fang</author>
<author>Luo Si</author>
<author>Aditya P Mathur</author>
</authors>
<title>Discriminative models of integrating document evidence and document-candidate associations for expert search. In</title>
<date>2010</date>
<booktitle>SIGIR,</booktitle>
<pages>683--690</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9247" citStr="Fang et al., 2010" startWordPosition="1545" endWordPosition="1548">erived by computing the posterior probability of z given du and ev, which is denoted as P(z|du, ev). P(z|du, ev) = �PLz � � � PK exp j=1αzjgj(du) δ ruv i=1ωzifi(du, ev) � � P �PLz � PK zexp j=1αzjgj(du) δ ruv i=1ωzifi(du, ev) (4) In M-step, we can obtain the following parameter update rules. ω∗ = z � � XK �� P(z|du, ev)log δ ωzifi(du, ev) i=1 α∗z = arg max αz X�X � � 1 �� � XLz P(z|du, ev) log exp αzjgj(du) Zdu u vj=1 (5) To optimize Equation (5), we employ the minFunc toolkit1 using Quasi-Newton strategy. We adopt Akaike Information Criteria (AIC) to determine the number of latent variables (Fang et al., 2010), which is calculated as 2m − 2L(ω, α), where m is the number of parameters in the model. LDTM holds two advantages over GM. (1) The combination parameters vary across various document types and hence lead to a gain of flexibility; (2) It offers probabilistic semantics for the latent document types and thus documents can be associated with multiple types. 3 Features This section presents the two types of features used in the discriminative models. Entitydocument features (i.e., f(e, d)) are used in the discriminative components of GM and LDTM. In 1http://www.cs.ubc.ca/˜schmidtm/ Software/minFu</context>
<context position="18094" citStr="Fang et al., 2010" startWordPosition="2952" endWordPosition="2955">iaro, 2013). Transfer learning is utilized to transfer the keyword importance learned from training pairs to query pairs (Zhou and Chang, 2013). However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global discriminative model with all features indifferently. We spotlight document-type features and study the impacts of them in discriminative mixture models. Mixture model has been applied and proved effective in multiple information retrieval tasks, such as expert search (Fang et al., 2010) and federated search (Hong and Si, 2012). By learning flexible combination weights for different types of training instances, mixture model can outperform global models with fixed weights for all instances. 6 Conclusion Cumulative Citation Recommendation (CCR) is an important task to automatically detect citationworthy documents from volume text streams for knowledge base entities. We study CCR as a classification problem and propose a latent document type model (LDTM) through introducing a latent layer in a discriminative model to capture the correlations between documents and their intrinsi</context>
</contexts>
<marker>Fang, Si, Mathur, 2010</marker>
<rawString>Yi Fang, Luo Si, and Aditya P. Mathur. 2010. Discriminative models of integrating document evidence and document-candidate associations for expert search. In SIGIR, pages 683–690. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Frank</author>
<author>M Kleiman-Weiner</author>
<author>D A Roberts</author>
<author>F Niu</author>
<author>C Zhang</author>
<author>C Re</author>
<author>I Soboroff</author>
</authors>
<title>Building an Entity-Centric Stream Filtering Test Collection for TREC 2012. In TREC.</title>
<date>2012</date>
<contexts>
<context position="1591" citStr="Frank et al., 2012" startWordPosition="224" endWordPosition="227">ifferent types of documents and yield flexible performance when dealing with a broad range of document types. An extensive set of experiments has been conducted on TREC-KBA-2013 dataset, and the results demonstrate that this model can yield a significant performance gain in recommendation quality as compared to the state-of-the-art. 1 Introduction Knowledge Bases (KBs), like Wikipedia, are playing increasingly important roles in numerous entity-based information retrieval tasks. Nevertheless, most KBs are hard to be up-to-date due to their manual maintenances by human editors. As reported in (Frank et al., 2012), there exists a median time lag of 356 days between the day a news article is published and the time that the news is cited in a Wikipedia article dedicated to the entity concerned by the news. The time lag would be reduced if relevant documents could be automatically detected as soon as they are published online ∗This work was partially performed when the first author was visiting Purdue University and Microsoft Research Asia. † Corresponding Author and then recommended to the editors. This task is studied as Cumulative Citation Recommendation (CCR). Formally, given a set of KB entities, CCR</context>
</contexts>
<marker>Frank, Kleiman-Weiner, Roberts, Niu, Zhang, Re, Soboroff, 2012</marker>
<rawString>J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, F. Niu, C. Zhang, C. Re, and I. Soboroff. 2012. Building an Entity-Centric Stream Filtering Test Collection for TREC 2012. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Frank</author>
<author>Steve J Bauer</author>
<author>Max Kleiman-Weiner</author>
<author>Daniel A Roberts</author>
<author>Nilesh Triouraneni</author>
<author>Ce Zhang</author>
<author>Christopher R`e</author>
</authors>
<title>Evaluating stream filtering for entity profile updates for trec 2013. In TREC.</title>
<date>2013</date>
<marker>Frank, Bauer, Kleiman-Weiner, Roberts, Triouraneni, Zhang, R`e, 2013</marker>
<rawString>John Frank, Steve J. Bauer, Max Kleiman-Weiner, Daniel A. Roberts, Nilesh Triouraneni, Ce Zhang, and Christopher R`e. 2013. Evaluating stream filtering for entity profile updates for trec 2013. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dzung Hong</author>
<author>Luo Si</author>
</authors>
<title>Mixture model with multiple centralized retrieval algorithms for result merging in federated search.</title>
<date>2012</date>
<booktitle>In SIGIR,</booktitle>
<pages>821--830</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="18135" citStr="Hong and Si, 2012" startWordPosition="2959" endWordPosition="2962">d to transfer the keyword importance learned from training pairs to query pairs (Zhou and Chang, 2013). However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global discriminative model with all features indifferently. We spotlight document-type features and study the impacts of them in discriminative mixture models. Mixture model has been applied and proved effective in multiple information retrieval tasks, such as expert search (Fang et al., 2010) and federated search (Hong and Si, 2012). By learning flexible combination weights for different types of training instances, mixture model can outperform global models with fixed weights for all instances. 6 Conclusion Cumulative Citation Recommendation (CCR) is an important task to automatically detect citationworthy documents from volume text streams for knowledge base entities. We study CCR as a classification problem and propose a latent document type model (LDTM) through introducing a latent layer in a discriminative model to capture the correlations between documents and their intrinsic types. Two variants of LDTM are impleme</context>
</contexts>
<marker>Hong, Si, 2012</marker>
<rawString>Dzung Hong and Luo Si. 2012. Mixture model with multiple centralized retrieval algorithms for result merging in federated search. In SIGIR, pages 821– 830. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brain Kjersten</author>
<author>Paul McNamee</author>
</authors>
<title>The hltcoe approach to the trec 2012 kba track.</title>
<date>2012</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="17333" citStr="Kjersten and McNamee, 2012" startWordPosition="2835" endWordPosition="2838">which also follows our intuition that topic-based features holding more dimensions than source-based features. Since we employ a naive combination strategy for the two types of features, the number of latent types of combine LDTM is more close to topic LDTM, which possesses more features than src LDTM. Model Vital Vital + Useful src LDTM 6 7 topic LDTM 9 15 combine LDTM 14 15 Table 3: Number of latent types determined by AIC. 5 Related Work There are three kinds of approaches developed for CCR in previous work: query expansion (Liu et al., 2013; Wang et al., 2013), classification such as SVM (Kjersten and McNamee, 2012) and Random Forest classifier (Bonnefoy et al., 2013; Balog et al., 2013), and learning to rank approaches (Wang et al., 2013; Balog and Ramampiaro, 2013). Transfer learning is utilized to transfer the keyword importance learned from training pairs to query pairs (Zhou and Chang, 2013). However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global discriminative model with all features indifferently. We spotlight document-type features and study the impacts of them in disc</context>
</contexts>
<marker>Kjersten, McNamee, 2012</marker>
<rawString>Brain Kjersten and Paul McNamee. 2012. The hltcoe approach to the trec 2012 kba track. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xitong Liu</author>
<author>Jeffrey Darko</author>
<author>Hui Fang</author>
</authors>
<title>A related entity based approach for knowledge base acceleration.</title>
<date>2013</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="13992" citStr="Liu et al., 2013" startWordPosition="2281" endWordPosition="2284">t Document Type Model (topic LDTM). A variant of LDTM that utilizes topic-based features as document-type features. • Combination Latent Document Type Model (combine LDTM). This approach utilizes source-based and topic-based features together as document-type features. In our experimental setting, we simply union the two feature vectors together into an integral feature vector. For reference, we also include three top-ranked approaches in TREC-KBA-2013 track. • BIT-MSRA (Wang et al., 2013). A global random forests classification method, the first place approach in TREC-KBA-2013 track. • UDEL (Liu et al., 2013). An entity-centric query expansion approach, the second place approach in TREC-KBA-2013 track. • Official Baseline (Frank et al., 2013). A strong baseline in which human annotators go through target entities and came up with a list of keywords for filtering vital documents. 4.4 Results and Discussion Improving precision is harder than improving recall for CCR (Frank et al., 2013). Therefore, we care more about recommendation quality of CCR. Precision and overall accuracy are adopted as metrics to evaluate different approaches. All the metrics are computed in the test pool of all entity-docume</context>
<context position="17256" citStr="Liu et al., 2013" startWordPosition="2823" endWordPosition="2826">imal number of latent topics is more than that of latent sources, which also follows our intuition that topic-based features holding more dimensions than source-based features. Since we employ a naive combination strategy for the two types of features, the number of latent types of combine LDTM is more close to topic LDTM, which possesses more features than src LDTM. Model Vital Vital + Useful src LDTM 6 7 topic LDTM 9 15 combine LDTM 14 15 Table 3: Number of latent types determined by AIC. 5 Related Work There are three kinds of approaches developed for CCR in previous work: query expansion (Liu et al., 2013; Wang et al., 2013), classification such as SVM (Kjersten and McNamee, 2012) and Random Forest classifier (Bonnefoy et al., 2013; Balog et al., 2013), and learning to rank approaches (Wang et al., 2013; Balog and Ramampiaro, 2013). Transfer learning is utilized to transfer the keyword importance learned from training pairs to query pairs (Zhou and Chang, 2013). However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global discriminative model with all features indifferent</context>
</contexts>
<marker>Liu, Darko, Fang, 2013</marker>
<rawString>Xitong Liu, Jeffrey Darko, and Hui Fang. 2013. A related entity based approach for knowledge base acceleration. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingang Wang</author>
<author>Dandan Song</author>
<author>Chin-Yew Lin</author>
<author>Lejian Liao</author>
</authors>
<title>Bit and msra at trec kba ccr track</title>
<date>2013</date>
<publisher>TREC.</publisher>
<contexts>
<context position="2452" citStr="Wang et al., 2013" startWordPosition="366" endWordPosition="369">ts could be automatically detected as soon as they are published online ∗This work was partially performed when the first author was visiting Purdue University and Microsoft Research Asia. † Corresponding Author and then recommended to the editors. This task is studied as Cumulative Citation Recommendation (CCR). Formally, given a set of KB entities, CCR is to filter relevant documents from a stream corpus and evaluate their citation-worthiness to the target entities. A variety of supervised approaches (e.g., classification, learning to rank) have been employed and achieved promising results (Wang et al., 2013; Balog and Ramampiaro, 2013; Balog et al., 2013). Nevertheless, most of them leverage all features indiscriminately to build a global relevance model, which leads to unsatisfactory performance. The documents can offer some prior knowledge, which is named as type in this paper. The type is the prior knowledge embedded in the document that impacts on the probability of its being recommended to KBs. For instance, when dealing with a document on “music” topic, we would like to have less weights put on a politician entity because this document is not likely to related to it, but more often related</context>
<context position="5827" citStr="Wang et al., 2013" startWordPosition="941" endWordPosition="944">tity e, otherwise r = −1. Formally, the parametric form of P(r=1|e,d) is expressed as P(r=1|e,d) = S(PKi=1wifi(e, d)), where S(x) is the standard logistic function, wi is the combination parameter for the ith feature. It is easy to derive that for different values of r, the only difference in P (r|e, d) is the sign within the logistic function. Therefore, we adopt the general representation of P(r|e, d)=S(r PKi=1 wifi(e, d)). This model is denoted as GM in this paper. Several previous approaches can be deemed as global models adopting different classification functions such as decision trees (Wang et al., 2013) and Support Vector Machine (SVM) (Bonnefoy et al., 2013). 2.2 Latent Document Type Model In GM, a fixed set of combination weights (i.e., w) are learned to optimize the overall performance for all entity-document pairs. However, the best combination strategy for a given pair is not always the best for the others since both the documents and entities are heterogeneous. Therefore, we may benefit from developing a document type dependent model in which we choose the combination strategy individually for each document type to optimize the performance for specific document types. Since it is not f</context>
<context position="10134" citStr="Wang et al., 2013" startWordPosition="1686" endWordPosition="1689">for the latent document types and thus documents can be associated with multiple types. 3 Features This section presents the two types of features used in the discriminative models. Entitydocument features (i.e., f(e, d)) are used in the discriminative components of GM and LDTM. In 1http://www.cs.ubc.ca/˜schmidtm/ Software/minFunc.html addition, LDTM requires document-type features (i.e., g(e)) to learn the mixing coefficients in the mixture component. Since our goal is not to develop new entitydocument features, we adopt the identical entitydocument feature set proposed in our previous work (Wang et al., 2013; Wang et al., 2015a; Wang et al., 2015b), which has been proved effective. In terms of document-type features, we consider two kinds of prior knowledge embedded in documents to model the correlations between documents and their latent types. Topic-based features One prior knowledge to model a document’s latent type is its intrinsic topics. As we have claimed, documents with one or more obvious topics are more likely to be recommended to KB than those without any explicit topic. We capture the underlying topics in documents with word co-occurrences. After removing stop words, we represent each</context>
<context position="13869" citStr="Wang et al., 2013" startWordPosition="2262" endWordPosition="2265"> Type Model (src LDTM). A variant of LDTM that utilizes source-based features as document-type features. • Topic-based Latent Document Type Model (topic LDTM). A variant of LDTM that utilizes topic-based features as document-type features. • Combination Latent Document Type Model (combine LDTM). This approach utilizes source-based and topic-based features together as document-type features. In our experimental setting, we simply union the two feature vectors together into an integral feature vector. For reference, we also include three top-ranked approaches in TREC-KBA-2013 track. • BIT-MSRA (Wang et al., 2013). A global random forests classification method, the first place approach in TREC-KBA-2013 track. • UDEL (Liu et al., 2013). An entity-centric query expansion approach, the second place approach in TREC-KBA-2013 track. • Official Baseline (Frank et al., 2013). A strong baseline in which human annotators go through target entities and came up with a list of keywords for filtering vital documents. 4.4 Results and Discussion Improving precision is harder than improving recall for CCR (Frank et al., 2013). Therefore, we care more about recommendation quality of CCR. Precision and overall accuracy </context>
<context position="17276" citStr="Wang et al., 2013" startWordPosition="2827" endWordPosition="2830">ent topics is more than that of latent sources, which also follows our intuition that topic-based features holding more dimensions than source-based features. Since we employ a naive combination strategy for the two types of features, the number of latent types of combine LDTM is more close to topic LDTM, which possesses more features than src LDTM. Model Vital Vital + Useful src LDTM 6 7 topic LDTM 9 15 combine LDTM 14 15 Table 3: Number of latent types determined by AIC. 5 Related Work There are three kinds of approaches developed for CCR in previous work: query expansion (Liu et al., 2013; Wang et al., 2013), classification such as SVM (Kjersten and McNamee, 2012) and Random Forest classifier (Bonnefoy et al., 2013; Balog et al., 2013), and learning to rank approaches (Wang et al., 2013; Balog and Ramampiaro, 2013). Transfer learning is utilized to transfer the keyword importance learned from training pairs to query pairs (Zhou and Chang, 2013). However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global discriminative model with all features indifferently. We spotlight doc</context>
</contexts>
<marker>Wang, Song, Lin, Liao, 2013</marker>
<rawString>Jingang Wang, Dandan Song, Chin-Yew Lin, and Lejian Liao. 2013. Bit and msra at trec kba ccr track 2013. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingang Wang</author>
<author>Lejian Liao</author>
<author>Dandan Song</author>
<author>Lerong Ma</author>
<author>Chin-Yew Lin</author>
<author>Yong Rui</author>
</authors>
<title>Resorting relevance evidences to cumulative citation recommendation for knowledge base acceleration.</title>
<date>2015</date>
<journal>In Web-Age Information Management,</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>9098</volume>
<pages>169--180</pages>
<publisher>Springer International Publishing.</publisher>
<contexts>
<context position="10153" citStr="Wang et al., 2015" startWordPosition="1690" endWordPosition="1693">ment types and thus documents can be associated with multiple types. 3 Features This section presents the two types of features used in the discriminative models. Entitydocument features (i.e., f(e, d)) are used in the discriminative components of GM and LDTM. In 1http://www.cs.ubc.ca/˜schmidtm/ Software/minFunc.html addition, LDTM requires document-type features (i.e., g(e)) to learn the mixing coefficients in the mixture component. Since our goal is not to develop new entitydocument features, we adopt the identical entitydocument feature set proposed in our previous work (Wang et al., 2013; Wang et al., 2015a; Wang et al., 2015b), which has been proved effective. In terms of document-type features, we consider two kinds of prior knowledge embedded in documents to model the correlations between documents and their latent types. Topic-based features One prior knowledge to model a document’s latent type is its intrinsic topics. As we have claimed, documents with one or more obvious topics are more likely to be recommended to KB than those without any explicit topic. We capture the underlying topics in documents with word co-occurrences. After removing stop words, we represent each document as a feat</context>
</contexts>
<marker>Wang, Liao, Song, Ma, Lin, Rui, 2015</marker>
<rawString>Jingang Wang, Lejian Liao, Dandan Song, Lerong Ma, Chin-Yew Lin, and Yong Rui. 2015a. Resorting relevance evidences to cumulative citation recommendation for knowledge base acceleration. In Web-Age Information Management, volume 9098 of Lecture Notes in Computer Science, pages 169–180. Springer International Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingang Wang</author>
<author>Dandan Song</author>
<author>Qifan Wang</author>
<author>Zhiwei Zhang</author>
<author>Luo Si</author>
<author>Lejian Liao</author>
<author>Chin-Yew Lin</author>
</authors>
<title>An entity class-depedent discriminative mixture model for cumulative citation recommendation.</title>
<date>2015</date>
<booktitle>In SIGIR.</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="10153" citStr="Wang et al., 2015" startWordPosition="1690" endWordPosition="1693">ment types and thus documents can be associated with multiple types. 3 Features This section presents the two types of features used in the discriminative models. Entitydocument features (i.e., f(e, d)) are used in the discriminative components of GM and LDTM. In 1http://www.cs.ubc.ca/˜schmidtm/ Software/minFunc.html addition, LDTM requires document-type features (i.e., g(e)) to learn the mixing coefficients in the mixture component. Since our goal is not to develop new entitydocument features, we adopt the identical entitydocument feature set proposed in our previous work (Wang et al., 2013; Wang et al., 2015a; Wang et al., 2015b), which has been proved effective. In terms of document-type features, we consider two kinds of prior knowledge embedded in documents to model the correlations between documents and their latent types. Topic-based features One prior knowledge to model a document’s latent type is its intrinsic topics. As we have claimed, documents with one or more obvious topics are more likely to be recommended to KB than those without any explicit topic. We capture the underlying topics in documents with word co-occurrences. After removing stop words, we represent each document as a feat</context>
</contexts>
<marker>Wang, Song, Wang, Zhang, Si, Liao, Lin, 2015</marker>
<rawString>Jingang Wang, Dandan Song, Qifan Wang, Zhiwei Zhang, Luo Si, Lejian Liao, and Chin-Yew Lin. 2015b. An entity class-depedent discriminative mixture model for cumulative citation recommendation. In SIGIR. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mianwei Zhou</author>
<author>Kevin Chen-Chuan Chang</author>
</authors>
<title>Entity-centric document filtering: boosting feature mapping through meta-features.</title>
<date>2013</date>
<booktitle>In CIKM,</booktitle>
<pages>119--128</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17619" citStr="Zhou and Chang, 2013" startWordPosition="2881" endWordPosition="2884">n src LDTM. Model Vital Vital + Useful src LDTM 6 7 topic LDTM 9 15 combine LDTM 14 15 Table 3: Number of latent types determined by AIC. 5 Related Work There are three kinds of approaches developed for CCR in previous work: query expansion (Liu et al., 2013; Wang et al., 2013), classification such as SVM (Kjersten and McNamee, 2012) and Random Forest classifier (Bonnefoy et al., 2013; Balog et al., 2013), and learning to rank approaches (Wang et al., 2013; Balog and Ramampiaro, 2013). Transfer learning is utilized to transfer the keyword importance learned from training pairs to query pairs (Zhou and Chang, 2013). However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. A compromised solution is to build a global discriminative model with all features indifferently. We spotlight document-type features and study the impacts of them in discriminative mixture models. Mixture model has been applied and proved effective in multiple information retrieval tasks, such as expert search (Fang et al., 2010) and federated search (Hong and Si, 2012). By learning flexible combination weights for different types of training instances</context>
</contexts>
<marker>Zhou, Chang, 2013</marker>
<rawString>Mianwei Zhou and Kevin Chen-Chuan Chang. 2013. Entity-centric document filtering: boosting feature mapping through meta-features. In CIKM, pages 119–128. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
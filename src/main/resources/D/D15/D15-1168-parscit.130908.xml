<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000141">
<title confidence="0.9785555">
Fine-grained Opinion Mining with
Recurrent Neural Networks and Word Embeddings
</title>
<author confidence="0.997367">
Pengfei Liu&apos;, Shafiq Joty2 and Helen Meng&apos;
</author>
<affiliation confidence="0.987875">
&apos;Department of Systems Engineering and Engineering Management,
The Chinese University of Hong Kong, Hong Kong SAR, China
2Qatar Computing Research Institute - HBKU, Doha, Qatar
</affiliation>
<email confidence="0.984272">
{pfliu, hmmeng}@se.cuhk.edu.hk, sjoty@qf.org.qa
</email>
<sectionHeader confidence="0.994778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916941176471">
The tasks in fine-grained opinion mining
can be regarded as either a token-level se-
quence labeling problem or as a semantic
compositional task. We propose a gen-
eral class of discriminative models based
on recurrent neural networks (RNNs) and
word embeddings that can be successfully
applied to such tasks without any task-
specific feature engineering effort. Our
experimental results on the task of opin-
ion target identification show that RNNs,
without using any hand-crafted features,
outperform feature-rich CRF-based mod-
els. Our framework is flexible, allows us to
incorporate other linguistic features, and
achieves results that rival the top perform-
ing systems in SemEval-2014.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99976545">
Fine-grained opinion mining involves identifying
the opinion holder who expresses the opinion, de-
tecting opinion expressions, measuring their inten-
sity and sentiment, and identifying the target or
aspect of the opinion (Wiebe et al., 2005). For ex-
ample, in the sentence “John says, the hard disk is
very noisy”, John, the opinion holder, expresses a
very negative (i.e., sentiment with intensity) opin-
ion towards the target “hard disk” using the opin-
ionated expression “very noisy”. A number of
NLP applications can benefit from fine-grained
opinion mining including opinion summarization
and opinion-oriented question answering.
The tasks in fine-grained opinion mining can be
regarded as either a token-level sequence labeling
problem or as a semantic compositional task at the
sequence (e.g., phrase) level. For example, iden-
tifying opinion holders, opinion expressions and
opinion targets can be formulated as a token-level
sequence tagging problem, where the task is to
</bodyText>
<table confidence="0.793634666666667">
The hard disk is very noisy
O B-TARG I-TARG O O O
O O O O B-EXPR I-EXPR
</table>
<tableCaption confidence="0.874666">
Table 1: An example sentence annotated with BIO
labels for opinion target (TARG tags) and for opin-
ion expression (EXPR tags) extraction.
</tableCaption>
<bodyText confidence="0.999218181818182">
label each word in a sentence using the conven-
tional BIO tagging scheme. For example, Table
1 shows a sentence tagged with BIO scheme for
opinion target (middle row) and for opinion ex-
pression (bottom row) identification tasks. On the
other hand, characterizing intensity and sentiment
of an opinionated expression can be regarded as a
semantic compositional problem, where the task is
to aggregate vector representations of tokens in a
meaningful way and later use them for sentiment
classification (Socher et al., 2013).
Conditional random fields (CRFs) (Lafferty et
al., 2001) have been quite successful for different
fine-grained opinion mining tasks, e.g., opinion
expression extraction (Yang and Cardie, 2012).
The state-of-the-art model for opinion target ex-
traction is also based on a CRF (Pontiki et al.,
2014). However, the success of CRFs depends
heavily on the use of an appropriate feature set and
feature function expansion, which often requires a
lot of engineering effort for each task in hand.
An alternative approach of deep learning auto-
matically learns latent features as distributed vec-
tors and have recently been shown to outperform
CRFs on similar tasks. For example, Irsoy and
Cardie (2014) apply deep recurrent neural net-
works (RNNs) to extract opinion expressions from
sentences and show that RNNs outperform CRFs.
Socher et al. (2013) propose recursive neural net-
works for a semantic compositional task to iden-
tify the sentiments of phrases and sentences hier-
archically using the syntactic parse trees.
Meanwhile, recent advances in word embed-
</bodyText>
<page confidence="0.866537">
1433
</page>
<note confidence="0.985344">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.997980391304348">
ding induction methods (Collobert and Weston,
2008; Mikolov et al., 2013b) have benefited re-
searchers in two ways: (i) they have contributed
to significant gains when used as extra word fea-
tures in existing NLP systems (Turian et al., 2010;
Lebret and Lebret, 2013), and (ii) they have en-
abled more effective training of RNNs by provid-
ing compact input representations of the words
(Mesnil et al., 2013; Irsoy and Cardie, 2014).
Motivated by the recent success of deep learn-
ing, in this paper we propose a general class of
models based on RNN architecture and word em-
beddings, that can be successfully applied to fine-
grained opinion mining tasks without any task-
specific feature engineering effort. We experiment
with several important RNN architectures includ-
ing Elman-RNN, Jordan-RNN, long short term
memory (LSTM) and their variations. We acquire
pre-trained word embeddings from several exter-
nal sources to give better initialization to our RNN
models. The RNN models then fine-tune the word
vectors during training to learn task-specific em-
beddings. We also present an architecture to in-
corporate other linguistic features into RNNs.
Our results on the task of opinion target extrac-
tion show that word embeddings improve the per-
formance of state-of-the-art CRF models, when
included as additional features. They also improve
RNNs when used as pre-trained word vectors and
fine-tuning them on the task gives the best results.
A comparison between models demonstrates that
RNNs outperform CRFs, even when they use word
embeddings as the only features. Incorporating
simple linguistic features into RNNs improves the
performance even further. Our best results with
LSTM RNN outperform the top performing sys-
tem on the Laptop dataset and achieve the second
best on the Restaurant dataset in SemEval-2014.
We make our source code available.1
In the remainder of this paper, after discussing
related work in Section 2, we present our RNN
models in Section 3. In Section 4, we briefly de-
scribe the pre-trained word embeddings. The ex-
periments and analysis of results are presented in
Section 5. Finally, we summarize our contribu-
tions with future directions in Section 6.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9665575">
A line of previous research in fine-grained opinion
mining focused on detecting opinion (subjective)
</bodyText>
<footnote confidence="0.878842">
1https://github.com/ppfliu/opinion-target
</footnote>
<bodyText confidence="0.999802647058823">
expressions, e.g., (Wilson et al., 2005; Breck et al.,
2007). The common approach was to formulate
the problem as a sequence tagging task and use
a CRF model. Later approaches extended this to
jointly identify opinion holders (Choi et al., 2005),
and intensity and polarity (Choi and Cardie, 2010).
Extracting aspect terms or opinion targets have
been actively investigated in the past. Typical ap-
proaches include association mining to find fre-
quent item sets (i.e., co-occurring words) as can-
didate aspects (Hu and Liu, 2004), classification-
based methods such as hidden Markov model (Jin
et al., 2009) and CRF (Shariaty and Moghaddam,
2011; Yang and Cardie, 2012; Yang and Cardie,
2013), as well as topic modeling techniques using
Latent Dirichlet Allocation (LDA) model and its
variants (Titov and McDonald, 2008; Lin and He,
2009; Moghaddam and Ester, 2012).
Conventional RNNs (e.g., Elman type) and
LSTM have been successfully applied to vari-
ous sequence prediction tasks, such as language
modeling (Mikolov et al., 2010; Sundermeyer et
al., 2012), speech recognition (Graves and Jaitly,
2014; Sak et al., 2014) and spoken language un-
derstanding (Mesnil et al., 2013). For sentiment
analysis, Socher et al. (2013) propose to use re-
cursive neural networks to hierarchically compose
semantic word vectors based on syntactic parse
trees, and use the vectors to identify the sentiments
of the phrases and sentences. Le and Zuidema
(2015) extended recursive neural networks with
LSTM to compute a parent vector in parse trees by
combining information of both output and LSTM
memory cells from its two children.
Most relevant to our work is the recent work of
Irsoy and Cardie (2014), where they apply deep
Elman-type RNN to extract opinion expressions
and show that deep RNN outperforms CRF, semi-
CRF and shallow RNN. They used word embed-
dings from Google without fine-tuning them.
Although inspired, our work differs from the
work of Irsoy and Cardie (2014) in many ways.
(i) We experiment with not only Elman-type, but
also with a Jordan-type and with a more advanced
LSTM RNN, and demonstrate the performance of
various RNN models. (ii) We use not only Google
embeddings as pre-trained word vectors, but also
other embeddings including SENNA and Amazon,
and show their performances. (iii) We also fine-
tune the embeddings for our task, which is shown
to be very crucial. (iv) We present an RNN ar-
</bodyText>
<page confidence="0.991383">
1434
</page>
<bodyText confidence="0.9998575">
chitecture to include other linguistic features and
show its effectiveness. (v) Finally, we present
a comprehensive experiment exploring different
embedding dimensions and hidden layer sizes for
all the variations of the RNNs (i.e., including fea-
tures and bi-directionality).
</bodyText>
<sectionHeader confidence="0.973196" genericHeader="method">
3 Recurrent Neural Models
</sectionHeader>
<bodyText confidence="0.9998979375">
The recurrent neural models in this section com-
pute compositional vector representations for
word sequences of arbitrary length. These high-
level (i.e., hidden-layer) distributed representa-
tions are then used as features to classify each to-
ken in the sentence. We first describe the com-
mon properties shared among the RNNs below,
followed by the descriptions of the specific RNNs.
Each word in the vocabulary V is represented
by a D dimensional vector in the shared look-up
table L E R|V |xD. Note that L is considered as
a model parameter to be learned. We can initial-
ize L randomly or by pre-trained word embedding
vectors (see Section 4). Given an input sentence
s = (s1, · · · , sT), we first transform it into a fea-
ture sequence by mapping each word token st E s
to an index in L. The look-up layer then cre-
ates a context vector xt E RmD covering m − 1
neighboring tokens for each st by concatenating
their respective vectors in L. For example, given
the context size m = 3, the context vector xt
for the word disk in Figure 1 is formed by con-
catenating the embeddings of hard, disk and is.
This window-based approach is intended to cap-
ture short-term dependencies between neighbor-
ing words in a sentence (Collobert et al., 2011).
The concatenated vector is then passed through
non-linear recurrent hidden layers to learn high-
level compositional representations, which are in
turn fed to the output layer for classification using
softmax. Formally, the probability of k-th label
in the output for classification into K classes:
</bodyText>
<equation confidence="0.93722425">
exp (wTk
ht)
P(yt = k|s,θ) = K ex wTh (1)
�k=1 p ( k t)
</equation>
<bodyText confidence="0.99949">
where, ht = φ(xt) defines the transformations
of xt through the hidden layers, and wk are the
weights from the last hidden layer to the output
layer. We fit the models by minimizing the nega-
tive log likelihood (NLL) of the training data. The
NLL for the sentence s can be written as:
</bodyText>
<equation confidence="0.977359">
ytk log P(yt = k|s, θ) (2)
</equation>
<bodyText confidence="0.999366285714286">
where, ytk = I(yt = k) is an indicator variable
to encode the gold labels, i.e., ytk = 1 if the gold
label yt = k, otherwise 0.2 The loss function mini-
mizes the cross-entropy between the predicted dis-
tribution and the target distribution (i.e., gold la-
bels). The main difference between the models
described below is how they compute ht = φ(xt).
</bodyText>
<subsectionHeader confidence="0.997581">
3.1 Elman-type RNN (Elman, 1990)
</subsectionHeader>
<bodyText confidence="0.999963">
In an Elman-type RNN (Fig. 1a), the output of the
hidden layer ht at time t is computed from a non-
linear transformation of the current input xt and
the previous hidden layer output ht−1. Formally,
</bodyText>
<equation confidence="0.996952">
ht = f(Uht−1 + Vxt + b) (3)
</equation>
<bodyText confidence="0.999983">
where f is a nonlinear function (e.g., sigmoid)
applied to the hidden units. U and V are weight
matrices between two consecutive hidden layers,
and between the input and the hidden layers, re-
spectively, and b is the bias vector.
This RNN thus creates internal states by re-
membering previous hidden layer, which allows it
to exhibit dynamic temporal behavior. We can in-
terpret ht as an intermediate representation sum-
marizing the past, which is in turn used to make a
final decision on the current input.
</bodyText>
<subsectionHeader confidence="0.999433">
3.2 Jordan-type RNN (Jordan, 1997)
</subsectionHeader>
<bodyText confidence="0.9999175">
Jordan-type RNNs (Fig. 1b) are similar to Elman-
type RNNs except that the hidden layer ht at time
t is fed from the previous output layer yt−1 instead
of the previous hidden layer ht−1. Formally,
</bodyText>
<equation confidence="0.992363">
ht = f(Uyt−1 + Vxt + b) (4)
</equation>
<bodyText confidence="0.999996272727273">
where U, V , b, and f are similarly defined as be-
fore. Both Elman-type and Jordan-type RNNs are
known as simple RNNs. These types of RNNs
are generally trained using stochastic gradient de-
scent (SGD) with backpropagation through time
(BPTT), where errors (i.e., gradients) are propa-
gated back through the edges over time.
One common issue with BPTT is that as the er-
rors get propagated, they may soon become very
small or very large that can lead to undesired val-
ues in weight matrices, causing the training to fail.
</bodyText>
<footnote confidence="0.94946">
2This is also known as one-hot vector representation.
</footnote>
<equation confidence="0.9943346">
T
t=1
J(θ) =
K
k=1
</equation>
<page confidence="0.961536">
1435
</page>
<figure confidence="0.994630633333333">
The hard disk is very
(a) Elman-type RNN
Yt- 1 Yt Yt+1
W
xt- 1 xt xt+1
ht- 1 Uht ht+1
V
The hard disk is very
(b) Jordan-type RNN
xt- 1 xt xt+1
ht
U
W
V
The hard disk is very
(c) Long Short-Term Memory (LSTM) RNN
letmletmletm
Yt 1 Yt Yt 1
xt
LSTM
Input Gate Output Gate
i
x x
t
Memory Cell
ft
x
Forget Gate
ot
ht
</figure>
<figureCaption confidence="0.842066666666667">
Figure 1: Elman-type, Jordan-type and LSTM RNNs with a lookup-table layer, a hidden layer and an
output layer. The concatenated context vector for the word “disk” at time t is xt = [xhard, xdisk, xis]
with a context window of size 3. One memory block in the LSTM hidden layer has been enlarged.
</figureCaption>
<bodyText confidence="0.999895125">
This is known as the vanishing and the exploding
gradients problem (Bengio et al., 1994). One sim-
ple way to overcome this issue is to use a truncated
BPTT (Mikolov, 2012) for restricting the back-
propagation to only few steps like 4 or 5. However,
this solution limits the RNN to capture long-range
dependencies. In the following, we describe an el-
egant RNN architecture to address this problem.
</bodyText>
<equation confidence="0.891549666666667">
Yr- 1 Y t + 1
Y t
hr- 1 h t + 1
</equation>
<subsectionHeader confidence="0.99942">
3.3 Long Short-Term Memory RNN
</subsectionHeader>
<bodyText confidence="0.999400533333333">
Long Short-Term Memory or LSTM (Hochreiter
and Schmidhuber, 1997) is specifically designed
to model long term dependencies in RNNs. The
recurrent layer in a standard LSTM is constituted
with special (hidden) units called memory blocks
(Fig. 1c). A memory block is composed of four
elements: (i) a memory cell c (i.e., a neuron) with
a self-connection, (ii) an input gate i to control the
flow of input signal into the neuron, (iii) an out-
put gate o to control the effect of the neuron ac-
tivation on other neurons, and (iv) a forget gate
f to allow the neuron to adaptively reset its cur-
rent state through the self-connection. The follow-
ing sequence of equations describe how a layer of
memory blocks is updated at every time step t:
</bodyText>
<equation confidence="0.9996398">
it = a(Uiht−1 +Vixt + Cict−1 + bi)
ft = a(Ufht−1 +Vfxt + Cfct−1 + bf)
ct = it O g(Ucht−1 + Vcxt + bc) + ft O ct−1 (7)
ot = a(Uoht−1 + Voxt + Coct + bo)
ht = ot O h(ct)
</equation>
<bodyText confidence="0.999767272727273">
where Uk, Vk and Ck are the weight matrices be-
tween two consecutive hidden layers, between the
input and the hidden layers, and between two con-
secutive cell activations, respectively, which are
associated with gate k (i.e., input, output, forget
and cell), and bk is the associated bias vector. The
symbol O denotes a element-wise product of the
two vectors. The gate function a is the sigmoid
activation, and g and h are the cell input and cell
output activations, typically a tanh. LSTMs are
generally trained using truncated or full BPTT.
</bodyText>
<equation confidence="0.5254935">
x t 1 xt xt 1
c t
</equation>
<subsectionHeader confidence="0.946151">
3.4 Bidirectionality
</subsectionHeader>
<bodyText confidence="0.991056655172414">
Notice that the RNNs defined above only get infor-
mation from the past. However, information from
the future could also be crucial. In our example
sentence (Table 1), to correctly tag the word hard
as a B-TARG, it is beneficial for the RNN to know
that the next word is disk. Our window-based ap-
proach, by considering the neighboring words, al-
ready captures short-term dependencies like this
from the future. However, it requires tuning to find
the right window size, and it disregards long-range
dependencies that go beyond the context window,
which is typically of size 1 (i.e., no context) to 5
(see Section 5.2). For instance, consider the two
sentences: (i) Do you know about the crunchy tuna
here, it is to die for. and (ii) Do you know about
the crunchy tuna here, it is imported from Norway.
The phrase crunchy tuna is an aspect term in the
first (subjective) sentence, but not in the second
(objective) one. The RNN models described above
will assign the same labels to crunchy and tuna in
both sentences, since the preceding sequences and
the context window (of size 1 to 5) are the same.
To capture long-range dependencies from the
future as well as from the past, we propose to use
bidirectional RNNs (Schuster and Paliwal, 1997),
which allow bidirectional links in the network. In
an Elman-type bidirectional RNN (Fig. 2a), the
��
forward hidden layer ht and the backward hidden
</bodyText>
<equation confidence="0.8224475">
��
layer ht at time t are computed as follows:
�� ht = f( U ht_1 + �� Vxt+ b ) (10)
ht = f(�� U ht_1 + �� V xt + b ) (11)
</equation>
<page confidence="0.826941">
1436
</page>
<figure confidence="0.815251">
I
</figure>
<figureCaption confidence="0.997345333333333">
Figure 2: (a) Bidirectional Elman-type RNN and
(b) Linguistic features concatenated with the hid-
den layer output in Elman-type RNN.
</figureCaption>
<bodyText confidence="0.9872828">
→− U , →− V and →−
where b are the forward weight ma-
trices as before; and ←−U , ←−V and ←−b are their back-
ward counterparts. The concatenated vector ht =
ht, ht] is passed to the output layer. We can thus
</bodyText>
<equation confidence="0.748002333333333">
x t 1 xt xt 1
h t
x t 1 xt xt 1
</equation>
<bodyText confidence="0.9999498">
interpret ht as an intermediate representation sum-
marizing the past and the future, which is then
used to make a final decision on the current input.
Similarly, the unidirectional LSTM RNN can be
extended to bidirectional LSTM by allowing bidi-
rectional connections in the hidden layer. This
amounts to having a backward counterpart for
each of the equations from 5 to 9.
Notice that the forward and the backward com-
putations of bidirectional RNNs are independently
done until they are combined in the output layer.
This means, during training, after backpropagat-
ing the errors from the output layer to the forward
and to the backward hidden layers, two indepen-
dent BPTT can be applied – one to each direction.
</bodyText>
<subsectionHeader confidence="0.998597">
3.5 Fine-tuning of Embeddings
</subsectionHeader>
<bodyText confidence="0.9999734">
In our RNN framework, we intend to avoid manual
feature engineering efforts by using word embed-
dings as the only features. As mentioned before,
we can initialize the embeddings randomly and
learn them as part of model parameters by back-
propagating the errors to the look-up layer. One
issue with random initialization is that it may lead
the SGD to get stuck in local minima (Murphy,
2012). On the other hand, one can plug the readily
available embeddings from external sources (Sec-
tion 4) in the RNN model and use them as features
without tuning them further for the task, as is done
in any other machine learning model. However,
this approach does not exploit the automatic fea-
ture learning capability of NN models, which is
one of the main motivations of using them.
In our work, we use the pre-trained word em-
beddings to better initialize our models, and we
fine-tune them for our task in training, which turns
out to be quite beneficial (see Section 5.2).
</bodyText>
<subsectionHeader confidence="0.998116">
3.6 Incorporating other Linguistic Features
</subsectionHeader>
<bodyText confidence="0.9999886">
Although NNs learn word features (i.e., embed-
dings) automatically, we may still be interested in
incorporating other linguistic features like part-of-
speech (POS) tags and chunk information to guide
the training and to learn a better model. However,
unlike word embeddings, we want these features
to be fixed during training. As shown in Figure
2b, this can be done in our RNNs by feeding these
additional features directly to the output layer, and
learn their associated weights in training.
</bodyText>
<sectionHeader confidence="0.993625" genericHeader="method">
4 Word Embeddings
</sectionHeader>
<bodyText confidence="0.9972415">
Word embeddings are distributed representations
of words, represented as real-valued, dense, and
low-dimensional vectors. Each dimension poten-
tially describes syntactic or semantic properties of
the word. Here we briefly describe the three types
of pre-trained embeddings that we use in our work.
</bodyText>
<subsectionHeader confidence="0.979778">
4.1 SENNA Embeddings
</subsectionHeader>
<bodyText confidence="0.999990214285714">
Collobert et al. (2011) present a unified NN archi-
tecture for various NLP tasks (e.g., POS tagging,
chunking, semantic role labeling, named entity
recognition) with a window-based approach and
a sentence-based approach (i.e., the input layer
is a sentence). Each word in the input layer is
represented by M features, each of which has an
embedding vector associated with it in a lookup
table. To give their network a better initializa-
tion, they learn word embeddings using a non-
probabilistic language model, which was trained
on English Wikipedia for about 2 months. They
released their 50-dimensional word embeddings
(vocabulary size 130K) under the name SENNA.3
</bodyText>
<subsectionHeader confidence="0.981502">
4.2 Google Embeddings
</subsectionHeader>
<bodyText confidence="0.999746333333333">
Mikolov et al. (2013a) propose two log-linear
models for computing word embeddings from
large corpora efficiently: (i) a bag-of-words model
CBOW that predicts the current word based on the
context words, and (ii) a skip-gram model that pre-
dicts surrounding words given the current word.
</bodyText>
<footnote confidence="0.517532">
3http://ronan.collobert.com/senna/
</footnote>
<figure confidence="0.9953001875">
(a) (b)
The hard disk is very
hft 1 hft hf, 1
Yt 1 Yt Yt 1
hb, , hbt hbt ,
The hard disk is
ht
ht ] Uht ht 1
Yt 1 Yt Yt 1
ft I
V
W
ft
ht
ft I
very
</figure>
<page confidence="0.981444">
1437
</page>
<bodyText confidence="0.9998265">
They released their pre-trained 300-dimensional
word embeddings (vocabulary size 3M) trained
by the skip-gram model on part of Google news
dataset containing about 100 billion words.4
</bodyText>
<subsectionHeader confidence="0.998914">
4.3 Amazon Embeddings
</subsectionHeader>
<bodyText confidence="0.999985090909091">
Since we work on customer reviews, which are
less formal than Wikipedia and news, we have also
trained domain-specific embeddings (vocabulary
size 1M) using the CBOW model of word2vec
tool (Mikolov et al., 2013b) from a large cor-
pus of Amazon reviews.5 The corpus contains
34, 686, 770 reviews (4.7 billion words) on Ama-
zon products from June 1995 to March 2013
(McAuley and Leskovec, 2013). For comparison
with SENNA and Google, we learn word embed-
dings of 50- and 300-dimensions.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999838333333333">
In this section, we present our experimental set-
tings and results for the task of opinion target ex-
traction from customer reviews.
</bodyText>
<subsectionHeader confidence="0.976479">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.998466357142857">
Datasets: In our experiments, we use the two
review datasets provided by the SemEval-2014
task 4: aspect-based sentiment analysis evaluation
campaign (Pontiki et al., 2014), namely the Laptop
and the Restaurant datasets. Table 2 shows some
basic statistics about the datasets. The majority
of aspect terms have only one word, while about
one third of them have multiple words. In both
datasets, some sentences have no aspect terms and
some have more than one aspect terms. We use the
standard train:test split to compare our results with
the SemEval best systems. In addition, we show a
more general performance of our models on the
two datasets based on 10–fold cross validation.
</bodyText>
<table confidence="0.997106428571429">
Laptop Restaurant
Train Test Train Test
Sentences 3045 800 3041 800
Sentence length 15 13 14 14
One-word targets 1494 364 2786 818
Multi-word targets 864 290 907 316
Total targets 2358 654 3693 1134
</table>
<tableCaption confidence="0.999313">
Table 2: Corpora statistics.
</tableCaption>
<footnote confidence="0.9996315">
4https://code.google.com/p/word2vec/
5https://snap.stanford.edu/data/web-Amazon.html
</footnote>
<bodyText confidence="0.9981793">
Evaluation Metric: The evaluation metric mea-
sures the standard precision, recall and F1 score
based on exact matches. This means that a candi-
date aspect term is considered to be correct only if
it exactly matches with the aspect term annotated
by the human. In all our experiments when com-
paring two models, we use paired t-test on the F1
scores to measure statistical significance and re-
port the corresponding p-value.
CRF Baseline: We use a linear-chain CRF (Laf-
ferty et al., 2001) of order 2 as our baseline, which
is the state-of-the-art model for opinion target ex-
traction (Pontiki et al., 2014). Specifically, the
CRF generates (binary) feature functions of order
1 and 2; see (Cuong et al., 2014) for higher or-
der CRFs. The features used in the baseline model
include the current word, its POS tag, its prefixes
and suffixes between one to four characters, its po-
sition, its stylistics (e.g., case, digit, symbol, al-
phanumeric), and its context (i.e., the same fea-
tures for the two preceding and the two following
words). In addition to the hand-crafted features,
we also include the three different types of word
embeddings described in Section 4.
RNN Settings: We pre-processed each dataset by
lowercasing all words and spelling out each digit
number as DIGIT. We then built the vocabulary
from the training set by marking rare words with
only one occurrence as UNKNOWN, and adding a
PADDDING word to make context windows for
boundary words.
To implement early stopping in SGD, we prepared
a validation set by separating out randomly 10%
of the available training data. The remaining 90%
is used for training. The weights in the network
were initialized by sampling from a small random
uniform distribution U(−0.2,0.2). The time step
in the truncated BPTT was fixed to 6 based on the
performance on the validation set; smaller values
hurt the performance, while larger values showed
no significant gains but increased the training time.
We use a fixed learning rate of 0.01, but we change
the batch size depending on the sentence length
following Mesnil et al. (2013). The net effect is a
variable step size in the SGD. We run SGD for 30
epochs, calculate the F1 score on the validation set
after each epoch, and stop if the accuracy starts to
decrease. The size of the context window and the
hidden layer are empirically set based on the per-
formance on the validation set. We experimented
</bodyText>
<page confidence="0.985278">
1438
</page>
<bodyText confidence="0.999733">
with the window size E 11, 3, 5}, and found 3 to be
the optimal on the validation set. The hidden layer
sizes we experimented with are 50, 100, 150, and
200; we report the optimal values in Table 3 (see
|hl |and |hr |columns).
Linguistic Features in RNNs: In addition to the
neural features, we also explore the contribution
of simple linguistic features in our RNN mod-
els using the architecture described in Section
3.6. Specifically, we encode four POS tag classes
(noun, adjective, verb, adverb) and BIO-tagged
chunk information (NP, VP, PP, ADJP, ADVP) as
binary features. We feed these extra features di-
rectly to the output layer of the RNNs and learn
their relative weights. Part-of-speech and phrasal
information are arguably the most informative
features for identifying aspect terms (i.e., aspect
terms are generally noun phrases). BIO tags could
be useful to find the right text spans (i.e., aspect
terms are unlikely to violate phrasal boundaries).
</bodyText>
<subsectionHeader confidence="0.92655">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.996447518518519">
Table 3 presents our results of aspect term extrac-
tion on the standard testset in F1 scores. In Table
4, we show the results on the whole datasets based
on 10-fold cross validation. RNNs in Table 4 are
trained using SENNA embeddings. We perform
significance tests on the 10-fold results. In the fol-
lowing, we highlight our main findings.
Contributions of Word Embeddings in CRF:
From the first group of results in Table 3, we
can observe that even though CRF uses a hand-
ful of hand-designed features, including word
embeddings still leads to sizable improvements
on both datasets. The domain-specific Amazon
embeddings (300 dim.) yield more general
performance across the datasets, delivering the
best gain of absolute 3.54% on the Laptop and
the second best on the Restaurant dataset. Google
embeddings give the best gain on the Restaurant
dataset (absolute 3.08%). The contribution of
embeddings in CRF is also validated by the
10-fold results in Table 4 (see first two rows),
where SENNA embeddings yield significant
improvements – absolute 1.47% on Laptop (p
&lt; 0.03) and absolute 1.24% on Restaurant (p &lt;
0.01). This demonstrates that word embeddings
offer generalizations that complement other strong
features, and thus should be considered.
</bodyText>
<table confidence="0.999941185185185">
System Dim. |hl |Laptop |hr |Restaurant
CRF Base - - 68.66 - 77.28
+SENNA 50 - 71.38 - 78.54
+Amazon 50 - 70.61 - 79.46
+Google 300 - 68.81 - 80.36
+Amazon 300 - 72.20 - 79.66
Jordan-RNN
+SENNA 50 200 71.41 200 78.83
+Amazon 50 100 73.21 150 79.01
+Google 300 150 73.42 200 79.89
+Amazon 300 50 72.43 200 78.30
Elman-RNN
+SENNA 50 100 73.86 150 79.89
+Amazon 50 100 74.43 100 80.37
+Google 300 100 72.91 100 79.54
+Amazon 300 200 73.67 100 79.82
Elman-RNN + Feat.
+SENNA 50 50 73.70 100 81.36
+Amazon 50 200 73.30 50 81.66
+Google 300 150 74.25 100 80.57
+Amazon 300 50 73.92 100 80.24
Bi-Elman-RNN
+SENNA 50 100 72.38 100 80.10
+Amazon 50 50 73.93 50 79.97
+Google 300 50 72.67 100 79.52
+Amazon 300 50 71.12 50 79.09
Bi-Elman-RNN + Feat.
+SENNA 50 100 73.30 50 80.34
+Amazon 50 50 74.57 50 82.06
+Google 300 50 74.56 100 78.99
+Amazon 300 50 73.56 100 79.97
LSTM-RNN
+SENNA 50 100 73.40 150 79.43
+Amazon 50 50 72.44 50 79.79
+Google 300 100 72.11 50 79.20
+Amazon 300 50 73.52 50 78.99
LSTM-RNN + Feat.
+SENNA 50 50 73.19 150 80.28
+Amazon 50 100 75.00 50 80.82
+Google 300 50 72.19 50 81.37
+Amazon 300 100 72.85 100 80.60
Bi-LSTM-RNN
+SENNA 50 50 72.60 150 79.89
+Amazon 50 100 74.03 100 79.36
+Google 300 50 70.90 50 78.80
+Amazon 300 150 71.25 150 78.88
Bi-LSTM-RNN + Feat.
+SENNA 50 100 74.02 150 81.06
+Amazon 50 100 73.58 50 80.51
+Google 300 100 71.05 50 79.39
+Amazon 300 100 73.81 150 80.67
SemEval-14 top systems
IHS RD - - 74.55 - 79.62
DLIREC - - 73.78 - 84.01
</table>
<tableCaption confidence="0.866203">
Table 3: F1-score performance for CRF baselines,
RNNs and SemEval’14 best systems on the stan-
dard Laptop and Restaturant testsets. |hl |and |hr|
columns show the number of hidden units.
</tableCaption>
<footnote confidence="0.350147">
CRF vs. RNNs: When we compare the results of
</footnote>
<page confidence="0.808132">
1439
</page>
<table confidence="0.99982375">
Model Laptop Restaurant
P R F1 P R F1
CRF Base 79.77 64.09 70.87 82.59 74.63 78.36
+ SENNA 78.23 67.38 72.34 81.21 78.12 79.60
Elman-RNN 82.03 72.68 76.97 81.96 78.41 80.08
+ Feat. 80.02 76.60 78.22 81.91 81.22 81.52
+ Bidir. 81.92 73.70 77.47 81.69 78.46 79.97
+ Feat. + Bidir. 81.00 75.70 78.17 82.80 80.44 81.57
LSTM-RNN 81.92 73.30 77.14 83.64 77.45 80.36
+ Feat. 80.70 75.82 78.00 81.80 81.39 81.54
+ Bidir. 81.31 74.20 77.37 81.66 79.23 80.37
+ Feat. + Bidir. 80.81 74.48 77.27 82.96 80.42 81.56
</table>
<tableCaption confidence="0.9651845">
Table 4: 10-fold cross validation results of the models on the two datasets. Elman- and LSTM-RNNs are
trained using SENNA embeddings.
</tableCaption>
<table confidence="0.999931076923077">
System Dim. Laptop Restaurant
Elman-RNN -tune +tune -tune +tune
+SENNA 50 60.85 73.86 75.78 79.89
+Amazon 50 15.51 74.43 22.85 80.37
+Random 50 38.26 72.99 56.98 78.44
+Google 300 67.91 72.91 74.73 79.54
+Amazon 300 15.51 73.67 22.85 79.82
Jordan-RNN -tune +tune -tune +tune
+SENNA 50 58.81 71.41 74.68 78.83
+Amazon 50 15.51 73.21 22.85 79.01
+Random 50 38.05 71.46 55.65 77.38
+Google 300 69.39 73.42 77.33 79.89
+Amazon 300 15.51 72.43 22.85 78.30
</table>
<tableCaption confidence="0.99959">
Table 5: Effects of fine-tuning in Elman-RNN and Jordan-RNN.
</tableCaption>
<bodyText confidence="0.999888136363636">
RNNs with those of CRF in Table 3, we see that
most of our RNN models outperform CRF mod-
els with the maximum absolute gains of 2.80% by
LSTM-RNN+Feat. on Laptop and 1.70% by Bi-
Elman-RNN+Feat. on Restaurant. What is re-
markable is that RNNs without any hand-crafted
features outperform feature-rich CRF models by a
good margin – absolute maximum gains of 2.23%
by Elman-RNN and 1.83% by Bi-LSTM-RNN
on Laptop. When we compare their general per-
formance on the 10-folds in Table 4, we observe
similar gains, maximum 5.88% on Laptop and
1.97% on Restaurant, which are significant with
p &lt; 6 × 10−6 on Laptop and p &lt; 2 × 10−4 on
Restaurant. These results demonstrate that RNNs
as sequence labelers are more effective than CRFs
for fine-grained opinion mining tasks. This can be
attributed to RNN’s ability to learn better features
automatically and to capture long-range sequential
dependencies between the output labels.
Comparison among RNN Models: A compari-
son among the RNN models in Table 3 tells that
Elman RNN generally outperforms Jordan RNN.
However, bi-directionality and LSTM do not pro-
vide clear gains over the simple Elman RNN.
In fact, bi-directionality hurts the performance
in most cases. This finding contrasts the find-
ing of Irsoy and Cardie (2014) in opinion ex-
pression detection task, where bi-directional El-
man RNNs outperform their uni-directional coun-
terparts. However, when we analyzed the data,
we found it to be unsurprising because aspect
terms are generally shorter than opinion expres-
sions. For example, the average length of an aspect
term in our Restaurant dataset is 1.4, where the
average length of an expressive subjective expres-
sion in their MPQA corpus is 3.3. Therefore, the
information required to correctly identify aspect
terms (e.g., hard disk) is already captured by the
simple (as opposed to LSTM) unidirectional link
and the context window covering the neighboring
words. LSTM and Bi-directionality increase the
number of parameters in the RNNs, which might
contribute to overfitting on this specific task.6
</bodyText>
<footnote confidence="0.941517">
6Bi-directional links double the number of parameters in
RNNs.
</footnote>
<page confidence="0.990619">
1440
</page>
<bodyText confidence="0.995267434782609">
As a partial solution to this problem, we experi-
mented with a bi-directional Elman-RNN, where
both directions share the same parameters. There-
fore, the number of parameters remains the same
as the uni-directional one. This modification im-
proves the performance over the non-shared one
slightly but not significantly. This demands for
better modeling of the two sources of information
rather than simple concatenation or sharing.
Contributions of Linguistic Features in RNNs:
Although our linguistic features are quite simple
(i.e., POS tags and chunk), they give gains on
both datasets when incorporated into Elman and
LSTM RNNs. The maximum gains on the stan-
dard testset (Table 3) are 0.64% on Laptop and
1.96% on Restaurant for Bi-Elman, and 1.48%
on Laptop and 1.58% on Restaurant for LSTM.
Similar gains are also observed on the 10-folds
in Table 4, where the maximum gains are 1.25%
on Laptop and 1.44% on Restaurant. These gains
are significant with p &lt; 0.004 on Laptop and p &lt;
6 × 10−5 on Restaurant. Linguistic features thus
complement word embeddings in RNNs.
</bodyText>
<subsectionHeader confidence="0.793429">
Importance of Fine-tuning in RNNs: Finally,
</subsectionHeader>
<bodyText confidence="0.9944485">
in order to show the importance of fine-tuning
the word embeddings in RNNs on our task, we
present in Table 5 the performance of Elman and
Jordan RNNs, when the embeddings are used as
they are (‘-tune’), and when they are fine-tuned
(‘+tune’) on the task. The table also shows
the contributions of pre-trained embeddings as
compared to random initialization. Surprisingly,
Amazon embeddings without fine-tuning deliver
the worst performance, even lower than the Ran-
dom initialization. We found that with Amazon
embeddings the network gets stuck in a local
minimum from the very first epoch.
Other pre-trained (untuned) embeddings improve
over the Amazon and Random by providing better
initialization. In most cases fine-tuning makes a
big difference. For example, the absolute gains for
fine-tuning SENNA embeddings in Elman RNN
are 13.01% in Laptop and 4.11% in Restaurant.
Remarkably, fine-tuning brings both Random and
Amazon embeddings close to the best ones.
Comparison with SemEval-2014 Systems:
When our RNN results are compared with the
top performing systems in the SemEval-2014
(last two rows in Table 3), we see that RNNs
without using any linguistic features achieve the
second best results on both Laptop and Restaurant
datasets. Note that these RNNs only use word
embeddings, while IHS RD and DLIREC use
complex features like dependency relations,
named entity, sentiment orientation of words,
word cluster and many more in their CRF models,
most of which are expensive to compute; see
(Toh and Wang, 2014; Chernyshevich, 2014).
The performance of our RNNs improves when
they are given access to very simple features like
POS tags and chunks, and LSTM-RNN+Feat.
achieves the best results on the Laptop dataset.
</bodyText>
<sectionHeader confidence="0.997007" genericHeader="conclusions">
6 Conclusion and Future Direction
</sectionHeader>
<bodyText confidence="0.999950862068965">
We presented a general class of discriminative
models based on recurrent neural network (RNN)
architecture and word embeddings, that can be
successfully applied to fine-grained opinion min-
ing tasks without any task-specific manual feature
engineering effort. We used pre-trained word em-
beddings from three external sources in different
RNN architectures including Elman-type, Jordan-
type, LSTM and their several variations.
Our results on the opinion target extraction task
demonstrate that word embeddings improve the
performance of both CRF and RNN models, how-
ever, fine-tuning them in RNNs on the task gives
the best results. RNNs outperform CRFs, even
when they use word embeddings as the only fea-
tures. Incorporating simple linguistic features into
RNNs improves the performance further. Our
best results with LSTM RNN outperform the top
performing system on the Laptop dataset and
achieve the second best on the Restaurant dataset
in SemEval-2014 evaluation campaign. We made
our code publicly available for research purposes.
In the future, we would like apply our models
to other fine-grained opinion mining tasks includ-
ing opinion expression detection and characteriz-
ing the intensity and sentiment of the opinion ex-
pressions. We would also like to explore to what
extent these tasks can be jointly modeled in an
RNN-based multi-task learning framework.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9987238">
We are grateful to the anonymous reviewers for
their insightful comments and suggestions to im-
prove the paper. This research is affiliated with the
Stanley Ho Big Data Decision Analytics Research
Centre of The Chinese University of Hong Kong.
</bodyText>
<page confidence="0.990129">
1441
</page>
<sectionHeader confidence="0.9825" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681352380952">
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gradi-
ent descent is difficult. IEEE Transactions on Neu-
ral Networks, 5(2):157–166.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of the 20th International Joint Conference
on Artifical Intelligence, pages 2683–2688. Morgan
Kaufmann Publishers Inc.
Maryna Chernyshevich. 2014. IHS R&amp;D Belarus:
Cross-domain extraction of product features using
conditional random fields. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), page 309.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their
attributes. In Proceedings of the ACL 2010 Confer-
ence Short Papers, pages 269–274. ACL.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In Proceedings of HLT/EMNLP, pages 355–
362. ACL.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Nguyen Viet Cuong, Nan Ye, Wee Sun Lee, and
Hai Leong Chieu. 2014. Conditional random field
with high-order dependencies for sequence labeling
and segmentation. The Journal of Machine Learn-
ing Research, 15(1):981–1009.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Alex Graves and Navdeep Jaitly. 2014. Towards end-
to-end speech recognition with recurrent neural net-
works. In Proceedings of ICML, pages 1764–1772.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
SIGKDD, pages 168–177. ACM.
Ozan Irsoy and Claire Cardie. 2014. Opinion mining
with deep recurrent neural networks. In Proceedings
of EMNLP, pages 720–728.
Wei Jin, Hung Hay Ho, and Rohini K Srihari. 2009. A
novel lexicalized HMM-based learning framework
for web opinion mining. In Proceedings of ICML,
pages 465–472. Citeseer.
Michael I Jordan. 1997. Serial order: A parallel dis-
tributed processing approach. Advances in psychol-
ogy, 121:471–495.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In Proceedings of ICML, pages
282–289.
Phong Le and Willem Zuidema. 2015. Compositional
distributional semantics with long short term mem-
ory. In Proceedings of the joint Conference on Lexi-
cal and Computational Semantics (*SEM).
R´emi Lebret and Ronan Lebret. 2013. Word emded-
dings through Hellinger PCA. arXiv preprint
arXiv:1312.5542.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of CIKM, pages 375–384. ACM.
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165–172. ACM.
Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Proceedings of IN-
TERSPEECH, pages 3771–3775.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of INTERSPEECH, pages 1045–1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Tomas Mikolov, 2012. Statistical Language Models
based on Neural Networks. PhD thesis, Brno Uni-
versity of Technology.
Samaneh Moghaddam and Martin Ester. 2012. On
the design of LDA models for aspect-based opinion
mining. In Proceedings of CIKM, pages 803–812.
ACM.
Kevin Murphy. 2012. Machine Learning A Probabilis-
tic Perspective. The MIT Press.
</reference>
<page confidence="0.866157">
1442
</page>
<reference confidence="0.999904661016949">
Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis,
Ion Androutsopoulos, John Pavlopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), pages 27–35.
Hasim Sak, Andrew Senior, and Franc¸oise Beaufays.
2014. Long short-term memory recurrent neural
network architectures for large scale acoustic model-
ing. In Proceedings of INTERSPEECH, pages 338–
342.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.
Shabnam Shariaty and Samaneh Moghaddam. 2011.
Fine-grained opinion mining using conditional ran-
dom fields. In International Conference on
Data Mining Workshops (ICDMW), pages 109–114.
IEEE.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, pages 1631–
1642. Citeseer.
Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.
2012. LSTM neural networks for language model-
ing. In Proceedings of INTERSPEECH, pages 194–
197.
Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of WWW, pages 111–120. ACM.
Zhiqiang Toh and Wenting Wang. 2014. DLIREC:
Aspect term extraction and term polarity classifica-
tion system. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
page 235.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of ACL, pages 384–394. ACL.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLT/EMNLP, pages 347–354. ACL.
Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-Markov conditional ran-
dom fields. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1335–1345. ACL.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics, pages 1640–1649. ACL.
</reference>
<page confidence="0.978827">
1443
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595661">
<title confidence="0.9972385">Fine-grained Opinion Mining Recurrent Neural Networks and Word Embeddings</title>
<author confidence="0.905243">Shafiq</author>
<author confidence="0.905243">Helen</author>
<affiliation confidence="0.885996333333333">of Systems Engineering and Engineering The Chinese University of Hong Kong, Hong Kong SAR, Computing Research Institute - HBKU, Doha, Qatar</affiliation>
<email confidence="0.977356">sjoty@qf.org.qa</email>
<abstract confidence="0.989040388888889">The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task. We propose a general class of discriminative models based on recurrent neural networks (RNNs) and word embeddings that can be successfully applied to such tasks without any taskspecific feature engineering effort. Our experimental results on the task of opinion target identification show that RNNs, without using any hand-crafted features, outperform feature-rich CRF-based models. Our framework is flexible, allows us to incorporate other linguistic features, and achieves results that rival the top performing systems in SemEval-2014.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Patrice Simard</author>
<author>Paolo Frasconi</author>
</authors>
<title>Learning long-term dependencies with gradient descent is difficult.</title>
<date>1994</date>
<journal>IEEE Transactions on Neural Networks,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="13594" citStr="Bengio et al., 1994" startWordPosition="2241" endWordPosition="2244"> Uht ht+1 V The hard disk is very (b) Jordan-type RNN xt- 1 xt xt+1 ht U W V The hard disk is very (c) Long Short-Term Memory (LSTM) RNN letmletmletm Yt 1 Yt Yt 1 xt LSTM Input Gate Output Gate i x x t Memory Cell ft x Forget Gate ot ht Figure 1: Elman-type, Jordan-type and LSTM RNNs with a lookup-table layer, a hidden layer and an output layer. The concatenated context vector for the word “disk” at time t is xt = [xhard, xdisk, xis] with a context window of size 3. One memory block in the LSTM hidden layer has been enlarged. This is known as the vanishing and the exploding gradients problem (Bengio et al., 1994). One simple way to overcome this issue is to use a truncated BPTT (Mikolov, 2012) for restricting the backpropagation to only few steps like 4 or 5. However, this solution limits the RNN to capture long-range dependencies. In the following, we describe an elegant RNN architecture to address this problem. Yr- 1 Y t + 1 Y t hr- 1 h t + 1 3.3 Long Short-Term Memory RNN Long Short-Term Memory or LSTM (Hochreiter and Schmidhuber, 1997) is specifically designed to model long term dependencies in RNNs. The recurrent layer in a standard LSTM is constituted with special (hidden) units called memory bl</context>
</contexts>
<marker>Bengio, Simard, Frasconi, 1994</marker>
<rawString>Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artifical Intelligence,</booktitle>
<pages>2683--2688</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="6395" citStr="Breck et al., 2007" startWordPosition="980" endWordPosition="983"> Restaurant dataset in SemEval-2014. We make our source code available.1 In the remainder of this paper, after discussing related work in Section 2, we present our RNN models in Section 3. In Section 4, we briefly describe the pre-trained word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, pages 2683–2688. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maryna Chernyshevich</author>
</authors>
<title>IHS R&amp;D Belarus: Cross-domain extraction of product features using conditional random fields.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014),</booktitle>
<pages>309</pages>
<contexts>
<context position="35188" citStr="Chernyshevich, 2014" startWordPosition="5919" endWordPosition="5920"> Amazon embeddings close to the best ones. Comparison with SemEval-2014 Systems: When our RNN results are compared with the top performing systems in the SemEval-2014 (last two rows in Table 3), we see that RNNs without using any linguistic features achieve the second best results on both Laptop and Restaurant datasets. Note that these RNNs only use word embeddings, while IHS RD and DLIREC use complex features like dependency relations, named entity, sentiment orientation of words, word cluster and many more in their CRF models, most of which are expensive to compute; see (Toh and Wang, 2014; Chernyshevich, 2014). The performance of our RNNs improves when they are given access to very simple features like POS tags and chunks, and LSTM-RNN+Feat. achieves the best results on the Laptop dataset. 6 Conclusion and Future Direction We presented a general class of discriminative models based on recurrent neural network (RNN) architecture and word embeddings, that can be successfully applied to fine-grained opinion mining tasks without any task-specific manual feature engineering effort. We used pre-trained word embeddings from three external sources in different RNN architectures including Elman-type, Jordan</context>
</contexts>
<marker>Chernyshevich, 2014</marker>
<rawString>Maryna Chernyshevich. 2014. IHS R&amp;D Belarus: Cross-domain extraction of product features using conditional random fields. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), page 309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Hierarchical sequential learning for extracting opinions and their attributes.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>269--274</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6632" citStr="Choi and Cardie, 2010" startWordPosition="1019" endWordPosition="1022">ined word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type</context>
</contexts>
<marker>Choi, Cardie, 2010</marker>
<rawString>Yejin Choi and Claire Cardie. 2010. Hierarchical sequential learning for extracting opinions and their attributes. In Proceedings of the ACL 2010 Conference Short Papers, pages 269–274. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>355--362</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6580" citStr="Choi et al., 2005" startWordPosition="1011" endWordPosition="1014">3. In Section 4, we briefly describe the pre-trained word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam a</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In Proceedings of HLT/EMNLP, pages 355– 362. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4057" citStr="Collobert and Weston, 2008" startWordPosition="611" endWordPosition="614">oy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston, 2008; Mikolov et al., 2013b) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems (Turian et al., 2010; Lebret and Lebret, 2013), and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words (Mesnil et al., 2013; Irsoy and Cardie, 2014). Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks with</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of ICML, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="10271" citStr="Collobert et al., 2011" startWordPosition="1627" endWordPosition="1630">dding vectors (see Section 4). Given an input sentence s = (s1, · · · , sT), we first transform it into a feature sequence by mapping each word token st E s to an index in L. The look-up layer then creates a context vector xt E RmD covering m − 1 neighboring tokens for each st by concatenating their respective vectors in L. For example, given the context size m = 3, the context vector xt for the word disk in Figure 1 is formed by concatenating the embeddings of hard, disk and is. This window-based approach is intended to capture short-term dependencies between neighboring words in a sentence (Collobert et al., 2011). The concatenated vector is then passed through non-linear recurrent hidden layers to learn highlevel compositional representations, which are in turn fed to the output layer for classification using softmax. Formally, the probability of k-th label in the output for classification into K classes: exp (wTk ht) P(yt = k|s,θ) = K ex wTh (1) �k=1 p ( k t) where, ht = φ(xt) defines the transformations of xt through the hidden layers, and wk are the weights from the last hidden layer to the output layer. We fit the models by minimizing the negative log likelihood (NLL) of the training data. The NLL</context>
<context position="19931" citStr="Collobert et al. (2011)" startWordPosition="3370" endWordPosition="3373">tter model. However, unlike word embeddings, we want these features to be fixed during training. As shown in Figure 2b, this can be done in our RNNs by feeding these additional features directly to the output layer, and learn their associated weights in training. 4 Word Embeddings Word embeddings are distributed representations of words, represented as real-valued, dense, and low-dimensional vectors. Each dimension potentially describes syntactic or semantic properties of the word. Here we briefly describe the three types of pre-trained embeddings that we use in our work. 4.1 SENNA Embeddings Collobert et al. (2011) present a unified NN architecture for various NLP tasks (e.g., POS tagging, chunking, semantic role labeling, named entity recognition) with a window-based approach and a sentence-based approach (i.e., the input layer is a sentence). Each word in the input layer is represented by M features, each of which has an embedding vector associated with it in a lookup table. To give their network a better initialization, they learn word embeddings using a nonprobabilistic language model, which was trained on English Wikipedia for about 2 months. They released their 50-dimensional word embeddings (voca</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nguyen Viet Cuong</author>
<author>Nan Ye</author>
<author>Wee Sun Lee</author>
<author>Hai Leong Chieu</author>
</authors>
<title>Conditional random field with high-order dependencies for sequence labeling and segmentation.</title>
<date>2014</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="23624" citStr="Cuong et al., 2014" startWordPosition="3976" endWordPosition="3979"> F1 score based on exact matches. This means that a candidate aspect term is considered to be correct only if it exactly matches with the aspect term annotated by the human. In all our experiments when comparing two models, we use paired t-test on the F1 scores to measure statistical significance and report the corresponding p-value. CRF Baseline: We use a linear-chain CRF (Lafferty et al., 2001) of order 2 as our baseline, which is the state-of-the-art model for opinion target extraction (Pontiki et al., 2014). Specifically, the CRF generates (binary) feature functions of order 1 and 2; see (Cuong et al., 2014) for higher order CRFs. The features used in the baseline model include the current word, its POS tag, its prefixes and suffixes between one to four characters, its position, its stylistics (e.g., case, digit, symbol, alphanumeric), and its context (i.e., the same features for the two preceding and the two following words). In addition to the hand-crafted features, we also include the three different types of word embeddings described in Section 4. RNN Settings: We pre-processed each dataset by lowercasing all words and spelling out each digit number as DIGIT. We then built the vocabulary from</context>
</contexts>
<marker>Cuong, Ye, Lee, Chieu, 2014</marker>
<rawString>Nguyen Viet Cuong, Nan Ye, Wee Sun Lee, and Hai Leong Chieu. 2014. Conditional random field with high-order dependencies for sequence labeling and segmentation. The Journal of Machine Learning Research, 15(1):981–1009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="11315" citStr="Elman, 1990" startWordPosition="1817" endWordPosition="1818">, and wk are the weights from the last hidden layer to the output layer. We fit the models by minimizing the negative log likelihood (NLL) of the training data. The NLL for the sentence s can be written as: ytk log P(yt = k|s, θ) (2) where, ytk = I(yt = k) is an indicator variable to encode the gold labels, i.e., ytk = 1 if the gold label yt = k, otherwise 0.2 The loss function minimizes the cross-entropy between the predicted distribution and the target distribution (i.e., gold labels). The main difference between the models described below is how they compute ht = φ(xt). 3.1 Elman-type RNN (Elman, 1990) In an Elman-type RNN (Fig. 1a), the output of the hidden layer ht at time t is computed from a nonlinear transformation of the current input xt and the previous hidden layer output ht−1. Formally, ht = f(Uht−1 + Vxt + b) (3) where f is a nonlinear function (e.g., sigmoid) applied to the hidden units. U and V are weight matrices between two consecutive hidden layers, and between the input and the hidden layers, respectively, and b is the bias vector. This RNN thus creates internal states by remembering previous hidden layer, which allows it to exhibit dynamic temporal behavior. We can interpre</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Navdeep Jaitly</author>
</authors>
<title>Towards endto-end speech recognition with recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>1764--1772</pages>
<contexts>
<context position="7431" citStr="Graves and Jaitly, 2014" startWordPosition="1143" endWordPosition="1146">urring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children. Most relevant to our work is the recent work of Irsoy and Cardie (2014), where they ap</context>
</contexts>
<marker>Graves, Jaitly, 2014</marker>
<rawString>Alex Graves and Navdeep Jaitly. 2014. Towards endto-end speech recognition with recurrent neural networks. In Proceedings of ICML, pages 1764–1772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="14029" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="2322" endWordPosition="2325">rd, xdisk, xis] with a context window of size 3. One memory block in the LSTM hidden layer has been enlarged. This is known as the vanishing and the exploding gradients problem (Bengio et al., 1994). One simple way to overcome this issue is to use a truncated BPTT (Mikolov, 2012) for restricting the backpropagation to only few steps like 4 or 5. However, this solution limits the RNN to capture long-range dependencies. In the following, we describe an elegant RNN architecture to address this problem. Yr- 1 Y t + 1 Y t hr- 1 h t + 1 3.3 Long Short-Term Memory RNN Long Short-Term Memory or LSTM (Hochreiter and Schmidhuber, 1997) is specifically designed to model long term dependencies in RNNs. The recurrent layer in a standard LSTM is constituted with special (hidden) units called memory blocks (Fig. 1c). A memory block is composed of four elements: (i) a memory cell c (i.e., a neuron) with a self-connection, (ii) an input gate i to control the flow of input signal into the neuron, (iii) an output gate o to control the effect of the neuron activation on other neurons, and (iv) a forget gate f to allow the neuron to adaptively reset its current state through the self-connection. The following sequence of equations des</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGKDD,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6861" citStr="Hu and Liu, 2004" startWordPosition="1055" endWordPosition="1058">mining focused on detecting opinion (subjective) 1https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of SIGKDD, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Opinion mining with deep recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>720--728</pages>
<contexts>
<context position="3451" citStr="Irsoy and Cardie (2014)" startWordPosition="524" endWordPosition="527">uite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston</context>
<context position="8016" citStr="Irsoy and Cardie (2014)" startWordPosition="1240" endWordPosition="1243">h recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children. Most relevant to our work is the recent work of Irsoy and Cardie (2014), where they apply deep Elman-type RNN to extract opinion expressions and show that deep RNN outperforms CRF, semiCRF and shallow RNN. They used word embeddings from Google without fine-tuning them. Although inspired, our work differs from the work of Irsoy and Cardie (2014) in many ways. (i) We experiment with not only Elman-type, but also with a Jordan-type and with a more advanced LSTM RNN, and demonstrate the performance of various RNN models. (ii) We use not only Google embeddings as pre-trained word vectors, but also other embeddings including SENNA and Amazon, and show their performance</context>
<context position="31680" citStr="Irsoy and Cardie (2014)" startWordPosition="5363" endWordPosition="5366">ese results demonstrate that RNNs as sequence labelers are more effective than CRFs for fine-grained opinion mining tasks. This can be attributed to RNN’s ability to learn better features automatically and to capture long-range sequential dependencies between the output labels. Comparison among RNN Models: A comparison among the RNN models in Table 3 tells that Elman RNN generally outperforms Jordan RNN. However, bi-directionality and LSTM do not provide clear gains over the simple Elman RNN. In fact, bi-directionality hurts the performance in most cases. This finding contrasts the finding of Irsoy and Cardie (2014) in opinion expression detection task, where bi-directional Elman RNNs outperform their uni-directional counterparts. However, when we analyzed the data, we found it to be unsurprising because aspect terms are generally shorter than opinion expressions. For example, the average length of an aspect term in our Restaurant dataset is 1.4, where the average length of an expressive subjective expression in their MPQA corpus is 3.3. Therefore, the information required to correctly identify aspect terms (e.g., hard disk) is already captured by the simple (as opposed to LSTM) unidirectional link and t</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Opinion mining with deep recurrent neural networks. In Proceedings of EMNLP, pages 720–728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Jin</author>
<author>Hung Hay Ho</author>
<author>Rohini K Srihari</author>
</authors>
<title>A novel lexicalized HMM-based learning framework for web opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>465--472</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6937" citStr="Jin et al., 2009" startWordPosition="1067" endWordPosition="1070">opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Soche</context>
</contexts>
<marker>Jin, Ho, Srihari, 2009</marker>
<rawString>Wei Jin, Hung Hay Ho, and Rohini K Srihari. 2009. A novel lexicalized HMM-based learning framework for web opinion mining. In Proceedings of ICML, pages 465–472. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Jordan</author>
</authors>
<title>Serial order: A parallel distributed processing approach.</title>
<date>1997</date>
<booktitle>Advances in psychology,</booktitle>
<pages>121--471</pages>
<contexts>
<context position="12079" citStr="Jordan, 1997" startWordPosition="1953" endWordPosition="1954">e previous hidden layer output ht−1. Formally, ht = f(Uht−1 + Vxt + b) (3) where f is a nonlinear function (e.g., sigmoid) applied to the hidden units. U and V are weight matrices between two consecutive hidden layers, and between the input and the hidden layers, respectively, and b is the bias vector. This RNN thus creates internal states by remembering previous hidden layer, which allows it to exhibit dynamic temporal behavior. We can interpret ht as an intermediate representation summarizing the past, which is in turn used to make a final decision on the current input. 3.2 Jordan-type RNN (Jordan, 1997) Jordan-type RNNs (Fig. 1b) are similar to Elmantype RNNs except that the hidden layer ht at time t is fed from the previous output layer yt−1 instead of the previous hidden layer ht−1. Formally, ht = f(Uyt−1 + Vxt + b) (4) where U, V , b, and f are similarly defined as before. Both Elman-type and Jordan-type RNNs are known as simple RNNs. These types of RNNs are generally trained using stochastic gradient descent (SGD) with backpropagation through time (BPTT), where errors (i.e., gradients) are propagated back through the edges over time. One common issue with BPTT is that as the errors get p</context>
</contexts>
<marker>Jordan, 1997</marker>
<rawString>Michael I Jordan. 1997. Serial order: A parallel distributed processing approach. Advances in psychology, 121:471–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2816" citStr="Lafferty et al., 2001" startWordPosition="424" endWordPosition="427">opinion expression (EXPR tags) extraction. label each word in a sentence using the conventional BIO tagging scheme. For example, Table 1 shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion expression (bottom row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. F</context>
<context position="23404" citStr="Lafferty et al., 2001" startWordPosition="3938" endWordPosition="3942">rgets 2358 654 3693 1134 Table 2: Corpora statistics. 4https://code.google.com/p/word2vec/ 5https://snap.stanford.edu/data/web-Amazon.html Evaluation Metric: The evaluation metric measures the standard precision, recall and F1 score based on exact matches. This means that a candidate aspect term is considered to be correct only if it exactly matches with the aspect term annotated by the human. In all our experiments when comparing two models, we use paired t-test on the F1 scores to measure statistical significance and report the corresponding p-value. CRF Baseline: We use a linear-chain CRF (Lafferty et al., 2001) of order 2 as our baseline, which is the state-of-the-art model for opinion target extraction (Pontiki et al., 2014). Specifically, the CRF generates (binary) feature functions of order 1 and 2; see (Cuong et al., 2014) for higher order CRFs. The features used in the baseline model include the current word, its POS tag, its prefixes and suffixes between one to four characters, its position, its stylistics (e.g., case, digit, symbol, alphanumeric), and its context (i.e., the same features for the two preceding and the two following words). In addition to the hand-crafted features, we also incl</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>Compositional distributional semantics with long short term memory.</title>
<date>2015</date>
<booktitle>In Proceedings of the joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="7772" citStr="Zuidema (2015)" startWordPosition="1200" endWordPosition="1201"> He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children. Most relevant to our work is the recent work of Irsoy and Cardie (2014), where they apply deep Elman-type RNN to extract opinion expressions and show that deep RNN outperforms CRF, semiCRF and shallow RNN. They used word embeddings from Google without fine-tuning them. Although inspired, our work differs from the work of Irsoy and Cardie (2014) in many ways. (i) We experiment with not only Elman-type, but also with a Jordan</context>
</contexts>
<marker>Zuidema, 2015</marker>
<rawString>Phong Le and Willem Zuidema. 2015. Compositional distributional semantics with long short term memory. In Proceedings of the joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R´emi Lebret</author>
<author>Ronan Lebret</author>
</authors>
<title>Word emdeddings through Hellinger PCA. arXiv preprint arXiv:1312.5542.</title>
<date>2013</date>
<contexts>
<context position="4272" citStr="Lebret and Lebret, 2013" startWordPosition="648" endWordPosition="651">compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston, 2008; Mikolov et al., 2013b) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems (Turian et al., 2010; Lebret and Lebret, 2013), and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words (Mesnil et al., 2013; Irsoy and Cardie, 2014). Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific feature engineering effort. We experiment with several important RNN architectures including Elman-RNN, Jordan-RNN, long short term memory (LSTM) and their variations. We acquire pre-trained wor</context>
</contexts>
<marker>Lebret, Lebret, 2013</marker>
<rawString>R´emi Lebret and Ronan Lebret. 2013. Word emdeddings through Hellinger PCA. arXiv preprint arXiv:1312.5542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>375--384</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7167" citStr="Lin and He, 2009" startWordPosition="1104" endWordPosition="1107">n holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of CIKM, pages 375–384. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian McAuley</author>
<author>Jure Leskovec</author>
</authors>
<title>Hidden factors and hidden topics: understanding rating dimensions with review text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th ACM conference on Recommender systems,</booktitle>
<pages>165--172</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="21664" citStr="McAuley and Leskovec, 2013" startWordPosition="3662" endWordPosition="3665"> ft I V W ft ht ft I very 1437 They released their pre-trained 300-dimensional word embeddings (vocabulary size 3M) trained by the skip-gram model on part of Google news dataset containing about 100 billion words.4 4.3 Amazon Embeddings Since we work on customer reviews, which are less formal than Wikipedia and news, we have also trained domain-specific embeddings (vocabulary size 1M) using the CBOW model of word2vec tool (Mikolov et al., 2013b) from a large corpus of Amazon reviews.5 The corpus contains 34, 686, 770 reviews (4.7 billion words) on Amazon products from June 1995 to March 2013 (McAuley and Leskovec, 2013). For comparison with SENNA and Google, we learn word embeddings of 50- and 300-dimensions. 5 Experiments In this section, we present our experimental settings and results for the task of opinion target extraction from customer reviews. 5.1 Experimental Settings Datasets: In our experiments, we use the two review datasets provided by the SemEval-2014 task 4: aspect-based sentiment analysis evaluation campaign (Pontiki et al., 2014), namely the Laptop and the Restaurant datasets. Table 2 shows some basic statistics about the datasets. The majority of aspect terms have only one word, while about</context>
</contexts>
<marker>McAuley, Leskovec, 2013</marker>
<rawString>Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165–172. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gr´egoire Mesnil</author>
<author>Xiaodong He</author>
<author>Li Deng</author>
<author>Yoshua Bengio</author>
</authors>
<title>Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding.</title>
<date>2013</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>3771--3775</pages>
<contexts>
<context position="4409" citStr="Mesnil et al., 2013" startWordPosition="672" endWordPosition="675">ces in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston, 2008; Mikolov et al., 2013b) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems (Turian et al., 2010; Lebret and Lebret, 2013), and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words (Mesnil et al., 2013; Irsoy and Cardie, 2014). Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific feature engineering effort. We experiment with several important RNN architectures including Elman-RNN, Jordan-RNN, long short term memory (LSTM) and their variations. We acquire pre-trained word embeddings from several external sources to give better initialization to our RNN models. The RNN models then fine-tune the word vector</context>
<context position="7506" citStr="Mesnil et al., 2013" startWordPosition="1156" endWordPosition="1159">hods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children. Most relevant to our work is the recent work of Irsoy and Cardie (2014), where they apply deep Elman-type RNN to extract opinion expressions and show that deep R</context>
<context position="24997" citStr="Mesnil et al. (2013)" startWordPosition="4205" endWordPosition="4208">plement early stopping in SGD, we prepared a validation set by separating out randomly 10% of the available training data. The remaining 90% is used for training. The weights in the network were initialized by sampling from a small random uniform distribution U(−0.2,0.2). The time step in the truncated BPTT was fixed to 6 based on the performance on the validation set; smaller values hurt the performance, while larger values showed no significant gains but increased the training time. We use a fixed learning rate of 0.01, but we change the batch size depending on the sentence length following Mesnil et al. (2013). The net effect is a variable step size in the SGD. We run SGD for 30 epochs, calculate the F1 score on the validation set after each epoch, and stop if the accuracy starts to decrease. The size of the context window and the hidden layer are empirically set based on the performance on the validation set. We experimented 1438 with the window size E 11, 3, 5}, and found 3 to be the optimal on the validation set. The hidden layer sizes we experimented with are 50, 100, 150, and 200; we report the optimal values in Table 3 (see |hl |and |hr |columns). Linguistic Features in RNNs: In addition to t</context>
</contexts>
<marker>Mesnil, He, Deng, Bengio, 2013</marker>
<rawString>Gr´egoire Mesnil, Xiaodong He, Li Deng, and Yoshua Bengio. 2013. Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding. In Proceedings of INTERSPEECH, pages 3771–3775.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="4079" citStr="Mikolov et al., 2013" startWordPosition="615" endWordPosition="618">eep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston, 2008; Mikolov et al., 2013b) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems (Turian et al., 2010; Lebret and Lebret, 2013), and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words (Mesnil et al., 2013; Irsoy and Cardie, 2014). Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific f</context>
<context position="20614" citStr="Mikolov et al. (2013" startWordPosition="3478" endWordPosition="3481"> POS tagging, chunking, semantic role labeling, named entity recognition) with a window-based approach and a sentence-based approach (i.e., the input layer is a sentence). Each word in the input layer is represented by M features, each of which has an embedding vector associated with it in a lookup table. To give their network a better initialization, they learn word embeddings using a nonprobabilistic language model, which was trained on English Wikipedia for about 2 months. They released their 50-dimensional word embeddings (vocabulary size 130K) under the name SENNA.3 4.2 Google Embeddings Mikolov et al. (2013a) propose two log-linear models for computing word embeddings from large corpora efficiently: (i) a bag-of-words model CBOW that predicts the current word based on the context words, and (ii) a skip-gram model that predicts surrounding words given the current word. 3http://ronan.collobert.com/senna/ (a) (b) The hard disk is very hft 1 hft hf, 1 Yt 1 Yt Yt 1 hb, , hbt hbt , The hard disk is ht ht ] Uht ht 1 Yt 1 Yt Yt 1 ft I V W ft ht ft I very 1437 They released their pre-trained 300-dimensional word embeddings (vocabulary size 3M) trained by the skip-gram model on part of Google news dataset</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="4079" citStr="Mikolov et al., 2013" startWordPosition="615" endWordPosition="618">eep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston, 2008; Mikolov et al., 2013b) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems (Turian et al., 2010; Lebret and Lebret, 2013), and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words (Mesnil et al., 2013; Irsoy and Cardie, 2014). Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific f</context>
<context position="20614" citStr="Mikolov et al. (2013" startWordPosition="3478" endWordPosition="3481"> POS tagging, chunking, semantic role labeling, named entity recognition) with a window-based approach and a sentence-based approach (i.e., the input layer is a sentence). Each word in the input layer is represented by M features, each of which has an embedding vector associated with it in a lookup table. To give their network a better initialization, they learn word embeddings using a nonprobabilistic language model, which was trained on English Wikipedia for about 2 months. They released their 50-dimensional word embeddings (vocabulary size 130K) under the name SENNA.3 4.2 Google Embeddings Mikolov et al. (2013a) propose two log-linear models for computing word embeddings from large corpora efficiently: (i) a bag-of-words model CBOW that predicts the current word based on the context words, and (ii) a skip-gram model that predicts surrounding words given the current word. 3http://ronan.collobert.com/senna/ (a) (b) The hard disk is very hft 1 hft hf, 1 Yt 1 Yt Yt 1 hb, , hbt hbt , The hard disk is ht ht ] Uht ht 1 Yt 1 Yt Yt 1 ft I V W ft ht ft I very 1437 They released their pre-trained 300-dimensional word embeddings (vocabulary size 3M) trained by the skip-gram model on part of Google news dataset</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Statistical Language Models based on Neural Networks.</title>
<date>2012</date>
<tech>PhD thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="13676" citStr="Mikolov, 2012" startWordPosition="2259" endWordPosition="2260">sk is very (c) Long Short-Term Memory (LSTM) RNN letmletmletm Yt 1 Yt Yt 1 xt LSTM Input Gate Output Gate i x x t Memory Cell ft x Forget Gate ot ht Figure 1: Elman-type, Jordan-type and LSTM RNNs with a lookup-table layer, a hidden layer and an output layer. The concatenated context vector for the word “disk” at time t is xt = [xhard, xdisk, xis] with a context window of size 3. One memory block in the LSTM hidden layer has been enlarged. This is known as the vanishing and the exploding gradients problem (Bengio et al., 1994). One simple way to overcome this issue is to use a truncated BPTT (Mikolov, 2012) for restricting the backpropagation to only few steps like 4 or 5. However, this solution limits the RNN to capture long-range dependencies. In the following, we describe an elegant RNN architecture to address this problem. Yr- 1 Y t + 1 Y t hr- 1 h t + 1 3.3 Long Short-Term Memory RNN Long Short-Term Memory or LSTM (Hochreiter and Schmidhuber, 1997) is specifically designed to model long term dependencies in RNNs. The recurrent layer in a standard LSTM is constituted with special (hidden) units called memory blocks (Fig. 1c). A memory block is composed of four elements: (i) a memory cell c (</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov, 2012. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samaneh Moghaddam</author>
<author>Martin Ester</author>
</authors>
<title>On the design of LDA models for aspect-based opinion mining.</title>
<date>2012</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>803--812</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7195" citStr="Moghaddam and Ester, 2012" startWordPosition="1108" endWordPosition="1111"> al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neu</context>
</contexts>
<marker>Moghaddam, Ester, 2012</marker>
<rawString>Samaneh Moghaddam and Martin Ester. 2012. On the design of LDA models for aspect-based opinion mining. In Proceedings of CIKM, pages 803–812. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Murphy</author>
</authors>
<title>Machine Learning A Probabilistic Perspective.</title>
<date>2012</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="18464" citStr="Murphy, 2012" startWordPosition="3135" endWordPosition="3136">his means, during training, after backpropagating the errors from the output layer to the forward and to the backward hidden layers, two independent BPTT can be applied – one to each direction. 3.5 Fine-tuning of Embeddings In our RNN framework, we intend to avoid manual feature engineering efforts by using word embeddings as the only features. As mentioned before, we can initialize the embeddings randomly and learn them as part of model parameters by backpropagating the errors to the look-up layer. One issue with random initialization is that it may lead the SGD to get stuck in local minima (Murphy, 2012). On the other hand, one can plug the readily available embeddings from external sources (Section 4) in the RNN model and use them as features without tuning them further for the task, as is done in any other machine learning model. However, this approach does not exploit the automatic feature learning capability of NN models, which is one of the main motivations of using them. In our work, we use the pre-trained word embeddings to better initialize our models, and we fine-tune them for our task in training, which turns out to be quite beneficial (see Section 5.2). 3.6 Incorporating other Ling</context>
</contexts>
<marker>Murphy, 2012</marker>
<rawString>Kevin Murphy. 2012. Machine Learning A Probabilistic Perspective. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Haris Papageorgiou</author>
<author>Dimitrios Galanis</author>
<author>Ion Androutsopoulos</author>
<author>John Pavlopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>27--35</pages>
<contexts>
<context position="3056" citStr="Pontiki et al., 2014" startWordPosition="460" endWordPosition="463">om row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic composit</context>
<context position="22099" citStr="Pontiki et al., 2014" startWordPosition="3729" endWordPosition="3732">, 2013b) from a large corpus of Amazon reviews.5 The corpus contains 34, 686, 770 reviews (4.7 billion words) on Amazon products from June 1995 to March 2013 (McAuley and Leskovec, 2013). For comparison with SENNA and Google, we learn word embeddings of 50- and 300-dimensions. 5 Experiments In this section, we present our experimental settings and results for the task of opinion target extraction from customer reviews. 5.1 Experimental Settings Datasets: In our experiments, we use the two review datasets provided by the SemEval-2014 task 4: aspect-based sentiment analysis evaluation campaign (Pontiki et al., 2014), namely the Laptop and the Restaurant datasets. Table 2 shows some basic statistics about the datasets. The majority of aspect terms have only one word, while about one third of them have multiple words. In both datasets, some sentences have no aspect terms and some have more than one aspect terms. We use the standard train:test split to compare our results with the SemEval best systems. In addition, we show a more general performance of our models on the two datasets based on 10–fold cross validation. Laptop Restaurant Train Test Train Test Sentences 3045 800 3041 800 Sentence length 15 13 1</context>
<context position="23521" citStr="Pontiki et al., 2014" startWordPosition="3959" endWordPosition="3962">data/web-Amazon.html Evaluation Metric: The evaluation metric measures the standard precision, recall and F1 score based on exact matches. This means that a candidate aspect term is considered to be correct only if it exactly matches with the aspect term annotated by the human. In all our experiments when comparing two models, we use paired t-test on the F1 scores to measure statistical significance and report the corresponding p-value. CRF Baseline: We use a linear-chain CRF (Lafferty et al., 2001) of order 2 as our baseline, which is the state-of-the-art model for opinion target extraction (Pontiki et al., 2014). Specifically, the CRF generates (binary) feature functions of order 1 and 2; see (Cuong et al., 2014) for higher order CRFs. The features used in the baseline model include the current word, its POS tag, its prefixes and suffixes between one to four characters, its position, its stylistics (e.g., case, digit, symbol, alphanumeric), and its context (i.e., the same features for the two preceding and the two following words). In addition to the hand-crafted features, we also include the three different types of word embeddings described in Section 4. RNN Settings: We pre-processed each dataset </context>
</contexts>
<marker>Pontiki, Papageorgiou, Galanis, Androutsopoulos, Pavlopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis, Ion Androutsopoulos, John Pavlopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hasim Sak</author>
<author>Andrew Senior</author>
<author>Franc¸oise Beaufays</author>
</authors>
<title>Long short-term memory recurrent neural network architectures for large scale acoustic modeling.</title>
<date>2014</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>338--342</pages>
<contexts>
<context position="7450" citStr="Sak et al., 2014" startWordPosition="1147" endWordPosition="1150">e aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children. Most relevant to our work is the recent work of Irsoy and Cardie (2014), where they apply deep Elman-type</context>
</contexts>
<marker>Sak, Senior, Beaufays, 2014</marker>
<rawString>Hasim Sak, Andrew Senior, and Franc¸oise Beaufays. 2014. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In Proceedings of INTERSPEECH, pages 338– 342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Schuster</author>
<author>Kuldip K Paliwal</author>
</authors>
<title>Bidirectional recurrent neural networks.</title>
<date>1997</date>
<journal>IEEE Transactions on Signal Processing,</journal>
<volume>45</volume>
<issue>11</issue>
<contexts>
<context position="16686" citStr="Schuster and Paliwal, 1997" startWordPosition="2804" endWordPosition="2807">). For instance, consider the two sentences: (i) Do you know about the crunchy tuna here, it is to die for. and (ii) Do you know about the crunchy tuna here, it is imported from Norway. The phrase crunchy tuna is an aspect term in the first (subjective) sentence, but not in the second (objective) one. The RNN models described above will assign the same labels to crunchy and tuna in both sentences, since the preceding sequences and the context window (of size 1 to 5) are the same. To capture long-range dependencies from the future as well as from the past, we propose to use bidirectional RNNs (Schuster and Paliwal, 1997), which allow bidirectional links in the network. In an Elman-type bidirectional RNN (Fig. 2a), the �� forward hidden layer ht and the backward hidden �� layer ht at time t are computed as follows: �� ht = f( U ht_1 + �� Vxt+ b ) (10) ht = f(�� U ht_1 + �� V xt + b ) (11) 1436 I Figure 2: (a) Bidirectional Elman-type RNN and (b) Linguistic features concatenated with the hidden layer output in Elman-type RNN. →− U , →− V and →− where b are the forward weight matrices as before; and ←−U , ←−V and ←−b are their backward counterparts. The concatenated vector ht = ht, ht] is passed to the output la</context>
</contexts>
<marker>Schuster, Paliwal, 1997</marker>
<rawString>Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673–2681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shabnam Shariaty</author>
<author>Samaneh Moghaddam</author>
</authors>
<title>Fine-grained opinion mining using conditional random fields.</title>
<date>2011</date>
<booktitle>In International Conference on Data Mining Workshops (ICDMW),</booktitle>
<pages>109--114</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6975" citStr="Shariaty and Moghaddam, 2011" startWordPosition="1073" endWordPosition="1076"> e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursi</context>
</contexts>
<marker>Shariaty, Moghaddam, 2011</marker>
<rawString>Shabnam Shariaty and Samaneh Moghaddam. 2011. Fine-grained opinion mining using conditional random fields. In International Conference on Data Mining Workshops (ICDMW), pages 109–114. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1631--1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2758" citStr="Socher et al., 2013" startWordPosition="416" endWordPosition="419"> with BIO labels for opinion target (TARG tags) and for opinion expression (EXPR tags) extraction. label each word in a sentence using the conventional BIO tagging scheme. For example, Table 1 shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion expression (bottom row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have </context>
<context position="7552" citStr="Socher et al. (2013)" startWordPosition="1163" endWordPosition="1166">2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children. Most relevant to our work is the recent work of Irsoy and Cardie (2014), where they apply deep Elman-type RNN to extract opinion expressions and show that deep RNN outperforms CRF, semiCRF and shallow RNN. T</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP, pages 1631– 1642. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>LSTM neural networks for language modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>194--197</pages>
<marker>Sundermeyer, Schl¨uter, Ney, 2012</marker>
<rawString>Martin Sundermeyer, Ralf Schl¨uter, and Hermann Ney. 2012. LSTM neural networks for language modeling. In Proceedings of INTERSPEECH, pages 194– 197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>111--120</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7149" citStr="Titov and McDonald, 2008" startWordPosition="1100" endWordPosition="1103">to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceedings of WWW, pages 111–120. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiqiang Toh</author>
<author>Wenting Wang</author>
</authors>
<title>DLIREC: Aspect term extraction and term polarity classification system.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014),</booktitle>
<pages>235</pages>
<contexts>
<context position="35166" citStr="Toh and Wang, 2014" startWordPosition="5915" endWordPosition="5918">ings both Random and Amazon embeddings close to the best ones. Comparison with SemEval-2014 Systems: When our RNN results are compared with the top performing systems in the SemEval-2014 (last two rows in Table 3), we see that RNNs without using any linguistic features achieve the second best results on both Laptop and Restaurant datasets. Note that these RNNs only use word embeddings, while IHS RD and DLIREC use complex features like dependency relations, named entity, sentiment orientation of words, word cluster and many more in their CRF models, most of which are expensive to compute; see (Toh and Wang, 2014; Chernyshevich, 2014). The performance of our RNNs improves when they are given access to very simple features like POS tags and chunks, and LSTM-RNN+Feat. achieves the best results on the Laptop dataset. 6 Conclusion and Future Direction We presented a general class of discriminative models based on recurrent neural network (RNN) architecture and word embeddings, that can be successfully applied to fine-grained opinion mining tasks without any task-specific manual feature engineering effort. We used pre-trained word embeddings from three external sources in different RNN architectures includ</context>
</contexts>
<marker>Toh, Wang, 2014</marker>
<rawString>Zhiqiang Toh and Wenting Wang. 2014. DLIREC: Aspect term extraction and term polarity classification system. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), page 235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of ACL,</booktitle>
<pages>384--394</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4246" citStr="Turian et al., 2010" startWordPosition="644" endWordPosition="647">works for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston, 2008; Mikolov et al., 2013b) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems (Turian et al., 2010; Lebret and Lebret, 2013), and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words (Mesnil et al., 2013; Irsoy and Cardie, 2014). Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific feature engineering effort. We experiment with several important RNN architectures including Elman-RNN, Jordan-RNN, long short term memory (LSTM) and their variations. </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of ACL, pages 384–394. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="1292" citStr="Wiebe et al., 2005" startWordPosition="182" endWordPosition="185">taskspecific feature engineering effort. Our experimental results on the task of opinion target identification show that RNNs, without using any hand-crafted features, outperform feature-rich CRF-based models. Our framework is flexible, allows us to incorporate other linguistic features, and achieves results that rival the top performing systems in SemEval-2014. 1 Introduction Fine-grained opinion mining involves identifying the opinion holder who expresses the opinion, detecting opinion expressions, measuring their intensity and sentiment, and identifying the target or aspect of the opinion (Wiebe et al., 2005). For example, in the sentence “John says, the hard disk is very noisy”, John, the opinion holder, expresses a very negative (i.e., sentiment with intensity) opinion towards the target “hard disk” using the opinionated expression “very noisy”. A number of NLP applications can benefit from fine-grained opinion mining including opinion summarization and opinion-oriented question answering. The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task at the sequence (e.g., phrase) level. For example, identifying opi</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>347--354</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6374" citStr="Wilson et al., 2005" startWordPosition="976" endWordPosition="979">he second best on the Restaurant dataset in SemEval-2014. We make our source code available.1 In the remainder of this paper, after discussing related work in Section 2, we present our RNN models in Section 3. In Section 4, we briefly describe the pre-trained word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 201</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT/EMNLP, pages 347–354. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Extracting opinion expressions with semi-Markov conditional random fields.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1335--1345</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2952" citStr="Yang and Cardie, 2012" startWordPosition="442" endWordPosition="445"> shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion expression (bottom row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show t</context>
<context position="6998" citStr="Yang and Cardie, 2012" startWordPosition="1077" endWordPosition="1080">reck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to h</context>
</contexts>
<marker>Yang, Cardie, 2012</marker>
<rawString>Bishan Yang and Claire Cardie. 2012. Extracting opinion expressions with semi-Markov conditional random fields. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1335–1345. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1640--1649</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="7022" citStr="Yang and Cardie, 2013" startWordPosition="1081" endWordPosition="1084"> common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose se</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
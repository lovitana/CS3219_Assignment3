<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006566">
<title confidence="0.9980375">
Syntactic Dependencies and Distributed Word Representations for
Chinese Analogy Detection and Mining
</title>
<author confidence="0.998533">
Likun Qiu&apos;&apos;2, Yue Zhang2, Yanan Lu3
</author>
<affiliation confidence="0.983502">
&apos;School of Chinese Language and Literature, Ludong University, China
2Singapore University of Technology and Design, Singapore
3Computer School, Wuhan University, China
</affiliation>
<email confidence="0.990757">
qiulikun@pku.edu.cn, yue zhang@sutd.edu.sg, luyanan@whu.edu.cn
</email>
<sectionHeader confidence="0.993602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994993125">
Distributed word representations capture
relational similarities by means of vec-
tor arithmetics, giving high accuracies on
analogy detection. We empirically inves-
tigate the use of syntactic dependencies
on improving Chinese analogy detection
based on distributed word representation-
s, showing that a dependency-based em-
beddings does not perform better than an
ngram-based embeddings, but dependen-
cy structures can be used to improve anal-
ogy detection by filtering candidates. In
addition, we show that distributed repre-
sentations of dependency structure can be
used for measuring relational similarities,
thereby help analogy mining.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946571428572">
Relational similarity measures the correspondence
between word-word relations (Medin et al., 1990).
It is relevant to many tasks in NLP (Turney, 2006),
such as word sense disambiguation, information
extraction, question answering, information re-
trieval, semantic role identification and metaphor
detection. Typical tasks on relational similarity in-
clude analogy detection, which measures the de-
gree of relational similarities, and analogy mining,
which extracts analogous word pairs from unstruc-
tured text.
Recently, distributed word representations (i.e.
embeddings) (Mikolov et al., 2013a; Mikolov et
al., 2013b; Levy and Goldberg, 2014b) have been
used for unsupervised analogy detection. Mikolov
et al. use attributional similarities between words
in a relation to compute relational similarities, and
show that the method outperforms the best sys-
tem in the SemEval 2012 shared task on analo-
gy detection. Levy and Goldberg (2014b) fur-
ther improve Mikolov’s relational similarity mea-
sure method using novel arithmetic combination-
s of attributional similarities. For simplicity, we
call the method of Mikolov et al. embedding-
based analogy detection, without stressing the dif-
ference between distributed and distributional (i.e.
counting-based) word representations.
Most work on embedding-based analogy detec-
tion uses relational similarities as a measure of the
quality of embeddings. However, relatively little
has been done in the opposite direction, exploring
how to leverage embeddings for improving rela-
tional similarity algorithms. We empirically study
the use of word embeddings for Chinese analogy
detection and mining, leveraging syntactic depen-
dencies, which has been shown to be closely asso-
ciated with semantic relations (Levin, 1993; Chi-
u et al., 2007). Compared with many other lan-
guages, this association is particularly strong for
Chinese, which is fully configurational and lack-
s morphology. To our knowledge, relatively little
work has been reported on Chinese relational sim-
ilarities, compared to other tasks in Chinese NLP,
including syntactic parsing, information extraction
and machine translation.
We work on three specific problems. First, we
study the effect of dependency-based word em-
beddings for analogy detection. There are two
variations of Mikolov et al’s skip-gram embed-
ding model, one training the distributed word rep-
resentation of a word using its context words in
local ngram window (Mikolov et al., 2013a), and
the other training the distributed representation of
a word using words in a syntactic dependency
context (Levy and Goldberg, 2014b; Bansal et al.,
2014). The latter has attracted much recent atten-
</bodyText>
<page confidence="0.908318">
2441
</page>
<note confidence="0.984327">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2441–2450,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.99996111627907">
tion due to its potential in capturing more syntac-
tic regularities. It has been shown to outperfor-
m the former in a variety of NLP tasks, and can
potentially also improve relation similarity. Our
experiments on both English and Chinese show
that the dependency-context embeddings consis-
tently under-perform ngram-context embeddings.
We give some theoretical justifications to the find-
ings.
Second, we propose to use syntactic depen-
dencies as a context for improving embedding-
based analogy detection, pruning the search space
and filtering noise using syntactic dependencies.
While highly useful for measuring relational sim-
ilarities, attributional similarities between words
are not the only source of information for analo-
gy detection. Traditional methods, such as Tur-
ney and Littman (2005), Turney (2006), Chiu et
al. (2007) and O´ S´eaghdha and Copestake (2009),
also leverage context between word pairs in a
corpus for better accuracies, which the current
embedding-based methods ignore. Results show
that our proposed method achieves significant im-
provements for this task.
Third, we show that a novel distributed repre-
sentation of syntactic dependencies between word
pairs can be used to mine analogous dependencies
from a large Chinese corpus. Inspired by the fact
that distributed word representations can be used
to measure word similarities, we use our distribut-
ed dependency representations to measure relation
similarities. We propose a bootstrapping algorith-
m for analogy mining using dependency embed-
dings, and experiments on a large Chinese corpus
show that the method can achieve a precision of
95.2% at a recall of 56.8%.
Our automatically-parsed corpus, trained em-
beddings and evaluation datasets are released
publicly at http://people.sutd.edu.sg/
˜yue_zhang/publication.html. To our
knowledge, we are the first to present results on
Chinese analogy detection and to release large-
scale Chinese word embeddings.
</bodyText>
<sectionHeader confidence="0.996131" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.995609">
2.1 Relational Similarity Tasks
</subsectionHeader>
<bodyText confidence="0.997357">
There are three main tasks for relational similarity.
This first is relation classification, which has been
used in Task 2 of SemEval 2012 (Jurgens et al.,
2012). In this task, all four words in two word
pairs are given, and one needs to judge whether
</bodyText>
<figureCaption confidence="0.47830575">
Figure 1: Dependency tree of the sentence
“1991* (in 1991) , (,) 4�C4 (Obama) A,tt
(President) *A, (graduate) t (from) T (Har-
vard) �k*M (Law School)”.
</figureCaption>
<bodyText confidence="0.98554555">
they belong to a same relation type. In order to
address this task, various supervised methods have
been used (Bollegala et al., 2008; Herdaˇgdelen and
Baroni, 2009; Turney, 2013).
The second task is analogy detection (Mikolov
et al., 2013b), which takes three words in two
word pairs, and searches for a most suitable word
from the vocabulary to recover the hidden word.
This task has been addressed using word embed-
dings (Mikolov et al., 2013b; Levy and Goldberg,
2014b).
The third task is analogy mining (Chiu et al.,
2007), which takes one word pair belonging to a
certain semantic relation as a seed, and searches
for all the word pairs that share the same relation
with the seed. Compared with relation classifi-
cation and analogy detection, analogy mining can
be practically more useful because it requires less
given information, and provides a large quantity of
analogous word pairs automatically.
</bodyText>
<subsectionHeader confidence="0.99947">
2.2 Skip-gram Word Embeddings
</subsectionHeader>
<bodyText confidence="0.9998983125">
As a by-product of neural language models (Ben-
gio et al., 2003; Mnih and Hinton, 2007), word
embeddings are distributed vector representations
of words, trained using local contexts. They cap-
ture linguistic regularities in languages (Mikolov
et al., 2013b) and have been used in various tasks
(Collobert and Weston, 2008; Turian et al., 2010;
Socher et al., 2011).
In this paper, we apply the Skip-gram method
of Mikolov et al. (2013a) for training embed-
dings, which works by maximizing the probabil-
ity of a word given a context of multiple words.
Mikolov et al. (2013b) use an ngram window as
the context, and observe that the resulting embed-
dings are highly useful for unsupervised analogy
detection.
</bodyText>
<page confidence="0.98415">
2442
</page>
<subsectionHeader confidence="0.996331">
2.3 Embedding-based Analogy Detection
</subsectionHeader>
<bodyText confidence="0.9999512">
Formally, the task of analogy detection is to find
a word b* given a pair of words a:b and a word
a* such that a*:b* is analogous to a:b. Mikolov
et al. (2013b) show that the task can be solved by
finding a word that maximizes:
</bodyText>
<equation confidence="0.868904">
score = sim(b*, b − a + a*) (1)
</equation>
<bodyText confidence="0.999975666666667">
where sim is a similarity measure, typically the
cosine function. Levy and Goldberg (2014b)
show that the Equation 1 is equivalent to:
</bodyText>
<equation confidence="0.928813">
score = cos(b*, b)−cos(b*, a)+cos(b*, a*) (2)
</equation>
<bodyText confidence="0.9994822">
As a result, the goal of analogy detection is to find
a word b* which is similar to b and a* but differ-
ent from a. Levy and Goldberg (2014b) further
propose to substitute the addictive functions in E-
quation 2 with multiplicative functions:
</bodyText>
<equation confidence="0.917313">
score = cos(b*,b)cos(b*,a*)/(cos(b*,a) + ε)
(3)
</equation>
<bodyText confidence="0.998395833333333">
Here ε = 0.001 is used to prevent division by zero.
Their experiments show that the use of Equation 3
can improve the state-of-the-art. Following Levy
and Goldberg (2014b), we refer to Equation 1 and
2 as 3COSADD and Equation 3 as 3COSMUL,
respectively.
</bodyText>
<subsectionHeader confidence="0.99781">
2.4 Chinese Relational Similarity
</subsectionHeader>
<bodyText confidence="0.979715447368421">
There are various types of relational similarities.
Syntactically, inflections can be treated as a type
of word-word relation (Mikolov et al., 2013b).
For example, the comparative pairs “good:better”
and “rough:rougher” are analogous, and the past
tense inflections “see:saw” and “return:returned”
are analogous. However, such inflectional rela-
tions do not apply to Chinese, which is fully con-
figurational and lacks morphology. Consequent-
ly, our main focus is semantic similarities, which
include antonymy (e.g. (A (hot):;? (cold)) VS
(A (fast):� (slow))), meronymy (e.g. (-* (car):*
1&apos; (wheel)) VS (;fit (bear):* (paw))), gender (e.g.
(W,k (man):*,k (woman)) VS (q-:E (king):
L (queen))) and function relations (e.g. (-cARL
(clothing):3F (wear)) VS (Mm1&apos; (hat):A (wear))),
etc.
Chiu et al. (2007) show that English semantic
relations are also reflected by syntactic dependen-
cies. Their finding coincides with Levin (1993),
who study English verbs. We find that this obser-
vation is even more prevalent for Chinese. In our
automatically-parsed Chinese corpus of 3.4 bil-
lion words (Section 5.1), 86.4% word pairs from
the analogy test dataset (Section 5.2) have corre-
sponding dependencies, each of which appearing
at least ten times.
The frequent correlation between semantic re-
lations and syntactic dependencies can be due
to the lack of morphology and function words
in Chinese. In fact, Chinese syntactic ambigui-
ties often need to be resolved by leveraging se-
mantic information (Xiong et al., 2005; Zhang
et al., 2014). Although not all occurrences of
semantically-related word pairs must also form a
syntactic dependency in a corpus, we show that
syntactic dependencies can effectively improve
analogy detection.
</bodyText>
<sectionHeader confidence="0.838036" genericHeader="method">
3 Dependency-context Word
Embeddings for Analogy Detection
</sectionHeader>
<bodyText confidence="0.980592068965518">
A first use of syntactic dependencies for
embedding-based analogy detection is to use
them directly for embeddings. Recently, a depen-
dency context has been used for the skip-gram
method, for capturing more syntactic regularities.
Taking the sentence in Figure 1 for example, a
bi-gram context for the word “.A (graduate)”
can be “4�C4 (Obama), ott (President), t
(from), , 4 (Harvard)”, while a dependency
context of the same word can be “1991c/ADV,
Ao tk/SBV, t/CMP, �k * M/POB t”1, where
“ADV, SBV, CMP, POB” indicate adverbial
modifier, subject, complement and prepositional
object, respectively.
It has been shown that a dependency context
leads to embeddings that better help parsing
(Bansal et al., 2014) and measuring word sim-
ilarity (Levy and Goldberg, 2014a), compared
with ngram contexts. However, little previous
work has systematically compared dependency
contexts with ngram contexts in analogy detec-
tion. We empirically study this problem (c.f Sec-
tion 6.3), finding that dependency context lead-
s to significantly worse analogy detection results
for both Chinese and English using state-of-the-art
embedding-based methods (Levy and Goldberg,
2014b). We give analysis in Section 6.4.
1The last token is a grand-child of “.A (graduate)”, via
the preposition “t (at)” (Levy and Goldberg, 2014a).
</bodyText>
<page confidence="0.961446">
2443
</page>
<sectionHeader confidence="0.840793" genericHeader="method">
4 Search Space Pruning Using Syntactic
Dependencies
</sectionHeader>
<bodyText confidence="0.991386848484848">
We study an alternative way of making use of syn-
tactic dependencies, by using them to prune the
vocabulary-sized search space of analogy detec-
tion. Given two word pairs a:b and a*:b*, where
b* is hidden and a is the head word, we search
for dependencies, taking a* as the head word. The
dependent words in the search candidates need to
share the POS tag of b. If there are several type-
s of dependencies between a and b, only the one
with highest frequency is used. We rank all result-
ing dependencies using the 3COSMUL objective,
and take the word b* in the highest-scored depen-
dencies as the answer.
For example, given the word pair (I
� (Sarajevo):AN, (Bosnia and Herzegovina)),
whose most frequency dependency is &lt;P4-14A
A (Sarajevo), A/SC A (Bosnia and Herzegovina),
ATT&gt;, and the unknown pair (*BSc (London):b*),
we acquire a list of dependencies, including &lt;Ô
í (London), *q (USA), ATT&gt;, &lt;Ôí (Lon-
don), CV- (Paris), COO&gt;, &lt;Ôí (London), ho
*k (Canada), ATT&gt; and &lt;Ôí (London), ;c�q
(England), ATT&gt;. Some of these dependencies,
such as &lt;Ôí (London), Ct- (Paris), COO&gt;,
are parsed as the coordinate relation (COO), and
thus pruned because the target syntactic relation is
ATT. From the resulting list, the 3COSMUL ob-
jective successfully ranks the triple &lt;Ôí (Lon-
don), ;c�q (England), ATT&gt; as the top candidate.
In contrast, Levy and Goldberg’s method takes “A
4 (South Africa)” as the answer, which does not
form an attributive-head phrase with “Ôí (Lon-
don)”.
</bodyText>
<sectionHeader confidence="0.9517695" genericHeader="method">
5 Analogy Mining Using Dependency
Embeddings
</sectionHeader>
<bodyText confidence="0.999786">
Formally, analogy mining is the task of mining
analogous dependencies &lt;x1, y1, r&gt;, &lt;x2, y2, r&gt;
...&lt;xn, yn, r&gt; that share the same relation r with
a given dependency &lt;a, b, r&gt;. We mine analo-
gous dependencies by considering relational sim-
ilarity and attributional similarity simultaneously
using the skip-gram model for embeddings.
</bodyText>
<subsectionHeader confidence="0.99339">
5.1 Dependency Embedding
</subsectionHeader>
<bodyText confidence="0.949404285714286">
Inspired by the fact that word similarities can be
measured by using distributed word representa-
tions, we hypothesize that relation similarities can
Input : dependency embedding DT, word
embedding DW, seed dependency s,
threshold α and Q.
Output: set of ranked dependencies WP.
</bodyText>
<figure confidence="0.997472214285714">
1 Function Mine (DT,DW,s,WP,α,Q):
2 begin
3 DTSet =∅;
4 MScore =0;
5 SimDT =GetSimDT (DT,s);
6 for each Triple G SimDT do
7 MWS =GetMWord (s);
8 HWS =GetHWord (s);
9 MWD =GetMWord (Triple);
10 HWD =GetHWord (Triple);
11 ScoreX =Sim (MWS,MWD,DW);
12 ScoreY =Sim (HWS,HWD,DW);
13 ScoreXY =ScoreX x ScoreY;
14 MScore =Max (ScoreXY,MScore);
15 TopK (ScoreXY,Triple,DTSet,α)
16 end
17 MScore =MScore x Q ;
18 for each Triple, ScoreXY G DTSet do
19 if ScoreXY &gt; MScore and Triple G/
WP then
20 AddToSet (Triple,WP);
21 s =Triple;
22 Mine (DT,DW,s,WP,α,Q);
23 end
24 end
25 end
26 WP =∅;
27 Mine (DT,DW,s,WP,α,Q);
</figure>
<figureCaption confidence="0.840689">
Algorithm 1: Bootstrapping for analogy
mining.
</figureCaption>
<bodyText confidence="0.999974652173913">
be measured by distributed relation representa-
tions. Based on the observation in Section 2.4,
semantically analogous word pairs typically have
syntactic dependencies. We use the skip-gram al-
gorithm to train distributed representations of syn-
tactic dependencies, and use them for mining anal-
ogous word pairs.
With respect to the skip-gram model, words are
the most common target for embeddings (Levy
and Goldberg, 2014b; Levy and Goldberg, 2014a;
Mikolov et al., 2013a), although continuous vec-
tor representations can be trained for other struc-
tures. For example, Mikolov et al. (2013a) take
idiomatic phrases as embedding targets. Depen-
dencies, which consist of a modifier word, a head
word and a syntactic relation between them, can
also be represented by continuous embeddings us-
ing the same algorithm.
To induce dependency embeddings, we take
the union of the dependency context of both the
dependent and the head of a dependency as the
context. For instance, in the example sentence,
the context of the dependency &lt;,O,bt (Presiden-
</bodyText>
<page confidence="0.96061">
2444
</page>
<bodyText confidence="0.9979758">
t), . A (graduate), SBV&gt; consists of four to-
kens: “1991*/ADV”, “4�C4/ATT”, “u/CMP”
and “{ÆR/POB t”. The same skip-gram algo-
rithm is used to train embeddings for dependency
structures.
</bodyText>
<subsectionHeader confidence="0.995845">
5.2 Analogy Mining by Bootstrapping
</subsectionHeader>
<bodyText confidence="0.997645348837209">
A bootstrapping algorithm is used to mine anal-
ogous word pairs based on dependency-context
word embeddings and dependency embeddings.
Algorithm 1 shows pseudocode of the recursive
bootstrapping algorithm.
The recursive function Mine (Algorithm 1)
contains three steps with six parameters, includ-
ing the dependency embeddings DT, word embed-
dings DW, a seed dependency s, and two thresh-
olds α and 0. Step 1 (lines 3 to 5) is an initial-
ization process, where the dependency embedding
is used to return up to 100 most similar dependen-
cies for the given seed s. These dependencies are
stored in SimDT, and the candidate analogous de-
pendency set DTSet is initialized to an empty set.
In Step 2 (lines 6 to 16), an analogous score S-
coreXY is computed for each dependency Triple
in SimDT by multiplying the similarity scores be-
tween the two dependents and the two heads in
Triple and s, respectively. Triple is stored into the
set DTSet if ScoreXY is ranked top α. The top 1
score in DTSet is referred to as MScore. In Step 3
(lines 17 to 24), if the score of a dependency Triple
in DTSet is larger than 0×MScore, it is used as a
new seed for mining more analogous dependen-
cies, by calling the function Mine recursively.
We take the seed dependency &lt; (play), 4A
(piano), VOB&gt; as an example to illustrate the
work-flow of the Mine function. In Step 1, a set of
similar dependencies (e.g., &lt; (play),-tfit (gui-
tar), VOB&gt;, &lt; (play), (lyra), VOB&gt;), is cal-
culated using the dependency embeddings DT and
stored in SimDT. Each dependency in SimDT is s-
cored in Step 2, and the top α scores are put into
the set DTSet. Finally, a dependency is used as
seed to mine new analogous dependencies if its s-
core is larger than a threshold (0×MScore). For
instance, the dependency &lt; (play), (lyra),
VOB&gt; is used to mine the new dependency &lt;
(play), * (zheng), VOB&gt;, which is then used
to mine other dependencies such as &lt; (blow),
J147gk (cucurbit flute), VOB&gt; and &lt; (blow),
I A$$ (sax), VOB&gt;.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999598">
6.1 Word Embeddings
</subsectionHeader>
<bodyText confidence="0.999983318181818">
We train three sets of word embeddings: NG5 (n-
gram context with 5 words to the left of the target
word and 5 words to the right), NG2 (2 words to
the left and right) and DEP (dependency context),
and one set of dependency embeddings DT (de-
pendency context), using the Skip-Gram model.
WORD2VEC2 is used to train NG5 and NG2, and
WORD2VECF3 is used to train DEP and DT. The
negative-sampling parameter is set to 15 in all the
training processes.
All embeddings are trained on a free Chinese
news archive4 that contains about 170 million-
s sentences and 3.4 billions words. We segment
and parse these sentences using the MVT imple-
mentation of ZPar 0.75 (Zhang and Clark, 2011),
which is trained on a large-scale annotated cor-
pus and achieves state-of-the-art analyzing accu-
racy on contemporary Chinese (Qiu et al., 2014)6.
Targets and contexts for word and dependency em-
beddings were filtered with a minimum frequency
of 100 and 10, respectively, and all the four types
of embeddings are trained with 200 dimensions.
</bodyText>
<subsectionHeader confidence="0.995313">
6.2 Datasets and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999423714285714">
Three datasets are used for evaluating Chinese
embeddings. First, we construct a set of se-
mantic analogy questions. This set contains five
types of semantic analogy questions, including
capital-country (136 word pairs, and 18354 anal-
ogy questions), provincial capital-province (28,
756), city-province (637, 386262), family mem-
ber (male-female) (18, 306) and currency-country
(62, 3782). We collect the five types of word pairs
and then produce analogy questions automatical-
ly by concatenating two word pairs. The resulting
analogy dataset contains 400K analogy question-
s. We refer to this dataset as the Chinese Analogy
Question Set (CAQS).
</bodyText>
<footnote confidence="0.996597083333333">
2http://code.google.com/p/word2vec/
3https://bitbucket.org/yoavgo/
word2vecf
4This dataset contains news articles in 2014 from various
news websites, and can be downloaded from http://pan.
baidu.com/s/1o6wRjp4
5http://people.sutd.edu.sg/\%7Eyue_
zhang/doc/doc/multiview.html
6The system achieves 96.1% , 92.6% and 83.28% F1-
score for words segmentation, joint POS-tagging and depen-
dency parsing, respectively, on 1493 manually annotated sen-
tences.
</footnote>
<page confidence="0.958961">
2445
</page>
<table confidence="0.999951666666667">
Data Metrics NG5 NG2 DEP
Cilin P@1 43.3% 45.9% 43.6%
P@5 31.1% 33.3% 32.6%
P@10 25.5% 27.5% 27.5%
P@20 20.5% 22.2% 22.7%
P@50 15.0% 16.2% 17.0%
P@100 11.5% 12.2% 12.8%
CWS Kendall’s τ 38.6% 44.1% 42.4%
Spearman’s ρ 54.5% 62.2% 60.7%
</table>
<tableCaption confidence="0.99996">
Table 1: Results on Cilin and CWS.
</tableCaption>
<bodyText confidence="0.999830714285715">
Because embeddings are central for analogy de-
tection, yet there is little large-scale evaluation
results on Chinese embeddings in the literature,
we perform embedding evaluation on two dataset-
s. The first one is the Chinese WordSim (CWS),
translated from the English WordSim-353 Set and
re-scored by native Chinese speakers (Jin and Wu,
2012). This dataset consists of 297 word pairs.
The second one is the Chinese thesaurus
Tongyicicilin (Cilin) (Che et al., 2010), which
groups 74,000 Chinese words into five-layer hi-
erarchies and has been used for evaluating the
accuracy of word similarity by traditional sparse
vector space models (Qiu et al., 2011; Jin et al.,
2012). The third level of Cilin, which contain-
s 1428 classes, is used to evaluate whether two
words are semantically similar.
For comparison between Chinese and English,
we also use an English analogy question dataset,
the Google dataset7 (Mikolov et al., 2013a), to e-
valuate the English word embeddings of Levy and
Goldberg (2014a)8 on analogy detection.
On both the CAQS and the Google dataset-
s, the 3COSMUL method (Levy and Goldberg,
2014b) is used to to answer analogy questions
based on given embeddings. The results on the
CWS dataset are evaluated using the two standard
metrics for the task, namely Spearman’s ρ and K-
endall’s τ rank correlation coefficients. The re-
sults on Cilin are evaluated using Precision@K:
the percentage of words from the top-K candidates
that belong to the Cilin category of the target word.
If one of the top-K candidates belongs to the same
third-level category in Cilin as the target word, the
candidate word is taken as correct.
</bodyText>
<footnote confidence="0.862277285714286">
6.3 Dependency-based and Word-based
Word Similarity and Analogy Detection
Word Similarity
7http://code.google.com/p/word2vec/
source/browse/trunk/questions-words.txt
8http://levyomer.wordpress.com/2014/
04/25/dependency-based-word-embeddings/
</footnote>
<table confidence="0.999976615384616">
Relation NG5 NG2 DEP
MUL capital-country 68.8% 52.7% 9.9%
capital-province 84.0% 87.7% 50.0%
city-province 80.9% 80.3% 22.6%
family 39.7% 45.1% 41.5%
currency 10.4% 9.9% 2.5%
All 80.0% 78.8% 22.0%
IMP capital-country 87.9% 88.0% 87.6%
capital-province 84.9% 86.8% 84.9%
city-province 91.8% 92.0% 90.5%
family 45.3% 48.0% 47.1%
currency 7.9% 7.0% 25.9%
All 90.9% 91.1% 89.8%
</table>
<tableCaption confidence="0.918707">
Table 2: Results on CAQS. MUL and IMP indi-
cate 3COSMUL and our improved method, re-
spectively.
</tableCaption>
<table confidence="0.999976571428571">
Relation NG5 NG2 DEP
capital-country 94.6% 84.5% 38.5%
capital-world 71.5% 64.7% 14.2%
city-in-state 53.2% 42.5% 13.1%
family 82.0% 81.2% 81.0%
currency 10.5% 10.7% 6.0%
All 63.7% 60.7% 38.8%
</table>
<tableCaption confidence="0.999839">
Table 3: English results on the Google set.
</tableCaption>
<bodyText confidence="0.997498">
Table 1 shows the results of the three Chinese
embedding on Cilin and CWS, where NG2 per-
forms much better than NG5 on both datasets.
This demonstrates that one does not need to use
large window sizes in training word-based embed-
dings for capturing word similarities. The result is
similar to the finding of Shi et al. (2010), which
indicates that a window size of 2 is better than a
window size of 4 for capturing word similarity by
using distributional word representations.
DEP performs slightly worse than NG2 on
CWS and Cilin in P@1 and P@5. However, it
achieves better results on Cilin in P@10 to P@100
when more candidate similar words are evaluated.
In contrast, NG5 and NG2 mix more semantically
related words. This finding is consistent with that
of Levy and Goldberg (2014a).
</bodyText>
<subsectionHeader confidence="0.763718">
Analogy Detection
</subsectionHeader>
<bodyText confidence="0.999802888888889">
Table 2 shows the results of the three Chinese
embeddings on CAQS. Unlike on Cilin and CWS,
NG5 outperforms DEP, and is also slightly better
than NG2. Similar tendency is shown in Table 3
for the three English embeddings evaluated on the
Google dataset. These results show that dependen-
cy embeddings are relatively weak for answering
analogy questions. On the other hand, the perfor-
mance also varies across different relation types.
</bodyText>
<page confidence="0.978833">
2446
</page>
<table confidence="0.999111454545455">
Target NG5 NG2 DEP
B (wear) 1A (shorts), ;4&apos; (slim-fit), 4&apos; 4&apos;B (wear), 4&apos;X (wear), 1A 4&apos; B (wear), 4&apos; X (wear), 
B (wear), 5&apos;F @ (coat), * f (shorts),  (wear), ;4&apos; (slim-fit) (wear), &amp;B (change cloths), 4F
(skirt) B (wear outside)
&apos; 41 ë *(Zhao Yun; P), �k�1 -4- (Liu ë* (Zhao Yun; P), �k�1-4- (Liu ë* (Zhao Yun; P), ¸fa (Han
(Guan Bei; P), ÃXAM (Zhuge Liang; P), Bei; P), Ü&apos;� (Zhang Fei; P), %7 Xin; P), %7 (Cao Cao; P), �k�1-4-
Yu; P) Ü&amp;quot;t (Zhang Fei; P), %7 (Cao (Cao Cao; P), Xû (Xiahou (Liu Bei; P), KIJ-Y (Asura; P)
Cao; P) Yuan; P)
x 2 EB (Shijiazhuang; C), 4Fa EB (Shijiazhuang; C), JkAt Ü)Z (Hefei; C), 4FH (Jinan; C),
(Zhengzhou; (Luoyang; C), Ü-Ic (Xian; C), A (Taiyuan; C), 4FH (Ji-nan; C), Ü ÉÇ (Wuhan; C), EB (Shiji-
C) 4 (Xuchang; C), k).t (Taiyuan) )Z (Hefei; C), Ü-1� (Xi-an; C) azhuang; C), Hw (Nanning; C)
</table>
<tableCaption confidence="0.999968">
Table 4: Comparison between NG2, NG5 and DEP Embeddings. (P: personal name, C: city name)
</tableCaption>
<subsectionHeader confidence="0.997911">
6.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999883571428572">
To analyze the difference between the three Chi-
nese embeddings methods qualitatively, we man-
ually inspect the words “B (wear)”, “&apos;411 (Guan
Yu, a person name in the novel ‘_: q iP; SC
(Romance of the three kingdoms)’)”, and “x2
(Zhengzhou, a city)”. Their most similar words
are shown in Table 4.
</bodyText>
<subsectionHeader confidence="0.954822">
Word Similarity
</subsectionHeader>
<bodyText confidence="0.999988555555556">
For the word “B (wear)”, both NG5 and NG2
yield similar words such as “4B (wear)”, “4
X (wear)”, “ (wear)” and related words such as
“1A (shorts)”, “;A (slim-fit)”, “4F@ (coat)”,
“*f (skirt)”, although NG5 gives more related
words. In contrast, DEP gives only words that are
similar both syntactically and semantically. This
observation holds for other verbs and nouns, and
can be explained by the context extraction method-
s. For instance, the word “B (wear)” usually takes
one of the words “1A (shorts)”, “4F@ (coat)”,
“*f (skirt)” as its object, and thus shares sim-
ilar contexts with them in NG5 and NG2. The
context extraction method in DEP, on the oth-
er hand, yields different context across syntactic
roles, such as verbs (e.g. “B (wear)”) and their
objects (e.g. “1A (shorts)” and “11F@ (coat)”).
Observations on the person name “&apos;4;1 (Guan
Yu)” and location “x2 (Zhengzhou)” are simi-
lar. For “&apos;4;1 (Guan Yu)”, NG5 and NG2 can
yield more person names in the same novel, while
DEP yields person names from other novels (i.e.
“¸fa (Hanxin)” and “KIJ-Y (Asura)”). For “x
2 (Zhengzhou)”, the provincial capital of “àH
(Henan)”, NG5 and NG2 give more cities in the
same province “àH (Henan)”, while DEP yields
capitals of other provinces.
</bodyText>
<subsectionHeader confidence="0.913916">
Analogy Detection
</subsectionHeader>
<bodyText confidence="0.999980806451613">
As mentioned in Section 2.3, both 3COSADD
and 3COSMUL seek a word b* that is similar to
b and a* but dissimilar to a. Ideally, the two word
pairs b:b* and a:a* should be semantically similar
while the two word pairs a:b and a*:b* should be
semantically related. Therefore, 3COSADD and
3COSMUL require the embeddings to give high-
er cosine scores for both semantically similar and
related words.
Our analysis above shows that word-context
embeddings tend to mix semantically related and
similar words, but dependency-context embed-
dings only capture semantic similarity. This partly
explains the reason that dependency-context word
embeddings are weak for analogy detection.
It has also been shown in Section 6.3 that the
performances of analogy detection vary across dif-
ferent types of relations, which indicates that there
are more sophisticated underlying factors. One in-
tuitive explanation is that different semantic rela-
tions correspond to different syntactic dependency
structures. For example, the male-female family
member relation is expected to stand less frequent-
ly in a syntactic dependency relation, compared
with geographic relations such as city-country,
which stand frequently in attributional syntactic
relations (e.g. “London, England”). As a result,
where the coupling between syntactic and seman-
tic relations is weak, our analysis in Section 6.3
and other work based on syntactic relations can
find limitations.
</bodyText>
<subsectionHeader confidence="0.9975805">
6.5 Syntactic Dependencies for Improved
Analogy Detection
</subsectionHeader>
<bodyText confidence="0.999805">
The results on CAQS using the method in Section
4 are shown in the IMP rows of Table 2. The
method achieves significant improvements (from
80.0% to 90.9% using NG5) compared with Levy
and Goldberg’s method. In addition, DEP al-
so performs significantly better than with MUL,
with an increase from 22.0% to 89.8%. The main
reason for this improvement is that the filtering
process using syntactic dependencies successfully
prunes noisy words.
</bodyText>
<page confidence="0.978069">
2447
</page>
<table confidence="0.998016571428571">
Seed Count Prec
°i (eat), *,* (apple), VOB 572 84.70%
~ (play), 4j* (piano), VOB 142 40.49%
3F (wear), -$k (clothing), VOB 452 67.37%
&apos;&apos;q (write), +ik (novel), VOB 441 53.40%
�q (China), AEY (Beijing), ATT 2224 95.23%
i Jot (Hubei), sqa (Wuhan), ATT 3201 96.34%
</table>
<tableCaption confidence="0.999136">
Table 5: Main results of Analogy Mining.
</tableCaption>
<bodyText confidence="0.999963785714286">
Error analysis shows that the main errors by the
improved method are quite different from those
by the baseline. For instance, the main errors of
Levy and Goldberg’s method for the city-province
relation are caused by giving another province as
the answer, while the improved method gives the
name of the country as answer. This is because ir-
relevant provinces do not co-occur frequently with
the city in syntactic dependencies, and hence can
be filtered by our method. On the other hand, both
the country name and province name co-occur fre-
quently with the city name in syntactic dependen-
cies, and our method cannot make a choice be-
tween them.
</bodyText>
<subsectionHeader confidence="0.9994685">
6.6 Dependency Structure Embeddings for
Analogy Mining
</subsectionHeader>
<bodyText confidence="0.999970104166667">
Shown in Table 5, we use six seeds to mine anal-
ogous dependencies. The first seed is used for de-
velopment and the others for test. The first three
seeds, the fourth seed and the last two seeds be-
long to the Use:Thing, Produce:Thing and Sub-
Location:Location relations, respectively. α and
Q are set to 20, and 0.6, respectively. Each set of
mined dependencies together with the seed depen-
dency and relation type is shown to two human
evaluators, who are required to give a Yes/No an-
swer to each dependency in the set. We take the
average scores of the two evaluators (the average
inter-annotator agreement is 0.95) as the final pre-
cision scores.
As shown in the table, the precisions using dif-
ferent seeds are quite different, ranging from 40%
to 96%. One possible reason is that different rela-
tions have different numbers of analogous depen-
dencies, ranging from dozens to thousands, and
thus the fixed thresholds tuned on a development
seed does not apply as effectively to all test cas-
es. For instance, “ (play)” and its analogous ac-
tions, “,!k (blow)” and “4--1 (play)”, are all human
actions on musical instruments, while the action-
s “&apos;It (eat)” and “-- (write)” can apply to many
patients. For the seed &lt; (play), M (piano),
VOB&gt;, irrelevant results such as &lt;* (use),
-1� (scissors), VOB&gt; and &lt;* (use), &gt;A (flash-
light), VOB&gt;, have the verb “* (use)”, which
is also a human action, yet cannot be considered
as usage of the patients “ -I� (scissors)” and “&gt;
A (flashlight)”. Because of the stricter selectional
preference of “ (play)”, its precision of analogy
mining is lower.
We tentatively measure the recall of the algo-
rithm by taking the first three types of word pairs
in CAQS as the gold set, which contains 801 word
pairs. All the three types of word pairs belong to
the relation Sub-Location:Location. The recall is
computed as the percentage of the gold word pairs
covered by the mined dependencies. When us-
ing the two seeds &lt;�a (Wuhan), illAE (Hubei),
ATT&gt; and &lt;AEq (Beijing), v{&apos;q (China), ATT&gt;
for analogy mining, the recalls are 50.2% and
11.3%, respectively. Their union recall is 56.8%.
When the precision of each seed is similar, we can
achieve better recall without precision loss by us-
ing more seeds.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999966857142857">
Turney (2006) introduces a latent relational anal-
ysis (LRA) model to measure relational similari-
ty, and apply a novel co-occurrence-based method
for analogy filtering. The model can be used
for both analogy detection and relation classifi-
cation, yet cannot scale up well to large dataset-
s due to the complexity of Singular Value De-
composition. Recently, distributed word repre-
sentations using the skip-gram model (Mikolov
et al., 2013a) has been shown to give competi-
tive results on analogy detection. Levy and Gold-
berg (2014a) extends the skip-gram method with
dependency-context embeddings. We study the ef-
fect of Levy and Goldberg’s embeddings on analo-
gy detection, and further extend their embeddings
to dependency-context dependency structure em-
beddings for analogy mining.
Chiu et al. (2007) presents a similarity graph
tranversal (SGT) method to mine analogous re-
lations from raw English text automatically, us-
ing syntactic dependencies to find candidate rela-
tions. The method is unsupervised, and can scale
up well to large data sets. However, Chiu et al.
(2007) mainly focuses on relations between sub-
jects and objects because of its word-pair extrac-
tion method. O´ S´eaghdha and Copestake (2009)
is a supervised method, which combines lexical
similarity and relational similarity to classify se-
</bodyText>
<page confidence="0.953797">
2448
</page>
<bodyText confidence="0.999903444444444">
mantic relations. These methods are based on dis-
tributional word representation models and fit for
classifying noun-noun word pairs. In contrast, our
methods are based on distributed word representa-
tion models, and can mine noun-noun word pairs
as well as verb-noun word pairs. In addition, our
analogy mining method is unsupervised, while the
methods of both Turney (2006) and O´ S´eaghdha
and Copestake (2009) are supervised.
</bodyText>
<sectionHeader confidence="0.99747" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999984571428571">
We studied several Chinese relational similarity
tasks to train embeddings under the context of dis-
tributed word representations using the skip-gram
model and syntactic dependencies. For Chinese
analogy detection, we compared word-context and
dependency-context embeddings, finding that the
former results in much better accuracies. Observ-
ing that common relations in Chinese are frequent-
ly represented by syntactic dependencies, we im-
proved Chinese analogy detection using a depen-
dency context. Further, we empirically studied
Chinese analogy mining by proposing a bootstrap-
ping algorithm using a novel distributed represen-
tation of syntactic dependencies.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999920583333333">
We thank the anonymous reviewers for their con-
structive comments, and gratefully acknowledge
the support of the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301,
the National Natural Science Foundation of Chi-
na (No. 61572245, 61170144, 61103089), Ma-
jor National Social Science Fund of China (No.
12&amp;ZD227), Scientific Research Foundation of
Shandong Province Outstanding Young Scientist
Award (No. BS2013DX020) and Humanities and
Social Science Projects of Ludong University (No.
WY2013003).
</bodyText>
<sectionHeader confidence="0.998624" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999816125">
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of ACL, Balti-
more, Maryland, June.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2008. Www sits the sat: Measuring re-
lational similarity on the web. In ECAI, pages 333–
337.
Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Lt-
p: A chinese language technology platform. In Pro-
ceedings of COLING, pages 13–16.
Andy Chiu, Pascal Poupart, and Chrysanne DiMar-
co. 2007. Generating lexical analogies using de-
pendency relations. In Proceedings of EMNLP-
CoNLL 2007, pages 561–570, Prague, Czech Re-
public, June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160–167. ACM.
Amac¸ Herdaˇgdelen and Marco Baroni. 2009. Bag-
pack: A general framework to represent semantic
relations. In Proceedings of the Workshop on Ge-
ometrical Models of Natural Language Semantics,
pages 33–40.
Peng Jin and Yunfang Wu. 2012. Semeval-2012 task 4:
evaluating chinese word similarity. In Proceedings
of the Sixth International Workshop on Semantic E-
valuation, pages 374–377.
Peng Jin, John Carroll, Yunfang Wu, and Diana Mc-
Carthy. 2012. Distributional similarity for chinese:
Exploiting characters and radicals. Mathematical
Problems in Engineering, 2012.
David A Jurgens, Peter D Turney, Saif M Mohammad,
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 356–364. Association for
Computational Linguistics.
Beth Levin. 1993. English verb classes and alterna-
tions: a preliminary investigation.
Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of ACL,
pages 302–308, Baltimore, Maryland, June.
Omer Levy and Yoav Goldberg. 2014b. Linguistic reg-
ularities in sparse and explicit word representations.
In Proceedings of CONLL, pages 171–180, Ann Ar-
bor, Michigan, June.
Douglas L Medin, Robert L Goldstone, and Dedre
Gentner. 1990. Similarity involving attributes and
relations: Judgments of similarity and difference are
not inverses. Psychological Science, 1(1):64–69.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751. Citeseer.
</reference>
<page confidence="0.829506">
2449
</page>
<reference confidence="0.99970608">
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML, pages 641–648. ACM.
Diarmuid O´ S´eaghdha and Ann Copestake. 2009. Us-
ing lexical and relational similarity to classify se-
mantic relations. In Proceedings of EACL 2009,
pages 621–629, Athens, Greece, March.
Likun Qiu, Yunfang Wu, and Yanqiu Shao. 2011.
Combining contextual and structural information for
supersense tagging of chinese unknown words. In
Computational Linguistics and Intelligent Text Pro-
cessing, pages 15–28. Springer.
Likun Qiu, Yue Zhang, Peng Jin, and Houfeng Wang.
2014. Multi-view chinese treebanking. In Proceed-
ings of COLING, pages 257–268, Dublin, Ireland,
August.
Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class
mining: Distributional vs. pattern-based approach-
es. In Proceedings of COLING, pages 993–1001,
Beijing, China, August.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of ICML, pages 129–136.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of A-
CL, pages 384–394.
Peter D Turney and Michael L Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251–278.
Peter D Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Peter D Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. arXiv preprint arXiv:1310.5042.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the penn chi-
nese treebank with semantic knowledge. In Natural
Language Processing–IJCNLP 2005, pages 70–81.
Springer.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
Meishan Zhang, Yue Zhang, Wanxiang Che, and T-
ing Liu. 2014. A semantics oriented grammar for
chinese treebanking. In Computational Linguistic-
s and Intelligent Text Processing, pages 366–378.
Springer.
</reference>
<page confidence="0.986156">
2450
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.834544">
<title confidence="0.9906375">Syntactic Dependencies and Distributed Word Representations Chinese Analogy Detection and Mining</title>
<author confidence="0.999505">Yue Yanan</author>
<affiliation confidence="0.993457666666667">of Chinese Language and Literature, Ludong University, University of Technology and Design, School, Wuhan University,</affiliation>
<email confidence="0.890198">qiulikun@pku.edu.cn,yuezhang@sutd.edu.sg,luyanan@whu.edu.cn</email>
<abstract confidence="0.998603235294118">Distributed word representations capture relational similarities by means of vector arithmetics, giving high accuracies on analogy detection. We empirically investigate the use of syntactic dependencies on improving Chinese analogy detection based on distributed word representations, showing that a dependency-based embeddings does not perform better than an ngram-based embeddings, but dependency structures can be used to improve analogy detection by filtering candidates. In addition, we show that distributed representations of dependency structure can be used for measuring relational similarities, thereby help analogy mining.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="3630" citStr="Bansal et al., 2014" startWordPosition="512" endWordPosition="515">lational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed word representation of a word using its context words in local ngram window (Mikolov et al., 2013a), and the other training the distributed representation of a word using words in a syntactic dependency context (Levy and Goldberg, 2014b; Bansal et al., 2014). The latter has attracted much recent atten2441 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2441–2450, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tion due to its potential in capturing more syntactic regularities. It has been shown to outperform the former in a variety of NLP tasks, and can potentially also improve relation similarity. Our experiments on both English and Chinese show that the dependency-context embeddings consistently under-perform ngram-context embeddings. We give some theoretica</context>
<context position="11435" citStr="Bansal et al., 2014" startWordPosition="1739" endWordPosition="1742"> directly for embeddings. Recently, a dependency context has been used for the skip-gram method, for capturing more syntactic regularities. Taking the sentence in Figure 1 for example, a bi-gram context for the word “.A (graduate)” can be “4�C4 (Obama), ott (President), t (from), , 4 (Harvard)”, while a dependency context of the same word can be “1991c/ADV, Ao tk/SBV, t/CMP, �k * M/POB t”1, where “ADV, SBV, CMP, POB” indicate adverbial modifier, subject, complement and prepositional object, respectively. It has been shown that a dependency context leads to embeddings that better help parsing (Bansal et al., 2014) and measuring word similarity (Levy and Goldberg, 2014a), compared with ngram contexts. However, little previous work has systematically compared dependency contexts with ngram contexts in analogy detection. We empirically study this problem (c.f Section 6.3), finding that dependency context leads to significantly worse analogy detection results for both Chinese and English using state-of-the-art embedding-based methods (Levy and Goldberg, 2014b). We give analysis in Section 6.4. 1The last token is a grand-child of “.A (graduate)”, via the preposition “t (at)” (Levy and Goldberg, 2014a). 2443</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of ACL, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="7250" citStr="Bengio et al., 2003" startWordPosition="1071" endWordPosition="1075">sk has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They capture linguistic regularities in languages (Mikolov et al., 2013b) and have been used in various tasks (Collobert and Weston, 2008; Turian et al., 2010; Socher et al., 2011). In this paper, we apply the Skip-gram method of Mikolov et al. (2013a) for training embeddings, which works by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly us</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danushka Bollegala</author>
<author>Yutaka Matsuo</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Www sits the sat: Measuring relational similarity on the web. In</title>
<date>2008</date>
<booktitle>ECAI,</booktitle>
<pages>333--337</pages>
<contexts>
<context position="6385" citStr="Bollegala et al., 2008" startWordPosition="930" endWordPosition="933">ease largescale Chinese word embeddings. 2 Background 2.1 Relational Similarity Tasks There are three main tasks for relational similarity. This first is relation classification, which has been used in Task 2 of SemEval 2012 (Jurgens et al., 2012). In this task, all four words in two word pairs are given, and one needs to judge whether Figure 1: Dependency tree of the sentence “1991* (in 1991) , (,) 4�C4 (Obama) A,tt (President) *A, (graduate) t (from) T (Harvard) �k*M (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analo</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2008</marker>
<rawString>Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2008. Www sits the sat: Measuring relational similarity on the web. In ECAI, pages 333– 337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Ltp: A chinese language technology platform.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>13--16</pages>
<contexts>
<context position="21019" citStr="Che et al., 2010" startWordPosition="3297" endWordPosition="3300">2.7% P@50 15.0% 16.2% 17.0% P@100 11.5% 12.2% 12.8% CWS Kendall’s τ 38.6% 44.1% 42.4% Spearman’s ρ 54.5% 62.2% 60.7% Table 1: Results on Cilin and CWS. Because embeddings are central for analogy detection, yet there is little large-scale evaluation results on Chinese embeddings in the literature, we perform embedding evaluation on two datasets. The first one is the Chinese WordSim (CWS), translated from the English WordSim-353 Set and re-scored by native Chinese speakers (Jin and Wu, 2012). This dataset consists of 297 word pairs. The second one is the Chinese thesaurus Tongyicicilin (Cilin) (Che et al., 2010), which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models (Qiu et al., 2011; Jin et al., 2012). The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar. For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset7 (Mikolov et al., 2013a), to evaluate the English word embeddings of Levy and Goldberg (2014a)8 on analogy detection. On both the CAQS and the Google datasets, </context>
</contexts>
<marker>Che, Li, Liu, 2010</marker>
<rawString>Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. Ltp: A chinese language technology platform. In Proceedings of COLING, pages 13–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andy Chiu</author>
<author>Pascal Poupart</author>
<author>Chrysanne DiMarco</author>
</authors>
<title>Generating lexical analogies using dependency relations.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL 2007,</booktitle>
<pages>561--570</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2794" citStr="Chiu et al., 2007" startWordPosition="383" endWordPosition="387">n, without stressing the difference between distributed and distributional (i.e. counting-based) word representations. Most work on embedding-based analogy detection uses relational similarities as a measure of the quality of embeddings. However, relatively little has been done in the opposite direction, exploring how to leverage embeddings for improving relational similarity algorithms. We empirically study the use of word embeddings for Chinese analogy detection and mining, leveraging syntactic dependencies, which has been shown to be closely associated with semantic relations (Levin, 1993; Chiu et al., 2007). Compared with many other languages, this association is particularly strong for Chinese, which is fully configurational and lacks morphology. To our knowledge, relatively little work has been reported on Chinese relational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed word representation of</context>
<context position="4700" citStr="Chiu et al. (2007)" startWordPosition="668" endWordPosition="671">on both English and Chinese show that the dependency-context embeddings consistently under-perform ngram-context embeddings. We give some theoretical justifications to the findings. Second, we propose to use syntactic dependencies as a context for improving embeddingbased analogy detection, pruning the search space and filtering noise using syntactic dependencies. While highly useful for measuring relational similarities, attributional similarities between words are not the only source of information for analogy detection. Traditional methods, such as Turney and Littman (2005), Turney (2006), Chiu et al. (2007) and O´ S´eaghdha and Copestake (2009), also leverage context between word pairs in a corpus for better accuracies, which the current embedding-based methods ignore. Results show that our proposed method achieves significant improvements for this task. Third, we show that a novel distributed representation of syntactic dependencies between word pairs can be used to mine analogous dependencies from a large Chinese corpus. Inspired by the fact that distributed word representations can be used to measure word similarities, we use our distributed dependency representations to measure relation simi</context>
<context position="6778" citStr="Chiu et al., 2007" startWordPosition="996" endWordPosition="999">, (,) 4�C4 (Obama) A,tt (President) *A, (graduate) t (from) T (Harvard) �k*M (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They ca</context>
<context position="9748" citStr="Chiu et al. (2007)" startWordPosition="1478" endWordPosition="1481"> pairs “good:better” and “rough:rougher” are analogous, and the past tense inflections “see:saw” and “return:returned” are analogous. However, such inflectional relations do not apply to Chinese, which is fully configurational and lacks morphology. Consequently, our main focus is semantic similarities, which include antonymy (e.g. (A (hot):;? (cold)) VS (A (fast):� (slow))), meronymy (e.g. (-* (car):* 1&apos; (wheel)) VS (;fit (bear):* (paw))), gender (e.g. (W,k (man):*,k (woman)) VS (q-:E (king): L (queen))) and function relations (e.g. (-cARL (clothing):3F (wear)) VS (Mm1&apos; (hat):A (wear))), etc. Chiu et al. (2007) show that English semantic relations are also reflected by syntactic dependencies. Their finding coincides with Levin (1993), who study English verbs. We find that this observation is even more prevalent for Chinese. In our automatically-parsed Chinese corpus of 3.4 billion words (Section 5.1), 86.4% word pairs from the analogy test dataset (Section 5.2) have corresponding dependencies, each of which appearing at least ten times. The frequent correlation between semantic relations and syntactic dependencies can be due to the lack of morphology and function words in Chinese. In fact, Chinese s</context>
<context position="32848" citStr="Chiu et al. (2007)" startWordPosition="5246" endWordPosition="5249">an be used for both analogy detection and relation classification, yet cannot scale up well to large datasets due to the complexity of Singular Value Decomposition. Recently, distributed word representations using the skip-gram model (Mikolov et al., 2013a) has been shown to give competitive results on analogy detection. Levy and Goldberg (2014a) extends the skip-gram method with dependency-context embeddings. We study the effect of Levy and Goldberg’s embeddings on analogy detection, and further extend their embeddings to dependency-context dependency structure embeddings for analogy mining. Chiu et al. (2007) presents a similarity graph tranversal (SGT) method to mine analogous relations from raw English text automatically, using syntactic dependencies to find candidate relations. The method is unsupervised, and can scale up well to large data sets. However, Chiu et al. (2007) mainly focuses on relations between subjects and objects because of its word-pair extraction method. O´ S´eaghdha and Copestake (2009) is a supervised method, which combines lexical similarity and relational similarity to classify se2448 mantic relations. These methods are based on distributional word representation models a</context>
</contexts>
<marker>Chiu, Poupart, DiMarco, 2007</marker>
<rawString>Andy Chiu, Pascal Poupart, and Chrysanne DiMarco. 2007. Generating lexical analogies using dependency relations. In Proceedings of EMNLPCoNLL 2007, pages 561–570, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7508" citStr="Collobert and Weston, 2008" startWordPosition="1110" endWordPosition="1113"> word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They capture linguistic regularities in languages (Mikolov et al., 2013b) and have been used in various tasks (Collobert and Weston, 2008; Turian et al., 2010; Socher et al., 2011). In this paper, we apply the Skip-gram method of Mikolov et al. (2013a) for training embeddings, which works by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised analogy detection. 2442 2.3 Embedding-based Analogy Detection Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. Mikolov et al. (2013b) show that the</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amac¸ Herdaˇgdelen</author>
<author>Marco Baroni</author>
</authors>
<title>Bagpack: A general framework to represent semantic relations.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>33--40</pages>
<marker>Herdaˇgdelen, Baroni, 2009</marker>
<rawString>Amac¸ Herdaˇgdelen and Marco Baroni. 2009. Bagpack: A general framework to represent semantic relations. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Jin</author>
<author>Yunfang Wu</author>
</authors>
<title>Semeval-2012 task 4: evaluating chinese word similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>374--377</pages>
<contexts>
<context position="20896" citStr="Jin and Wu, 2012" startWordPosition="3277" endWordPosition="3280">. 2445 Data Metrics NG5 NG2 DEP Cilin P@1 43.3% 45.9% 43.6% P@5 31.1% 33.3% 32.6% P@10 25.5% 27.5% 27.5% P@20 20.5% 22.2% 22.7% P@50 15.0% 16.2% 17.0% P@100 11.5% 12.2% 12.8% CWS Kendall’s τ 38.6% 44.1% 42.4% Spearman’s ρ 54.5% 62.2% 60.7% Table 1: Results on Cilin and CWS. Because embeddings are central for analogy detection, yet there is little large-scale evaluation results on Chinese embeddings in the literature, we perform embedding evaluation on two datasets. The first one is the Chinese WordSim (CWS), translated from the English WordSim-353 Set and re-scored by native Chinese speakers (Jin and Wu, 2012). This dataset consists of 297 word pairs. The second one is the Chinese thesaurus Tongyicicilin (Cilin) (Che et al., 2010), which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models (Qiu et al., 2011; Jin et al., 2012). The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar. For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset7 (Mikolov et al., 2013a), to evaluate</context>
</contexts>
<marker>Jin, Wu, 2012</marker>
<rawString>Peng Jin and Yunfang Wu. 2012. Semeval-2012 task 4: evaluating chinese word similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 374–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Jin</author>
<author>John Carroll</author>
<author>Yunfang Wu</author>
<author>Diana McCarthy</author>
</authors>
<title>Distributional similarity for chinese: Exploiting characters and radicals.</title>
<date>2012</date>
<booktitle>Mathematical Problems in Engineering,</booktitle>
<contexts>
<context position="21226" citStr="Jin et al., 2012" startWordPosition="3331" endWordPosition="3334">et there is little large-scale evaluation results on Chinese embeddings in the literature, we perform embedding evaluation on two datasets. The first one is the Chinese WordSim (CWS), translated from the English WordSim-353 Set and re-scored by native Chinese speakers (Jin and Wu, 2012). This dataset consists of 297 word pairs. The second one is the Chinese thesaurus Tongyicicilin (Cilin) (Che et al., 2010), which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models (Qiu et al., 2011; Jin et al., 2012). The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar. For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset7 (Mikolov et al., 2013a), to evaluate the English word embeddings of Levy and Goldberg (2014a)8 on analogy detection. On both the CAQS and the Google datasets, the 3COSMUL method (Levy and Goldberg, 2014b) is used to to answer analogy questions based on given embeddings. The results on the CWS dataset are evaluated using the two standard metrics for the task, namel</context>
</contexts>
<marker>Jin, Carroll, Wu, McCarthy, 2012</marker>
<rawString>Peng Jin, John Carroll, Yunfang Wu, and Diana McCarthy. 2012. Distributional similarity for chinese: Exploiting characters and radicals. Mathematical Problems in Engineering, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Jurgens</author>
<author>Peter D Turney</author>
<author>Saif M Mohammad</author>
<author>Keith J Holyoak</author>
</authors>
<title>Semeval-2012 task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>356--364</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6010" citStr="Jurgens et al., 2012" startWordPosition="863" endWordPosition="866">ings, and experiments on a large Chinese corpus show that the method can achieve a precision of 95.2% at a recall of 56.8%. Our automatically-parsed corpus, trained embeddings and evaluation datasets are released publicly at http://people.sutd.edu.sg/ ˜yue_zhang/publication.html. To our knowledge, we are the first to present results on Chinese analogy detection and to release largescale Chinese word embeddings. 2 Background 2.1 Relational Similarity Tasks There are three main tasks for relational similarity. This first is relation classification, which has been used in Task 2 of SemEval 2012 (Jurgens et al., 2012). In this task, all four words in two word pairs are given, and one needs to judge whether Figure 1: Dependency tree of the sentence “1991* (in 1991) , (,) 4�C4 (Obama) A,tt (President) *A, (graduate) t (from) T (Harvard) �k*M (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the</context>
</contexts>
<marker>Jurgens, Turney, Mohammad, Holyoak, 2012</marker>
<rawString>David A Jurgens, Peter D Turney, Saif M Mohammad, and Keith J Holyoak. 2012. Semeval-2012 task 2: Measuring degrees of relational similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 356–364. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English verb classes and alternations: a preliminary investigation.</title>
<date>1993</date>
<contexts>
<context position="2774" citStr="Levin, 1993" startWordPosition="381" endWordPosition="382">logy detection, without stressing the difference between distributed and distributional (i.e. counting-based) word representations. Most work on embedding-based analogy detection uses relational similarities as a measure of the quality of embeddings. However, relatively little has been done in the opposite direction, exploring how to leverage embeddings for improving relational similarity algorithms. We empirically study the use of word embeddings for Chinese analogy detection and mining, leveraging syntactic dependencies, which has been shown to be closely associated with semantic relations (Levin, 1993; Chiu et al., 2007). Compared with many other languages, this association is particularly strong for Chinese, which is fully configurational and lacks morphology. To our knowledge, relatively little work has been reported on Chinese relational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed wo</context>
<context position="9873" citStr="Levin (1993)" startWordPosition="1498" endWordPosition="1499">s. However, such inflectional relations do not apply to Chinese, which is fully configurational and lacks morphology. Consequently, our main focus is semantic similarities, which include antonymy (e.g. (A (hot):;? (cold)) VS (A (fast):� (slow))), meronymy (e.g. (-* (car):* 1&apos; (wheel)) VS (;fit (bear):* (paw))), gender (e.g. (W,k (man):*,k (woman)) VS (q-:E (king): L (queen))) and function relations (e.g. (-cARL (clothing):3F (wear)) VS (Mm1&apos; (hat):A (wear))), etc. Chiu et al. (2007) show that English semantic relations are also reflected by syntactic dependencies. Their finding coincides with Levin (1993), who study English verbs. We find that this observation is even more prevalent for Chinese. In our automatically-parsed Chinese corpus of 3.4 billion words (Section 5.1), 86.4% word pairs from the analogy test dataset (Section 5.2) have corresponding dependencies, each of which appearing at least ten times. The frequent correlation between semantic relations and syntactic dependencies can be due to the lack of morphology and function words in Chinese. In fact, Chinese syntactic ambiguities often need to be resolved by leveraging semantic information (Xiong et al., 2005; Zhang et al., 2014). A</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English verb classes and alternations: a preliminary investigation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>302--308</pages>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="1665" citStr="Levy and Goldberg, 2014" startWordPosition="218" endWordPosition="221">larity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best system in the SemEval 2012 shared task on analogy detection. Levy and Goldberg (2014b) further improve Mikolov’s relational similarity measure method using novel arithmetic combinations of attributional similarities. For simplicity, we call the method of Mikolov et al. embeddingbased analogy detection, without stressing the difference between distributed and distributional (i.e. counting</context>
<context position="3607" citStr="Levy and Goldberg, 2014" startWordPosition="508" endWordPosition="511">een reported on Chinese relational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed word representation of a word using its context words in local ngram window (Mikolov et al., 2013a), and the other training the distributed representation of a word using words in a syntactic dependency context (Levy and Goldberg, 2014b; Bansal et al., 2014). The latter has attracted much recent atten2441 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2441–2450, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tion due to its potential in capturing more syntactic regularities. It has been shown to outperform the former in a variety of NLP tasks, and can potentially also improve relation similarity. Our experiments on both English and Chinese show that the dependency-context embeddings consistently under-perform ngram-context embeddings. </context>
<context position="6722" citStr="Levy and Goldberg, 2014" startWordPosition="986" endWordPosition="989">r Figure 1: Dependency tree of the sentence “1991* (in 1991) , (,) 4�C4 (Obama) A,tt (President) *A, (graduate) t (from) T (Harvard) �k*M (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector represe</context>
<context position="8283" citStr="Levy and Goldberg (2014" startWordPosition="1250" endWordPosition="1253"> by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised analogy detection. 2442 2.3 Embedding-based Analogy Detection Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. Mikolov et al. (2013b) show that the task can be solved by finding a word that maximizes: score = sim(b*, b − a + a*) (1) where sim is a similarity measure, typically the cosine function. Levy and Goldberg (2014b) show that the Equation 1 is equivalent to: score = cos(b*, b)−cos(b*, a)+cos(b*, a*) (2) As a result, the goal of analogy detection is to find a word b* which is similar to b and a* but different from a. Levy and Goldberg (2014b) further propose to substitute the addictive functions in Equation 2 with multiplicative functions: score = cos(b*,b)cos(b*,a*)/(cos(b*,a) + ε) (3) Here ε = 0.001 is used to prevent division by zero. Their experiments show that the use of Equation 3 can improve the state-of-the-art. Following Levy and Goldberg (2014b), we refer to Equation 1 and 2 as 3COSADD and Equ</context>
<context position="11490" citStr="Levy and Goldberg, 2014" startWordPosition="1748" endWordPosition="1751">text has been used for the skip-gram method, for capturing more syntactic regularities. Taking the sentence in Figure 1 for example, a bi-gram context for the word “.A (graduate)” can be “4�C4 (Obama), ott (President), t (from), , 4 (Harvard)”, while a dependency context of the same word can be “1991c/ADV, Ao tk/SBV, t/CMP, �k * M/POB t”1, where “ADV, SBV, CMP, POB” indicate adverbial modifier, subject, complement and prepositional object, respectively. It has been shown that a dependency context leads to embeddings that better help parsing (Bansal et al., 2014) and measuring word similarity (Levy and Goldberg, 2014a), compared with ngram contexts. However, little previous work has systematically compared dependency contexts with ngram contexts in analogy detection. We empirically study this problem (c.f Section 6.3), finding that dependency context leads to significantly worse analogy detection results for both Chinese and English using state-of-the-art embedding-based methods (Levy and Goldberg, 2014b). We give analysis in Section 6.4. 1The last token is a grand-child of “.A (graduate)”, via the preposition “t (at)” (Levy and Goldberg, 2014a). 2443 4 Search Space Pruning Using Syntactic Dependencies We</context>
<context position="15297" citStr="Levy and Goldberg, 2014" startWordPosition="2361" endWordPosition="2364">oreXY &gt; MScore and Triple G/ WP then 20 AddToSet (Triple,WP); 21 s =Triple; 22 Mine (DT,DW,s,WP,α,Q); 23 end 24 end 25 end 26 WP =∅; 27 Mine (DT,DW,s,WP,α,Q); Algorithm 1: Bootstrapping for analogy mining. be measured by distributed relation representations. Based on the observation in Section 2.4, semantically analogous word pairs typically have syntactic dependencies. We use the skip-gram algorithm to train distributed representations of syntactic dependencies, and use them for mining analogous word pairs. With respect to the skip-gram model, words are the most common target for embeddings (Levy and Goldberg, 2014b; Levy and Goldberg, 2014a; Mikolov et al., 2013a), although continuous vector representations can be trained for other structures. For example, Mikolov et al. (2013a) take idiomatic phrases as embedding targets. Dependencies, which consist of a modifier word, a head word and a syntactic relation between them, can also be represented by continuous embeddings using the same algorithm. To induce dependency embeddings, we take the union of the dependency context of both the dependent and the head of a dependency as the context. For instance, in the example sentence, the context of the dependency</context>
<context position="21551" citStr="Levy and Goldberg (2014" startWordPosition="3384" endWordPosition="3387">ord pairs. The second one is the Chinese thesaurus Tongyicicilin (Cilin) (Che et al., 2010), which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models (Qiu et al., 2011; Jin et al., 2012). The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar. For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset7 (Mikolov et al., 2013a), to evaluate the English word embeddings of Levy and Goldberg (2014a)8 on analogy detection. On both the CAQS and the Google datasets, the 3COSMUL method (Levy and Goldberg, 2014b) is used to to answer analogy questions based on given embeddings. The results on the CWS dataset are evaluated using the two standard metrics for the task, namely Spearman’s ρ and Kendall’s τ rank correlation coefficients. The results on Cilin are evaluated using Precision@K: the percentage of words from the top-K candidates that belong to the Cilin category of the target word. If one of the top-K candidates belongs to the same third-level category in Cilin as the target word, the </context>
<context position="23916" citStr="Levy and Goldberg (2014" startWordPosition="3750" endWordPosition="3753"> does not need to use large window sizes in training word-based embeddings for capturing word similarities. The result is similar to the finding of Shi et al. (2010), which indicates that a window size of 2 is better than a window size of 4 for capturing word similarity by using distributional word representations. DEP performs slightly worse than NG2 on CWS and Cilin in P@1 and P@5. However, it achieves better results on Cilin in P@10 to P@100 when more candidate similar words are evaluated. In contrast, NG5 and NG2 mix more semantically related words. This finding is consistent with that of Levy and Goldberg (2014a). Analogy Detection Table 2 shows the results of the three Chinese embeddings on CAQS. Unlike on Cilin and CWS, NG5 outperforms DEP, and is also slightly better than NG2. Similar tendency is shown in Table 3 for the three English embeddings evaluated on the Google dataset. These results show that dependency embeddings are relatively weak for answering analogy questions. On the other hand, the performance also varies across different relation types. 2446 Target NG5 NG2 DEP B (wear) 1A (shorts), ;4&apos; (slim-fit), 4&apos; 4&apos;B (wear), 4&apos;X (wear), 1A 4&apos; B (wear), 4&apos; X (wear),  B (wear), 5&apos;F @ (coat), *</context>
<context position="32576" citStr="Levy and Goldberg (2014" startWordPosition="5206" endWordPosition="5210"> similar, we can achieve better recall without precision loss by using more seeds. 7 Related Work Turney (2006) introduces a latent relational analysis (LRA) model to measure relational similarity, and apply a novel co-occurrence-based method for analogy filtering. The model can be used for both analogy detection and relation classification, yet cannot scale up well to large datasets due to the complexity of Singular Value Decomposition. Recently, distributed word representations using the skip-gram model (Mikolov et al., 2013a) has been shown to give competitive results on analogy detection. Levy and Goldberg (2014a) extends the skip-gram method with dependency-context embeddings. We study the effect of Levy and Goldberg’s embeddings on analogy detection, and further extend their embeddings to dependency-context dependency structure embeddings for analogy mining. Chiu et al. (2007) presents a similarity graph tranversal (SGT) method to mine analogous relations from raw English text automatically, using syntactic dependencies to find candidate relations. The method is unsupervised, and can scale up well to large data sets. However, Chiu et al. (2007) mainly focuses on relations between subjects and objec</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014a. Dependencybased word embeddings. In Proceedings of ACL, pages 302–308, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Linguistic regularities in sparse and explicit word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of CONLL,</booktitle>
<pages>171--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1665" citStr="Levy and Goldberg, 2014" startWordPosition="218" endWordPosition="221">larity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best system in the SemEval 2012 shared task on analogy detection. Levy and Goldberg (2014b) further improve Mikolov’s relational similarity measure method using novel arithmetic combinations of attributional similarities. For simplicity, we call the method of Mikolov et al. embeddingbased analogy detection, without stressing the difference between distributed and distributional (i.e. counting</context>
<context position="3607" citStr="Levy and Goldberg, 2014" startWordPosition="508" endWordPosition="511">een reported on Chinese relational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed word representation of a word using its context words in local ngram window (Mikolov et al., 2013a), and the other training the distributed representation of a word using words in a syntactic dependency context (Levy and Goldberg, 2014b; Bansal et al., 2014). The latter has attracted much recent atten2441 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2441–2450, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tion due to its potential in capturing more syntactic regularities. It has been shown to outperform the former in a variety of NLP tasks, and can potentially also improve relation similarity. Our experiments on both English and Chinese show that the dependency-context embeddings consistently under-perform ngram-context embeddings. </context>
<context position="6722" citStr="Levy and Goldberg, 2014" startWordPosition="986" endWordPosition="989">r Figure 1: Dependency tree of the sentence “1991* (in 1991) , (,) 4�C4 (Obama) A,tt (President) *A, (graduate) t (from) T (Harvard) �k*M (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector represe</context>
<context position="8283" citStr="Levy and Goldberg (2014" startWordPosition="1250" endWordPosition="1253"> by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised analogy detection. 2442 2.3 Embedding-based Analogy Detection Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. Mikolov et al. (2013b) show that the task can be solved by finding a word that maximizes: score = sim(b*, b − a + a*) (1) where sim is a similarity measure, typically the cosine function. Levy and Goldberg (2014b) show that the Equation 1 is equivalent to: score = cos(b*, b)−cos(b*, a)+cos(b*, a*) (2) As a result, the goal of analogy detection is to find a word b* which is similar to b and a* but different from a. Levy and Goldberg (2014b) further propose to substitute the addictive functions in Equation 2 with multiplicative functions: score = cos(b*,b)cos(b*,a*)/(cos(b*,a) + ε) (3) Here ε = 0.001 is used to prevent division by zero. Their experiments show that the use of Equation 3 can improve the state-of-the-art. Following Levy and Goldberg (2014b), we refer to Equation 1 and 2 as 3COSADD and Equ</context>
<context position="11490" citStr="Levy and Goldberg, 2014" startWordPosition="1748" endWordPosition="1751">text has been used for the skip-gram method, for capturing more syntactic regularities. Taking the sentence in Figure 1 for example, a bi-gram context for the word “.A (graduate)” can be “4�C4 (Obama), ott (President), t (from), , 4 (Harvard)”, while a dependency context of the same word can be “1991c/ADV, Ao tk/SBV, t/CMP, �k * M/POB t”1, where “ADV, SBV, CMP, POB” indicate adverbial modifier, subject, complement and prepositional object, respectively. It has been shown that a dependency context leads to embeddings that better help parsing (Bansal et al., 2014) and measuring word similarity (Levy and Goldberg, 2014a), compared with ngram contexts. However, little previous work has systematically compared dependency contexts with ngram contexts in analogy detection. We empirically study this problem (c.f Section 6.3), finding that dependency context leads to significantly worse analogy detection results for both Chinese and English using state-of-the-art embedding-based methods (Levy and Goldberg, 2014b). We give analysis in Section 6.4. 1The last token is a grand-child of “.A (graduate)”, via the preposition “t (at)” (Levy and Goldberg, 2014a). 2443 4 Search Space Pruning Using Syntactic Dependencies We</context>
<context position="15297" citStr="Levy and Goldberg, 2014" startWordPosition="2361" endWordPosition="2364">oreXY &gt; MScore and Triple G/ WP then 20 AddToSet (Triple,WP); 21 s =Triple; 22 Mine (DT,DW,s,WP,α,Q); 23 end 24 end 25 end 26 WP =∅; 27 Mine (DT,DW,s,WP,α,Q); Algorithm 1: Bootstrapping for analogy mining. be measured by distributed relation representations. Based on the observation in Section 2.4, semantically analogous word pairs typically have syntactic dependencies. We use the skip-gram algorithm to train distributed representations of syntactic dependencies, and use them for mining analogous word pairs. With respect to the skip-gram model, words are the most common target for embeddings (Levy and Goldberg, 2014b; Levy and Goldberg, 2014a; Mikolov et al., 2013a), although continuous vector representations can be trained for other structures. For example, Mikolov et al. (2013a) take idiomatic phrases as embedding targets. Dependencies, which consist of a modifier word, a head word and a syntactic relation between them, can also be represented by continuous embeddings using the same algorithm. To induce dependency embeddings, we take the union of the dependency context of both the dependent and the head of a dependency as the context. For instance, in the example sentence, the context of the dependency</context>
<context position="21551" citStr="Levy and Goldberg (2014" startWordPosition="3384" endWordPosition="3387">ord pairs. The second one is the Chinese thesaurus Tongyicicilin (Cilin) (Che et al., 2010), which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models (Qiu et al., 2011; Jin et al., 2012). The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar. For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset7 (Mikolov et al., 2013a), to evaluate the English word embeddings of Levy and Goldberg (2014a)8 on analogy detection. On both the CAQS and the Google datasets, the 3COSMUL method (Levy and Goldberg, 2014b) is used to to answer analogy questions based on given embeddings. The results on the CWS dataset are evaluated using the two standard metrics for the task, namely Spearman’s ρ and Kendall’s τ rank correlation coefficients. The results on Cilin are evaluated using Precision@K: the percentage of words from the top-K candidates that belong to the Cilin category of the target word. If one of the top-K candidates belongs to the same third-level category in Cilin as the target word, the </context>
<context position="23916" citStr="Levy and Goldberg (2014" startWordPosition="3750" endWordPosition="3753"> does not need to use large window sizes in training word-based embeddings for capturing word similarities. The result is similar to the finding of Shi et al. (2010), which indicates that a window size of 2 is better than a window size of 4 for capturing word similarity by using distributional word representations. DEP performs slightly worse than NG2 on CWS and Cilin in P@1 and P@5. However, it achieves better results on Cilin in P@10 to P@100 when more candidate similar words are evaluated. In contrast, NG5 and NG2 mix more semantically related words. This finding is consistent with that of Levy and Goldberg (2014a). Analogy Detection Table 2 shows the results of the three Chinese embeddings on CAQS. Unlike on Cilin and CWS, NG5 outperforms DEP, and is also slightly better than NG2. Similar tendency is shown in Table 3 for the three English embeddings evaluated on the Google dataset. These results show that dependency embeddings are relatively weak for answering analogy questions. On the other hand, the performance also varies across different relation types. 2446 Target NG5 NG2 DEP B (wear) 1A (shorts), ;4&apos; (slim-fit), 4&apos; 4&apos;B (wear), 4&apos;X (wear), 1A 4&apos; B (wear), 4&apos; X (wear),  B (wear), 5&apos;F @ (coat), *</context>
<context position="32576" citStr="Levy and Goldberg (2014" startWordPosition="5206" endWordPosition="5210"> similar, we can achieve better recall without precision loss by using more seeds. 7 Related Work Turney (2006) introduces a latent relational analysis (LRA) model to measure relational similarity, and apply a novel co-occurrence-based method for analogy filtering. The model can be used for both analogy detection and relation classification, yet cannot scale up well to large datasets due to the complexity of Singular Value Decomposition. Recently, distributed word representations using the skip-gram model (Mikolov et al., 2013a) has been shown to give competitive results on analogy detection. Levy and Goldberg (2014a) extends the skip-gram method with dependency-context embeddings. We study the effect of Levy and Goldberg’s embeddings on analogy detection, and further extend their embeddings to dependency-context dependency structure embeddings for analogy mining. Chiu et al. (2007) presents a similarity graph tranversal (SGT) method to mine analogous relations from raw English text automatically, using syntactic dependencies to find candidate relations. The method is unsupervised, and can scale up well to large data sets. However, Chiu et al. (2007) mainly focuses on relations between subjects and objec</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014b. Linguistic regularities in sparse and explicit word representations. In Proceedings of CONLL, pages 171–180, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Medin</author>
<author>Robert L Goldstone</author>
<author>Dedre Gentner</author>
</authors>
<title>Similarity involving attributes and relations: Judgments of similarity and difference are not inverses.</title>
<date>1990</date>
<journal>Psychological Science,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1125" citStr="Medin et al., 1990" startWordPosition="143" endWordPosition="146">gy detection. We empirically investigate the use of syntactic dependencies on improving Chinese analogy detection based on distributed word representations, showing that a dependency-based embeddings does not perform better than an ngram-based embeddings, but dependency structures can be used to improve analogy detection by filtering candidates. In addition, we show that distributed representations of dependency structure can be used for measuring relational similarities, thereby help analogy mining. 1 Introduction Relational similarity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolo</context>
</contexts>
<marker>Medin, Goldstone, Gentner, 1990</marker>
<rawString>Douglas L Medin, Robert L Goldstone, and Dedre Gentner. 1990. Similarity involving attributes and relations: Judgments of similarity and difference are not inverses. Psychological Science, 1(1):64–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="1616" citStr="Mikolov et al., 2013" startWordPosition="210" endWordPosition="213">analogy mining. 1 Introduction Relational similarity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best system in the SemEval 2012 shared task on analogy detection. Levy and Goldberg (2014b) further improve Mikolov’s relational similarity measure method using novel arithmetic combinations of attributional similarities. For simplicity, we call the method of Mikolov et al. embeddingbased analogy detection, without stressing the difference betw</context>
<context position="3469" citStr="Mikolov et al., 2013" startWordPosition="487" endWordPosition="490">is particularly strong for Chinese, which is fully configurational and lacks morphology. To our knowledge, relatively little work has been reported on Chinese relational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed word representation of a word using its context words in local ngram window (Mikolov et al., 2013a), and the other training the distributed representation of a word using words in a syntactic dependency context (Levy and Goldberg, 2014b; Bansal et al., 2014). The latter has attracted much recent atten2441 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2441–2450, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tion due to its potential in capturing more syntactic regularities. It has been shown to outperform the former in a variety of NLP tasks, and can potentially also improve relation similarity. Our</context>
<context position="6491" citStr="Mikolov et al., 2013" startWordPosition="946" endWordPosition="949">asks for relational similarity. This first is relation classification, which has been used in Task 2 of SemEval 2012 (Jurgens et al., 2012). In this task, all four words in two word pairs are given, and one needs to judge whether Figure 1: Dependency tree of the sentence “1991* (in 1991) , (,) 4�C4 (Obama) A,tt (President) *A, (graduate) t (from) T (Harvard) �k*M (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, an</context>
<context position="7756" citStr="Mikolov et al. (2013" startWordPosition="1155" endWordPosition="1158"> pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They capture linguistic regularities in languages (Mikolov et al., 2013b) and have been used in various tasks (Collobert and Weston, 2008; Turian et al., 2010; Socher et al., 2011). In this paper, we apply the Skip-gram method of Mikolov et al. (2013a) for training embeddings, which works by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised analogy detection. 2442 2.3 Embedding-based Analogy Detection Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. Mikolov et al. (2013b) show that the task can be solved by finding a word that maximizes: score = sim(b*, b − a + a*) (1) where sim is a similarity measure, typically the cosine function. Levy and Goldberg (2014b) show that the Equation 1 is equivalent to: score = cos(b*, b)−cos(b*, </context>
<context position="9098" citStr="Mikolov et al., 2013" startWordPosition="1385" endWordPosition="1388">ent from a. Levy and Goldberg (2014b) further propose to substitute the addictive functions in Equation 2 with multiplicative functions: score = cos(b*,b)cos(b*,a*)/(cos(b*,a) + ε) (3) Here ε = 0.001 is used to prevent division by zero. Their experiments show that the use of Equation 3 can improve the state-of-the-art. Following Levy and Goldberg (2014b), we refer to Equation 1 and 2 as 3COSADD and Equation 3 as 3COSMUL, respectively. 2.4 Chinese Relational Similarity There are various types of relational similarities. Syntactically, inflections can be treated as a type of word-word relation (Mikolov et al., 2013b). For example, the comparative pairs “good:better” and “rough:rougher” are analogous, and the past tense inflections “see:saw” and “return:returned” are analogous. However, such inflectional relations do not apply to Chinese, which is fully configurational and lacks morphology. Consequently, our main focus is semantic similarities, which include antonymy (e.g. (A (hot):;? (cold)) VS (A (fast):� (slow))), meronymy (e.g. (-* (car):* 1&apos; (wheel)) VS (;fit (bear):* (paw))), gender (e.g. (W,k (man):*,k (woman)) VS (q-:E (king): L (queen))) and function relations (e.g. (-cARL (clothing):3F (wear)) </context>
<context position="15346" citStr="Mikolov et al., 2013" startWordPosition="2369" endWordPosition="2372">iple,WP); 21 s =Triple; 22 Mine (DT,DW,s,WP,α,Q); 23 end 24 end 25 end 26 WP =∅; 27 Mine (DT,DW,s,WP,α,Q); Algorithm 1: Bootstrapping for analogy mining. be measured by distributed relation representations. Based on the observation in Section 2.4, semantically analogous word pairs typically have syntactic dependencies. We use the skip-gram algorithm to train distributed representations of syntactic dependencies, and use them for mining analogous word pairs. With respect to the skip-gram model, words are the most common target for embeddings (Levy and Goldberg, 2014b; Levy and Goldberg, 2014a; Mikolov et al., 2013a), although continuous vector representations can be trained for other structures. For example, Mikolov et al. (2013a) take idiomatic phrases as embedding targets. Dependencies, which consist of a modifier word, a head word and a syntactic relation between them, can also be represented by continuous embeddings using the same algorithm. To induce dependency embeddings, we take the union of the dependency context of both the dependent and the head of a dependency as the context. For instance, in the example sentence, the context of the dependency &lt;,O,bt (Presiden2444 t), . A (graduate), SBV&gt; co</context>
<context position="21481" citStr="Mikolov et al., 2013" startWordPosition="3372" endWordPosition="3375">Chinese speakers (Jin and Wu, 2012). This dataset consists of 297 word pairs. The second one is the Chinese thesaurus Tongyicicilin (Cilin) (Che et al., 2010), which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models (Qiu et al., 2011; Jin et al., 2012). The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar. For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset7 (Mikolov et al., 2013a), to evaluate the English word embeddings of Levy and Goldberg (2014a)8 on analogy detection. On both the CAQS and the Google datasets, the 3COSMUL method (Levy and Goldberg, 2014b) is used to to answer analogy questions based on given embeddings. The results on the CWS dataset are evaluated using the two standard metrics for the task, namely Spearman’s ρ and Kendall’s τ rank correlation coefficients. The results on Cilin are evaluated using Precision@K: the percentage of words from the top-K candidates that belong to the Cilin category of the target word. If one of the top-K candidates belo</context>
<context position="32485" citStr="Mikolov et al., 2013" startWordPosition="5191" endWordPosition="5194">and 11.3%, respectively. Their union recall is 56.8%. When the precision of each seed is similar, we can achieve better recall without precision loss by using more seeds. 7 Related Work Turney (2006) introduces a latent relational analysis (LRA) model to measure relational similarity, and apply a novel co-occurrence-based method for analogy filtering. The model can be used for both analogy detection and relation classification, yet cannot scale up well to large datasets due to the complexity of Singular Value Decomposition. Recently, distributed word representations using the skip-gram model (Mikolov et al., 2013a) has been shown to give competitive results on analogy detection. Levy and Goldberg (2014a) extends the skip-gram method with dependency-context embeddings. We study the effect of Levy and Goldberg’s embeddings on analogy detection, and further extend their embeddings to dependency-context dependency structure embeddings for analogy mining. Chiu et al. (2007) presents a similarity graph tranversal (SGT) method to mine analogous relations from raw English text automatically, using syntactic dependencies to find candidate relations. The method is unsupervised, and can scale up well to large da</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1616" citStr="Mikolov et al., 2013" startWordPosition="210" endWordPosition="213">analogy mining. 1 Introduction Relational similarity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best system in the SemEval 2012 shared task on analogy detection. Levy and Goldberg (2014b) further improve Mikolov’s relational similarity measure method using novel arithmetic combinations of attributional similarities. For simplicity, we call the method of Mikolov et al. embeddingbased analogy detection, without stressing the difference betw</context>
<context position="3469" citStr="Mikolov et al., 2013" startWordPosition="487" endWordPosition="490">is particularly strong for Chinese, which is fully configurational and lacks morphology. To our knowledge, relatively little work has been reported on Chinese relational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed word representation of a word using its context words in local ngram window (Mikolov et al., 2013a), and the other training the distributed representation of a word using words in a syntactic dependency context (Levy and Goldberg, 2014b; Bansal et al., 2014). The latter has attracted much recent atten2441 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2441–2450, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. tion due to its potential in capturing more syntactic regularities. It has been shown to outperform the former in a variety of NLP tasks, and can potentially also improve relation similarity. Our</context>
<context position="6491" citStr="Mikolov et al., 2013" startWordPosition="946" endWordPosition="949">asks for relational similarity. This first is relation classification, which has been used in Task 2 of SemEval 2012 (Jurgens et al., 2012). In this task, all four words in two word pairs are given, and one needs to judge whether Figure 1: Dependency tree of the sentence “1991* (in 1991) , (,) 4�C4 (Obama) A,tt (President) *A, (graduate) t (from) T (Harvard) �k*M (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, an</context>
<context position="7756" citStr="Mikolov et al. (2013" startWordPosition="1155" endWordPosition="1158"> pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They capture linguistic regularities in languages (Mikolov et al., 2013b) and have been used in various tasks (Collobert and Weston, 2008; Turian et al., 2010; Socher et al., 2011). In this paper, we apply the Skip-gram method of Mikolov et al. (2013a) for training embeddings, which works by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised analogy detection. 2442 2.3 Embedding-based Analogy Detection Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. Mikolov et al. (2013b) show that the task can be solved by finding a word that maximizes: score = sim(b*, b − a + a*) (1) where sim is a similarity measure, typically the cosine function. Levy and Goldberg (2014b) show that the Equation 1 is equivalent to: score = cos(b*, b)−cos(b*, </context>
<context position="9098" citStr="Mikolov et al., 2013" startWordPosition="1385" endWordPosition="1388">ent from a. Levy and Goldberg (2014b) further propose to substitute the addictive functions in Equation 2 with multiplicative functions: score = cos(b*,b)cos(b*,a*)/(cos(b*,a) + ε) (3) Here ε = 0.001 is used to prevent division by zero. Their experiments show that the use of Equation 3 can improve the state-of-the-art. Following Levy and Goldberg (2014b), we refer to Equation 1 and 2 as 3COSADD and Equation 3 as 3COSMUL, respectively. 2.4 Chinese Relational Similarity There are various types of relational similarities. Syntactically, inflections can be treated as a type of word-word relation (Mikolov et al., 2013b). For example, the comparative pairs “good:better” and “rough:rougher” are analogous, and the past tense inflections “see:saw” and “return:returned” are analogous. However, such inflectional relations do not apply to Chinese, which is fully configurational and lacks morphology. Consequently, our main focus is semantic similarities, which include antonymy (e.g. (A (hot):;? (cold)) VS (A (fast):� (slow))), meronymy (e.g. (-* (car):* 1&apos; (wheel)) VS (;fit (bear):* (paw))), gender (e.g. (W,k (man):*,k (woman)) VS (q-:E (king): L (queen))) and function relations (e.g. (-cARL (clothing):3F (wear)) </context>
<context position="15346" citStr="Mikolov et al., 2013" startWordPosition="2369" endWordPosition="2372">iple,WP); 21 s =Triple; 22 Mine (DT,DW,s,WP,α,Q); 23 end 24 end 25 end 26 WP =∅; 27 Mine (DT,DW,s,WP,α,Q); Algorithm 1: Bootstrapping for analogy mining. be measured by distributed relation representations. Based on the observation in Section 2.4, semantically analogous word pairs typically have syntactic dependencies. We use the skip-gram algorithm to train distributed representations of syntactic dependencies, and use them for mining analogous word pairs. With respect to the skip-gram model, words are the most common target for embeddings (Levy and Goldberg, 2014b; Levy and Goldberg, 2014a; Mikolov et al., 2013a), although continuous vector representations can be trained for other structures. For example, Mikolov et al. (2013a) take idiomatic phrases as embedding targets. Dependencies, which consist of a modifier word, a head word and a syntactic relation between them, can also be represented by continuous embeddings using the same algorithm. To induce dependency embeddings, we take the union of the dependency context of both the dependent and the head of a dependency as the context. For instance, in the example sentence, the context of the dependency &lt;,O,bt (Presiden2444 t), . A (graduate), SBV&gt; co</context>
<context position="21481" citStr="Mikolov et al., 2013" startWordPosition="3372" endWordPosition="3375">Chinese speakers (Jin and Wu, 2012). This dataset consists of 297 word pairs. The second one is the Chinese thesaurus Tongyicicilin (Cilin) (Che et al., 2010), which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models (Qiu et al., 2011; Jin et al., 2012). The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar. For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset7 (Mikolov et al., 2013a), to evaluate the English word embeddings of Levy and Goldberg (2014a)8 on analogy detection. On both the CAQS and the Google datasets, the 3COSMUL method (Levy and Goldberg, 2014b) is used to to answer analogy questions based on given embeddings. The results on the CWS dataset are evaluated using the two standard metrics for the task, namely Spearman’s ρ and Kendall’s τ rank correlation coefficients. The results on Cilin are evaluated using Precision@K: the percentage of words from the top-K candidates that belong to the Cilin category of the target word. If one of the top-K candidates belo</context>
<context position="32485" citStr="Mikolov et al., 2013" startWordPosition="5191" endWordPosition="5194">and 11.3%, respectively. Their union recall is 56.8%. When the precision of each seed is similar, we can achieve better recall without precision loss by using more seeds. 7 Related Work Turney (2006) introduces a latent relational analysis (LRA) model to measure relational similarity, and apply a novel co-occurrence-based method for analogy filtering. The model can be used for both analogy detection and relation classification, yet cannot scale up well to large datasets due to the complexity of Singular Value Decomposition. Recently, distributed word representations using the skip-gram model (Mikolov et al., 2013a) has been shown to give competitive results on analogy detection. Levy and Goldberg (2014a) extends the skip-gram method with dependency-context embeddings. We study the effect of Levy and Goldberg’s embeddings on analogy detection, and further extend their embeddings to dependency-context dependency structure embeddings for analogy mining. Chiu et al. (2007) presents a similarity graph tranversal (SGT) method to mine analogous relations from raw English text automatically, using syntactic dependencies to find candidate relations. The method is unsupervised, and can scale up well to large da</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746– 751. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>641--648</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7274" citStr="Mnih and Hinton, 2007" startWordPosition="1076" endWordPosition="1079"> using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They capture linguistic regularities in languages (Mikolov et al., 2013b) and have been used in various tasks (Collobert and Weston, 2008; Turian et al., 2010; Socher et al., 2011). In this paper, we apply the Skip-gram method of Mikolov et al. (2013a) for training embeddings, which works by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised an</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of ICML, pages 641–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Using lexical and relational similarity to classify semantic relations.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009,</booktitle>
<pages>621--629</pages>
<location>Athens, Greece,</location>
<marker>S´eaghdha, Copestake, 2009</marker>
<rawString>Diarmuid O´ S´eaghdha and Ann Copestake. 2009. Using lexical and relational similarity to classify semantic relations. In Proceedings of EACL 2009, pages 621–629, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Likun Qiu</author>
<author>Yunfang Wu</author>
<author>Yanqiu Shao</author>
</authors>
<title>Combining contextual and structural information for supersense tagging of chinese unknown words.</title>
<date>2011</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>15--28</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="21207" citStr="Qiu et al., 2011" startWordPosition="3327" endWordPosition="3330">alogy detection, yet there is little large-scale evaluation results on Chinese embeddings in the literature, we perform embedding evaluation on two datasets. The first one is the Chinese WordSim (CWS), translated from the English WordSim-353 Set and re-scored by native Chinese speakers (Jin and Wu, 2012). This dataset consists of 297 word pairs. The second one is the Chinese thesaurus Tongyicicilin (Cilin) (Che et al., 2010), which groups 74,000 Chinese words into five-layer hierarchies and has been used for evaluating the accuracy of word similarity by traditional sparse vector space models (Qiu et al., 2011; Jin et al., 2012). The third level of Cilin, which contains 1428 classes, is used to evaluate whether two words are semantically similar. For comparison between Chinese and English, we also use an English analogy question dataset, the Google dataset7 (Mikolov et al., 2013a), to evaluate the English word embeddings of Levy and Goldberg (2014a)8 on analogy detection. On both the CAQS and the Google datasets, the 3COSMUL method (Levy and Goldberg, 2014b) is used to to answer analogy questions based on given embeddings. The results on the CWS dataset are evaluated using the two standard metrics </context>
</contexts>
<marker>Qiu, Wu, Shao, 2011</marker>
<rawString>Likun Qiu, Yunfang Wu, and Yanqiu Shao. 2011. Combining contextual and structural information for supersense tagging of chinese unknown words. In Computational Linguistics and Intelligent Text Processing, pages 15–28. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Likun Qiu</author>
<author>Yue Zhang</author>
<author>Peng Jin</author>
<author>Houfeng Wang</author>
</authors>
<title>Multi-view chinese treebanking.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>257--268</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="18957" citStr="Qiu et al., 2014" startWordPosition="2994" endWordPosition="2997">one set of dependency embeddings DT (dependency context), using the Skip-Gram model. WORD2VEC2 is used to train NG5 and NG2, and WORD2VECF3 is used to train DEP and DT. The negative-sampling parameter is set to 15 in all the training processes. All embeddings are trained on a free Chinese news archive4 that contains about 170 millions sentences and 3.4 billions words. We segment and parse these sentences using the MVT implementation of ZPar 0.75 (Zhang and Clark, 2011), which is trained on a large-scale annotated corpus and achieves state-of-the-art analyzing accuracy on contemporary Chinese (Qiu et al., 2014)6. Targets and contexts for word and dependency embeddings were filtered with a minimum frequency of 100 and 10, respectively, and all the four types of embeddings are trained with 200 dimensions. 6.2 Datasets and Evaluation Metrics Three datasets are used for evaluating Chinese embeddings. First, we construct a set of semantic analogy questions. This set contains five types of semantic analogy questions, including capital-country (136 word pairs, and 18354 analogy questions), provincial capital-province (28, 756), city-province (637, 386262), family member (male-female) (18, 306) and currency</context>
</contexts>
<marker>Qiu, Zhang, Jin, Wang, 2014</marker>
<rawString>Likun Qiu, Yue Zhang, Peng Jin, and Houfeng Wang. 2014. Multi-view chinese treebanking. In Proceedings of COLING, pages 257–268, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuming Shi</author>
<author>Huibin Zhang</author>
<author>Xiaojie Yuan</author>
<author>JiRong Wen</author>
</authors>
<title>Corpus-based semantic class mining: Distributional vs. pattern-based approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>993--1001</pages>
<location>Beijing, China,</location>
<contexts>
<context position="23458" citStr="Shi et al. (2010)" startWordPosition="3672" endWordPosition="3675"> indicate 3COSMUL and our improved method, respectively. Relation NG5 NG2 DEP capital-country 94.6% 84.5% 38.5% capital-world 71.5% 64.7% 14.2% city-in-state 53.2% 42.5% 13.1% family 82.0% 81.2% 81.0% currency 10.5% 10.7% 6.0% All 63.7% 60.7% 38.8% Table 3: English results on the Google set. Table 1 shows the results of the three Chinese embedding on Cilin and CWS, where NG2 performs much better than NG5 on both datasets. This demonstrates that one does not need to use large window sizes in training word-based embeddings for capturing word similarities. The result is similar to the finding of Shi et al. (2010), which indicates that a window size of 2 is better than a window size of 4 for capturing word similarity by using distributional word representations. DEP performs slightly worse than NG2 on CWS and Cilin in P@1 and P@5. However, it achieves better results on Cilin in P@10 to P@100 when more candidate similar words are evaluated. In contrast, NG5 and NG2 mix more semantically related words. This finding is consistent with that of Levy and Goldberg (2014a). Analogy Detection Table 2 shows the results of the three Chinese embeddings on CAQS. Unlike on Cilin and CWS, NG5 outperforms DEP, and is </context>
</contexts>
<marker>Shi, Zhang, Yuan, Wen, 2010</marker>
<rawString>Shuming Shi, Huibin Zhang, Xiaojie Yuan, and JiRong Wen. 2010. Corpus-based semantic class mining: Distributional vs. pattern-based approaches. In Proceedings of COLING, pages 993–1001, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="7551" citStr="Socher et al., 2011" startWordPosition="1118" endWordPosition="1121"> seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They capture linguistic regularities in languages (Mikolov et al., 2013b) and have been used in various tasks (Collobert and Weston, 2008; Turian et al., 2010; Socher et al., 2011). In this paper, we apply the Skip-gram method of Mikolov et al. (2013a) for training embeddings, which works by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised analogy detection. 2442 2.3 Embedding-based Analogy Detection Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. Mikolov et al. (2013b) show that the task can be solved by finding a word that </context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of ICML, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="7529" citStr="Turian et al., 2010" startWordPosition="1114" endWordPosition="1117">ame relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They capture linguistic regularities in languages (Mikolov et al., 2013b) and have been used in various tasks (Collobert and Weston, 2008; Turian et al., 2010; Socher et al., 2011). In this paper, we apply the Skip-gram method of Mikolov et al. (2013a) for training embeddings, which works by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised analogy detection. 2442 2.3 Embedding-based Analogy Detection Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. Mikolov et al. (2013b) show that the task can be solved b</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Corpusbased learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="4665" citStr="Turney and Littman (2005)" startWordPosition="661" endWordPosition="665">rove relation similarity. Our experiments on both English and Chinese show that the dependency-context embeddings consistently under-perform ngram-context embeddings. We give some theoretical justifications to the findings. Second, we propose to use syntactic dependencies as a context for improving embeddingbased analogy detection, pruning the search space and filtering noise using syntactic dependencies. While highly useful for measuring relational similarities, attributional similarities between words are not the only source of information for analogy detection. Traditional methods, such as Turney and Littman (2005), Turney (2006), Chiu et al. (2007) and O´ S´eaghdha and Copestake (2009), also leverage context between word pairs in a corpus for better accuracies, which the current embedding-based methods ignore. Results show that our proposed method achieves significant improvements for this task. Third, we show that a novel distributed representation of syntactic dependencies between word pairs can be used to mine analogous dependencies from a large Chinese corpus. Inspired by the fact that distributed word representations can be used to measure word similarities, we use our distributed dependency repre</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter D Turney and Michael L Littman. 2005. Corpusbased learning of analogies and semantic relations. Machine Learning, 60(1-3):251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1177" citStr="Turney, 2006" startWordPosition="155" endWordPosition="156">tic dependencies on improving Chinese analogy detection based on distributed word representations, showing that a dependency-based embeddings does not perform better than an ngram-based embeddings, but dependency structures can be used to improve analogy detection by filtering candidates. In addition, we show that distributed representations of dependency structure can be used for measuring relational similarities, thereby help analogy mining. 1 Introduction Relational similarity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between word</context>
<context position="4680" citStr="Turney (2006)" startWordPosition="666" endWordPosition="667">ur experiments on both English and Chinese show that the dependency-context embeddings consistently under-perform ngram-context embeddings. We give some theoretical justifications to the findings. Second, we propose to use syntactic dependencies as a context for improving embeddingbased analogy detection, pruning the search space and filtering noise using syntactic dependencies. While highly useful for measuring relational similarities, attributional similarities between words are not the only source of information for analogy detection. Traditional methods, such as Turney and Littman (2005), Turney (2006), Chiu et al. (2007) and O´ S´eaghdha and Copestake (2009), also leverage context between word pairs in a corpus for better accuracies, which the current embedding-based methods ignore. Results show that our proposed method achieves significant improvements for this task. Third, we show that a novel distributed representation of syntactic dependencies between word pairs can be used to mine analogous dependencies from a large Chinese corpus. Inspired by the fact that distributed word representations can be used to measure word similarities, we use our distributed dependency representations to m</context>
<context position="32064" citStr="Turney (2006)" startWordPosition="5127" endWordPosition="5128">ing the first three types of word pairs in CAQS as the gold set, which contains 801 word pairs. All the three types of word pairs belong to the relation Sub-Location:Location. The recall is computed as the percentage of the gold word pairs covered by the mined dependencies. When using the two seeds &lt;�a (Wuhan), illAE (Hubei), ATT&gt; and &lt;AEq (Beijing), v{&apos;q (China), ATT&gt; for analogy mining, the recalls are 50.2% and 11.3%, respectively. Their union recall is 56.8%. When the precision of each seed is similar, we can achieve better recall without precision loss by using more seeds. 7 Related Work Turney (2006) introduces a latent relational analysis (LRA) model to measure relational similarity, and apply a novel co-occurrence-based method for analogy filtering. The model can be used for both analogy detection and relation classification, yet cannot scale up well to large datasets due to the complexity of Singular Value Decomposition. Recently, distributed word representations using the skip-gram model (Mikolov et al., 2013a) has been shown to give competitive results on analogy detection. Levy and Goldberg (2014a) extends the skip-gram method with dependency-context embeddings. We study the effect </context>
<context position="33733" citStr="Turney (2006)" startWordPosition="5386" endWordPosition="5387">) mainly focuses on relations between subjects and objects because of its word-pair extraction method. O´ S´eaghdha and Copestake (2009) is a supervised method, which combines lexical similarity and relational similarity to classify se2448 mantic relations. These methods are based on distributional word representation models and fit for classifying noun-noun word pairs. In contrast, our methods are based on distributed word representation models, and can mine noun-noun word pairs as well as verb-noun word pairs. In addition, our analogy mining method is unsupervised, while the methods of both Turney (2006) and O´ S´eaghdha and Copestake (2009) are supervised. 8 Conclusion We studied several Chinese relational similarity tasks to train embeddings under the context of distributed word representations using the skip-gram model and syntactic dependencies. For Chinese analogy detection, we compared word-context and dependency-context embeddings, finding that the former results in much better accuracies. Observing that common relations in Chinese are frequently represented by syntactic dependencies, we improved Chinese analogy detection using a dependency context. Further, we empirically studied Chin</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Distributional semantics beyond words: Supervised learning of analogy and paraphrase. arXiv preprint arXiv:1310.5042.</title>
<date>2013</date>
<contexts>
<context position="6431" citStr="Turney, 2013" startWordPosition="938" endWordPosition="939">.1 Relational Similarity Tasks There are three main tasks for relational similarity. This first is relation classification, which has been used in Task 2 of SemEval 2012 (Jurgens et al., 2012). In this task, all four words in two word pairs are given, and one needs to judge whether Figure 1: Dependency tree of the sentence “1991* (in 1991) , (,) 4�C4 (Obama) A,tt (President) *A, (graduate) t (from) T (Harvard) �k*M (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practicall</context>
</contexts>
<marker>Turney, 2013</marker>
<rawString>Peter D Turney. 2013. Distributional semantics beyond words: Supervised learning of analogy and paraphrase. arXiv preprint arXiv:1310.5042.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the penn chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Natural Language Processing–IJCNLP</booktitle>
<pages>70--81</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10449" citStr="Xiong et al., 2005" startWordPosition="1589" endWordPosition="1592"> Their finding coincides with Levin (1993), who study English verbs. We find that this observation is even more prevalent for Chinese. In our automatically-parsed Chinese corpus of 3.4 billion words (Section 5.1), 86.4% word pairs from the analogy test dataset (Section 5.2) have corresponding dependencies, each of which appearing at least ten times. The frequent correlation between semantic relations and syntactic dependencies can be due to the lack of morphology and function words in Chinese. In fact, Chinese syntactic ambiguities often need to be resolved by leveraging semantic information (Xiong et al., 2005; Zhang et al., 2014). Although not all occurrences of semantically-related word pairs must also form a syntactic dependency in a corpus, we show that syntactic dependencies can effectively improve analogy detection. 3 Dependency-context Word Embeddings for Analogy Detection A first use of syntactic dependencies for embedding-based analogy detection is to use them directly for embeddings. Recently, a dependency context has been used for the skip-gram method, for capturing more syntactic regularities. Taking the sentence in Figure 1 for example, a bi-gram context for the word “.A (graduate)” ca</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the penn chinese treebank with semantic knowledge. In Natural Language Processing–IJCNLP 2005, pages 70–81. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="18813" citStr="Zhang and Clark, 2011" startWordPosition="2972" endWordPosition="2975">context with 5 words to the left of the target word and 5 words to the right), NG2 (2 words to the left and right) and DEP (dependency context), and one set of dependency embeddings DT (dependency context), using the Skip-Gram model. WORD2VEC2 is used to train NG5 and NG2, and WORD2VECF3 is used to train DEP and DT. The negative-sampling parameter is set to 15 in all the training processes. All embeddings are trained on a free Chinese news archive4 that contains about 170 millions sentences and 3.4 billions words. We segment and parse these sentences using the MVT implementation of ZPar 0.75 (Zhang and Clark, 2011), which is trained on a large-scale annotated corpus and achieves state-of-the-art analyzing accuracy on contemporary Chinese (Qiu et al., 2014)6. Targets and contexts for word and dependency embeddings were filtered with a minimum frequency of 100 and 10, respectively, and all the four types of embeddings are trained with 200 dimensions. 6.2 Datasets and Evaluation Metrics Three datasets are used for evaluating Chinese embeddings. First, we construct a set of semantic analogy questions. This set contains five types of semantic analogy questions, including capital-country (136 word pairs, and </context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>A semantics oriented grammar for chinese treebanking.</title>
<date>2014</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>366--378</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10470" citStr="Zhang et al., 2014" startWordPosition="1593" endWordPosition="1596">ides with Levin (1993), who study English verbs. We find that this observation is even more prevalent for Chinese. In our automatically-parsed Chinese corpus of 3.4 billion words (Section 5.1), 86.4% word pairs from the analogy test dataset (Section 5.2) have corresponding dependencies, each of which appearing at least ten times. The frequent correlation between semantic relations and syntactic dependencies can be due to the lack of morphology and function words in Chinese. In fact, Chinese syntactic ambiguities often need to be resolved by leveraging semantic information (Xiong et al., 2005; Zhang et al., 2014). Although not all occurrences of semantically-related word pairs must also form a syntactic dependency in a corpus, we show that syntactic dependencies can effectively improve analogy detection. 3 Dependency-context Word Embeddings for Analogy Detection A first use of syntactic dependencies for embedding-based analogy detection is to use them directly for embeddings. Recently, a dependency context has been used for the skip-gram method, for capturing more syntactic regularities. Taking the sentence in Figure 1 for example, a bi-gram context for the word “.A (graduate)” can be “4�C4 (Obama), o</context>
</contexts>
<marker>Zhang, Zhang, Che, Liu, 2014</marker>
<rawString>Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu. 2014. A semantics oriented grammar for chinese treebanking. In Computational Linguistics and Intelligent Text Processing, pages 366–378. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
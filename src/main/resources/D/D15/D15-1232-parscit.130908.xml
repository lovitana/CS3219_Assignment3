<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.948916">
Summarization Based on Embedding Distributions
</title>
<author confidence="0.62262">
Hayato Kobayashi Masaki Noguchi Taichi Yatsuka
</author>
<affiliation confidence="0.4942">
Yahoo Japan Corporation
</affiliation>
<note confidence="0.741078">
9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan
{hakobaya, manoguch, tyatsuka}@yahoo-corp.jp
</note>
<sectionHeader confidence="0.980884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982761904762">
In this study, we consider a summariza-
tion method using the document level sim-
ilarity based on embeddings, or distributed
representations of words, where we as-
sume that an embedding of each word can
represent its “meaning.” We formalize our
task as the problem of maximizing a sub-
modular function defined by the negative
summation of the nearest neighbors’ dis-
tances on embedding distributions, each
of which represents a set of word embed-
dings in a document. We proved the sub-
modularity of our objective function and
that our problem is asymptotically related
to the KL-divergence between the prob-
ability density functions that correspond
to a document and its summary in a con-
tinuous space. An experiment using a
real dataset demonstrated that our method
performed better than the existing method
based on sentence-level similarity.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975576923077">
Document summarization aims to rephrase a doc-
ument in a short form called a summary while
keeping its “meaning.” In the present study, we
aim to characterize the meaning of a document us-
ing embeddings or distributed representations of
words in the document, where an embedding of
each word is represented as a real valued vector
in a Euclidean space that corresponds to the word
(Mikolov et al., 2013a; Mikolov et al., 2013b).
Many previous studies have investigated sum-
marization (Lin and Bilmes, 2010; Lin and
Bilmes, 2011; Lin and Bilmes, 2012; Sipos et al.,
2012; Morita et al., 2013), but to the best of our
knowledge, only one (K˚ageb¨ack et al., 2014) con-
sidered a direct summarization method using em-
beddings, where the summarization problem was
formalized as maximizing a submodular function
defined by the summation of cosine similarities
based on sentence embeddings. Essentially, this
method assumes linear meanings since the objec-
tive function is characterized by the summation of
sentence-level similarities. However, this assump-
tion is not always valid in real documents, and thus
there may be a better combination of two other
sentences than the best and second best sentences
in terms of similarity in a document.
In this study, we consider a summarization
method based on document-level similarity, where
we assume the non-linearity of meanings. First,
we examine an objective function defined by a co-
sine similarity based on document embeddings in-
stead of sentence embeddings. Unfortunately, in
contrast to our intuition, this similarity is not sub-
modular, which we disprove later. Thus, we pro-
pose a valid submodular function based on em-
bedding distributions, each of which represents
a set of word embeddings in a document, as the
document-level similarity. Our objective func-
tion is calculated based on the nearest neighbors’
distances on embedding distributions, which can
be proved to be asymptotically related to KL-
divergence in a continuous space. Several stud-
ies (Lerman and McDonald, 2009; Haghighi and
Vanderwende, 2009) have addressed summariza-
tion using KL-divergence, but they calculated KL-
divergence based on word distributions in a dis-
crete space. In other words, our study is the first
attempt to summarize by asymptotically estimat-
ing KL-divergence based on embedding distribu-
tions in a continuous space. In addition, they in-
volved the inference of complex models, whereas
our method is quite simple but still powerful.
</bodyText>
<sectionHeader confidence="0.98951" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.998491666666667">
We treat a document as a bag-of-sentences and
a sentence as a bag-of-words. Formally, let
D be a document, and we refer to an element
</bodyText>
<page confidence="0.948277">
1984
</page>
<note confidence="0.65915">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1984–1989,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.976427">
s ∈ D of a sentence and w ∈ s of a word.
We denote the size of a set S by |S|. Note
that D and s are defined as multisets. For
example, we can define a document such as
D := {s1,s2} with s1 := {just, do, it}
and s2 := {never, say, never}, which cor-
respond to two sentences “Just do it” and “Never
say never,” respectively. From the definition, we
have |s1 |= 3 and |s2 |= 3.
</bodyText>
<subsectionHeader confidence="0.984479">
2.1 Submodularity
</subsectionHeader>
<bodyText confidence="0.979885125">
Submodularity is a property of set functions,
which is similar to the convexity or concavity of
continuous functions.
We formally define submodularity as follows.
Definition 1(Submodularity). Given a set X, a set
function f : 2X → R is called submodular if for
any two sets S1 and S2 such that S1 ⊂ S2 ⊂ X
and element x ∈ X \ S2,
</bodyText>
<equation confidence="0.5559235">
f(S1 ∪ {x}) − f(S1) ≥ f(S2 ∪ {x}) − f(S2).
For simplicity, we define fS(x) := f(S ∪
{x}) − f(S), which is called the marginal value
of x with respect to S. A set function f is called
monotone if fS(x) ≥ 0 for any set S ⊂ X and
element x ∈ X \ S.
</equation>
<bodyText confidence="0.987806444444444">
If a set function f is monotone submodular, we
can approximate the optimal solution efficiently
by a simple greedy algorithm, which iteratively
selects x* = argmaxxEX\Si fSi(x) where ties
are broken arbitrarily, and we substitute Si+1 =
Si ∪ {x*} in the i-th iteration beginning with
S0 = ∅. This algorithm is quite simple but it is
guaranteed to find a near optimal solution within
1 − 1/e ≈ 0.63 (Calinescu et al., 2007).
</bodyText>
<subsectionHeader confidence="0.99855">
2.2 Embedding
</subsectionHeader>
<bodyText confidence="0.999637625">
An embedding or distributed representation of a
word is a real valued vector in an m-dimensional
Euclidean space Rm, which expresses the “mean-
ing” of the word. We denote an embedding of a
word w by w~ ∈ Rm. If for any two words w1 and
w2, the meaning of w1 is similar to that of w2, then
~w1 is expected to be near to ~w2.
A recent study (Mikolov et al., 2013a) showed
that a simple log-bilinear model can learn high
quality embeddings to obtain a better result than
recurrent neural networks, where the concept of
embeddings was originally proposed in studies of
neural language models (Bengio et al., 2003). In
the present study, we use the CW Vector1 and
W2V Vector2 which are also used in the previous
study (K˚ageb¨ack et al., 2014).
</bodyText>
<sectionHeader confidence="0.98381" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.999691076923077">
In this study, we focus on a summarization task
as sentence selection in a document. The opti-
mization framework in our task is the same as in
the previous study and formalized in Algorithm 1,
where ws represents the pre-defined weight or cost
of a sentence s, e.g., sentence length, and r is
its scaling factor. This algorithm, called modified
greedy, was proposed in (Lin and Bilmes, 2010)
and interestingly performed better than the state-
of-the-art abstractive approach as shown in (Lin
and Bilmes, 2011). Note that we have omitted the
notation of D from f for simplicity because D is
fixed in an optimization process.
</bodyText>
<listItem confidence="0.363750666666667">
Algorithm 1: Modified greedy algorithm.
Data: Document D, objective function f, and
summary size `.
</listItem>
<equation confidence="0.979247">
Result: Summary C ⊂ D.
1 C ← ∅; U ← D;
2 while U =6 ∅ do
s* ← argmaxsEU fC(s)/(ws)r;
if E ws + ws* ≤ ` then C ← C ∪ {s*};
sEC
U ← U \ {s*};
6 s* ← argmaxsED:ws&lt;` f({s});
7 return C ← argmaxC,E1C,1s*11 f(C�);
</equation>
<bodyText confidence="0.936443">
Similarity Based on Document Embeddings
First, we examine an objective function fCos de-
fined by a cosine similarity based on document
embeddings. An embedding of a document D is
defined as vD := E EwEs ~w. We formalize
</bodyText>
<equation confidence="0.74529775">
sED
the objective function fCos as follows.
fCos(C) := kvCk kvDk .
vC · vD
</equation>
<bodyText confidence="0.9708035">
Note that the optimal solution does not change, if
we use an average embedding vD/ EsED |s |in-
stead of vD. The next theorem shows that a solu-
tion of fCos by Algorithm 1 is not guaranteed to be
near optimal.
Theorem 1. fCos is not submodular.
</bodyText>
<footnote confidence="0.993904333333333">
1http://metaoptimize.com/projects/
wordreprs
2https://code.google.com/p/word2vec
</footnote>
<figure confidence="0.90935">
3
4
5
</figure>
<page confidence="0.946781">
1985
</page>
<bodyText confidence="0.972623222222222">
Proof. A simple counterexample is sufficient to
prove the theorem. Let us consider D := {s1 :=
{w1}, s2 := {w2}, s3 := {w3}, s4 := {w4}}
with corresponding vectors 1w1 := (1, 1), 1w2 :=
(1, 2), 1w3 := (1, −1), and 1w4 := (1, −2), re-
spectively. In this case, the document embedding
VD is (4,0). We set C1 := {s1} and C2 :=
{s1, s2}. Clearly, C1 ⊂ C2. However, we ob-
tain fCos
</bodyText>
<equation confidence="0.979994857142857">
C1(s4) = fCos({s1, s4}) − fCos({s1}) ≈
0.187 and fCos
C2(s4) = fCos({s1, s2, s4}) −
fCos({s1, s2}) ≈ 0.394. Therefore, we have
fCos
C2(s4) &gt; fCos
C1(s4).
</equation>
<bodyText confidence="0.875905">
Similarity Based on Embedding Distributions
We propose a valid submodular objective function
fNN based on embedding distributions. The key
observation is that for any two embedding distri-
butions A and B, when A is similar to B, each em-
bedding in A should be near to some embedding
in B. In order to formalize this idea, we define the
nearest neighbor of a word w in a summary C as
n(w, C) := argminv∈s:s∈C,~w6=~v d(1w,1v), where d
is the Euclidian distance in the embedding space,
i.e., d(1w,1v) := k1w − 1vk. We denote the dis-
tance of w to its nearest neighbor n := n(w, C)
by N(w, C) := d(1w, 1n). Finally, we define fNN as
follows:
</bodyText>
<equation confidence="0.960539">
XfNN(C) := − X g(N(w, C)),
s∈D w∈s
</equation>
<bodyText confidence="0.999834636363636">
where g is a non-decreasing scaling function. The
function fNN represents the negative value −6 of
dissimilarity 6 between a document and summary
based on embedding distributions. Note that we
can use sentence embeddings instead of word em-
beddings as embedding distributions, although we
focus on word embeddings in this section.
The next theorem shows the monotone submod-
ularity of our objective function, which means that
a solution of fNN by Algorithm 1 is guaranteed to
be near optimal.
</bodyText>
<construct confidence="0.5130419">
Theorem 2. fNN is monotone submodular.
Proof. (Monotonicity) First, we prove the mono-
tonicity. For simplicity, we use the follow-
ing two abbreviations: Cs := C ∪ {s} and
PDw := P Pw∈s. For any set C ⊂
s∈D
D of sentences and sentence s ∈ D \ C,
we have fNNC (s) = fNN(Cs) − fNN(C) =
PDw (g(N(w, C)) − g(N(w, Cs))). Since C ⊂
Cs, obviously N(w, C) ≥ N(w, Cs) holds.
</construct>
<bodyText confidence="0.901138">
Therefore, we obtain fNNC (s) ≥ 0 from the non-
decreasing property of g.
(Submodularity) Next, we prove the submodu-
larity. For any two sets C1 and C2 of sentences
such that C1 ⊂ C2 ⊂ D, and sentence s ∈ D\C2,
we have fNNC1(s) − fNNC2(s) = fNN(Cs1) − fNN(C1) −
</bodyText>
<equation confidence="0.834955866666667">
(fNN(Cs2) − fNN(C2)) = PD w (g(N(w, C1)) −
g(N(w, Cs1)) − g(N(w, C2)) + g(N(w, Cs2))).
Let α := g(N(w, C1)) − g(N(w, Cs1)) −
g(N(w, C2)) + g(N(w, Cs2)).
If n(w, Cs2) ∈ s, then n(w, Cs1) ∈ s holds,
since Cs1 ⊂ Cs2. This means that N(w, Cs2) =
N(w, Cs1) = N(w, {s}). Clearly, N(w, C1) ≥
N(w, C2), since C1 ⊂ C2. Therefore, we obtain
α ≥ 0 from the non-decreasing property of g.
If n(w, Cs2) ∈/ s and n(w, Cs1) ∈/ s, we
have N(w, Cs1) = N(w, C1) and N(w, Cs2) =
N(w, C2). This indicates that α = 0.
If n(w, Cs2) ∈/ s and n(w, Cs1) ∈ s, so sim-
ilarly N(w, Cs1) ≤ N(w, C1) and N(w, Cs2) =
N(w, C2) hold. Therefore, we obtain α ≥ 0.
</equation>
<bodyText confidence="0.999717">
The objective function fNN is simply heuristic
for small documents, but the next theorem shows
that fNN is asymptotically related to an approxima-
tion of KL-divergence in a continuous space, if g
is a logarithmic function. This result implies that
we can use mathematical techniques of a contin-
uous space for different NLP tasks, by mapping a
document into a continuous space based on word
embeddings.
</bodyText>
<construct confidence="0.968958333333333">
Theorem 3. Suppose that we have a document D
and two summaries C1 and C2 such that |C1 |=
|C2|, which are samples drawn from some proba-
bility density functions p, q, and r, i.e., D ∼ p,
C1 ∼ q, and C2 ∼ r, respectively. If the scal-
ing function g of fNN is a logarithmic function,
the order relation of the expectations of fNN(C1)
and fNN(C2) is asymptotically the same as that of
the KL-divergences DKL(p  ||r) and DKL(p  ||q),
i.e., E[fNN(C1)] − E[fNN(C2)] &gt; 0 ⇔ DKL(p ||
r) − DKL(p  ||q) &gt; 0, as |D |→ ∞, |C1 |→ ∞,
and |C2 |→ ∞.
</construct>
<bodyText confidence="0.92284">
Proof. Let m be the dimension on embeddings.
Using a divergence estimator based on nearest
neighbor distances in (P´erez-Cruz, 2009; Wang
et al., 2009), we can approximate DKL(p  ||q)
</bodyText>
<equation confidence="0.937185545454545">
PD
bDKL(D, C1) := m w ln N(w,C1)
N(w,D) + ln |C1|
|D|−1.
|D|
Therefore, we obtain bDKL(D, C2)− bDKL(D, C1) ∝
PDw ln N(w, C2) − PDw lnN(w, C1). Since
g(x) = ln(x), we have fNN(C1) − fNN(C2) &gt; 0 if
by
1986
D,-,(D, C1) &gt; 0 holds.
</equation>
<bodyText confidence="0.826475666666667">
The fact that E[D,-,(D, C1)] → D,-,(p  ||q) as
|C1 |→ ∞ and |D |→ ∞ concludes the theo-
rem.
</bodyText>
<sectionHeader confidence="0.996892" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999927761904762">
We compared our two proposed methods DocEmb
and EmbDist with two state-of-the-art methods
SenEmb and TfIdf. The first two methods
DocEmb and EmbDist represent Algorithm 1
with our proposed objective functions f&apos;s and
fNN, respectively. TfIdf represents Algorithm 1
with an objective function based on the sum of co-
sine similarities of tf-idf vectors that correspond
to sentences, which was proposed in (Lin and
Bilmes, 2011). SenEmb uses a cosine similar-
ity measure based on embeddings instead of tf-idf
vectors in the same framework as TfIdf, which
was proposed in (K˚ageb¨ack et al., 2014).
We conducted an experiment with almost the
same setting as in the previous study, where they
used the Opinosis dataset (Ganesan et al., 2010).
This dataset is a collection of user reviews in 51
different topics such as hotels, cars, and products;
thus, it is more appropriate for evaluating sum-
marization of user-generated content than well-
known DUC datasets, which consist of formal
news articles. Each topic in the collection com-
prises 50–575 sentences and includes four and five
gold standard summaries created by human au-
thors, each of which comprises 1–3 sentences.
We ran an optimization process to choose sen-
tences within 100 words3 by setting the summary
size and weights as E = 100 and ws = |s|
for any sentence s, respectively. As for TfIdf
and SenEmb, we set a cluster size of k-means as
k = |D|/5 and chose the best value for a threshold
coefficient α, trade-off coefficient A, and the scal-
ing factor r, as in (Lin and Bilmes, 2011). Note
that our functions DocEmb and EmbDist have
only one parameter r, and we similarly chose the
best value of r. Regarding DocEmb, EmbDist,
and SenEmb, we used the best embeddings from
the CW Vector and W2V Vector for each method,
and created document and sentence embeddings
by averaging word embeddings with tf-idf weights
since it performed better in this experiment. In the
case of EmbDist, we used a variant of fNN based
</bodyText>
<footnote confidence="0.86774575">
3The previous work used a sentence-based constraint as
f = 2 and w3 = 1, but we changed the setting since the
variation in length has a noticeable impact on ROUGE scores
as suggested in (Hong et al., 2014).
</footnote>
<table confidence="0.999239">
R-1 R-2 R-3 R-4
ApxOpt 62.22 21.60 8.71 4.56
EmbDist (ln x) 56.00 16.70 4.93 1.89
EmbDist (x) 55.70 15.73 4.59 1.84
EmbDist (ex) 56.29 15.96 4.43 1.39
DocEmb 55.80 13.59 3.23 0.90
SenEmb 53.96 15.42 3.97 1.10
TfIdf 52.97 17.24 5.40 1.49
</table>
<tableCaption confidence="0.9498625">
Table 1: ROUGE-N (R-N) metrics of DocEmb,
EmbDist, SenEmb, and TfIdf.
</tableCaption>
<bodyText confidence="0.997051702702703">
on distributions of sentence embeddings. In ad-
dition, we examined three scaling functions: log-
arithmic, linear, and exponential functions, i.e.,
ln x, x, ex, respectively.
We calculated the ROUGE-N metric (Lin,
2004)4, which is a widely-used evaluation met-
ric for summarization methods. ROUGE-N is
based on the co-occurrence statistics of N-grams,
and especially ROUGE-1 has been shown to have
the highest correlation with human summaries
(Lin and Hovy, 2003). ROUGE-N is similar
to the BLEU metric for machine translation, but
ROUGE-N is a recall-based metric while BLEU is
a precision-based metric.
Table 1 shows the results obtained for ROUGE-
N (N ≤ 4) using DocEmb, EmbDist, SenEmb,
and TfIdf. ApxOpt represents the approxima-
tion results of the optimal solution in our prob-
lem, where we optimized ROUGE-1 with the gold
standard summaries by Algorithm 1. The ob-
tained results indicate that our proposed method
EmbDist with exponential scaling performed the
best for ROUGE-1, which is the best metric in
terms of correlation with human summaries. The
W2V Vector was the best choice for EmbDist.
Furthermore, the other proposed method DocEmb
performed better than the state-of-the-art methods
SenEmb and TfIdf, although DocEmb is not
theoretically guaranteed to obtain a near optimal
solution. These results imply that our methods
based on the document-level similarity can capture
more complex meanings than the sentence-level
similarity. On the other hand, TfIdf with tf-idf
vectors performed the worst for ROUGE-1. A pos-
sible reason is that a wide variety of expressions
by users made it difficult to calculate similarities.
This also suggests that embedding-based methods
</bodyText>
<footnote confidence="0.988331">
4We used their software ROUGE version 1.5.5 with the
parameters: -n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0.
</footnote>
<note confidence="0.8195265">
and only if �D,-,(D, C2) −
�
</note>
<page confidence="0.989383">
1987
</page>
<bodyText confidence="0.999912647058824">
naturally have robustness for user-generated con-
tent.
In the case of N ≥ 2, TfIdf performed the best
for ROUGE-2 and ROUGE-3, while EmbDist
with logarithmic scaling is better than TfIdf
for ROUGE-4. According to (Lin and Hovy,
2003), the higher order ROUGE-N is worse than
ROUGE-1 since it tends to score grammatical-
ity rather than content. Conversely, Rankel et al.
(2013) reports that there is a dataset where the
higher order ROUGE-N is correlated with human
summaries well. We may need to conduct human
judgments to decide which metric is the best in
this dataset for more accurate comparison. How-
ever, it is still important that our simple objective
functions can obtain good results competing with
the state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.996005" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999909647058824">
In this study, we proposed simple but power-
ful summarization methods using the document-
level similarity based on embeddings, or dis-
tributed representations of words. Our experimen-
tal results demonstrated that the proposed meth-
ods performed better than the existing state-of-the-
art methods based on the sentence-level similar-
ity. This implies that the document-level similar-
ity can capture more complex meanings than the
sentence-level similarity.
Recently, Kusner et al. (2015) independently
discovered a similar definition to our objective
function fNN through a different approach. They
constructed a dissimilarity measure based on a
framework using Earth Mover’s Distance (EMD)
developed in the image processing field (Rubner
et al., 1998; Rubner et al., 2000). EMD is a con-
sistent measure of distance between two distribu-
tions of points. Interestingly, their heuristic lower
bound of EMD is exactly the same as −fNN with a
linear scaling function, i.e., g(x) = x. Moreover,
they showed that this bound appears to be tight
in real datasets. This suggests that our intuitive
framework can theoretically connect the two well-
known measures, KL-divergence and EMD, based
on the scaling of distance. Note that, to the best of
our knowledge, there is currently no known study
that considers such a theoretical relationship.
In future research, we will explore other scal-
ing functions suitable for our problem or different
problems. A promising direction is to consider a
relative scaling function to extract a biased sum-
mary of a document. This direction should be use-
ful for query-focused summarization tasks.
</bodyText>
<sectionHeader confidence="0.995468" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997634">
The authors would like to thank the reviewers
for their helpful comments, especially about Earth
Mover’s Distance.
</bodyText>
<sectionHeader confidence="0.997798" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992214111111111">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search (JMLR 2003), 3:1532–4435.
Gruia Calinescu, Chandra Chekuri, Martin P´al, and
Jan Vondr´ak. 2007. Maximizing a Submodular Set
Function Subject to a Matroid Constraint (Extended
Abstract). In Proceedings of the 12th International
Conference on Integer Programming and Combina-
torial Optimization (IPCO 2007), pages 182–196.
Springer-Verlag.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: A Graph-based Approach to Ab-
stractive Summarization of Highly Redundant Opin-
ions. In Proceedings of the 23rd International
Conference on Computational Linguistics (COLING
2010), pages 340–348. Association for Computa-
tional Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-document Summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT 2009), pages 362–
370. Association for Computational Linguistics.
Kai Hong, John Conroy, Benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A Repository
of State of the Art and Competitive Baseline Sum-
maries for Generic News Summarization. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC 2014),
pages 26–31. European Language Resources Asso-
ciation (ELRA).
Mikael K˚ageb¨ack, Olof Mogren, Nina Tahmasebi, and
Devdatt Dubhashi. 2014. Extractive Summariza-
tion using Continuous Vector Space Models. In Pro-
ceedings of the 2nd Workshop on Continuous Vec-
tor Space Models and their Compositionality (CVSC
2014), pages 31–39. Association for Computational
Linguistics.
Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kil-
ian Q. Weinberger. 2015. From Word Embed-
dings To Document Distances. In Proceedings of the
32nd International Conference on Machine Learn-
ing (ICML 2015), pages 957–966. JMLR.org.
</reference>
<page confidence="0.956241">
1988
</page>
<reference confidence="0.999194124999999">
Kevin Lerman and Ryan McDonald. 2009. Contrastive
Summarization: An Experiment with Consumer Re-
views. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT 2009), pages 113–
116. Association for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2010. Multi-document Sum-
marization via Budgeted Maximization of Submod-
ular Functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-HLT 2010), pages 912–920. Asso-
ciation for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2011. A Class of Submodu-
lar Functions for Document Summarization. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2011),
pages 510–520. Association for Computational Lin-
guistics.
Hui Lin and Jeff Bilmes. 2012. Learning Mixtures
of Submodular Shells with Application to Document
Summarization. In Proceedings of the 28th Confer-
ence on Uncertainty in Artificial Intelligence (UAI
2012), pages 479–490. AUAI.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic Evaluation of Summaries Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology (NAACL-HLT 2003). As-
sociation for Computational Linguistics.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004). Association for Computational Lin-
guistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013a. Distributed Representations of Words and
Phrases and their Compositionality. In Advances
in Neural Information Processing Systems 26 (NIPS
2013), pages 3111–3119. Curran Associates, Inc.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic Regularities in Continuous Space
Word Representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT 2013), pages
746–751. Association for Computational Linguis-
tics.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree Extractive Sum-
marization via Submodular Maximization. In Pro-
ceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2013),
pages 1023–1032. Association for Computational
Linguistics.
Fernando P´erez-Cruz. 2009. Estimation of Infor-
mation Theoretic Measures for Continuous Random
Variables. In Advances in Neural Information Pro-
cessing Systems 21 (NIPS 2009), pages 1257–1264.
Curran Associates, Inc.
Peter A. Rankel, John M. Conroy, Hoa Trang Dang,
and Ani Nenkova. 2013. A Decade of Automatic
Content Evaluation of News Summaries: Reassess-
ing the State of the Art. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2013), pages 131–136. As-
sociation for Computational Linguistics.
Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas.
1998. A Metric for Distributions with Applications
to Image Databases. In Proceedings of the Sixth In-
ternational Conference on Computer Vision (ICCV
1998), pages 59–66. IEEE Computer Society.
Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas.
2000. The Earth Mover’s Distance as a Metric for
Image Retrieval. International Journal of Computer
Vision, 40(2):99–121.
Ruben Sipos, Adith Swaminathan, Pannaga Shiv-
aswamy, and Thorsten Joachims. 2012. Tempo-
ral Corpus Summarization Using Submodular Word
Coverage. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management (CIKM 2012), pages 754–763. ACM.
Qing Wang, S.R. Kulkarni, and S. Verdu. 2009. Di-
vergence Estimation for Multidimensional Densities
Via k-Nearest-Neighbor Distances. IEEE Transac-
tions on Information Theory, 55(5):2392–2405.
</reference>
<page confidence="0.996916">
1989
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903101">
<title confidence="0.999796">Summarization Based on Embedding Distributions</title>
<author confidence="0.982999">Hayato Kobayashi Masaki Noguchi Taichi Yatsuka</author>
<affiliation confidence="0.988391">Yahoo Japan Corporation</affiliation>
<address confidence="0.950007">9-7-1 Akasaka, Minato-ku, Tokyo 107-6211,</address>
<email confidence="0.977755">manoguch,</email>
<abstract confidence="0.999542954545455">In this study, we consider a summarization method using the document level similarity based on embeddings, or distributed representations of words, where we assume that an embedding of each word can represent its “meaning.” We formalize our task as the problem of maximizing a submodular function defined by the negative summation of the nearest neighbors’ dison each of which represents a set of word embeddings in a document. We proved the submodularity of our objective function and that our problem is asymptotically related to the KL-divergence between the probability density functions that correspond to a document and its summary in a continuous space. An experiment using a real dataset demonstrated that our method performed better than the existing method based on sentence-level similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research (JMLR</journal>
<pages>3--1532</pages>
<contexts>
<context position="5891" citStr="Bengio et al., 2003" startWordPosition="991" endWordPosition="994">2 Embedding An embedding or distributed representation of a word is a real valued vector in an m-dimensional Euclidean space Rm, which expresses the “meaning” of the word. We denote an embedding of a word w by w~ ∈ Rm. If for any two words w1 and w2, the meaning of w1 is similar to that of w2, then ~w1 is expected to be near to ~w2. A recent study (Mikolov et al., 2013a) showed that a simple log-bilinear model can learn high quality embeddings to obtain a better result than recurrent neural networks, where the concept of embeddings was originally proposed in studies of neural language models (Bengio et al., 2003). In the present study, we use the CW Vector1 and W2V Vector2 which are also used in the previous study (K˚ageb¨ack et al., 2014). 3 Proposed Method In this study, we focus on a summarization task as sentence selection in a document. The optimization framework in our task is the same as in the previous study and formalized in Algorithm 1, where ws represents the pre-defined weight or cost of a sentence s, e.g., sentence length, and r is its scaling factor. This algorithm, called modified greedy, was proposed in (Lin and Bilmes, 2010) and interestingly performed better than the stateof-the-art </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research (JMLR 2003), 3:1532–4435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gruia Calinescu</author>
<author>Chandra Chekuri</author>
<author>Martin P´al</author>
<author>Jan Vondr´ak</author>
</authors>
<title>Maximizing a Submodular Set Function Subject to a Matroid Constraint (Extended Abstract).</title>
<date>2007</date>
<booktitle>In Proceedings of the 12th International Conference on Integer Programming and Combinatorial Optimization (IPCO</booktitle>
<pages>182--196</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Calinescu, Chekuri, P´al, Vondr´ak, 2007</marker>
<rawString>Gruia Calinescu, Chandra Chekuri, Martin P´al, and Jan Vondr´ak. 2007. Maximizing a Submodular Set Function Subject to a Matroid Constraint (Extended Abstract). In Proceedings of the 12th International Conference on Integer Programming and Combinatorial Optimization (IPCO 2007), pages 182–196. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kavita Ganesan</author>
<author>ChengXiang Zhai</author>
<author>Jiawei Han</author>
</authors>
<title>Opinosis: A Graph-based Approach to Abstractive Summarization of Highly Redundant Opinions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010),</booktitle>
<pages>340--348</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12711" citStr="Ganesan et al., 2010" startWordPosition="2266" endWordPosition="2269">irst two methods DocEmb and EmbDist represent Algorithm 1 with our proposed objective functions f&apos;s and fNN, respectively. TfIdf represents Algorithm 1 with an objective function based on the sum of cosine similarities of tf-idf vectors that correspond to sentences, which was proposed in (Lin and Bilmes, 2011). SenEmb uses a cosine similarity measure based on embeddings instead of tf-idf vectors in the same framework as TfIdf, which was proposed in (K˚ageb¨ack et al., 2014). We conducted an experiment with almost the same setting as in the previous study, where they used the Opinosis dataset (Ganesan et al., 2010). This dataset is a collection of user reviews in 51 different topics such as hotels, cars, and products; thus, it is more appropriate for evaluating summarization of user-generated content than wellknown DUC datasets, which consist of formal news articles. Each topic in the collection comprises 50–575 sentences and includes four and five gold standard summaries created by human authors, each of which comprises 1–3 sentences. We ran an optimization process to choose sentences within 100 words3 by setting the summary size and weights as E = 100 and ws = |s| for any sentence s, respectively. As </context>
</contexts>
<marker>Ganesan, Zhai, Han, 2010</marker>
<rawString>Kavita Ganesan, ChengXiang Zhai, and Jiawei Han. 2010. Opinosis: A Graph-based Approach to Abstractive Summarization of Highly Redundant Opinions. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 340–348. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring Content Models for Multi-document Summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3119" citStr="Haghighi and Vanderwende, 2009" startWordPosition="483" endWordPosition="486">osine similarity based on document embeddings instead of sentence embeddings. Unfortunately, in contrast to our intuition, this similarity is not submodular, which we disprove later. Thus, we propose a valid submodular function based on embedding distributions, each of which represents a set of word embeddings in a document, as the document-level similarity. Our objective function is calculated based on the nearest neighbors’ distances on embedding distributions, which can be proved to be asymptotically related to KLdivergence in a continuous space. Several studies (Lerman and McDonald, 2009; Haghighi and Vanderwende, 2009) have addressed summarization using KL-divergence, but they calculated KLdivergence based on word distributions in a discrete space. In other words, our study is the first attempt to summarize by asymptotically estimating KL-divergence based on embedding distributions in a continuous space. In addition, they involved the inference of complex models, whereas our method is quite simple but still powerful. 2 Preliminaries We treat a document as a bag-of-sentences and a sentence as a bag-of-words. Formally, let D be a document, and we refer to an element 1984 Proceedings of the 2015 Conference on </context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring Content Models for Multi-document Summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2009), pages 362– 370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Hong</author>
<author>John Conroy</author>
<author>Benoit Favre</author>
<author>Alex Kulesza</author>
<author>Hui Lin</author>
<author>Ani Nenkova</author>
</authors>
<title>A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2014),</booktitle>
<pages>26--31</pages>
<contexts>
<context position="14148" citStr="Hong et al., 2014" startWordPosition="2520" endWordPosition="2523">functions DocEmb and EmbDist have only one parameter r, and we similarly chose the best value of r. Regarding DocEmb, EmbDist, and SenEmb, we used the best embeddings from the CW Vector and W2V Vector for each method, and created document and sentence embeddings by averaging word embeddings with tf-idf weights since it performed better in this experiment. In the case of EmbDist, we used a variant of fNN based 3The previous work used a sentence-based constraint as f = 2 and w3 = 1, but we changed the setting since the variation in length has a noticeable impact on ROUGE scores as suggested in (Hong et al., 2014). R-1 R-2 R-3 R-4 ApxOpt 62.22 21.60 8.71 4.56 EmbDist (ln x) 56.00 16.70 4.93 1.89 EmbDist (x) 55.70 15.73 4.59 1.84 EmbDist (ex) 56.29 15.96 4.43 1.39 DocEmb 55.80 13.59 3.23 0.90 SenEmb 53.96 15.42 3.97 1.10 TfIdf 52.97 17.24 5.40 1.49 Table 1: ROUGE-N (R-N) metrics of DocEmb, EmbDist, SenEmb, and TfIdf. on distributions of sentence embeddings. In addition, we examined three scaling functions: logarithmic, linear, and exponential functions, i.e., ln x, x, ex, respectively. We calculated the ROUGE-N metric (Lin, 2004)4, which is a widely-used evaluation metric for summarization methods. ROUG</context>
</contexts>
<marker>Hong, Conroy, Favre, Kulesza, Lin, Nenkova, 2014</marker>
<rawString>Kai Hong, John Conroy, Benoit Favre, Alex Kulesza, Hui Lin, and Ani Nenkova. 2014. A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2014), pages 26–31. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikael K˚ageb¨ack</author>
<author>Olof Mogren</author>
<author>Nina Tahmasebi</author>
<author>Devdatt Dubhashi</author>
</authors>
<title>Extractive Summarization using Continuous Vector Space Models.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC 2014),</booktitle>
<pages>31--39</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>K˚ageb¨ack, Mogren, Tahmasebi, Dubhashi, 2014</marker>
<rawString>Mikael K˚ageb¨ack, Olof Mogren, Nina Tahmasebi, and Devdatt Dubhashi. 2014. Extractive Summarization using Continuous Vector Space Models. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC 2014), pages 31–39. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt J Kusner</author>
<author>Yu Sun</author>
<author>Nicholas I Kolkin</author>
<author>Kilian Q Weinberger</author>
</authors>
<title>From Word Embeddings To Document Distances.</title>
<date>2015</date>
<booktitle>In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015),</booktitle>
<pages>957--966</pages>
<contexts>
<context position="17509" citStr="Kusner et al. (2015)" startWordPosition="3060" endWordPosition="3063">r, it is still important that our simple objective functions can obtain good results competing with the state-of-the-art methods. 5 Conclusion In this study, we proposed simple but powerful summarization methods using the documentlevel similarity based on embeddings, or distributed representations of words. Our experimental results demonstrated that the proposed methods performed better than the existing state-of-theart methods based on the sentence-level similarity. This implies that the document-level similarity can capture more complex meanings than the sentence-level similarity. Recently, Kusner et al. (2015) independently discovered a similar definition to our objective function fNN through a different approach. They constructed a dissimilarity measure based on a framework using Earth Mover’s Distance (EMD) developed in the image processing field (Rubner et al., 1998; Rubner et al., 2000). EMD is a consistent measure of distance between two distributions of points. Interestingly, their heuristic lower bound of EMD is exactly the same as −fNN with a linear scaling function, i.e., g(x) = x. Moreover, they showed that this bound appears to be tight in real datasets. This suggests that our intuitive </context>
</contexts>
<marker>Kusner, Sun, Kolkin, Weinberger, 2015</marker>
<rawString>Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From Word Embeddings To Document Distances. In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), pages 957–966. JMLR.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lerman</author>
<author>Ryan McDonald</author>
</authors>
<title>Contrastive Summarization: An Experiment with Consumer Reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT</booktitle>
<pages>113--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3086" citStr="Lerman and McDonald, 2009" startWordPosition="479" endWordPosition="482">ive function defined by a cosine similarity based on document embeddings instead of sentence embeddings. Unfortunately, in contrast to our intuition, this similarity is not submodular, which we disprove later. Thus, we propose a valid submodular function based on embedding distributions, each of which represents a set of word embeddings in a document, as the document-level similarity. Our objective function is calculated based on the nearest neighbors’ distances on embedding distributions, which can be proved to be asymptotically related to KLdivergence in a continuous space. Several studies (Lerman and McDonald, 2009; Haghighi and Vanderwende, 2009) have addressed summarization using KL-divergence, but they calculated KLdivergence based on word distributions in a discrete space. In other words, our study is the first attempt to summarize by asymptotically estimating KL-divergence based on embedding distributions in a continuous space. In addition, they involved the inference of complex models, whereas our method is quite simple but still powerful. 2 Preliminaries We treat a document as a bag-of-sentences and a sentence as a bag-of-words. Formally, let D be a document, and we refer to an element 1984 Proce</context>
</contexts>
<marker>Lerman, McDonald, 2009</marker>
<rawString>Kevin Lerman and Ryan McDonald. 2009. Contrastive Summarization: An Experiment with Consumer Reviews. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2009), pages 113– 116. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Multi-document Summarization via Budgeted Maximization of Submodular Functions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010),</booktitle>
<pages>912--920</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1572" citStr="Lin and Bilmes, 2010" startWordPosition="241" endWordPosition="244">nstrated that our method performed better than the existing method based on sentence-level similarity. 1 Introduction Document summarization aims to rephrase a document in a short form called a summary while keeping its “meaning.” In the present study, we aim to characterize the meaning of a document using embeddings or distributed representations of words in the document, where an embedding of each word is represented as a real valued vector in a Euclidean space that corresponds to the word (Mikolov et al., 2013a; Mikolov et al., 2013b). Many previous studies have investigated summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Lin and Bilmes, 2012; Sipos et al., 2012; Morita et al., 2013), but to the best of our knowledge, only one (K˚ageb¨ack et al., 2014) considered a direct summarization method using embeddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings. Essentially, this method assumes linear meanings since the objective function is characterized by the summation of sentence-level similarities. However, this assumption is not always valid in real documents, and thus there </context>
<context position="6430" citStr="Lin and Bilmes, 2010" startWordPosition="1086" endWordPosition="1089">was originally proposed in studies of neural language models (Bengio et al., 2003). In the present study, we use the CW Vector1 and W2V Vector2 which are also used in the previous study (K˚ageb¨ack et al., 2014). 3 Proposed Method In this study, we focus on a summarization task as sentence selection in a document. The optimization framework in our task is the same as in the previous study and formalized in Algorithm 1, where ws represents the pre-defined weight or cost of a sentence s, e.g., sentence length, and r is its scaling factor. This algorithm, called modified greedy, was proposed in (Lin and Bilmes, 2010) and interestingly performed better than the stateof-the-art abstractive approach as shown in (Lin and Bilmes, 2011). Note that we have omitted the notation of D from f for simplicity because D is fixed in an optimization process. Algorithm 1: Modified greedy algorithm. Data: Document D, objective function f, and summary size `. Result: Summary C ⊂ D. 1 C ← ∅; U ← D; 2 while U =6 ∅ do s* ← argmaxsEU fC(s)/(ws)r; if E ws + ws* ≤ ` then C ← C ∪ {s*}; sEC U ← U \ {s*}; 6 s* ← argmaxsED:ws&lt;` f({s}); 7 return C ← argmaxC,E1C,1s*11 f(C�); Similarity Based on Document Embeddings First, we examine an </context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>Hui Lin and Jeff Bilmes. 2010. Multi-document Summarization via Budgeted Maximization of Submodular Functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010), pages 912–920. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A Class of Submodular Functions for Document Summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011),</booktitle>
<pages>510--520</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1594" citStr="Lin and Bilmes, 2011" startWordPosition="245" endWordPosition="248">od performed better than the existing method based on sentence-level similarity. 1 Introduction Document summarization aims to rephrase a document in a short form called a summary while keeping its “meaning.” In the present study, we aim to characterize the meaning of a document using embeddings or distributed representations of words in the document, where an embedding of each word is represented as a real valued vector in a Euclidean space that corresponds to the word (Mikolov et al., 2013a; Mikolov et al., 2013b). Many previous studies have investigated summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Lin and Bilmes, 2012; Sipos et al., 2012; Morita et al., 2013), but to the best of our knowledge, only one (K˚ageb¨ack et al., 2014) considered a direct summarization method using embeddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings. Essentially, this method assumes linear meanings since the objective function is characterized by the summation of sentence-level similarities. However, this assumption is not always valid in real documents, and thus there may be a better combin</context>
<context position="6546" citStr="Lin and Bilmes, 2011" startWordPosition="1103" endWordPosition="1106"> CW Vector1 and W2V Vector2 which are also used in the previous study (K˚ageb¨ack et al., 2014). 3 Proposed Method In this study, we focus on a summarization task as sentence selection in a document. The optimization framework in our task is the same as in the previous study and formalized in Algorithm 1, where ws represents the pre-defined weight or cost of a sentence s, e.g., sentence length, and r is its scaling factor. This algorithm, called modified greedy, was proposed in (Lin and Bilmes, 2010) and interestingly performed better than the stateof-the-art abstractive approach as shown in (Lin and Bilmes, 2011). Note that we have omitted the notation of D from f for simplicity because D is fixed in an optimization process. Algorithm 1: Modified greedy algorithm. Data: Document D, objective function f, and summary size `. Result: Summary C ⊂ D. 1 C ← ∅; U ← D; 2 while U =6 ∅ do s* ← argmaxsEU fC(s)/(ws)r; if E ws + ws* ≤ ` then C ← C ∪ {s*}; sEC U ← U \ {s*}; 6 s* ← argmaxsED:ws&lt;` f({s}); 7 return C ← argmaxC,E1C,1s*11 f(C�); Similarity Based on Document Embeddings First, we examine an objective function fCos defined by a cosine similarity based on document embeddings. An embedding of a document D is</context>
<context position="12401" citStr="Lin and Bilmes, 2011" startWordPosition="2214" endWordPosition="2217">(w, C1). Since g(x) = ln(x), we have fNN(C1) − fNN(C2) &gt; 0 if by 1986 D,-,(D, C1) &gt; 0 holds. The fact that E[D,-,(D, C1)] → D,-,(p ||q) as |C1 |→ ∞ and |D |→ ∞ concludes the theorem. 4 Experiments We compared our two proposed methods DocEmb and EmbDist with two state-of-the-art methods SenEmb and TfIdf. The first two methods DocEmb and EmbDist represent Algorithm 1 with our proposed objective functions f&apos;s and fNN, respectively. TfIdf represents Algorithm 1 with an objective function based on the sum of cosine similarities of tf-idf vectors that correspond to sentences, which was proposed in (Lin and Bilmes, 2011). SenEmb uses a cosine similarity measure based on embeddings instead of tf-idf vectors in the same framework as TfIdf, which was proposed in (K˚ageb¨ack et al., 2014). We conducted an experiment with almost the same setting as in the previous study, where they used the Opinosis dataset (Ganesan et al., 2010). This dataset is a collection of user reviews in 51 different topics such as hotels, cars, and products; thus, it is more appropriate for evaluating summarization of user-generated content than wellknown DUC datasets, which consist of formal news articles. Each topic in the collection com</context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A Class of Submodular Functions for Document Summarization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 510–520. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>Learning Mixtures of Submodular Shells with Application to Document Summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI 2012),</booktitle>
<pages>479--490</pages>
<publisher>AUAI.</publisher>
<contexts>
<context position="1616" citStr="Lin and Bilmes, 2012" startWordPosition="249" endWordPosition="252">an the existing method based on sentence-level similarity. 1 Introduction Document summarization aims to rephrase a document in a short form called a summary while keeping its “meaning.” In the present study, we aim to characterize the meaning of a document using embeddings or distributed representations of words in the document, where an embedding of each word is represented as a real valued vector in a Euclidean space that corresponds to the word (Mikolov et al., 2013a; Mikolov et al., 2013b). Many previous studies have investigated summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Lin and Bilmes, 2012; Sipos et al., 2012; Morita et al., 2013), but to the best of our knowledge, only one (K˚ageb¨ack et al., 2014) considered a direct summarization method using embeddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings. Essentially, this method assumes linear meanings since the objective function is characterized by the summation of sentence-level similarities. However, this assumption is not always valid in real documents, and thus there may be a better combination of two other sen</context>
</contexts>
<marker>Lin, Bilmes, 2012</marker>
<rawString>Hui Lin and Jeff Bilmes. 2012. Learning Mixtures of Submodular Shells with Application to Document Summarization. In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI 2012), pages 479–490. AUAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram CoOccurrence Statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT 2003). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14916" citStr="Lin and Hovy, 2003" startWordPosition="2643" endWordPosition="2646">.39 DocEmb 55.80 13.59 3.23 0.90 SenEmb 53.96 15.42 3.97 1.10 TfIdf 52.97 17.24 5.40 1.49 Table 1: ROUGE-N (R-N) metrics of DocEmb, EmbDist, SenEmb, and TfIdf. on distributions of sentence embeddings. In addition, we examined three scaling functions: logarithmic, linear, and exponential functions, i.e., ln x, x, ex, respectively. We calculated the ROUGE-N metric (Lin, 2004)4, which is a widely-used evaluation metric for summarization methods. ROUGE-N is based on the co-occurrence statistics of N-grams, and especially ROUGE-1 has been shown to have the highest correlation with human summaries (Lin and Hovy, 2003). ROUGE-N is similar to the BLEU metric for machine translation, but ROUGE-N is a recall-based metric while BLEU is a precision-based metric. Table 1 shows the results obtained for ROUGEN (N ≤ 4) using DocEmb, EmbDist, SenEmb, and TfIdf. ApxOpt represents the approximation results of the optimal solution in our problem, where we optimized ROUGE-1 with the gold standard summaries by Algorithm 1. The obtained results indicate that our proposed method EmbDist with exponential scaling performed the best for ROUGE-1, which is the best metric in terms of correlation with human summaries. The W2V Vec</context>
<context position="16517" citStr="Lin and Hovy, 2003" startWordPosition="2907" endWordPosition="2910">n the other hand, TfIdf with tf-idf vectors performed the worst for ROUGE-1. A possible reason is that a wide variety of expressions by users made it difficult to calculate similarities. This also suggests that embedding-based methods 4We used their software ROUGE version 1.5.5 with the parameters: -n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0. and only if �D,-,(D, C2) − � 1987 naturally have robustness for user-generated content. In the case of N ≥ 2, TfIdf performed the best for ROUGE-2 and ROUGE-3, while EmbDist with logarithmic scaling is better than TfIdf for ROUGE-4. According to (Lin and Hovy, 2003), the higher order ROUGE-N is worse than ROUGE-1 since it tends to score grammaticality rather than content. Conversely, Rankel et al. (2013) reports that there is a dataset where the higher order ROUGE-N is correlated with human summaries well. We may need to conduct human judgments to decide which metric is the best in this dataset for more accurate comparison. However, it is still important that our simple objective functions can obtain good results competing with the state-of-the-art methods. 5 Conclusion In this study, we proposed simple but powerful summarization methods using the docume</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic Evaluation of Summaries Using N-gram CoOccurrence Statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT 2003). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14673" citStr="Lin, 2004" startWordPosition="2608" endWordPosition="2609">in length has a noticeable impact on ROUGE scores as suggested in (Hong et al., 2014). R-1 R-2 R-3 R-4 ApxOpt 62.22 21.60 8.71 4.56 EmbDist (ln x) 56.00 16.70 4.93 1.89 EmbDist (x) 55.70 15.73 4.59 1.84 EmbDist (ex) 56.29 15.96 4.43 1.39 DocEmb 55.80 13.59 3.23 0.90 SenEmb 53.96 15.42 3.97 1.10 TfIdf 52.97 17.24 5.40 1.49 Table 1: ROUGE-N (R-N) metrics of DocEmb, EmbDist, SenEmb, and TfIdf. on distributions of sentence embeddings. In addition, we examined three scaling functions: logarithmic, linear, and exponential functions, i.e., ln x, x, ex, respectively. We calculated the ROUGE-N metric (Lin, 2004)4, which is a widely-used evaluation metric for summarization methods. ROUGE-N is based on the co-occurrence statistics of N-grams, and especially ROUGE-1 has been shown to have the highest correlation with human summaries (Lin and Hovy, 2003). ROUGE-N is similar to the BLEU metric for machine translation, but ROUGE-N is a recall-based metric while BLEU is a precision-based metric. Table 1 shows the results obtained for ROUGEN (N ≤ 4) using DocEmb, EmbDist, SenEmb, and TfIdf. ApxOpt represents the approximation results of the optimal solution in our problem, where we optimized ROUGE-1 with the</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Distributed Representations of Words and Phrases and their Compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26 (NIPS 2013),</booktitle>
<pages>3111--3119</pages>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="1470" citStr="Mikolov et al., 2013" startWordPosition="226" endWordPosition="229">orrespond to a document and its summary in a continuous space. An experiment using a real dataset demonstrated that our method performed better than the existing method based on sentence-level similarity. 1 Introduction Document summarization aims to rephrase a document in a short form called a summary while keeping its “meaning.” In the present study, we aim to characterize the meaning of a document using embeddings or distributed representations of words in the document, where an embedding of each word is represented as a real valued vector in a Euclidean space that corresponds to the word (Mikolov et al., 2013a; Mikolov et al., 2013b). Many previous studies have investigated summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Lin and Bilmes, 2012; Sipos et al., 2012; Morita et al., 2013), but to the best of our knowledge, only one (K˚ageb¨ack et al., 2014) considered a direct summarization method using embeddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings. Essentially, this method assumes linear meanings since the objective function is characterized by the summation of senten</context>
<context position="5642" citStr="Mikolov et al., 2013" startWordPosition="953" endWordPosition="956">where ties are broken arbitrarily, and we substitute Si+1 = Si ∪ {x*} in the i-th iteration beginning with S0 = ∅. This algorithm is quite simple but it is guaranteed to find a near optimal solution within 1 − 1/e ≈ 0.63 (Calinescu et al., 2007). 2.2 Embedding An embedding or distributed representation of a word is a real valued vector in an m-dimensional Euclidean space Rm, which expresses the “meaning” of the word. We denote an embedding of a word w by w~ ∈ Rm. If for any two words w1 and w2, the meaning of w1 is similar to that of w2, then ~w1 is expected to be near to ~w2. A recent study (Mikolov et al., 2013a) showed that a simple log-bilinear model can learn high quality embeddings to obtain a better result than recurrent neural networks, where the concept of embeddings was originally proposed in studies of neural language models (Bengio et al., 2003). In the present study, we use the CW Vector1 and W2V Vector2 which are also used in the previous study (K˚ageb¨ack et al., 2014). 3 Proposed Method In this study, we focus on a summarization task as sentence selection in a document. The optimization framework in our task is the same as in the previous study and formalized in Algorithm 1, where ws r</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013a. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 3111–3119. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT</booktitle>
<pages>746--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1470" citStr="Mikolov et al., 2013" startWordPosition="226" endWordPosition="229">orrespond to a document and its summary in a continuous space. An experiment using a real dataset demonstrated that our method performed better than the existing method based on sentence-level similarity. 1 Introduction Document summarization aims to rephrase a document in a short form called a summary while keeping its “meaning.” In the present study, we aim to characterize the meaning of a document using embeddings or distributed representations of words in the document, where an embedding of each word is represented as a real valued vector in a Euclidean space that corresponds to the word (Mikolov et al., 2013a; Mikolov et al., 2013b). Many previous studies have investigated summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Lin and Bilmes, 2012; Sipos et al., 2012; Morita et al., 2013), but to the best of our knowledge, only one (K˚ageb¨ack et al., 2014) considered a direct summarization method using embeddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings. Essentially, this method assumes linear meanings since the objective function is characterized by the summation of senten</context>
<context position="5642" citStr="Mikolov et al., 2013" startWordPosition="953" endWordPosition="956">where ties are broken arbitrarily, and we substitute Si+1 = Si ∪ {x*} in the i-th iteration beginning with S0 = ∅. This algorithm is quite simple but it is guaranteed to find a near optimal solution within 1 − 1/e ≈ 0.63 (Calinescu et al., 2007). 2.2 Embedding An embedding or distributed representation of a word is a real valued vector in an m-dimensional Euclidean space Rm, which expresses the “meaning” of the word. We denote an embedding of a word w by w~ ∈ Rm. If for any two words w1 and w2, the meaning of w1 is similar to that of w2, then ~w1 is expected to be near to ~w2. A recent study (Mikolov et al., 2013a) showed that a simple log-bilinear model can learn high quality embeddings to obtain a better result than recurrent neural networks, where the concept of embeddings was originally proposed in studies of neural language models (Bengio et al., 2003). In the present study, we use the CW Vector1 and W2V Vector2 which are also used in the previous study (K˚ageb¨ack et al., 2014). 3 Proposed Method In this study, we focus on a summarization task as sentence selection in a document. The optimization framework in our task is the same as in the previous study and formalized in Algorithm 1, where ws r</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013), pages 746–751. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hajime Morita</author>
<author>Ryohei Sasano</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Subtree Extractive Summarization via Submodular Maximization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1023--1032</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1658" citStr="Morita et al., 2013" startWordPosition="257" endWordPosition="260">evel similarity. 1 Introduction Document summarization aims to rephrase a document in a short form called a summary while keeping its “meaning.” In the present study, we aim to characterize the meaning of a document using embeddings or distributed representations of words in the document, where an embedding of each word is represented as a real valued vector in a Euclidean space that corresponds to the word (Mikolov et al., 2013a; Mikolov et al., 2013b). Many previous studies have investigated summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Lin and Bilmes, 2012; Sipos et al., 2012; Morita et al., 2013), but to the best of our knowledge, only one (K˚ageb¨ack et al., 2014) considered a direct summarization method using embeddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings. Essentially, this method assumes linear meanings since the objective function is characterized by the summation of sentence-level similarities. However, this assumption is not always valid in real documents, and thus there may be a better combination of two other sentences than the best and second best sente</context>
</contexts>
<marker>Morita, Sasano, Takamura, Okumura, 2013</marker>
<rawString>Hajime Morita, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2013. Subtree Extractive Summarization via Submodular Maximization. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1023–1032. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando P´erez-Cruz</author>
</authors>
<title>Estimation of Information Theoretic Measures for Continuous Random Variables.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 21 (NIPS</booktitle>
<pages>1257--1264</pages>
<publisher>Curran Associates, Inc.</publisher>
<marker>P´erez-Cruz, 2009</marker>
<rawString>Fernando P´erez-Cruz. 2009. Estimation of Information Theoretic Measures for Continuous Random Variables. In Advances in Neural Information Processing Systems 21 (NIPS 2009), pages 1257–1264. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Rankel</author>
<author>John M Conroy</author>
<author>Hoa Trang Dang</author>
<author>Ani Nenkova</author>
</authors>
<title>A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>131--136</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16658" citStr="Rankel et al. (2013)" startWordPosition="2930" endWordPosition="2933">s made it difficult to calculate similarities. This also suggests that embedding-based methods 4We used their software ROUGE version 1.5.5 with the parameters: -n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0. and only if �D,-,(D, C2) − � 1987 naturally have robustness for user-generated content. In the case of N ≥ 2, TfIdf performed the best for ROUGE-2 and ROUGE-3, while EmbDist with logarithmic scaling is better than TfIdf for ROUGE-4. According to (Lin and Hovy, 2003), the higher order ROUGE-N is worse than ROUGE-1 since it tends to score grammaticality rather than content. Conversely, Rankel et al. (2013) reports that there is a dataset where the higher order ROUGE-N is correlated with human summaries well. We may need to conduct human judgments to decide which metric is the best in this dataset for more accurate comparison. However, it is still important that our simple objective functions can obtain good results competing with the state-of-the-art methods. 5 Conclusion In this study, we proposed simple but powerful summarization methods using the documentlevel similarity based on embeddings, or distributed representations of words. Our experimental results demonstrated that the proposed meth</context>
</contexts>
<marker>Rankel, Conroy, Dang, Nenkova, 2013</marker>
<rawString>Peter A. Rankel, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. 2013. A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 131–136. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yossi Rubner</author>
<author>Carlo Tomasi</author>
<author>Leonidas J Guibas</author>
</authors>
<title>A Metric for Distributions with Applications to Image Databases.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth International Conference on Computer Vision (ICCV</booktitle>
<pages>59--66</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="17773" citStr="Rubner et al., 1998" startWordPosition="3098" endWordPosition="3101">s, or distributed representations of words. Our experimental results demonstrated that the proposed methods performed better than the existing state-of-theart methods based on the sentence-level similarity. This implies that the document-level similarity can capture more complex meanings than the sentence-level similarity. Recently, Kusner et al. (2015) independently discovered a similar definition to our objective function fNN through a different approach. They constructed a dissimilarity measure based on a framework using Earth Mover’s Distance (EMD) developed in the image processing field (Rubner et al., 1998; Rubner et al., 2000). EMD is a consistent measure of distance between two distributions of points. Interestingly, their heuristic lower bound of EMD is exactly the same as −fNN with a linear scaling function, i.e., g(x) = x. Moreover, they showed that this bound appears to be tight in real datasets. This suggests that our intuitive framework can theoretically connect the two wellknown measures, KL-divergence and EMD, based on the scaling of distance. Note that, to the best of our knowledge, there is currently no known study that considers such a theoretical relationship. In future research, </context>
</contexts>
<marker>Rubner, Tomasi, Guibas, 1998</marker>
<rawString>Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. 1998. A Metric for Distributions with Applications to Image Databases. In Proceedings of the Sixth International Conference on Computer Vision (ICCV 1998), pages 59–66. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yossi Rubner</author>
<author>Carlo Tomasi</author>
<author>Leonidas J Guibas</author>
</authors>
<title>The Earth Mover’s Distance as a Metric for Image Retrieval.</title>
<date>2000</date>
<journal>International Journal of Computer Vision,</journal>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="17795" citStr="Rubner et al., 2000" startWordPosition="3102" endWordPosition="3105">resentations of words. Our experimental results demonstrated that the proposed methods performed better than the existing state-of-theart methods based on the sentence-level similarity. This implies that the document-level similarity can capture more complex meanings than the sentence-level similarity. Recently, Kusner et al. (2015) independently discovered a similar definition to our objective function fNN through a different approach. They constructed a dissimilarity measure based on a framework using Earth Mover’s Distance (EMD) developed in the image processing field (Rubner et al., 1998; Rubner et al., 2000). EMD is a consistent measure of distance between two distributions of points. Interestingly, their heuristic lower bound of EMD is exactly the same as −fNN with a linear scaling function, i.e., g(x) = x. Moreover, they showed that this bound appears to be tight in real datasets. This suggests that our intuitive framework can theoretically connect the two wellknown measures, KL-divergence and EMD, based on the scaling of distance. Note that, to the best of our knowledge, there is currently no known study that considers such a theoretical relationship. In future research, we will explore other </context>
</contexts>
<marker>Rubner, Tomasi, Guibas, 2000</marker>
<rawString>Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. 2000. The Earth Mover’s Distance as a Metric for Image Retrieval. International Journal of Computer Vision, 40(2):99–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruben Sipos</author>
<author>Adith Swaminathan</author>
<author>Pannaga Shivaswamy</author>
<author>Thorsten Joachims</author>
</authors>
<title>Temporal Corpus Summarization Using Submodular Word Coverage.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012),</booktitle>
<pages>754--763</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1636" citStr="Sipos et al., 2012" startWordPosition="253" endWordPosition="256"> based on sentence-level similarity. 1 Introduction Document summarization aims to rephrase a document in a short form called a summary while keeping its “meaning.” In the present study, we aim to characterize the meaning of a document using embeddings or distributed representations of words in the document, where an embedding of each word is represented as a real valued vector in a Euclidean space that corresponds to the word (Mikolov et al., 2013a; Mikolov et al., 2013b). Many previous studies have investigated summarization (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Lin and Bilmes, 2012; Sipos et al., 2012; Morita et al., 2013), but to the best of our knowledge, only one (K˚ageb¨ack et al., 2014) considered a direct summarization method using embeddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings. Essentially, this method assumes linear meanings since the objective function is characterized by the summation of sentence-level similarities. However, this assumption is not always valid in real documents, and thus there may be a better combination of two other sentences than the best</context>
</contexts>
<marker>Sipos, Swaminathan, Shivaswamy, Joachims, 2012</marker>
<rawString>Ruben Sipos, Adith Swaminathan, Pannaga Shivaswamy, and Thorsten Joachims. 2012. Temporal Corpus Summarization Using Submodular Word Coverage. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012), pages 754–763. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Wang</author>
<author>S R Kulkarni</author>
<author>S Verdu</author>
</authors>
<title>Divergence Estimation for Multidimensional Densities Via k-Nearest-Neighbor Distances.</title>
<date>2009</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>55</volume>
<issue>5</issue>
<contexts>
<context position="11614" citStr="Wang et al., 2009" startWordPosition="2072" endWordPosition="2075">s C1 and C2 such that |C1 |= |C2|, which are samples drawn from some probability density functions p, q, and r, i.e., D ∼ p, C1 ∼ q, and C2 ∼ r, respectively. If the scaling function g of fNN is a logarithmic function, the order relation of the expectations of fNN(C1) and fNN(C2) is asymptotically the same as that of the KL-divergences DKL(p ||r) and DKL(p ||q), i.e., E[fNN(C1)] − E[fNN(C2)] &gt; 0 ⇔ DKL(p || r) − DKL(p ||q) &gt; 0, as |D |→ ∞, |C1 |→ ∞, and |C2 |→ ∞. Proof. Let m be the dimension on embeddings. Using a divergence estimator based on nearest neighbor distances in (P´erez-Cruz, 2009; Wang et al., 2009), we can approximate DKL(p ||q) PD bDKL(D, C1) := m w ln N(w,C1) N(w,D) + ln |C1| |D|−1. |D| Therefore, we obtain bDKL(D, C2)− bDKL(D, C1) ∝ PDw ln N(w, C2) − PDw lnN(w, C1). Since g(x) = ln(x), we have fNN(C1) − fNN(C2) &gt; 0 if by 1986 D,-,(D, C1) &gt; 0 holds. The fact that E[D,-,(D, C1)] → D,-,(p ||q) as |C1 |→ ∞ and |D |→ ∞ concludes the theorem. 4 Experiments We compared our two proposed methods DocEmb and EmbDist with two state-of-the-art methods SenEmb and TfIdf. The first two methods DocEmb and EmbDist represent Algorithm 1 with our proposed objective functions f&apos;s and fNN, respectively. T</context>
</contexts>
<marker>Wang, Kulkarni, Verdu, 2009</marker>
<rawString>Qing Wang, S.R. Kulkarni, and S. Verdu. 2009. Divergence Estimation for Multidimensional Densities Via k-Nearest-Neighbor Distances. IEEE Transactions on Information Theory, 55(5):2392–2405.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
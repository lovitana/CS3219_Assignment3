<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008236">
<title confidence="0.995295">
An Improved Non-monotonic Transition System for Dependency Parsing
</title>
<author confidence="0.999488">
Matthew Honnibal Mark Johnson
</author>
<affiliation confidence="0.80224">
spaCy.io Department of Computing
Berlin, Germany Macquarie University
matt@spacy.io Sydney, Australia
</affiliation>
<email confidence="0.995768">
mark.johnson@mq.edu.edu.au
</email>
<sectionHeader confidence="0.997355" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911307692308">
Transition-based dependency parsers usu-
ally use transition systems that monotoni-
cally extend partial parse states until they
identify a complete parse tree. Honni-
bal et al. (2013) showed that greedy one-
best parsing accuracy can be improved
by adding additional non-monotonic tran-
sitions that permit the parser to “repair”
earlier parsing mistakes by “over-writing”
earlier parsing decisions. This increases
the size of the set of complete parse trees
that each partial parse state can derive, en-
abling such a parser to escape the “gar-
den paths” that can trap monotonic greedy
transition-based dependency parsers.
We describe a new set of non-monotonic
transitions that permits a partial parse state
to derive a larger set of completed parse
trees than previous work, which allows our
parser to escape from a larger set of gar-
den paths. A parser with our new non-
monotonic transition system has 91.85%
directed attachment accuracy, an improve-
ment of 0.6% over a comparable parser us-
ing the standard monotonic arc-eager tran-
sitions.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958260869565">
Recent work from Dyer et al. (2015) and Weiss
et al. (2015) show that neural network models can
improve greedy transition-based parsers dramat-
ically, even beyond the 20% error reduction re-
ported by Chen and Manning (2014). Improve-
ments on beam-search parsing are much more lim-
ited, due to the difficulty of applying neural net-
works to structured prediction.
We suggest that the lack of a ready search so-
lution may present the next barrier to further im-
provements in accuracy. Some degree of search
flexibility seems inherently necessary, no mat-
ter how powerful the local model becomes, as
even the human sentence processor can be ‘garden
pathed’ by local structural ambiguities.
We take inspiration from Frazier and Rayner
(1982) and other psycholinguists and propose re-
pair actions as a light-weight alternative to beam-
search. In a transition-based dependency parser,
transitions map parse states to parse states, ulti-
mately producing completed parse trees. This pro-
cess is non-deterministic, since usually more than
one transition can apply to a parse state. This
means that each partial parse state can be associ-
ated with a set of complete parse trees (i.e., the
complete parses that can be produced by applying
sequences of transitions to the partial parse state).
In general adding additional transitions (mono-
tonic or non-monotonic) increases the number of
complete parse trees that any given partial parse
state can derive.
We explore adding non-monotonic parsing tran-
sitions to a greedy arc-eager dependency parser in
this paper, in order to permit the parser to recover
from attachment errors made early in the parsing
process. These additional non-monotonic transi-
tions permit the parser to modify what would have
been irrevocable parsing decisions in the mono-
tonic arc-eager system when later information jus-
tifies this action. Thus one effect of adding the
non-monotonic parsing transitions is to effectively
delay the location in the input where the parser
must ultimately commit to a particular attachment.
Our transition-system builds on the work of
Honnibal et al. (2013) and Nivre and Fernandez-
Gonzalez (2014), who each present modifications
</bodyText>
<page confidence="0.874148">
1373
</page>
<note confidence="0.651981">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373–1378,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999961657142857">
to the arc-eager transition system that introduce
some non-monotonic behaviour, resulting in small
improvements in accuracy. However, these sys-
tems only apply non-monotonic transitions to a
relatively small number of configurations, so they
can only have a small impact on parse accuracy.
We introduce a non-monotonic transition sys-
tem that combines ideas from these two ap-
proaches, and allows substantially more repair ca-
pability (and hence search flexibility). We ob-
serve a 0.6% improvement in accuracy on the
OntoNotes corpus, which is an error reduction of
6.25% over a competitive baseline. A parser using
our transition system is guaranteed to run in linear
time, and the modifications to the algorithm have
no negative impact on run-time in our implemen-
tation.
Very recently there has been considerable suc-
cess in applying neural network models to pre-
dict which transition to apply in greedy one-best
transition-based parsing. In their preprints, both
Dyer et al. (2015) and Weiss et al. (2015) re-
port error reductions of around 20-30% for greedy
one-best parsing, and much more modest im-
provements for transition-based parsers with beam
search. Because the neural network approaches
improve the local model that predicts which tran-
sition to apply next, while this paper suggests
changes to the transition system itself, it is rea-
sonable to expect that the improvements reported
here are largely orthogonal to those obtained us-
ing the neural network techniques. In future work
we would like to explore integrating such neural
network models of transition prediction with the
extended transition system proposed here.
</bodyText>
<sectionHeader confidence="0.9609915" genericHeader="method">
2 Improved non-monotonic transition
system
</sectionHeader>
<bodyText confidence="0.999939625">
Our transition-system is based on the tree-
constrained arc-eager system of Nivre and
Fernandez-Gonzalez (2014), which extends the
classic arc-eager system (Nivre, 2003) with a new
non-monotonic operation that they call “Unshift”.
They introduce the Unshift action to repair con-
figurations where the buffer is exhausted and the
stack contains multiple words that are without in-
coming arcs (i.e. without governors). The origi-
nal arc-eager configuration outputs partial parses
in this situation.
Nivre and Fernandez-Gonzalez restrict their
Unshift action, such that it can only be applied
when the buffer is exhausted and the word on top
of the stack has no incoming arc. In this config-
uration, the Unshift action is the only action that
can be applied. The use of the new action is there-
fore entirely deterministic, and they do not need
to produce example configurations for the Unshift
action during training. They train their model with
what Goldberg and Nivre (2012) term a ‘static or-
acle’, which can only label configurations that are
consistent with the gold-standard parse.
We take the Nivre and Fernandez-Gonzalez
(2014) Unshift operation, and import it into the
non-monotonic parsing model of Honnibal et al.
(2013), which uses a dynamic oracle to determine
the gold-standard actions for configurations pro-
duced by the parser. This training strategy is crit-
ical to the success of a non-monotonic transition
system. The model cannot learn to recover from
previous errors if the training data cannot contain
configurations that result from incorrect actions.
Honnibal et al. (2013) allow the parser to cor-
rect prior misclassifications between the Shift and
Right-Arc actions. Both of these actions push the
first word of the buffer onto the stack, but the
Right-Arc action also adds an arc. After the Right-
Arc is applied, the top two words of the stack are
connected.
In the original arc-eager system, the presence
or absence of this arc determines which of the two
pop moves, Reduce or Left-Arc, is valid. If the arc
is present, then Left-Arc is excluded; if it is absent,
the Reduce action is excluded. Honnibal et al.
(2013) argue that these deterministic constraints
are unmotivated when the parser is trained using a
dynamic, instead of static, oracle. Instead of a con-
straint, they suggest that consistency be achieved
by refining the logic of the actions, so that they
have a broader applicability. Instead of preventing
the Left-Arc from applying when the word on top
of the stack has an incoming arc, they update the
definition of the Left-Arc so that it first deletes the
existing arc if necessary. A corresponding change
is made to the Reduce action: if the model predicts
Reduce when the word on top of the stack has no
incoming arc, the ‘missing’ arc is inserted. The
arc is labelled by noting the best-scoring Right-
Arc label on each Shift action, so that the label
can be assigned during non-monotonic Reduce.
We show that the Nivre and Fernandez-
Gonzalez Unshift operation serves as a far supe-
rior non-monotonic Reduce action than the one
</bodyText>
<page confidence="0.955512">
1374
</page>
<table confidence="0.952420857142857">
Notation
(σ, β, A, S) is a configuration, where
σ|s is a stack of word indices with topmost element s
b|β is a buffer of word indices with first element b
A is a vector of head indices
A(i) = j denotes an arc wj → wz
S is a bit-vector used to prevent Shift/Unshift cycles
Initial ([ ], [1...n], A(1) = 1)
Terminal ([i], [ ], A)
Shift (σ, b|β, A, S(b) = 0) ⇒ (σ|b,β, A, S(b) = 1)
Right-Arc (σ|s, b|β, A, S) ⇒ (σ|s|b, β, A(b) = s, S)
Reduce (σ|s, β, A(s) =6 0, S) ⇒ (σ, β, A, S)
Unshift (σ|s, β, A(s) = 0, S) ⇒ (σ, s|β, A, S)
Left-Arc (σ|s, b|β, A, S) ⇒ (σ, s|β, A(s) = b, S
</table>
<tableCaption confidence="0.993213">
Table 1: Our non-monotonic transition system, which integrates the Unshift action of Nivre and
</tableCaption>
<bodyText confidence="0.9336266">
Fernandez-Gonzalez (2014) into the model of Honnibal et al. (2013).
Honnibal et al. use in their system, and that the re-
sulting transition system improves parse accuracy
by considerably more than either the Honnibal et
al or Nivre et al systems do.
</bodyText>
<subsectionHeader confidence="0.991976">
2.1 Definition of Transition System
</subsectionHeader>
<bodyText confidence="0.990962055555556">
The hybrid transition system is defined in Table
1. Arcs are stored in a vector, A, where the entry
A(i) = j denotes an arc wj → wz. Words are
pushed from the buffer β onto the stack σ, using
either the Shift or the Right-Arc actions.
If a word was pushed with the Shift action, it
will not have an incoming arc. The new Unshift
action will then be valid, at any point at which the
word is on top of the stack — even after many ac-
tions have been performed.
The Unshift action pops the top word of the
stack, s, and places it at the start of the buffer.
Parsing then proceeds as normal. To prevent cy-
cles, the Shift action checks and sets a bit in the
new boolean vector S. The Shift action is invalid
if S(b) = 1, for a word b at the front of the buffer.
This bit will be set if the word was previously
Shifted, and then Unshifted.
At worst, each word can be pushed and popped
from the stack twice, so parsing is guaranteed to
terminate after a maximum of 4n transitions for a
sentence of length n.
The terminal condition is reached when the
buffer is exhausted and exactly one word remains
on the stack. This word will be deemed the root
of the sentence. No ‘dummy’ root token is neces-
sary, removing the need to choose whether the to-
ken is placed at the beginning or end of the buffer
(Ballesteros and Nivre, 2013).
Note that if the two words each seem like the
governor of the sentence, such that the parser
deems all incoming arcs to these words unlikely,
the transition system is guaranteed to arrive at a
configuration where these two words are adjacent
to each other. The model can then predict an arc
between them, initiated by either word.
</bodyText>
<subsectionHeader confidence="0.991655">
2.2 Dynamic Training Oracle
</subsectionHeader>
<bodyText confidence="0.960312153846154">
Goldberg and Nivre (2013) describe three ques-
tions that need to be answered in order to imple-
ment their training algorithm.
Exploration Policy: When do we follow an incor-
rect transition, and which one do we follow?
We always follow the predicted transition, i.e.
their two hyper-parameters are set k = 1 and
p = 1.0.
Optimality: What constitutes an optimal transi-
tion in configurations from which the gold tree is
not reachable?
We follow Honnibal et al. (2013) in defining a
transition as optimal if it:
</bodyText>
<listItem confidence="0.9989545">
1. Renders no additional arcs unreachable using
the monotonic arc-eager transitions; and
2. Renders no additional arcs unreachable using
the non-monotonic transitions.
</listItem>
<bodyText confidence="0.6570995">
Said another way, we mark a transition as opti-
mal if it leads to an analysis with as few errors
as possible, and in cases of ties, uses as few non-
monotonic transitions as possible.
</bodyText>
<page confidence="0.966373">
1375
</page>
<bodyText confidence="0.98412225">
For example, given the input string I saw Jack,
consider a configuration where saw is on the stack,
Jack is at the front of the buffer, and I is attached to
saw. The gold arcs are saw —* I and saw —* Jack.
In the monotonic system, the Shift action would
make the gold arc saw —* Jack newly unreachable.
In our system, this arc is still reachable after Shift,
via the Unshift action, but we consider the Shift
move non-optimal, so that the non-monotonic ac-
tions are reserved as ”repair” operations.
Oracle: Given a definition of optimality, how do
we calculate the set of optimal transitions in a
given configuration?
Goldberg and Nivre (2013) show that with the
monotonic arc-eager actions, the following arcs
are reachable from an arbitrary configuration:
</bodyText>
<listItem confidence="0.964315857142857">
1. Arcs {wi —* wj : i E σ, j E β1 — i.e. all
arcs from stack words to buffer words;
2. Arcs {wi —* wj : i E β, j E σ, A(j) = 01
— i.e. all arcs from buffer words to headless
stack words;
3. Arcs {wi —* wj : i E β, j E β1 — i.e. all
arcs between words in buffer.
</listItem>
<bodyText confidence="0.8930535">
Our non-monotonic actions additionally allow the
following arcs to be reached:
</bodyText>
<listItem confidence="0.991477428571429">
4. Arcs {wi —* wj : i E β, j E σ,A(j) =�01
(LeftArc can now ”clobber” existing heads)
5. Arcs {wi —* wj or wj —* wi : i, j E σ, i &lt;
j,A(j) = 01 — i.e. if a word i is on the
stack, it can reach an arc to or from a word j
ahead of it on the stack if that word does not
have a head set.
</listItem>
<bodyText confidence="0.598101">
In practice, we therefore only need to add two
rules to determine the set of optimal transitions:
</bodyText>
<listItem confidence="0.9947582">
1. If σ0 has a head, and its true head is in the
buffer, the Reduce action is now non-optimal.
2. If σ0 does not have a head, and its true head
is in the stack, the LeftArc action is now non-
optimal.
</listItem>
<bodyText confidence="0.999952071428571">
The oracle calculation is simple because the sys-
tem preserves the arc decomposition property that
Goldberg and Nivre (2013) prove for the arc eager
system: if two arcs of a projective tree are individ-
ually reachable from a configuration, a projective
tree that includes both arcs is also reachable. To
see that this property is preserved in our system,
consider that an arc h —* d between two stack
words is only unreachable if h &lt; d and A(d) =� 0.
But a projective tree with arc h —* d cannot also
have an arc x —* y such that h &lt; x &lt; d &lt; y.
So there can be no other arc part of the same pro-
jective tree as h —* d that would require d to be
assigned to some other head.
</bodyText>
<sectionHeader confidence="0.991317" genericHeader="method">
3 Training Procedure
</sectionHeader>
<bodyText confidence="0.99998596">
We follow Honnibal et al. (2013) in using the
dynamic oracle-based search-and-learn training
strategy introduced by Goldberg and Nivre (2012).
A dynamic oracle is a function that labels config-
urations with gold-standard actions. Importantly,
a dynamic oracle can label arbitrary configura-
tions, while a so-called ‘static’ oracle can only as-
sign labels to configurations that are part of gold-
standard derivations.
We employ the dynamic oracle in an on-
line learning strategy, similar to imitation-based
learning, where the examples are configurations
produced by following the current model’s pre-
dictions. The configurations are labelled by
the dynamic oracle, which determines which of
the available actions excludes the fewest gold-
standard arcs.
Often, multiple actions will be labelled as gold-
standard for a given configuration. This implies ei-
ther spurious ambiguity (the same analysis reach-
able via different derivations) or previous errors,
such that the best parse reachable by different ac-
tions are equally bad. When this occurs, we base
the perceptron update on the highest-scoring gold-
standard label.
</bodyText>
<subsectionHeader confidence="0.999822">
3.1 Single class for Unshift/Reduce
</subsectionHeader>
<bodyText confidence="0.999978357142857">
The Unshift and Reduce actions are applicable to
a disjoint set of configurations. If the word on top
of the stack already has an incoming arc, the Re-
duce move is valid; otherwise, the Unshift move
is valid. For the purpose of training and predic-
tion, we therefore model these actions as a single
class, which we interpret based on the configura-
tion. This allows us to learn the Unshift action
more effectively, as it is allowed to share a repre-
sentation with the Reduce move. In preliminary
development, we found that assigning a distinct
class to the Unshift action was not effective. We
plan to evaluate this option more rigorously in fu-
ture work.
</bodyText>
<page confidence="0.993942">
1376
</page>
<sectionHeader confidence="0.999237" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999996925925926">
We implemented a greedy transition-based parser,
and used rich contextual features following Zhang
and Nivre (2011). We extended the feature set to
include Brown cluster features, using the cluster
prefix trick described by Koo and Collins (2010).
Brown clusters are a standard way to improve
the cross-domain performance of supervised lin-
ear models. The use of Brown cluster features ac-
counts for the 0.7% improvement in accuracy ob-
served between our baseline parser and the Gold-
berg and Nivre (2012) result shown in Table 2.
The two models are otherwise the same.
Part-of-speech tags were predicted using a
greedy averaged perceptron model that achieved
97.2% accuracy on the evaluation data. Most pre-
vious work uses a n-way jack-knifing to train the
stacked tagger/parser model. For convenience, we
instead train the tagger at the same time as the
parser, as both allow online learning. We find this
makes no difference to accuracy.
Our parsers are trained and evaluated on the
same data used by Tetreault et al. (2015) in their
recent ‘bake-off’ of leading dependency pars-
ing models. Specifically, we use the OntoNotes
corpus converted into dependencies using the
ClearNLP 3.1 converter, with the train / dev / test
split of the CoNLL 2012 shared task.
</bodyText>
<sectionHeader confidence="0.999895" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99983895">
We implemented three previous versions of the
arc-eager transition system, in order to evaluate
the effect of our proposed transition-system on
parser accuracy. The four systems differ only in
their transition system — they are otherwise iden-
tical. All use identical features, and all are trained
with the dynamic oracle.
Orig. Arc Eager (Nivre, 2003): the origi-
nal arc-eager system, which constrains the Re-
duce and Left-Arc actions to ensure monotonic-
ity; Prev. Non-Monotic (Honnibal et al., 2013):
relaxes the monotonicity constraints, allowing
Left-Arc to ”clobber” existing arcs, and insert-
ing missing arcs on Reduce with a simple heuris-
tic; Tree Constrained (Nivre and Fernandez-
Gonzalez, 2014): adds an Unshift action to the
arc-eager system, that is only employed when the
buffer is exhausted; This work: merges the Un-
shift action into our previous non-monotonic tran-
sition system.
</bodyText>
<table confidence="0.9997615">
Transition System Search UAS LAS
Orig. Arc Eager Greedy 91.25 89.40
Tree Constrained Greedy 91.40 89.50
Prev. Non-Monotonic Greedy 91.36 89.52
This work Greedy 91.85 89.91
Chen and Manning (2014) Greedy 89.59 87.63
Goldberg and Nivre (2012) Greedy 90.54 88.75
Choi and Mccallum (2013) Branch 92.26 90.84
Zhang and Nivre (2011) Beam32 92.24 90.50
Bohnet(2010) Graph 92.50 90.70
</table>
<tableCaption confidence="0.953648666666667">
Table 2: Our non-monotonic transition system improves
accuracy by 0.6% unlabelled attachment score, for a final
score of 91.85 on the OntoNotes corpus.
</tableCaption>
<bodyText confidence="0.998580384615385">
Table 2 shows the unlabelled and labelled at-
tachment scores of the parsers on the evaluation
data. The two previous non-monotonic systems,
Prev. Non-monotonic and Tree Constrained, were
slightly more accurate than the Orig. Arc Eager
system. Our new transition-system had a much
bigger impact, improving UAS by 0.6% and LAS
by 0.51%. To put the scores in context, we have
also included figures reported in a recent sur-
vey of the current state-of-the-art (Tetreault et al.,
2015). Our parser out-performs existing greedy
parsers, and is much more efficient than non-
greedy parsers.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99992456">
This paper integrates innovations from Honnibal
et al. (2013) and Nivre and Fernandez-Gonzalez
(2014) to produce a novel non-monotonic set of
transitions for transition-based dependency pars-
ing. Doing this required us to use the dynamic
oracle of Goldberg and Nivre (2012) during train-
ing in order to produce configurations that exercise
the non-monotonic transitions. We show that this
combination of innovations results in a parser with
91.85% directed accuracy, which is an improve-
ment of 0.6% directed accuracy over an equivalent
arc-standard parser. Interestingly, the Honnibal et
al and Nivre et al innovations applied on their own
only produce improvements of 0.11% and 0.15%
respectively, so it seems that these improvements
taken together do interact synergistically.
Because our innovation largely affects the
search space of a greedy one-best parser, it is
likely to be independent of the recent improve-
ments in parsing accuracy that come from using
neural networks to predict the best next parsing
transition. In future work we plan to combine
such neural network models with a version of our
parser that incorporates a much larger set of non-
monotonic parsing transitions.
</bodyText>
<page confidence="0.99307">
1377
</page>
<sectionHeader confidence="0.991793" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971344383561644">
Miguel Ballesteros and Joakim Nivre. 2013. Go-
ing to the roots of dependency parsing. Compu-
tational Linguistics. 39:1.
Danqi Chen and Christopher Manning. 2014. A
fast and accurate dependency parser using neu-
ral networks. In Proceedings of the 2014
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 740–
750. Association for Computational Linguis-
tics, Doha, Qatar.
Chris Dyer, Miguel Ballesteros, Wang Ling,
Austin Matthews, and Noah A. Smith. 2015.
Transition-based dependency parsing with stack
long short-term memory. In Proceedings of
the 53nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 13–24. Association for Computa-
tional Linguistics, Beijing, China.
Lyn Frazier and Keith Rayner. 1982. Making and
correcting errors during sentence comprehen-
sion: Eye movements in the analysis of struc-
turally ambiguous sentences. Cognitive Psy-
chology, 14(2):178–210.
Yoav Goldberg and Joakim Nivre. 2012. A dy-
namic oracle for arc-eager dependency parsing.
In Proceedings of the 24th International Con-
ference on Computational Linguistics (Coling
2012). Association for Computational Linguis-
tics, Mumbai, India.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic or-
acles. TACL, 1:403–414.
Matthew Honnibal, Yoav Goldberg, and Mark
Johnson. 2013. A non-monotonic arc-eager
transition system for dependency parsing. In
Proceedings of the Seventeenth Conference on
Computational Natural Language Learning,
pages 163–172. Association for Computational
Linguistics, Sofia, Bulgaria.
Terry Koo and Michael Collins. 2010. Efficient
third-order dependency parsers. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1–
11.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings
of the 8th International Workshop on Parsing
Technologies (IWPT), pages 149–160.
Joakim Nivre and Daniel Fernandez-Gonzalez.
2014. Arc-eager parsing with the tree con-
straint. Computational Linguistics, 40(2):259–
267.
Joel Tetreault, Jin ho Choi, and Amanda Stent.
2015. It depends: Dependency parser com-
parison using a web-based evaluation tool. In
Proceedings of the 53nd Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 13–24. Association
for Computational Linguistics, Beijing, China.
David Weiss, Chris Alberti, Michael Collins, and
Slav Petrov. 2015. Structured training for neural
network transition-based parsing. In Proceed-
ings of the 53nd Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume
1: Long Papers), pages 13–24. Association for
Computational Linguistics, Beijing, China.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local
features. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies,
pages 188–193. Association for Computational
Linguistics, Portland, Oregon, USA.
</reference>
<page confidence="0.993897">
1378
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910244">
<title confidence="0.999963">An Improved Non-monotonic Transition System for Dependency Parsing</title>
<author confidence="0.999992">Matthew Honnibal Mark Johnson</author>
<affiliation confidence="0.991299">spaCy.io Department of Computing Berlin, Germany Macquarie University</affiliation>
<address confidence="0.991078">Australia</address>
<email confidence="0.997017">mark.johnson@mq.edu.edu.au</email>
<abstract confidence="0.997677740740741">Transition-based dependency parsers usually use transition systems that monotonically extend partial parse states until they identify a complete parse tree. Honnibal et al. (2013) showed that greedy onebest parsing accuracy can be improved by adding additional non-monotonic transitions that permit the parser to “repair” earlier parsing mistakes by “over-writing” earlier parsing decisions. This increases the size of the set of complete parse trees that each partial parse state can derive, enabling such a parser to escape the “garden paths” that can trap monotonic greedy transition-based dependency parsers. We describe a new set of non-monotonic transitions that permits a partial parse state to derive a larger set of completed parse trees than previous work, which allows our parser to escape from a larger set of garden paths. A parser with our new nonmonotonic transition system has 91.85% directed attachment accuracy, an improvement of 0.6% over a comparable parser using the standard monotonic arc-eager transitions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
<author>Joakim Nivre</author>
</authors>
<title>Going to the roots of dependency parsing.</title>
<date>2013</date>
<journal>Computational Linguistics.</journal>
<volume>39</volume>
<contexts>
<context position="10635" citStr="Ballesteros and Nivre, 2013" startWordPosition="1767" endWordPosition="1770">on is invalid if S(b) = 1, for a word b at the front of the buffer. This bit will be set if the word was previously Shifted, and then Unshifted. At worst, each word can be pushed and popped from the stack twice, so parsing is guaranteed to terminate after a maximum of 4n transitions for a sentence of length n. The terminal condition is reached when the buffer is exhausted and exactly one word remains on the stack. This word will be deemed the root of the sentence. No ‘dummy’ root token is necessary, removing the need to choose whether the token is placed at the beginning or end of the buffer (Ballesteros and Nivre, 2013). Note that if the two words each seem like the governor of the sentence, such that the parser deems all incoming arcs to these words unlikely, the transition system is guaranteed to arrive at a configuration where these two words are adjacent to each other. The model can then predict an arc between them, initiated by either word. 2.2 Dynamic Training Oracle Goldberg and Nivre (2013) describe three questions that need to be answered in order to implement their training algorithm. Exploration Policy: When do we follow an incorrect transition, and which one do we follow? We always follow the pre</context>
</contexts>
<marker>Ballesteros, Nivre, 2013</marker>
<rawString>Miguel Ballesteros and Joakim Nivre. 2013. Going to the roots of dependency parsing. Computational Linguistics. 39:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="1501" citStr="Chen and Manning (2014)" startWordPosition="225" endWordPosition="228">onotonic transitions that permits a partial parse state to derive a larger set of completed parse trees than previous work, which allows our parser to escape from a larger set of garden paths. A parser with our new nonmonotonic transition system has 91.85% directed attachment accuracy, an improvement of 0.6% over a comparable parser using the standard monotonic arc-eager transitions. 1 Introduction Recent work from Dyer et al. (2015) and Weiss et al. (2015) show that neural network models can improve greedy transition-based parsers dramatically, even beyond the 20% error reduction reported by Chen and Manning (2014). Improvements on beam-search parsing are much more limited, due to the difficulty of applying neural networks to structured prediction. We suggest that the lack of a ready search solution may present the next barrier to further improvements in accuracy. Some degree of search flexibility seems inherently necessary, no matter how powerful the local model becomes, as even the human sentence processor can be ‘garden pathed’ by local structural ambiguities. We take inspiration from Frazier and Rayner (1982) and other psycholinguists and propose repair actions as a light-weight alternative to beams</context>
<context position="18391" citStr="Chen and Manning (2014)" startWordPosition="3122" endWordPosition="3125">v. Non-Monotic (Honnibal et al., 2013): relaxes the monotonicity constraints, allowing Left-Arc to ”clobber” existing arcs, and inserting missing arcs on Reduce with a simple heuristic; Tree Constrained (Nivre and FernandezGonzalez, 2014): adds an Unshift action to the arc-eager system, that is only employed when the buffer is exhausted; This work: merges the Unshift action into our previous non-monotonic transition system. Transition System Search UAS LAS Orig. Arc Eager Greedy 91.25 89.40 Tree Constrained Greedy 91.40 89.50 Prev. Non-Monotonic Greedy 91.36 89.52 This work Greedy 91.85 89.91 Chen and Manning (2014) Greedy 89.59 87.63 Goldberg and Nivre (2012) Greedy 90.54 88.75 Choi and Mccallum (2013) Branch 92.26 90.84 Zhang and Nivre (2011) Beam32 92.24 90.50 Bohnet(2010) Graph 92.50 90.70 Table 2: Our non-monotonic transition system improves accuracy by 0.6% unlabelled attachment score, for a final score of 91.85 on the OntoNotes corpus. Table 2 shows the unlabelled and labelled attachment scores of the parsers on the evaluation data. The two previous non-monotonic systems, Prev. Non-monotonic and Tree Constrained, were slightly more accurate than the Orig. Arc Eager system. Our new transition-syste</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740– 750. Association for Computational Linguistics, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Miguel Ballesteros</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Noah A Smith</author>
</authors>
<title>Transition-based dependency parsing with stack long short-term memory.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>13--24</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1315" citStr="Dyer et al. (2015)" startWordPosition="195" endWordPosition="198">artial parse state can derive, enabling such a parser to escape the “garden paths” that can trap monotonic greedy transition-based dependency parsers. We describe a new set of non-monotonic transitions that permits a partial parse state to derive a larger set of completed parse trees than previous work, which allows our parser to escape from a larger set of garden paths. A parser with our new nonmonotonic transition system has 91.85% directed attachment accuracy, an improvement of 0.6% over a comparable parser using the standard monotonic arc-eager transitions. 1 Introduction Recent work from Dyer et al. (2015) and Weiss et al. (2015) show that neural network models can improve greedy transition-based parsers dramatically, even beyond the 20% error reduction reported by Chen and Manning (2014). Improvements on beam-search parsing are much more limited, due to the difficulty of applying neural networks to structured prediction. We suggest that the lack of a ready search solution may present the next barrier to further improvements in accuracy. Some degree of search flexibility seems inherently necessary, no matter how powerful the local model becomes, as even the human sentence processor can be ‘gard</context>
<context position="4616" citStr="Dyer et al. (2015)" startWordPosition="708" endWordPosition="711">m these two approaches, and allows substantially more repair capability (and hence search flexibility). We observe a 0.6% improvement in accuracy on the OntoNotes corpus, which is an error reduction of 6.25% over a competitive baseline. A parser using our transition system is guaranteed to run in linear time, and the modifications to the algorithm have no negative impact on run-time in our implementation. Very recently there has been considerable success in applying neural network models to predict which transition to apply in greedy one-best transition-based parsing. In their preprints, both Dyer et al. (2015) and Weiss et al. (2015) report error reductions of around 20-30% for greedy one-best parsing, and much more modest improvements for transition-based parsers with beam search. Because the neural network approaches improve the local model that predicts which transition to apply next, while this paper suggests changes to the transition system itself, it is reasonable to expect that the improvements reported here are largely orthogonal to those obtained using the neural network techniques. In future work we would like to explore integrating such neural network models of transition prediction with</context>
</contexts>
<marker>Dyer, Ballesteros, Ling, Matthews, Smith, 2015</marker>
<rawString>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A. Smith. 2015. Transition-based dependency parsing with stack long short-term memory. In Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13–24. Association for Computational Linguistics, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
<author>Keith Rayner</author>
</authors>
<title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences.</title>
<date>1982</date>
<journal>Cognitive Psychology,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="2009" citStr="Frazier and Rayner (1982)" startWordPosition="308" endWordPosition="311">greedy transition-based parsers dramatically, even beyond the 20% error reduction reported by Chen and Manning (2014). Improvements on beam-search parsing are much more limited, due to the difficulty of applying neural networks to structured prediction. We suggest that the lack of a ready search solution may present the next barrier to further improvements in accuracy. Some degree of search flexibility seems inherently necessary, no matter how powerful the local model becomes, as even the human sentence processor can be ‘garden pathed’ by local structural ambiguities. We take inspiration from Frazier and Rayner (1982) and other psycholinguists and propose repair actions as a light-weight alternative to beamsearch. In a transition-based dependency parser, transitions map parse states to parse states, ultimately producing completed parse trees. This process is non-deterministic, since usually more than one transition can apply to a parse state. This means that each partial parse state can be associated with a set of complete parse trees (i.e., the complete parses that can be produced by applying sequences of transitions to the partial parse state). In general adding additional transitions (monotonic or non-m</context>
</contexts>
<marker>Frazier, Rayner, 1982</marker>
<rawString>Lyn Frazier and Keith Rayner. 1982. Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology, 14(2):178–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>A dynamic oracle for arc-eager dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (Coling 2012). Association for Computational Linguistics,</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="6272" citStr="Goldberg and Nivre (2012)" startWordPosition="968" endWordPosition="971">multiple words that are without incoming arcs (i.e. without governors). The original arc-eager configuration outputs partial parses in this situation. Nivre and Fernandez-Gonzalez restrict their Unshift action, such that it can only be applied when the buffer is exhausted and the word on top of the stack has no incoming arc. In this configuration, the Unshift action is the only action that can be applied. The use of the new action is therefore entirely deterministic, and they do not need to produce example configurations for the Unshift action during training. They train their model with what Goldberg and Nivre (2012) term a ‘static oracle’, which can only label configurations that are consistent with the gold-standard parse. We take the Nivre and Fernandez-Gonzalez (2014) Unshift operation, and import it into the non-monotonic parsing model of Honnibal et al. (2013), which uses a dynamic oracle to determine the gold-standard actions for configurations produced by the parser. This training strategy is critical to the success of a non-monotonic transition system. The model cannot learn to recover from previous errors if the training data cannot contain configurations that result from incorrect actions. Honn</context>
<context position="14367" citStr="Goldberg and Nivre (2012)" startWordPosition="2477" endWordPosition="2480">achable from a configuration, a projective tree that includes both arcs is also reachable. To see that this property is preserved in our system, consider that an arc h —* d between two stack words is only unreachable if h &lt; d and A(d) =� 0. But a projective tree with arc h —* d cannot also have an arc x —* y such that h &lt; x &lt; d &lt; y. So there can be no other arc part of the same projective tree as h —* d that would require d to be assigned to some other head. 3 Training Procedure We follow Honnibal et al. (2013) in using the dynamic oracle-based search-and-learn training strategy introduced by Goldberg and Nivre (2012). A dynamic oracle is a function that labels configurations with gold-standard actions. Importantly, a dynamic oracle can label arbitrary configurations, while a so-called ‘static’ oracle can only assign labels to configurations that are part of goldstandard derivations. We employ the dynamic oracle in an online learning strategy, similar to imitation-based learning, where the examples are configurations produced by following the current model’s predictions. The configurations are labelled by the dynamic oracle, which determines which of the available actions excludes the fewest goldstandard a</context>
<context position="16542" citStr="Goldberg and Nivre (2012)" startWordPosition="2826" endWordPosition="2830">to the Unshift action was not effective. We plan to evaluate this option more rigorously in future work. 1376 4 Experiments We implemented a greedy transition-based parser, and used rich contextual features following Zhang and Nivre (2011). We extended the feature set to include Brown cluster features, using the cluster prefix trick described by Koo and Collins (2010). Brown clusters are a standard way to improve the cross-domain performance of supervised linear models. The use of Brown cluster features accounts for the 0.7% improvement in accuracy observed between our baseline parser and the Goldberg and Nivre (2012) result shown in Table 2. The two models are otherwise the same. Part-of-speech tags were predicted using a greedy averaged perceptron model that achieved 97.2% accuracy on the evaluation data. Most previous work uses a n-way jack-knifing to train the stacked tagger/parser model. For convenience, we instead train the tagger at the same time as the parser, as both allow online learning. We find this makes no difference to accuracy. Our parsers are trained and evaluated on the same data used by Tetreault et al. (2015) in their recent ‘bake-off’ of leading dependency parsing models. Specifically,</context>
<context position="18436" citStr="Goldberg and Nivre (2012)" startWordPosition="3129" endWordPosition="3132">axes the monotonicity constraints, allowing Left-Arc to ”clobber” existing arcs, and inserting missing arcs on Reduce with a simple heuristic; Tree Constrained (Nivre and FernandezGonzalez, 2014): adds an Unshift action to the arc-eager system, that is only employed when the buffer is exhausted; This work: merges the Unshift action into our previous non-monotonic transition system. Transition System Search UAS LAS Orig. Arc Eager Greedy 91.25 89.40 Tree Constrained Greedy 91.40 89.50 Prev. Non-Monotonic Greedy 91.36 89.52 This work Greedy 91.85 89.91 Chen and Manning (2014) Greedy 89.59 87.63 Goldberg and Nivre (2012) Greedy 90.54 88.75 Choi and Mccallum (2013) Branch 92.26 90.84 Zhang and Nivre (2011) Beam32 92.24 90.50 Bohnet(2010) Graph 92.50 90.70 Table 2: Our non-monotonic transition system improves accuracy by 0.6% unlabelled attachment score, for a final score of 91.85 on the OntoNotes corpus. Table 2 shows the unlabelled and labelled attachment scores of the parsers on the evaluation data. The two previous non-monotonic systems, Prev. Non-monotonic and Tree Constrained, were slightly more accurate than the Orig. Arc Eager system. Our new transition-system had a much bigger impact, improving UAS by </context>
</contexts>
<marker>Goldberg, Nivre, 2012</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2012. A dynamic oracle for arc-eager dependency parsing. In Proceedings of the 24th International Conference on Computational Linguistics (Coling 2012). Association for Computational Linguistics, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parsers with non-deterministic oracles.</title>
<date>2013</date>
<tech>TACL,</tech>
<pages>1--403</pages>
<contexts>
<context position="11021" citStr="Goldberg and Nivre (2013)" startWordPosition="1833" endWordPosition="1836">e word remains on the stack. This word will be deemed the root of the sentence. No ‘dummy’ root token is necessary, removing the need to choose whether the token is placed at the beginning or end of the buffer (Ballesteros and Nivre, 2013). Note that if the two words each seem like the governor of the sentence, such that the parser deems all incoming arcs to these words unlikely, the transition system is guaranteed to arrive at a configuration where these two words are adjacent to each other. The model can then predict an arc between them, initiated by either word. 2.2 Dynamic Training Oracle Goldberg and Nivre (2013) describe three questions that need to be answered in order to implement their training algorithm. Exploration Policy: When do we follow an incorrect transition, and which one do we follow? We always follow the predicted transition, i.e. their two hyper-parameters are set k = 1 and p = 1.0. Optimality: What constitutes an optimal transition in configurations from which the gold tree is not reachable? We follow Honnibal et al. (2013) in defining a transition as optimal if it: 1. Renders no additional arcs unreachable using the monotonic arc-eager transitions; and 2. Renders no additional arcs u</context>
<context position="12497" citStr="Goldberg and Nivre (2013)" startWordPosition="2089" endWordPosition="2092">n the input string I saw Jack, consider a configuration where saw is on the stack, Jack is at the front of the buffer, and I is attached to saw. The gold arcs are saw —* I and saw —* Jack. In the monotonic system, the Shift action would make the gold arc saw —* Jack newly unreachable. In our system, this arc is still reachable after Shift, via the Unshift action, but we consider the Shift move non-optimal, so that the non-monotonic actions are reserved as ”repair” operations. Oracle: Given a definition of optimality, how do we calculate the set of optimal transitions in a given configuration? Goldberg and Nivre (2013) show that with the monotonic arc-eager actions, the following arcs are reachable from an arbitrary configuration: 1. Arcs {wi —* wj : i E σ, j E β1 — i.e. all arcs from stack words to buffer words; 2. Arcs {wi —* wj : i E β, j E σ, A(j) = 01 — i.e. all arcs from buffer words to headless stack words; 3. Arcs {wi —* wj : i E β, j E β1 — i.e. all arcs between words in buffer. Our non-monotonic actions additionally allow the following arcs to be reached: 4. Arcs {wi —* wj : i E β, j E σ,A(j) =�01 (LeftArc can now ”clobber” existing heads) 5. Arcs {wi —* wj or wj —* wi : i, j E σ, i &lt; j,A(j) = 01 </context>
</contexts>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. TACL, 1:403–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>Yoav Goldberg</author>
<author>Mark Johnson</author>
</authors>
<title>A non-monotonic arc-eager transition system for dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>163--172</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="3367" citStr="Honnibal et al. (2013)" startWordPosition="520" endWordPosition="523">rsing transitions to a greedy arc-eager dependency parser in this paper, in order to permit the parser to recover from attachment errors made early in the parsing process. These additional non-monotonic transitions permit the parser to modify what would have been irrevocable parsing decisions in the monotonic arc-eager system when later information justifies this action. Thus one effect of adding the non-monotonic parsing transitions is to effectively delay the location in the input where the parser must ultimately commit to a particular attachment. Our transition-system builds on the work of Honnibal et al. (2013) and Nivre and FernandezGonzalez (2014), who each present modifications 1373 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373–1378, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. to the arc-eager transition system that introduce some non-monotonic behaviour, resulting in small improvements in accuracy. However, these systems only apply non-monotonic transitions to a relatively small number of configurations, so they can only have a small impact on parse accuracy. We introduce a non-monotonic transition</context>
<context position="6526" citStr="Honnibal et al. (2013)" startWordPosition="1007" endWordPosition="1010">er is exhausted and the word on top of the stack has no incoming arc. In this configuration, the Unshift action is the only action that can be applied. The use of the new action is therefore entirely deterministic, and they do not need to produce example configurations for the Unshift action during training. They train their model with what Goldberg and Nivre (2012) term a ‘static oracle’, which can only label configurations that are consistent with the gold-standard parse. We take the Nivre and Fernandez-Gonzalez (2014) Unshift operation, and import it into the non-monotonic parsing model of Honnibal et al. (2013), which uses a dynamic oracle to determine the gold-standard actions for configurations produced by the parser. This training strategy is critical to the success of a non-monotonic transition system. The model cannot learn to recover from previous errors if the training data cannot contain configurations that result from incorrect actions. Honnibal et al. (2013) allow the parser to correct prior misclassifications between the Shift and Right-Arc actions. Both of these actions push the first word of the buffer onto the stack, but the Right-Arc action also adds an arc. After the RightArc is appl</context>
<context position="9101" citStr="Honnibal et al. (2013)" startWordPosition="1472" endWordPosition="1475">uffer of word indices with first element b A is a vector of head indices A(i) = j denotes an arc wj → wz S is a bit-vector used to prevent Shift/Unshift cycles Initial ([ ], [1...n], A(1) = 1) Terminal ([i], [ ], A) Shift (σ, b|β, A, S(b) = 0) ⇒ (σ|b,β, A, S(b) = 1) Right-Arc (σ|s, b|β, A, S) ⇒ (σ|s|b, β, A(b) = s, S) Reduce (σ|s, β, A(s) =6 0, S) ⇒ (σ, β, A, S) Unshift (σ|s, β, A(s) = 0, S) ⇒ (σ, s|β, A, S) Left-Arc (σ|s, b|β, A, S) ⇒ (σ, s|β, A(s) = b, S Table 1: Our non-monotonic transition system, which integrates the Unshift action of Nivre and Fernandez-Gonzalez (2014) into the model of Honnibal et al. (2013). Honnibal et al. use in their system, and that the resulting transition system improves parse accuracy by considerably more than either the Honnibal et al or Nivre et al systems do. 2.1 Definition of Transition System The hybrid transition system is defined in Table 1. Arcs are stored in a vector, A, where the entry A(i) = j denotes an arc wj → wz. Words are pushed from the buffer β onto the stack σ, using either the Shift or the Right-Arc actions. If a word was pushed with the Shift action, it will not have an incoming arc. The new Unshift action will then be valid, at any point at which the</context>
<context position="11457" citStr="Honnibal et al. (2013)" startWordPosition="1908" endWordPosition="1911">guration where these two words are adjacent to each other. The model can then predict an arc between them, initiated by either word. 2.2 Dynamic Training Oracle Goldberg and Nivre (2013) describe three questions that need to be answered in order to implement their training algorithm. Exploration Policy: When do we follow an incorrect transition, and which one do we follow? We always follow the predicted transition, i.e. their two hyper-parameters are set k = 1 and p = 1.0. Optimality: What constitutes an optimal transition in configurations from which the gold tree is not reachable? We follow Honnibal et al. (2013) in defining a transition as optimal if it: 1. Renders no additional arcs unreachable using the monotonic arc-eager transitions; and 2. Renders no additional arcs unreachable using the non-monotonic transitions. Said another way, we mark a transition as optimal if it leads to an analysis with as few errors as possible, and in cases of ties, uses as few nonmonotonic transitions as possible. 1375 For example, given the input string I saw Jack, consider a configuration where saw is on the stack, Jack is at the front of the buffer, and I is attached to saw. The gold arcs are saw —* I and saw —* Ja</context>
<context position="14258" citStr="Honnibal et al. (2013)" startWordPosition="2463" endWordPosition="2466">berg and Nivre (2013) prove for the arc eager system: if two arcs of a projective tree are individually reachable from a configuration, a projective tree that includes both arcs is also reachable. To see that this property is preserved in our system, consider that an arc h —* d between two stack words is only unreachable if h &lt; d and A(d) =� 0. But a projective tree with arc h —* d cannot also have an arc x —* y such that h &lt; x &lt; d &lt; y. So there can be no other arc part of the same projective tree as h —* d that would require d to be assigned to some other head. 3 Training Procedure We follow Honnibal et al. (2013) in using the dynamic oracle-based search-and-learn training strategy introduced by Goldberg and Nivre (2012). A dynamic oracle is a function that labels configurations with gold-standard actions. Importantly, a dynamic oracle can label arbitrary configurations, while a so-called ‘static’ oracle can only assign labels to configurations that are part of goldstandard derivations. We employ the dynamic oracle in an online learning strategy, similar to imitation-based learning, where the examples are configurations produced by following the current model’s predictions. The configurations are label</context>
<context position="17806" citStr="Honnibal et al., 2013" startWordPosition="3032" endWordPosition="3035">o dependencies using the ClearNLP 3.1 converter, with the train / dev / test split of the CoNLL 2012 shared task. 5 Results We implemented three previous versions of the arc-eager transition system, in order to evaluate the effect of our proposed transition-system on parser accuracy. The four systems differ only in their transition system — they are otherwise identical. All use identical features, and all are trained with the dynamic oracle. Orig. Arc Eager (Nivre, 2003): the original arc-eager system, which constrains the Reduce and Left-Arc actions to ensure monotonicity; Prev. Non-Monotic (Honnibal et al., 2013): relaxes the monotonicity constraints, allowing Left-Arc to ”clobber” existing arcs, and inserting missing arcs on Reduce with a simple heuristic; Tree Constrained (Nivre and FernandezGonzalez, 2014): adds an Unshift action to the arc-eager system, that is only employed when the buffer is exhausted; This work: merges the Unshift action into our previous non-monotonic transition system. Transition System Search UAS LAS Orig. Arc Eager Greedy 91.25 89.40 Tree Constrained Greedy 91.40 89.50 Prev. Non-Monotonic Greedy 91.36 89.52 This work Greedy 91.85 89.91 Chen and Manning (2014) Greedy 89.59 8</context>
<context position="19396" citStr="Honnibal et al. (2013)" startWordPosition="3280" endWordPosition="3283">ent scores of the parsers on the evaluation data. The two previous non-monotonic systems, Prev. Non-monotonic and Tree Constrained, were slightly more accurate than the Orig. Arc Eager system. Our new transition-system had a much bigger impact, improving UAS by 0.6% and LAS by 0.51%. To put the scores in context, we have also included figures reported in a recent survey of the current state-of-the-art (Tetreault et al., 2015). Our parser out-performs existing greedy parsers, and is much more efficient than nongreedy parsers. 6 Conclusions and Future Work This paper integrates innovations from Honnibal et al. (2013) and Nivre and Fernandez-Gonzalez (2014) to produce a novel non-monotonic set of transitions for transition-based dependency parsing. Doing this required us to use the dynamic oracle of Goldberg and Nivre (2012) during training in order to produce configurations that exercise the non-monotonic transitions. We show that this combination of innovations results in a parser with 91.85% directed accuracy, which is an improvement of 0.6% directed accuracy over an equivalent arc-standard parser. Interestingly, the Honnibal et al and Nivre et al innovations applied on their own only produce improvemen</context>
</contexts>
<marker>Honnibal, Goldberg, Johnson, 2013</marker>
<rawString>Matthew Honnibal, Yoav Goldberg, and Mark Johnson. 2013. A non-monotonic arc-eager transition system for dependency parsing. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 163–172. Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--11</pages>
<contexts>
<context position="16287" citStr="Koo and Collins (2010)" startWordPosition="2784" endWordPosition="2787">le class, which we interpret based on the configuration. This allows us to learn the Unshift action more effectively, as it is allowed to share a representation with the Reduce move. In preliminary development, we found that assigning a distinct class to the Unshift action was not effective. We plan to evaluate this option more rigorously in future work. 1376 4 Experiments We implemented a greedy transition-based parser, and used rich contextual features following Zhang and Nivre (2011). We extended the feature set to include Brown cluster features, using the cluster prefix trick described by Koo and Collins (2010). Brown clusters are a standard way to improve the cross-domain performance of supervised linear models. The use of Brown cluster features accounts for the 0.7% improvement in accuracy observed between our baseline parser and the Goldberg and Nivre (2012) result shown in Table 2. The two models are otherwise the same. Part-of-speech tags were predicted using a greedy averaged perceptron model that achieved 97.2% accuracy on the evaluation data. Most previous work uses a n-way jack-knifing to train the stacked tagger/parser model. For convenience, we instead train the tagger at the same time as</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient third-order dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1– 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="5473" citStr="Nivre, 2003" startWordPosition="839" endWordPosition="840">edicts which transition to apply next, while this paper suggests changes to the transition system itself, it is reasonable to expect that the improvements reported here are largely orthogonal to those obtained using the neural network techniques. In future work we would like to explore integrating such neural network models of transition prediction with the extended transition system proposed here. 2 Improved non-monotonic transition system Our transition-system is based on the treeconstrained arc-eager system of Nivre and Fernandez-Gonzalez (2014), which extends the classic arc-eager system (Nivre, 2003) with a new non-monotonic operation that they call “Unshift”. They introduce the Unshift action to repair configurations where the buffer is exhausted and the stack contains multiple words that are without incoming arcs (i.e. without governors). The original arc-eager configuration outputs partial parses in this situation. Nivre and Fernandez-Gonzalez restrict their Unshift action, such that it can only be applied when the buffer is exhausted and the word on top of the stack has no incoming arc. In this configuration, the Unshift action is the only action that can be applied. The use of the ne</context>
<context position="17659" citStr="Nivre, 2003" startWordPosition="3011" endWordPosition="3012">lt et al. (2015) in their recent ‘bake-off’ of leading dependency parsing models. Specifically, we use the OntoNotes corpus converted into dependencies using the ClearNLP 3.1 converter, with the train / dev / test split of the CoNLL 2012 shared task. 5 Results We implemented three previous versions of the arc-eager transition system, in order to evaluate the effect of our proposed transition-system on parser accuracy. The four systems differ only in their transition system — they are otherwise identical. All use identical features, and all are trained with the dynamic oracle. Orig. Arc Eager (Nivre, 2003): the original arc-eager system, which constrains the Reduce and Left-Arc actions to ensure monotonicity; Prev. Non-Monotic (Honnibal et al., 2013): relaxes the monotonicity constraints, allowing Left-Arc to ”clobber” existing arcs, and inserting missing arcs on Reduce with a simple heuristic; Tree Constrained (Nivre and FernandezGonzalez, 2014): adds an Unshift action to the arc-eager system, that is only employed when the buffer is exhausted; This work: merges the Unshift action into our previous non-monotonic transition system. Transition System Search UAS LAS Orig. Arc Eager Greedy 91.25 8</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Daniel Fernandez-Gonzalez</author>
</authors>
<title>Arc-eager parsing with the tree constraint.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>2</issue>
<pages>267</pages>
<contexts>
<context position="5415" citStr="Nivre and Fernandez-Gonzalez (2014)" startWordPosition="829" endWordPosition="832">eam search. Because the neural network approaches improve the local model that predicts which transition to apply next, while this paper suggests changes to the transition system itself, it is reasonable to expect that the improvements reported here are largely orthogonal to those obtained using the neural network techniques. In future work we would like to explore integrating such neural network models of transition prediction with the extended transition system proposed here. 2 Improved non-monotonic transition system Our transition-system is based on the treeconstrained arc-eager system of Nivre and Fernandez-Gonzalez (2014), which extends the classic arc-eager system (Nivre, 2003) with a new non-monotonic operation that they call “Unshift”. They introduce the Unshift action to repair configurations where the buffer is exhausted and the stack contains multiple words that are without incoming arcs (i.e. without governors). The original arc-eager configuration outputs partial parses in this situation. Nivre and Fernandez-Gonzalez restrict their Unshift action, such that it can only be applied when the buffer is exhausted and the word on top of the stack has no incoming arc. In this configuration, the Unshift action</context>
<context position="9060" citStr="Nivre and Fernandez-Gonzalez (2014)" startWordPosition="1464" endWordPosition="1467">tack of word indices with topmost element s b|β is a buffer of word indices with first element b A is a vector of head indices A(i) = j denotes an arc wj → wz S is a bit-vector used to prevent Shift/Unshift cycles Initial ([ ], [1...n], A(1) = 1) Terminal ([i], [ ], A) Shift (σ, b|β, A, S(b) = 0) ⇒ (σ|b,β, A, S(b) = 1) Right-Arc (σ|s, b|β, A, S) ⇒ (σ|s|b, β, A(b) = s, S) Reduce (σ|s, β, A(s) =6 0, S) ⇒ (σ, β, A, S) Unshift (σ|s, β, A(s) = 0, S) ⇒ (σ, s|β, A, S) Left-Arc (σ|s, b|β, A, S) ⇒ (σ, s|β, A(s) = b, S Table 1: Our non-monotonic transition system, which integrates the Unshift action of Nivre and Fernandez-Gonzalez (2014) into the model of Honnibal et al. (2013). Honnibal et al. use in their system, and that the resulting transition system improves parse accuracy by considerably more than either the Honnibal et al or Nivre et al systems do. 2.1 Definition of Transition System The hybrid transition system is defined in Table 1. Arcs are stored in a vector, A, where the entry A(i) = j denotes an arc wj → wz. Words are pushed from the buffer β onto the stack σ, using either the Shift or the Right-Arc actions. If a word was pushed with the Shift action, it will not have an incoming arc. The new Unshift action will</context>
<context position="19436" citStr="Nivre and Fernandez-Gonzalez (2014)" startWordPosition="3285" endWordPosition="3288">n the evaluation data. The two previous non-monotonic systems, Prev. Non-monotonic and Tree Constrained, were slightly more accurate than the Orig. Arc Eager system. Our new transition-system had a much bigger impact, improving UAS by 0.6% and LAS by 0.51%. To put the scores in context, we have also included figures reported in a recent survey of the current state-of-the-art (Tetreault et al., 2015). Our parser out-performs existing greedy parsers, and is much more efficient than nongreedy parsers. 6 Conclusions and Future Work This paper integrates innovations from Honnibal et al. (2013) and Nivre and Fernandez-Gonzalez (2014) to produce a novel non-monotonic set of transitions for transition-based dependency parsing. Doing this required us to use the dynamic oracle of Goldberg and Nivre (2012) during training in order to produce configurations that exercise the non-monotonic transitions. We show that this combination of innovations results in a parser with 91.85% directed accuracy, which is an improvement of 0.6% directed accuracy over an equivalent arc-standard parser. Interestingly, the Honnibal et al and Nivre et al innovations applied on their own only produce improvements of 0.11% and 0.15% respectively, so i</context>
</contexts>
<marker>Nivre, Fernandez-Gonzalez, 2014</marker>
<rawString>Joakim Nivre and Daniel Fernandez-Gonzalez. 2014. Arc-eager parsing with the tree constraint. Computational Linguistics, 40(2):259– 267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Jin ho Choi</author>
<author>Amanda Stent</author>
</authors>
<title>It depends: Dependency parser comparison using a web-based evaluation tool.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>13--24</pages>
<location>Beijing, China.</location>
<contexts>
<context position="17063" citStr="Tetreault et al. (2015)" startWordPosition="2914" endWordPosition="2917">he 0.7% improvement in accuracy observed between our baseline parser and the Goldberg and Nivre (2012) result shown in Table 2. The two models are otherwise the same. Part-of-speech tags were predicted using a greedy averaged perceptron model that achieved 97.2% accuracy on the evaluation data. Most previous work uses a n-way jack-knifing to train the stacked tagger/parser model. For convenience, we instead train the tagger at the same time as the parser, as both allow online learning. We find this makes no difference to accuracy. Our parsers are trained and evaluated on the same data used by Tetreault et al. (2015) in their recent ‘bake-off’ of leading dependency parsing models. Specifically, we use the OntoNotes corpus converted into dependencies using the ClearNLP 3.1 converter, with the train / dev / test split of the CoNLL 2012 shared task. 5 Results We implemented three previous versions of the arc-eager transition system, in order to evaluate the effect of our proposed transition-system on parser accuracy. The four systems differ only in their transition system — they are otherwise identical. All use identical features, and all are trained with the dynamic oracle. Orig. Arc Eager (Nivre, 2003): th</context>
<context position="19203" citStr="Tetreault et al., 2015" startWordPosition="3251" endWordPosition="3254">: Our non-monotonic transition system improves accuracy by 0.6% unlabelled attachment score, for a final score of 91.85 on the OntoNotes corpus. Table 2 shows the unlabelled and labelled attachment scores of the parsers on the evaluation data. The two previous non-monotonic systems, Prev. Non-monotonic and Tree Constrained, were slightly more accurate than the Orig. Arc Eager system. Our new transition-system had a much bigger impact, improving UAS by 0.6% and LAS by 0.51%. To put the scores in context, we have also included figures reported in a recent survey of the current state-of-the-art (Tetreault et al., 2015). Our parser out-performs existing greedy parsers, and is much more efficient than nongreedy parsers. 6 Conclusions and Future Work This paper integrates innovations from Honnibal et al. (2013) and Nivre and Fernandez-Gonzalez (2014) to produce a novel non-monotonic set of transitions for transition-based dependency parsing. Doing this required us to use the dynamic oracle of Goldberg and Nivre (2012) during training in order to produce configurations that exercise the non-monotonic transitions. We show that this combination of innovations results in a parser with 91.85% directed accuracy, whi</context>
</contexts>
<marker>Tetreault, Choi, Stent, 2015</marker>
<rawString>Joel Tetreault, Jin ho Choi, and Amanda Stent. 2015. It depends: Dependency parser comparison using a web-based evaluation tool. In Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13–24. Association for Computational Linguistics, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Weiss</author>
<author>Chris Alberti</author>
<author>Michael Collins</author>
<author>Slav Petrov</author>
</authors>
<title>Structured training for neural network transition-based parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>13--24</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1339" citStr="Weiss et al. (2015)" startWordPosition="200" endWordPosition="203">derive, enabling such a parser to escape the “garden paths” that can trap monotonic greedy transition-based dependency parsers. We describe a new set of non-monotonic transitions that permits a partial parse state to derive a larger set of completed parse trees than previous work, which allows our parser to escape from a larger set of garden paths. A parser with our new nonmonotonic transition system has 91.85% directed attachment accuracy, an improvement of 0.6% over a comparable parser using the standard monotonic arc-eager transitions. 1 Introduction Recent work from Dyer et al. (2015) and Weiss et al. (2015) show that neural network models can improve greedy transition-based parsers dramatically, even beyond the 20% error reduction reported by Chen and Manning (2014). Improvements on beam-search parsing are much more limited, due to the difficulty of applying neural networks to structured prediction. We suggest that the lack of a ready search solution may present the next barrier to further improvements in accuracy. Some degree of search flexibility seems inherently necessary, no matter how powerful the local model becomes, as even the human sentence processor can be ‘garden pathed’ by local stru</context>
<context position="4640" citStr="Weiss et al. (2015)" startWordPosition="713" endWordPosition="716"> and allows substantially more repair capability (and hence search flexibility). We observe a 0.6% improvement in accuracy on the OntoNotes corpus, which is an error reduction of 6.25% over a competitive baseline. A parser using our transition system is guaranteed to run in linear time, and the modifications to the algorithm have no negative impact on run-time in our implementation. Very recently there has been considerable success in applying neural network models to predict which transition to apply in greedy one-best transition-based parsing. In their preprints, both Dyer et al. (2015) and Weiss et al. (2015) report error reductions of around 20-30% for greedy one-best parsing, and much more modest improvements for transition-based parsers with beam search. Because the neural network approaches improve the local model that predicts which transition to apply next, while this paper suggests changes to the transition system itself, it is reasonable to expect that the improvements reported here are largely orthogonal to those obtained using the neural network techniques. In future work we would like to explore integrating such neural network models of transition prediction with the extended transition</context>
</contexts>
<marker>Weiss, Alberti, Collins, Petrov, 2015</marker>
<rawString>David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. 2015. Structured training for neural network transition-based parsing. In Proceedings of the 53nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13–24. Association for Computational Linguistics, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transitionbased dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="16156" citStr="Zhang and Nivre (2011)" startWordPosition="2763" endWordPosition="2766">valid; otherwise, the Unshift move is valid. For the purpose of training and prediction, we therefore model these actions as a single class, which we interpret based on the configuration. This allows us to learn the Unshift action more effectively, as it is allowed to share a representation with the Reduce move. In preliminary development, we found that assigning a distinct class to the Unshift action was not effective. We plan to evaluate this option more rigorously in future work. 1376 4 Experiments We implemented a greedy transition-based parser, and used rich contextual features following Zhang and Nivre (2011). We extended the feature set to include Brown cluster features, using the cluster prefix trick described by Koo and Collins (2010). Brown clusters are a standard way to improve the cross-domain performance of supervised linear models. The use of Brown cluster features accounts for the 0.7% improvement in accuracy observed between our baseline parser and the Goldberg and Nivre (2012) result shown in Table 2. The two models are otherwise the same. Part-of-speech tags were predicted using a greedy averaged perceptron model that achieved 97.2% accuracy on the evaluation data. Most previous work u</context>
<context position="18522" citStr="Zhang and Nivre (2011)" startWordPosition="3143" endWordPosition="3146">rting missing arcs on Reduce with a simple heuristic; Tree Constrained (Nivre and FernandezGonzalez, 2014): adds an Unshift action to the arc-eager system, that is only employed when the buffer is exhausted; This work: merges the Unshift action into our previous non-monotonic transition system. Transition System Search UAS LAS Orig. Arc Eager Greedy 91.25 89.40 Tree Constrained Greedy 91.40 89.50 Prev. Non-Monotonic Greedy 91.36 89.52 This work Greedy 91.85 89.91 Chen and Manning (2014) Greedy 89.59 87.63 Goldberg and Nivre (2012) Greedy 90.54 88.75 Choi and Mccallum (2013) Branch 92.26 90.84 Zhang and Nivre (2011) Beam32 92.24 90.50 Bohnet(2010) Graph 92.50 90.70 Table 2: Our non-monotonic transition system improves accuracy by 0.6% unlabelled attachment score, for a final score of 91.85 on the OntoNotes corpus. Table 2 shows the unlabelled and labelled attachment scores of the parsers on the evaluation data. The two previous non-monotonic systems, Prev. Non-monotonic and Tree Constrained, were slightly more accurate than the Orig. Arc Eager system. Our new transition-system had a much bigger impact, improving UAS by 0.6% and LAS by 0.51%. To put the scores in context, we have also included figures rep</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transitionbased dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193. Association for Computational Linguistics, Portland, Oregon, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
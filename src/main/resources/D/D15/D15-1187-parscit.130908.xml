<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000769">
<title confidence="0.9953635">
Unsupervised Negation Focus Identification with Word-Topic Graph
Model
</title>
<author confidence="0.884155">
Bowei Zou Qiaoming Zhu Guodong Zhou*
</author>
<affiliation confidence="0.848522">
Natural Language Processing Lab, School of Computer Science and Technology
</affiliation>
<address confidence="0.898083">
Soochow University, Suzhou, 215006, China
</address>
<email confidence="0.998442">
zoubowei@gmail.com, {qmzhu, gdzhou}@suda.edu.cn
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999460058823529">
Due to the commonality in natural language,
negation focus plays a critical role in deep
understanding of context. However, existing
studies for negation focus identification ma-
jor on supervised learning which is time-
consuming and expensive due to manual
preparation of annotated corpus. To address
this problem, we propose an unsupervised
word-topic graph model to represent and
measure the focus candidates from both lexi-
cal and topic perspectives. Moreover, we
propose a document-sensitive biased Pag-
eRank algorithm to optimize the ranking
scores of focus candidates. Evaluation on the
*SEM 2012 shared task corpus shows that
our proposed method outperforms the state of
the art on negation focus identification.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9989739">
Negation is used to reverse the polarity of part of
statements that are otherwise affirmative by de-
fault (Blanco and Moldovan, 2011), which is
common in natural language. Negation focus is
defined as the special part in sentence, which is
most prominently or explicitly negated by a neg-
ative expression. For example, sentence (1) could
be interpreted as He stopped, but not until he got
to Jackson Hole with a positive part he stopped
and a negative part until he got to Jackson Hole.
</bodyText>
<listItem confidence="0.800643">
(1) He didn&apos;t stop until he got to Jackson Hole.
</listItem>
<bodyText confidence="0.9996768">
Our previous work (Zou et al., 2014) showed
that contextual information plays a critical role
on negation focus identification. For better illus-
tration of this conclusion, they manually analyze
the evidences for 100 negation focuses. It is sur-
</bodyText>
<note confidence="0.47606">
* Corresponding author
</note>
<bodyText confidence="0.999951102564103">
prising that 77 focuses can be identified with
help of contextual information. This indicates
that negation focus is often related with what
authors repeatedly states in context. In this paper,
we thus focus on graph-based ranking methods
(Mihalcea and Tarau, 2004) which first build a
word graph according to word co-occurrences
within document, and then use random walk al-
gorithms (e.g., PageRank) to measure word im-
portance.
However, for negation focus identification, the
graph-based methods may suffer from the fol-
lowing two problems: (a) the words in graph-
based methods are strongly connected by co-
occurrence rather than semantic content, which
do not necessarily guarantee that they are rele-
vant to the negation focus in context; and (b)
identifying a negation focus may be affected by
not only the relatedness of surrounding words
but also its importance in current document
which is not considered in standard random walk
algorithms.
To address the above problems, we propose a
word-topic graph model by adding a topical layer
on the original word layer to capture the seman-
tic clues from both lexical and topic perspectives.
Besides, a document-sensitive PageRank algo-
rithm is also proposed to optimize the graph
model by considering the document’s major top-
ics. Experimental results indicate that our word-
topic graph model outperforms other baseline
methods. Moreover, our model is unsupervised
and requires only un-annotated text for training.
The rest of this paper is organized as follows.
Section 2 overviews the related work. Section 3
introduces our word-topic graph model with con-
textual discourse information. Section 4 reports
the experimental results and analysis. Finally, we
conclude our work in Section 5.
</bodyText>
<page confidence="0.944361">
1632
</page>
<note confidence="0.854968">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1632–1636,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<sectionHeader confidence="0.998924" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999985266666667">
So far there is little work on negation focus iden-
tification, which was pioneered by Blanco and
Moldovan (2011) who investigated the negation
phenomenon in semantic relations and proposed
a supervised learning approach to identify the
focus of a negation expression. However, alt-
hough Morante and Blanco (2012) proposed ne-
gation focus identification as one of the
*SEM’2012 shared tasks, only one team (Rosen-
berg and Bergler, 2012) participated in. They
identified negation focus by three heuristic rules.
Our previous work (Zou et al., 2014) demon-
strates the effectiveness of contextual infor-
mation for negation focus identification. On this
basis, we further optimize the graph model in
both the topical layer and the PageRank algo-
rithm in this paper.
In recent years, many algorithms are widely
used to incorporate word graph models and topi-
cal information within random walk. Our work is
originally inspired by Liu et al. (2010). Their
method runs decomposed Topical PageRank
(TPR) for each topic separately, and then calcu-
lates the word scores with respect to different
topics. When setting the edge weights, only word
co-occurrence is considered. Different from their
work, our word-topic graph model runs on a two-
layers (word layer and topical layer) graph model
and sets the edge weights by measuring both
word similarity and topic distribution.
</bodyText>
<sectionHeader confidence="0.998351" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.9998525">
The word-topic graph model consists of word
layer and topical layer, as shown in Figure 1.
While the word layer is constructed according to
word co-occurrence within a sliding window,
which expresses the cohesion relationship be-
tween words in the context, the topical layer is to
refine the graph model over the discourse con-
textual information.
</bodyText>
<figureCaption confidence="0.992873">
Figure 1. Word-topic graph model.
</figureCaption>
<subsectionHeader confidence="0.999774">
3.1 Constructing Word Layer
</subsectionHeader>
<bodyText confidence="0.999940235294118">
The word layer is constructed according to word
co-occurrence within a sliding window, which
expresses the cohesion relationship between
words in the context. It can be denoted as Lword
(W, Ew), where vertex set W={wi} represents the
set of words in one document and link set Ew
={eij|wi, wj∈W} represents the set of directed
edges between these words. Note that only con-
tent words are considered. Namely, we consider
nouns, verbs, adjectives, and adverbs.
The link directions are added from the first
word pointing to other words within a sliding s-
width sentence window. Directed edge ewij is
weighted to represent the relatedness between
word wi and word wj in a document with transi-
tion probability Pw(j|i) from i to j, which is nor-
malized as follows:
</bodyText>
<equation confidence="0.998886333333333">
(  |)
j i   ,
sim w w
( , )
i k
k:wi  wk
</equation>
<bodyText confidence="0.999684166666667">
where the denominator represents the out-degree
of vertex wi , and sim(wi,wj) denotes the similari-
ty between word wi and wj. In this paper, both
corpus-based and knowledge-based methods are
evaluated to calculate the similarity between
words.
</bodyText>
<listItem confidence="0.9944015">
• Word co-occurrence. If word wi and word wj
occur in a s-width sliding sentence window,
sim(wi,wj) increases 1.
• WordNet similarity (Miller, 1995). In this
</listItem>
<bodyText confidence="0.905311857142857">
paper, we adopt the path similarity function
to measure relatedness of nouns and verbs,
and adopt the similar to function to measure
relatedness of adjectives and adverbs by us-
ing the NLTK toolkit1 (Bird et al., 2009).
Note that sim(wi,wi) = 0 to avoid self-transition,
and sim(wi,wj) and sim(wj,wi) may not be equal.
</bodyText>
<subsectionHeader confidence="0.98668">
3.2 Preliminaries for Topical Layer
</subsectionHeader>
<bodyText confidence="0.9999708">
To infer the latent topic distributions of words,
Latent Dirichlet Allocation (LDA) (Blei et al.,
2003), a typical of topic model, is directly ap-
plied. By the set of topics which derive from a
corpus, we can obtain:
</bodyText>
<listItem confidence="0.962379166666667">
• P(t|w), the probability of topic t given word
w, which indicates how much that word w
focuses on topic t, and
• P(t|d), the probability of topic t given doc-
ument d, which indicates how much that
document d focuses on topic t.
</listItem>
<equation confidence="0.9390865">
1 http://www.nltk.org/
Pw
sim w w
( , )
i j
(1)
</equation>
<page confidence="0.805966">
1633
</page>
<bodyText confidence="0.9975245">
Then, the similarity between two words or be-
tween word wi and document d can be measured
by the similarity between their corresponding
topic distributions. Formally, we denote a topic
distribution as θ, and measure the similarity by
using:
</bodyText>
<listItem confidence="0.875633888888889">
• Dot-product. We consider the topic distribu-
tions as vectors and apply the dot-product, a
geometrically motivated function, to calcu-
late the similarity:
Inner( ,. ,w)  ,,  wj  tkT P(tk  |wi) P(tk  |wj ), (2)
(3)
• Kullback Leibler (KL) divergence (Lin,
1991). Considering the asymmetry (Eq.(4)),
we obtain a symmetrized measure by Eq.(5).
</listItem>
<equation confidence="0.985541272727273">
D ( , )
   i k
i j t T i k 2
k  P t
( )log .
P t
P t
j k
( )
( ) (4)
DKL(i,j)D(i,j)D(j,i). (5)
</equation>
<bodyText confidence="0.9974812">
Note that DKL(θi, θj) is undefined if Pi(tk)=0
or Pj(tk)=0 for any tk ET. For this reason,
only the topics which make both Pi(tk)≠0
and Pj(tk)≠0 are adopted to calculate the KL
divergence between two topic distributions.
</bodyText>
<subsectionHeader confidence="0.997009">
3.3 Word-Topic Graph Model
</subsectionHeader>
<bodyText confidence="0.9974451">
The word layer can well capture the relatedness
between words, but just partially model the nega-
tion focus since it is more directly related with
topic than content. Therefore, we add one more
layer to refine the graph model over the topical
information, as shown in Figure 1. Formally, the
word-topic graph is defined as Gtopic(W, T, Ew, Et),
where vertex set T={ti} represents the set of top-
ics in all of documents in corpus and link set Et
={etij|wiEW, tj ET} represents the set of undi-
rected edges between words and topics.
Considering that the topical layer can provide
more contextual semantic information, we refine
the relatedness between words by using a topical
transition probability Pt(j|i) which is calculated
by two kinds of measurements:
Here, the similarity is measured by the dot-
product or the KL divergence (using reciprocals).
On this basis, the word transition probability
Pw(j|i) is updated as following:
</bodyText>
<equation confidence="0.970661">
Pw(j|i)Pw(j|i)(1 Pt i) (7)
</equation>
<bodyText confidence="0.998847583333333">
where µE[0,1] is the coefficient controlling the
relative contributions from the lexical infor-
mation and the topical information.
Moreover, the weights of word vertices are
calculated by a PageRank algorithm. In standard
PageRank (Page et al., 1998), words are set to be
the same value, which indicates there is equal
importance to all of words in a document. How-
ever, intuitively, we should allocate higher
weights to those words with high relevance to the
document. Therefore, we assign a document-
sensitive value to word wi:
</bodyText>
<figure confidence="0.587316222222222">
R w sim ( , ) (8)
( )   
d i w d
i
 sim ( , )
 
w W w d
k
k 
</figure>
<bodyText confidence="0.850528">
and calculate the weights of word vertices itera-
tively by using a biased PageRank algorithm:
</bodyText>
<equation confidence="0.7629555">
Score (n 1)(wi) ji Score (n)(wj)P (j  |i)
(1)Rd(wi). (9)
</equation>
<bodyText confidence="0.999144909090909">
All of the PageRank algorithms are terminated
when the number of iterations reaches 100 or the
difference of each vertex between consecutive
iterations is less than 0.001.
Finally, according to the annotation guidelines
(Blanco and Moldovan, 2011), the focus is al-
ways a full text of a semantic role. Thus, we se-
lect all of semantic roles in sentence as candidate
focuses for ranking. The ranking score of a can-
didate focus f is computed by averaging the
scores of all words inside the candidate:
</bodyText>
<equation confidence="0.977077">
(10)
Count(f, , )
</equation>
<bodyText confidence="0.999959">
where count(f,•) denotes the number of content
words within the candidate. Then the top ranked
candidate is chosen as the negation focus.
</bodyText>
<sectionHeader confidence="0.997245" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999975153846154">
To evaluate the performance of our word-topic
graph model for negation focus identification, we
carry out experiments on the *SEM&apos;2012 shared
task corpus2. As a freely downloadable resource,
the corpus is annotated on top of PropBank,
which uses the WSJ section of the Penn Tree-
Bank. In particular, negation focus annotation on
this corpus is restricted to verbal negations
(Blanco and Moldovan, 2011). In total, this cor-
pus provides 3,544 instances of negation focus
annotations. Although for each instance, the cor-
pus only provides the current sentence, the pre-
vious and next sentences as its context, we sort to
</bodyText>
<figure confidence="0.759004736842105">
(wi , w ) j
(  |)
j i   .
sim ( , )
 
w w
i k
k w w
: i  k
Pt
sim
(6)
(,.,d)wid tkTP(tk  |wi)P(tk  |d).
Inner
scoreavg (f) 
wi f

score(w)
i
</figure>
<footnote confidence="0.944281">
2 http://www.clips.ua.ac.be/sem2012-st-neg/
</footnote>
<page confidence="0.993722">
1634
</page>
<bodyText confidence="0.999882866666667">
the Penn TreeBank3 to obtain the corresponding
document as its discourse context. For fair com-
parison, we adopt the same partition as
*SEM’2012 shared task in our experiments.
We evaluate our results in terms of accuracy.
To see whether an improvement is statistically
significant, we conduct significance testing using
the paired t-test.
For estimating the topical transition probabil-
ity Pt(j|i) and the document-sensitive value Rd(wi),
we employ GibbsLDA++4, an LDA model using
Gibbs Sampling technique for parameter estima-
tion and inference (Griffiths, 2002). We set the
parameters α = 50/T and fl = 0.1 as Griffiths and
Steyvers (2004) suggested.
</bodyText>
<subsectionHeader confidence="0.99851">
4.1 Influences of Parameters
</subsectionHeader>
<bodyText confidence="0.99975195">
There are two major parameters in our models
that may influence the performance, including: (a)
the damping factor µ of the word transition prob-
ability P’ w(j|i) (Eq.(7)) and (b) the damping fac-
tor A of the word-topic graph model (Eq.(9)).
Figure 2 shows the accuracy when varying µ
from 0.1 to 0.9 with an interval of 0.1 and when
varying A from 0.05 to 1 with an interval of 0.05.
We notice that the best performance is achieved
when µ=0.6. It indicates that the direct lexical
information contributes slightly more than the
topical information. The results also show the
complementarity between these two kinds of in-
formation on negation focus identification.
For A, it has very little, if any, effect on per-
formance, when A is set from 0.5 to 0.85. It indi-
cates that the contextual information (the first
term in Eq.(9)) contributes more than the docu-
ment information (the second term in Eq.(9)) on
negation focus identification.
</bodyText>
<figureCaption confidence="0.996783">
Figure 2. Influence of the damping factors µ and A.
</figureCaption>
<bodyText confidence="0.99998725">
Moreover, the results also show that these two
parameters have little impact in a certain range
on performance (µ:0.4~0.6; A:0.5~0.85), which
suggests that the approach is robust to a certain
</bodyText>
<footnote confidence="0.99762">
3 http://www.cis.upenn.edu/~treebank/
4 http://gibbslda.sourceforge.net/
</footnote>
<bodyText confidence="0.999023363636364">
extent. Therefore, we set µ=0.6 and A=0.7 in the
following experiments.
Besides, we also evaluate the other minor pa-
rameters in our model. Due to space limit, we do
not report all of results here and set parameters to
the following values: setting window size s=1
(the previous and next sentences) and the number
of topic T=40, adopting the word co-occurrence
similarity to calculate the similarity between
words, and using dot-product to measure both
Pt(j|i) and Rd(wi).
</bodyText>
<subsectionHeader confidence="0.999745">
4.2 Comparison with Other Methods
</subsectionHeader>
<bodyText confidence="0.9987398">
In the word-topic graph models, two primary
improvements are proposed: (a) updating the
word transition probability Pw(j|i) by adding a
topical layer (“TL”), and (b) assigning a docu-
ment-sensitive value to word node (“DS”).
</bodyText>
<table confidence="0.98537525">
model Acc.
WLM 52.61
WTGM (TL) 65.74
WTGM(TL+DS) 69.39
</table>
<tableCaption confidence="0.999404">
Table 1. Performance of the word-topic graph
</tableCaption>
<bodyText confidence="0.964954823529412">
model.
Table 1 shows that the word-topic graph mod-
el (WTGM) is significantly better (+16.78%,
p&lt;0.01) than the graph model with only word
layer (WLM), which justifies the effectiveness of
the topical layer. In addition, the results also in-
dicate that the word-topic graph model not only
takes the topical information into account (“TL”),
but also considers the semantic relationship in
current document (“DS”).
We select two supervised baseline methods to
compare with our word-topic graph model. One
is a decision tree-based system described in
Blanco and Moldovan (2011), and the other one
is a SVM-based system which takes advantage of
both syntactic features and contextual features
(Zou et al., 2014).
</bodyText>
<table confidence="0.99010425">
system Acc.
B&amp;M(2011) 63.20
Zou et al.(2014) 67.14
Ours 69.39
</table>
<tableCaption confidence="0.999894">
Table 2. Comparison Results.
</tableCaption>
<bodyText confidence="0.981001625">
Table 2 shows that our word-topic graph
model performs significantly better than the two
others by 6.19% (p&lt;0.01) and 2.25% (p&lt;0.01),
respectively. The results support our viewpoint
that the topical information in context can help
to find the negation focus, and the word-topic
graph model we proposed is effective. Moreo-
ver, it is also worth noting that our method is
</bodyText>
<page confidence="0.968302">
1635
</page>
<bodyText confidence="0.9999394">
unsupervised, which does not need the prior
knowledge for training, while the other two su-
pervised baselines employ the golden features,
such as the POS tag, constituent tree, and de-
pendency tree.
</bodyText>
<sectionHeader confidence="0.997701" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999963789473684">
In this paper, we propose an unsupervised word-
topic graph model, which represents and
measures the word importance by using contex-
tual information from both lexical and topical
perspectives. And then, we propose a document-
sensitive biased PageRank algorithm to calculate
the ranking scores of negation focus candidates.
Experimental results show that our method
achieves better performance than other baselines
without any annotated data.
The main shortcoming of our method is that
not all of negation focus can be identified by the
context. As our statistics, at least 17% of samples
are hard to be determined by human beings when
ignoring the information in current sentence.
Therefore, in future work, we will focus on in-
vestigating an effective method to integrate the
local lexical/syntactic information and the global
contextual discourse information.
</bodyText>
<sectionHeader confidence="0.997522" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.983549571428571">
This research is supported by the National Natu-
ral Science Foundation of China, No.61272260,
No.61331011, No.61273320, and the Major Pro-
ject of College Natural Science Foundation of
Jiangsu Province, No.11KJA520003. The au-
thors would like to thank the anonymous review-
ers for their insightful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.990221" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.998602754716981">
Steven Bird, Edward Loper, and Ewan Klein. 2009.
Natural Language Processing with Python.
O’Reilly Media Inc.
Eduardo Blanco and Dan Moldovan. 2011. Semantic
Representation of Negation Using Focus Detection.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, pages
581-589, Portland, Oregon, June 19-24, 2011.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993-1022, January.
Tom Griffiths. 2002. Gibbs sampling in the generative
model of Latent Dirichlet Allocation. Tech. rep.,
Stanford University.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Sciences of the United States of Ameri-
ca, 101(Suppl. 1), 5228-5235.
Jianhua Lin. 1991. Divergence measures based on
Shannon entropy. IEEE Transactions on Infor-
mation Theory, 37(14), 145-151.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic Keyphrase Extrac-
tion via Topic Decomposition. In Proceedings of
the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 366-376,
MIT, Massachusetts, USA, 9-11 October 2010.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 404-411.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39-41.
Roser Morante and Eduardo Blanco. 2012. *SEM
2012 Shared Task: Resolving the Scope and Focus
of Negation. In Proceedings of the First Joint Con-
ference on Lexical and Computational Semantics,
pages 265-274, Montreal, Canada, June 7-8, 2012.
Lawrence Page, Sergey Brin, Rajeev Motwani, et al.
1998. The pagerank citation ranking: Bringing or-
der to the web. Technical report, Stanford Univer-
sity.
Sabine Rosenberg and Sabine Bergler. 2012. UCon-
cordia: CLaC Negation Focus Detection at *Sem
2012. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics, pages
294-300, Montreal, Canada, June 7-8, 2012.
Bowei Zou, Qiaoming Zhu, and Guodong Zhou. 2014.
Negation Focus Identification with Contextual
Discourse Information. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 522-530, Baltimore, Mar-
yland, USA, June 23-25 2014.
</reference>
<page confidence="0.992388">
1636
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.686913">
<title confidence="0.961492">Unsupervised Negation Focus Identification with Word-Topic Graph Model</title>
<author confidence="0.975086">Zou Qiaoming Zhu Guodong</author>
<affiliation confidence="0.991132">Natural Language Processing Lab, School of Computer Science and</affiliation>
<address confidence="0.770397">Soochow University, Suzhou, 215006, China</address>
<email confidence="0.988886">zoubowei@gmail.com,{qmzhu,gdzhou}@suda.edu.cn</email>
<abstract confidence="0.999045388888889">Due to the commonality in natural language, negation focus plays a critical role in deep understanding of context. However, existing studies for negation focus identification major on supervised learning which is timeconsuming and expensive due to manual preparation of annotated corpus. To address this problem, we propose an unsupervised word-topic graph model to represent and measure the focus candidates from both lexical and topic perspectives. Moreover, we propose a document-sensitive biased PageRank algorithm to optimize the ranking scores of focus candidates. Evaluation on the *SEM 2012 shared task corpus shows that our proposed method outperforms the state of the art on negation focus identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media Inc.</booktitle>
<contexts>
<context position="6946" citStr="Bird et al., 2009" startWordPosition="1100" endWordPosition="1103">where the denominator represents the out-degree of vertex wi , and sim(wi,wj) denotes the similarity between word wi and wj. In this paper, both corpus-based and knowledge-based methods are evaluated to calculate the similarity between words. • Word co-occurrence. If word wi and word wj occur in a s-width sliding sentence window, sim(wi,wj) increases 1. • WordNet similarity (Miller, 1995). In this paper, we adopt the path similarity function to measure relatedness of nouns and verbs, and adopt the similar to function to measure relatedness of adjectives and adverbs by using the NLTK toolkit1 (Bird et al., 2009). Note that sim(wi,wi) = 0 to avoid self-transition, and sim(wi,wj) and sim(wj,wi) may not be equal. 3.2 Preliminaries for Topical Layer To infer the latent topic distributions of words, Latent Dirichlet Allocation (LDA) (Blei et al., 2003), a typical of topic model, is directly applied. By the set of topics which derive from a corpus, we can obtain: • P(t|w), the probability of topic t given word w, which indicates how much that word w focuses on topic t, and • P(t|d), the probability of topic t given document d, which indicates how much that document d focuses on topic t. 1 http://www.nltk.o</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduardo Blanco</author>
<author>Dan Moldovan</author>
</authors>
<title>Semantic Representation of Negation Using Focus Detection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>581--589</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="1145" citStr="Blanco and Moldovan, 2011" startWordPosition="161" endWordPosition="164">ive due to manual preparation of annotated corpus. To address this problem, we propose an unsupervised word-topic graph model to represent and measure the focus candidates from both lexical and topic perspectives. Moreover, we propose a document-sensitive biased PageRank algorithm to optimize the ranking scores of focus candidates. Evaluation on the *SEM 2012 shared task corpus shows that our proposed method outperforms the state of the art on negation focus identification. 1 Introduction Negation is used to reverse the polarity of part of statements that are otherwise affirmative by default (Blanco and Moldovan, 2011), which is common in natural language. Negation focus is defined as the special part in sentence, which is most prominently or explicitly negated by a negative expression. For example, sentence (1) could be interpreted as He stopped, but not until he got to Jackson Hole with a positive part he stopped and a negative part until he got to Jackson Hole. (1) He didn&apos;t stop until he got to Jackson Hole. Our previous work (Zou et al., 2014) showed that contextual information plays a critical role on negation focus identification. For better illustration of this conclusion, they manually analyze the </context>
<context position="3869" citStr="Blanco and Moldovan (2011)" startWordPosition="592" endWordPosition="595">otated text for training. The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 introduces our word-topic graph model with contextual discourse information. Section 4 reports the experimental results and analysis. Finally, we conclude our work in Section 5. 1632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1632–1636, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work So far there is little work on negation focus identification, which was pioneered by Blanco and Moldovan (2011) who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer</context>
<context position="10480" citStr="Blanco and Moldovan, 2011" startWordPosition="1723" endWordPosition="1726">itively, we should allocate higher weights to those words with high relevance to the document. Therefore, we assign a documentsensitive value to word wi: R w sim ( , ) (8) ( )    d i w d i  sim ( , )   w W w d k k  and calculate the weights of word vertices iteratively by using a biased PageRank algorithm: Score (n 1)(wi) ji Score (n)(wj)P (j |i) (1)Rd(wi). (9) All of the PageRank algorithms are terminated when the number of iterations reaches 100 or the difference of each vertex between consecutive iterations is less than 0.001. Finally, according to the annotation guidelines (Blanco and Moldovan, 2011), the focus is always a full text of a semantic role. Thus, we select all of semantic roles in sentence as candidate focuses for ranking. The ranking score of a candidate focus f is computed by averaging the scores of all words inside the candidate: (10) Count(f, , ) where count(f,•) denotes the number of content words within the candidate. Then the top ranked candidate is chosen as the negation focus. 4 Experimental Results To evaluate the performance of our word-topic graph model for negation focus identification, we carry out experiments on the *SEM&apos;2012 shared task corpus2. As a freely do</context>
<context position="15065" citStr="Blanco and Moldovan (2011)" startWordPosition="2476" endWordPosition="2479">S) 69.39 Table 1. Performance of the word-topic graph model. Table 1 shows that the word-topic graph model (WTGM) is significantly better (+16.78%, p&lt;0.01) than the graph model with only word layer (WLM), which justifies the effectiveness of the topical layer. In addition, the results also indicate that the word-topic graph model not only takes the topical information into account (“TL”), but also considers the semantic relationship in current document (“DS”). We select two supervised baseline methods to compare with our word-topic graph model. One is a decision tree-based system described in Blanco and Moldovan (2011), and the other one is a SVM-based system which takes advantage of both syntactic features and contextual features (Zou et al., 2014). system Acc. B&amp;M(2011) 63.20 Zou et al.(2014) 67.14 Ours 69.39 Table 2. Comparison Results. Table 2 shows that our word-topic graph model performs significantly better than the two others by 6.19% (p&lt;0.01) and 2.25% (p&lt;0.01), respectively. The results support our viewpoint that the topical information in context can help to find the negation focus, and the word-topic graph model we proposed is effective. Moreover, it is also worth noting that our method is 1635 </context>
</contexts>
<marker>Blanco, Moldovan, 2011</marker>
<rawString>Eduardo Blanco and Dan Moldovan. 2011. Semantic Representation of Negation Using Focus Detection. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 581-589, Portland, Oregon, June 19-24, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="7186" citStr="Blei et al., 2003" startWordPosition="1137" endWordPosition="1140">s. • Word co-occurrence. If word wi and word wj occur in a s-width sliding sentence window, sim(wi,wj) increases 1. • WordNet similarity (Miller, 1995). In this paper, we adopt the path similarity function to measure relatedness of nouns and verbs, and adopt the similar to function to measure relatedness of adjectives and adverbs by using the NLTK toolkit1 (Bird et al., 2009). Note that sim(wi,wi) = 0 to avoid self-transition, and sim(wi,wj) and sim(wj,wi) may not be equal. 3.2 Preliminaries for Topical Layer To infer the latent topic distributions of words, Latent Dirichlet Allocation (LDA) (Blei et al., 2003), a typical of topic model, is directly applied. By the set of topics which derive from a corpus, we can obtain: • P(t|w), the probability of topic t given word w, which indicates how much that word w focuses on topic t, and • P(t|d), the probability of topic t given document d, which indicates how much that document d focuses on topic t. 1 http://www.nltk.org/ Pw sim w w ( , ) i j (1) 1633 Then, the similarity between two words or between word wi and document d can be measured by the similarity between their corresponding topic distributions. Formally, we denote a topic distribution as θ, and</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993-1022, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Griffiths</author>
</authors>
<title>Gibbs sampling in the generative model of Latent Dirichlet Allocation.</title>
<date>2002</date>
<tech>Tech. rep.,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="12293" citStr="Griffiths, 2002" startWordPosition="2032" endWordPosition="2033">(w) i 2 http://www.clips.ua.ac.be/sem2012-st-neg/ 1634 the Penn TreeBank3 to obtain the corresponding document as its discourse context. For fair comparison, we adopt the same partition as *SEM’2012 shared task in our experiments. We evaluate our results in terms of accuracy. To see whether an improvement is statistically significant, we conduct significance testing using the paired t-test. For estimating the topical transition probability Pt(j|i) and the document-sensitive value Rd(wi), we employ GibbsLDA++4, an LDA model using Gibbs Sampling technique for parameter estimation and inference (Griffiths, 2002). We set the parameters α = 50/T and fl = 0.1 as Griffiths and Steyvers (2004) suggested. 4.1 Influences of Parameters There are two major parameters in our models that may influence the performance, including: (a) the damping factor µ of the word transition probability P’ w(j|i) (Eq.(7)) and (b) the damping factor A of the word-topic graph model (Eq.(9)). Figure 2 shows the accuracy when varying µ from 0.1 to 0.9 with an interval of 0.1 and when varying A from 0.05 to 1 with an interval of 0.05. We notice that the best performance is achieved when µ=0.6. It indicates that the direct lexical i</context>
</contexts>
<marker>Griffiths, 2002</marker>
<rawString>Tom Griffiths. 2002. Gibbs sampling in the generative model of Latent Dirichlet Allocation. Tech. rep., Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>101</volume>
<pages>5228--5235</pages>
<contexts>
<context position="12371" citStr="Griffiths and Steyvers (2004)" startWordPosition="2046" endWordPosition="2049">reeBank3 to obtain the corresponding document as its discourse context. For fair comparison, we adopt the same partition as *SEM’2012 shared task in our experiments. We evaluate our results in terms of accuracy. To see whether an improvement is statistically significant, we conduct significance testing using the paired t-test. For estimating the topical transition probability Pt(j|i) and the document-sensitive value Rd(wi), we employ GibbsLDA++4, an LDA model using Gibbs Sampling technique for parameter estimation and inference (Griffiths, 2002). We set the parameters α = 50/T and fl = 0.1 as Griffiths and Steyvers (2004) suggested. 4.1 Influences of Parameters There are two major parameters in our models that may influence the performance, including: (a) the damping factor µ of the word transition probability P’ w(j|i) (Eq.(7)) and (b) the damping factor A of the word-topic graph model (Eq.(9)). Figure 2 shows the accuracy when varying µ from 0.1 to 0.9 with an interval of 0.1 and when varying A from 0.05 to 1 with an interval of 0.05. We notice that the best performance is achieved when µ=0.6. It indicates that the direct lexical information contributes slightly more than the topical information. The results</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl. 1), 5228-5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on Shannon entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>14</issue>
<pages>145--151</pages>
<contexts>
<context position="8087" citStr="Lin, 1991" startWordPosition="1303" endWordPosition="1304">ates how much that document d focuses on topic t. 1 http://www.nltk.org/ Pw sim w w ( , ) i j (1) 1633 Then, the similarity between two words or between word wi and document d can be measured by the similarity between their corresponding topic distributions. Formally, we denote a topic distribution as θ, and measure the similarity by using: • Dot-product. We consider the topic distributions as vectors and apply the dot-product, a geometrically motivated function, to calculate the similarity: Inner( ,. ,w)  ,,  wj  tkT P(tk |wi) P(tk |wj ), (2) (3) • Kullback Leibler (KL) divergence (Lin, 1991). Considering the asymmetry (Eq.(4)), we obtain a symmetrized measure by Eq.(5). D ( , )    i k i j t T i k 2 k  P t ( )log . P t P t j k ( ) ( ) (4) DKL(i,j)D(i,j)D(j,i). (5) Note that DKL(θi, θj) is undefined if Pi(tk)=0 or Pj(tk)=0 for any tk ET. For this reason, only the topics which make both Pi(tk)≠0 and Pj(tk)≠0 are adopted to calculate the KL divergence between two topic distributions. 3.3 Word-Topic Graph Model The word layer can well capture the relatedness between words, but just partially model the negation focus since it is more directly related with topic than conten</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on Shannon entropy. IEEE Transactions on Information Theory, 37(14), 145-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Wenyi Huang</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Automatic Keyphrase Extraction via Topic Decomposition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>366--376</pages>
<location>MIT, Massachusetts, USA,</location>
<contexts>
<context position="4690" citStr="Liu et al. (2010)" startWordPosition="722" endWordPosition="725">osed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspired by Liu et al. (2010). Their method runs decomposed Topical PageRank (TPR) for each topic separately, and then calculates the word scores with respect to different topics. When setting the edge weights, only word co-occurrence is considered. Different from their work, our word-topic graph model runs on a twolayers (word layer and topical layer) graph model and sets the edge weights by measuring both word similarity and topic distribution. 3 Methods The word-topic graph model consists of word layer and topical layer, as shown in Figure 1. While the word layer is constructed according to word co-occurrence within a </context>
</contexts>
<marker>Liu, Huang, Zheng, Sun, 2010</marker>
<rawString>Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. Automatic Keyphrase Extraction via Topic Decomposition. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 366-376, MIT, Massachusetts, USA, 9-11 October 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="2078" citStr="Mihalcea and Tarau, 2004" startWordPosition="315" endWordPosition="318"> negative part until he got to Jackson Hole. (1) He didn&apos;t stop until he got to Jackson Hole. Our previous work (Zou et al., 2014) showed that contextual information plays a critical role on negation focus identification. For better illustration of this conclusion, they manually analyze the evidences for 100 negation focuses. It is sur* Corresponding author prising that 77 focuses can be identified with help of contextual information. This indicates that negation focus is often related with what authors repeatedly states in context. In this paper, we thus focus on graph-based ranking methods (Mihalcea and Tarau, 2004) which first build a word graph according to word co-occurrences within document, and then use random walk algorithms (e.g., PageRank) to measure word importance. However, for negation focus identification, the graph-based methods may suffer from the following two problems: (a) the words in graphbased methods are strongly connected by cooccurrence rather than semantic content, which do not necessarily guarantee that they are relevant to the negation focus in context; and (b) identifying a negation focus may be affected by not only the relatedness of surrounding words but also its importance in</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404-411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<pages>38--11</pages>
<contexts>
<context position="6719" citStr="Miller, 1995" startWordPosition="1063" endWordPosition="1064">cted edge ewij is weighted to represent the relatedness between word wi and word wj in a document with transition probability Pw(j|i) from i to j, which is normalized as follows: ( |) j i   , sim w w ( , ) i k k:wi  wk where the denominator represents the out-degree of vertex wi , and sim(wi,wj) denotes the similarity between word wi and wj. In this paper, both corpus-based and knowledge-based methods are evaluated to calculate the similarity between words. • Word co-occurrence. If word wi and word wj occur in a s-width sliding sentence window, sim(wi,wj) increases 1. • WordNet similarity (Miller, 1995). In this paper, we adopt the path similarity function to measure relatedness of nouns and verbs, and adopt the similar to function to measure relatedness of adjectives and adverbs by using the NLTK toolkit1 (Bird et al., 2009). Note that sim(wi,wi) = 0 to avoid self-transition, and sim(wi,wj) and sim(wj,wi) may not be equal. 3.2 Preliminaries for Topical Layer To infer the latent topic distributions of words, Latent Dirichlet Allocation (LDA) (Blei et al., 2003), a typical of topic model, is directly applied. By the set of topics which derive from a corpus, we can obtain: • P(t|w), the probab</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for english. Commun. ACM, 38(11):39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Eduardo Blanco</author>
</authors>
<title>SEM 2012 Shared Task: Resolving the Scope and Focus of Negation.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>265--274</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="4068" citStr="Morante and Blanco (2012)" startWordPosition="621" endWordPosition="624">ection 4 reports the experimental results and analysis. Finally, we conclude our work in Section 5. 1632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1632–1636, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work So far there is little work on negation focus identification, which was pioneered by Blanco and Moldovan (2011) who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspire</context>
</contexts>
<marker>Morante, Blanco, 2012</marker>
<rawString>Roser Morante and Eduardo Blanco. 2012. *SEM 2012 Shared Task: Resolving the Scope and Focus of Negation. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 265-274, Montreal, Canada, June 7-8, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="9730" citStr="Page et al., 1998" startWordPosition="1582" endWordPosition="1585">al semantic information, we refine the relatedness between words by using a topical transition probability Pt(j|i) which is calculated by two kinds of measurements: Here, the similarity is measured by the dotproduct or the KL divergence (using reciprocals). On this basis, the word transition probability Pw(j|i) is updated as following: Pw(j|i)Pw(j|i)(1 Pt i) (7) where µE[0,1] is the coefficient controlling the relative contributions from the lexical information and the topical information. Moreover, the weights of word vertices are calculated by a PageRank algorithm. In standard PageRank (Page et al., 1998), words are set to be the same value, which indicates there is equal importance to all of words in a document. However, intuitively, we should allocate higher weights to those words with high relevance to the document. Therefore, we assign a documentsensitive value to word wi: R w sim ( , ) (8) ( )    d i w d i  sim ( , )   w W w d k k  and calculate the weights of word vertices iteratively by using a biased PageRank algorithm: Score (n 1)(wi) ji Score (n)(wj)P (j |i) (1)Rd(wi). (9) All of the PageRank algorithms are terminated when the number of iterations reaches 100 or the di</context>
</contexts>
<marker>Page, Brin, Motwani, 1998</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, et al. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Rosenberg</author>
<author>Sabine Bergler</author>
</authors>
<title>UConcordia: CLaC Negation Focus Detection at *Sem</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>294--300</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="4189" citStr="Rosenberg and Bergler, 2012" startWordPosition="640" endWordPosition="644">f the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1632–1636, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 2 Related Work So far there is little work on negation focus identification, which was pioneered by Blanco and Moldovan (2011) who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspired by Liu et al. (2010). Their method runs decomposed Topical PageRank (TPR) for each topic separately, and then calculate</context>
</contexts>
<marker>Rosenberg, Bergler, 2012</marker>
<rawString>Sabine Rosenberg and Sabine Bergler. 2012. UConcordia: CLaC Negation Focus Detection at *Sem 2012. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 294-300, Montreal, Canada, June 7-8, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bowei Zou</author>
<author>Qiaoming Zhu</author>
<author>Guodong Zhou</author>
</authors>
<title>Negation Focus Identification with Contextual Discourse Information.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>522--530</pages>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="1583" citStr="Zou et al., 2014" startWordPosition="240" endWordPosition="243"> negation focus identification. 1 Introduction Negation is used to reverse the polarity of part of statements that are otherwise affirmative by default (Blanco and Moldovan, 2011), which is common in natural language. Negation focus is defined as the special part in sentence, which is most prominently or explicitly negated by a negative expression. For example, sentence (1) could be interpreted as He stopped, but not until he got to Jackson Hole with a positive part he stopped and a negative part until he got to Jackson Hole. (1) He didn&apos;t stop until he got to Jackson Hole. Our previous work (Zou et al., 2014) showed that contextual information plays a critical role on negation focus identification. For better illustration of this conclusion, they manually analyze the evidences for 100 negation focuses. It is sur* Corresponding author prising that 77 focuses can be identified with help of contextual information. This indicates that negation focus is often related with what authors repeatedly states in context. In this paper, we thus focus on graph-based ranking methods (Mihalcea and Tarau, 2004) which first build a word graph according to word co-occurrences within document, and then use random wal</context>
<context position="4300" citStr="Zou et al., 2014" startWordPosition="658" endWordPosition="661">ber 2015. c�2015 Association for Computational Linguistics. 2 Related Work So far there is little work on negation focus identification, which was pioneered by Blanco and Moldovan (2011) who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, although Morante and Blanco (2012) proposed negation focus identification as one of the *SEM’2012 shared tasks, only one team (Rosenberg and Bergler, 2012) participated in. They identified negation focus by three heuristic rules. Our previous work (Zou et al., 2014) demonstrates the effectiveness of contextual information for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algorithm in this paper. In recent years, many algorithms are widely used to incorporate word graph models and topical information within random walk. Our work is originally inspired by Liu et al. (2010). Their method runs decomposed Topical PageRank (TPR) for each topic separately, and then calculates the word scores with respect to different topics. When setting the edge weights, only word co-occurrence is c</context>
<context position="15198" citStr="Zou et al., 2014" startWordPosition="2498" endWordPosition="2501">.78%, p&lt;0.01) than the graph model with only word layer (WLM), which justifies the effectiveness of the topical layer. In addition, the results also indicate that the word-topic graph model not only takes the topical information into account (“TL”), but also considers the semantic relationship in current document (“DS”). We select two supervised baseline methods to compare with our word-topic graph model. One is a decision tree-based system described in Blanco and Moldovan (2011), and the other one is a SVM-based system which takes advantage of both syntactic features and contextual features (Zou et al., 2014). system Acc. B&amp;M(2011) 63.20 Zou et al.(2014) 67.14 Ours 69.39 Table 2. Comparison Results. Table 2 shows that our word-topic graph model performs significantly better than the two others by 6.19% (p&lt;0.01) and 2.25% (p&lt;0.01), respectively. The results support our viewpoint that the topical information in context can help to find the negation focus, and the word-topic graph model we proposed is effective. Moreover, it is also worth noting that our method is 1635 unsupervised, which does not need the prior knowledge for training, while the other two supervised baselines employ the golden featur</context>
</contexts>
<marker>Zou, Zhu, Zhou, 2014</marker>
<rawString>Bowei Zou, Qiaoming Zhu, and Guodong Zhou. 2014. Negation Focus Identification with Contextual Discourse Information. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 522-530, Baltimore, Maryland, USA, June 23-25 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.977945">
Reordering Grammar Induction
</title>
<author confidence="0.938727">
Milos Stanojevic Khalil Sima’an
</author>
<affiliation confidence="0.9275195">
ILLC ILLC
University of Amsterdam University of Amsterdam
</affiliation>
<email confidence="0.988767">
m.stanojevic@uva.nl k.simaan@uva.nl
</email>
<sectionHeader confidence="0.993621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847956521739">
We present a novel approach for unsu-
pervised induction of a Reordering Gram-
mar using a modified form of permuta-
tion trees (Zhang and Gildea, 2007), which
we apply to preordering in phrase-based
machine translation. Unlike previous ap-
proaches, we induce in one step both the
hierarchical structure and the transduction
function over it from word-aligned parallel
corpora. Furthermore, our model (1) han-
dles non-ITG reordering patterns (up to
5-ary branching), (2) is learned from all
derivations by treating not only labeling
but also bracketing as latent variable, (3) is
entirely unlexicalized at the level of re-
ordering rules, and (4) requires no linguis-
tic annotation.
Our model is evaluated both for accuracy
in predicting target order, and for its im-
pact on translation quality. We report sig-
nificant performance gains over phrase re-
ordering, and over two known preordering
baselines for English-Japanese.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913781818182">
Preordering (Collins et al., 2005) aims at permut-
ing the words of a source sentence s into a new
order ´s, hopefully close to a plausible target word
order. Preordering is often used to bridge long dis-
tance reorderings (e.g., in Japanese- or German-
English), before applying phrase-based models
(Koehn et al., 2007). Preordering is often bro-
ken down into two steps: finding a suitable tree
structure, and then finding a transduction function
over it. A common approach is to use monolin-
gual syntactic trees and focus on finding a trans-
duction function of the sibling subtrees under the
nodes (Lerner and Petrov, 2013; Xia and Mccord,
2004). The (direct correspondence) assumption
underlying this approach is that permuting the sib-
lings of nodes in a source syntactic tree can pro-
duce a plausible target order. An alternative ap-
proach creates reordering rules manually and then
learns the right structure for applying these rules
(Katz-Brown et al., 2011). Others attempt learn-
ing the transduction structure and the transduction
function in two separate, consecutive steps (DeN-
ero and Uszkoreit, 2011). Here we address the
challenge of learning both the trees and the trans-
duction functions jointly, in one fell swoop, from
word-aligned parallel corpora.
Learning both trees and transductions jointly
raises two questions. How to obtain suitable trees
for the source sentence and how to learn a distri-
bution over random variables specifically aimed
at reordering in a hierarchical model? In this
work we solve both challenges by using the fac-
torizations of permutations into Permutation Trees
(PETs) (Zhang and Gildea, 2007). As we ex-
plain next, PETs can be crucial for exposing the
hierarchical reordering patterns found in word-
alignments.
We obtain permutations in the training data by
segmenting every word-aligned source-target pair
into minimal phrase pairs; the resulting alignment
between minimal phrases is written as a permuta-
tion (1:1 and onto) on the source side. Every per-
mutation can be factorized into a forest of PETs
(over the source sentences) which we use as a la-
tent treebank for training a Probabilistic Context-
Free Grammar (PCFG) tailor made for preorder-
ing as we explain next.
Figure 1 shows two alternative PETs for the
same permutation over minimal phrases. The
nodes have labels (like P3142) which stand for lo-
cal permutations (called prime permutation) over
the child nodes; for example, the root label P3142
stands for prime permutation (3,1, 4, 2), which
says that the first child of the root becomes 3rd on
the target side, the second becomes 1st, the third
</bodyText>
<page confidence="0.992088">
44
</page>
<note confidence="0.9850305">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 44–54,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999569217391304">
becomes 4th and the fourth becomes 2nd. The
prime permutations are non-factorizable permuta-
tions like (1, 2), (2, 1) and (2, 4, 1, 3).
We think PETs are suitable for learning pre-
ordering for two reasons. Firstly, PETs specify ex-
actly the phrase pairs defined by the permutation.
Secondly, every permutation is factorizable into
prime permutations only (Albert and Atkinson,
2005). Therefore, PETs expose maximal sharing
between different permutations in terms of both
phrases and their reordering. We expect this to be
advantageous for learning hierarchical reordering.
For learning preordering, we first extract an ini-
tial PCFG from the latent treebank of PETs over
the source sentences only. We initialize the non-
terminal set of this PCFG to the prime permuta-
tions decorating the PET nodes. Subsequently we
split these coarse labels in the same way as latent
variable splitting is learned for treebank parsing
(Matsuzaki et al., 2005; Prescher, 2005; Petrov et
al., 2006; Saluja et al., 2014). Unlike treebank
parsing, however, our training treebank is latent
because it consists of a whole forest of PETs per
training instance (s).
Learning the splits on a latent treebank of PETs
results in a Reordering PCFG which we use to
parse input source sentences into split-decorated
trees, i.e., the labels are the splits of prime permu-
tations. After parsing s, we map the splits back on
their initial prime permutations, and then retrieve
a reordered version s´ of s. In this sense, our latent
splits are dedicated to reordering.
We face two technical difficulties alien to work
on latent PCFGs in treebank parsing. Firstly, as
mentioned above, permutations may factorize into
more than one PET (a forest) leading to a latent
training treebank.1 And secondly, after we parse
a source string s, we are interested in ´s, the per-
muted version of s, not in the best derivation/PET.
Exact computation is a known NP-Complete prob-
lem (Sima’an, 2002). We solve this by a new
Minimum-Bayes Risk decoding approach using
Kendall reordering score as loss function, which
is an efficient measure over permutations (Birch
and Osborne, 2011; Isozaki et al., 2010a).
In summary, this paper contributes:
</bodyText>
<listItem confidence="0.811300272727273">
• A novel latent hierarchical source reordering
model working over all derivations of PETs
1All PETs for the same permutation share the same set
of prime permutations but differ only in bracketing structure
(Zhang and Gildea, 2007).
• A label splitting approach based on PCFGs
over minimal phrases as terminals, learned
from an ambiguous treebank, where the label
splits start out from prime permutations.
• A fast Minimum Bayes Risk decoding over
Kendall T reordering score for selecting ´s.
</listItem>
<bodyText confidence="0.967186210526316">
We report results for extensive experiments on
English-Japanese showing that our Reordering
PCFG gives substantial improvements when used
as preordering for phrase-based models, outper-
forming two existing baselines for this task.
2 PETs and the Hidden Treebank
We aim at learning a PCFG which we will use for
parsing source sentences s into synchronous trees,
from which we can obtain a reordered source ver-
sion ´s. Since PCFGs are non-synchronous gram-
mars, we will use the nonterminal labels to encode
reordering transductions, i.e., this PCFG is implic-
itly an SCFG. We can do this because s and s´ are
over the same alphabet.
Here, we have access only to a word-aligned
parallel corpus, not a treebank. The following
steps summarize our approach for acquiring a la-
tent treebank and how it is used for learning a Re-
ordering PCFG:
</bodyText>
<listItem confidence="0.999380875">
1. Obtain a permutation over minimal phrases
from every word-alignment.
2. Obtain a latent treebank of PETs by factoriz-
ing the permutations.
3. Extract a PCFG from the PETs with initial
nonterminals taken from the PETs.
4. Learn to split the initial nonterminals and es-
timate rule probabilities.
</listItem>
<bodyText confidence="0.999917176470588">
These steps are detailed in the next section, but we
will start out with an intuitive exposition of PETs,
the latent treebank and the Reordering Grammar.
Figure 1 shows examples of how PETs look
like – see (Zhang and Gildea, 2007) for algorith-
mic details. Here we label the nodes with nonter-
minals which stand for prime permutations from
the operators on the PETs. For example, non-
terminals P12, P21 and P3142 correspond re-
spectively to reordering transducers (1, 2), (2, 1)
and (3, 1, 4, 2). A prime permutation on a source
node p is a transduction dictating how the chil-
dren of p are reordered at the target side, e.g.,
P21 inverts the child order. We must stress that
any similarity with ITG (Wu, 1997) is restricted
to the fact that the straight and inverted operators
of ITG are the binary case of prime permutations
</bodyText>
<page confidence="0.996945">
45
</page>
<figure confidence="0.993838428571429">
P3142
P12
P21
P12
Professor Chomsky , I would like to thank you
P3142
P12
P21
P12
Professor Chomsky , I would like to thank you
Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken
(a) Canonical PET
Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken
(b) Alternative PET
</figure>
<figureCaption confidence="0.999985">
Figure 1: Possible Permutation Trees (PETs) for one sentence pair
</figureCaption>
<bodyText confidence="0.999500153846154">
in PETs (P12 and P21). ITGs recognize only the
binarizable permutations, which is a major restric-
tion when used on the data: there are many non-
binarizable permutations in actual data (Welling-
ton et al., 2006). In contrast, our PETs are ob-
tained by factorizing permutations obtained from
the data, i.e., they exactly fit the range of prime
permutations in the parallel corpus. In practice we
limit them to maximum arity 5.
We can extract PCFG rules from the PETs, e.g.,
P21 → P12 P2413. However, these rules are
decorated with too coarse labels. A similar prob-
lem was encountered in non-lexicalized monolin-
gual parsing, and one solution was to lexicalize
the productions (Collins, 2003) using head words.
But linguistic heads do not make sense for PETs,
so we opt for the alternative approach (Matsuzaki
et al., 2005), which splits the nonterminals and
softly percolates the splits through the trees gradu-
ally fitting them to the training data. Splitting has
a shadow side, however, because it leads to com-
binatorial explosion in grammar size.
Suppose for example node P21 could split into
P211 and P212 and similarly P2413 splits into
P24131 and 24132. This means that rule P21 →
P12 P2413 will form eight new rules:
</bodyText>
<equation confidence="0.99945625">
P211 → P121 P24131 P211 → P121 P24132
P211 → P122 P24131 P211 → P122 P24132
P212 → P121 P24131 P212 → P121 P24132
P212 → P122 P24131 P212 → P122 P24132
</equation>
<bodyText confidence="0.999589111111111">
Should we want to split each nonterminal into
30 subcategories, then an n-ary rule will split
into 30n+1 new rules, which is prohibitively large.
Here we use the “unary trick” as in Figure 2. The
superscript on the nonterminals denotes the child
position from left to right. For example P2121
means that this node is a second child, and the
mother nonterminal label is P211. For the running
example rule, this gives the following rules:
</bodyText>
<equation confidence="0.9997924">
P211 → P211 1 P212 1 P212 → P2112 P2122
P2111 → P121 P2121 → P24131
P2111 → P122 P212 1 → P24132
P2112 → P121 P212 2 → P 24131
P2112 → P122 P212 2 → P 24132
</equation>
<bodyText confidence="0.9994541">
The unary trick leads to substantial reduction in
grammar size, e.g., for arity 5 rules and 30 splits
we could have had 306 = 729000000 split-rules,
but with the unary trick we only have 30+302∗5 =
4530 split rules. The unary trick was used in
early lexicalized parsing work (Carroll and Rooth,
1998).2 This split PCFG constitutes a latent
PCFG because the splits cannot be read of a tree-
bank. It must be learned from the latent treebank
of PETs, as described next.
</bodyText>
<figureCaption confidence="0.991034">
Figure 2: Permutation Tree with unary trick
</figureCaption>
<sectionHeader confidence="0.912541" genericHeader="method">
3 Details of Latent Reordering PCFG
</sectionHeader>
<bodyText confidence="0.7500156">
Obtaining permutations Given a source sen-
tence s and its alignment a to a target sentence
2After applying the unary trick, we add a constraint on
splitting: all nonterminals on an n-ary branching rule must
be split simultaneously.
</bodyText>
<equation confidence="0.967267083333333">
Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken
Professor Chomsky , I would like to thank you
P121 P122
P121 P122
P12 P21
P31421
P12
P211 P212
P31422
P3142
P31423
P31424
</equation>
<page confidence="0.979974">
46
</page>
<bodyText confidence="0.99993975">
t in the training corpus, we segment (s, a, t) into
a sequence of minimal phrases sm (maximal se-
quence) such that the reordering between these
minimal phrases constitutes a permutation 7rm.
We do not extract non-contiguous or non-minimal
phrases because reordering them often involves
complicated transductions which could hamper
the performance of our learning algorithm.3
Unaligned words Next we describe the use of
the factorization of permutations into PET forests
for training a PCFG model. But first we need
to extend the PETs to allow for unaligned words.
An unaligned word is joined with a neighboring
phrase to the left or the right, depending on the
source language properties (e.g., whether the lan-
guage is head-initial or -final (Chomsky, 1970)).
Our experiments use English as source language
(head-initial), so the unaligned words are joined
to phrases to their right. This modifies a PET by
adding a new binary branching node p (dominat-
ing the unaligned word and the phrase it is joined
to) which is labeled with a dedicated nonterminal:
P01 if the unaligned word joins to the right and
P10 if it joins to the left.
</bodyText>
<subsectionHeader confidence="0.997868">
3.1 Probability model
</subsectionHeader>
<bodyText confidence="0.999994733333333">
We decompose the permutation 7rm into a forest
of permutation trees PEF(7rm) in O(n3), follow-
ing algorithms in (Zhang et al., 2008; Zhang and
Gildea, 2007) with trivial modifications. Each
PET 0 E PEF(7rm) is a different bracketing
(differing in binary branching structure only). We
consider the bracketing hidden in the latent tree-
bank, and apply unsupervised learning to induce a
distribution over possible bracketings. Our prob-
ability model starts from the joint probability of a
sequence of minimal phrases sm and a permuta-
tion 7rm over it. This demands summing over all
PETs 0 in the forest PEF(7rm), and for every
PET also over all its label splits, which are given
by the grammar derivations d:
</bodyText>
<equation confidence="0.99637">
XP (sm, 7rm) = X P(d, sm) (1)
DEPEF(7r,) dED
</equation>
<bodyText confidence="0.9929985">
The probability of a derivation d is a product of
probabilities of all the rules r that build it:
</bodyText>
<equation confidence="0.915579">
XP(sm,7rm) = X Y P(r) (2)
DEPEF(7r,) dED rEd
</equation>
<footnote confidence="0.565497">
3Which differs from (Quirk and Menezes, 2006).
</footnote>
<bodyText confidence="0.9999">
As usual, the parameters of this model are the
PCFG rule probabilities which are estimated from
the latent treebank using EM as explained next.
</bodyText>
<subsectionHeader confidence="0.999559">
3.2 Learning Splits on Latent Treebank
</subsectionHeader>
<bodyText confidence="0.9999679375">
For training the latent PCFG over the latent tree-
bank, we resort to EM (Dempster et al., 1977)
which estimates PCFG rule probabilities to max-
imize the likelihood of the parallel corpus in-
stances. Computing expectations for EM is
done efficiently using Inside-Outside (Lari and
Young, 1990). As in other state splitting models
(Matsuzaki et al., 2005), after splitting the non-
terminals, we distribute the probability uniformly
over the new rules, and we add to each new rule
some random noise to break the symmetry. We
split the non-terminals only once as in (Matsuzaki
et al., 2005) (unlike (Petrov et al., 2006)). For es-
timating the distribution for unknown words we
replace all words that appear ≤ 3 times with the
“UNKNOWN” token.
</bodyText>
<subsectionHeader confidence="0.94567">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.9998965">
We use CKY+ (Chappelier and Rajman, 1998) to
parse a source sentence s into a forest using the
learned split PCFG. Unfortunately, computing the
most-likely permutation (or alternatively ´s) as in
</bodyText>
<equation confidence="0.999351">
X X
argmax P(d, 7rm)
7rEΠ DEPEF(7r) dED
</equation>
<bodyText confidence="0.999932">
from a lattice of permutations H using a PCFG
is NP-complete (Sima’an, 2002). Existing tech-
niques, like variational decoding or Minimum-
Bayes Risk (MBR), used for minimizing loss over
trees as in (Petrov and Klein, 2007), are not di-
rectly applicable here. Hence, we opt for mini-
mizing the risk of making an error under a loss
function over permutations using the MBR deci-
sion rule (Kumar and Byrne, 2004):
</bodyText>
<equation confidence="0.972569">
Loss(7r, 7rr)P(7rr) (3)
</equation>
<bodyText confidence="0.999981">
The loss function we minimize is Kendall T (Birch
and Osborne, 2011; Isozaki et al., 2010a) which
is a ratio of wrongly ordered pairs of words (in-
cluding gapped pairs) to the total number of pairs.
We do Monte Carlo sampling of 10000 derivations
from the chart of the s and then find the least risky
permutation in terms of this loss. We sample from
the true distribution by sampling edges recursively
</bodyText>
<equation confidence="0.94226025">
7rˆ = argmin
7r
X
7rr
</equation>
<page confidence="0.97575">
47
</page>
<bodyText confidence="0.999036363636364">
using their inside probabilities. An empirical dis-
tribution over permutations P(π) is given by the
relative frequency of π in the sample.
With large samples it is hard to efficiently com-
pute expected Kendall τ loss for each sampled
hypothesis. For sentence of length k and sam-
ple of size n the complexity of a naive algorithm
is O(n2k2). Computing Kendall τ alone takes
O(k2). We use the fact that Kendall τ decom-
poses as a linear function over all skip-bigrams b
that could be built for any permutation of length k:
</bodyText>
<equation confidence="0.7918745">
�Kendall(π, πr) =
b
</equation>
<bodyText confidence="0.9984752">
Here δ returns 1 if permutation π contains the skip
bigram b, otherwise it returns 0. With this decom-
position we can use the method from (DeNero et
al., 2009) to efficiently compute the MBR hypoth-
esis. Combining Equations 3 and 4 we get:
</bodyText>
<equation confidence="0.949435">
δ(πr, b)P(πr) (5)
</equation>
<bodyText confidence="0.999731">
We can move the summation inside and reformu-
late the expected Kendall τ loss as expectation
over the skip-bigrams of the permutation.
</bodyText>
<equation confidence="0.996961">
L � J
(1 − δ(π, b)) πr δ(πr, b)P(πr) (6)
(1 − δ(π, b))EP(πr)δ(πr, b) (7)
</equation>
<listItem confidence="0.947469363636364">
• Baseline C: LADER (Neubig et al., 2012):
latent variable preordering that is based on
ITG and large-margin training with latent
variables. We used LADER in standard set-
tings without any linguistic features (POS
tags or syntactic trees).
And we test four variants of our model:
• RGleft - only canonical left branching PET
• RGright - only canonical right branching PET
• RGITG-forest - all PETs that are binary (ITG)
• RGPET-forest - all PETs.
</listItem>
<bodyText confidence="0.998328461538461">
We test these models on English-Japanese
NTCIR-8 Patent Translation (PATMT) Task. For
tuning we use all NTCIR-7 dev sets and for test-
ing the test set from NTCIR-9 from both direc-
tions. All used data was tokenized (English with
Moses tokenizer and Japanese with KyTea 5) and
filtered for sentences between 4 and 50 words. A
subset of this data is used for training the Reorder-
ing Grammar, obtained by filtering out sentences
that have prime permutations of arity &gt; 5, and for
the ITG version arity &gt; 2. Baseline C was trained
on 600 sentences because training is prohibitively
slow. Table 1 shows the sizes of data used.
</bodyText>
<table confidence="0.917861111111111">
corpus #sents #words #words
source target
train RGPET 786k 21M –
train RGITG 783k 21M –
train LADER 600 15k –
train translation 950k 25M 30M
tune translation 2k 55K 66K
test translation 3k 78K 93K
1 − δ(π, b) δ(πr, b) (4)
</table>
<figure confidence="0.8277488">
k(k−1)
2
πˆ = argmin E � 1 − δ(π, b)
π πr b k(k−1)
2
= argmin
π
�
b
= argmin
π
�
b
= argmax � δ(π, b)EP(πr)δ(πr, b) (8) Table 1: Data stats
π b
</figure>
<bodyText confidence="0.998033">
This means we need to pass through the sampled
list only twice: (1) to compute expectations over
skip bigrams and (2) to compute expected loss of
each sampled permutation. The time complexity
is O(nk2) which is quite fast in practice.
</bodyText>
<sectionHeader confidence="0.999805" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.992841">
We conduct experiments with three baselines:
</bodyText>
<listItem confidence="0.688144428571429">
• Baseline A: No preordering.
• Baseline B: Rule based preordering (Isozaki
et al., 2010b), which first obtains an HPSG
parse tree using Enju parser 4 and after that
swaps the children by moving the syntactic
head to the final position to account for differ-
ent head orientation in English and Japanese.
</listItem>
<footnote confidence="0.8636">
4http://www.nactem.ac.uk/enju/
</footnote>
<bodyText confidence="0.998520833333333">
The Reordering Grammar was trained for 10 it-
erations of EM on train RG data. We use 30 splits
for binary non-terminals and 3 for non-binary.
Training on this dataset takes 2 days and parsing
tuning and testing set without any pruning takes
11 and 18 hours respectively.
</bodyText>
<subsectionHeader confidence="0.9539">
4.1 Intrinsic evaluation
</subsectionHeader>
<bodyText confidence="0.999967714285714">
We test how well our model predicts gold reorder-
ings before translation by training the alignment
model using MGIZA++ 6 on the training corpus
and using it to align the test corpus. Gold re-
orderings for the test corpus are obtained by sort-
ing words by their average target position and
(unaligned words follow their right neighboring
</bodyText>
<footnote confidence="0.9995855">
5http://www.phontron.com/kytea/
6http://www.kyloo.net/software/doku.php/mgiza:overview
</footnote>
<page confidence="0.998655">
48
</page>
<bodyText confidence="0.992882818181818">
word). We use Kendall T score for evaluation
(note the difference with Section 3.3 where we de-
fined it as a loss function).
Table 2 shows that our models outperform all
baselines on this task. The only strange result
here is that rule-based preordering obtains a lower
score than no preordering, which might be an ar-
tifact of the Enju parser changing the tokenization
of its input, so the Kendall T of this system might
not really reflect the real quality of the preorder-
ing. All other systems use the same tokenization.
</bodyText>
<table confidence="0.99823975">
Kendall T
ANo preordering 0.7655
BRule based 0.7567
CLADER 0.8176
RGleft-branching 0.8201
RGright-branching 0.8246
RGITG-forest 0.823
RGPET-forest 0.8255
</table>
<tableCaption confidence="0.995101">
Table 2: Reordering prediction
</tableCaption>
<subsectionHeader confidence="0.990892">
4.2 Extrinsic evaluation in MT
</subsectionHeader>
<bodyText confidence="0.998288947368421">
The reordered output of all the mentioned base-
lines and versions of our model are translated with
phrase-based MT system (Koehn et al., 2007) (dis-
tortion limit set to 6 with distance based reordering
model) that is trained on gold preordering of the
training data 7 s´ − t. The only exception is Base-
line A which is trained on original s − t.
We use a 5-gram language model trained with
KenLM 8, tune 3 times with kb-mira (Cherry and
Foster, 2012) to account for tuner instability and
evaluated using Multeval 9 for statistical signifi-
cance on 3 metrics: BLEU (Papineni et al., 2002),
METEOR (Denkowski and Lavie, 2014) and TER
(Snover et al., 2006). We additionally report
RIBES score (Isozaki et al., 2010a) that concen-
trates on word order more than other metrics.
Single or all PETs? In Table 3 we see that
using all PETs during training makes a big im-
pact on performance. Only the all PETs variants
</bodyText>
<footnote confidence="0.969079666666667">
7Earlier work on preordering applies the preordering
model to the training data to obtain a parallel corpus of
guessed s´ − t pairs, which are the word re-aligned and then
used for training the back-end MT system (Khalilov and
Sima’an, 2011). We skip this, we take the risk of mismatch
between the preordering and the back-end system, but this
simplifies training and saves a good amount of training time.
8http://kheafield.com/code/kenlm/
9https://github.com/jhclark/multeval
</footnote>
<table confidence="0.999757555555556">
System BLEU I METEOR I TER ↓ RIBES I
ANo preord. 27.8 48.9 59.2 68.29
BRule based 29.6 48.7 59.2 71.12
CLADER 31.1 50.5 56.0 74.29
RGleft 31.2AB 50.5AB 56.3AB 74.45
RGright 31.4AB 50.5AB 56.3AB 75.29
C
RGITG-forest 31.6ABC 50.8ABC 55.7ABC 75.29
RGPET-forest 32.0ABC 51.0ABC 55.7ABC 75.62
</table>
<tableCaption confidence="0.861531">
Table 3: Comparison of different preordering
models. Superscripts A, B and C signify if the sys-
</tableCaption>
<bodyText confidence="0.998407358974359">
tem is significantly better (p &lt; 0.05) than the re-
spective baseline or significantly worse (in which
case it is a subscript). Significance tests were not
computed for RIBES. Score is bold if the system
is significantly better than all the baselines.
(RGITG-forest and RGPET-forest) significantly outper-
form all baselines. If we are to choose a single
PET per training instance, then learning RG from
only left-branching PETs (the one usually cho-
sen in other work, e.g. (Saluja et al., 2014)) per-
forms slightly worse than the right-branching PET.
This is possibly because English is mostly right-
branching. So even though both PETs describe the
same reordering, RGright captures reordering over
English input better than RGleft.
All PETs or binary only? RGPET-forest performs
significantly better than RGITG-forest (p &lt; 0.05).
Non-ITG reordering operators are predicted rarely
(in only 99 sentences of the test set), but they
make a difference, because these operators often
appear high in the predicted PET. Furthermore,
having these operators during training might allow
for better fit to the data.
How much reordering is resolved by the
Reordering Grammar? Obviously, completely
factorizing out the reordering from the transla-
tion process is impossible because reordering de-
pends to a certain degree on target lexical choice.
To quantify the contribution of Reordering Gram-
mar, we tested decoding with different distortion
limit values in the SMT system. We compare the
phrase-based (PB) system with distance based cost
function for reordering (Koehn et al., 2007) with
and without preordering.
Figure 3 shows that Reordering Grammar
gives substantial performance improvements at
all distortion limits (both BLEU and RIBES).
RGPET-forest is less sensitive to changes in decoder
distortion limit than standard PBSMT. The perfor-
</bodyText>
<page confidence="0.999507">
49
</page>
<figureCaption confidence="0.999921">
Figure 3: Distortion effect on BLEU and RIBES
</figureCaption>
<bodyText confidence="0.985201111111111">
mance of RGPET-forest varies only by 1.1 BLEU
points while standard PBSMT by 4.3 BLEU
points. Some local reordering in the decoder
seems to help RGPET-forest but large distortion
limits seem to degrade the preordering choice.
This shows also that the improved performance of
RGPET-forest is not only a result of efficiently ex-
ploring the full space of permutations, but also a
result of improved scoring of permutations.
</bodyText>
<table confidence="0.9992842">
System BLEU I METEOR I TER ↓ RIBES I
DPBMSD 29.6 50.1 58.0 68.97
EHiero 32.6 52.1 54.5 74.12
RGPET-forest+MSD 32.4D 51.3D 55.3D E 75.72
E
</table>
<tableCaption confidence="0.999912">
Table 4: Comparison to MSD and Hiero
</tableCaption>
<bodyText confidence="0.998315904761905">
Does the improvement remain for a decoder
with MSD reordering model? We compare the
RGPET-forest preordered model against a decoder
that uses the strong MSD model (Tillmann, 2004;
Koehn et al., 2007). Table 4 shows that using
Reordering Grammar as front-end to MSD re-
ordering (full Moses) improves performance by
2.8 BLEU points. The improvement is confirmed
by METEOR, TER and RIBES. Our preordering
model and MSD are complementary – the Re-
ordering Grammar captures long distance reorder-
ing, while MSD possibly does better local reorder-
ings, especially reorderings conditioned on the
lexical part of translation units.
Interestingly, the MSD model (BLEU 29.6)
improves over distance-based reordering (BLEU
27.8) by (BLEU 1.8), whereas the difference be-
tween these systems as back-ends to Reordering
Grammar (respectively BLEU 32.4 and 32.0) is
far smaller (0.4 BLEU). This suggests that a ma-
jor share of reorderings can be handled well by
preordering without conditioning on target lexical
choice. Furthermore, this shows that RGPET-forest
preordering is not very sensitive to the decoder’s
reordering model.
Comparison to a Hierarchical model (Hiero).
Hierarchical preordering is not intended for a hi-
erarchical model as Hiero (Chiang, 2005). Yet,
here we compare our preordering system (PB
MSD+RG) to Hiero for completeness, while we
should keep in mind that Hiero’s reordering model
has access to much richer training data. We will
discuss these differences shortly.
Table 4 shows that the difference in BLEU is
not statistically significant, but there is more dif-
ference in METEOR and TER. RIBES, which
concentrates more on reordering, prefers Reorder-
ing Grammar over Hiero. It is somewhat sur-
prising that a preordering model combined with a
phrase-based model succeeds to rival Hiero’s per-
formance on English-Japanese. Especially when
looking at the differences between the two:
</bodyText>
<listItem confidence="0.9444119">
1. Reordering Grammar uses only minimal
phrases, while Hiero uses composite (longer)
phrases which encapsulate internal reorder-
ings, but also non-contiguous phrases.
2. Hiero conditions its reordering on the lexical
target side, whereas the Reordering Grammar
does not (by definition).
3. Hiero uses a range of features, e.g., a lan-
guage model, while Reordering Grammar is
a mere generative PCFG.
</listItem>
<bodyText confidence="0.9998442">
The advantages of Hiero can be brought to bear
upon Reordering Grammar by reformulating it as
a discriminative model.
Which structure is learned? Figure 4 shows
an example PET output showing how our model
learns: (1) that the article “the” has no equiva-
lent in Japanese, (2) that verbs go after their ob-
ject, (3) to use postpositions instead of preposi-
tions, and (4) to correctly group certain syntactic
units, e.g. NPs and VPs.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.99952">
The majority of work on preordering is based
on syntactic parse trees, e.g., (Lerner and Petrov,
2013; Khalilov and Sima’an, 2011; Xia and Mc-
cord, 2004). Here we concentrate on work that
has common aspects with this work. Neubig et
</bodyText>
<page confidence="0.996516">
50
</page>
<figureCaption confidence="0.999541">
Figure 4: Example parse of English sentence that predicts reordering for English-Japanese
</figureCaption>
<figure confidence="0.996297315789474">
P12
P12
P21
P21
P21
P21
DC
current i flowing
P12
P12
through
P12
P01 P12
the feeding conductor 3 produces
magnetic
P12
field b1
P12
.
</figure>
<bodyText confidence="0.999560431818182">
al (2012) trains a latent non-probabilistic discrim-
inative model for preordering as an ITG-like gram-
mar limited to binarizable permutations. Tromble
and Eisner (2009) use ITG but do not train the
grammar. They only use it to constrain the lo-
cal search. DeNero and Uszkoreit (2011) present
two separate consecutive steps for unsupervised
induction of hierarchical structure (ITG) and the
induction of a reordering function over it. In con-
trast, here we learn both the structure and the re-
ordering function simultaneously. Furthermore, at
test time, our inference with MBR over a mea-
sure of permutation (Kendall) allows exploiting
both structure and reordering weights for infer-
ence, whereas test-time inference in (DeNero and
Uszkoreit, 2011) is also a two step process – the
parser forwards to the next stage the best parse.
Dyer and Resnik (2010) treat reordering as a la-
tent variable and try to sum over all derivations
that lead not only to the same reordering but also
to the same translation. In their work they consider
all permutations allowed by a given syntactic tree.
Saers et al (2012) induce synchronous gram-
mar for translation by splitting the non-terminals,
but unlike our approach they split generic non-
terminals and not operators. Their most expres-
sive grammar covers only binarizable permuta-
tions. The decoder that uses this model does not
try to sum over many derivations that have the
same yield. They do not make independence as-
sumption like our “unary trick” which is proba-
bly the reason they do not split more than 8 times.
They do not compare their results to any other
SMT system and test on a very small dataset.
Saluja et al (2014) attempts inducing a refined
Hiero grammar (latent synchronous CFG) from
Normalized Decomposition Trees (NDT) (Zhang
et al., 2008). While there are similarities with
the present work, there are major differences. On
the similarity side, NDTs are decomposing align-
ments in ways similar to PETs, and both Saluja’s
and our models refine the labels on the nodes of
these decompositions. However, there are major
differences between the two:
</bodyText>
<listItem confidence="0.955832722222222">
• Our model is completely monolingual and
unlexicalized (does not condition its reorder-
ing on the translation) in contrast with the La-
tent SCFG used in (Saluja et al., 2014),
• Our Latent PCFG label splits are defined
as refinements of prime permutations, i.e.,
specifically designed for learning reordering,
whereas (Saluja et al., 2014) aims at learn-
ing label splitting that helps predicting NDTs
from source sentences,
• Our model exploits all PETs and all deriva-
tions, both during training (latent treebank)
and during inferences. In (Saluja et al., 2014)
only left branching NDT derivations are used
for learning the model.
• The training data used by (Saluja et al., 2014)
is about 60 times smaller in number of words
than the data used here; the test set of (Saluja
</listItem>
<bodyText confidence="0.987797">
et al., 2014) also consists of far shorter sen-
tences where reordering could be less crucial.
A related work with a similar intuition is presented
in (Maillette de Buy Wenniger and Sima’an,
2014), where nodes of a tree structure similar
to PETs are labeled with reordering patterns ob-
tained by factorizing word alignments into Hierar-
chical Alignment Trees. These patterns are used
for labeling the standard Hiero grammar. Unlike
this work, the labels extracted by (Maillette de
Buy Wenniger and Sima’an, 2014) are clustered
manually into less than a dozen labels without the
possibility of fitting the labels to the training data.
</bodyText>
<page confidence="0.99801">
51
</page>
<sectionHeader confidence="0.999367" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998388">
We present a generative Reordering PCFG model
learned from latent treebanks over PETs obtained
by factorizing permutations over minimal phrase
pairs. Our Reordering PCFG handles non-ITG
reordering patterns (up to 5-ary branching) and
it works with all PETs that factorize a permuta-
tion (rather than a single PET). To the best of our
knowledge this is the first time both extensions
are shown to improve performance. The empiri-
cal results on English-Japanese show that (1) when
used for preordering, the Reordering PCFG helps
particularly with relieving the phrase-based model
from long range reorderings, (2) combined with
a state-of-the-art phrase model, Reordering PCFG
shows performance not too different from Hiero,
supporting the common wisdom of factorizing
long range reordering outside the decoder, (3) Re-
ordering PCFG generates derivations that seem
to coincide well with linguistically-motivated re-
ordering patterns for English-Japanese. There are
various direction we would like to explore, the
most obvious of which are integrating the learned
reordering with other feature functions in a dis-
criminative setting, and extending the model to
deal with non-contiguous minimal phrases.
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99942175">
This work is supported by STW grant nr. 12271
and NWO VICI grant nr. 277-89-002. We thank
Wilker Aziz for comments on earlier version of the
paper and discussions about MBR and sampling.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997032777777778">
Michael H. Albert and Mike D. Atkinson. 2005. Sim-
ple Permutations and Pattern Restricted Permuta-
tions. Discrete Mathematics, 300(1-3):1–15.
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing Metrics for MT. In Proceedings of the Associ-
ation for Computational Linguistics, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.
Glenn Carroll and Mats Rooth. 1998. Valence Induc-
tion with a Head-Lexicalized PCFG. In In Proceed-
ings of Third Conference on Empirical Methods in
Natural Language Processing.
Jean-C´edric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochastic
CFG. In Proceedings of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133–137.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 427–436.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
263–270, Ann Arbor, Michigan, June.
Noam Chomsky. 1970. Remarks on Nominalization.
In Roderick A. Jacobs and Peter S. Rosenbaum, ed-
itors, Readings in English Transformational Gram-
mar, pages 184–221. Ginn, Boston.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, ACL ’05, pages 531–540.
Michael Collins. 2003. Head-Driven Statistical Mod-
els for Natural Language Parsing. Comput. Lin-
guist., 29(4):589–637, December.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. JOURNAL OF THE ROYAL STA-
TISTICAL SOCIETY, SERIES B, 39(1):1–38.
John DeNero and Jakob Uszkoreit. 2011. Inducing
Sentence Structure from Parallel Corpora for Re-
ordering. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 193–203.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast Consensus Decoding over Translation Forests.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages
567–575.
Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings of the
EACL 2014 Workshop on Statistical Machine Trans-
lation.
Chris Dyer and Philip Resnik. 2010. Context-free Re-
ordering, Finite-state Translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ’10, pages 858–
866.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
Evaluation of Translation Quality for Distant Lan-
guage Pairs. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 944–952.
</reference>
<page confidence="0.98987">
52
</page>
<reference confidence="0.998360099099099">
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head Finalization: A Sim-
ple Reordering Rule for SOV Languages. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, WMT ’10,
pages 244–251.
Jason Katz-Brown, Slav Petrov, Ryan McDon-
ald, Franz Och, David Talbot, Hiroshi Ichikawa,
Masakazu Seno, and Hideto Kazawa. 2011. Train-
ing a Parser for Machine Translation Reordering.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
183–192, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Maxim Khalilov and Khalil Sima’an. 2011. Context-
Sensitive Syntactic Source-Reordering by Statistical
Transduction. In IJCNLP, pages 38–46.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration Ses-
sions, ACL ’07, pages 177–180.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proceedings of the Joint Conference
on Human Language Technologies and the Annual
Meeting of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT-NAACL).
K. Lari and S. J. Young. 1990. The Estimation of
Stochastic Context-Free Grammars using the Inside-
Outside Algorithm. Computer Speech and Lan-
guage, 4:35–56.
Uri Lerner and Slav Petrov. 2013. Source-Side Clas-
sifier Preordering for Machine Translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 513–
523. Association for Computational Linguistics.
Gideon Maillette de Buy Wenniger and Khalil Sima’an.
2014. Bilingual Markov Reordering Labels for Hi-
erarchical SMT. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 138–147, Doha, Qatar,
October.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with Latent Annotations.
In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, ACL ’05,
pages 75–82.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a Discriminative Parser to Optimize
Machine Translation Reordering. In Conference on
Empirical Methods in Natural Language Processing
and Natural Language Learning (EMNLP-CoNLL),
pages 843–853, Jeju, Korea, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and
Interpretable Tree Annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Detlef Prescher. 2005. Inducing Head-Driven PCFGs
with Latent Heads: Refining a Tree-Bank Grammar
for Parsing. In In ECML05.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proceedings of
HLT-NAACL 2006. ACL/SIGPARSE, May.
Markus Saers, Karteek Addanki, and Dekai Wu. 2012.
From Finite-State to Inversion Transductions: To-
ward Unsupervised Bilingual Grammar Induction.
In COLING 2012, 24th International Conference on
Computational Linguistics, Proceedings of the Con-
ference: Technical Papers, 8-15 December 2012,
Mumbai, India, pages 2325–2340.
A. Saluja, C. Dyer, and S. B. Cohen. 2014. Latent-
Variable Synchronous CFGs for Hierarchical Trans-
lation. Proceedings of EMNLP.
Khalil Sima’an. 2002. Computational Complex-
ity of Probabilistic Disambiguation. Grammars,
5(2):125–151.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223–231.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ’04, pages 101–104.
Roy Tromble and Jason Eisner. 2009. Learning Linear
Ordering Problems for Better Translation. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1007–1016, Singapore, August.
</reference>
<page confidence="0.986829">
53
</page>
<reference confidence="0.999831434782609">
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan
Melamed. 2006. Empirical lower bounds on the
complexity of translational equivalence. In In Pro-
ceedings of ACL 2006, pages 977–984.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Comput. Linguist., 23(3):377–403, Septem-
ber.
Fei Xia and Michael Mccord. 2004. Improving a
Statistical MT System with Automatically Learned
Rewrite Patterns. In Proceedings of Coling 2004,
pages 508–514, Geneva, Switzerland, August. COL-
ING.
Hao Zhang and Daniel Gildea. 2007. Factorization
of Synchronous Context-Free Grammars in Linear
Time. In NAACL Workshop on Syntax and Structure
in Statistical Translation (SSST), pages 25–32.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting Synchronous Grammar Rules
From Word-Level Alignments in Linear Time. In
Proceedings of the 22nd International Conference
on Computational Linguistics (COLING-08), pages
1081–1088, Manchester, UK.
</reference>
<page confidence="0.999026">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920216">
<title confidence="0.999798">Reordering Grammar Induction</title>
<author confidence="0.999098">Milos Stanojevic Khalil Sima’an</author>
<affiliation confidence="0.9992055">ILLC ILLC University of Amsterdam University of Amsterdam</affiliation>
<email confidence="0.965264">m.stanojevic@uva.nlk.simaan@uva.nl</email>
<abstract confidence="0.998128333333333">We present a novel approach for unsupervised induction of a Reordering Grammar using a modified form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation. Unlike previous approaches, we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora. Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation. Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael H Albert</author>
<author>Mike D Atkinson</author>
</authors>
<date>2005</date>
<booktitle>Simple Permutations and Pattern Restricted Permutations. Discrete Mathematics,</booktitle>
<pages>300--1</pages>
<contexts>
<context position="4248" citStr="Albert and Atkinson, 2005" startWordPosition="666" endWordPosition="669">n the target side, the second becomes 1st, the third 44 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 44–54, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. becomes 4th and the fourth becomes 2nd. The prime permutations are non-factorizable permutations like (1, 2), (2, 1) and (2, 4, 1, 3). We think PETs are suitable for learning preordering for two reasons. Firstly, PETs specify exactly the phrase pairs defined by the permutation. Secondly, every permutation is factorizable into prime permutations only (Albert and Atkinson, 2005). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering. For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only. We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluj</context>
</contexts>
<marker>Albert, Atkinson, 2005</marker>
<rawString>Michael H. Albert and Mike D. Atkinson. 2005. Simple Permutations and Pattern Restricted Permutations. Discrete Mathematics, 300(1-3):1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>Reordering Metrics for MT.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="5991" citStr="Birch and Osborne, 2011" startWordPosition="950" endWordPosition="953">re dedicated to reordering. We face two technical difficulties alien to work on latent PCFGs in treebank parsing. Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank.1 And secondly, after we parse a source string s, we are interested in ´s, the permuted version of s, not in the best derivation/PET. Exact computation is a known NP-Complete problem (Sima’an, 2002). We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (Birch and Osborne, 2011; Isozaki et al., 2010a). In summary, this paper contributes: • A novel latent hierarchical source reordering model working over all derivations of PETs 1All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure (Zhang and Gildea, 2007). • A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations. • A fast Minimum Bayes Risk decoding over Kendall T reordering score for selecting ´s. We report results for extensive experiments o</context>
<context position="15549" citStr="Birch and Osborne, 2011" startWordPosition="2590" endWordPosition="2593">PCFG. Unfortunately, computing the most-likely permutation (or alternatively ´s) as in X X argmax P(d, 7rm) 7rEΠ DEPEF(7r) dED from a lattice of permutations H using a PCFG is NP-complete (Sima’an, 2002). Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here. Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004): Loss(7r, 7rr)P(7rr) (3) The loss function we minimize is Kendall T (Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (including gapped pairs) to the total number of pairs. We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss. We sample from the true distribution by sampling edges recursively 7rˆ = argmin 7r X 7rr 47 using their inside probabilities. An empirical distribution over permutations P(π) is given by the relative frequency of π in the sample. With large samples it is hard to efficiently compute expected Kendall τ loss for each sampled hypothes</context>
</contexts>
<marker>Birch, Osborne, 2011</marker>
<rawString>Alexandra Birch and Miles Osborne. 2011. Reordering Metrics for MT. In Proceedings of the Association for Computational Linguistics, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Mats Rooth</author>
</authors>
<title>Valence Induction with a Head-Lexicalized PCFG. In</title>
<date>1998</date>
<booktitle>In Proceedings of Third Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11127" citStr="Carroll and Rooth, 1998" startWordPosition="1842" endWordPosition="1845">ght. For example P2121 means that this node is a second child, and the mother nonterminal label is P211. For the running example rule, this gives the following rules: P211 → P211 1 P212 1 P212 → P2112 P2122 P2111 → P121 P2121 → P24131 P2111 → P122 P212 1 → P24132 P2112 → P121 P212 2 → P 24131 P2112 → P122 P212 2 → P 24132 The unary trick leads to substantial reduction in grammar size, e.g., for arity 5 rules and 30 splits we could have had 306 = 729000000 split-rules, but with the unary trick we only have 30+302∗5 = 4530 split rules. The unary trick was used in early lexicalized parsing work (Carroll and Rooth, 1998).2 This split PCFG constitutes a latent PCFG because the splits cannot be read of a treebank. It must be learned from the latent treebank of PETs, as described next. Figure 2: Permutation Tree with unary trick 3 Details of Latent Reordering PCFG Obtaining permutations Given a source sentence s and its alignment a to a target sentence 2After applying the unary trick, we add a constraint on splitting: all nonterminals on an n-ary branching rule must be split simultaneously. Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken Professor Chomsky , I would like to thank you P121 P122 </context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Glenn Carroll and Mats Rooth. 1998. Valence Induction with a Head-Lexicalized PCFG. In In Proceedings of Third Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A Generalized CYK Algorithm for Parsing Stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proceedings of the First Workshop on Tabulation in Parsing and Deduction,</booktitle>
<pages>133--137</pages>
<contexts>
<context position="14858" citStr="Chappelier and Rajman, 1998" startWordPosition="2473" endWordPosition="2476"> of the parallel corpus instances. Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990). As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry. We split the non-terminals only once as in (Matsuzaki et al., 2005) (unlike (Petrov et al., 2006)). For estimating the distribution for unknown words we replace all words that appear ≤ 3 times with the “UNKNOWN” token. 3.3 Inference We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG. Unfortunately, computing the most-likely permutation (or alternatively ´s) as in X X argmax P(d, 7rm) 7rEΠ DEPEF(7r) dED from a lattice of permutations H using a PCFG is NP-complete (Sima’an, 2002). Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here. Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004): </context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-C´edric Chappelier and Martin Rajman. 1998. A Generalized CYK Algorithm for Parsing Stochastic CFG. In Proceedings of the First Workshop on Tabulation in Parsing and Deduction, pages 133–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>427--436</pages>
<contexts>
<context position="20872" citStr="Cherry and Foster, 2012" startWordPosition="3520" endWordPosition="3523">based 0.7567 CLADER 0.8176 RGleft-branching 0.8201 RGright-branching 0.8246 RGITG-forest 0.823 RGPET-forest 0.8255 Table 2: Reordering prediction 4.2 Extrinsic evaluation in MT The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 s´ − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics. Single or all PETs? In Table 3 we see that using all PETs during training makes a big impact on performance. Only the all PETs variants 7Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed s´ − t pairs, which are t</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 427–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="25903" citStr="Chiang, 2005" startWordPosition="4316" endWordPosition="4317">gly, the MSD model (BLEU 29.6) improves over distance-based reordering (BLEU 27.8) by (BLEU 1.8), whereas the difference between these systems as back-ends to Reordering Grammar (respectively BLEU 32.4 and 32.0) is far smaller (0.4 BLEU). This suggests that a major share of reorderings can be handled well by preordering without conditioning on target lexical choice. Furthermore, this shows that RGPET-forest preordering is not very sensitive to the decoder’s reordering model. Comparison to a Hierarchical model (Hiero). Hierarchical preordering is not intended for a hierarchical model as Hiero (Chiang, 2005). Yet, here we compare our preordering system (PB MSD+RG) to Hiero for completeness, while we should keep in mind that Hiero’s reordering model has access to much richer training data. We will discuss these differences shortly. Table 4 shows that the difference in BLEU is not statistically significant, but there is more difference in METEOR and TER. RIBES, which concentrates more on reordering, prefers Reordering Grammar over Hiero. It is somewhat surprising that a preordering model combined with a phrase-based model succeeds to rival Hiero’s performance on English-Japanese. Especially when lo</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Remarks on Nominalization. In</title>
<date>1970</date>
<booktitle>Readings in English Transformational Grammar,</booktitle>
<pages>184--221</pages>
<editor>Roderick A. Jacobs and Peter S. Rosenbaum, editors,</editor>
<publisher>Ginn,</publisher>
<location>Boston.</location>
<contexts>
<context position="12552" citStr="Chomsky, 1970" startWordPosition="2081" endWordPosition="2082"> minimal phrases constitutes a permutation 7rm. We do not extract non-contiguous or non-minimal phrases because reordering them often involves complicated transductions which could hamper the performance of our learning algorithm.3 Unaligned words Next we describe the use of the factorization of permutations into PET forests for training a PCFG model. But first we need to extend the PETs to allow for unaligned words. An unaligned word is joined with a neighboring phrase to the left or the right, depending on the source language properties (e.g., whether the language is head-initial or -final (Chomsky, 1970)). Our experiments use English as source language (head-initial), so the unaligned words are joined to phrases to their right. This modifies a PET by adding a new binary branching node p (dominating the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P01 if the unaligned word joins to the right and P10 if it joins to the left. 3.1 Probability model We decompose the permutation 7rm into a forest of permutation trees PEF(7rm) in O(n3), following algorithms in (Zhang et al., 2008; Zhang and Gildea, 2007) with trivial modifications. Each PET 0 E PEF(7r</context>
</contexts>
<marker>Chomsky, 1970</marker>
<rawString>Noam Chomsky. 1970. Remarks on Nominalization. In Roderick A. Jacobs and Peter S. Rosenbaum, editors, Readings in English Transformational Grammar, pages 184–221. Ginn, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="9549" citStr="Collins, 2003" startWordPosition="1554" endWordPosition="1555">, which is a major restriction when used on the data: there are many nonbinarizable permutations in actual data (Wellington et al., 2006). In contrast, our PETs are obtained by factorizing permutations obtained from the data, i.e., they exactly fit the range of prime permutations in the parallel corpus. In practice we limit them to maximum arity 5. We can extract PCFG rules from the PETs, e.g., P21 → P12 P2413. However, these rules are decorated with too coarse labels. A similar problem was encountered in non-lexicalized monolingual parsing, and one solution was to lexicalize the productions (Collins, 2003) using head words. But linguistic heads do not make sense for PETs, so we opt for the alternative approach (Matsuzaki et al., 2005), which splits the nonterminals and softly percolates the splits through the trees gradually fitting them to the training data. Splitting has a shadow side, however, because it leads to combinatorial explosion in grammar size. Suppose for example node P21 could split into P211 and P212 and similarly P2413 splits into P24131 and 24132. This means that rule P21 → P12 P2413 will form eight new rules: P211 → P121 P24131 P211 → P121 P24132 P211 → P122 P24131 P211 → P122</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. Comput. Linguist., 29(4):589–637, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="14163" citStr="Dempster et al., 1977" startWordPosition="2359" endWordPosition="2362">est PEF(7rm), and for every PET also over all its label splits, which are given by the grammar derivations d: XP (sm, 7rm) = X P(d, sm) (1) DEPEF(7r,) dED The probability of a derivation d is a product of probabilities of all the rules r that build it: XP(sm,7rm) = X Y P(r) (2) DEPEF(7r,) dED rEd 3Which differs from (Quirk and Menezes, 2006). As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next. 3.2 Learning Splits on Latent Treebank For training the latent PCFG over the latent treebank, we resort to EM (Dempster et al., 1977) which estimates PCFG rule probabilities to maximize the likelihood of the parallel corpus instances. Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990). As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry. We split the non-terminals only once as in (Matsuzaki et al., 2005) (unlike (Petrov et al., 2006)). For estimating the distribution for unknown words we replace all words that app</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing Sentence Structure from Parallel Corpora for Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>193--203</pages>
<contexts>
<context position="2189" citStr="DeNero and Uszkoreit, 2011" startWordPosition="336" endWordPosition="340">o use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004). The (direct correspondence) assumption underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order. An alternative approach creates reordering rules manually and then learns the right structure for applying these rules (Katz-Brown et al., 2011). Others attempt learning the transduction structure and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011). Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned parallel corpora. Learning both trees and transductions jointly raises two questions. How to obtain suitable trees for the source sentence and how to learn a distribution over random variables specifically aimed at reordering in a hierarchical model? In this work we solve both challenges by using the factorizations of permutations into Permutation Trees (PETs) (Zhang and Gildea, 2007). As we explain next, PETs can be crucial for exposing the hierarchical reord</context>
<context position="28125" citStr="DeNero and Uszkoreit (2011)" startWordPosition="4678" endWordPosition="4681">and Sima’an, 2011; Xia and Mccord, 2004). Here we concentrate on work that has common aspects with this work. Neubig et 50 Figure 4: Example parse of English sentence that predicts reordering for English-Japanese P12 P12 P21 P21 P21 P21 DC current i flowing P12 P12 through P12 P01 P12 the feeding conductor 3 produces magnetic P12 field b1 P12 . al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations. Tromble and Eisner (2009) use ITG but do not train the grammar. They only use it to constrain the local search. DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it. In contrast, here we learn both the structure and the reordering function simultaneously. Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process – the parser forwards to the next stage the best parse. Dyer and Resnik (2010) treat reordering as a latent var</context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing Sentence Structure from Parallel Corpora for Reordering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 193–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>David Chiang</author>
<author>Kevin Knight</author>
</authors>
<title>Fast Consensus Decoding over Translation Forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>567--575</pages>
<contexts>
<context position="16604" citStr="DeNero et al., 2009" startWordPosition="2783" endWordPosition="2786">ns P(π) is given by the relative frequency of π in the sample. With large samples it is hard to efficiently compute expected Kendall τ loss for each sampled hypothesis. For sentence of length k and sample of size n the complexity of a naive algorithm is O(n2k2). Computing Kendall τ alone takes O(k2). We use the fact that Kendall τ decomposes as a linear function over all skip-bigrams b that could be built for any permutation of length k: �Kendall(π, πr) = b Here δ returns 1 if permutation π contains the skip bigram b, otherwise it returns 0. With this decomposition we can use the method from (DeNero et al., 2009) to efficiently compute the MBR hypothesis. Combining Equations 3 and 4 we get: δ(πr, b)P(πr) (5) We can move the summation inside and reformulate the expected Kendall τ loss as expectation over the skip-bigrams of the permutation. L � J (1 − δ(π, b)) πr δ(πr, b)P(πr) (6) (1 − δ(π, b))EP(πr)δ(πr, b) (7) • Baseline C: LADER (Neubig et al., 2012): latent variable preordering that is based on ITG and large-margin training with latent variables. We used LADER in standard settings without any linguistic features (POS tags or syntactic trees). And we test four variants of our model: • RGleft - only </context>
</contexts>
<marker>DeNero, Chiang, Knight, 2009</marker>
<rawString>John DeNero, David Chiang, and Kevin Knight. 2009. Fast Consensus Decoding over Translation Forests. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2 - Volume 2, ACL ’09, pages 567–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor Universal: Language Specific Translation Evaluation for Any Target Language.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="21044" citStr="Denkowski and Lavie, 2014" startWordPosition="3547" endWordPosition="3550"> in MT The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 s´ − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics. Single or all PETs? In Table 3 we see that using all PETs during training makes a big impact on performance. Only the all PETs variants 7Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed s´ − t pairs, which are the word re-aligned and then used for training the back-end MT system (Khalilov and Sima’an, 2011). We skip this, we take the risk of mismatch between the preordering and th</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor Universal: Language Specific Translation Evaluation for Any Target Language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Context-free Reordering, Finite-state Translation. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>858--866</pages>
<contexts>
<context position="28692" citStr="Dyer and Resnik (2010)" startWordPosition="4768" endWordPosition="4771">constrain the local search. DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it. In contrast, here we learn both the structure and the reordering function simultaneously. Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process – the parser forwards to the next stage the best parse. Dyer and Resnik (2010) treat reordering as a latent variable and try to sum over all derivations that lead not only to the same reordering but also to the same translation. In their work they consider all permutations allowed by a given syntactic tree. Saers et al (2012) induce synchronous grammar for translation by splitting the non-terminals, but unlike our approach they split generic nonterminals and not operators. Their most expressive grammar covers only binarizable permutations. The decoder that uses this model does not try to sum over many derivations that have the same yield. They do not make independence a</context>
</contexts>
<marker>Dyer, Resnik, 2010</marker>
<rawString>Chris Dyer and Philip Resnik. 2010. Context-free Reordering, Finite-state Translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 858– 866.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
</authors>
<title>Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic Evaluation of Translation Quality for Distant Language Pairs.</title>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>944--952</pages>
<marker>Isozaki, Hirao, Duh, </marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010a. Automatic Evaluation of Translation Quality for Distant Language Pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 944–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>Head Finalization: A Simple Reordering Rule for SOV Languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>244--251</pages>
<contexts>
<context position="6013" citStr="Isozaki et al., 2010" startWordPosition="954" endWordPosition="957">g. We face two technical difficulties alien to work on latent PCFGs in treebank parsing. Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank.1 And secondly, after we parse a source string s, we are interested in ´s, the permuted version of s, not in the best derivation/PET. Exact computation is a known NP-Complete problem (Sima’an, 2002). We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (Birch and Osborne, 2011; Isozaki et al., 2010a). In summary, this paper contributes: • A novel latent hierarchical source reordering model working over all derivations of PETs 1All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure (Zhang and Gildea, 2007). • A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations. • A fast Minimum Bayes Risk decoding over Kendall T reordering score for selecting ´s. We report results for extensive experiments on English-Japanese sho</context>
<context position="15571" citStr="Isozaki et al., 2010" startWordPosition="2594" endWordPosition="2597">uting the most-likely permutation (or alternatively ´s) as in X X argmax P(d, 7rm) 7rEΠ DEPEF(7r) dED from a lattice of permutations H using a PCFG is NP-complete (Sima’an, 2002). Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here. Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004): Loss(7r, 7rr)P(7rr) (3) The loss function we minimize is Kendall T (Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (including gapped pairs) to the total number of pairs. We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss. We sample from the true distribution by sampling edges recursively 7rˆ = argmin 7r X 7rr 47 using their inside probabilities. An empirical distribution over permutations P(π) is given by the relative frequency of π in the sample. With large samples it is hard to efficiently compute expected Kendall τ loss for each sampled hypothesis. For sentence of le</context>
<context position="18723" citStr="Isozaki et al., 2010" startWordPosition="3169" endWordPosition="3172">950k 25M 30M tune translation 2k 55K 66K test translation 3k 78K 93K 1 − δ(π, b) δ(πr, b) (4) k(k−1) 2 πˆ = argmin E � 1 − δ(π, b) π πr b k(k−1) 2 = argmin π � b = argmin π � b = argmax � δ(π, b)EP(πr)δ(πr, b) (8) Table 1: Data stats π b This means we need to pass through the sampled list only twice: (1) to compute expectations over skip bigrams and (2) to compute expected loss of each sampled permutation. The time complexity is O(nk2) which is quite fast in practice. 4 Experiments We conduct experiments with three baselines: • Baseline A: No preordering. • Baseline B: Rule based preordering (Isozaki et al., 2010b), which first obtains an HPSG parse tree using Enju parser 4 and after that swaps the children by moving the syntactic head to the final position to account for different head orientation in English and Japanese. 4http://www.nactem.ac.uk/enju/ The Reordering Grammar was trained for 10 iterations of EM on train RG data. We use 30 splits for binary non-terminals and 3 for non-binary. Training on this dataset takes 2 days and parsing tuning and testing set without any pruning takes 11 and 18 hours respectively. 4.1 Intrinsic evaluation We test how well our model predicts gold reorderings before</context>
<context position="21132" citStr="Isozaki et al., 2010" startWordPosition="3562" endWordPosition="3565">lated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 s´ − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics. Single or all PETs? In Table 3 we see that using all PETs during training makes a big impact on performance. Only the all PETs variants 7Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed s´ − t pairs, which are the word re-aligned and then used for training the back-end MT system (Khalilov and Sima’an, 2011). We skip this, we take the risk of mismatch between the preordering and the back-end system, but this simplifies training and saves a good amount of training time</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010b. Head Finalization: A Simple Reordering Rule for SOV Languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 244–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Katz-Brown</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Franz Och</author>
<author>David Talbot</author>
<author>Hiroshi Ichikawa</author>
<author>Masakazu Seno</author>
<author>Hideto Kazawa</author>
</authors>
<title>Training a Parser for Machine Translation Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>183--192</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2043" citStr="Katz-Brown et al., 2011" startWordPosition="316" endWordPosition="319">s often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it. A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004). The (direct correspondence) assumption underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order. An alternative approach creates reordering rules manually and then learns the right structure for applying these rules (Katz-Brown et al., 2011). Others attempt learning the transduction structure and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011). Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned parallel corpora. Learning both trees and transductions jointly raises two questions. How to obtain suitable trees for the source sentence and how to learn a distribution over random variables specifically aimed at reordering in a hierarchical model? In this work we solve both challenges by using the factorizations o</context>
</contexts>
<marker>Katz-Brown, Petrov, McDonald, Och, Talbot, Ichikawa, Seno, Kazawa, 2011</marker>
<rawString>Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz Och, David Talbot, Hiroshi Ichikawa, Masakazu Seno, and Hideto Kazawa. 2011. Training a Parser for Machine Translation Reordering. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxim Khalilov</author>
<author>Khalil Sima’an</author>
</authors>
<title>ContextSensitive Syntactic Source-Reordering by Statistical Transduction.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>38--46</pages>
<marker>Khalilov, Sima’an, 2011</marker>
<rawString>Maxim Khalilov and Khalil Sima’an. 2011. ContextSensitive Syntactic Source-Reordering by Statistical Transduction. In IJCNLP, pages 38–46.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondˇrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="1404" citStr="Koehn et al., 2007" startWordPosition="212" endWordPosition="215"> rules, and (4) requires no linguistic annotation. Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese. 1 Introduction Preordering (Collins et al., 2005) aims at permuting the words of a source sentence s into a new order ´s, hopefully close to a plausible target word order. Preordering is often used to bridge long distance reorderings (e.g., in Japanese- or GermanEnglish), before applying phrase-based models (Koehn et al., 2007). Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it. A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004). The (direct correspondence) assumption underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order. An alternative approach creates reordering rules manually and then learns the right structure for applyin</context>
<context position="20566" citStr="Koehn et al., 2007" startWordPosition="3461" endWordPosition="3464">s a lower score than no preordering, which might be an artifact of the Enju parser changing the tokenization of its input, so the Kendall T of this system might not really reflect the real quality of the preordering. All other systems use the same tokenization. Kendall T ANo preordering 0.7655 BRule based 0.7567 CLADER 0.8176 RGleft-branching 0.8201 RGright-branching 0.8246 RGITG-forest 0.823 RGPET-forest 0.8255 Table 2: Reordering prediction 4.2 Extrinsic evaluation in MT The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 s´ − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order</context>
<context position="23753" citStr="Koehn et al., 2007" startWordPosition="3977" endWordPosition="3980">ause these operators often appear high in the predicted PET. Furthermore, having these operators during training might allow for better fit to the data. How much reordering is resolved by the Reordering Grammar? Obviously, completely factorizing out the reordering from the translation process is impossible because reordering depends to a certain degree on target lexical choice. To quantify the contribution of Reordering Grammar, we tested decoding with different distortion limit values in the SMT system. We compare the phrase-based (PB) system with distance based cost function for reordering (Koehn et al., 2007) with and without preordering. Figure 3 shows that Reordering Grammar gives substantial performance improvements at all distortion limits (both BLEU and RIBES). RGPET-forest is less sensitive to changes in decoder distortion limit than standard PBSMT. The perfor49 Figure 3: Distortion effect on BLEU and RIBES mance of RGPET-forest varies only by 1.1 BLEU points while standard PBSMT by 4.3 BLEU points. Some local reordering in the decoder seems to help RGPET-forest but large distortion limits seem to degrade the preordering choice. This shows also that the improved performance of RGPET-forest i</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-Risk Decoding for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="15456" citStr="Kumar and Byrne, 2004" startWordPosition="2575" endWordPosition="2578">pelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG. Unfortunately, computing the most-likely permutation (or alternatively ´s) as in X X argmax P(d, 7rm) 7rEΠ DEPEF(7r) dED from a lattice of permutations H using a PCFG is NP-complete (Sima’an, 2002). Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here. Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004): Loss(7r, 7rr)P(7rr) (3) The loss function we minimize is Kendall T (Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (including gapped pairs) to the total number of pairs. We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss. We sample from the true distribution by sampling edges recursively 7rˆ = argmin 7r X 7rr 47 using their inside probabilities. An empirical distribution over permutations P(π) is given by the relative frequency of π in the sample. With larg</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The Estimation of Stochastic Context-Free Grammars using the InsideOutside Algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="14358" citStr="Lari and Young, 1990" startWordPosition="2388" endWordPosition="2391">oduct of probabilities of all the rules r that build it: XP(sm,7rm) = X Y P(r) (2) DEPEF(7r,) dED rEd 3Which differs from (Quirk and Menezes, 2006). As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next. 3.2 Learning Splits on Latent Treebank For training the latent PCFG over the latent treebank, we resort to EM (Dempster et al., 1977) which estimates PCFG rule probabilities to maximize the likelihood of the parallel corpus instances. Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990). As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry. We split the non-terminals only once as in (Matsuzaki et al., 2005) (unlike (Petrov et al., 2006)). For estimating the distribution for unknown words we replace all words that appear ≤ 3 times with the “UNKNOWN” token. 3.3 Inference We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG. Unfortunately, computing t</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The Estimation of Stochastic Context-Free Grammars using the InsideOutside Algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Lerner</author>
<author>Slav Petrov</author>
</authors>
<title>Source-Side Classifier Preordering for Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>513--523</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1705" citStr="Lerner and Petrov, 2013" startWordPosition="263" endWordPosition="266">duction Preordering (Collins et al., 2005) aims at permuting the words of a source sentence s into a new order ´s, hopefully close to a plausible target word order. Preordering is often used to bridge long distance reorderings (e.g., in Japanese- or GermanEnglish), before applying phrase-based models (Koehn et al., 2007). Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it. A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004). The (direct correspondence) assumption underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order. An alternative approach creates reordering rules manually and then learns the right structure for applying these rules (Katz-Brown et al., 2011). Others attempt learning the transduction structure and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011). Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop</context>
<context position="27487" citStr="Lerner and Petrov, 2013" startWordPosition="4569" endWordPosition="4572">, e.g., a language model, while Reordering Grammar is a mere generative PCFG. The advantages of Hiero can be brought to bear upon Reordering Grammar by reformulating it as a discriminative model. Which structure is learned? Figure 4 shows an example PET output showing how our model learns: (1) that the article “the” has no equivalent in Japanese, (2) that verbs go after their object, (3) to use postpositions instead of prepositions, and (4) to correctly group certain syntactic units, e.g. NPs and VPs. 5 Related work The majority of work on preordering is based on syntactic parse trees, e.g., (Lerner and Petrov, 2013; Khalilov and Sima’an, 2011; Xia and Mccord, 2004). Here we concentrate on work that has common aspects with this work. Neubig et 50 Figure 4: Example parse of English sentence that predicts reordering for English-Japanese P12 P12 P21 P21 P21 P21 DC current i flowing P12 P12 through P12 P01 P12 the feeding conductor 3 produces magnetic P12 field b1 P12 . al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations. Tromble and Eisner (2009) use ITG but do not train the grammar. They only use it to constrain the loc</context>
</contexts>
<marker>Lerner, Petrov, 2013</marker>
<rawString>Uri Lerner and Slav Petrov. 2013. Source-Side Classifier Preordering for Machine Translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 513– 523. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Maillette de Buy Wenniger</author>
<author>Khalil Sima’an</author>
</authors>
<title>Bilingual Markov Reordering Labels for Hierarchical SMT.</title>
<date>2014</date>
<booktitle>In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>138--147</pages>
<location>Doha, Qatar,</location>
<marker>Wenniger, Sima’an, 2014</marker>
<rawString>Gideon Maillette de Buy Wenniger and Khalil Sima’an. 2014. Bilingual Markov Reordering Labels for Hierarchical SMT. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 138–147, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with Latent Annotations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="4804" citStr="Matsuzaki et al., 2005" startWordPosition="754" endWordPosition="757">torizable into prime permutations only (Albert and Atkinson, 2005). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering. For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only. We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014). Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s). Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations. After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version s´ of s. In this sense, our latent splits are dedicated to reordering. We face </context>
<context position="9680" citStr="Matsuzaki et al., 2005" startWordPosition="1575" endWordPosition="1578">n et al., 2006). In contrast, our PETs are obtained by factorizing permutations obtained from the data, i.e., they exactly fit the range of prime permutations in the parallel corpus. In practice we limit them to maximum arity 5. We can extract PCFG rules from the PETs, e.g., P21 → P12 P2413. However, these rules are decorated with too coarse labels. A similar problem was encountered in non-lexicalized monolingual parsing, and one solution was to lexicalize the productions (Collins, 2003) using head words. But linguistic heads do not make sense for PETs, so we opt for the alternative approach (Matsuzaki et al., 2005), which splits the nonterminals and softly percolates the splits through the trees gradually fitting them to the training data. Splitting has a shadow side, however, because it leads to combinatorial explosion in grammar size. Suppose for example node P21 could split into P211 and P212 and similarly P2413 splits into P24131 and 24132. This means that rule P21 → P12 P2413 will form eight new rules: P211 → P121 P24131 P211 → P121 P24132 P211 → P122 P24131 P211 → P122 P24132 P212 → P121 P24131 P212 → P121 P24132 P212 → P122 P24131 P212 → P122 P24132 Should we want to split each nonterminal into 3</context>
<context position="14419" citStr="Matsuzaki et al., 2005" startWordPosition="2398" endWordPosition="2401">(sm,7rm) = X Y P(r) (2) DEPEF(7r,) dED rEd 3Which differs from (Quirk and Menezes, 2006). As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next. 3.2 Learning Splits on Latent Treebank For training the latent PCFG over the latent treebank, we resort to EM (Dempster et al., 1977) which estimates PCFG rule probabilities to maximize the likelihood of the parallel corpus instances. Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990). As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry. We split the non-terminals only once as in (Matsuzaki et al., 2005) (unlike (Petrov et al., 2006)). For estimating the distribution for unknown words we replace all words that appear ≤ 3 times with the “UNKNOWN” token. 3.3 Inference We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG. Unfortunately, computing the most-likely permutation (or alternatively ´s) as in X X ar</context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with Latent Annotations. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 75–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
</authors>
<title>Inducing a Discriminative Parser to Optimize Machine Translation Reordering.</title>
<date>2012</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing and Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>843--853</pages>
<location>Jeju, Korea,</location>
<contexts>
<context position="16950" citStr="Neubig et al., 2012" startWordPosition="2847" endWordPosition="2850"> a linear function over all skip-bigrams b that could be built for any permutation of length k: �Kendall(π, πr) = b Here δ returns 1 if permutation π contains the skip bigram b, otherwise it returns 0. With this decomposition we can use the method from (DeNero et al., 2009) to efficiently compute the MBR hypothesis. Combining Equations 3 and 4 we get: δ(πr, b)P(πr) (5) We can move the summation inside and reformulate the expected Kendall τ loss as expectation over the skip-bigrams of the permutation. L � J (1 − δ(π, b)) πr δ(πr, b)P(πr) (6) (1 − δ(π, b))EP(πr)δ(πr, b) (7) • Baseline C: LADER (Neubig et al., 2012): latent variable preordering that is based on ITG and large-margin training with latent variables. We used LADER in standard settings without any linguistic features (POS tags or syntactic trees). And we test four variants of our model: • RGleft - only canonical left branching PET • RGright - only canonical right branching PET • RGITG-forest - all PETs that are binary (ITG) • RGPET-forest - all PETs. We test these models on English-Japanese NTCIR-8 Patent Translation (PATMT) Task. For tuning we use all NTCIR-7 dev sets and for testing the test set from NTCIR-9 from both directions. All used d</context>
</contexts>
<marker>Neubig, Watanabe, Mori, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, and Shinsuke Mori. 2012. Inducing a Discriminative Parser to Optimize Machine Translation Reordering. In Conference on Empirical Methods in Natural Language Processing and Natural Language Learning (EMNLP-CoNLL), pages 843–853, Jeju, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="21008" citStr="Papineni et al., 2002" startWordPosition="3542" endWordPosition="3545">diction 4.2 Extrinsic evaluation in MT The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 s´ − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics. Single or all PETs? In Table 3 we see that using all PETs during training makes a big impact on performance. Only the all PETs variants 7Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed s´ − t pairs, which are the word re-aligned and then used for training the back-end MT system (Khalilov and Sima’an, 2011). We skip this, we take the risk of mis</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="15272" citStr="Petrov and Klein, 2007" startWordPosition="2541" endWordPosition="2544">5) (unlike (Petrov et al., 2006)). For estimating the distribution for unknown words we replace all words that appear ≤ 3 times with the “UNKNOWN” token. 3.3 Inference We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG. Unfortunately, computing the most-likely permutation (or alternatively ´s) as in X X argmax P(d, 7rm) 7rEΠ DEPEF(7r) dED from a lattice of permutations H using a PCFG is NP-complete (Sima’an, 2002). Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here. Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004): Loss(7r, 7rr)P(7rr) (3) The loss function we minimize is Kendall T (Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (including gapped pairs) to the total number of pairs. We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss. We sample from the true distribution by sampling edges </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4841" citStr="Petrov et al., 2006" startWordPosition="760" endWordPosition="763">Albert and Atkinson, 2005). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering. For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only. We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014). Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s). Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations. After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version s´ of s. In this sense, our latent splits are dedicated to reordering. We face two technical difficulties alien to w</context>
<context position="14681" citStr="Petrov et al., 2006" startWordPosition="2443" endWordPosition="2446">reebank For training the latent PCFG over the latent treebank, we resort to EM (Dempster et al., 1977) which estimates PCFG rule probabilities to maximize the likelihood of the parallel corpus instances. Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990). As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry. We split the non-terminals only once as in (Matsuzaki et al., 2005) (unlike (Petrov et al., 2006)). For estimating the distribution for unknown words we replace all words that appear ≤ 3 times with the “UNKNOWN” token. 3.3 Inference We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG. Unfortunately, computing the most-likely permutation (or alternatively ´s) as in X X argmax P(d, 7rm) 7rEΠ DEPEF(7r) dED from a lattice of permutations H using a PCFG is NP-complete (Sima’an, 2002). Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detlef Prescher</author>
</authors>
<title>Inducing Head-Driven PCFGs with Latent Heads: Refining a Tree-Bank Grammar for Parsing. In</title>
<date>2005</date>
<booktitle>In ECML05.</booktitle>
<contexts>
<context position="4820" citStr="Prescher, 2005" startWordPosition="758" endWordPosition="759">mutations only (Albert and Atkinson, 2005). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering. For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only. We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014). Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s). Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations. After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version s´ of s. In this sense, our latent splits are dedicated to reordering. We face two technical di</context>
</contexts>
<marker>Prescher, 2005</marker>
<rawString>Detlef Prescher. 2005. Inducing Head-Driven PCFGs with Latent Heads: Refining a Tree-Bank Grammar for Parsing. In In ECML05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<publisher>ACL/SIGPARSE,</publisher>
<contexts>
<context position="13884" citStr="Quirk and Menezes, 2006" startWordPosition="2311" endWordPosition="2314">en in the latent treebank, and apply unsupervised learning to induce a distribution over possible bracketings. Our probability model starts from the joint probability of a sequence of minimal phrases sm and a permutation 7rm over it. This demands summing over all PETs 0 in the forest PEF(7rm), and for every PET also over all its label splits, which are given by the grammar derivations d: XP (sm, 7rm) = X P(d, sm) (1) DEPEF(7r,) dED The probability of a derivation d is a product of probabilities of all the rules r that build it: XP(sm,7rm) = X Y P(r) (2) DEPEF(7r,) dED rEd 3Which differs from (Quirk and Menezes, 2006). As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next. 3.2 Learning Splits on Latent Treebank For training the latent PCFG over the latent treebank, we resort to EM (Dempster et al., 1977) which estimates PCFG rule probabilities to maximize the likelihood of the parallel corpus instances. Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990). As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation. In Proceedings of HLT-NAACL 2006. ACL/SIGPARSE, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Karteek Addanki</author>
<author>Dekai Wu</author>
</authors>
<title>From Finite-State to Inversion Transductions: Toward Unsupervised Bilingual Grammar Induction.</title>
<date>2012</date>
<booktitle>In COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers,</booktitle>
<pages>8--15</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="28941" citStr="Saers et al (2012)" startWordPosition="4813" endWordPosition="4816">nd the reordering function simultaneously. Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process – the parser forwards to the next stage the best parse. Dyer and Resnik (2010) treat reordering as a latent variable and try to sum over all derivations that lead not only to the same reordering but also to the same translation. In their work they consider all permutations allowed by a given syntactic tree. Saers et al (2012) induce synchronous grammar for translation by splitting the non-terminals, but unlike our approach they split generic nonterminals and not operators. Their most expressive grammar covers only binarizable permutations. The decoder that uses this model does not try to sum over many derivations that have the same yield. They do not make independence assumption like our “unary trick” which is probably the reason they do not split more than 8 times. They do not compare their results to any other SMT system and test on a very small dataset. Saluja et al (2014) attempts inducing a refined Hiero gram</context>
</contexts>
<marker>Saers, Addanki, Wu, 2012</marker>
<rawString>Markus Saers, Karteek Addanki, and Dekai Wu. 2012. From Finite-State to Inversion Transductions: Toward Unsupervised Bilingual Grammar Induction. In COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, 8-15 December 2012, Mumbai, India, pages 2325–2340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Saluja</author>
<author>C Dyer</author>
<author>S B Cohen</author>
</authors>
<title>LatentVariable Synchronous CFGs for Hierarchical Translation.</title>
<date>2014</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4863" citStr="Saluja et al., 2014" startWordPosition="764" endWordPosition="767">2005). Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering. We expect this to be advantageous for learning hierarchical reordering. For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only. We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes. Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014). Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s). Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations. After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version s´ of s. In this sense, our latent splits are dedicated to reordering. We face two technical difficulties alien to work on latent PCFGs in</context>
<context position="22678" citStr="Saluja et al., 2014" startWordPosition="3814" endWordPosition="3817">rest 32.0ABC 51.0ABC 55.7ABC 75.62 Table 3: Comparison of different preordering models. Superscripts A, B and C signify if the system is significantly better (p &lt; 0.05) than the respective baseline or significantly worse (in which case it is a subscript). Significance tests were not computed for RIBES. Score is bold if the system is significantly better than all the baselines. (RGITG-forest and RGPET-forest) significantly outperform all baselines. If we are to choose a single PET per training instance, then learning RG from only left-branching PETs (the one usually chosen in other work, e.g. (Saluja et al., 2014)) performs slightly worse than the right-branching PET. This is possibly because English is mostly rightbranching. So even though both PETs describe the same reordering, RGright captures reordering over English input better than RGleft. All PETs or binary only? RGPET-forest performs significantly better than RGITG-forest (p &lt; 0.05). Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET. Furthermore, having these operators during training might allow for better fit to t</context>
<context position="29502" citStr="Saluja et al (2014)" startWordPosition="4911" endWordPosition="4914">ions allowed by a given syntactic tree. Saers et al (2012) induce synchronous grammar for translation by splitting the non-terminals, but unlike our approach they split generic nonterminals and not operators. Their most expressive grammar covers only binarizable permutations. The decoder that uses this model does not try to sum over many derivations that have the same yield. They do not make independence assumption like our “unary trick” which is probably the reason they do not split more than 8 times. They do not compare their results to any other SMT system and test on a very small dataset. Saluja et al (2014) attempts inducing a refined Hiero grammar (latent synchronous CFG) from Normalized Decomposition Trees (NDT) (Zhang et al., 2008). While there are similarities with the present work, there are major differences. On the similarity side, NDTs are decomposing alignments in ways similar to PETs, and both Saluja’s and our models refine the labels on the nodes of these decompositions. However, there are major differences between the two: • Our model is completely monolingual and unlexicalized (does not condition its reordering on the translation) in contrast with the Latent SCFG used in (Saluja et </context>
<context position="30725" citStr="Saluja et al., 2014" startWordPosition="5111" endWordPosition="5114">, 2014), • Our Latent PCFG label splits are defined as refinements of prime permutations, i.e., specifically designed for learning reordering, whereas (Saluja et al., 2014) aims at learning label splitting that helps predicting NDTs from source sentences, • Our model exploits all PETs and all derivations, both during training (latent treebank) and during inferences. In (Saluja et al., 2014) only left branching NDT derivations are used for learning the model. • The training data used by (Saluja et al., 2014) is about 60 times smaller in number of words than the data used here; the test set of (Saluja et al., 2014) also consists of far shorter sentences where reordering could be less crucial. A related work with a similar intuition is presented in (Maillette de Buy Wenniger and Sima’an, 2014), where nodes of a tree structure similar to PETs are labeled with reordering patterns obtained by factorizing word alignments into Hierarchical Alignment Trees. These patterns are used for labeling the standard Hiero grammar. Unlike this work, the labels extracted by (Maillette de Buy Wenniger and Sima’an, 2014) are clustered manually into less than a dozen labels without the possibility of fitting the labels to th</context>
</contexts>
<marker>Saluja, Dyer, Cohen, 2014</marker>
<rawString>A. Saluja, C. Dyer, and S. B. Cohen. 2014. LatentVariable Synchronous CFGs for Hierarchical Translation. Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<date>2002</date>
<journal>Computational Complexity of Probabilistic Disambiguation. Grammars,</journal>
<volume>5</volume>
<issue>2</issue>
<marker>Sima’an, 2002</marker>
<rawString>Khalil Sima’an. 2002. Computational Complexity of Probabilistic Disambiguation. Grammars, 5(2):125–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="21074" citStr="Snover et al., 2006" startWordPosition="3553" endWordPosition="3556">he mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 s´ − t. The only exception is Baseline A which is trained on original s − t. We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006). We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics. Single or all PETs? In Table 3 we see that using all PETs during training makes a big impact on performance. Only the all PETs variants 7Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed s´ − t pairs, which are the word re-aligned and then used for training the back-end MT system (Khalilov and Sima’an, 2011). We skip this, we take the risk of mismatch between the preordering and the back-end system, but this si</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004: Short Papers, HLTNAACL-Short ’04,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="24838" citStr="Tillmann, 2004" startWordPosition="4154" endWordPosition="4155">t large distortion limits seem to degrade the preordering choice. This shows also that the improved performance of RGPET-forest is not only a result of efficiently exploring the full space of permutations, but also a result of improved scoring of permutations. System BLEU I METEOR I TER ↓ RIBES I DPBMSD 29.6 50.1 58.0 68.97 EHiero 32.6 52.1 54.5 74.12 RGPET-forest+MSD 32.4D 51.3D 55.3D E 75.72 E Table 4: Comparison to MSD and Hiero Does the improvement remain for a decoder with MSD reordering model? We compare the RGPET-forest preordered model against a decoder that uses the strong MSD model (Tillmann, 2004; Koehn et al., 2007). Table 4 shows that using Reordering Grammar as front-end to MSD reordering (full Moses) improves performance by 2.8 BLEU points. The improvement is confirmed by METEOR, TER and RIBES. Our preordering model and MSD are complementary – the Reordering Grammar captures long distance reordering, while MSD possibly does better local reorderings, especially reorderings conditioned on the lexical part of translation units. Interestingly, the MSD model (BLEU 29.6) improves over distance-based reordering (BLEU 27.8) by (BLEU 1.8), whereas the difference between these systems as ba</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of HLT-NAACL 2004: Short Papers, HLTNAACL-Short ’04, pages 101–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning Linear Ordering Problems for Better Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1007--1016</pages>
<location>Singapore,</location>
<contexts>
<context position="28011" citStr="Tromble and Eisner (2009)" startWordPosition="4656" endWordPosition="4659">The majority of work on preordering is based on syntactic parse trees, e.g., (Lerner and Petrov, 2013; Khalilov and Sima’an, 2011; Xia and Mccord, 2004). Here we concentrate on work that has common aspects with this work. Neubig et 50 Figure 4: Example parse of English sentence that predicts reordering for English-Japanese P12 P12 P21 P21 P21 P21 DC current i flowing P12 P12 through P12 P01 P12 the feeding conductor 3 produces magnetic P12 field b1 P12 . al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations. Tromble and Eisner (2009) use ITG but do not train the grammar. They only use it to constrain the local search. DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it. In contrast, here we learn both the structure and the reordering function simultaneously. Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step proce</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning Linear Ordering Problems for Better Translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1007–1016, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Wellington</author>
<author>Sonjia Waxmonsky</author>
<author>I Dan Melamed</author>
</authors>
<title>Empirical lower bounds on the complexity of translational equivalence. In</title>
<date>2006</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>977--984</pages>
<contexts>
<context position="9072" citStr="Wellington et al., 2006" startWordPosition="1472" endWordPosition="1476"> ITG are the binary case of prime permutations 45 P3142 P12 P21 P12 Professor Chomsky , I would like to thank you P3142 P12 P21 P12 Professor Chomsky , I would like to thank you Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken (a) Canonical PET Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken (b) Alternative PET Figure 1: Possible Permutation Trees (PETs) for one sentence pair in PETs (P12 and P21). ITGs recognize only the binarizable permutations, which is a major restriction when used on the data: there are many nonbinarizable permutations in actual data (Wellington et al., 2006). In contrast, our PETs are obtained by factorizing permutations obtained from the data, i.e., they exactly fit the range of prime permutations in the parallel corpus. In practice we limit them to maximum arity 5. We can extract PCFG rules from the PETs, e.g., P21 → P12 P2413. However, these rules are decorated with too coarse labels. A similar problem was encountered in non-lexicalized monolingual parsing, and one solution was to lexicalize the productions (Collins, 2003) using head words. But linguistic heads do not make sense for PETs, so we opt for the alternative approach (Matsuzaki et al</context>
</contexts>
<marker>Wellington, Waxmonsky, Melamed, 2006</marker>
<rawString>Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. 2006. Empirical lower bounds on the complexity of translational equivalence. In In Proceedings of ACL 2006, pages 977–984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="8378" citStr="Wu, 1997" startWordPosition="1354" endWordPosition="1355"> of PETs, the latent treebank and the Reordering Grammar. Figure 1 shows examples of how PETs look like – see (Zhang and Gildea, 2007) for algorithmic details. Here we label the nodes with nonterminals which stand for prime permutations from the operators on the PETs. For example, nonterminals P12, P21 and P3142 correspond respectively to reordering transducers (1, 2), (2, 1) and (3, 1, 4, 2). A prime permutation on a source node p is a transduction dictating how the children of p are reordered at the target side, e.g., P21 inverts the child order. We must stress that any similarity with ITG (Wu, 1997) is restricted to the fact that the straight and inverted operators of ITG are the binary case of prime permutations 45 P3142 P12 P21 P12 Professor Chomsky , I would like to thank you P3142 P12 P21 P12 Professor Chomsky , I would like to thank you Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken (a) Canonical PET Ebenso möchte Ich Ihnen , Herr Professor Chomsky , herzlich danken (b) Alternative PET Figure 1: Possible Permutation Trees (PETs) for one sentence pair in PETs (P12 and P21). ITGs recognize only the binarizable permutations, which is a major restriction when used on</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Comput. Linguist., 23(3):377–403, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael Mccord</author>
</authors>
<title>Improving a Statistical MT System with Automatically Learned Rewrite Patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>508--514</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1728" citStr="Xia and Mccord, 2004" startWordPosition="267" endWordPosition="270">ins et al., 2005) aims at permuting the words of a source sentence s into a new order ´s, hopefully close to a plausible target word order. Preordering is often used to bridge long distance reorderings (e.g., in Japanese- or GermanEnglish), before applying phrase-based models (Koehn et al., 2007). Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it. A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004). The (direct correspondence) assumption underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order. An alternative approach creates reordering rules manually and then learns the right structure for applying these rules (Katz-Brown et al., 2011). Others attempt learning the transduction structure and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011). Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned par</context>
<context position="27538" citStr="Xia and Mccord, 2004" startWordPosition="4577" endWordPosition="4581"> a mere generative PCFG. The advantages of Hiero can be brought to bear upon Reordering Grammar by reformulating it as a discriminative model. Which structure is learned? Figure 4 shows an example PET output showing how our model learns: (1) that the article “the” has no equivalent in Japanese, (2) that verbs go after their object, (3) to use postpositions instead of prepositions, and (4) to correctly group certain syntactic units, e.g. NPs and VPs. 5 Related work The majority of work on preordering is based on syntactic parse trees, e.g., (Lerner and Petrov, 2013; Khalilov and Sima’an, 2011; Xia and Mccord, 2004). Here we concentrate on work that has common aspects with this work. Neubig et 50 Figure 4: Example parse of English sentence that predicts reordering for English-Japanese P12 P12 P21 P21 P21 P21 DC current i flowing P12 P12 through P12 P01 P12 the feeding conductor 3 produces magnetic P12 field b1 P12 . al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations. Tromble and Eisner (2009) use ITG but do not train the grammar. They only use it to constrain the local search. DeNero and Uszkoreit (2011) present two </context>
</contexts>
<marker>Xia, Mccord, 2004</marker>
<rawString>Fei Xia and Michael Mccord. 2004. Improving a Statistical MT System with Automatically Learned Rewrite Patterns. In Proceedings of Coling 2004, pages 508–514, Geneva, Switzerland, August. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Factorization of Synchronous Context-Free Grammars in Linear Time.</title>
<date>2007</date>
<booktitle>In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2712" citStr="Zhang and Gildea, 2007" startWordPosition="419" endWordPosition="422">ucture and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011). Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned parallel corpora. Learning both trees and transductions jointly raises two questions. How to obtain suitable trees for the source sentence and how to learn a distribution over random variables specifically aimed at reordering in a hierarchical model? In this work we solve both challenges by using the factorizations of permutations into Permutation Trees (PETs) (Zhang and Gildea, 2007). As we explain next, PETs can be crucial for exposing the hierarchical reordering patterns found in wordalignments. We obtain permutations in the training data by segmenting every word-aligned source-target pair into minimal phrase pairs; the resulting alignment between minimal phrases is written as a permutation (1:1 and onto) on the source side. Every permutation can be factorized into a forest of PETs (over the source sentences) which we use as a latent treebank for training a Probabilistic ContextFree Grammar (PCFG) tailor made for preordering as we explain next. Figure 1 shows two altern</context>
<context position="6284" citStr="Zhang and Gildea, 2007" startWordPosition="996" endWordPosition="999">e are interested in ´s, the permuted version of s, not in the best derivation/PET. Exact computation is a known NP-Complete problem (Sima’an, 2002). We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (Birch and Osborne, 2011; Isozaki et al., 2010a). In summary, this paper contributes: • A novel latent hierarchical source reordering model working over all derivations of PETs 1All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure (Zhang and Gildea, 2007). • A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations. • A fast Minimum Bayes Risk decoding over Kendall T reordering score for selecting ´s. We report results for extensive experiments on English-Japanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrase-based models, outperforming two existing baselines for this task. 2 PETs and the Hidden Treebank We aim at learning a PCFG which we will use for parsing source sentences s in</context>
<context position="7903" citStr="Zhang and Gildea, 2007" startWordPosition="1266" endWordPosition="1269"> our approach for acquiring a latent treebank and how it is used for learning a Reordering PCFG: 1. Obtain a permutation over minimal phrases from every word-alignment. 2. Obtain a latent treebank of PETs by factorizing the permutations. 3. Extract a PCFG from the PETs with initial nonterminals taken from the PETs. 4. Learn to split the initial nonterminals and estimate rule probabilities. These steps are detailed in the next section, but we will start out with an intuitive exposition of PETs, the latent treebank and the Reordering Grammar. Figure 1 shows examples of how PETs look like – see (Zhang and Gildea, 2007) for algorithmic details. Here we label the nodes with nonterminals which stand for prime permutations from the operators on the PETs. For example, nonterminals P12, P21 and P3142 correspond respectively to reordering transducers (1, 2), (2, 1) and (3, 1, 4, 2). A prime permutation on a source node p is a transduction dictating how the children of p are reordered at the target side, e.g., P21 inverts the child order. We must stress that any similarity with ITG (Wu, 1997) is restricted to the fact that the straight and inverted operators of ITG are the binary case of prime permutations 45 P3142</context>
<context position="13104" citStr="Zhang and Gildea, 2007" startWordPosition="2175" endWordPosition="2178">s (e.g., whether the language is head-initial or -final (Chomsky, 1970)). Our experiments use English as source language (head-initial), so the unaligned words are joined to phrases to their right. This modifies a PET by adding a new binary branching node p (dominating the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P01 if the unaligned word joins to the right and P10 if it joins to the left. 3.1 Probability model We decompose the permutation 7rm into a forest of permutation trees PEF(7rm) in O(n3), following algorithms in (Zhang et al., 2008; Zhang and Gildea, 2007) with trivial modifications. Each PET 0 E PEF(7rm) is a different bracketing (differing in binary branching structure only). We consider the bracketing hidden in the latent treebank, and apply unsupervised learning to induce a distribution over possible bracketings. Our probability model starts from the joint probability of a sequence of minimal phrases sm and a permutation 7rm over it. This demands summing over all PETs 0 in the forest PEF(7rm), and for every PET also over all its label splits, which are given by the grammar derivations d: XP (sm, 7rm) = X P(d, sm) (1) DEPEF(7r,) dED The prob</context>
</contexts>
<marker>Zhang, Gildea, 2007</marker>
<rawString>Hao Zhang and Daniel Gildea. 2007. Factorization of Synchronous Context-Free Grammars in Linear Time. In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST), pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08),</booktitle>
<pages>1081--1088</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="13079" citStr="Zhang et al., 2008" startWordPosition="2171" endWordPosition="2174">e language properties (e.g., whether the language is head-initial or -final (Chomsky, 1970)). Our experiments use English as source language (head-initial), so the unaligned words are joined to phrases to their right. This modifies a PET by adding a new binary branching node p (dominating the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P01 if the unaligned word joins to the right and P10 if it joins to the left. 3.1 Probability model We decompose the permutation 7rm into a forest of permutation trees PEF(7rm) in O(n3), following algorithms in (Zhang et al., 2008; Zhang and Gildea, 2007) with trivial modifications. Each PET 0 E PEF(7rm) is a different bracketing (differing in binary branching structure only). We consider the bracketing hidden in the latent treebank, and apply unsupervised learning to induce a distribution over possible bracketings. Our probability model starts from the joint probability of a sequence of minimal phrases sm and a permutation 7rm over it. This demands summing over all PETs 0 in the forest PEF(7rm), and for every PET also over all its label splits, which are given by the grammar derivations d: XP (sm, 7rm) = X P(d, sm) (1</context>
<context position="29632" citStr="Zhang et al., 2008" startWordPosition="4929" endWordPosition="4932">ls, but unlike our approach they split generic nonterminals and not operators. Their most expressive grammar covers only binarizable permutations. The decoder that uses this model does not try to sum over many derivations that have the same yield. They do not make independence assumption like our “unary trick” which is probably the reason they do not split more than 8 times. They do not compare their results to any other SMT system and test on a very small dataset. Saluja et al (2014) attempts inducing a refined Hiero grammar (latent synchronous CFG) from Normalized Decomposition Trees (NDT) (Zhang et al., 2008). While there are similarities with the present work, there are major differences. On the similarity side, NDTs are decomposing alignments in ways similar to PETs, and both Saluja’s and our models refine the labels on the nodes of these decompositions. However, there are major differences between the two: • Our model is completely monolingual and unlexicalized (does not condition its reordering on the translation) in contrast with the Latent SCFG used in (Saluja et al., 2014), • Our Latent PCFG label splits are defined as refinements of prime permutations, i.e., specifically designed for learn</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting Synchronous Grammar Rules From Word-Level Alignments in Linear Time. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08), pages 1081–1088, Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
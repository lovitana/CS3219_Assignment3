<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000379">
<title confidence="0.978743">
SSHLDA: A Semi-Supervised Hierarchical Topic Model
</title>
<author confidence="0.99841">
Xian-Ling Mao*; Zhao-Yan Ming°, Tat-Seng Chua°, Si Li4, Hongfei Yan*t Xiaoming Li*
</author>
<affiliation confidence="0.999777333333333">
*Department of Computer Science and Technology, Peking University, China
°School of Computing, National University of Singapore, Singapore
4School of ICE, Beijing University of Posts and Telecommunications, China
</affiliation>
<email confidence="0.976981">
{xianlingmao,lxm}@pku.edu.cn, yhf@net.pku.edu.cn
{chuats,mingzhaoyan}@nus.edu.sg, lisi@bupt.edu.cn
</email>
<sectionHeader confidence="0.998585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996679962962963">
Supervised hierarchical topic modeling and
unsupervised hierarchical topic modeling are
usually used to obtain hierarchical topics, such
as hLLDA and hLDA. Supervised hierarchi-
cal topic modeling makes heavy use of the in-
formation from observed hierarchical labels,
but cannot explore new topics; while unsu-
pervised hierarchical topic modeling is able
to detect automatically new topics in the data
space, but does not make use of any informa-
tion from hierarchical labels. In this paper, we
propose a semi-supervised hierarchical topic
model which aims to explore new topics auto-
matically in the data space while incorporating
the information from observed hierarchical la-
bels into the modeling process, called Semi-
Supervised Hierarchical Latent Dirichlet Al-
location (SSHLDA). We also prove that hLDA
and hLLDA are special cases of SSHLDA. We
conduct experiments on Yahoo! Answers and
ODP datasets, and assess the performance in
terms of perplexity and clustering. The ex-
perimental results show that predictive ability
of SSHLDA is better than that of baselines,
and SSHLDA can also achieve significant im-
provement over baselines for clustering on the
FScore measure.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99456017948718">
Topic models, such as latent Dirichlet allocation
(LDA), are useful NLP tools for the statistical anal-
ysis of document collections and other discrete data.
*This work was done in National University of Singapore.
†Corresponding author.
Furthermore, hierarchical topic modeling is able to
obtain the relations between topics — parent-child
and sibling relations. Unsupervised hierarchical
topic modeling is able to detect automatically new
topics in the data space, such as hierarchical La-
tent Dirichlet Allocation (hLDA) (Blei et al., 2004).
hLDA makes use of nested Dirichlet Process to auto-
matically obtain a L-level hierarchy of topics. Mod-
ern Web documents, however, are not merely col-
lections of words. They are usually documents with
hierarchical labels – such as Web pages and their
placement in hierarchical directories (Ming et al.,
2010). Unsupervised hierarchical topic modeling
cannot make use of any information from hierarchi-
cal labels, thus supervised hierarchical topic models,
such as hierarchical Labeled Latent Dirichlet Allo-
cation (hLLDA) (Petinot et al., 2011), are proposed
to tackle this problem. hLLDA uses hierarchical la-
bels to automatically build corresponding topic for
each label, but it cannot find new latent topics in the
data space, only depending on hierarchy of labels.
As we know that only about 10% of an iceberg’s
mass is seen outside while about 90% of it is unseen,
deep down in water. We think that a corpus with hi-
erarchical labels should include not only observed
topics of labels, but also there are more latent top-
ics, just like icebergs. hLLDA can make use of the
information from labels; while hLDA can explore
latent topics. How can we combine the merits of the
two types of models into one model?
An intuitive and simple combinational method is
like this: first, we use hierarchy of labels as basic hi-
erarchy, called Base Tree (BT); then we use hLDA
to build automatically topic hierarchy for each leaf
</bodyText>
<page confidence="0.941908">
800
</page>
<note confidence="0.7905175">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 800–809, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.997264942307693">
node in BT, called Leaf Topic Hierarchy (LTH); fi-
nally, we add each LTH to corresponding leaf in the
BT and obtain a hierarchy for the entire dataset. We
refer the method as Simp-hLDA. The performance
of the Simp-hLDA is not so good, as can be seen
from the example in Figure 3 (b). The drawbacks
are: (i) the leaves in BT do not obtain reasonable
and right words distribution, such as “Computers &amp;
Internet” node in Figure 3 (b), its topical words, “the
to you and a”, is not about “Computers &amp; Internet”;
(ii) the non-leaf nodes in BT cannot obtain words
distribution, such as “Health” node in Figure 3 (b);
(iii) it is a heuristic method, and thus Simp-hLDA
has no solid theoretical basis.
To tackle the above drawbacks, we explore the
use of probabilistic models for such a task where
the hierarchical labels are merely viewed as a part
of a hierarchy of topics, and the topics of a path in
the whole hierarchy generate a corresponding doc-
ument. Our proposed generative model learns both
the latent topics of the underlying data and the la-
beling strategies in a joint model, by leveraging on
the hierarchical structure of labels and Hierarchical
Dirichlet Process.
We demonstrate the effectiveness of the proposed
model on large, real-world datasets in the question
answering and website category domains on two
tasks: the topic modeling of documents, and the use
of the generated topics for document clustering. Our
results show that our joint, semi-hierarchical model
outperforms the state-of-the-art supervised and un-
supervised hierarchical algorithms. The contribu-
tions of this paper are threefold: (1) We propose a
joint, generative semi-supervised hierarchical topic
model, i.e. Semi-Supervised Hierarchical Latent
Dirichlet Allocation (SSHLDA), to overcome the
defects of hLDA and hLLDA while combining the
their merits. SSHLDA is able to not only explore
new latent topics in the data space, but also makes
use of the information from the hierarchy of ob-
served labels; (2) We prove that hLDA and hLLDA
are special cases of SSHLDA; (3) We develop a
gibbs sampling inference algorithm for the proposed
model.
The remainder of this paper is organized as fol-
lows. We review related work in Section 2. In Sec-
tion 3, we introduce some preliminaries; while we
introduce SSHLDA in Section 4. Section 5 details
a gibbs sampling inference algorithm for SSHLDA;
while Section 6 presents the experimental results.
Finally, we conclude the paper and suggest direc-
tions for future research in Section 7.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999219365853658">
There have been many variations of topic mod-
els. The existing topic models can be divided
into four categories: Unsupervised non-hierarchical
topic models, Unsupervised hierarchical topic mod-
els, and their corresponding supervised counter-
parts.
Unsupervised non-hierarchical topic models are
widely studied, such as LSA (Deerwester et al.,
1990), pLSA (Hofmann, 1999), LDA (Blei et al.,
2003), Hierarchical-concept TM (Chemudugunta et
al., 2008c; Chemudugunta et al., 2008b), Corre-
lated TM (Blei and Lafferty, 2006) and Concept TM
(Chemudugunta et al., 2008a; Chemudugunta et al.,
2008b) etc. The most famous one is Latent Dirichlet
Allocation (LDA). LDA is similar to pLSA, except
that in LDA the topic distribution is assumed to have
a Dirichlet prior. LDA is a completely unsupervised
algorithm that models each document as a mixture
of topics. Another famous model that not only rep-
resents topic correlations, but also learns them, is
the Correlated Topic Model (CTM). Topics in CTM
are not independent; however it is noted that only
pairwise correlations are modeled, and the number
of parameters in the covariance matrix grows as the
square of the number of topics.
However, the above models cannot capture the
relation between super and sub topics. To address
this problem, many models have been proposed
to model the relations, such as Hierarchical LDA
(HLDA) (Blei et al., 2004), Hierarchical Dirichlet
processes (HDP) (Teh et al., 2006), Pachinko Allo-
cation Model (PAM) (Li and McCallum, 2006) and
Hierarchical PAM (HPAM) (Mimno et al., 2007)
etc. The relations are usually in the form of a hi-
erarchy, such as the tree or Directed Acyclic Graph
(DAG). Blei et al. (2004) proposed the hLDA model
that simultaneously learns the structure of a topic
hierarchy and the topics that are contained within
that hierarchy. This algorithm can be used to extract
topic hierarchies from large document collections.
Although unsupervised topic models are suffi-
</bodyText>
<page confidence="0.99622">
801
</page>
<bodyText confidence="0.999985">
ciently expressive to model multiple topics per doc-
ument, they are inappropriate for labeled corpora be-
cause they are unable to incorporate the observed la-
bels into their learning procedure. Several modifica-
tions of LDA to incorporate supervision have been
proposed in the literature. Two such models, Su-
pervised LDA (Blei and McAuliffe, 2007; Blei and
McAuliffe, 2010) and DiscLDA (Lacoste-Julien et
al., 2008) are first proposed to model documents as-
sociated only with a single label. Another category
of models, such as the MM-LDA (Ramage et al.,
2009b), Author TM (Rosen-Zvi et al., 2004), Flat-
LDA (Rubin et al., 2011), Prior-LDA (Rubin et al.,
2011), Dependency-LDA (Rubin et al., 2011) and
Partially LDA (PLDA) (Ramage et al., 2011) etc.,
are not constrained to one label per document be-
cause they model each document as a bag of words
with a bag of labels. However, these models obtain
topics that do not correspond directly with the la-
bels. Labeled LDA (LLDA) (Ramage et al., 2009a)
can be used to solve this problem.
None of these non-hierarchical supervised mod-
els, however, leverage on dependency structure,
such as parent-child relation, in the label space. For
hierarchical labeled data, there are also few models
that are able to handle the label relations in data.
To the best of our knowledge, only hLLDA (Petinot
et al., 2011) and HSLDA (Perotte et al., 2011) are
proposed for this kind of data. HSLDA cannot ob-
tain a probability distribution for a label. Although
hLLDA can obtain a distribution over words for each
label, hLLDA is unable to capture the relations be-
tween parent and child node using parameters, and it
also cannot detect automatically latent topics in the
data space. In this paper, we will propose a genera-
tive topic model to tackle these problems of hLLDA.
</bodyText>
<sectionHeader confidence="0.997246" genericHeader="method">
3 Preliminaries
</sectionHeader>
<bodyText confidence="0.9998155">
The nested Chinese restaurant process (nCRP) is a
distribution over hierarchical partitions (Blei et al.,
2004). It generalizes the Chinese restaurant process
(CRP), which is a distribution over partitions. The
CRP can be described by the following metaphor.
Imagine a restaurant with an infinite number of ta-
bles, and imagine customers entering the restaurant
in sequence. The dth customer sits at a table accord-
</bodyText>
<tableCaption confidence="0.999355">
Table 1: Notations used in the paper.
</tableCaption>
<bodyText confidence="0.818992821428571">
Sym Description
V Vocabulary (word set), w is a word in V
D Document collection
Tj The set of paths in the sub-tree whose root is the
jth leaf node in the hierarchy of observed topics
m A document m that consists of words and labels
wm The text of document m, wi is ith words in w
cm The topic set of document m
com The set of topics with observed labels for document m
cem The set of topics without labels for document m
ce_m The set of latent topics for all documents other than m
zem The assignment of the words in the mth document
to one of the latent topics
wem The set of the words belonging to one of the latent
topics in the the mth document
zm,n The assignment of the nth word in the mth document
to one of the L available topics
z The set of zm,n for all words in all documents
ci A topic in the ith level in the hierarchy
B The word distribution set for Z, i.e., IB1zEc
α Dirichlet prior of B
dci The multinomial distribution over the sub-topics of ci_1
µci Dirichlet prior of dci
77 Dirichlet prior of 3
,3 The multinomial distribution of words
Bm The distributions over topics for document m
9 The set for Bm, m E I1� ���� D1
ing to the following distribution,
</bodyText>
<equation confidence="0.9338925">
m k if k is previous occupied (
p(cd = k|c1:(d−1)) a γ if k is a new tabel, ll)
</equation>
<bodyText confidence="0.999987090909091">
where Mk is the number of previous customers sit-
ting at table k and -y is a positive scalar. After D cus-
tomers have sat down, their seating plan describes a
partition of D items.
In the nested CRP, imagine now that tables are or-
ganized in a hierarchy: there is one table at the first
level; it is associated with an infinite number of ta-
bles at the second level; each second-level table is
associated with an infinite number of tables at the
third level; and so on until the Lth level. Each cus-
tomer enters at the first level and comes out at the
Lth level, generating a path with L tables as she sits
in each restaurant. Moving from a table at level l to
one of its subtables at level l+1, the customer draws
following the CRP using Formula (1). In this paper,
we will make use of nested CRP to explore latent
topics in data space.
To elaborate our model, we first define two con-
cepts. If a model can learn a distribution over words
for a label, we refer the topic with a corresponding
label as a labeled topic. If a model can learn an un-
seen and latent topic without a label, we refer the
</bodyText>
<page confidence="0.995664">
802
</page>
<figureCaption confidence="0.8002225">
Figure 1: The graphical model of SSHLDA.
topic as a latent topic.
</figureCaption>
<sectionHeader confidence="0.9765085" genericHeader="method">
4 The Semi-Supervised Hierarchical Topic
Model
</sectionHeader>
<bodyText confidence="0.996427107142857">
In this section, we will introduce a semi-
supervised hierarchical topic model, i.e., the Semi-
Supervised Hierarchical Latent Dirichlet Allocation
(SSHLDA). SSHLDA is a probabilistic graphical
model that describes a process for generating a hi-
erarchical labeled document collection. Like hi-
erarchical Labeled LDA (hLLDA) (Petinot et al.,
2011), SSHLDA can incorporate labeled topics into
the generative process of documents. On the other
hand, like hierarchical Latent Dirichlet Allocation
(hLDA) (Blei et al., 2004), SSHLDA can automat-
ically explore latent topic in data space, and extend
the existing hierarchy of observed topics. SSHLDA
makes use of not only observed topics, but also la-
tent topics.
The graphical model of SSHLDA is illustrated in
Figure 1. In the model, N is the number of words in
a document, D is the total number of documents in
a collection, M is the number of leaf nodes in hier-
archical observed nodes, cz is a node in the ith level
in the hierarchical tree, η, α and µ i are dirichlet
prior parameters, βk is a distribution over words, θ
is a document-specific distribution over topics, δ&apos;i is
a multinomial distribution over observed sub-topics
of topic cz, w is an observed word, z is the topic
assigned to w, Dirk(.) is a k-dimensional Dirichlet
distribution, Tj is a set of paths in the hierarchy of
latent topics for jth leaf node in the hierarchy of ob-
</bodyText>
<figureCaption confidence="0.976760833333333">
Figure 2: One illustration of SSHLDA. The tree has 5
levels. The shaded nodes are observed topics, and circled
nodes are latent topics. The latent topics are generated
automatically by SSHLDA model. After learning, each
node in this tree will obtain a corresponding probability
distribution over words, i.e. a topic.
</figureCaption>
<bodyText confidence="0.9951296">
served topics, γ is a Multi-nomial distribution over
paths in the tree. All notations used in this paper are
listed in Table 1.
SSHLDA, as shown in Figure 1, assumes the fol-
lowing generative process:
</bodyText>
<listItem confidence="0.944046133333333">
(1) For each table k ∈ T in the infinite tree,
(a) Draw a topic βk ∼ Dir(η).
(2) For each document, m ∈ {1, 2,..., D}
(a) Let c1 be the root node.
(b) For each level l ∈ {2,..., L}:
(i) If nodes in this level have been observed,
draw a node cl from Mult(δcl−1|µcl−1).
(ii) Otherwise, draw a table cl from restaurant
cl_1 using Formula (1).
(c) Draw an L-dimensional topic proportion vec-
tor θ,,,, from Dir(α).
(d) For each word n ∈ {1, ..., N}:
(i) Draw z ∈ {1, ..., L} from Mult(θ).
(ii) Draw wn from the topic associated with
restaurant c,.
</listItem>
<bodyText confidence="0.999826916666667">
As the example showed in Figure 2, we assume
that we have known a hierarchy of observed top-
ics: {A1,A2,A17,A3,A4}, and assume the height
of the desired topical tree is L = 5. All circled
nodes are latent topics, and shaded nodes are ob-
served topics. A possible generative process for a
document m can be: It starts from A1, and chooses
node A17 at level 2, and then chooses A18, A20 and
A25 in the following levels. Thus we obtain a path:
c,,,, = {A1, A17, A18, A20, A25}. After getting the
path for m, SSHLDA generates each word from one
of topics in this set of topics c,,,,.
</bodyText>
<page confidence="0.997653">
803
</page>
<sectionHeader confidence="0.958845" genericHeader="method">
5 Probabilistic Inference and
</sectionHeader>
<bodyText confidence="0.999978848484849">
In this section, we describe a Gibbs sampling al-
gorithm for sampling from the posterior and corre-
sponding topics in the SSHLDA model. The Gibbs
sampler provides a method for simultaneously ex-
ploring the model parameter space (the latent topics
of the whole corpus) and the model structure space
(L-level trees).
In SSHLDA, we sample the paths cm for docu-
ment m and the per-word level allocations to topics
in those paths zm,n. Thus, we approximate the pos-
terior p(cm, zm|γ, η, w, µ). The hyper-parameter γ
reflects the tendency of the customers in each restau-
rant to share tables, η denotes the expected variance
of the underlying topics (e.g., η « 1 will tend to
choose topics with fewer high-probability words),
µci is the dirichlet prior of δci, and µ is the set of
µci. wm,n denotes the nth word in the mth docu-
ment; and cm,l represents the restaurant correspond-
ing to the lth-level topic in document m; and zm,n,
the assignment of the nth word in the mth document
to one of the L available topics. All other variables
in the model, θ and β, are integrated out. The Gibbs
sampler thus assesses the values of zm,n and cm,l.
The Gibbs sampler can be divided into two main
steps: the sampling of level allocations and the sam-
pling of path assignments.
First, given the values of the SSHLDA hidden
variables, we sample the cm,l variables which are as-
sociated with the CRP prior. Noting that cm is com-
posed of com and cem, com is the set of observed
topics for document m, and cem is the set of latent
topics for document m. The conditional distribution
for cm, the L topics associated with document m, is:
</bodyText>
<equation confidence="0.97242075">
p(cm|z, w, c−m, µ)
=p(com|µ)p(cem|zem, wem, ce−m)
∝p(com|µ)p(wem|cem, we−m, zem)
p(cem|ce−m) (2)
where
p(com|µ) = |com|−1 p(ci,m|µci) (3)
∏
i=0
p(wem|cem, we−m, zem)
(
|cem |∏Γ(n. cem,l,−m + |V |η)
∏ w Γ(nwcem,l,−m + η)×
l��
)
(4)
Γ(n.cem,l,−m + n·cem,l,m + |V |η)
</equation>
<bodyText confidence="0.956775416666667">
ce−m is the set of latent topics for all documents
other than m, zem is the assignment of the words
in the mth document to one of the latent topics, and
wem is the set of the words belonging to one of the
latent topics in the the mth document. nwc −m is
em,l,
the number of instances of word w that have been
assigned to the topic indexed by cem,l, not including
those in the document m.
Second, given the current state of the SSHLDA,
we sample the zm,n variables of the underlying
SSHLDA model as follows:
</bodyText>
<equation confidence="0.994504">
p(zm,n = j|z−(m,n), w, cm, µ)
nwm,n
−n,j + ηwm,n ·n. −(m,n) + |V  |(5)
</equation>
<bodyText confidence="0.999647181818182">
Having obtained the full conditional distribution,
the Gibbs sampling algorithm is then straightfor-
ward. The zm,n variables are initialized to determine
the initial state of the Markov chain. The chain is
then run for a number of iterations, each time find-
ing a new state by sampling each zm,n from the dis-
tribution specified by Equation (5). After obtain-
ing individual word assignments z, we can estimate
the topic multinomials and the per-document mixing
proportions. Specifically, the topic multinomials are
estimated as:
</bodyText>
<equation confidence="0.996679">
(6)
|V |η + ∑n.zcm,j
</equation>
<bodyText confidence="0.9981585">
while the per-document mixing proportions fixed
can be estimated as:
</bodyText>
<equation confidence="0.991082333333333">
α + nm .,j
θm,j = ,j ∈ 1, ...,|cm |(7)
|cm|α + nm .,.
</equation>
<subsectionHeader confidence="0.964021">
5.1 Relation to Existing Models
</subsectionHeader>
<bodyText confidence="0.999465">
In this section, we draw comparisons with the cur-
rent state-of-the-art models for hierarchical topic
</bodyText>
<equation confidence="0.995100714285714">
∏w Γ(nwcem,l,−m + nwcem,l,m + η)
nm−n,j + α
nm−n,. + |cm|
∝
βem,j,i = p(wi|zem,j) =
η + n wi
zcm,j
</equation>
<page confidence="0.99147">
804
</page>
<bodyText confidence="0.999808625">
modeling (Blei et al., 2004; Petinot et al., 2011) and
show that at certain choices of the parameters of our
model, these methods fall out as special cases.
Our method generalises not only hierarchi-
cal Latent Dirichlet Allocation (hLDA), but also
Hierarchical Labeled Latent Dirichlet Allocation
(hLLDA). Our proposed model provides a unified
framework allowing us to model hierarchical labels
while to explore new latent topics.
Equivalence to hLDA As introduced in Section 2,
hLDA is a unsupervised hierarchical topic model. In
this case, there are no observed nodes, that is, the
corpus has no hierarchical labels. This means cm is
equal to ce„t,m; meanwhile the factor p(co„t,m|µ) is
always equal to one because each document has root
node, and this allows us to rewrite Formula (2) as:
</bodyText>
<equation confidence="0.9975675">
p(cm|z, w, c−m, µ)
∝p(wc„t|c, w−m, z)p(cm|c−m) (8)
</equation>
<bodyText confidence="0.999402875">
which is exactly the same as the conditional distribu-
tion for cm, the L topics associated with document
m in hLDA model. In this case, our model becomes
equivalent to the hLDA model.
Equivalence to hLLDA hLLDA is a supervised hi-
erarchical topic model, which means all nodes in hi-
erarchy are observed. In this case, cm is equal to
co„t,m, and this allows us to rewrite Formula (2) as:
</bodyText>
<equation confidence="0.910634">
p(cm|z, w,c−m, µ) = p(cm|µ) ∝ p(c,,-|µ) (9)
</equation>
<bodyText confidence="0.99959675">
which is exactly the same as the step “ Draw a
random path assignment cm” in the generative pro-
cess for hLLDA. Consequentially, in this sense our
model is equivalent to hLLDA.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999948">
We demonstrate the effectiveness of the proposed
model on large, real-world datasets in the question
answering and website category domains on two
tasks: the topic modeling of documents, and the use
of the generated topics for document clustering.
</bodyText>
<subsectionHeader confidence="0.956141">
6.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9997855">
To construct comprehensive datasets for our ex-
periments, we crawled data from two websites.
First, we crawled nearly all the questions and as-
sociated answer pairs (QA pairs) of two top cat-
</bodyText>
<tableCaption confidence="0.999055">
Table 2: The statistics of the datasets.
</tableCaption>
<table confidence="0.99557525">
Datasets #labels #paths Max level #docs
Y Ans 46 35 4 6,345,786
O Hlth 6695 6505 10 54939
O Home 2432 2364 9 24254
</table>
<bodyText confidence="0.996273111111111">
egories of Yahoo! Answers: Computers &amp; Inter-
net and Health. This produced forty-three sub-
categories from 2005.11 to 2008.11, and an archive
of 6,345,786 QA documents. We refer the Yahoo!
Answer data as Y Ans.
In addition, we first crawled two categories of
Open Directory Project (ODP)*: Home and Health.
Then, we removed all categories whose number of
Web sites is less than 3. Finally, for each of Web
sites in categories, we submited the url of each Web
site to Google and used the words in the snippet and
title of the first returned result to extend the sum-
mary of the Web site. We denote the data from the
category Home as O Home, and the data from the
category Health as O Hlth.
The statistics of all datasets are summarized in Ta-
ble 2. From this table, we can see that these datasets
are very diverse: Y Ans has much fewer labels than
O Hlth and O Home, but have much more docu-
ments for each label; meanwhile the depth of hierar-
chical tree for O Hlth and O Home can reach level
9 or above.
All experiments are based on the results of models
with a burn-in of 10000 Gibbs sampling iterations,
symmetric priors α = 0.1 and free parameter q = 1.0;
and for µ, we can obtain the estimation of µcz by
fixed-point iteration (Minka, 2003).
</bodyText>
<subsectionHeader confidence="0.999903">
6.2 Case Study
</subsectionHeader>
<bodyText confidence="0.999586909090909">
With topic modeling, the top associated words of
topics can be used as good descriptors for topics in
a hierarchy (Blei et al., 2003; Blei and McAuliffe,
2010). We show in Figure 3 a pair of compara-
tive example of the proposed model and a baseline
model over Y Ans dataset. The tree-based topic vi-
sualizations of Figure 3 (a) and (b) are the results of
SSHLDA and Simp-hLDA.
We have three major observations from the exam-
ple: (i) SSHLDA is a unified and generative model,
after learning, it can obtain a hierarchy of topics;
</bodyText>
<footnote confidence="0.467529">
*http://dmoz.org/
</footnote>
<page confidence="0.996653">
805
</page>
<figureCaption confidence="0.9813636">
Figure 3: (a) A sub network discovered on Y Ans dataset using SSHLDA, and the whole tree has 74 nodes; (b) A sub
network discovered on Y Ans dataset using Simp-hLDA algorithm, and the whole tree has 89 nodes. In both figures,
the shaded and squared nodes are observed labels, not topics; the shaded and round nodes are topics with observed
labels; blue nodes are topics but without labels and the yellow node is one of leaves in hierarchy of labels. Each topic
represented by top 5 terms.
</figureCaption>
<bodyText confidence="0.999966857142857">
while Simp-hLDA is a heuristic method, and its re-
sult is a mixture of label nodes and topical nodes.
For example, Figure 3 (b) shows that the hierarchy
includes label nodes and topic nodes, and each of la-
beled nodes just has a label, but label nodes in Fig-
ure 3 (a) have their corresponding topics. (ii) Dur-
ing obtaining a hierarchy, SSHLDA makes use of the
information from observed labels, thus it can gener-
ate a logical, structual hierarchy with parent-child
relations; while Simp-hLDA does not incorporate
prior information of labels into its generation pro-
cess, thus although it can obtain a hierarchy, many
parent-child pairs have not parent-child relation. For
example, in Figure 3 (b), although label “root” is
a parent of label “Computers &amp; Internet”, the topi-
cal words of label “Computers &amp; Internet” show the
topical node is not a child of label “root”. How-
ever, in Figure 3 (a), label “root” and “Computers
&amp; Internet” has corresponding parent-child relation
between their topical words. (iii) In a hierarchy of
topics, if a topical node has correspending label, the
label can help people understand descendant topi-
cal nodes. For example, when we know node “er-
ror files click screen virus” in Figure 3 (a) has its
label “Computers &amp; Internet”, we can understand
the child topic “hard screen usb power dell” is about
“computer hardware”. However, in Figure 3 (b), the
labels in parent nodes cannot provide much informa-
tion to understand descendant topical nodes because
many label nodes have not corresponding right topi-
cal words, such as label “Computers &amp; Internet”, its
topical words, “the to you and a”, do not reflect the
connotation of the label.
These observations further confirm that SSHLDA
is better than the baseline model.
</bodyText>
<subsectionHeader confidence="0.998898">
6.3 Perplexity Comparison
</subsectionHeader>
<bodyText confidence="0.9999964">
A good topic model should be able to generalize to
unseen data. To measure the prediction ability of
our model and baselines, we compute the perplex-
ity for each document d in the test sets. Perplex-
ity, which is widely used in the language modeling
and topic modeling community, is equivalent alge-
braically to the inverse of the geometric mean per-
word likelihood (Blei et al., 2003). Lower perplexity
scores mean better. Our model, SSHLDA, will com-
pare with three state-of-the-art models, i.e. Simp-
hLDA, hLDA and hLLDA. Simp-hLDA has been
introduced in Section 1, and hLDA and hLLDA has
been reviewed in Section 2. We keep 80% of the data
collection as the training set and use the remaining
collection as the held-out test set. We build the mod-
</bodyText>
<page confidence="0.996263">
806
</page>
<bodyText confidence="0.9997874">
els based on the train set and compute the preplexity
of the test set to evaluate the models. Thus, our goal
is to achieve lower perplexity score on a held-out test
set. The perplexity of M test documents is calculated
as:
</bodyText>
<equation confidence="0.99026625">
{
l
perplexity(Dtest) = exp − `Ed=1EMN&apos;y�1log p(wdm) 1 (10)
tal=1 Nd
</equation>
<bodyText confidence="0.999933722222223">
where Dteat is the test collection of M documents,
Nd is document length of document d and wdm is
mth word in document d.
We present the results over the O Hlth dataset in
Figure 4. We choose top 3-level labels as observed,
and assume other labels are not observed, i.e. l = 3.
From the figure, we can see that the perplexities of
SSHLDA, are lower than that of Simp-hLDA, hLDA
and hLLDA at different value of the tree height pa-
rameter, i.e. L E 15, 6, 7, 8}. It shows that the
performance of SSHLDA is always better than the
state-of-the-art baselines, and means that our pro-
posed model can model the hierarchical labeled data
better than the state-of-the-art models. We can also
obtain similar experimental results over Y Ans and
O Home datasets, and their detailed description is
not included in this paper due to the limitation of
space.
</bodyText>
<subsectionHeader confidence="0.999872">
6.4 Clustering performance
</subsectionHeader>
<bodyText confidence="0.999746842105263">
To evaluate indirectly the performance of the pro-
posed model, we compare the clustering perfor-
mance of following systems: 1) the proposed model;
2) Simp-hLDA; 3) hLDA; 4) agglomerative cluster-
ing algorithm. There are many agglomerative clus-
tering algorithms, and in this paper, we make use
of the single-linkage method in a software package
called CLUTO (Karypis, 2005) to obtain hierarchies
of clusters over our datasets, with words as features.
We refer the method as h-clustering.
Given a document collection DS with a H-level hi-
erarchy of labels, each label in the hierarchy and cor-
responding documents will be taken as the ground
truth of clustering algorithms. The hierarchy of la-
bels denoted as GT-tree. The process of evaluation
is as follows. First, we choose top l-level labels
in GT-tree as an observed hierarchy, i.e. Base Tree
(BT), and we need to construct a L-level hierarchy
(l &lt; L &lt;= H) over the documents DS using a
</bodyText>
<figureCaption confidence="0.9922984">
Figure 4: Perplexities of hLLDA, hLDA, Simp-hLDA
and SSHLDA. The results are run over the O Hlth
dataset, with the height of the hierarchy of observed la-
bels l = 3. The X-axis is the height of the whole topical
tree (L), and Y-axis is the perplexity.
</figureCaption>
<bodyText confidence="0.999985555555556">
model. The remaining labels in GT-tree and cor-
responding documents are the ground truth classes,
each class denoted as Ci. Then, (i) for h-clustering,
we run single-linkage method over the documents
DS. (ii) for Simp-hLDA, hLDA runs on the doc-
uments in each leaf-node in BT, and the height pa-
rameter is (L − l) for each hLDA. After training,
each document is assigned to top-1 topic accord-
ing to the distribution over topics for the document.
Each topic and corresponding documents forms a
new cluster. (iii) for hLDA, hLDA runs on all docu-
ments in DS, and the height parameter is L. Similar
to Simp-hLDA, each document is assigned to top-
1 topic. Each topic and corresponding documents
forms a new cluster. (iv) for SSHLDA, we set height
parameter as L. After training, each document is
also assigned to top-1 topic. Topics and their cor-
responding documents form a hierarchy of clusters.
</bodyText>
<subsectionHeader confidence="0.456598">
6.4.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999643625">
For each dataset we obtain corresponding clusters
using the various models described in previous sec-
tions. Thus we can use clustering metrics to measure
the quality of various algorithms by using a measure
that takes into account the overall set of clusters that
are represented in the new generated part of a hier-
archical tree.
One such measure is the FScore measure, intro-
</bodyText>
<page confidence="0.989774">
807
</page>
<bodyText confidence="0.9909824">
duced by (Manning et al., 2008). Given a particular
class Cr of size nr and a particular cluster Si of size
ni, suppose nri documents in the cluster Si belong
to Cr, then the FScore of this class and cluster is
defined to be
</bodyText>
<equation confidence="0.997314666666667">
2 � R(Cr, Si) � P(Cr, Si)
F(Cr, Si) = (11)
R(Cr, Si) + P(Cr, Si)
</equation>
<bodyText confidence="0.9878885">
where R(Cr, Si) is the recall value defined as
nri/nr, and P(Cr, Si) is the precision value defined
as nri/ni for the class Cr and the cluster Si. The FS-
core of the class Cr, is the maximum FScore value
attained at any node in the hierarchical clustering
tree T. That is,
</bodyText>
<equation confidence="0.5841725">
F(Cr) = max F(Cr, Si). (12)
S;ET
</equation>
<bodyText confidence="0.998976666666667">
The FScore of the entire clustering solution is then
defined to be the sum of the individual class FScore
weighted according to the class size.
</bodyText>
<equation confidence="0.658962">
F(Cr), (13)
</equation>
<bodyText confidence="0.999897333333333">
where c is the total number of classes. In general, the
higher the FScore values, the better the clustering
solution is.
</bodyText>
<subsectionHeader confidence="0.767858">
6.4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99794945">
Each of hLDA, Simp-hLDA and SSHLDA needs
a parameter—the height of the topical tree, i.e. L;
and for Simp-hLDA and SSHLDA, they need an-
other parameter—the height of the hierarchical ob-
served labels, i.e l. The h-clustering does not have
any height parameters, thus its FScore will keep the
same values at different height of the topical tree.
With choosing the height of hierarchical labels for
O Home as 4, i.e. l = 4, the results of our model
and baselines with respect to the height of a hierar-
chy are shown in Figure 5.
From the figure, we can see that our proposed
model can achieve consistent improvement over
the baseline models at different height, i.e. L E
15, 6, 7, 8}. For example, the performance of
SSHLDA can reach 0.396 at height 5 while the h-
clustering, hLDA and hLLDA only achieve 0.295,
0.328 and 0.349 at the same height. The result shows
that our model can achieve about 34.2%, 20.7% and
13.5% improvements over h-clustering, hLDA and
</bodyText>
<figureCaption confidence="0.9880346">
Figure 5: FScore measures of h-clustering, hLDA,
Simp-hLDA and SSHLDA. The results are run over the
O Home dataset, with the height of the hierarchy of ob-
served labels l = 3. The X-axis is the height of the whole
topical tree (L), and Y-axis is the FScore measure.
</figureCaption>
<bodyText confidence="0.999890666666667">
hLLDA at height 5. The improvements are signifi-
cant by t-test at the 95% significance level. We can
also obtain similar experimental results over Y Ans
and O Hlth. However, for the same reason of limita-
tion of space, their detailed descriptions are skipped
in this paper.
</bodyText>
<sectionHeader confidence="0.996313" genericHeader="conclusions">
7 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.9999732">
In this paper, we have proposed a semi-supervised
hierarchical topic models, i.e. SSHLDA, which aims
to solve the drawbacks of hLDA and hLLDA while
combine their merits. Specially, SSHLDA incorpo-
rates the information of labels into generative pro-
cess of topic modeling while exploring latent topics
in data space. In addition, we have also proved that
hLDA and hLLDA are special cases of SSHLDA.
We have conducted experiments on the Yahoo! An-
swers and ODP datasets, and assessed the perfor-
mance in terms of Perplexity and FScore measure.
The experimental results show that the prediction
ability of SSHLDA is the best, and SSHLDA can
also achieve significant improvement over the base-
lines on Fscore measure.
In the future, we will continue to explore novel
topic models for hierarchical labeled data to further
improve the effectiveness; meanwhile we will also
apply SSHLDA to other media forms, such as im-
age, to solve related problems in these areas.
</bodyText>
<figure confidence="0.7054424">
nr
FScore =
n
Ec
r=1
</figure>
<page confidence="0.990488">
808
</page>
<sectionHeader confidence="0.999502" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.950594333333333">
This work was partially supported by NSFC with Grant
No.61073082, 60933004, 70903008 and NExT Search
Centre, which is supported by the Singapore National Re-
search Foundation &amp; Interactive Digital Media R&amp;D Pro-
gram Office, MDA under research grant (WBS:R-252-
300-001-490).
</bodyText>
<sectionHeader confidence="0.997505" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999865936842105">
D. Blei and J. Lafferty. 2006. Correlated topic mod-
els. Advances in neural information processing sys-
tems, 18:147.
D.M. Blei and J.D. McAuliffe. 2007. Supervised topic
models. In Proceeding of the Neural Information Pro-
cessing Systems(nips).
D.M. Blei and J.D. McAuliffe. 2010. Supervised topic
models. Arxiv preprint arXiv:1003.0783.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet allocation. The Journal of Machine Learning
Research, 3:993–1022.
D. Blei, T.L. Griffiths, M.I. Jordan, and J.B. Tenenbaum.
2004. Hierarchical topic models and the nested chi-
nese restaurant process. Advances in neural informa-
tion processing systems, 16:106.
C. Chemudugunta, A. Holloway, P. Smyth, and
M. Steyvers. 2008a. Modeling documents by com-
bining semantic concepts with unsupervised statistical
learning. The Semantic Web-ISWC 2008, pages 229–
244.
C. Chemudugunta, P. Smyth, and M. Steyvers. 2008b.
Combining concept hierarchies and statistical topic
models. In Proceeding of the 17th ACM conference on
Information and knowledge management, pages 1469–
1470. ACM.
C. Chemudugunta, P. Smyth, and M. Steyvers. 2008c.
Text modeling using unsupervised topic models and
concept hierarchies. Arxiv preprint arXiv:0808.0973.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American society for informa-
tion science, 41(6):391–407.
T. Hofmann. 1999. Probabilistic latent semantic analy-
sis. In Proc. of Uncertainty in Artificial Intelligence,
UAI’99, page 21. Citeseer.
G. Karypis. 2005. Cluto: Software for
clustering high dimensional datasets. In-
ternet Website (last accessed, June 2008),
http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.
S. Lacoste-Julien, F. Sha, and M.I. Jordan. 2008. ndis-
clda: Discriminative learning for dimensionality re-
duction and classification. Advances in Neural Infor-
mation Processing Systems, 21.
W. Li and A. McCallum. 2006. Pachinko allocation:
Dag-structured mixture models of topic correlations.
In Proceedings of the 23rd international conference on
Machine learning, pages 577–584. ACM.
C.D. Manning, P. Raghavan, and H. Schutze. 2008. In-
troduction to information retrieval, volume 1. Cam-
bridge University Press Cambridge.
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures of
hierarchical topics with pachinko allocation. In Pro-
ceedings of the 24th international conference on Ma-
chine learning, pages 633–640. ACM.
Z.Y. Ming, K. Wang, and T.S. Chua. 2010. Prototype
hierarchy based clustering for the categorization and
navigation of web collections. In Proceeding of the
33rd international ACM SIGIR, pages 2–9. ACM.
T.P. Minka. 2003. Estimating a dirichlet distribution.
Annals of Physics, 2000(8):1–13.
A. Perotte, N. Bartlett, N. Elhadad, and F. Wood. 2011.
Hierarchically supervised latent dirichlet allocation.
Neural Information Processing Systems (to appear).
Y. Petinot, K. McKeown, and K. Thadani. 2011. A
hierarchical model of web summaries. In Proceed-
ings of the 49th Annual Meeting of the ACL: Human
Language Technologies: short papers-Volume 2, pages
670–675. ACL.
D. Ramage, D. Hall, R. Nallapati, and C.D. Manning.
2009a. Labeled lda: A supervised topic model for
credit attribution in multi-labeled corpora. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 1-Volume 1,
pages 248–256. Association for Computational Lin-
guistics.
D. Ramage, P. Heymann, C.D. Manning, and H. Garcia-
Molina. 2009b. Clustering the tagged web. In Pro-
ceedings of the Second ACM International Conference
on Web Search and Data Mining, pages 54–63. ACM.
D. Ramage, C.D. Manning, and S. Dumais. 2011. Par-
tially labeled topic models for interpretable text min-
ing. In Proceedings of the 17th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 457–465. ACM.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. The author-topic model for authors and doc-
uments. In Proceedings of the 20th conference on
Uncertainty in artificial intelligence, pages 487–494.
AUAI Press.
T.N. Rubin, A. Chambers, P. Smyth, and M. Steyvers.
2011. Statistical topic models for multi-label docu-
ment classification. Arxiv preprint arXiv:1107.2462.
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. 2006.
Hierarchical dirichlet processes. Journal of the Amer-
ican Statistical Association, 101(476):1566–1581.
</reference>
<page confidence="0.998894">
809
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.688113">
<title confidence="0.999608">SSHLDA: A Semi-Supervised Hierarchical Topic Model</title>
<author confidence="0.999959">Tat-Seng Si Hongfei</author>
<affiliation confidence="0.9969065">of Computer Science and Technology, Peking University, of Computing, National University of Singapore,</affiliation>
<address confidence="0.711228">of ICE, Beijing University of Posts and Telecommunications,</address>
<email confidence="0.979726">lisi@bupt.edu.cn</email>
<abstract confidence="0.999363285714286">Supervised hierarchical topic modeling and unsupervised hierarchical topic modeling are usually used to obtain hierarchical topics, such as hLLDA and hLDA. Supervised hierarchical topic modeling makes heavy use of the information from observed hierarchical labels, but cannot explore new topics; while unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, but does not make use of any information from hierarchical labels. In this paper, we propose a semi-supervised hierarchical topic model which aims to explore new topics automatically in the data space while incorporating the information from observed hierarchical lainto the modeling process, called Semi- Supervised Hierarchical Latent Dirichlet Al- We also prove that hLDA and hLLDA are special cases of SSHLDA. We conduct experiments on Yahoo! Answers and ODP datasets, and assess the performance in terms of perplexity and clustering. The experimental results show that predictive ability of SSHLDA is better than that of baselines, and SSHLDA can also achieve significant improvement over baselines for clustering on the FScore measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>J Lafferty</author>
</authors>
<title>Correlated topic models. Advances in neural information processing systems,</title>
<date>2006</date>
<pages>18--147</pages>
<contexts>
<context position="6864" citStr="Blei and Lafferty, 2006" startWordPosition="1064" endWordPosition="1067">ults. Finally, we conclude the paper and suggest directions for future research in Section 7. 2 Related Work There have been many variations of topic models. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts. Unsupervised non-hierarchical topic models are widely studied, such as LSA (Deerwester et al., 1990), pLSA (Hofmann, 1999), LDA (Blei et al., 2003), Hierarchical-concept TM (Chemudugunta et al., 2008c; Chemudugunta et al., 2008b), Correlated TM (Blei and Lafferty, 2006) and Concept TM (Chemudugunta et al., 2008a; Chemudugunta et al., 2008b) etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA, except that in LDA the topic distribution is assumed to have a Dirichlet prior. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwise correlations are modeled, and the number of parameters in the covari</context>
</contexts>
<marker>Blei, Lafferty, 2006</marker>
<rawString>D. Blei and J. Lafferty. 2006. Correlated topic models. Advances in neural information processing systems, 18:147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D McAuliffe</author>
</authors>
<title>Supervised topic models.</title>
<date>2007</date>
<booktitle>In Proceeding of the Neural Information Processing Systems(nips).</booktitle>
<contexts>
<context position="8652" citStr="Blei and McAuliffe, 2007" startWordPosition="1354" endWordPosition="1357">) proposed the hLDA model that simultaneously learns the structure of a topic hierarchy and the topics that are contained within that hierarchy. This algorithm can be used to extract topic hierarchies from large document collections. Although unsupervised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b), Author TM (Rosen-Zvi et al., 2004), FlatLDA (Rubin et al., 2011), Prior-LDA (Rubin et al., 2011), Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the l</context>
</contexts>
<marker>Blei, McAuliffe, 2007</marker>
<rawString>D.M. Blei and J.D. McAuliffe. 2007. Supervised topic models. In Proceeding of the Neural Information Processing Systems(nips).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>J D McAuliffe</author>
</authors>
<title>Supervised topic models. Arxiv preprint arXiv:1003.0783.</title>
<date>2010</date>
<contexts>
<context position="8679" citStr="Blei and McAuliffe, 2010" startWordPosition="1358" endWordPosition="1361">that simultaneously learns the structure of a topic hierarchy and the topics that are contained within that hierarchy. This algorithm can be used to extract topic hierarchies from large document collections. Although unsupervised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b), Author TM (Rosen-Zvi et al., 2004), FlatLDA (Rubin et al., 2011), Prior-LDA (Rubin et al., 2011), Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (</context>
<context position="22918" citStr="Blei and McAuliffe, 2010" startWordPosition="3915" endWordPosition="3918">ts are very diverse: Y Ans has much fewer labels than O Hlth and O Home, but have much more documents for each label; meanwhile the depth of hierarchical tree for O Hlth and O Home can reach level 9 or above. All experiments are based on the results of models with a burn-in of 10000 Gibbs sampling iterations, symmetric priors α = 0.1 and free parameter q = 1.0; and for µ, we can obtain the estimation of µcz by fixed-point iteration (Minka, 2003). 6.2 Case Study With topic modeling, the top associated words of topics can be used as good descriptors for topics in a hierarchy (Blei et al., 2003; Blei and McAuliffe, 2010). We show in Figure 3 a pair of comparative example of the proposed model and a baseline model over Y Ans dataset. The tree-based topic visualizations of Figure 3 (a) and (b) are the results of SSHLDA and Simp-hLDA. We have three major observations from the example: (i) SSHLDA is a unified and generative model, after learning, it can obtain a hierarchy of topics; *http://dmoz.org/ 805 Figure 3: (a) A sub network discovered on Y Ans dataset using SSHLDA, and the whole tree has 74 nodes; (b) A sub network discovered on Y Ans dataset using Simp-hLDA algorithm, and the whole tree has 89 nodes. In </context>
</contexts>
<marker>Blei, McAuliffe, 2010</marker>
<rawString>D.M. Blei and J.D. McAuliffe. 2010. Supervised topic models. Arxiv preprint arXiv:1003.0783.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="6741" citStr="Blei et al., 2003" startWordPosition="1047" endWordPosition="1050">n 4. Section 5 details a gibbs sampling inference algorithm for SSHLDA; while Section 6 presents the experimental results. Finally, we conclude the paper and suggest directions for future research in Section 7. 2 Related Work There have been many variations of topic models. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts. Unsupervised non-hierarchical topic models are widely studied, such as LSA (Deerwester et al., 1990), pLSA (Hofmann, 1999), LDA (Blei et al., 2003), Hierarchical-concept TM (Chemudugunta et al., 2008c; Chemudugunta et al., 2008b), Correlated TM (Blei and Lafferty, 2006) and Concept TM (Chemudugunta et al., 2008a; Chemudugunta et al., 2008b) etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA, except that in LDA the topic distribution is assumed to have a Dirichlet prior. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are n</context>
<context position="22891" citStr="Blei et al., 2003" startWordPosition="3911" endWordPosition="3914">e that these datasets are very diverse: Y Ans has much fewer labels than O Hlth and O Home, but have much more documents for each label; meanwhile the depth of hierarchical tree for O Hlth and O Home can reach level 9 or above. All experiments are based on the results of models with a burn-in of 10000 Gibbs sampling iterations, symmetric priors α = 0.1 and free parameter q = 1.0; and for µ, we can obtain the estimation of µcz by fixed-point iteration (Minka, 2003). 6.2 Case Study With topic modeling, the top associated words of topics can be used as good descriptors for topics in a hierarchy (Blei et al., 2003; Blei and McAuliffe, 2010). We show in Figure 3 a pair of comparative example of the proposed model and a baseline model over Y Ans dataset. The tree-based topic visualizations of Figure 3 (a) and (b) are the results of SSHLDA and Simp-hLDA. We have three major observations from the example: (i) SSHLDA is a unified and generative model, after learning, it can obtain a hierarchy of topics; *http://dmoz.org/ 805 Figure 3: (a) A sub network discovered on Y Ans dataset using SSHLDA, and the whole tree has 74 nodes; (b) A sub network discovered on Y Ans dataset using Simp-hLDA algorithm, and the w</context>
<context position="25948" citStr="Blei et al., 2003" startWordPosition="4435" endWordPosition="4438">words, such as label “Computers &amp; Internet”, its topical words, “the to you and a”, do not reflect the connotation of the label. These observations further confirm that SSHLDA is better than the baseline model. 6.3 Perplexity Comparison A good topic model should be able to generalize to unseen data. To measure the prediction ability of our model and baselines, we compute the perplexity for each document d in the test sets. Perplexity, which is widely used in the language modeling and topic modeling community, is equivalent algebraically to the inverse of the geometric mean perword likelihood (Blei et al., 2003). Lower perplexity scores mean better. Our model, SSHLDA, will compare with three state-of-the-art models, i.e. SimphLDA, hLDA and hLLDA. Simp-hLDA has been introduced in Section 1, and hLDA and hLLDA has been reviewed in Section 2. We keep 80% of the data collection as the training set and use the remaining collection as the held-out test set. We build the mod806 els based on the train set and compute the preplexity of the test set to evaluate the models. Thus, our goal is to achieve lower perplexity score on a held-out test set. The perplexity of M test documents is calculated as: { l perple</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>T L Griffiths</author>
<author>M I Jordan</author>
<author>J B Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested chinese restaurant process. Advances in neural information processing systems,</title>
<date>2004</date>
<pages>16--106</pages>
<contexts>
<context position="2177" citStr="Blei et al., 2004" startWordPosition="305" endWordPosition="308">provement over baselines for clustering on the FScore measure. 1 Introduction Topic models, such as latent Dirichlet allocation (LDA), are useful NLP tools for the statistical analysis of document collections and other discrete data. *This work was done in National University of Singapore. †Corresponding author. Furthermore, hierarchical topic modeling is able to obtain the relations between topics — parent-child and sibling relations. Unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, such as hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al., 2004). hLDA makes use of nested Dirichlet Process to automatically obtain a L-level hierarchy of topics. Modern Web documents, however, are not merely collections of words. They are usually documents with hierarchical labels – such as Web pages and their placement in hierarchical directories (Ming et al., 2010). Unsupervised hierarchical topic modeling cannot make use of any information from hierarchical labels, thus supervised hierarchical topic models, such as hierarchical Labeled Latent Dirichlet Allocation (hLLDA) (Petinot et al., 2011), are proposed to tackle this problem. hLLDA uses hierarchi</context>
<context position="7736" citStr="Blei et al., 2004" startWordPosition="1208" endWordPosition="1211">etely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwise correlations are modeled, and the number of parameters in the covariance matrix grows as the square of the number of topics. However, the above models cannot capture the relation between super and sub topics. To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) (Blei et al., 2004), Hierarchical Dirichlet processes (HDP) (Teh et al., 2006), Pachinko Allocation Model (PAM) (Li and McCallum, 2006) and Hierarchical PAM (HPAM) (Mimno et al., 2007) etc. The relations are usually in the form of a hierarchy, such as the tree or Directed Acyclic Graph (DAG). Blei et al. (2004) proposed the hLDA model that simultaneously learns the structure of a topic hierarchy and the topics that are contained within that hierarchy. This algorithm can be used to extract topic hierarchies from large document collections. Although unsupervised topic models are suffi801 ciently expressive to mode</context>
<context position="10231" citStr="Blei et al., 2004" startWordPosition="1619" endWordPosition="1622">ledge, only hLLDA (Petinot et al., 2011) and HSLDA (Perotte et al., 2011) are proposed for this kind of data. HSLDA cannot obtain a probability distribution for a label. Although hLLDA can obtain a distribution over words for each label, hLLDA is unable to capture the relations between parent and child node using parameters, and it also cannot detect automatically latent topics in the data space. In this paper, we will propose a generative topic model to tackle these problems of hLLDA. 3 Preliminaries The nested Chinese restaurant process (nCRP) is a distribution over hierarchical partitions (Blei et al., 2004). It generalizes the Chinese restaurant process (CRP), which is a distribution over partitions. The CRP can be described by the following metaphor. Imagine a restaurant with an infinite number of tables, and imagine customers entering the restaurant in sequence. The dth customer sits at a table accordTable 1: Notations used in the paper. Sym Description V Vocabulary (word set), w is a word in V D Document collection Tj The set of paths in the sub-tree whose root is the jth leaf node in the hierarchy of observed topics m A document m that consists of words and labels wm The text of document m, </context>
<context position="13546" citStr="Blei et al., 2004" startWordPosition="2232" endWordPosition="2235">: The graphical model of SSHLDA. topic as a latent topic. 4 The Semi-Supervised Hierarchical Topic Model In this section, we will introduce a semisupervised hierarchical topic model, i.e., the SemiSupervised Hierarchical Latent Dirichlet Allocation (SSHLDA). SSHLDA is a probabilistic graphical model that describes a process for generating a hierarchical labeled document collection. Like hierarchical Labeled LDA (hLLDA) (Petinot et al., 2011), SSHLDA can incorporate labeled topics into the generative process of documents. On the other hand, like hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al., 2004), SSHLDA can automatically explore latent topic in data space, and extend the existing hierarchy of observed topics. SSHLDA makes use of not only observed topics, but also latent topics. The graphical model of SSHLDA is illustrated in Figure 1. In the model, N is the number of words in a document, D is the total number of documents in a collection, M is the number of leaf nodes in hierarchical observed nodes, cz is a node in the ith level in the hierarchical tree, η, α and µ i are dirichlet prior parameters, βk is a distribution over words, θ is a document-specific distribution over topics, δ&apos;</context>
<context position="19466" citStr="Blei et al., 2004" startWordPosition="3306" endWordPosition="3309"> specified by Equation (5). After obtaining individual word assignments z, we can estimate the topic multinomials and the per-document mixing proportions. Specifically, the topic multinomials are estimated as: (6) |V |η + ∑n.zcm,j while the per-document mixing proportions fixed can be estimated as: α + nm .,j θm,j = ,j ∈ 1, ...,|cm |(7) |cm|α + nm .,. 5.1 Relation to Existing Models In this section, we draw comparisons with the current state-of-the-art models for hierarchical topic ∏w Γ(nwcem,l,−m + nwcem,l,m + η) nm−n,j + α nm−n,. + |cm| ∝ βem,j,i = p(wi|zem,j) = η + n wi zcm,j 804 modeling (Blei et al., 2004; Petinot et al., 2011) and show that at certain choices of the parameters of our model, these methods fall out as special cases. Our method generalises not only hierarchical Latent Dirichlet Allocation (hLDA), but also Hierarchical Labeled Latent Dirichlet Allocation (hLLDA). Our proposed model provides a unified framework allowing us to model hierarchical labels while to explore new latent topics. Equivalence to hLDA As introduced in Section 2, hLDA is a unsupervised hierarchical topic model. In this case, there are no observed nodes, that is, the corpus has no hierarchical labels. This mean</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>D. Blei, T.L. Griffiths, M.I. Jordan, and J.B. Tenenbaum. 2004. Hierarchical topic models and the nested chinese restaurant process. Advances in neural information processing systems, 16:106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chemudugunta</author>
<author>A Holloway</author>
<author>P Smyth</author>
<author>M Steyvers</author>
</authors>
<title>Modeling documents by combining semantic concepts with unsupervised statistical learning. The Semantic Web-ISWC</title>
<date>2008</date>
<pages>229--244</pages>
<contexts>
<context position="6793" citStr="Chemudugunta et al., 2008" startWordPosition="1053" endWordPosition="1056">ence algorithm for SSHLDA; while Section 6 presents the experimental results. Finally, we conclude the paper and suggest directions for future research in Section 7. 2 Related Work There have been many variations of topic models. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts. Unsupervised non-hierarchical topic models are widely studied, such as LSA (Deerwester et al., 1990), pLSA (Hofmann, 1999), LDA (Blei et al., 2003), Hierarchical-concept TM (Chemudugunta et al., 2008c; Chemudugunta et al., 2008b), Correlated TM (Blei and Lafferty, 2006) and Concept TM (Chemudugunta et al., 2008a; Chemudugunta et al., 2008b) etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA, except that in LDA the topic distribution is assumed to have a Dirichlet prior. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwi</context>
</contexts>
<marker>Chemudugunta, Holloway, Smyth, Steyvers, 2008</marker>
<rawString>C. Chemudugunta, A. Holloway, P. Smyth, and M. Steyvers. 2008a. Modeling documents by combining semantic concepts with unsupervised statistical learning. The Semantic Web-ISWC 2008, pages 229– 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chemudugunta</author>
<author>P Smyth</author>
<author>M Steyvers</author>
</authors>
<title>Combining concept hierarchies and statistical topic models.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM conference on Information and knowledge management,</booktitle>
<pages>1469--1470</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6793" citStr="Chemudugunta et al., 2008" startWordPosition="1053" endWordPosition="1056">ence algorithm for SSHLDA; while Section 6 presents the experimental results. Finally, we conclude the paper and suggest directions for future research in Section 7. 2 Related Work There have been many variations of topic models. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts. Unsupervised non-hierarchical topic models are widely studied, such as LSA (Deerwester et al., 1990), pLSA (Hofmann, 1999), LDA (Blei et al., 2003), Hierarchical-concept TM (Chemudugunta et al., 2008c; Chemudugunta et al., 2008b), Correlated TM (Blei and Lafferty, 2006) and Concept TM (Chemudugunta et al., 2008a; Chemudugunta et al., 2008b) etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA, except that in LDA the topic distribution is assumed to have a Dirichlet prior. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwi</context>
</contexts>
<marker>Chemudugunta, Smyth, Steyvers, 2008</marker>
<rawString>C. Chemudugunta, P. Smyth, and M. Steyvers. 2008b. Combining concept hierarchies and statistical topic models. In Proceeding of the 17th ACM conference on Information and knowledge management, pages 1469– 1470. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chemudugunta</author>
<author>P Smyth</author>
<author>M Steyvers</author>
</authors>
<title>Text modeling using unsupervised topic models and concept hierarchies. Arxiv preprint arXiv:0808.0973.</title>
<date>2008</date>
<contexts>
<context position="6793" citStr="Chemudugunta et al., 2008" startWordPosition="1053" endWordPosition="1056">ence algorithm for SSHLDA; while Section 6 presents the experimental results. Finally, we conclude the paper and suggest directions for future research in Section 7. 2 Related Work There have been many variations of topic models. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts. Unsupervised non-hierarchical topic models are widely studied, such as LSA (Deerwester et al., 1990), pLSA (Hofmann, 1999), LDA (Blei et al., 2003), Hierarchical-concept TM (Chemudugunta et al., 2008c; Chemudugunta et al., 2008b), Correlated TM (Blei and Lafferty, 2006) and Concept TM (Chemudugunta et al., 2008a; Chemudugunta et al., 2008b) etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA, except that in LDA the topic distribution is assumed to have a Dirichlet prior. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwi</context>
</contexts>
<marker>Chemudugunta, Smyth, Steyvers, 2008</marker>
<rawString>C. Chemudugunta, P. Smyth, and M. Steyvers. 2008c. Text modeling using unsupervised topic models and concept hierarchies. Arxiv preprint arXiv:0808.0973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American society for information science,</journal>
<pages>41--6</pages>
<contexts>
<context position="6694" citStr="Deerwester et al., 1990" startWordPosition="1039" endWordPosition="1042">me preliminaries; while we introduce SSHLDA in Section 4. Section 5 details a gibbs sampling inference algorithm for SSHLDA; while Section 6 presents the experimental results. Finally, we conclude the paper and suggest directions for future research in Section 7. 2 Related Work There have been many variations of topic models. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts. Unsupervised non-hierarchical topic models are widely studied, such as LSA (Deerwester et al., 1990), pLSA (Hofmann, 1999), LDA (Blei et al., 2003), Hierarchical-concept TM (Chemudugunta et al., 2008c; Chemudugunta et al., 2008b), Correlated TM (Blei and Lafferty, 2006) and Concept TM (Chemudugunta et al., 2008a; Chemudugunta et al., 2008b) etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA, except that in LDA the topic distribution is assumed to have a Dirichlet prior. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Co</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proc. of Uncertainty in Artificial Intelligence, UAI’99,</booktitle>
<pages>21</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6716" citStr="Hofmann, 1999" startWordPosition="1044" endWordPosition="1045">duce SSHLDA in Section 4. Section 5 details a gibbs sampling inference algorithm for SSHLDA; while Section 6 presents the experimental results. Finally, we conclude the paper and suggest directions for future research in Section 7. 2 Related Work There have been many variations of topic models. The existing topic models can be divided into four categories: Unsupervised non-hierarchical topic models, Unsupervised hierarchical topic models, and their corresponding supervised counterparts. Unsupervised non-hierarchical topic models are widely studied, such as LSA (Deerwester et al., 1990), pLSA (Hofmann, 1999), LDA (Blei et al., 2003), Hierarchical-concept TM (Chemudugunta et al., 2008c; Chemudugunta et al., 2008b), Correlated TM (Blei and Lafferty, 2006) and Concept TM (Chemudugunta et al., 2008a; Chemudugunta et al., 2008b) etc. The most famous one is Latent Dirichlet Allocation (LDA). LDA is similar to pLSA, except that in LDA the topic distribution is assumed to have a Dirichlet prior. LDA is a completely unsupervised algorithm that models each document as a mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>T. Hofmann. 1999. Probabilistic latent semantic analysis. In Proc. of Uncertainty in Artificial Intelligence, UAI’99, page 21. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Karypis</author>
</authors>
<title>Cluto: Software for clustering high dimensional datasets. Internet Website (last accessed,</title>
<date>2005</date>
<note>http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.</note>
<contexts>
<context position="27845" citStr="Karypis, 2005" startWordPosition="4767" endWordPosition="4768">ata better than the state-of-the-art models. We can also obtain similar experimental results over Y Ans and O Home datasets, and their detailed description is not included in this paper due to the limitation of space. 6.4 Clustering performance To evaluate indirectly the performance of the proposed model, we compare the clustering performance of following systems: 1) the proposed model; 2) Simp-hLDA; 3) hLDA; 4) agglomerative clustering algorithm. There are many agglomerative clustering algorithms, and in this paper, we make use of the single-linkage method in a software package called CLUTO (Karypis, 2005) to obtain hierarchies of clusters over our datasets, with words as features. We refer the method as h-clustering. Given a document collection DS with a H-level hierarchy of labels, each label in the hierarchy and corresponding documents will be taken as the ground truth of clustering algorithms. The hierarchy of labels denoted as GT-tree. The process of evaluation is as follows. First, we choose top l-level labels in GT-tree as an observed hierarchy, i.e. Base Tree (BT), and we need to construct a L-level hierarchy (l &lt; L &lt;= H) over the documents DS using a Figure 4: Perplexities of hLLDA, hL</context>
</contexts>
<marker>Karypis, 2005</marker>
<rawString>G. Karypis. 2005. Cluto: Software for clustering high dimensional datasets. Internet Website (last accessed, June 2008), http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Julien</author>
<author>F Sha</author>
<author>M I Jordan</author>
</authors>
<title>ndisclda: Discriminative learning for dimensionality reduction and classification.</title>
<date>2008</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>21</pages>
<contexts>
<context position="8721" citStr="Lacoste-Julien et al., 2008" startWordPosition="1364" endWordPosition="1367">e of a topic hierarchy and the topics that are contained within that hierarchy. This algorithm can be used to extract topic hierarchies from large document collections. Although unsupervised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b), Author TM (Rosen-Zvi et al., 2004), FlatLDA (Rubin et al., 2011), Prior-LDA (Rubin et al., 2011), Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve</context>
</contexts>
<marker>Lacoste-Julien, Sha, Jordan, 2008</marker>
<rawString>S. Lacoste-Julien, F. Sha, and M.I. Jordan. 2008. ndisclda: Discriminative learning for dimensionality reduction and classification. Advances in Neural Information Processing Systems, 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Pachinko allocation: Dag-structured mixture models of topic correlations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd international conference on Machine learning,</booktitle>
<pages>577--584</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7852" citStr="Li and McCallum, 2006" startWordPosition="1225" endWordPosition="1228">y represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwise correlations are modeled, and the number of parameters in the covariance matrix grows as the square of the number of topics. However, the above models cannot capture the relation between super and sub topics. To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) (Blei et al., 2004), Hierarchical Dirichlet processes (HDP) (Teh et al., 2006), Pachinko Allocation Model (PAM) (Li and McCallum, 2006) and Hierarchical PAM (HPAM) (Mimno et al., 2007) etc. The relations are usually in the form of a hierarchy, such as the tree or Directed Acyclic Graph (DAG). Blei et al. (2004) proposed the hLDA model that simultaneously learns the structure of a topic hierarchy and the topics that are contained within that hierarchy. This algorithm can be used to extract topic hierarchies from large document collections. Although unsupervised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate th</context>
</contexts>
<marker>Li, McCallum, 2006</marker>
<rawString>W. Li and A. McCallum. 2006. Pachinko allocation: Dag-structured mixture models of topic correlations. In Proceedings of the 23rd international conference on Machine learning, pages 577–584. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Schutze</author>
</authors>
<title>Introduction to information retrieval, volume 1.</title>
<date>2008</date>
<publisher>Cambridge University Press Cambridge.</publisher>
<contexts>
<context position="29983" citStr="Manning et al., 2008" startWordPosition="5137" endWordPosition="5140">ster. (iv) for SSHLDA, we set height parameter as L. After training, each document is also assigned to top-1 topic. Topics and their corresponding documents form a hierarchy of clusters. 6.4.1 Evaluation Metrics For each dataset we obtain corresponding clusters using the various models described in previous sections. Thus we can use clustering metrics to measure the quality of various algorithms by using a measure that takes into account the overall set of clusters that are represented in the new generated part of a hierarchical tree. One such measure is the FScore measure, intro807 duced by (Manning et al., 2008). Given a particular class Cr of size nr and a particular cluster Si of size ni, suppose nri documents in the cluster Si belong to Cr, then the FScore of this class and cluster is defined to be 2 � R(Cr, Si) � P(Cr, Si) F(Cr, Si) = (11) R(Cr, Si) + P(Cr, Si) where R(Cr, Si) is the recall value defined as nri/nr, and P(Cr, Si) is the precision value defined as nri/ni for the class Cr and the cluster Si. The FScore of the class Cr, is the maximum FScore value attained at any node in the hierarchical clustering tree T. That is, F(Cr) = max F(Cr, Si). (12) S;ET The FScore of the entire clustering </context>
</contexts>
<marker>Manning, Raghavan, Schutze, 2008</marker>
<rawString>C.D. Manning, P. Raghavan, and H. Schutze. 2008. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Mixtures of hierarchical topics with pachinko allocation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 24th international conference on Machine learning,</booktitle>
<pages>633--640</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7901" citStr="Mimno et al., 2007" startWordPosition="1233" endWordPosition="1236">m, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwise correlations are modeled, and the number of parameters in the covariance matrix grows as the square of the number of topics. However, the above models cannot capture the relation between super and sub topics. To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) (Blei et al., 2004), Hierarchical Dirichlet processes (HDP) (Teh et al., 2006), Pachinko Allocation Model (PAM) (Li and McCallum, 2006) and Hierarchical PAM (HPAM) (Mimno et al., 2007) etc. The relations are usually in the form of a hierarchy, such as the tree or Directed Acyclic Graph (DAG). Blei et al. (2004) proposed the hLDA model that simultaneously learns the structure of a topic hierarchy and the topics that are contained within that hierarchy. This algorithm can be used to extract topic hierarchies from large document collections. Although unsupervised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. </context>
</contexts>
<marker>Mimno, Li, McCallum, 2007</marker>
<rawString>D. Mimno, W. Li, and A. McCallum. 2007. Mixtures of hierarchical topics with pachinko allocation. In Proceedings of the 24th international conference on Machine learning, pages 633–640. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Y Ming</author>
<author>K Wang</author>
<author>T S Chua</author>
</authors>
<title>Prototype hierarchy based clustering for the categorization and navigation of web collections.</title>
<date>2010</date>
<booktitle>In Proceeding of the 33rd international ACM SIGIR,</booktitle>
<pages>2--9</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2484" citStr="Ming et al., 2010" startWordPosition="355" endWordPosition="358">uthor. Furthermore, hierarchical topic modeling is able to obtain the relations between topics — parent-child and sibling relations. Unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, such as hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al., 2004). hLDA makes use of nested Dirichlet Process to automatically obtain a L-level hierarchy of topics. Modern Web documents, however, are not merely collections of words. They are usually documents with hierarchical labels – such as Web pages and their placement in hierarchical directories (Ming et al., 2010). Unsupervised hierarchical topic modeling cannot make use of any information from hierarchical labels, thus supervised hierarchical topic models, such as hierarchical Labeled Latent Dirichlet Allocation (hLLDA) (Petinot et al., 2011), are proposed to tackle this problem. hLLDA uses hierarchical labels to automatically build corresponding topic for each label, but it cannot find new latent topics in the data space, only depending on hierarchy of labels. As we know that only about 10% of an iceberg’s mass is seen outside while about 90% of it is unseen, deep down in water. We think that a corpu</context>
</contexts>
<marker>Ming, Wang, Chua, 2010</marker>
<rawString>Z.Y. Ming, K. Wang, and T.S. Chua. 2010. Prototype hierarchy based clustering for the categorization and navigation of web collections. In Proceeding of the 33rd international ACM SIGIR, pages 2–9. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T P Minka</author>
</authors>
<title>Estimating a dirichlet distribution.</title>
<date>2003</date>
<journal>Annals of Physics,</journal>
<volume>2000</volume>
<issue>8</issue>
<contexts>
<context position="22742" citStr="Minka, 2003" startWordPosition="3886" endWordPosition="3887">as O Home, and the data from the category Health as O Hlth. The statistics of all datasets are summarized in Table 2. From this table, we can see that these datasets are very diverse: Y Ans has much fewer labels than O Hlth and O Home, but have much more documents for each label; meanwhile the depth of hierarchical tree for O Hlth and O Home can reach level 9 or above. All experiments are based on the results of models with a burn-in of 10000 Gibbs sampling iterations, symmetric priors α = 0.1 and free parameter q = 1.0; and for µ, we can obtain the estimation of µcz by fixed-point iteration (Minka, 2003). 6.2 Case Study With topic modeling, the top associated words of topics can be used as good descriptors for topics in a hierarchy (Blei et al., 2003; Blei and McAuliffe, 2010). We show in Figure 3 a pair of comparative example of the proposed model and a baseline model over Y Ans dataset. The tree-based topic visualizations of Figure 3 (a) and (b) are the results of SSHLDA and Simp-hLDA. We have three major observations from the example: (i) SSHLDA is a unified and generative model, after learning, it can obtain a hierarchy of topics; *http://dmoz.org/ 805 Figure 3: (a) A sub network discover</context>
</contexts>
<marker>Minka, 2003</marker>
<rawString>T.P. Minka. 2003. Estimating a dirichlet distribution. Annals of Physics, 2000(8):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Perotte</author>
<author>N Bartlett</author>
<author>N Elhadad</author>
<author>F Wood</author>
</authors>
<title>Hierarchically supervised latent dirichlet allocation. Neural Information Processing Systems</title>
<date>2011</date>
<note>(to appear).</note>
<contexts>
<context position="9686" citStr="Perotte et al., 2011" startWordPosition="1529" endWordPosition="1532">nstrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve this problem. None of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in the label space. For hierarchical labeled data, there are also few models that are able to handle the label relations in data. To the best of our knowledge, only hLLDA (Petinot et al., 2011) and HSLDA (Perotte et al., 2011) are proposed for this kind of data. HSLDA cannot obtain a probability distribution for a label. Although hLLDA can obtain a distribution over words for each label, hLLDA is unable to capture the relations between parent and child node using parameters, and it also cannot detect automatically latent topics in the data space. In this paper, we will propose a generative topic model to tackle these problems of hLLDA. 3 Preliminaries The nested Chinese restaurant process (nCRP) is a distribution over hierarchical partitions (Blei et al., 2004). It generalizes the Chinese restaurant process (CRP), </context>
</contexts>
<marker>Perotte, Bartlett, Elhadad, Wood, 2011</marker>
<rawString>A. Perotte, N. Bartlett, N. Elhadad, and F. Wood. 2011. Hierarchically supervised latent dirichlet allocation. Neural Information Processing Systems (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Petinot</author>
<author>K McKeown</author>
<author>K Thadani</author>
</authors>
<title>A hierarchical model of web summaries.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies: short</booktitle>
<volume>2</volume>
<pages>670--675</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2718" citStr="Petinot et al., 2011" startWordPosition="387" endWordPosition="390">ace, such as hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al., 2004). hLDA makes use of nested Dirichlet Process to automatically obtain a L-level hierarchy of topics. Modern Web documents, however, are not merely collections of words. They are usually documents with hierarchical labels – such as Web pages and their placement in hierarchical directories (Ming et al., 2010). Unsupervised hierarchical topic modeling cannot make use of any information from hierarchical labels, thus supervised hierarchical topic models, such as hierarchical Labeled Latent Dirichlet Allocation (hLLDA) (Petinot et al., 2011), are proposed to tackle this problem. hLLDA uses hierarchical labels to automatically build corresponding topic for each label, but it cannot find new latent topics in the data space, only depending on hierarchy of labels. As we know that only about 10% of an iceberg’s mass is seen outside while about 90% of it is unseen, deep down in water. We think that a corpus with hierarchical labels should include not only observed topics of labels, but also there are more latent topics, just like icebergs. hLLDA can make use of the information from labels; while hLDA can explore latent topics. How can </context>
<context position="9653" citStr="Petinot et al., 2011" startWordPosition="1523" endWordPosition="1526">ge et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve this problem. None of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in the label space. For hierarchical labeled data, there are also few models that are able to handle the label relations in data. To the best of our knowledge, only hLLDA (Petinot et al., 2011) and HSLDA (Perotte et al., 2011) are proposed for this kind of data. HSLDA cannot obtain a probability distribution for a label. Although hLLDA can obtain a distribution over words for each label, hLLDA is unable to capture the relations between parent and child node using parameters, and it also cannot detect automatically latent topics in the data space. In this paper, we will propose a generative topic model to tackle these problems of hLLDA. 3 Preliminaries The nested Chinese restaurant process (nCRP) is a distribution over hierarchical partitions (Blei et al., 2004). It generalizes the C</context>
<context position="13373" citStr="Petinot et al., 2011" startWordPosition="2207" endWordPosition="2210"> words for a label, we refer the topic with a corresponding label as a labeled topic. If a model can learn an unseen and latent topic without a label, we refer the 802 Figure 1: The graphical model of SSHLDA. topic as a latent topic. 4 The Semi-Supervised Hierarchical Topic Model In this section, we will introduce a semisupervised hierarchical topic model, i.e., the SemiSupervised Hierarchical Latent Dirichlet Allocation (SSHLDA). SSHLDA is a probabilistic graphical model that describes a process for generating a hierarchical labeled document collection. Like hierarchical Labeled LDA (hLLDA) (Petinot et al., 2011), SSHLDA can incorporate labeled topics into the generative process of documents. On the other hand, like hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al., 2004), SSHLDA can automatically explore latent topic in data space, and extend the existing hierarchy of observed topics. SSHLDA makes use of not only observed topics, but also latent topics. The graphical model of SSHLDA is illustrated in Figure 1. In the model, N is the number of words in a document, D is the total number of documents in a collection, M is the number of leaf nodes in hierarchical observed nodes, cz is a node i</context>
<context position="19489" citStr="Petinot et al., 2011" startWordPosition="3310" endWordPosition="3313">ion (5). After obtaining individual word assignments z, we can estimate the topic multinomials and the per-document mixing proportions. Specifically, the topic multinomials are estimated as: (6) |V |η + ∑n.zcm,j while the per-document mixing proportions fixed can be estimated as: α + nm .,j θm,j = ,j ∈ 1, ...,|cm |(7) |cm|α + nm .,. 5.1 Relation to Existing Models In this section, we draw comparisons with the current state-of-the-art models for hierarchical topic ∏w Γ(nwcem,l,−m + nwcem,l,m + η) nm−n,j + α nm−n,. + |cm| ∝ βem,j,i = p(wi|zem,j) = η + n wi zcm,j 804 modeling (Blei et al., 2004; Petinot et al., 2011) and show that at certain choices of the parameters of our model, these methods fall out as special cases. Our method generalises not only hierarchical Latent Dirichlet Allocation (hLDA), but also Hierarchical Labeled Latent Dirichlet Allocation (hLLDA). Our proposed model provides a unified framework allowing us to model hierarchical labels while to explore new latent topics. Equivalence to hLDA As introduced in Section 2, hLDA is a unsupervised hierarchical topic model. In this case, there are no observed nodes, that is, the corpus has no hierarchical labels. This means cm is equal to ce„t,m</context>
</contexts>
<marker>Petinot, McKeown, Thadani, 2011</marker>
<rawString>Y. Petinot, K. McKeown, and K. Thadani. 2011. A hierarchical model of web summaries. In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies: short papers-Volume 2, pages 670–675. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>D Hall</author>
<author>R Nallapati</author>
<author>C D Manning</author>
</authors>
<title>Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>248--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8864" citStr="Ramage et al., 2009" startWordPosition="1389" endWordPosition="1392">ent collections. Although unsupervised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b), Author TM (Rosen-Zvi et al., 2004), FlatLDA (Rubin et al., 2011), Prior-LDA (Rubin et al., 2011), Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve this problem. None of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in t</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>D. Ramage, D. Hall, R. Nallapati, and C.D. Manning. 2009a. Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 248–256. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>P Heymann</author>
<author>C D Manning</author>
<author>H GarciaMolina</author>
</authors>
<title>Clustering the tagged web.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>54--63</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8864" citStr="Ramage et al., 2009" startWordPosition="1389" endWordPosition="1392">ent collections. Although unsupervised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b), Author TM (Rosen-Zvi et al., 2004), FlatLDA (Rubin et al., 2011), Prior-LDA (Rubin et al., 2011), Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve this problem. None of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in t</context>
</contexts>
<marker>Ramage, Heymann, Manning, GarciaMolina, 2009</marker>
<rawString>D. Ramage, P. Heymann, C.D. Manning, and H. GarciaMolina. 2009b. Clustering the tagged web. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 54–63. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>C D Manning</author>
<author>S Dumais</author>
</authors>
<title>Partially labeled topic models for interpretable text mining.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>457--465</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9048" citStr="Ramage et al., 2011" startWordPosition="1419" endWordPosition="1422">unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b), Author TM (Rosen-Zvi et al., 2004), FlatLDA (Rubin et al., 2011), Prior-LDA (Rubin et al., 2011), Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve this problem. None of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in the label space. For hierarchical labeled data, there are also few models that are able to handle the label relations in data. To the best of our knowledge, only hLLDA (Petinot et al., </context>
</contexts>
<marker>Ramage, Manning, Dumais, 2011</marker>
<rawString>D. Ramage, C.D. Manning, and S. Dumais. 2011. Partially labeled topic models for interpretable text mining. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 457–465. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosen-Zvi</author>
<author>T Griffiths</author>
<author>M Steyvers</author>
<author>P Smyth</author>
</authors>
<title>The author-topic model for authors and documents.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th conference on Uncertainty in artificial intelligence,</booktitle>
<pages>487--494</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="8902" citStr="Rosen-Zvi et al., 2004" startWordPosition="1395" endWordPosition="1398">ised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b), Author TM (Rosen-Zvi et al., 2004), FlatLDA (Rubin et al., 2011), Prior-LDA (Rubin et al., 2011), Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve this problem. None of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in the label space. For hierarchical label</context>
</contexts>
<marker>Rosen-Zvi, Griffiths, Steyvers, Smyth, 2004</marker>
<rawString>M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. 2004. The author-topic model for authors and documents. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 487–494. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T N Rubin</author>
<author>A Chambers</author>
<author>P Smyth</author>
<author>M Steyvers</author>
</authors>
<title>Statistical topic models for multi-label document classification. Arxiv preprint arXiv:1107.2462.</title>
<date>2011</date>
<contexts>
<context position="8932" citStr="Rubin et al., 2011" startWordPosition="1401" endWordPosition="1404">ntly expressive to model multiple topics per document, they are inappropriate for labeled corpora because they are unable to incorporate the observed labels into their learning procedure. Several modifications of LDA to incorporate supervision have been proposed in the literature. Two such models, Supervised LDA (Blei and McAuliffe, 2007; Blei and McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008) are first proposed to model documents associated only with a single label. Another category of models, such as the MM-LDA (Ramage et al., 2009b), Author TM (Rosen-Zvi et al., 2004), FlatLDA (Rubin et al., 2011), Prior-LDA (Rubin et al., 2011), Dependency-LDA (Rubin et al., 2011) and Partially LDA (PLDA) (Ramage et al., 2011) etc., are not constrained to one label per document because they model each document as a bag of words with a bag of labels. However, these models obtain topics that do not correspond directly with the labels. Labeled LDA (LLDA) (Ramage et al., 2009a) can be used to solve this problem. None of these non-hierarchical supervised models, however, leverage on dependency structure, such as parent-child relation, in the label space. For hierarchical labeled data, there are also few mo</context>
</contexts>
<marker>Rubin, Chambers, Smyth, Steyvers, 2011</marker>
<rawString>T.N. Rubin, A. Chambers, P. Smyth, and M. Steyvers. 2011. Statistical topic models for multi-label document classification. Arxiv preprint arXiv:1107.2462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="7795" citStr="Teh et al., 2006" startWordPosition="1216" endWordPosition="1219">mixture of topics. Another famous model that not only represents topic correlations, but also learns them, is the Correlated Topic Model (CTM). Topics in CTM are not independent; however it is noted that only pairwise correlations are modeled, and the number of parameters in the covariance matrix grows as the square of the number of topics. However, the above models cannot capture the relation between super and sub topics. To address this problem, many models have been proposed to model the relations, such as Hierarchical LDA (HLDA) (Blei et al., 2004), Hierarchical Dirichlet processes (HDP) (Teh et al., 2006), Pachinko Allocation Model (PAM) (Li and McCallum, 2006) and Hierarchical PAM (HPAM) (Mimno et al., 2007) etc. The relations are usually in the form of a hierarchy, such as the tree or Directed Acyclic Graph (DAG). Blei et al. (2004) proposed the hLDA model that simultaneously learns the structure of a topic hierarchy and the topics that are contained within that hierarchy. This algorithm can be used to extract topic hierarchies from large document collections. Although unsupervised topic models are suffi801 ciently expressive to model multiple topics per document, they are inappropriate for </context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei. 2006. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
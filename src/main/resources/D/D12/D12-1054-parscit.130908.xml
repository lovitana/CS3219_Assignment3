<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.057831">
<title confidence="0.873483">
Lyrics, Music, and Emotions
</title>
<author confidence="0.997337">
Rada Mihalcea Carlo Strapparava
</author>
<affiliation confidence="0.999105">
University of North Texas FBK-irst
</affiliation>
<email confidence="0.998981">
rada@cs.unt.edu strappa@fbk.eu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801461538462">
In this paper, we explore the classification of
emotions in songs, using the music and the
lyrics representation of the songs. We intro-
duce a novel corpus of music and lyrics, con-
sisting of 100 songs annotated for emotions.
We show that textual and musical features can
both be successfully used for emotion recog-
nition in songs. Moreover, through compar-
ative experiments, we show that the joint use
of lyrics and music brings significant improve-
ments over each of the individual textual and
musical classifiers, with error rate reductions
of up to 31%.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923842105263">
Language and music are peculiar characteristics of
human beings. The capability of producing and
enjoying language and music appears in every hu-
man society, regardless of the richness of its culture
(Nettl, 2000).
Importantly, language and music complement
each other in many different ways. For instance,
looking at music and language in terms of fea-
tures, we can observe that music organizes pitch
and rhythm in ways that language does not, and it
lacks the specificity of language in terms of seman-
tic meaning. On the other hand, language is built
from categories that are absent in music (e.g., nouns
and verbs), whereas music seems to have a deeper
power over our emotions than does ordinary speech.
Composers, musicians, and researchers in poetry
and literature alike have been long fascinated by the
combination of language and music, even since the
time of the earliest written records of music encoun-
tered in musical settings for poetry. Despite this in-
terest, and despite the long history of the interaction
between music and lyrics, there is only little work
that explicitly focuses on the connection between
music and lyrics.
In this paper, we focus on the connection between
the musical and linguistic representations in popu-
lar songs, and their role in the expression of affect.
We introduce a novel corpus of lyrics and music, an-
notated for emotions at line level, and explore the
automatic recognition of emotions using both tex-
tual and musical features. Through comparative ex-
periments, we show that emotion recognition can be
performed using either textual or musical features,
and that the joint use of lyrics and music can im-
prove significantly over classifiers that use only one
dimension at a time. We believe our results demon-
strate the promise of using joint music-lyric models
for song processing.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999842166666667">
The literature on music analysis is noticeably large,
and there are several studies concerning the music’s
power over emotions (Juslin and Sloboda, 2001),
thinking (Rauscher et al., 1993), or physical effort
(Karageorghis and Priest, 2008).
In particular, there has been significant research
in music and psychology focusing on the idea of a
parallel between affective cues in music and speech
(Sundberg, 1982; Scherer, 1995). For instance,
(Scherer, 2004) investigated the types of emotions
that can be induced by music, their mechanisms, and
how they can be empirically measured. (Juslin and
</bodyText>
<page confidence="0.955235">
590
</page>
<note confidence="0.767151">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 590–599, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999553777777778">
Laukka, 2003) conducted a comprehensive review
of vocal expressions and music performance, find-
ing substantial overlap in the cues used to convey
basic emotions in speech and music.
The work most closely related to ours is the com-
bination of audio and lyrics for emotion classifica-
tion in songs, as thoroughly surveyed in (Kim et al.,
2010). Although several methods have been pro-
posed, including a combination of textual features
and beats per minute and MPEG descriptors (Yang
and Lee, 2004); individual audio and text classifiers
for arousal and valence, followed by a combination
through meta-learning (Yang et al., 2008); and the
use of crowdsourcing labeling from Last.fm to col-
lect large datasets of songs annotated for emotions
(Laurier et al., 2008; Hu et al., 2009), all this pre-
vious work was done at song level, and most of
it focused on valence-arousal classifications. None
of the previous methods considered the fine-grained
classification of emotions at line level, as we do, and
none of them considered the six Ekman emotions
used in our work.
Other related work consists of the development
of tools for music accessing, filtering, classification,
and retrieval, focusing primarily on music in digital
format such as MIDI. For instance, the task of music
retrieval and music recommendation has received a
lot of attention from both the arts and the computer
science communities (see for instance (Orio, 2006)
for an introduction to this task). There are also sev-
eral works on MIDI analysis. Among them, partic-
ularly relevant to our research is the work by (Das
et al., 2000), who described an analysis of predom-
inant up-down motion types within music, through
extraction of the kinematic variables of music veloc-
ity and acceleration from MIDI data streams. (Catal-
tepe et al., 2007) addressed music genre classifica-
tion (e.g., classic, jazz, pop) using MIDI and au-
dio features, while (Wang et al., 2004) automati-
cally aligned acoustic musical signals with their cor-
responding textual lyrics. MIDI files are typically
organized into one or more parallel “tracks” for in-
dependent recording and editing. A reliable system
to identify the MIDI track containing the melody1
is very relevant for music information retrieval, and
</bodyText>
<footnote confidence="0.774748666666667">
1A melody can be defined as a “cantabile” sequence of
notes, usually the sequence that a listener can remember after
hearing a song.
</footnote>
<bodyText confidence="0.999920434782609">
there are several approaches that have been proposed
to address this issue (Rizo et al., 2006; Velusamy et
al., 2007).
Another related study concerned with the interac-
tion of lyrics and music using an annotated corpus is
found in (O’Hara, 2011), who presented preliminary
research that checks whether the expressive meaning
of a particular harmony or harmonic sequence could
be deduced from the lyrics it accompanies, by us-
ing harmonically annotated chords from the Usenet
group alt.guitar.tab.
Finally, in natural language processing, there are
a few studies that mainly exploited the lyrics com-
ponent of the songs, while generally ignoring the
musical component. For instance, (Mahedero et al.,
2005) dealt with language identification, structure
extraction, and thematic categorization for lyrics.
(Xia et al., 2008) addressed the task of sentiment
classification in lyrics, recognizing positive and neg-
ative moods in a large dataset of Chinese pop songs,
while (Yang and Lee, 2009) approached the problem
of emotion identification in lyrics, classifying songs
from allmusic.com using a set of 23 emotions.
</bodyText>
<sectionHeader confidence="0.8916395" genericHeader="method">
3 A Corpus of Music and Lyrics
Annotated for Emotions
</sectionHeader>
<bodyText confidence="0.999981904761905">
To enable our exploration of emotions in songs, we
compiled a corpus of 100 popular songs (e.g., Danc-
ing Queen by ABBA, Hotel California by Eagles,
Let it Be by The Beatles). Popular songs exert a
lot of power on people, both at an individual level
as well as on groups, mainly because of the mes-
sage and emotions they convey. Songs can lift our
moods, make us dance, or move us to tears. Songs
are able to embody deep feelings, usually through a
combined effect of both music and lyrics.
The corpus is built starting with the MIDI tracks
of each song, by extracting the parallel alignment
of melody and lyrics. Given the non-homogeneous
quality of the MIDI files available on the Web, we
asked a professional MIDI provider for high quality
MIDI files produced for singers and musicians. The
MIDI files, which were purchased from the provider,
contain also lyrics that are synchronized with the
notes. In these MIDI files, the melody channel is un-
equivocally decided by the provider, making it easier
to extract the music and the corresponding lyrics.
</bodyText>
<page confidence="0.996694">
591
</page>
<bodyText confidence="0.999798942857143">
MIDI format. MIDI is an industry-standard pro-
tocol that enables electronic musical instruments,
computers, and other electronic equipment to com-
municate and synchronize with each other. Unlike
analog devices, MIDI does not transmit an audio
signal: it sends event messages about musical no-
tation, pitch, and intensity, control signals for pa-
rameters such as volume, vibrato, and panning, and
cues and clock signals to set the tempo. As an elec-
tronic protocol, it is notable for its widespread adop-
tion throughout the music industry.
MIDI files are typically created using computer-
based sequencing software that organizes MIDI
messages into one or more parallel “tracks” for in-
dependent recording, editing, and playback. In most
sequencers, each track is assigned to a specific MIDI
channel, which can be then associated to specific in-
strument patches. MIDI files can also contain lyrics,
which can be displayed in synchrony with the music.
Starting with the MIDI tracks of a song, we ex-
tract and explicitly encode the following features.
At the song level, the key of the song (e.g., G ma-
jor, C minor). At the line level, we represent the
raising, which is the musical interval (in half-steps)
between the first note in the line and the most impor-
tant note (i.e., the note in the line with the longest
duration). Finally, at the note level, we encode the
time code of the note with respect to the beginning
of the song; the note aligned with the corresponding
syllable; the degree of the note with relation to the
key of the song; and the duration of the note.
Table 1 shows statistics on the corpus. An exam-
ple from the corpus, consisting of the first two lines
from the Beatles’ song A hard day’s night, is illus-
trated in Figure 3.
</bodyText>
<table confidence="0.927006">
SONGS 100
SONGS IN “MAJOR” KEY 59
SONGS IN “MINOR” KEY 41
LINES 4,976
ALIGNED SYLLABLES / NOTES 34,045
</table>
<tableCaption confidence="0.999839">
Table 1: Some statistics of the corpus
</tableCaption>
<subsectionHeader confidence="0.810834">
Emotion Annotations with Mechanical Turk. In
</subsectionHeader>
<bodyText confidence="0.999976647058824">
order to explore the classification of emotions in
songs, we needed a gold standard consisting of man-
ual emotion annotations of the songs. Following
previous work on emotion annotation of text (Alm
et al., 2005; Strapparava and Mihalcea, 2007), to
annotate the emotions in songs we use the six ba-
sic emotions proposed by (Ekman, 1993): ANGER,
DISGUST, FEAR, JOY, SADNESS, SURPRISE. To col-
lect the annotations, we use the Amazon Mechanical
Turk service, which was previously found to pro-
duce reliable annotations with a quality comparable
to those generated by experts (Snow et al., 2008).
The annotations are collected at line level, with a
separate annotation for each of the six emotions. We
collect numerical annotations using a scale between
0 and 10, with 0 corresponding to the absence of an
emotion, and 10 corresponding to the highest inten-
sity. Each HIT (i.e., annotation session) contains an
entire song, with a number of lines ranging from 14
to 110, for an average of 50 lines per song.
The annotators were instructed to: (1) Score the
emotions from the writer perspective, not their own
perspective; (2) Read and interpret each line in con-
text; i.e., they were asked to read and understand
the entire song before producing any annotations;
(3) Produce the six emotion annotations independent
from each other, accounting for the fact that a line
could contain none, one, or multiple emotions. In
addition to the lyrics, the song was also available
online, so they could listen to it in case they were
not familiar with it. The annotators were also given
three different examples to illustrate the annotation.
While the use of crowdsourcing for data annota-
tion can result in a large number of annotations in
a very short amount of time, it also has the draw-
back of potential spamming that can interfere with
the quality of the annotations. To address this aspect,
we used two different techniques to prevent spam.
First, in each song we inserted a “checkpoint” at a
random position in the song – a fake line that reads
“Please enter 7 for each of the six emotions.” Those
annotators who did not follow this concrete instruc-
tion were deemed as spammers who produce anno-
tations without reading the content of the song, and
thus removed. Second, for each remaining annota-
tor, we calculated the Pearson correlation between
her emotion scores and the average emotion scores
of all the other annotators. Those annotators with a
correlation with the average of the other annotators
below 0.4 were also removed, thus leaving only the
reliable annotators in the pool.
</bodyText>
<page confidence="0.993256">
592
</page>
<construct confidence="0.976417846153846">
&lt;song filename=AHARDDAY.m2a&gt;
&lt;key time=0&gt;G major&lt;/key&gt;
&lt;line pvers=1 raising=3 anger=1.5 disgust=0.7 sadness=2.5 surprise=0.8 &gt;
&lt;token time=5040 orig−note=B degree=3 duration=210&gt;IT&lt;/token&gt;
&lt;token time=5050 orig−note=B degree=3 duration=210&gt;’S &lt;/token&gt;
&lt;token time=5280 orig−note=C’ degree=4 duration=210&gt;BEEN &lt;/token&gt;
&lt;token time=5520 orig−note=B degree=3 duration=210&gt;A &lt;/token&gt;
&lt;token time=5760 orig−note=D’ degree=5 duration=810&gt;HARD &lt;/token&gt;
&lt;token time=6720 orig−note=D’ degree=5 duration=570&gt;DAY&lt;/token&gt;
&lt;token time=6730 orig−note=D’ degree=5 duration=570&gt;’S &lt;/token&gt;
&lt;token time=7440 orig−note=D’ degree=5 duration=690&gt;NIGHT&lt;/token&gt;
&lt;/line&gt;
&lt;line pvers=2 raising=5 anger=3.5 disgust=2 sadness=1.2 surprise=0.2 &gt;
&lt;token time=8880 orig−note=C’ degree=4 duration=212&gt;AND &lt;/token&gt;
&lt;token time=9120 orig−note=D’ degree=5 duration=210&gt;I&lt;/token&gt;
&lt;token time=9130 orig−note=D’ degree=5 duration=210&gt;’VE &lt;/token&gt;
&lt;token time=9360 orig−note=C’ degree=4 duration=210&gt;BEEN &lt;/token&gt;
&lt;token time=9600 orig−note=D’ degree=5 duration=210&gt;WOR&lt;/token&gt;
&lt;token time=9840 orig−note=F’ degree=7− duration=930&gt;KING &lt;/token&gt;
&lt;token time=10800 orig−note=D’ degree=5 duration=210&gt;LI&lt;/token&gt;
&lt;token time=11040 orig−note=C’ degree=4 duration=210&gt;KE &lt;/token&gt;
&lt;token time=11050 orig−note=C’ degree=4 duration=210&gt;A &lt;/token&gt;
&lt;token time=11280 orig−note=D’ degree=5 duration=330&gt;D&lt;/token&gt;
&lt;token time=11640 orig−note=C’ degree=4 duration=90&gt;O&lt;/token&gt;
&lt;token time=11760 orig−note=B degree=3 duration=330&gt;G&lt;/token&gt;
&lt;/line&gt;
</construct>
<figureCaption confidence="0.99934">
Figure 1: Two lines of a song in the corpus: It-’s been a hard day-’s night, And I-’ve been wor-king li-ke a d-o-g
</figureCaption>
<bodyText confidence="0.999936615384616">
For each song, we start by asking for ten annota-
tions. After spam removal, we were left with about
two-five annotations per song. The final annotations
are produced by averaging the emotions scores pro-
duced by the reliable annotators. Figure 3 shows
an example of the emotion scores produced for two
lines. The overall correlation between the remain-
ing reliable annotators was calculated as 0.73, which
represents a strong correlation.
For each of the six emotions, Table 2 shows the
number of lines that had that emotion present (i.e.,
the score of the emotion was different from 0), as
well as the average score for that emotion over all
4,976 lines in the corpus. Perhaps not surprisingly,
the emotions that are dominant in the corpus are JOY
and SADNESS – which are the emotions that are of-
ten invoked by people as the reason behind a song.
Note that the emotions do not exclude each other:
i.e., a line that is labeled as containing JOY may also
contain a certain amount of SADNESS, which is the
reason for the high percentage of songs containing
both JOY and SADNESS. The emotional load for
the overlapping emotions is however very different.
For instance, the lines that have a JOY score of 5
or higher have an average SADNESS score of 0.34.
Conversely, the lines with a SADNESS score of 5 or
</bodyText>
<table confidence="0.967329125">
Number
Emotion lines Average
ANGER 2,516 0.95
DISGUST 2,461 0.71
FEAR 2,719 0.77
JOY 3,890 3.24
SADNESS 3,840 2.27
SURPRISE 2,982 0.83
</table>
<tableCaption confidence="0.885695666666667">
Table 2: Emotions in the corpus of 100 songs: number
of lines including a certain emotion, and average emotion
score computed over all the 4,976 lines.
</tableCaption>
<bodyText confidence="0.939659">
higher have a JOY score of 0.22.
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="method">
4 Experiments and Evaluations
</sectionHeader>
<bodyText confidence="0.9999217">
Through our experiments, we seek to determine the
extent to which we can automatically determine the
emotional load of each line in a song, for each of the
six emotion dimensions.
We use two main classes of features: textual fea-
tures, which build upon the textual representation of
the lyrics; and musical features, which rely on the
musical notation associated with the songs. We run
three sets of experiments. The first one is intended to
determine the usefulness of the textual features for
</bodyText>
<page confidence="0.994917">
593
</page>
<bodyText confidence="0.999059">
emotion classification. The second set specifically
focuses on the musical features. Finally, the last set
of experiments makes joint use of textual and musi-
cal features.
The experiments are run using linear regression,2
and the results are evaluated by measuring the Pear-
son correlation between the classifier predictions
and the gold standard. For each experiment, a ten-
fold cross validation is run on the entire dataset.3
</bodyText>
<subsectionHeader confidence="0.988947">
4.1 Textual Features
</subsectionHeader>
<bodyText confidence="0.999936333333334">
First, we attempt to identify the emotions in a line
by relying exclusively on the features that can be de-
rived from the lyrics of the song. We decided to fo-
cus on those features that were successfully used in
the past for emotion classification (Strapparava and
Mihalcea, 2008). Specifically, we use: (1) unigram
features obtained from a bag-of-words representa-
tion, which are the features typically used by corpus-
based methods; and (2) lexicon features, indicating
the appartenance of a word to a semantic class de-
fined in manually crafted lexicons, which are often
used by knowledge-based methods.
Unigrams. We use a bag-of-words representation
of the lyrics to derive unigram counts, which are
then used as input features. First, we build a vo-
cabulary consisting of all the words, including stop-
words, occurring in the lyrics of the training set. We
then remove those words that have a frequency be-
low 10 (value determined empirically on a small de-
velopment set). The remaining words represent the
unigram features, which are then associated with a
value corresponding to the frequency of the unigram
inside each line. Note that we also attempted to use
higher order n-grams (bigrams and trigrams), but
evaluations on a small development dataset did not
show any improvements over the unigram model,
and thus all the experiments are run using unigrams.
Semantic Classes. We also derive and use coarse
textual features, by using mappings between words
and semantic classes. Specifically, we use the Lin-
</bodyText>
<footnote confidence="0.921489571428571">
2We use the Weka machine learning toolkit.
3There is no clear way to determine a baseline for these
experiments. A simple baseline that we calculated, which as-
sumed by default an emotional score equal to the average of the
scores on the training data, and measured the correlation be-
tween these default scores and the gold standard, consistently
led to correlations close to 0 (0.0081-0.0221).
</footnote>
<bodyText confidence="0.99942795">
guistic Inquiry and Word Count (LIWC) and Word-
Net Affect (WA) to derive coarse textual features.
LIWC was developed as a resource for psycholin-
guistic analysis (Pennebaker and Francis, 1999;
Pennebaker and King, 1999). The 2001 version of
LIWC includes about 2,200 words and word stems
grouped into about 70 broad categories relevant to
psychological processes (e.g., emotion, cognition).
WA (Strapparava and Valitutti, 2004) is a resource
that was created starting with WordNet, by annotat-
ing synsets with several emotions. It uses several re-
sources for affective information, including the emo-
tion classification of Ortony (Ortony et al., 1987).
From WA, we extract the words corresponding to
the six basic emotions used in our experiments. For
each semantic class, we infer a feature indicating the
number of words in a line belonging to that class.
Table 3 shows the Pearson correlations obtained
for each of the six emotions, when using only uni-
grams, only semantic classes, or both.
</bodyText>
<table confidence="0.999653666666667">
Emotion Unigrams Semantic All
Classes Textual
ANGER 0.5525 0.3044 0.5658
DISGUST 0.4246 0.2394 0.4322
FEAR 0.3744 0.2443 0.4041
JOY 0.5636 0.3659 0.5769
SADNESS 0.5291 0.3006 0.5418
SURPRISE 0.3214 0.2153 0.3392
AVERAGE 0.4609 0.2783 0.4766
</table>
<tableCaption confidence="0.9915815">
Table 3: Evaluations using textual features: unigrams,
semantic classes, and all the textual features.
</tableCaption>
<subsectionHeader confidence="0.99605">
4.2 Musical Features.
</subsectionHeader>
<bodyText confidence="0.999971666666667">
In a second set of experiments, we explore the role
played by the musical features. While the musical
notation of a song offers several characteristics that
could be potentially useful for our classification ex-
periments (e.g., notes, measures, dynamics, tempo),
in these initial experiments we decided to focus on
two main features, namely the notes and the key.
Notes. A note is a sign used in the musical nota-
tion associated with a song, to represent the relative
duration and pitch of a sound. In traditional mu-
sic theory, the notes are represented using the first
seven letters of the alphabet (C-D-E-F-G-A-B), al-
</bodyText>
<page confidence="0.996674">
594
</page>
<bodyText confidence="0.999952260869565">
though other notations can also be used. Notes can
be modified by “accidentals” – a sharp or a flat sym-
bol that can change the note by half a tone. A written
note can also have associated a value, which refers
to its duration (e.g., whole note; eighth note). Simi-
lar to the unigram features, for each note, we record
a feature indicating the frequency of that note inside
a line.
Key. The key of a song refers to the harmony or
“pitch class” used for a song, e.g., C major, or F#.
Sometime the term minor or major can be appended
to a key, to indicate a minor or a major scale. For
instance, a song in “the key of C minor” means that
the song is harmonically centered on the note C, and
it makes use of the minor scale whose first note is C.
The key system is the structural foundation of most
of the Western music. We use a simple feature that
reflects the key of the song. Note that with a few ex-
ceptions, when more than one key is used in a song,
all the lines in a song will have the same key.
Table 4 shows the results obtained in these clas-
sification experiments, when using only the notes as
features, only the key, or both.
</bodyText>
<table confidence="0.999398888888889">
Emotion Notes Key All
Musical
ANGER 0.2453 0.4083 0.4405
DISGUST 0.1485 0.2922 0.3199
FEAR 0.1361 0.2203 0.2450
JOY 0.1533 0.3835 0.4001
SADNESS 0.1738 0.3502 0.3762
SURPRISE 0.0983 0.2241 0.2412
AVERAGE 0.1592 0.3131 0.3371
</table>
<tableCaption confidence="0.9790875">
Table 4: Evaluations using musical features: notes, key,
and all the musical features.
</tableCaption>
<subsectionHeader confidence="0.999716">
4.3 Joint Textual and Musical Features.
</subsectionHeader>
<bodyText confidence="0.99966075">
To explore the usefulness of the joint lyrics and mu-
sic representation, we also run a set of experiments
that use all the textual and musical features. Table 5
shows the Pearson correlations obtained when us-
ing all the features. To facilitate the comparison,
the table also includes the results obtained with the
textual-only and musical-only features (reported in
Tables 3 and 4).
</bodyText>
<table confidence="0.999935555555556">
Emotion All All Textual &amp;
Textual Musical Musical
ANGER 0.5658 0.4405 0.6679
DISGUST 0.4322 0.3199 0.5068
FEAR 0.4041 0.2450 0.4384
JOY 0.5769 0.4001 0.6456
SADNESS 0.5418 0.3762 0.6193
SURPRISE 0.3392 0.2412 0.3855
AVERAGE 0.4766 0.3371 0.5439
</table>
<tableCaption confidence="0.9881025">
Table 5: Evaluations using both textual and musical fea-
tures.
</tableCaption>
<sectionHeader confidence="0.998432" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99992293939394">
One clear conclusion can be drawn from these ex-
periments: the textual and musical features are both
useful for the classification of emotions in songs,
and, more importantly, their joint use leads to the
highest classification results. Specifically, the joint
model gives an error rate reduction of 12.9% with
respect to the classifier that uses only textual fea-
tures, and 31.2% with respect to the classifier that
uses only musical features. This supports the idea
that lyrics and music represent orthogonal dimen-
sions for the classification of emotions in songs.
Among the six emotions considered, the largest
improvements are observed for JOY, SADNESS, and
ANGER. This was somehow expected for the first
two emotions, since they appear to be dominant in
the corpus (see Table 2), but comes as a surprise
for ANGER, which is less dominant. Further explo-
rations are needed to determine the reason for this
effect.
Looking at the features considered, textual fea-
tures appear to be the most useful. Nonetheless,
the addition of the musical features brings clear im-
provements, as shown in the last column from the
same table.
Additionally, we made several further analyses of
the results, as described below.
Feature ablation. To determine the role played by
each of the feature groups we consider, we run an
ablation study where we remove one feature group
at a time from the complete set of features and mea-
sure the accuracy of the resulting classifier. Table 6
shows the feature ablation results. Note that feature
ablation can also be done in the reverse direction, by
</bodyText>
<page confidence="0.995753">
595
</page>
<table confidence="0.999897181818182">
Emotion All All features, excluding
Features
Unigrams Semantic Semantic Classes
Classes Notes Key and Notes
ANGER 0.6679 0.4996 0.5525 0.6573 0.6068 0.6542
DISGUST 0.5068 0.3831 0.4246 0.5013 0.4439 0.4814
FEAR 0.4384 0.3130 0.3744 0.4313 0.4150 0.4114
JOY 0.6456 0.5141 0.5636 0.6432 0.5829 0.6274
SADNESS 0.6193 0.4586 0.5291 0.6176 0.5540 0.6029
SURPRISE 0.3855 0.3083 0.3214 0.3824 0.3421 0.3721
AVERAGE 0.5439 0.4127 0.4609 0.5388 0.4908 0.5249
</table>
<tableCaption confidence="0.973676">
Table 6: Ablation studies excluding one feature group at a time.
</tableCaption>
<table confidence="0.999968777777778">
Emotion Baseline Textual Musical Textual and
Musical
ANGER 89.27% 91.14% 89.63% 92.40%
DISGUST 93.85% 94.67% 93.85% 94.77%
FEAR 93.58% 93.87% 93.58% 93.87%
JOY 50.26% 70.92% 61.95% 75.64%
SADNESS 67.40% 75.84% 70.65% 79.42%
SURPRISE 94.83% 94.83% 94.83% 94.83%
AVERAGE 81.53% 86.87% 84.08% 88.49%
</table>
<tableCaption confidence="0.999812">
Table 7: Evaluations using a coarse-grained binary classification.
</tableCaption>
<bodyText confidence="0.998632681818182">
keeping only one group of features at a time; the re-
sults obtained with the individual feature groups are
already reported in Tables 3 and 4.
The ablation studies confirm the findings from our
earlier experiments: while the unigrams and the keys
are the most predictive features, the semantic classes
and the notes are also contributing to the final clas-
sification even if to a lesser extent. To measure the
effect of these groups of somehow weaker features
(semantic classes and notes), we also perform an ab-
lation experiment where we remove both these fea-
ture groups from the feature set. The results are re-
ported in the last column of Table 6.
Coarse-grained classification. As an additional
evaluation, we transform the task into a binary clas-
sification by using a threshold empirically set at 3.
Thus, to generate the coarse binary annotations, if
the score of an emotion is below 3, we record it as
“negative” (i.e., the emotion is absent), whereas if
the score is equal to or above 3, we record it as “pos-
itive” (i.e., the emotion is present).
For the classification, we use Support Vector Ma-
chines (SVM), which are binary classifiers that seek
to find the hyperplane that best separates a set of pos-
itive examples from a set of negative examples, with
maximum margin (Vapnik, 1995). Applications of
SVM classifiers to text categorization led to some of
the best results reported in the literature (Joachims,
1998).
Table 7 shows the results obtained for each of the
six emotions, and for the three major settings that
we considered: textual features only, musical fea-
tures only, and a classifier that jointly uses the tex-
tual and the musical features. As before, the classi-
fication accuracy for each experiment is reported as
the average of the accuracies obtained during a ten-
fold cross-validation on the corpus. The table also
shows a baseline, computed as the average of the
accuracies obtained when using the most frequent
class observed on the training data for each fold.
As seen from the table, on average, the joint use of
textual and musical features is also beneficial for this
binary coarser-grained classification. Perhaps not
surprisingly, the effect of the classifier is stronger for
</bodyText>
<page confidence="0.995803">
596
</page>
<table confidence="0.9999192">
1,000 news headlines 4,976 song lines
Emotion Best result (Strapparava Joint Text
SEMEVAL’07 and Mihalcea, 08) and Music
ANGER 0.3233 0.1978 0.6679
DISGUST 0.1855 0.1354 0.5068
FEAR 0.4492 0.2956 0.4384
JOY 0.2611 0.1381 0.6456
SADNESS 0.4098 0.1601 0.6193
SURPRISE 0.1671 0.1235 0.3855
AVERAGE 0.2993 0.1750 0.5439
</table>
<tableCaption confidence="0.999907">
Table 8: Results obtained in previous work on emotion classification.
</tableCaption>
<bodyText confidence="0.999687911764706">
those emotions that are dominant in the corpus, i.e.,
JOY and SADNESS (see Table 2). The improvement
obtained with the classifiers is much smaller for the
other emotions (or even absent, e.g., for SURPRISE),
which is also explained by their high baseline of over
90%.
Comparison to previous work. There is no pre-
vious research that has considered the joint use of
lyrics and songs representations for emotion classifi-
cation at line level, and thus we cannot draw a direct
comparison with other work on emotion classifica-
tion in songs.
Nonetheless, as a point of reference, we consider
the previous work done on emotion classification of
texts. Table 8 shows the results obtained in previ-
ous work for the recognition of emotions in a corpus
consisting of 1,000 news headlines (Strapparava and
Mihalcea, 2007) annotated for the same six emo-
tions. Specifically, the table shows the best over-
all correlation results obtained by the three emotion
recognition systems in the SEMEVAL task on Affec-
tive Text (Strapparava and Mihalcea, 2007): (Chau-
martin, 2007; Kozareva et al., 2007; Katz et al.,
2007). The table also shows the best results obtained
in follow up work carried out on the same dataset
(Strapparava and Mihalcea, 2008).
Except for one emotion (FEAR), the correlation
figures we obtain are significantly higher than those
reported in previous work. As mentioned before,
however, a direct comparison cannot be made, since
the earlier work used a different, smaller dataset.
Moreover, our corpus of songs is likely to be more
emotionally loaded than the news titles used in pre-
vious work.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999888466666667">
Popular songs express universally understood mean-
ings and embody experiences and feelings shared by
many, usually through a combined effect of both mu-
sic and lyrics. In this paper, we introduced a novel
corpus of music and lyrics, annotated for emotions at
line level, and we used this corpus to explore the au-
tomatic recognition of emotions in songs. Through
experiments carried out on the dataset of 100 songs,
we showed that emotion recognition can be per-
formed using either textual or musical features, and
that the joint use of lyrics and music can improve
significantly over classifiers that use only one di-
mension at a time.
The dataset introduced in this paper is available
by request from the authors of the paper.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999674">
The authors are grateful to Rajitha Schellenberg
for her help with collecting the emotion annota-
tions. Carlo Strapparava was partially supported by
a Google Research Award. Rada Mihalcea’s work
was in part supported by the National Science Foun-
dation award #0917170. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of the National Science Foun-
dation.
</bodyText>
<sectionHeader confidence="0.998697" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.975641666666667">
C. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: Machine learning for text-based emotion predic-
tion. In Proceedings of the Conference on Empirical
</reference>
<page confidence="0.989953">
597
</page>
<reference confidence="0.995448583333334">
Methods in Natural Language Processing, pages 347–
354, Vancouver, Canada.
Z. Cataltepe, Y. Yaslan, and A. Sonmez. 2007. Mu-
sic genre classification using MIDI and audio features.
Journal on Advances in Signal Processing.
F.R. Chaumartin. 2007. Upar7: A knowledge-based sys-
tem for headline sentiment tagging. In Proceedings of
the Fourth International Workshop on Semantic Evalu-
ations (SemEval-2007), Prague, Czech Republic, June.
M. Das, D. Howard, and S. Smith. 2000. The kinematic
analysis of motion curves through MIDI data analysis.
Organised Sound, 5(1):137–145.
P. Ekman. 1993. Facial expression of emotion. Ameri-
can Psychologist, 48:384–392.
X. Hu, J. S. Downie, and A. F. Ehmann. 2009. Lyric
text mining in music mood classification. In Proceed-
ings of the International Society for Music Information
Retrieval Conference, Kobe, Japan.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137–142, Chemnitz, Germany.
P. Juslin and P. Laukka. 2003. Communication of emo-
tion in vocal expression and music performance: Dif-
ferent channels, same code? Psychological Bulletin,
129:770–814.
P. N. Juslin and J. A. Sloboda, editors. 2001. Music and
Emotion: Theory and Research. Oxford University
Press.
C. Karageorghis and D. Priest. 2008. Music in sport and
exercise : An update on research and application. The
Sport Journal, 11(3).
P. Katz, M. Singleton, and R. Wicentowski. 2007.
Swat-mp:the semeval-2007 systems for task 5 and
task 14. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic, June.
Y. Kim, E. Schmidt, R. Migneco, B. Morton, P. Richard-
son, J. Scott, J. Speck, and D. Turnbull. 2010. Mu-
sic emotion recognition: A state of the art review. In
International Symposium on Music Information Re-
trieval.
Z. Kozareva, B. Navarro, S. Vazquez, and A. Mon-
toyo. 2007. Ua-zbsa: A headline emotion classifi-
cation through web information. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, June.
C. Laurier, J. Grivolla, and P. Herrera. 2008. Multimodal
music mood classification using audio and lyrics. In
Proceedings of the International Conference on Ma-
chine Learning and Applications, Barcelona, Spain.
J. Mahedero, A. Martinez, and P. Cano. 2005. Natu-
ral language processing of lyrics. In Proceedings of
MM’05, Singapore, November.
B. Nettl. 2000. An ethnomusicologist contemplates
universals in musical sound and musical culture. In
N. Wallin, B. Merker, and S. Brown, editors, The
origins of music, pages 463—472. MIT Press, Cam-
bridge, MA.
T. O’Hara. 2011. Inferring the meaning of chord se-
quences via lyrics. In Proceedings of 2nd Workshop
on Music Recommendation and Discovery (WOMRAD
2011), Chicago, IL, October.
N. Orio. 2006. Music retrieval: A tutorial and re-
view. Foundations and Trends in Information Re-
trieval, 1(1):1–90, November.
A. Ortony, G. L. Clore, and M. A. Foss. 1987. The ref-
erential structure of the affective lexicon. Cognitive
Science, (11).
J. Pennebaker and M. Francis. 1999. Linguistic inquiry
and word count: LIWC. Erlbaum Publishers.
J. Pennebaker and L. King. 1999. Linguistic styles: Lan-
guage use as an individual difference. Journal of Per-
sonality and Social Psychology, (77).
F. Rauscher, G. Shaw, and K. Ky. 1993. Music and spa-
tial task performance. Nature, 365.
D. Rizo, P. Ponce de Leon, C. Perez-Sancho, A. Pertusa,
and J. Inesta. 2006. A pattern recognition approach
for melody track selection in MIDI files. In Proceed-
ings of 7th International Symposyum on Music Infor-
mation Retrieval (ISMIR-06), pages 61–66, Victoria,
Canada, October.
K. Scherer. 1995. Expression of emotion in voice and
music. Journal of Voice, 9:235–248.
K. Scherer. 2004. Which emotions can be induced by
music? what are the underlying mechanisms? and
how can we measure them ? Journal of New Music
Research, 33:239–251.
R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast – but is it good? evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, Honolulu, Hawaii.
C. Strapparava and R. Mihalcea. 2007. Semeval-2007
task 14: Affective text. In Proceedings of the 4th
International Workshop on the Semantic Evaluations
(SemEval 2007), Prague, Czech Republic.
C. Strapparava and R. Mihalcea. 2008. Learning to iden-
tify emotions in text. In Proceedings of the ACM Con-
ference on Applied Computing ACM-SAC 2008, Fort-
aleza, Brazile.
C. Strapparava and A. Valitutti. 2004. Wordnet-affect:
an affective extension of wordnet. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation, Lisbon.
J. Sundberg. 1982. Speech, song, and emotions. In
M. Clynes, editor, Music, Mind and Brain: The Neu-
ropsychology ofMusic. Plenum Press, New York.
</reference>
<page confidence="0.979393">
598
</page>
<reference confidence="0.999877777777778">
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.
S. Velusamy, B. Thoshkahna, and K. Ramakrishnan.
2007. Novel melody line identification algorithm for
polyphonic MIDI music. In Proceedings of 13th In-
ternational Multimedia Modeling Conference (MMM
2007), Singapore, January.
Y. Wang, M. Kan, T. Nwe, A. Shenoy, and J. Yin. 2004.
LyricAlly: Automatic synchronization of acoustic mu-
sical signals and textual lyrics. In Proceedings of
MM’04, New York, October.
Y. Xia, L. Wang, K.F. Wong, and M. Xu. 2008. Lyric-
based song sentiment classification with sentiment
vector space model. In Proceedings of the Association
for Computational Linguistics, Columbus, Ohio.
D. Yang and W. Lee. 2004. Disambiguating music
emotion using software agents. In Proceedings of the
International Conference on Music Information Re-
trieval, Barcelona, Spain.
D. Yang and W. Lee. 2009. Music emotion identification
from lyrics. In Proceedings of 11th IEEE Symposium
on Multimedia.
Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, I.-B. Liao, Y.-C. Ho,
and H. Chen. 2008. Toward multi-modal music emo-
tion classification. In Proceedings of the 9th Pacific
Rim Conference on Multimedia: Advances in Multi-
media Information Processing.
</reference>
<page confidence="0.998743">
599
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.807714">
<title confidence="0.999039">Lyrics, Music, and Emotions</title>
<author confidence="0.999778">Rada Mihalcea Carlo Strapparava</author>
<affiliation confidence="0.994061">University of North Texas FBK-irst</affiliation>
<email confidence="0.880003">rada@cs.unt.edustrappa@fbk.eu</email>
<abstract confidence="0.992945928571428">In this paper, we explore the classification of emotions in songs, using the music and the lyrics representation of the songs. We introduce a novel corpus of music and lyrics, consisting of 100 songs annotated for emotions. We show that textual and musical features can both be successfully used for emotion recognition in songs. Moreover, through comparative experiments, we show that the joint use of lyrics and music brings significant improvements over each of the individual textual and musical classifiers, with error rate reductions of up to 31%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Alm</author>
<author>D Roth</author>
<author>R Sproat</author>
</authors>
<title>Emotions from text: Machine learning for text-based emotion prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="10102" citStr="Alm et al., 2005" startWordPosition="1644" endWordPosition="1647">ey of the song; and the duration of the note. Table 1 shows statistics on the corpus. An example from the corpus, consisting of the first two lines from the Beatles’ song A hard day’s night, is illustrated in Figure 3. SONGS 100 SONGS IN “MAJOR” KEY 59 SONGS IN “MINOR” KEY 41 LINES 4,976 ALIGNED SYLLABLES / NOTES 34,045 Table 1: Some statistics of the corpus Emotion Annotations with Mechanical Turk. In order to explore the classification of emotions in songs, we needed a gold standard consisting of manual emotion annotations of the songs. Following previous work on emotion annotation of text (Alm et al., 2005; Strapparava and Mihalcea, 2007), to annotate the emotions in songs we use the six basic emotions proposed by (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, SURPRISE. To collect the annotations, we use the Amazon Mechanical Turk service, which was previously found to produce reliable annotations with a quality comparable to those generated by experts (Snow et al., 2008). The annotations are collected at line level, with a separate annotation for each of the six emotions. We collect numerical annotations using a scale between 0 and 10, with 0 corresponding to the absence of an emotion, and</context>
</contexts>
<marker>Alm, Roth, Sproat, 2005</marker>
<rawString>C. Alm, D. Roth, and R. Sproat. 2005. Emotions from text: Machine learning for text-based emotion prediction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 347– 354, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Cataltepe</author>
<author>Y Yaslan</author>
<author>A Sonmez</author>
</authors>
<title>Music genre classification using MIDI and audio features.</title>
<date>2007</date>
<booktitle>Journal on Advances in Signal Processing.</booktitle>
<contexts>
<context position="5177" citStr="Cataltepe et al., 2007" startWordPosition="824" endWordPosition="828">val, focusing primarily on music in digital format such as MIDI. For instance, the task of music retrieval and music recommendation has received a lot of attention from both the arts and the computer science communities (see for instance (Orio, 2006) for an introduction to this task). There are also several works on MIDI analysis. Among them, particularly relevant to our research is the work by (Das et al., 2000), who described an analysis of predominant up-down motion types within music, through extraction of the kinematic variables of music velocity and acceleration from MIDI data streams. (Cataltepe et al., 2007) addressed music genre classification (e.g., classic, jazz, pop) using MIDI and audio features, while (Wang et al., 2004) automatically aligned acoustic musical signals with their corresponding textual lyrics. MIDI files are typically organized into one or more parallel “tracks” for independent recording and editing. A reliable system to identify the MIDI track containing the melody1 is very relevant for music information retrieval, and 1A melody can be defined as a “cantabile” sequence of notes, usually the sequence that a listener can remember after hearing a song. there are several approach</context>
</contexts>
<marker>Cataltepe, Yaslan, Sonmez, 2007</marker>
<rawString>Z. Cataltepe, Y. Yaslan, and A. Sonmez. 2007. Music genre classification using MIDI and audio features. Journal on Advances in Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F R Chaumartin</author>
</authors>
<title>Upar7: A knowledge-based system for headline sentiment tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28834" citStr="Chaumartin, 2007" startWordPosition="4640" endWordPosition="4642">el, and thus we cannot draw a direct comparison with other work on emotion classification in songs. Nonetheless, as a point of reference, we consider the previous work done on emotion classification of texts. Table 8 shows the results obtained in previous work for the recognition of emotions in a corpus consisting of 1,000 news headlines (Strapparava and Mihalcea, 2007) annotated for the same six emotions. Specifically, the table shows the best overall correlation results obtained by the three emotion recognition systems in the SEMEVAL task on Affective Text (Strapparava and Mihalcea, 2007): (Chaumartin, 2007; Kozareva et al., 2007; Katz et al., 2007). The table also shows the best results obtained in follow up work carried out on the same dataset (Strapparava and Mihalcea, 2008). Except for one emotion (FEAR), the correlation figures we obtain are significantly higher than those reported in previous work. As mentioned before, however, a direct comparison cannot be made, since the earlier work used a different, smaller dataset. Moreover, our corpus of songs is likely to be more emotionally loaded than the news titles used in previous work. 6 Conclusions Popular songs express universally understood</context>
</contexts>
<marker>Chaumartin, 2007</marker>
<rawString>F.R. Chaumartin. 2007. Upar7: A knowledge-based system for headline sentiment tagging. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Das</author>
<author>D Howard</author>
<author>S Smith</author>
</authors>
<title>The kinematic analysis of motion curves through MIDI data analysis.</title>
<date>2000</date>
<journal>Organised Sound,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="4970" citStr="Das et al., 2000" startWordPosition="792" endWordPosition="795"> level, as we do, and none of them considered the six Ekman emotions used in our work. Other related work consists of the development of tools for music accessing, filtering, classification, and retrieval, focusing primarily on music in digital format such as MIDI. For instance, the task of music retrieval and music recommendation has received a lot of attention from both the arts and the computer science communities (see for instance (Orio, 2006) for an introduction to this task). There are also several works on MIDI analysis. Among them, particularly relevant to our research is the work by (Das et al., 2000), who described an analysis of predominant up-down motion types within music, through extraction of the kinematic variables of music velocity and acceleration from MIDI data streams. (Cataltepe et al., 2007) addressed music genre classification (e.g., classic, jazz, pop) using MIDI and audio features, while (Wang et al., 2004) automatically aligned acoustic musical signals with their corresponding textual lyrics. MIDI files are typically organized into one or more parallel “tracks” for independent recording and editing. A reliable system to identify the MIDI track containing the melody1 is ver</context>
</contexts>
<marker>Das, Howard, Smith, 2000</marker>
<rawString>M. Das, D. Howard, and S. Smith. 2000. The kinematic analysis of motion curves through MIDI data analysis. Organised Sound, 5(1):137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
</authors>
<title>Facial expression of emotion.</title>
<date>1993</date>
<journal>American Psychologist,</journal>
<pages>48--384</pages>
<contexts>
<context position="10226" citStr="Ekman, 1993" startWordPosition="1667" endWordPosition="1668">the first two lines from the Beatles’ song A hard day’s night, is illustrated in Figure 3. SONGS 100 SONGS IN “MAJOR” KEY 59 SONGS IN “MINOR” KEY 41 LINES 4,976 ALIGNED SYLLABLES / NOTES 34,045 Table 1: Some statistics of the corpus Emotion Annotations with Mechanical Turk. In order to explore the classification of emotions in songs, we needed a gold standard consisting of manual emotion annotations of the songs. Following previous work on emotion annotation of text (Alm et al., 2005; Strapparava and Mihalcea, 2007), to annotate the emotions in songs we use the six basic emotions proposed by (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, SURPRISE. To collect the annotations, we use the Amazon Mechanical Turk service, which was previously found to produce reliable annotations with a quality comparable to those generated by experts (Snow et al., 2008). The annotations are collected at line level, with a separate annotation for each of the six emotions. We collect numerical annotations using a scale between 0 and 10, with 0 corresponding to the absence of an emotion, and 10 corresponding to the highest intensity. Each HIT (i.e., annotation session) contains an entire song, with a number of li</context>
</contexts>
<marker>Ekman, 1993</marker>
<rawString>P. Ekman. 1993. Facial expression of emotion. American Psychologist, 48:384–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Hu</author>
<author>J S Downie</author>
<author>A F Ehmann</author>
</authors>
<title>Lyric text mining in music mood classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Society for Music Information Retrieval Conference,</booktitle>
<location>Kobe, Japan.</location>
<contexts>
<context position="4154" citStr="Hu et al., 2009" startWordPosition="657" endWordPosition="660">s in speech and music. The work most closely related to ours is the combination of audio and lyrics for emotion classification in songs, as thoroughly surveyed in (Kim et al., 2010). Although several methods have been proposed, including a combination of textual features and beats per minute and MPEG descriptors (Yang and Lee, 2004); individual audio and text classifiers for arousal and valence, followed by a combination through meta-learning (Yang et al., 2008); and the use of crowdsourcing labeling from Last.fm to collect large datasets of songs annotated for emotions (Laurier et al., 2008; Hu et al., 2009), all this previous work was done at song level, and most of it focused on valence-arousal classifications. None of the previous methods considered the fine-grained classification of emotions at line level, as we do, and none of them considered the six Ekman emotions used in our work. Other related work consists of the development of tools for music accessing, filtering, classification, and retrieval, focusing primarily on music in digital format such as MIDI. For instance, the task of music retrieval and music recommendation has received a lot of attention from both the arts and the computer </context>
</contexts>
<marker>Hu, Downie, Ehmann, 2009</marker>
<rawString>X. Hu, J. S. Downie, and A. F. Ehmann. 2009. Lyric text mining in music mood classification. In Proceedings of the International Society for Music Information Retrieval Conference, Kobe, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with Support Vector Machines: learning with mny relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<location>Chemnitz, Germany.</location>
<contexts>
<context position="26608" citStr="Joachims, 1998" startWordPosition="4282" endWordPosition="4283"> to generate the coarse binary annotations, if the score of an emotion is below 3, we record it as “negative” (i.e., the emotion is absent), whereas if the score is equal to or above 3, we record it as “positive” (i.e., the emotion is present). For the classification, we use Support Vector Machines (SVM), which are binary classifiers that seek to find the hyperplane that best separates a set of positive examples from a set of negative examples, with maximum margin (Vapnik, 1995). Applications of SVM classifiers to text categorization led to some of the best results reported in the literature (Joachims, 1998). Table 7 shows the results obtained for each of the six emotions, and for the three major settings that we considered: textual features only, musical features only, and a classifier that jointly uses the textual and the musical features. As before, the classification accuracy for each experiment is reported as the average of the accuracies obtained during a tenfold cross-validation on the corpus. The table also shows a baseline, computed as the average of the accuracies obtained when using the most frequent class observed on the training data for each fold. As seen from the table, on average,</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with Support Vector Machines: learning with mny relevant features. In Proceedings of the European Conference on Machine Learning, pages 137–142, Chemnitz, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Juslin</author>
<author>P Laukka</author>
</authors>
<title>Communication of emotion in vocal expression and music performance: Different channels, same code?</title>
<date>2003</date>
<journal>Psychological Bulletin,</journal>
<pages>129--770</pages>
<marker>Juslin, Laukka, 2003</marker>
<rawString>P. Juslin and P. Laukka. 2003. Communication of emotion in vocal expression and music performance: Different channels, same code? Psychological Bulletin, 129:770–814.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>Music and Emotion: Theory and Research.</booktitle>
<editor>P. N. Juslin and J. A. Sloboda, editors.</editor>
<publisher>Oxford University Press.</publisher>
<marker>2001</marker>
<rawString>P. N. Juslin and J. A. Sloboda, editors. 2001. Music and Emotion: Theory and Research. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Karageorghis</author>
<author>D Priest</author>
</authors>
<title>Music in sport and exercise : An update on research and application.</title>
<date>2008</date>
<journal>The Sport Journal,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="2781" citStr="Karageorghis and Priest, 2008" startWordPosition="444" endWordPosition="447">l and musical features. Through comparative experiments, we show that emotion recognition can be performed using either textual or musical features, and that the joint use of lyrics and music can improve significantly over classifiers that use only one dimension at a time. We believe our results demonstrate the promise of using joint music-lyric models for song processing. 2 Related Work The literature on music analysis is noticeably large, and there are several studies concerning the music’s power over emotions (Juslin and Sloboda, 2001), thinking (Rauscher et al., 1993), or physical effort (Karageorghis and Priest, 2008). In particular, there has been significant research in music and psychology focusing on the idea of a parallel between affective cues in music and speech (Sundberg, 1982; Scherer, 1995). For instance, (Scherer, 2004) investigated the types of emotions that can be induced by music, their mechanisms, and how they can be empirically measured. (Juslin and 590 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 590–599, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics La</context>
</contexts>
<marker>Karageorghis, Priest, 2008</marker>
<rawString>C. Karageorghis and D. Priest. 2008. Music in sport and exercise : An update on research and application. The Sport Journal, 11(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Katz</author>
<author>M Singleton</author>
<author>R Wicentowski</author>
</authors>
<title>Swat-mp:the semeval-2007 systems for task 5 and task 14.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28877" citStr="Katz et al., 2007" startWordPosition="4647" endWordPosition="4650">arison with other work on emotion classification in songs. Nonetheless, as a point of reference, we consider the previous work done on emotion classification of texts. Table 8 shows the results obtained in previous work for the recognition of emotions in a corpus consisting of 1,000 news headlines (Strapparava and Mihalcea, 2007) annotated for the same six emotions. Specifically, the table shows the best overall correlation results obtained by the three emotion recognition systems in the SEMEVAL task on Affective Text (Strapparava and Mihalcea, 2007): (Chaumartin, 2007; Kozareva et al., 2007; Katz et al., 2007). The table also shows the best results obtained in follow up work carried out on the same dataset (Strapparava and Mihalcea, 2008). Except for one emotion (FEAR), the correlation figures we obtain are significantly higher than those reported in previous work. As mentioned before, however, a direct comparison cannot be made, since the earlier work used a different, smaller dataset. Moreover, our corpus of songs is likely to be more emotionally loaded than the news titles used in previous work. 6 Conclusions Popular songs express universally understood meanings and embody experiences and feelin</context>
</contexts>
<marker>Katz, Singleton, Wicentowski, 2007</marker>
<rawString>P. Katz, M. Singleton, and R. Wicentowski. 2007. Swat-mp:the semeval-2007 systems for task 5 and task 14. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kim</author>
<author>E Schmidt</author>
<author>R Migneco</author>
<author>B Morton</author>
<author>P Richardson</author>
<author>J Scott</author>
<author>J Speck</author>
<author>D Turnbull</author>
</authors>
<title>Music emotion recognition: A state of the art review.</title>
<date>2010</date>
<booktitle>In International Symposium on Music Information Retrieval.</booktitle>
<contexts>
<context position="3719" citStr="Kim et al., 2010" startWordPosition="588" endWordPosition="591">empirically measured. (Juslin and 590 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 590–599, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Laukka, 2003) conducted a comprehensive review of vocal expressions and music performance, finding substantial overlap in the cues used to convey basic emotions in speech and music. The work most closely related to ours is the combination of audio and lyrics for emotion classification in songs, as thoroughly surveyed in (Kim et al., 2010). Although several methods have been proposed, including a combination of textual features and beats per minute and MPEG descriptors (Yang and Lee, 2004); individual audio and text classifiers for arousal and valence, followed by a combination through meta-learning (Yang et al., 2008); and the use of crowdsourcing labeling from Last.fm to collect large datasets of songs annotated for emotions (Laurier et al., 2008; Hu et al., 2009), all this previous work was done at song level, and most of it focused on valence-arousal classifications. None of the previous methods considered the fine-grained </context>
</contexts>
<marker>Kim, Schmidt, Migneco, Morton, Richardson, Scott, Speck, Turnbull, 2010</marker>
<rawString>Y. Kim, E. Schmidt, R. Migneco, B. Morton, P. Richardson, J. Scott, J. Speck, and D. Turnbull. 2010. Music emotion recognition: A state of the art review. In International Symposium on Music Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>B Navarro</author>
<author>S Vazquez</author>
<author>A Montoyo</author>
</authors>
<title>Ua-zbsa: A headline emotion classification through web information.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="28857" citStr="Kozareva et al., 2007" startWordPosition="4643" endWordPosition="4646">nnot draw a direct comparison with other work on emotion classification in songs. Nonetheless, as a point of reference, we consider the previous work done on emotion classification of texts. Table 8 shows the results obtained in previous work for the recognition of emotions in a corpus consisting of 1,000 news headlines (Strapparava and Mihalcea, 2007) annotated for the same six emotions. Specifically, the table shows the best overall correlation results obtained by the three emotion recognition systems in the SEMEVAL task on Affective Text (Strapparava and Mihalcea, 2007): (Chaumartin, 2007; Kozareva et al., 2007; Katz et al., 2007). The table also shows the best results obtained in follow up work carried out on the same dataset (Strapparava and Mihalcea, 2008). Except for one emotion (FEAR), the correlation figures we obtain are significantly higher than those reported in previous work. As mentioned before, however, a direct comparison cannot be made, since the earlier work used a different, smaller dataset. Moreover, our corpus of songs is likely to be more emotionally loaded than the news titles used in previous work. 6 Conclusions Popular songs express universally understood meanings and embody ex</context>
</contexts>
<marker>Kozareva, Navarro, Vazquez, Montoyo, 2007</marker>
<rawString>Z. Kozareva, B. Navarro, S. Vazquez, and A. Montoyo. 2007. Ua-zbsa: A headline emotion classification through web information. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Laurier</author>
<author>J Grivolla</author>
<author>P Herrera</author>
</authors>
<title>Multimodal music mood classification using audio and lyrics.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Machine Learning and Applications,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="4136" citStr="Laurier et al., 2008" startWordPosition="653" endWordPosition="656">o convey basic emotions in speech and music. The work most closely related to ours is the combination of audio and lyrics for emotion classification in songs, as thoroughly surveyed in (Kim et al., 2010). Although several methods have been proposed, including a combination of textual features and beats per minute and MPEG descriptors (Yang and Lee, 2004); individual audio and text classifiers for arousal and valence, followed by a combination through meta-learning (Yang et al., 2008); and the use of crowdsourcing labeling from Last.fm to collect large datasets of songs annotated for emotions (Laurier et al., 2008; Hu et al., 2009), all this previous work was done at song level, and most of it focused on valence-arousal classifications. None of the previous methods considered the fine-grained classification of emotions at line level, as we do, and none of them considered the six Ekman emotions used in our work. Other related work consists of the development of tools for music accessing, filtering, classification, and retrieval, focusing primarily on music in digital format such as MIDI. For instance, the task of music retrieval and music recommendation has received a lot of attention from both the arts</context>
</contexts>
<marker>Laurier, Grivolla, Herrera, 2008</marker>
<rawString>C. Laurier, J. Grivolla, and P. Herrera. 2008. Multimodal music mood classification using audio and lyrics. In Proceedings of the International Conference on Machine Learning and Applications, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mahedero</author>
<author>A Martinez</author>
<author>P Cano</author>
</authors>
<title>Natural language processing of lyrics.</title>
<date>2005</date>
<booktitle>In Proceedings of MM’05, Singapore,</booktitle>
<contexts>
<context position="6453" citStr="Mahedero et al., 2005" startWordPosition="1025" endWordPosition="1028"> et al., 2006; Velusamy et al., 2007). Another related study concerned with the interaction of lyrics and music using an annotated corpus is found in (O’Hara, 2011), who presented preliminary research that checks whether the expressive meaning of a particular harmony or harmonic sequence could be deduced from the lyrics it accompanies, by using harmonically annotated chords from the Usenet group alt.guitar.tab. Finally, in natural language processing, there are a few studies that mainly exploited the lyrics component of the songs, while generally ignoring the musical component. For instance, (Mahedero et al., 2005) dealt with language identification, structure extraction, and thematic categorization for lyrics. (Xia et al., 2008) addressed the task of sentiment classification in lyrics, recognizing positive and negative moods in a large dataset of Chinese pop songs, while (Yang and Lee, 2009) approached the problem of emotion identification in lyrics, classifying songs from allmusic.com using a set of 23 emotions. 3 A Corpus of Music and Lyrics Annotated for Emotions To enable our exploration of emotions in songs, we compiled a corpus of 100 popular songs (e.g., Dancing Queen by ABBA, Hotel California b</context>
</contexts>
<marker>Mahedero, Martinez, Cano, 2005</marker>
<rawString>J. Mahedero, A. Martinez, and P. Cano. 2005. Natural language processing of lyrics. In Proceedings of MM’05, Singapore, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Nettl</author>
</authors>
<title>An ethnomusicologist contemplates universals in musical sound and musical culture.</title>
<date>2000</date>
<booktitle>The origins of music,</booktitle>
<pages>463--472</pages>
<editor>In N. Wallin, B. Merker, and S. Brown, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="916" citStr="Nettl, 2000" startWordPosition="142" endWordPosition="143">yrics, consisting of 100 songs annotated for emotions. We show that textual and musical features can both be successfully used for emotion recognition in songs. Moreover, through comparative experiments, we show that the joint use of lyrics and music brings significant improvements over each of the individual textual and musical classifiers, with error rate reductions of up to 31%. 1 Introduction Language and music are peculiar characteristics of human beings. The capability of producing and enjoying language and music appears in every human society, regardless of the richness of its culture (Nettl, 2000). Importantly, language and music complement each other in many different ways. For instance, looking at music and language in terms of features, we can observe that music organizes pitch and rhythm in ways that language does not, and it lacks the specificity of language in terms of semantic meaning. On the other hand, language is built from categories that are absent in music (e.g., nouns and verbs), whereas music seems to have a deeper power over our emotions than does ordinary speech. Composers, musicians, and researchers in poetry and literature alike have been long fascinated by the combi</context>
</contexts>
<marker>Nettl, 2000</marker>
<rawString>B. Nettl. 2000. An ethnomusicologist contemplates universals in musical sound and musical culture. In N. Wallin, B. Merker, and S. Brown, editors, The origins of music, pages 463—472. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T O’Hara</author>
</authors>
<title>Inferring the meaning of chord sequences via lyrics. In</title>
<date>2011</date>
<booktitle>Proceedings of 2nd Workshop on Music Recommendation and Discovery (WOMRAD 2011),</booktitle>
<location>Chicago, IL,</location>
<marker>O’Hara, 2011</marker>
<rawString>T. O’Hara. 2011. Inferring the meaning of chord sequences via lyrics. In Proceedings of 2nd Workshop on Music Recommendation and Discovery (WOMRAD 2011), Chicago, IL, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Orio</author>
</authors>
<title>Music retrieval: A tutorial and review.</title>
<date>2006</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="4804" citStr="Orio, 2006" startWordPosition="763" endWordPosition="764">ng level, and most of it focused on valence-arousal classifications. None of the previous methods considered the fine-grained classification of emotions at line level, as we do, and none of them considered the six Ekman emotions used in our work. Other related work consists of the development of tools for music accessing, filtering, classification, and retrieval, focusing primarily on music in digital format such as MIDI. For instance, the task of music retrieval and music recommendation has received a lot of attention from both the arts and the computer science communities (see for instance (Orio, 2006) for an introduction to this task). There are also several works on MIDI analysis. Among them, particularly relevant to our research is the work by (Das et al., 2000), who described an analysis of predominant up-down motion types within music, through extraction of the kinematic variables of music velocity and acceleration from MIDI data streams. (Cataltepe et al., 2007) addressed music genre classification (e.g., classic, jazz, pop) using MIDI and audio features, while (Wang et al., 2004) automatically aligned acoustic musical signals with their corresponding textual lyrics. MIDI files are ty</context>
</contexts>
<marker>Orio, 2006</marker>
<rawString>N. Orio. 2006. Music retrieval: A tutorial and review. Foundations and Trends in Information Retrieval, 1(1):1–90, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ortony</author>
<author>G L Clore</author>
<author>M A Foss</author>
</authors>
<title>The referential structure of the affective lexicon.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<volume>11</volume>
<contexts>
<context position="19217" citStr="Ortony et al., 1987" startWordPosition="3051" endWordPosition="3054">ount (LIWC) and WordNet Affect (WA) to derive coarse textual features. LIWC was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999; Pennebaker and King, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., emotion, cognition). WA (Strapparava and Valitutti, 2004) is a resource that was created starting with WordNet, by annotating synsets with several emotions. It uses several resources for affective information, including the emotion classification of Ortony (Ortony et al., 1987). From WA, we extract the words corresponding to the six basic emotions used in our experiments. For each semantic class, we infer a feature indicating the number of words in a line belonging to that class. Table 3 shows the Pearson correlations obtained for each of the six emotions, when using only unigrams, only semantic classes, or both. Emotion Unigrams Semantic All Classes Textual ANGER 0.5525 0.3044 0.5658 DISGUST 0.4246 0.2394 0.4322 FEAR 0.3744 0.2443 0.4041 JOY 0.5636 0.3659 0.5769 SADNESS 0.5291 0.3006 0.5418 SURPRISE 0.3214 0.2153 0.3392 AVERAGE 0.4609 0.2783 0.4766 Table 3: Evaluat</context>
</contexts>
<marker>Ortony, Clore, Foss, 1987</marker>
<rawString>A. Ortony, G. L. Clore, and M. A. Foss. 1987. The referential structure of the affective lexicon. Cognitive Science, (11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pennebaker</author>
<author>M Francis</author>
</authors>
<title>Linguistic inquiry and word count: LIWC.</title>
<date>1999</date>
<publisher>Erlbaum Publishers.</publisher>
<contexts>
<context position="18760" citStr="Pennebaker and Francis, 1999" startWordPosition="2982" endWordPosition="2985">emantic classes. Specifically, we use the Lin2We use the Weka machine learning toolkit. 3There is no clear way to determine a baseline for these experiments. A simple baseline that we calculated, which assumed by default an emotional score equal to the average of the scores on the training data, and measured the correlation between these default scores and the gold standard, consistently led to correlations close to 0 (0.0081-0.0221). guistic Inquiry and Word Count (LIWC) and WordNet Affect (WA) to derive coarse textual features. LIWC was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999; Pennebaker and King, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., emotion, cognition). WA (Strapparava and Valitutti, 2004) is a resource that was created starting with WordNet, by annotating synsets with several emotions. It uses several resources for affective information, including the emotion classification of Ortony (Ortony et al., 1987). From WA, we extract the words corresponding to the six basic emotions used in our experiments. For each semantic class, we infer a feature in</context>
</contexts>
<marker>Pennebaker, Francis, 1999</marker>
<rawString>J. Pennebaker and M. Francis. 1999. Linguistic inquiry and word count: LIWC. Erlbaum Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pennebaker</author>
<author>L King</author>
</authors>
<title>Linguistic styles: Language use as an individual difference.</title>
<date>1999</date>
<journal>Journal of Personality and Social Psychology,</journal>
<volume>77</volume>
<contexts>
<context position="18788" citStr="Pennebaker and King, 1999" startWordPosition="2986" endWordPosition="2989"> we use the Lin2We use the Weka machine learning toolkit. 3There is no clear way to determine a baseline for these experiments. A simple baseline that we calculated, which assumed by default an emotional score equal to the average of the scores on the training data, and measured the correlation between these default scores and the gold standard, consistently led to correlations close to 0 (0.0081-0.0221). guistic Inquiry and Word Count (LIWC) and WordNet Affect (WA) to derive coarse textual features. LIWC was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999; Pennebaker and King, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., emotion, cognition). WA (Strapparava and Valitutti, 2004) is a resource that was created starting with WordNet, by annotating synsets with several emotions. It uses several resources for affective information, including the emotion classification of Ortony (Ortony et al., 1987). From WA, we extract the words corresponding to the six basic emotions used in our experiments. For each semantic class, we infer a feature indicating the number of words</context>
</contexts>
<marker>Pennebaker, King, 1999</marker>
<rawString>J. Pennebaker and L. King. 1999. Linguistic styles: Language use as an individual difference. Journal of Personality and Social Psychology, (77).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rauscher</author>
<author>G Shaw</author>
<author>K Ky</author>
</authors>
<title>Music and spatial task performance.</title>
<date>1993</date>
<journal>Nature,</journal>
<volume>365</volume>
<contexts>
<context position="2729" citStr="Rauscher et al., 1993" startWordPosition="437" endWordPosition="440">ic recognition of emotions using both textual and musical features. Through comparative experiments, we show that emotion recognition can be performed using either textual or musical features, and that the joint use of lyrics and music can improve significantly over classifiers that use only one dimension at a time. We believe our results demonstrate the promise of using joint music-lyric models for song processing. 2 Related Work The literature on music analysis is noticeably large, and there are several studies concerning the music’s power over emotions (Juslin and Sloboda, 2001), thinking (Rauscher et al., 1993), or physical effort (Karageorghis and Priest, 2008). In particular, there has been significant research in music and psychology focusing on the idea of a parallel between affective cues in music and speech (Sundberg, 1982; Scherer, 1995). For instance, (Scherer, 2004) investigated the types of emotions that can be induced by music, their mechanisms, and how they can be empirically measured. (Juslin and 590 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 590–599, Jeju Island, Korea, 12–14 July 2012.</context>
</contexts>
<marker>Rauscher, Shaw, Ky, 1993</marker>
<rawString>F. Rauscher, G. Shaw, and K. Ky. 1993. Music and spatial task performance. Nature, 365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rizo</author>
<author>P Ponce de Leon</author>
<author>C Perez-Sancho</author>
<author>A Pertusa</author>
<author>J Inesta</author>
</authors>
<title>A pattern recognition approach for melody track selection in MIDI files.</title>
<date>2006</date>
<booktitle>In Proceedings of 7th International Symposyum on Music Information Retrieval (ISMIR-06),</booktitle>
<pages>61--66</pages>
<location>Victoria, Canada,</location>
<marker>Rizo, de Leon, Perez-Sancho, Pertusa, Inesta, 2006</marker>
<rawString>D. Rizo, P. Ponce de Leon, C. Perez-Sancho, A. Pertusa, and J. Inesta. 2006. A pattern recognition approach for melody track selection in MIDI files. In Proceedings of 7th International Symposyum on Music Information Retrieval (ISMIR-06), pages 61–66, Victoria, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Scherer</author>
</authors>
<title>Expression of emotion in voice and music.</title>
<date>1995</date>
<journal>Journal of Voice,</journal>
<pages>9--235</pages>
<contexts>
<context position="2967" citStr="Scherer, 1995" startWordPosition="475" endWordPosition="476">e significantly over classifiers that use only one dimension at a time. We believe our results demonstrate the promise of using joint music-lyric models for song processing. 2 Related Work The literature on music analysis is noticeably large, and there are several studies concerning the music’s power over emotions (Juslin and Sloboda, 2001), thinking (Rauscher et al., 1993), or physical effort (Karageorghis and Priest, 2008). In particular, there has been significant research in music and psychology focusing on the idea of a parallel between affective cues in music and speech (Sundberg, 1982; Scherer, 1995). For instance, (Scherer, 2004) investigated the types of emotions that can be induced by music, their mechanisms, and how they can be empirically measured. (Juslin and 590 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 590–599, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Laukka, 2003) conducted a comprehensive review of vocal expressions and music performance, finding substantial overlap in the cues used to convey basic emotions in speech and music. The wo</context>
</contexts>
<marker>Scherer, 1995</marker>
<rawString>K. Scherer. 1995. Expression of emotion in voice and music. Journal of Voice, 9:235–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Scherer</author>
</authors>
<title>Which emotions can be induced by music? what are the underlying mechanisms? and how can we measure them ?</title>
<date>2004</date>
<journal>Journal of New Music Research,</journal>
<pages>33--239</pages>
<contexts>
<context position="2998" citStr="Scherer, 2004" startWordPosition="479" endWordPosition="480">s that use only one dimension at a time. We believe our results demonstrate the promise of using joint music-lyric models for song processing. 2 Related Work The literature on music analysis is noticeably large, and there are several studies concerning the music’s power over emotions (Juslin and Sloboda, 2001), thinking (Rauscher et al., 1993), or physical effort (Karageorghis and Priest, 2008). In particular, there has been significant research in music and psychology focusing on the idea of a parallel between affective cues in music and speech (Sundberg, 1982; Scherer, 1995). For instance, (Scherer, 2004) investigated the types of emotions that can be induced by music, their mechanisms, and how they can be empirically measured. (Juslin and 590 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 590–599, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Laukka, 2003) conducted a comprehensive review of vocal expressions and music performance, finding substantial overlap in the cues used to convey basic emotions in speech and music. The work most closely related to ours</context>
</contexts>
<marker>Scherer, 2004</marker>
<rawString>K. Scherer. 2004. Which emotions can be induced by music? what are the underlying mechanisms? and how can we measure them ? Journal of New Music Research, 33:239–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Honolulu, Hawaii.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>Semeval-2007 task 14: Affective text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on the Semantic Evaluations (SemEval</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10135" citStr="Strapparava and Mihalcea, 2007" startWordPosition="1648" endWordPosition="1651">d the duration of the note. Table 1 shows statistics on the corpus. An example from the corpus, consisting of the first two lines from the Beatles’ song A hard day’s night, is illustrated in Figure 3. SONGS 100 SONGS IN “MAJOR” KEY 59 SONGS IN “MINOR” KEY 41 LINES 4,976 ALIGNED SYLLABLES / NOTES 34,045 Table 1: Some statistics of the corpus Emotion Annotations with Mechanical Turk. In order to explore the classification of emotions in songs, we needed a gold standard consisting of manual emotion annotations of the songs. Following previous work on emotion annotation of text (Alm et al., 2005; Strapparava and Mihalcea, 2007), to annotate the emotions in songs we use the six basic emotions proposed by (Ekman, 1993): ANGER, DISGUST, FEAR, JOY, SADNESS, SURPRISE. To collect the annotations, we use the Amazon Mechanical Turk service, which was previously found to produce reliable annotations with a quality comparable to those generated by experts (Snow et al., 2008). The annotations are collected at line level, with a separate annotation for each of the six emotions. We collect numerical annotations using a scale between 0 and 10, with 0 corresponding to the absence of an emotion, and 10 corresponding to the highest </context>
<context position="28590" citStr="Strapparava and Mihalcea, 2007" startWordPosition="4600" endWordPosition="4603">en absent, e.g., for SURPRISE), which is also explained by their high baseline of over 90%. Comparison to previous work. There is no previous research that has considered the joint use of lyrics and songs representations for emotion classification at line level, and thus we cannot draw a direct comparison with other work on emotion classification in songs. Nonetheless, as a point of reference, we consider the previous work done on emotion classification of texts. Table 8 shows the results obtained in previous work for the recognition of emotions in a corpus consisting of 1,000 news headlines (Strapparava and Mihalcea, 2007) annotated for the same six emotions. Specifically, the table shows the best overall correlation results obtained by the three emotion recognition systems in the SEMEVAL task on Affective Text (Strapparava and Mihalcea, 2007): (Chaumartin, 2007; Kozareva et al., 2007; Katz et al., 2007). The table also shows the best results obtained in follow up work carried out on the same dataset (Strapparava and Mihalcea, 2008). Except for one emotion (FEAR), the correlation figures we obtain are significantly higher than those reported in previous work. As mentioned before, however, a direct comparison ca</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>C. Strapparava and R. Mihalcea. 2007. Semeval-2007 task 14: Affective text. In Proceedings of the 4th International Workshop on the Semantic Evaluations (SemEval 2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>Learning to identify emotions in text.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACM Conference on Applied Computing ACM-SAC</booktitle>
<location>Fortaleza, Brazile.</location>
<contexts>
<context position="16950" citStr="Strapparava and Mihalcea, 2008" startWordPosition="2690" endWordPosition="2693"> the last set of experiments makes joint use of textual and musical features. The experiments are run using linear regression,2 and the results are evaluated by measuring the Pearson correlation between the classifier predictions and the gold standard. For each experiment, a tenfold cross validation is run on the entire dataset.3 4.1 Textual Features First, we attempt to identify the emotions in a line by relying exclusively on the features that can be derived from the lyrics of the song. We decided to focus on those features that were successfully used in the past for emotion classification (Strapparava and Mihalcea, 2008). Specifically, we use: (1) unigram features obtained from a bag-of-words representation, which are the features typically used by corpusbased methods; and (2) lexicon features, indicating the appartenance of a word to a semantic class defined in manually crafted lexicons, which are often used by knowledge-based methods. Unigrams. We use a bag-of-words representation of the lyrics to derive unigram counts, which are then used as input features. First, we build a vocabulary consisting of all the words, including stopwords, occurring in the lyrics of the training set. We then remove those words </context>
<context position="29008" citStr="Strapparava and Mihalcea, 2008" startWordPosition="4669" endWordPosition="4672">ous work done on emotion classification of texts. Table 8 shows the results obtained in previous work for the recognition of emotions in a corpus consisting of 1,000 news headlines (Strapparava and Mihalcea, 2007) annotated for the same six emotions. Specifically, the table shows the best overall correlation results obtained by the three emotion recognition systems in the SEMEVAL task on Affective Text (Strapparava and Mihalcea, 2007): (Chaumartin, 2007; Kozareva et al., 2007; Katz et al., 2007). The table also shows the best results obtained in follow up work carried out on the same dataset (Strapparava and Mihalcea, 2008). Except for one emotion (FEAR), the correlation figures we obtain are significantly higher than those reported in previous work. As mentioned before, however, a direct comparison cannot be made, since the earlier work used a different, smaller dataset. Moreover, our corpus of songs is likely to be more emotionally loaded than the news titles used in previous work. 6 Conclusions Popular songs express universally understood meanings and embody experiences and feelings shared by many, usually through a combined effect of both music and lyrics. In this paper, we introduced a novel corpus of music</context>
</contexts>
<marker>Strapparava, Mihalcea, 2008</marker>
<rawString>C. Strapparava and R. Mihalcea. 2008. Learning to identify emotions in text. In Proceedings of the ACM Conference on Applied Computing ACM-SAC 2008, Fortaleza, Brazile.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
</authors>
<title>Wordnet-affect: an affective extension of wordnet.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation,</booktitle>
<location>Lisbon.</location>
<contexts>
<context position="18996" citStr="Strapparava and Valitutti, 2004" startWordPosition="3016" endWordPosition="3019">l score equal to the average of the scores on the training data, and measured the correlation between these default scores and the gold standard, consistently led to correlations close to 0 (0.0081-0.0221). guistic Inquiry and Word Count (LIWC) and WordNet Affect (WA) to derive coarse textual features. LIWC was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999; Pennebaker and King, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., emotion, cognition). WA (Strapparava and Valitutti, 2004) is a resource that was created starting with WordNet, by annotating synsets with several emotions. It uses several resources for affective information, including the emotion classification of Ortony (Ortony et al., 1987). From WA, we extract the words corresponding to the six basic emotions used in our experiments. For each semantic class, we infer a feature indicating the number of words in a line belonging to that class. Table 3 shows the Pearson correlations obtained for each of the six emotions, when using only unigrams, only semantic classes, or both. Emotion Unigrams Semantic All Classe</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>C. Strapparava and A. Valitutti. 2004. Wordnet-affect: an affective extension of wordnet. In Proceedings of the 4th International Conference on Language Resources and Evaluation, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sundberg</author>
</authors>
<title>Speech, song, and emotions.</title>
<date>1982</date>
<booktitle>The Neuropsychology ofMusic.</booktitle>
<editor>In M. Clynes, editor, Music, Mind and Brain:</editor>
<publisher>Plenum Press,</publisher>
<location>New York.</location>
<contexts>
<context position="2951" citStr="Sundberg, 1982" startWordPosition="473" endWordPosition="474">music can improve significantly over classifiers that use only one dimension at a time. We believe our results demonstrate the promise of using joint music-lyric models for song processing. 2 Related Work The literature on music analysis is noticeably large, and there are several studies concerning the music’s power over emotions (Juslin and Sloboda, 2001), thinking (Rauscher et al., 1993), or physical effort (Karageorghis and Priest, 2008). In particular, there has been significant research in music and psychology focusing on the idea of a parallel between affective cues in music and speech (Sundberg, 1982; Scherer, 1995). For instance, (Scherer, 2004) investigated the types of emotions that can be induced by music, their mechanisms, and how they can be empirically measured. (Juslin and 590 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 590–599, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Laukka, 2003) conducted a comprehensive review of vocal expressions and music performance, finding substantial overlap in the cues used to convey basic emotions in speech a</context>
</contexts>
<marker>Sundberg, 1982</marker>
<rawString>J. Sundberg. 1982. Speech, song, and emotions. In M. Clynes, editor, Music, Mind and Brain: The Neuropsychology ofMusic. Plenum Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="26476" citStr="Vapnik, 1995" startWordPosition="4262" endWordPosition="4263">. As an additional evaluation, we transform the task into a binary classification by using a threshold empirically set at 3. Thus, to generate the coarse binary annotations, if the score of an emotion is below 3, we record it as “negative” (i.e., the emotion is absent), whereas if the score is equal to or above 3, we record it as “positive” (i.e., the emotion is present). For the classification, we use Support Vector Machines (SVM), which are binary classifiers that seek to find the hyperplane that best separates a set of positive examples from a set of negative examples, with maximum margin (Vapnik, 1995). Applications of SVM classifiers to text categorization led to some of the best results reported in the literature (Joachims, 1998). Table 7 shows the results obtained for each of the six emotions, and for the three major settings that we considered: textual features only, musical features only, and a classifier that jointly uses the textual and the musical features. As before, the classification accuracy for each experiment is reported as the average of the accuracies obtained during a tenfold cross-validation on the corpus. The table also shows a baseline, computed as the average of the acc</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Velusamy</author>
<author>B Thoshkahna</author>
<author>K Ramakrishnan</author>
</authors>
<title>Novel melody line identification algorithm for polyphonic MIDI music.</title>
<date>2007</date>
<booktitle>In Proceedings of 13th International Multimedia Modeling Conference (MMM 2007), Singapore,</booktitle>
<contexts>
<context position="5868" citStr="Velusamy et al., 2007" startWordPosition="936" endWordPosition="939">g MIDI and audio features, while (Wang et al., 2004) automatically aligned acoustic musical signals with their corresponding textual lyrics. MIDI files are typically organized into one or more parallel “tracks” for independent recording and editing. A reliable system to identify the MIDI track containing the melody1 is very relevant for music information retrieval, and 1A melody can be defined as a “cantabile” sequence of notes, usually the sequence that a listener can remember after hearing a song. there are several approaches that have been proposed to address this issue (Rizo et al., 2006; Velusamy et al., 2007). Another related study concerned with the interaction of lyrics and music using an annotated corpus is found in (O’Hara, 2011), who presented preliminary research that checks whether the expressive meaning of a particular harmony or harmonic sequence could be deduced from the lyrics it accompanies, by using harmonically annotated chords from the Usenet group alt.guitar.tab. Finally, in natural language processing, there are a few studies that mainly exploited the lyrics component of the songs, while generally ignoring the musical component. For instance, (Mahedero et al., 2005) dealt with lan</context>
</contexts>
<marker>Velusamy, Thoshkahna, Ramakrishnan, 2007</marker>
<rawString>S. Velusamy, B. Thoshkahna, and K. Ramakrishnan. 2007. Novel melody line identification algorithm for polyphonic MIDI music. In Proceedings of 13th International Multimedia Modeling Conference (MMM 2007), Singapore, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>M Kan</author>
<author>T Nwe</author>
<author>A Shenoy</author>
<author>J Yin</author>
</authors>
<title>LyricAlly: Automatic synchronization of acoustic musical signals and textual lyrics.</title>
<date>2004</date>
<booktitle>In Proceedings of MM’04,</booktitle>
<location>New York,</location>
<contexts>
<context position="5298" citStr="Wang et al., 2004" startWordPosition="845" endWordPosition="848">ation has received a lot of attention from both the arts and the computer science communities (see for instance (Orio, 2006) for an introduction to this task). There are also several works on MIDI analysis. Among them, particularly relevant to our research is the work by (Das et al., 2000), who described an analysis of predominant up-down motion types within music, through extraction of the kinematic variables of music velocity and acceleration from MIDI data streams. (Cataltepe et al., 2007) addressed music genre classification (e.g., classic, jazz, pop) using MIDI and audio features, while (Wang et al., 2004) automatically aligned acoustic musical signals with their corresponding textual lyrics. MIDI files are typically organized into one or more parallel “tracks” for independent recording and editing. A reliable system to identify the MIDI track containing the melody1 is very relevant for music information retrieval, and 1A melody can be defined as a “cantabile” sequence of notes, usually the sequence that a listener can remember after hearing a song. there are several approaches that have been proposed to address this issue (Rizo et al., 2006; Velusamy et al., 2007). Another related study concer</context>
</contexts>
<marker>Wang, Kan, Nwe, Shenoy, Yin, 2004</marker>
<rawString>Y. Wang, M. Kan, T. Nwe, A. Shenoy, and J. Yin. 2004. LyricAlly: Automatic synchronization of acoustic musical signals and textual lyrics. In Proceedings of MM’04, New York, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Xia</author>
<author>L Wang</author>
<author>K F Wong</author>
<author>M Xu</author>
</authors>
<title>Lyricbased song sentiment classification with sentiment vector space model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6570" citStr="Xia et al., 2008" startWordPosition="1040" endWordPosition="1043">nnotated corpus is found in (O’Hara, 2011), who presented preliminary research that checks whether the expressive meaning of a particular harmony or harmonic sequence could be deduced from the lyrics it accompanies, by using harmonically annotated chords from the Usenet group alt.guitar.tab. Finally, in natural language processing, there are a few studies that mainly exploited the lyrics component of the songs, while generally ignoring the musical component. For instance, (Mahedero et al., 2005) dealt with language identification, structure extraction, and thematic categorization for lyrics. (Xia et al., 2008) addressed the task of sentiment classification in lyrics, recognizing positive and negative moods in a large dataset of Chinese pop songs, while (Yang and Lee, 2009) approached the problem of emotion identification in lyrics, classifying songs from allmusic.com using a set of 23 emotions. 3 A Corpus of Music and Lyrics Annotated for Emotions To enable our exploration of emotions in songs, we compiled a corpus of 100 popular songs (e.g., Dancing Queen by ABBA, Hotel California by Eagles, Let it Be by The Beatles). Popular songs exert a lot of power on people, both at an individual level as wel</context>
</contexts>
<marker>Xia, Wang, Wong, Xu, 2008</marker>
<rawString>Y. Xia, L. Wang, K.F. Wong, and M. Xu. 2008. Lyricbased song sentiment classification with sentiment vector space model. In Proceedings of the Association for Computational Linguistics, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yang</author>
<author>W Lee</author>
</authors>
<title>Disambiguating music emotion using software agents.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Music Information Retrieval,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="3872" citStr="Yang and Lee, 2004" startWordPosition="612" endWordPosition="615">tural Language Learning, pages 590–599, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics Laukka, 2003) conducted a comprehensive review of vocal expressions and music performance, finding substantial overlap in the cues used to convey basic emotions in speech and music. The work most closely related to ours is the combination of audio and lyrics for emotion classification in songs, as thoroughly surveyed in (Kim et al., 2010). Although several methods have been proposed, including a combination of textual features and beats per minute and MPEG descriptors (Yang and Lee, 2004); individual audio and text classifiers for arousal and valence, followed by a combination through meta-learning (Yang et al., 2008); and the use of crowdsourcing labeling from Last.fm to collect large datasets of songs annotated for emotions (Laurier et al., 2008; Hu et al., 2009), all this previous work was done at song level, and most of it focused on valence-arousal classifications. None of the previous methods considered the fine-grained classification of emotions at line level, as we do, and none of them considered the six Ekman emotions used in our work. Other related work consists of t</context>
</contexts>
<marker>Yang, Lee, 2004</marker>
<rawString>D. Yang and W. Lee. 2004. Disambiguating music emotion using software agents. In Proceedings of the International Conference on Music Information Retrieval, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yang</author>
<author>W Lee</author>
</authors>
<title>Music emotion identification from lyrics.</title>
<date>2009</date>
<booktitle>In Proceedings of 11th IEEE Symposium on Multimedia.</booktitle>
<contexts>
<context position="6736" citStr="Yang and Lee, 2009" startWordPosition="1067" endWordPosition="1070">ce could be deduced from the lyrics it accompanies, by using harmonically annotated chords from the Usenet group alt.guitar.tab. Finally, in natural language processing, there are a few studies that mainly exploited the lyrics component of the songs, while generally ignoring the musical component. For instance, (Mahedero et al., 2005) dealt with language identification, structure extraction, and thematic categorization for lyrics. (Xia et al., 2008) addressed the task of sentiment classification in lyrics, recognizing positive and negative moods in a large dataset of Chinese pop songs, while (Yang and Lee, 2009) approached the problem of emotion identification in lyrics, classifying songs from allmusic.com using a set of 23 emotions. 3 A Corpus of Music and Lyrics Annotated for Emotions To enable our exploration of emotions in songs, we compiled a corpus of 100 popular songs (e.g., Dancing Queen by ABBA, Hotel California by Eagles, Let it Be by The Beatles). Popular songs exert a lot of power on people, both at an individual level as well as on groups, mainly because of the message and emotions they convey. Songs can lift our moods, make us dance, or move us to tears. Songs are able to embody deep fe</context>
</contexts>
<marker>Yang, Lee, 2009</marker>
<rawString>D. Yang and W. Lee. 2009. Music emotion identification from lyrics. In Proceedings of 11th IEEE Symposium on Multimedia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-H Yang</author>
<author>Y-C Lin</author>
<author>H-T Cheng</author>
<author>I-B Liao</author>
<author>Y-C Ho</author>
<author>H Chen</author>
</authors>
<title>Toward multi-modal music emotion classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Pacific Rim Conference on Multimedia: Advances in Multimedia Information Processing.</booktitle>
<contexts>
<context position="4004" citStr="Yang et al., 2008" startWordPosition="631" endWordPosition="634"> 2003) conducted a comprehensive review of vocal expressions and music performance, finding substantial overlap in the cues used to convey basic emotions in speech and music. The work most closely related to ours is the combination of audio and lyrics for emotion classification in songs, as thoroughly surveyed in (Kim et al., 2010). Although several methods have been proposed, including a combination of textual features and beats per minute and MPEG descriptors (Yang and Lee, 2004); individual audio and text classifiers for arousal and valence, followed by a combination through meta-learning (Yang et al., 2008); and the use of crowdsourcing labeling from Last.fm to collect large datasets of songs annotated for emotions (Laurier et al., 2008; Hu et al., 2009), all this previous work was done at song level, and most of it focused on valence-arousal classifications. None of the previous methods considered the fine-grained classification of emotions at line level, as we do, and none of them considered the six Ekman emotions used in our work. Other related work consists of the development of tools for music accessing, filtering, classification, and retrieval, focusing primarily on music in digital format</context>
</contexts>
<marker>Yang, Lin, Cheng, Liao, Ho, Chen, 2008</marker>
<rawString>Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, I.-B. Liao, Y.-C. Ho, and H. Chen. 2008. Toward multi-modal music emotion classification. In Proceedings of the 9th Pacific Rim Conference on Multimedia: Advances in Multimedia Information Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
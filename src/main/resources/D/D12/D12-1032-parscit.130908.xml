<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9719">
Name Phylogeny: A Generative Model of String Variation
</title>
<author confidence="0.993764">
Nicholas Andrews and Jason Eisner and Mark Dredze
</author>
<affiliation confidence="0.886020666666667">
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA
</affiliation>
<email confidence="0.998892">
{noa,eisner,mdredze}@jhu.edu
</email>
<sectionHeader confidence="0.994802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999638647058823">
Many linguistic and textual processes involve transduc-
tion of strings. We show how to learn a stochastic trans-
ducer from an unorganized collection of strings (rather
than string pairs). The role of the transducer is to orga-
nize the collection. Our generative model explains simi-
larities among the strings by supposing that some strings
in the collection were not generated ab initio, but were in-
stead derived by transduction from other, “similar” strings
in the collection. Our variational EM learning algorithm
alternately reestimates this phylogeny and the transducer
parameters. The final learned transducer can quickly link
any test name into the final phylogeny, thereby locating
variants of the test name. We find that our method can
effectively find name variants in a corpus of web strings
used to refer to persons in Wikipedia, improving over stan-
dard untrained distances such as Jaro-Winkler and Leven-
shtein distance.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964109090909">
Systematic relationships between pairs of strings
are at the core of problems such as transliteration
(Knight and Graehl, 1998), morphology (Dreyer and
Eisner, 2011), cross-document coreference resolu-
tion (Bagga and Baldwin, 1998), canonicalization
(Culotta et al., 2007), and paraphrasing (Barzilay and
Lee, 2003). Stochastic transducers such as proba-
bilistic finite-state transducers are often used to cap-
ture such relationships. They model a conditional
distribution p(y I x), and are ordinarily trained on
input-output pairs of strings (Dreyer et al., 2008).
In this paper, we are interested in learning from
an unorganized collection of strings, some of which
might have been derived from others by transforma-
tive linguistic processes such as abbreviation, mor-
phological derivation, historical sound or spelling
change, loanword formation, translation, transliter-
ation, editing, or transcription error. We assume that
each string was derived from at most one parent, but
may give rise to any number of children.
The difficulty is that most or all of these parent-
child relationships are unobserved. We must recon-
struct this evolutionary phylogeny. At the same time,
we must fit the parameters of a model of the relevant
linguistic process p(y I x), which says what sort of
children y might plausibly be derived from parent x.
Learning this model of p(y I x) helps us organize the
training collection by reconstructing its phylogeny,
and also permits us to generalize to new forms.
We will focus on the problem of name varia-
tion. We observe a collection of person names—full
names, nicknames, abbreviated or misspelled names,
etc. Some of these names can refer to the same per-
son; we hope to detect this. It would be an unlikely
coincidence if two mentions of John Jacob Jingle-
heimer Schmidt referred to different people, since
this is a long and unusual name. Similarly, John Ja-
cob Jingelhimer Smith and Dr. J. J. Jingleheimer may
also be related names for this person. That is, these
names may be derived from one another, via unseen
relationships, although we cannot be sure.
Readers may be reminded of unsupervised clus-
tering, in which “suspiciously similar” points can be
explained as having been generated by the same clus-
ter. Since each name is linked to at most one parent,
our setting resembles single-link clustering—with a
learned, asymmetric distance measure p(y I x).
We will propose a generative process that makes
explicit assumptions about how strings are copied
with mutation. It is assumed to have generated all the
names in the collection, in an unknown order. Given
learned parameters, we can ask the model whether a
name Dr. J. J. Jingelheimer in the collection is more
likely to have been generated from scratch, or derived
from some previous name.
</bodyText>
<sectionHeader confidence="0.772797" genericHeader="introduction">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.994234">
Several previous papers have also considered learn-
ing transducers or other models of word pairs when
</bodyText>
<page confidence="0.981142">
344
</page>
<note confidence="0.7654145">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 344–355, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999553604651163">
the pairing between inputs and outputs is not given.
Most commonly, one observes parallel or compa-
rable corpora in two languages, and must recon-
struct a matching from one language’s words to the
other’s before training on the resulting pairs (Schafer,
2006b; Klementiev and Roth, 2006; Haghighi et al.,
2008; Snyder et al., 2010; Sajjad et al., 2011).
Hall and Klein (2010) extend this setting to more
than two languages, where the phylogenetic tree is
known. A given lexeme (abstract word) can be re-
alized in each language by at most one word (string
type), derived from the parent language’s realization
of the same lexeme. The system must match words
that share an underlying lexeme (i.e., cognates), cre-
ating a matching of each language’s vocabulary to its
parent language’s vocabulary. A further challenge is
that the parent words are unobserved ancestral forms.
Similarly, Dreyer and Eisner (2011) organize
words into morphological paradigms of a given
structure. Again words with the same underlying lex-
eme (i.e., morphemes) must be identified. A lexeme
can be realized in each grammatical inflection (such
as “first person plural present”) by exactly one word
type, related to other inflected forms of the same lex-
eme, which as above may be unobserved. Their in-
ference setting is closer to ours because the input is
an unorganized collection of words—input words are
not tagged with their grammatical inflections. This
contrasts with the usual multilingual setting where
each word is tagged with its true language.
In one way, our problem differs significantly from
the above problems. We are interested in random
variation that may occur within a language as well
as across languages. A person name may have un-
boundedly many different variants. This is unlike
the above problems, in which a lexeme has at most
K realizations, where K is the (small) number of
languages or inflections.1 We cannot assign the ob-
served strings to positions in an existing structure
that is shared across all lexemes, such as a given phy-
logenetic tree whose K nodes represent languages,
or a given inflectional grid whose K cells represent
grammatical inflections. Rather, we must organize
</bodyText>
<footnote confidence="0.974956">
1In the above problems, one learns a set of O(K) or O(K2)
specialized transducers that relate Latin to Italian, singular to
plural, etc. We instead use one global mutation model that ap-
plies to all names—but see footnote 14 on incorporating special-
ized transductions (Latin to Italian) within our mutation model.
</footnote>
<bodyText confidence="0.999694583333333">
them into a idiosyncratic phylogenetic tree whose
nodes are the string types or tokens themselves.
Names and words are not the only non-biological
objects that are copied with mutation. Documents,
database records, bibliographic entries, code, and
images can evolve in the same way. Reconstructing
these relationships has been considered by a number
of papers on authorship attribution, near-duplicate
detection, deduplication, record linkage, and plagia-
rism detection. A few such papers reconstruct a phy-
logeny, as in the case of chain letters (Bennett et
al., 2003), malware (Karim et al., 2005), or images
(Dias et al., 2012). In fact, the last of these uses the
same minimum spanning tree method that we apply
in §5.3. However, these papers do not train a similar-
ity measure as we do. To our knowledge, these two
techniques have not been combined outside biology.
In molecular evolutionary analysis, phylogenetic
techniques have often been combined with estima-
tion of some parametric model of mutation (Tamura
et al., 2011). However, names mutate differently
from biological sequences, and our mutation model
for names (§4, §8) reflects that. We also posit a spe-
cific process (§3) that generates the name phylogeny.
</bodyText>
<sectionHeader confidence="0.813772" genericHeader="method">
2 An Example
</sectionHeader>
<bodyText confidence="0.998145157894737">
A fragment of a phylogeny for person names is
shown in Figure 1. Our procedure learned this auto-
matically from a collection of name tokens, without
observing any input/output pairs. The nodes of the
phylogeny are the observed name types,2 each one
associated with a count of observed tokens.
Each arrow corresponds to a hypothesized mu-
tation. These mutations reflect linguistic processes
such as misspelling, initialism, nicknaming, translit-
eration, etc. As an exception, however, each ar-
row from the distinguished root node Q generates
an initial name for a new entity. The descendants of
this initial name are other names that subsequently
evolved for that entity. Thus, the child subtrees of Q
give a partition of the name types into entities.
Thanks to the phylogeny, the seemingly disparate
names Ghareeb Nawaz and Muinuddin Chishti are
seen to refer to the same entity. They may be traced
back to their common ancestor Khawaja Gharib-
</bodyText>
<footnote confidence="0.9063685">
2We cannot currently hypothesize unobserved intermediate
forms, e.g., common ancestors of similar strings. See §6.2.
</footnote>
<page confidence="0.998214">
345
</page>
<figureCaption confidence="0.99946">
Figure 1: A portion of a spanning tree found by our model.
</figureCaption>
<figure confidence="0.873749166666667">
Khwaja Gharib Nawaz
Ghareeb Nawaz
Muinuddin Chishti
Khawaja Gharibnawaz Muinuddin Hasan Chisty
Khwaja Muin al-Din Chishti
Khwaja Moinuddin Chishti
Khwaja gharibnawaz
Thomas Pynchon, Jr.
Thomas Pynchon Jr.
Thomas Ruggles Pynchon, Jr.
Thomas Ruggles Pynchon Jr.
Thomas R. Pynchon, Jr.
</figure>
<figureCaption confidence="0.703187">
Thomas R. Pynchon Jr.
Thomas R. Pynchon
</figureCaption>
<bodyText confidence="0.999075230769231">
nawaz Muinuddin Hasan Chisty, from which both
were derived via successive mutations.
Not shown in Figure 1 is our learned family p of
conditional probability distributions, which models
the likely mutations in this corpus. Our EM learn-
ing procedure found p jointly with the phylogeny.
Specifically, it alternated between improving p and
improving the distribution over phylogenies. At the
end, we extracted the single best phylogeny.
Together, the learned p and the phylogeny in Fig-
ure 1 form an explanation of the observed collection
of names. What makes it more probable than other
explanations? Informally, two properties:
</bodyText>
<listItem confidence="0.990034823529412">
• Each node in the tree is plausibly derived from
its parent. More precisely, the product of
the edge probabilities under p is comparatively
high. A different p would have reduced the
probability of the events in this phylogeny. A
different phylogeny would have involved a more
improbable collection of events, such as replac-
ing Chishti with Pynchon, or generating many
unrelated copies of Pynchon directly from Q.
• In the phylogeny, the parent names tend to be
used often enough that it is plausible for variants
of these names to have emerged. Our model
says that new tokens are derived from previ-
ously generated tokens. Thus—other things
equal—Barack Obama is more plausibly a vari-
ant of Barack Obama, Jr. than of Barack
Obama, Sr. (which has fewer tokens).
</listItem>
<sectionHeader confidence="0.976606" genericHeader="method">
3 A Generative Model of Tokens
</sectionHeader>
<bodyText confidence="0.9892831">
Our model should reflect the reasons that name vari-
ation exists. A named entity has the form y = (e, w)
where w is a string being used to refer to entity e. A
single entity e may be referred to on different occa-
sions by different name strings w. We suppose that
this is the result of copying the entity with occasional
mutation of its name (as in asexual reproduction).
Thus, we assume the following simple generative
process that produces an ordered sequence of tokens
y1, y2,. . ., where yi = (ei, wi).
</bodyText>
<listItem confidence="0.993727666666667">
• After the first k tokens y1,... yk have been gen-
erated, the author responsible for generating yk+1
must choose whom to talk about next. She is likely
to think of someone she has heard about often in the
past. So to make this choice, she selects one of the
previous tokens yi uniformly at random, each having
probability 1/(k + α); or else she selects Q, with
probability α/(k + α).
• If the author selected a previous token yi, then
</listItem>
<bodyText confidence="0.823616777777778">
with probability 1 − µ she copies it faithfully, so
yk+1 = yi. But with probability µ, she instead draws
a mutated token yk+1 = (ek+1, wk+1) from the mu-
tation model p(· yi). This preserves the entity
(ek+1 = ei with probability 1), but the new name
wk+1 is a stochastic transduction of wi drawn from
p(· wi).3 For example, in referring to ei, the author
may shorten and respell wi = Khwaja Gharib Nawaz
into wk+1 = Ghareeb Nawaz (Figure 1).
</bodyText>
<listItem confidence="0.968243625">
• If the author selected Q, she must choose a fresh
entity yk+1 = (ek+1, wk+1) to talk about. So she
sets ek+1 to a newly created entity, sampling its name
wk+1 from the distribution p(· Q). For example,
wk+1 = Thomas Ruggles Pynchon, Jr. (Figure 1).
Nothing prevents wk+1 from being a name that is al-
ready in use for another entity (i.e., wk+1 may equal
wj for some j G k).
</listItem>
<footnote confidence="0.99601225">
3Straightforward extensions are to allow a variable mutation
rate µ(yi) that depends on properties of yi, and to allow wk+1
to depend on known properties of ei. See footnote 14 for further
discussion of enriched tokens.
</footnote>
<page confidence="0.997625">
346
</page>
<subsectionHeader confidence="0.995987">
3.1 Relationship to other models
</subsectionHeader>
<bodyText confidence="0.997326744680851">
If we ignore the name strings, we can see that the
sequence of entities e1, e2,... eN is being generated
from a Chinese restaurant process (CRP) with con-
centration parameter α. To the extent that α is low
(so that o is rarely used), a few randomly chosen en-
tities will dominate the corpus.
The CRP is equivalent to sampling e1, e2,... IID
from an unknown distribution that was itself drawn
from a Dirichlet process with concentration α. This
is indeed a standard model of a distribution over en-
tities. For example, Hall et al. (2008) use it to model
venues in bibliographic entries.
From this characterization of the CRP, one can see
that any permutation of this entity sequence would
have the same probability. That is, our distribution
over sequences of entities e is exchangeable.
However, our distribution over sequences of
named entities y = (e, w) is non-exchangeable.
It assigns different probabilities to different order-
ings of the same tokens. This is because our model
posits that later authors are influenced by earlier au-
thors, copying entity names from them with muta-
tion. So ordering is important. The mutation process
is not symmetric—for example, Figure 1 reflects a
tendency to shorten rather than lengthen names.
Non-exchangeability is one way that our present
model differs from (parametric) transformation mod-
els (Eisner, 2002) and (non-parametric) transforma-
tion processes (Andrews and Eisner, 2011). These
too are defined using mutation of strings or other
types. From a transformation process, one can draw
a distribution over types, from which the tokens are
then sampled IID. This results in an exchangeable
sequence of tokens, just as in the Dirichlet process.
We avoid transformation models here for three
reasons. (1) Inference is more expensive. (2) A
transformation process seems less realistic as a
model of authorship. It constructs a distribution over
derivational paths, similar to the paths in Figure 1.
It effectively says that each token is generated by re-
capitulating some previously used path from Q, but
with some chance of deviating at each step. For an
author to generate a name token this way, she would
have to know the whole derivational history of the
previous name she was adapting. Our present model
instead allows an author simply to select a name she
previously saw and copy or mutate its surface form.
</bodyText>
<listItem confidence="0.993351">
(3) One should presumably prefer to explain a novel
name y as a mutation of a frequent name x, other
things equal (§2). But surprisingly, inference under
the transformation process does not prefer this.4
</listItem>
<bodyText confidence="0.999362538461538">
Another view of our present model comes from
the literature on random graphs (e.g., for modeling
social networks or the link structure of the web). In
a preferential attachment model, a graph’s vertices
are added one by one, and each vertex selects some
previous vertices as its neighbors. Our phylogeny
is a preferential attachment tree, a random directed
graph in which each vertex selects a single previous
vertex as its parent. Specifically, it is a random recur-
sive tree (Smythe and Mahmoud, 1995) whose ver-
tices are the tokens.5 To this simple random topol-
ogy we have added a random labeling process with
mutation. The first α vertices are labeled with Q.
</bodyText>
<sectionHeader confidence="0.997458" genericHeader="method">
4 A Mutation Model for Strings
</sectionHeader>
<bodyText confidence="0.9998994375">
Our model in §3 samples the next token y, when it is
not simply a faithful copy, from p(y x) or p(y Q) .
The key step there is to sample the name string wy
from p(wy wx) or p(wy Q) .
Our model of these distributions could easily in-
corporate detailed linguistic knowledge of the muta-
tion process (see §8). Here we describe the specific
model that we use in our experiments. Like many
such models, it can be regarded as a stochastic finite-
state string-to-string transducer parameterized by 0.
There is much prior work on stochastic models of
edit distance (Ristad and Yianilos, 1998; Bilenko and
Mooney, 2003; Oncina and Sebban, 2006; Schafer,
2006a; Bouchard-Côté et al., 2008; Dreyer et al.,
2008, among others). For the present experiments,
we designed a moderately simple one that employs
</bodyText>
<listItem confidence="0.7472578">
(1) conditioning on one character of right context,
(2) latent “edit” and “no-edit” regions to capture the
fact that groups of edits are often made in close prox-
imity, and (3) some simple special handling for the
distribution conditioned on the root p(wy Q) .
</listItem>
<bodyText confidence="0.997821">
We assume a stochastic mutation process which,
when given an input string wx, edits it from left to
</bodyText>
<footnote confidence="0.9979802">
4The very fact that x has been frequently observed demon-
strates that it has often chosen to stop mutating. This implies
that it is likely to choose stop again rather than mutate into y.
5This is not the tree shown in Figure 1, whose vertices are
types rather than tokens.
</footnote>
<page confidence="0.997406">
347
</page>
<bodyText confidence="0.998416948717949">
right into an output string wy. Then p(wy  |wx) is
the total probability of all operation sequences on wx
that would produce wy. This total can be computed
in time O(|wx |· |wy|) by dynamic programming.
Our process has four character-level edit opera-
tions: copy, substitute, insert, delete. It also has a
distinguished no-edit operation that behaves exactly
like copy. At each step, the process first randomly
chooses whether to edit or no-edit, conditioned only
on whether the previous operation was an edit. If it
chooses to edit, it chooses a random edit type with
some probability conditioned on the next input char-
acter. In the case of insert or substitute, it then ran-
domly chooses an output character, conditioned on
the type of edit and the next input character.
It is common to mutate a name by editing con-
tiguous substrings (e.g., words). Contiguous regions
of copying versus editing can be modeled by a low
probability of transitioning between no-edit and edit
regions.6 Note that an edit region may include some
copy edits (or substitute edits that replace a charac-
ter with itself) without leaving the edit region. This
is why we distinguish copy from no-edit.
Input and output strings are augmented with a
trailing Eos (“end-of-string”) symbol that is seen by
the single-character lookahead. If the next character
is Eos, the only available edit is insert. Alternatively,
if the process selects no-edit, then Eos is copied to
the output string and the process terminates.
In the case of p(wy  |Q), the input string is empty,
and both input and output are augmented with a trail-
ing Eos&apos; character that behaves like Eos. Then wy
is generated by a sequence of insertions followed by
a copy. These are conditioned as usual on the next
character, here Eos&apos;, so the model can learn to insert
more or different characters when the input is Q.
The parameters 0 determining the conditional
probabilities of the different operations and charac-
ters are estimated with backoff smoothing.
</bodyText>
<sectionHeader confidence="0.999531" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.930449272727273">
The input to inference is a collection of named entity
tokens y. Most are untagged tokens of the form y =
(?, w). In a semi-supervised setting, however, some
6This somewhat resembles the traditional affine gap penalty
in computational biology (Gusfield, 1997), which makes dele-
tions or insertions cheaper if they are consecutive. We instead
make consecutive edits cheaper regardless of the edit type.
of the tokens may be tagged tokens of the form y =
(e, w), whose true entity is known. The entity tags
place a constraint on the phylogeny, since each child
subtree of Q must correspond to exactly one entity.
</bodyText>
<subsectionHeader confidence="0.993997">
5.1 An unrealistically supervised setting
</subsectionHeader>
<bodyText confidence="0.999914916666667">
Suppose we were lucky enough to fully observe the
sequence of named entity tokens yi = (ei, wi) pro-
duced by our generative model. That is, suppose all
tokens were tagged and we knew their ordering.
Yet there would still be something to infer: which
tokens were derived from which previous tokens.
This phylogeny is described by a spanning tree over
the tokens. Let us see how to infer it.
For each potential edge x —* y between named
entity tokens, define S(y  |x) to be the probability of
choosing x and copying it (possibly with mutation)
to obtain y. So
</bodyText>
<equation confidence="0.999893">
S(yj  |Q) = α p(yj  |Q) (1)
S(yj  |yi) = µ p(yj  |yi) + (1 − µ)✶(yj = yi) (2)
</equation>
<bodyText confidence="0.981637888888889">
except that if i &gt; j or if ei =� ej, then S(yj  |yi) = 0
(since yj can only be derived from an earlier token
yi with the same entity).
Now the prior probability of generating yi, ... yN
with a given phylogenetic tree is easily seen to be a
product over all tree edges, Hj S(yj  |pa(yj)) where
pa(yj) is the parent of yj. As a result, it is known
that the following are efficient to compute from the
(N + 1) x (N + 1) matrix of S values (see §5.3):
</bodyText>
<listItem confidence="0.986464">
(a) the max-probability spanning tree
(b) the total probability of all spanning trees
(c) the marginal probability of each edge, under the
posterior distribution on spanning trees
</listItem>
<bodyText confidence="0.758594272727273">
(a) is our single best guess of the phylogeny. We use
this during evaluation. (b) gives the model likeli-
hood, i.e., the total probability of the observed data
yi, ... yN. To locally maximize the model likeli-
hood, (c) can serve as the E step of our EM algorithm
(§6) for tuning our mutation model. The M step then
retrains the mutation model’s parameters 0 on input-
output pairs wi —* wj, weighting each pair by its
edge’s posterior marginal probability (c), since that
is the expected count of a wi —* wj mutation. This
computation is iterated.
</bodyText>
<page confidence="0.991401">
348
</page>
<subsectionHeader confidence="0.994627">
5.2 The unsupervised setting
</subsectionHeader>
<bodyText confidence="0.999974857142857">
Now we turn to a real setting—fully unsupervised
data. Two issues will force us to use an approximate
inference algorithm. First, we have an untagged cor-
pus: a token’s entity tag a is never observed. Second,
the order of the tokens is not observed, so we do not
know which other tokens are candidate parents.
Our first approximation is to consider only phylo-
genies over types rather than tokens.7 The type phy-
logeny in Figure 1 represents a set of possible token
phylogenies. Each node of Figure 1 represents an
untagged name type y = (?, w). By grouping all ny
tokens of this type into a single node, we mean that
the first token of y was derived by mutation from the
parent node, while each later token of y was derived
by copying an (unspecified) earlier token of y.
A token phylogeny cannot be represented in this
way if two or more tokens of y were created by mu-
tations. In that case, their name strings are equal only
by coincidence. They may have different parents
(perhaps of different entities), whereas the y node in
a type phylogeny can have only one parent.
We argue, however, that these unrepresentable to-
ken phylogenies are comparatively unlikely a poste-
riori and can be reasonably ignored during inference.
The first token of y is necessarily a mutation, but later
tokens are much more likely to be copies. The prob-
ability of generating a later token y by copying some
previous token is at least
</bodyText>
<equation confidence="0.999063125">
(1 − µ)/(N + α),
y1, ... yN in that order and respecting the phylogeny:
⎛ ⎞
1 ny−1
k + α! Y g(y  |pa(y)) i (1 − µ)
i=1
(
y∈Y 3)
</equation>
<bodyText confidence="0.923564">
where g(y
pa(y)) is a factor for generating the first
token of y fr
</bodyText>
<equation confidence="0.88194625">
|
om its parent pa(y), defined by
g(y  |♦) = α · p(y  |♦) (4)
g(y  |x) = µ · (# tokens of x preceding
</equation>
<bodyText confidence="0.9482965">
first token of y) · p(y  |x) (5)
y know the token order: by
product of expectations of its individual
To
find the expectation of (5), observe that the expected
number of tokens of x that precede the first token of
y is
since each of the nx tokens of x has
</bodyText>
<equation confidence="0.8969672">
a
+ 1) chan
factors.8
nx/(ny+1),
1/(ny
</equation>
<bodyText confidence="0.99275875">
ce of falling before all ny tokens
of y. It follows that the approximated probability of
generating all tokens in some order, with our given
type parentage, is proportional to
</bodyText>
<equation confidence="0.850425571428571">
N
Y
k=1
while the probability of generating it in some other where Y
way is at most y∈Y
max(α p(y  |♦), µ max p(y  |x))
x∈Y
</equation>
<bodyText confidence="0.999812142857143">
where Y is the set of observed types. The second
probability is typically much smaller: an author is
unlikely to invent exactly the observed string y, cer-
tainly from ♦ but even by mutating a similar string
x (especially when the mutation rate µ is small).
How do we evaluate a type phylogeny? Con-
sider the probability of generating untagged tokens
</bodyText>
<footnote confidence="0.987271">
7Working over types improves the quality of our second ap-
proximation, and also speeds up the spanning tree algorithms.
§6 explains how to regard this approximation as variational EM.
</footnote>
<equation confidence="0.9864315">
=
p(y
(7)
s(y
x) = µ
p(y
x)
|♦)
α·
|♦)
|
·
|
·nx/(ny + 1) (8)
</equation>
<bodyText confidence="0.99479275">
of a given type parentage tree is
is
the product of individual edge weights defined by
Thus, we are again eligible to use the spanning tree
</bodyText>
<equation confidence="0.330806333333333">
edge-factored—it
s.
§5.3 below.
</equation>
<bodyText confidence="0.999384545454545">
But we do not actuall
assumption, our input corpus is only an unordered
bag of tokens. So we must treat the hidden order-
ing like any other hidden variable and maximize the
marginal likelihood, which sums (3) over all possi-
ble orderings (permutations). This sum can be re-
garded as the number of permutations N! (which is
fixed given the corpus) times the expectation of (3)
for a permutation chosen uniformly at random.
This leads to our second approximation. We ap-
proximate this expectation of the product (3) with a
</bodyText>
<equation confidence="0.89919">
s(y  |pa(y)) (6)
s(y
</equation>
<bodyText confidence="0.9996115">
and the constant of proportionality depends on the
corpus.
The above equations are analogous to those in
§5.1. Again, the approximate posterior probability
</bodyText>
<footnote confidence="0.70966925">
algorithms in
general this is an overe
8In
stimate for each phylogeny.
</footnote>
<page confidence="0.998798">
349
</page>
<bodyText confidence="0.999980333333333">
Notice that the ratio α/µ controls the preference
for an entity to descend from Q versus an existing
entity. Thus, by tuning this ratio, we can control
the number of entities inferred by our method, where
each entity corresponds to one of the child subtrees
of Q.
Also note that nx in the numerator of (8) means
that y&apos;s parent is more likely to be frequent. Also,
ny + 1 in the denominator means that a frequent y is
not as likely to have any parent x =� Q, because its
first token probably falls early in the sequence where
there are fewer available parents x =� Q.
</bodyText>
<subsectionHeader confidence="0.999549">
5.3 Spanning tree algorithms
</subsectionHeader>
<bodyText confidence="0.999976555555556">
Define a complete directed graph G over the vertices
Y U {Q}. The weight of an edge x —* y is defined
by δ(y  |x). The (approximate) posterior probability
of a given phylogeny given our evidence, is propor-
tional to the product of the δ values of its edges.
Formally, let T♦(G) denote the set of spanning
trees of G rooted at Q, and define the weight of a
particular spanning tree T E T♦(G) to be the prod-
uct of the weights of its edges:
</bodyText>
<equation confidence="0.9980685">
w(T) = fl δ(y  |x) (9)
(x→y)ET
</equation>
<bodyText confidence="0.687921">
Then the posterior probability of spanning tree T is
</bodyText>
<equation confidence="0.993115">
pθ (T) = Z(G) (10)
</equation>
<bodyText confidence="0.999873571428571">
where Z(G) = ETET♦(G) w(T) is the partition
function, i.e. the total probability of generating the
data G via any spanning tree of the form we consider.
This distribution is determined by the parameters θ
of the transducer pθ, along with the ratio α/µ.
There exist several algorithms to find the sin-
gle maximum-probability spanning tree, notably Tar-
jan&apos;s implementation of the Chu-Liu-Edmonds algo-
rithm, which runs in O(m log n) for a sparse graph
or O(n2) for a dense graph (Tarjan, 1977). Figure 1
shows a spanning tree found by our model using Tar-
jan&apos;s algorithm. Here n is the number of vertices
(in our case, types and o), while m is the number of
edges (which we can keep small by pruning, §6.1).
</bodyText>
<sectionHeader confidence="0.988637" genericHeader="method">
6 Training the Transducer with EM
</sectionHeader>
<bodyText confidence="0.999761958333333">
Our inference algorithm assumes that we know the
transducer parameters θ. We now explain how to op-
timize θ to maximize the marginal likelihood of the
training data. This marginal likelihood sums over all
the other latent variables in the model—the spanning
tree, the alignments between strings, and the hidden
token ordering.
The EM procedure repeats the following until con-
vergence:
E-step: Given θ, compute the posterior marginal
probabilities cxy of all possible phylogeny
edges.
M-step Given all cxy, retrain θ to assign a high
conditional probability to the mutations on the
probable edges.
We actually use a variational EM algorithm: our
E step approximates the true distribution q over all
phylogenies with the closest distribution p that as-
signs positive probability only to type-based phylo-
genies. This distribution is given by (10) and min-
imizes KL(p  ||q). We argued in section §5.2 that
it should be a good approximation. The posterior
marginal probability of a directed edge from vertex
x to vertex y, according to (10), is
</bodyText>
<equation confidence="0.988676">
cxy = E pθ(T) (11)
TET♦(G)�(x→y)ET
</equation>
<bodyText confidence="0.984277769230769">
The probability cxy is a “pseudocount” for the ex-
pected number of mutations from x to y. This is at
most 1 under our assumptions.
Calculating cxy requires summing over all span-
ning trees of G, of which there are nn−2 for a fully
connected graph with n vertices. Fortunately, Tutte
(1984) shows how to compute this sum by the fol-
lowing method, which extends Kirchhoff&apos;s classi-
cal matrix-tree theorem to weighted directed graphs.
This result has previously been employed in non-
projective dependency parsing (Koo et al., 2007;
Smith and Smith, 2007).
Let L E Rnxn denote the Laplacian of G, namely
</bodyText>
<equation confidence="0.9976055">
L= �Ex, δ(y  |x&apos;) if x = y (12)
−δ(y  |x) if x =� y
</equation>
<bodyText confidence="0.999344333333333">
Tutte&apos;s theorem relates the determinant of the Lapla-
cian to the spanning trees in graph G. In particular,
the cofactor L0,0 is equal to the sum of the weights
</bodyText>
<page confidence="0.992607">
350
</page>
<bodyText confidence="0.9981974">
of all directed spanning trees rooted at 0, which (sup-
posing ♦ is indexed at 0) yields the partition function
Z(G).
The edge marginals of interest are related to the
log partition function by
</bodyText>
<equation confidence="0.942557777777778">
cxy = ∂Z(G) (13)
∂δ(y  |x)
which has the closed-form solution
cxy = f s(y  |♦)Lyy if x = y (14)
11 δ(
y  |x) (L−&apos;−
=6
Ley) if x
y
</equation>
<bodyText confidence="0.999889055555556">
Thus, the problem of computing edge marginals re-
duces to that of computing a matrix inverse, which
may be done in O(n3) time.
At the M step, we retrain the mutation model pa-
rameters θ to maximize Exy cxy log p(wy  |wx).
This is tantamount to maximum conditional likeli-
hood training on a supervised collection of (wx, wy)
pairs that are respectively weighted by cxy.
The M step is nontrivial because the term p(wy |
wx) sums over a hidden alignment between two
strings. It may be performed by an inner loop of EM,
where the E step uses dynamic programming to ef-
ficiently consider all possible alignments, as in (Ris-
tad and Yianilos, 1996). In practice, we have found it
effective to take only a single step of this inner loop.
Such a Generalized EM procedure enjoys the same
convergence properties as EM, but may reach a local
optimum faster (Dempster et al., 1977).
</bodyText>
<subsectionHeader confidence="0.999795">
6.1 Pruning the graph
</subsectionHeader>
<bodyText confidence="0.999965875">
For large graphs, it is essential to prune the number
of edges to avoid considering all n(n − 1) input-
output pairs. To prune the graph, we eliminate all
edges between strings that do not share any common
trigrams (case- and diacritic-insensitive), by setting
their matrix entries to 0. As a result, the graph Lapla-
cian is a sparse matrix, which often allows faster
matrix inversion using preconditioned iterative algo-
rithms. Furthermore, pruned edges do not appear in
any spanning tree, so the E step will find that their
posterior marginal probabilities are 0. This means
that the input-output pairs corresponding to these
edges can be ignored when re-estimating the trans-
ducer parameters in the M step. We found that prun-
ing significantly improves training time with no ap-
preciable loss in performance.9
</bodyText>
<subsectionHeader confidence="0.999897">
6.2 Training with unobserved tokens?
</subsectionHeader>
<bodyText confidence="0.99994975">
A deficiency of our method is that it assumes that
authors of our corpus have only been exposed to pre-
vious tokens in our corpus. In principle, one could
also train with U additional tokens (e, w) where we
observe neither e nor w, for very large U. This is the
“universe of discourse” in which our authors oper-
ate.10 In this case, we would need (expensive) new
algorithms to reconstruct the strings w. However,
this model could infer a more realistic phylogeny by
positing unobserved ancestral or intermediate forms
that relate the observed tokens, as in transformation
models (Eisner, 2002; Andrews and Eisner, 2011).
</bodyText>
<sectionHeader confidence="0.98721" genericHeader="evaluation">
7 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.995313">
7.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.999865947368421">
Scraping Wikipedia. Wikipedia documents many
variant names for entities. As a result, it has fre-
quently been used as a source for mining name vari-
ations, both within and across languages (Parton et
al., 2008; Cucerzan, 2007). We used Wikipedia to
create a list of name aliases for different entities.
Specifically, we mined English Wikipedia11 for all
redirects: page names that lead directly to another
page. Redirects are created by Wikipedia users for
resolving common name variants to the correct page.
For example, the pages titled Barack Obama Ju-
nior and Barack Hussein Obama automatically redi-
rect to the page titled Barack Obama. This redirec-
tion implies that the first two are name variants of
the third. Collecting all such links within English
Wikipedia yields a large number of aliases for each
page. However, many redirects are for topics other
than individual people, and these would be poor ex-
amples of name variation. In addition, some phrases
</bodyText>
<footnote confidence="0.9891856">
9For instance, on a dataset of approximately 6000 distinct
names, pruning reduced the number of outgoing edges at each
vertex to fewer than 100 per vertex.
10Notice that the N observed tokens would be approximately
exchangeable in this setting: they are unlikely to depend on one
another when N « U, and hence their order no longer matters
much. In effect, generating the U hidden tokens constructs a rich
distribution (analogous to a sample from the Dirichlet process)
from which the N observed tokens are then sampled IID.
11Using a Wikipedia dump from February 2, 2011.
</footnote>
<page confidence="0.982583">
351
</page>
<note confidence="0.917795">
Ho Chi Minh, Ho chi mihn, Ho-Chi Minh, Ho Chih-minh
Guy Fawkes, Guy fawkes, Guy faux, Guy Falks, Guy Faukes, Guy Fawks, Guy foxe, Guy Falkes
Nicholas II of Russia, Nikolai Aleksandrovich Romanov, Nicholas Alexandrovich of Russia, Nicolas II
Bill Gates, Lord Billy, Bill Gates, BillGates, Billy Gates, William Gates III, William H. Gates
William Shakespeare, William shekspere, William shakspeare, Bill Shakespear
Bill Clinton, Billll Clinton, William Jefferson Blythe IV, Bill J. Clinton, William J Clinton
</note>
<figureCaption confidence="0.998663">
Figure 2: Sample alias lists scraped from Wikipedia. Note that only partial alias lists are shown for space reasons.
</figureCaption>
<bodyText confidence="0.990837891891892">
that redirect to an entity are descriptions rather than
names. For example, 44th President of the United
States also links to Barack Obama, but it is not a
name variant.
Freebase filtering. To improve data quality we used
Freebase, a structured knowledge base that incorpo-
rates information from Wikipedia. Among its struc-
tured information are entity types, including the type
“person.” We filtered the Wikipedia redirect col-
lection to remove pairs where the target page was
not listed as a person in Freebase. Additionally, to
remove redirects that were not proper names (44th
President of the United States), we applied a series
of rule based filters to remove bad aliases: removing
numerical names, parentheticals after names, quota-
tion marks, and names longer than 5 tokens, since
we found that these long names were rarely person
names (e.g. United States Ambassador to the Eu-
ropean Union, Success Through a Positive Mental
Attitude which links to the author Napoleon Hill.)
While not perfect, these modifications dramatically
improved quality. The result was a list of 78,079 dif-
ferent person entities, each with one or more known
names or aliases. Some typical names are shown in
Figure 2.
Estimating empirical type counts. Our method is
really intended to be run on a corpus of string to-
kens. However, for experimental purposes, we in-
stead use the above dataset of string types because
this allows us to use the “ground truth” given by
the Wikipedia redirects. To synthesize token counts,
empirical token frequencies for each type were esti-
mated from the LDC Gigaword corpus,12 which is
a corpus of newswire text spanning several years.
Wikipedia name types that did not appear in Giga-
word were assigned a “backoff count” of one. Note
that by virtue of the domain, many misspellings will
</bodyText>
<subsubsectionHeader confidence="0.189883">
12LDC Catalog No. LDC2003T05.
</subsubsectionHeader>
<bodyText confidence="0.984488333333333">
not appear; however, edges “popular” names (which
may be canonical names) will be assigned higher
weight.
</bodyText>
<subsectionHeader confidence="0.978273">
7.2 Experiments
</subsectionHeader>
<bodyText confidence="0.99998065625">
We begin by evaluating the generalization ability of a
transducer trained using a transformation model. To
do so, we measure log-likelihood on held-out entity
title and alias pairs. We then verify that the general-
ization ability according to log-likelihood translates
into gains for a name matching task. For the experi-
ments in this section, we use α = 0.9 and p = 0.1.13
Held-out log-likelihood. We construct pairs of en-
tity title (input) and alias (output) names from the
Wikipedia data. For different amounts of supervised
data, we trained the transformation model on the
training set, and plotted the log-likelihood of held-
out test data for the transducer parameters at each it-
eration of EM. The held-out test set is constructed
from a disjoint set of Wikipedia entities, the same
number of entities as in the training set. We used
different corpora of 1000 and 1500 entities for train
and test.
Name matching. For each alias a in a test set (not
seen at training time), we produce a ranking of test
entity titles t according to transducer probabilities
pe(a I t). A good transducer should assign high
probability to transformations from the correct ti-
tle for the alias. Mean reciprocal rank (MRR) is a
commonly used metric to estimate the quality of a
ranking, which we report in Figure 4. The reported
mean is over all aliases in the test data. In addition to
evaluating the ranking for different initializations of
our transducer, we compare to two baselines: Lev-
enshtein distance and Jaro-Winkler similarity. Jaro-
Winkler is a measure on strings that was specifically
designed for record linkage (Winkler, 1999). The
</bodyText>
<footnote confidence="0.650263">
13We did not find these parameters to be sensitive.
</footnote>
<page confidence="0.982673">
352
</page>
<figure confidence="0.999206151515152">
Held out log-likelihood
1500000 1 2 3 4 5 6 7 8 9
EM iteration
(a) 1000 entities.
sup=0
sup=5
sup=25
sup=100
sup=250
90000
100000
140000
110000
120000
130000
150000
160000
170000
Held out log-likelihood
180000
190000
200000
210000
220000
230000
sup=0
sup=5
sup=25
sup=100
sup=250
2400000 1 2 3 4 5 6 7 8 9
EM iteration
(b) 1500 entities.
</figure>
<figureCaption confidence="0.990233333333333">
Figure 3: Learning curves for different initializations of the transducer parameters. Above, “sup=100” (for instance)
means that 100 entities were used as training data to initialize the transducer parameters (constructing pairs between
all title-alias pairs for those Wikpedia entities).
</figureCaption>
<figure confidence="0.9997221">
0.85
0.80
0.75
jwink
lev
sup10
semi10
unsup
sup
1500
</figure>
<figureCaption confidence="0.974765">
Figure 4: Mean reciprocal rank (MRR) results for differ-
ent training conditions: “sup10” means that 10 entities
(roughly 40 name pairs) were used as training data for
the transducer; “semi10” means that the “sup10” model
was used as initialization before re-estimating the param-
eters using our model; “unsup” is the transducer trained
using our model without any initial supervision; “sup” is
trained on all 1500 entities in the training set (an upper
bound on performance); “jwink” and “lev” correspond to
Jaro-Winkler and Levenshtein distance baselines.
</figureCaption>
<bodyText confidence="0.999942333333333">
matching experiments were performed on a corpus
of 1500 entities (with separate corpora of the same
size for training and test).
</bodyText>
<sectionHeader confidence="0.996606" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999844682926829">
We have presented a new unsupervised method for
learning string-to-string transducers. It learns from
a collection of related strings whose relationships are
unknown. The key idea is that some strings are mu-
tations of common strings that occurred earlier. We
compute a distribution over the unknown phyloge-
netic tree that relates these strings, and use it to rees-
timate the transducer parameters via EM.
One direction for future work would be more so-
phisticated transduction models than the one we de-
veloped in §4. For names, this could include learn-
ing common nicknames (nonparametrically); explic-
itly modeling abbreviation processes such as initials;
conditioning on name components such as title and
middle name; and transliterating across languages.14
In other domains, one could model bibliographic en-
try propagation, derivational morphology, or histor-
ical sound change (again using language tags).
Another future direction would be to incorporate
the context of tokens in order to help reconstruct
which tokens are coreferent. For example, we might
extend the generative story to generate a context for
token (e, w) conditioned on e. Combining contex-
tual similarity with string similarity has previously
proved very useful for identifying cognates (Schafer
and Yarowsky, 2002; Schafer, 2006b; Bergsma and
Van Durme, 2011). In our setting it would help to
distinguish people with identical names, as well as
determining whether two people with similar names
are really the same.
14These last two points suggest that the mutation model
should operate not on simple (entity, string) pairs, but on richer
representations in which the name has been parsed into its com-
ponents (Eisenstein et al., 2011), labeled with a language ID,
and perhaps labeled with a phonological pronunciation. These
additional properties of a named entity may be either observed
or latent in training data. For example, if wy and fy denote the
string and language of name y, then define p(y  |x) = p(fy |
fx) · p(wy  |fy, fx, wx). The second factor captures translitera-
tion from language fx to language fy, e.g., by using §4&apos;s model
with an (fx, fy)-specific parameter setting.
</bodyText>
<figure confidence="0.9958325">
MRR
0.70
0.65
0.60
</figure>
<page confidence="0.997352">
353
</page>
<sectionHeader confidence="0.993614" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999955831858407">
Nicholas Andrews and Jason Eisner. 2011. Transformation pro-
cess priors. In NIPS 2011 Workshop on Bayesian Nonpara-
metrics: Hope or Hype?, Sierra Nevada, Spain, December.
Extended abstract (3 pages).
A. Bagga and B. Baldwin. 1998. Algorithms for scoring coref-
erence chains. In LREC.
Regina Barzilay and Lillian Lee. 2003. Learning to para-
phrase: an unsupervised approach using multiple-sequence
alignment. In Proc. of NAACL-HLT, pages 16–23, Strouds-
burg, PA, USA.
C. H. Bennett, M. Li, , and B. Ma. 2003. Chain letters
and evolutionary histories. Scienti�c American, 288(3):76–
81, June. More mathematical version available at http:
//www.cs.uwaterloo.ca/~mli/chain.html.
Shane Bergsma and Benjamin Van Durme. 2011. Learning
bilingual lexicons using the visual similarity of labeled web
images. In Proc. of IJCAI, pages 1764–1769, Barcelona,
Spain.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive
duplicate detection using learnable string similarity mea-
sures. In Proc. of ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD &apos;03, pages
39–48, New York, NY, USA. ACM.
Alexandre Bouchard-Côté, Percy Liang, Thomas Griffiths, and
Dan Klein. 2008. A probabilistic approach to language
change. In Proc. of NIPS, pages 169–176.
S. Cucerzan. 2007. Large-scale named entity disambiguation
based on Wikipedia data. In Proc. of EMNLP.
Aron Culotta, Michael Wick, Robert Hall, Matthew Marzilli,
and Andrew McCallum. 2007. Canonicalization of database
records using adaptive similarity measures. In Proc. of ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD &apos;07, pages 201–209.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society. Series B (Method-
ological), 39(1):1–38.
Z. Dias, A. Rocha, and S. Goldenstein. 2012. Image phy-
logeny by minimal spanning trees. IEEE Trans. on Informa-
tion Forensics and Security, 7(2):774–788, April.
Markus Dreyer and Jason Eisner. 2011. Discovering morpho-
logical paradigms from plain text using a Dirichlet process
mixture model. In Proc. of EMNLP, pages 616–627. Sup-
plementary material (9 pages) also available.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-
variable modeling of string transductions with finite-state
methods. In Proc. of EMNLP, pages 1080–1089, Honolulu,
Hawaii, October. Association for Computational Linguistics.
Jacob Eisenstein, Tae Yano, William Cohen, Noah Smith, and
Eric Xing. 2011. Structured databases of named entities
from Bayesian nonparametrics. In Proc. of the Firstworkshop
on Unsupervised Learning in NLP, pages 2–12, Edinburgh,
Scotland, July. Association for Computational Linguistics.
Jason Eisner. 2002. Transformational priors over grammars.
In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Philadelphia, July.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences—Computer Science and Computational Biology.
Cambridge University Press.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan
Klein. 2008. Learning bilingual lexicons from monolingual
corpora. In Proc. of ACL-08: HLT, pages 771–779.
David Hall and Dan Klein. 2010. Finding cognates using phylo-
genies. In Association for Computational Linguistics (ACL).
Rob Hall, Charles Sutton, and Andrew McCallum. 2008. Un-
supervised deduplication using cross-field dependencies. In
Proc. of the ACM SIGKDD International Conference On
Knowledge Discovery and Data Mining, KDD &apos;08, pages
310–317.
Md. Enamul. Karim, Andrew Walenstein, Arun Lakhotia, and
Laxmi Parida. 2005. Malware phylogeny generation using
permutations of code. Journal in Computer Virology, 1(1–
2):13–23.
Alexandre Klementiev and Dan Roth. 2006. Weakly supervised
named entity transliteration and discovery from multilingual
comparable corpora. In Proc. of COLING-ACL, pages 817–
824.
K. Knight and J. Graehl. 1998. Machine transliteration. Com-
putational Linguistics, 24:599–612.
Terry Koo, Amir Globerson, Xavier Carreras, and Michael
Collins. 2007. Structured prediction models via the matrix-
tree theorem. In Proc. of EMNLP-CoNLL, pages 141–150.
Jose Oncina and Marc Sebban. 2006. Using learned conditional
distributions as edit distance. In Proc. of the 2006 Joint IAPR
international Conference on Structural, Syntactic, and Statis-
tical Pattern Recognition, SSPR&apos;06/SPR&apos;06, pages 403–411.
Kristen Parton, Kathleen R. McKeown, James Allan, and En-
rique Henestroza. 2008. Simultaneous multilingual search
for translingual information retrieval. In Proceeding of the
ACM conference on Information and Knowledge Manage-
ment, CIKM &apos;08, pages 719–728.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learning string
edit distance. Technical Report CS-TR-532-96, Princeton
University, Department of Computer Science.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recognition and
Machine Intelligence, 20(5):522–532, May.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2011.
An algorithm for unsupervised transliteration mining with an
application to word alignment. In Proc. of ACL, pages 430–
439.
Charles Schafer and David Yarowsky. 2002. Inducing transla-
tion lexicons via diverse similarity measures and bridge lan-
guages. In Proc. of CONLL, pages 146–152.
Charles Schafer. 2006a. Novel probabilistic finite-state transduc-
ers for cognate and transliteration modeling. In 7th Biennial
Conference of the Association for Machine Translation in the
Americas (AMTA).
Charles Schafer. 2006b. Translation Discovery Using Diverse
Smilarity Measures. Ph.D. thesis, Johns Hopkins University.
David A. Smith and Noah A. Smith. 2007. Probabilistic mod-
els of nonprojective dependency trees. In Proc. of EMNLP-
CoNLL, pages 132–140.
</reference>
<page confidence="0.989656">
354
</page>
<reference confidence="0.996498555555556">
R. T. Smythe and H. M. Mahmoud. 1995. A survey of recur-
sive trees. Theory of Probability and Mathematical Statistics,
51(1-27).
Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A
statistical model for lost language decipherment. In Proc. of
ACL, pages 1048–1057.
Koichiro Tamura, Daniel Peterson, Nicholas Peterson, Glen
Stecher, Masatoshi Nei, and Sudhir Kumar. 2011. Mega5:
Molecular evolutionary genetics analysis using maximum
likelihood, evolutionary distance, and maximum parsimony
methods. Molecular Biology and Evolution, 28(10):2731–
2739.
R E Tarjan. 1977. Finding optimum branchings. Networks,
7(1):25-35.
W. Tutte. 1984. Graph Theory. Addison-Wesley.
William E. Winkler. 1999. The state of record linkage and cur-
rent research problems. Technical report, Statistical Research
Division, U.S. Census Bureau.
</reference>
<page confidence="0.999007">
355
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.602707">
<title confidence="0.999781">Name Phylogeny: A Generative Model of String Variation</title>
<author confidence="0.99953">Andrews Eisner</author>
<affiliation confidence="0.99974">Department of Computer Science and Human Language Technology Center of</affiliation>
<address confidence="0.8133815">Johns Hopkins 3400 N. Charles St., Baltimore, MD 21218</address>
<email confidence="0.999759">noa@jhu.edu</email>
<email confidence="0.999759">eisner@jhu.edu</email>
<email confidence="0.999759">mdredze@jhu.edu</email>
<abstract confidence="0.997725555555556">Many linguistic and textual processes involve transduction of strings. We show how to learn a stochastic transducer from an unorganized collection of strings (rather than string pairs). The role of the transducer is to organize the collection. Our generative model explains similarities among the strings by supposing that some strings the collection were not generated but were instead derived by transduction from other, “similar” strings in the collection. Our variational EM learning algorithm alternately reestimates this phylogeny and the transducer parameters. The final learned transducer can quickly link any test name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Andrews</author>
<author>Jason Eisner</author>
</authors>
<title>Transformation process priors.</title>
<date>2011</date>
<journal>Extended abstract</journal>
<booktitle>In NIPS 2011 Workshop on Bayesian Nonparametrics: Hope or Hype?,</booktitle>
<volume>3</volume>
<pages>pages).</pages>
<location>Sierra Nevada, Spain,</location>
<contexts>
<context position="14391" citStr="Andrews and Eisner, 2011" startWordPosition="2350" endWordPosition="2353">r distribution over sequences of named entities y = (e, w) is non-exchangeable. It assigns different probabilities to different orderings of the same tokens. This is because our model posits that later authors are influenced by earlier authors, copying entity names from them with mutation. So ordering is important. The mutation process is not symmetric—for example, Figure 1 reflects a tendency to shorten rather than lengthen names. Non-exchangeability is one way that our present model differs from (parametric) transformation models (Eisner, 2002) and (non-parametric) transformation processes (Andrews and Eisner, 2011). These too are defined using mutation of strings or other types. From a transformation process, one can draw a distribution over types, from which the tokens are then sampled IID. This results in an exchangeable sequence of tokens, just as in the Dirichlet process. We avoid transformation models here for three reasons. (1) Inference is more expensive. (2) A transformation process seems less realistic as a model of authorship. It constructs a distribution over derivational paths, similar to the paths in Figure 1. It effectively says that each token is generated by recapitulating some previousl</context>
<context position="32379" citStr="Andrews and Eisner, 2011" startWordPosition="5549" endWordPosition="5552">tokens? A deficiency of our method is that it assumes that authors of our corpus have only been exposed to previous tokens in our corpus. In principle, one could also train with U additional tokens (e, w) where we observe neither e nor w, for very large U. This is the “universe of discourse” in which our authors operate.10 In this case, we would need (expensive) new algorithms to reconstruct the strings w. However, this model could infer a more realistic phylogeny by positing unobserved ancestral or intermediate forms that relate the observed tokens, as in transformation models (Eisner, 2002; Andrews and Eisner, 2011). 7 Experimental Evaluation 7.1 Data preparation Scraping Wikipedia. Wikipedia documents many variant names for entities. As a result, it has frequently been used as a source for mining name variations, both within and across languages (Parton et al., 2008; Cucerzan, 2007). We used Wikipedia to create a list of name aliases for different entities. Specifically, we mined English Wikipedia11 for all redirects: page names that lead directly to another page. Redirects are created by Wikipedia users for resolving common name variants to the correct page. For example, the pages titled Barack Obama J</context>
</contexts>
<marker>Andrews, Eisner, 2011</marker>
<rawString>Nicholas Andrews and Jason Eisner. 2011. Transformation process priors. In NIPS 2011 Workshop on Bayesian Nonparametrics: Hope or Hype?, Sierra Nevada, Spain, December. Extended abstract (3 pages).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="1468" citStr="Bagga and Baldwin, 1998" startWordPosition="216" endWordPosition="219">y and the transducer parameters. The final learned transducer can quickly link any test name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y I x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morphological derivation, historical sound or spelling change, loanw</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for scoring coreference chains. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>16--23</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1552" citStr="Barzilay and Lee, 2003" startWordPosition="227" endWordPosition="230">st name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y I x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morphological derivation, historical sound or spelling change, loanword formation, translation, transliteration, editing, or transcription error. We ass</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiple-sequence alignment. In Proc. of NAACL-HLT, pages 16–23, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Bennett</author>
<author>M Li</author>
</authors>
<title>Chain letters and evolutionary histories.</title>
<date>2003</date>
<journal>Scienti�c American,</journal>
<volume>288</volume>
<issue>3</issue>
<note>More mathematical version available at http: //www.cs.uwaterloo.ca/~mli/chain.html.</note>
<marker>Bennett, Li, 2003</marker>
<rawString>C. H. Bennett, M. Li, , and B. Ma. 2003. Chain letters and evolutionary histories. Scienti�c American, 288(3):76– 81, June. More mathematical version available at http: //www.cs.uwaterloo.ca/~mli/chain.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Learning bilingual lexicons using the visual similarity of labeled web images.</title>
<date>2011</date>
<booktitle>In Proc. of IJCAI,</booktitle>
<pages>1764--1769</pages>
<location>Barcelona,</location>
<marker>Bergsma, Van Durme, 2011</marker>
<rawString>Shane Bergsma and Benjamin Van Durme. 2011. Learning bilingual lexicons using the visual similarity of labeled web images. In Proc. of IJCAI, pages 1764–1769, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Bilenko</author>
<author>Raymond J Mooney</author>
</authors>
<title>Adaptive duplicate detection using learnable string similarity measures.</title>
<date>2003</date>
<booktitle>In Proc. of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;03,</booktitle>
<pages>39--48</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="16827" citStr="Bilenko and Mooney, 2003" startWordPosition="2769" endWordPosition="2772">Model for Strings Our model in §3 samples the next token y, when it is not simply a faithful copy, from p(y x) or p(y Q) . The key step there is to sample the name string wy from p(wy wx) or p(wy Q) . Our model of these distributions could easily incorporate detailed linguistic knowledge of the mutation process (see §8). Here we describe the specific model that we use in our experiments. Like many such models, it can be regarded as a stochastic finitestate string-to-string transducer parameterized by 0. There is much prior work on stochastic models of edit distance (Ristad and Yianilos, 1998; Bilenko and Mooney, 2003; Oncina and Sebban, 2006; Schafer, 2006a; Bouchard-Côté et al., 2008; Dreyer et al., 2008, among others). For the present experiments, we designed a moderately simple one that employs (1) conditioning on one character of right context, (2) latent “edit” and “no-edit” regions to capture the fact that groups of edits are often made in close proximity, and (3) some simple special handling for the distribution conditioned on the root p(wy Q) . We assume a stochastic mutation process which, when given an input string wx, edits it from left to 4The very fact that x has been frequently observed demo</context>
</contexts>
<marker>Bilenko, Mooney, 2003</marker>
<rawString>Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive duplicate detection using learnable string similarity measures. In Proc. of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;03, pages 39–48, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Côté</author>
<author>Percy Liang</author>
<author>Thomas Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>A probabilistic approach to language change.</title>
<date>2008</date>
<booktitle>In Proc. of NIPS,</booktitle>
<pages>169--176</pages>
<contexts>
<context position="16896" citStr="Bouchard-Côté et al., 2008" startWordPosition="2779" endWordPosition="2782"> is not simply a faithful copy, from p(y x) or p(y Q) . The key step there is to sample the name string wy from p(wy wx) or p(wy Q) . Our model of these distributions could easily incorporate detailed linguistic knowledge of the mutation process (see §8). Here we describe the specific model that we use in our experiments. Like many such models, it can be regarded as a stochastic finitestate string-to-string transducer parameterized by 0. There is much prior work on stochastic models of edit distance (Ristad and Yianilos, 1998; Bilenko and Mooney, 2003; Oncina and Sebban, 2006; Schafer, 2006a; Bouchard-Côté et al., 2008; Dreyer et al., 2008, among others). For the present experiments, we designed a moderately simple one that employs (1) conditioning on one character of right context, (2) latent “edit” and “no-edit” regions to capture the fact that groups of edits are often made in close proximity, and (3) some simple special handling for the distribution conditioned on the root p(wy Q) . We assume a stochastic mutation process which, when given an input string wx, edits it from left to 4The very fact that x has been frequently observed demonstrates that it has often chosen to stop mutating. This implies that</context>
</contexts>
<marker>Bouchard-Côté, Liang, Griffiths, Klein, 2008</marker>
<rawString>Alexandre Bouchard-Côté, Percy Liang, Thomas Griffiths, and Dan Klein. 2008. A probabilistic approach to language change. In Proc. of NIPS, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="32652" citStr="Cucerzan, 2007" startWordPosition="5594" endWordPosition="5595">iscourse” in which our authors operate.10 In this case, we would need (expensive) new algorithms to reconstruct the strings w. However, this model could infer a more realistic phylogeny by positing unobserved ancestral or intermediate forms that relate the observed tokens, as in transformation models (Eisner, 2002; Andrews and Eisner, 2011). 7 Experimental Evaluation 7.1 Data preparation Scraping Wikipedia. Wikipedia documents many variant names for entities. As a result, it has frequently been used as a source for mining name variations, both within and across languages (Parton et al., 2008; Cucerzan, 2007). We used Wikipedia to create a list of name aliases for different entities. Specifically, we mined English Wikipedia11 for all redirects: page names that lead directly to another page. Redirects are created by Wikipedia users for resolving common name variants to the correct page. For example, the pages titled Barack Obama Junior and Barack Hussein Obama automatically redirect to the page titled Barack Obama. This redirection implies that the first two are name variants of the third. Collecting all such links within English Wikipedia yields a large number of aliases for each page. However, ma</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>S. Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Robert Hall</author>
<author>Matthew Marzilli</author>
<author>Andrew McCallum</author>
</authors>
<title>Canonicalization of database records using adaptive similarity measures.</title>
<date>2007</date>
<booktitle>In Proc. of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;07,</booktitle>
<pages>201--209</pages>
<contexts>
<context position="1509" citStr="Culotta et al., 2007" startWordPosition="221" endWordPosition="224">earned transducer can quickly link any test name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y I x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morphological derivation, historical sound or spelling change, loanword formation, translation, transliterati</context>
</contexts>
<marker>Culotta, Wick, Hall, Marzilli, McCallum, 2007</marker>
<rawString>Aron Culotta, Michael Wick, Robert Hall, Matthew Marzilli, and Andrew McCallum. 2007. Canonicalization of database records using adaptive similarity measures. In Proc. of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;07, pages 201–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="30895" citStr="Dempster et al., 1977" startWordPosition="5302" endWordPosition="5305">maximum conditional likelihood training on a supervised collection of (wx, wy) pairs that are respectively weighted by cxy. The M step is nontrivial because the term p(wy | wx) sums over a hidden alignment between two strings. It may be performed by an inner loop of EM, where the E step uses dynamic programming to efficiently consider all possible alignments, as in (Ristad and Yianilos, 1996). In practice, we have found it effective to take only a single step of this inner loop. Such a Generalized EM procedure enjoys the same convergence properties as EM, but may reach a local optimum faster (Dempster et al., 1977). 6.1 Pruning the graph For large graphs, it is essential to prune the number of edges to avoid considering all n(n − 1) inputoutput pairs. To prune the graph, we eliminate all edges between strings that do not share any common trigrams (case- and diacritic-insensitive), by setting their matrix entries to 0. As a result, the graph Laplacian is a sparse matrix, which often allows faster matrix inversion using preconditioned iterative algorithms. Furthermore, pruned edges do not appear in any spanning tree, so the E step will find that their posterior marginal probabilities are 0. This means tha</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Dias</author>
<author>A Rocha</author>
<author>S Goldenstein</author>
</authors>
<title>Image phylogeny by minimal spanning trees.</title>
<date>2012</date>
<journal>IEEE Trans. on Information Forensics and Security,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="7492" citStr="Dias et al., 2012" startWordPosition="1185" endWordPosition="1188">a idiosyncratic phylogenetic tree whose nodes are the string types or tokens themselves. Names and words are not the only non-biological objects that are copied with mutation. Documents, database records, bibliographic entries, code, and images can evolve in the same way. Reconstructing these relationships has been considered by a number of papers on authorship attribution, near-duplicate detection, deduplication, record linkage, and plagiarism detection. A few such papers reconstruct a phylogeny, as in the case of chain letters (Bennett et al., 2003), malware (Karim et al., 2005), or images (Dias et al., 2012). In fact, the last of these uses the same minimum spanning tree method that we apply in §5.3. However, these papers do not train a similarity measure as we do. To our knowledge, these two techniques have not been combined outside biology. In molecular evolutionary analysis, phylogenetic techniques have often been combined with estimation of some parametric model of mutation (Tamura et al., 2011). However, names mutate differently from biological sequences, and our mutation model for names (§4, §8) reflects that. We also posit a specific process (§3) that generates the name phylogeny. 2 An Exa</context>
</contexts>
<marker>Dias, Rocha, Goldenstein, 2012</marker>
<rawString>Z. Dias, A. Rocha, and S. Goldenstein. 2012. Image phylogeny by minimal spanning trees. IEEE Trans. on Information Forensics and Security, 7(2):774–788, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering morphological paradigms from plain text using a Dirichlet process mixture model.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>616--627</pages>
<note>Supplementary material (9 pages) also available.</note>
<contexts>
<context position="1403" citStr="Dreyer and Eisner, 2011" startWordPosition="208" endWordPosition="211">ional EM learning algorithm alternately reestimates this phylogeny and the transducer parameters. The final learned transducer can quickly link any test name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y I x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, mor</context>
<context position="5276" citStr="Dreyer and Eisner (2011)" startWordPosition="830" endWordPosition="833"> 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) organize words into morphological paradigms of a given structure. Again words with the same underlying lexeme (i.e., morphemes) must be identified. A lexeme can be realized in each grammatical inflection (such as “first person plural present”) by exactly one word type, related to other inflected forms of the same lexeme, which as above may be unobserved. Their inference setting is closer to ours because the input is an unorganized collection of words—input words are not tagged with their grammatical inflections. This contrasts with the usual multilingual setting where each word is tagged with</context>
</contexts>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a Dirichlet process mixture model. In Proc. of EMNLP, pages 616–627. Supplementary material (9 pages) also available.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Latentvariable modeling of string transductions with finite-state methods.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1080--1089</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1799" citStr="Dreyer et al., 2008" startWordPosition="264" endWordPosition="267">h as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y I x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morphological derivation, historical sound or spelling change, loanword formation, translation, transliteration, editing, or transcription error. We assume that each string was derived from at most one parent, but may give rise to any number of children. The difficulty is that most or all of these parentchild relationships are unobserved. We must reconstruct this evolutionary phylogeny. At the sa</context>
<context position="16917" citStr="Dreyer et al., 2008" startWordPosition="2783" endWordPosition="2786">py, from p(y x) or p(y Q) . The key step there is to sample the name string wy from p(wy wx) or p(wy Q) . Our model of these distributions could easily incorporate detailed linguistic knowledge of the mutation process (see §8). Here we describe the specific model that we use in our experiments. Like many such models, it can be regarded as a stochastic finitestate string-to-string transducer parameterized by 0. There is much prior work on stochastic models of edit distance (Ristad and Yianilos, 1998; Bilenko and Mooney, 2003; Oncina and Sebban, 2006; Schafer, 2006a; Bouchard-Côté et al., 2008; Dreyer et al., 2008, among others). For the present experiments, we designed a moderately simple one that employs (1) conditioning on one character of right context, (2) latent “edit” and “no-edit” regions to capture the fact that groups of edits are often made in close proximity, and (3) some simple special handling for the distribution conditioned on the root p(wy Q) . We assume a stochastic mutation process which, when given an input string wx, edits it from left to 4The very fact that x has been frequently observed demonstrates that it has often chosen to stop mutating. This implies that it is likely to choo</context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latentvariable modeling of string transductions with finite-state methods. In Proc. of EMNLP, pages 1080–1089, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Tae Yano</author>
<author>William Cohen</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>Structured databases of named entities from Bayesian nonparametrics.</title>
<date>2011</date>
<booktitle>In Proc. of the Firstworkshop on Unsupervised Learning in NLP,</booktitle>
<pages>2--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Eisenstein, Yano, Cohen, Smith, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Tae Yano, William Cohen, Noah Smith, and Eric Xing. 2011. Structured databases of named entities from Bayesian nonparametrics. In Proc. of the Firstworkshop on Unsupervised Learning in NLP, pages 2–12, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Transformational priors over grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Philadelphia,</location>
<contexts>
<context position="14318" citStr="Eisner, 2002" startWordPosition="2343" endWordPosition="2344">ion over sequences of entities e is exchangeable. However, our distribution over sequences of named entities y = (e, w) is non-exchangeable. It assigns different probabilities to different orderings of the same tokens. This is because our model posits that later authors are influenced by earlier authors, copying entity names from them with mutation. So ordering is important. The mutation process is not symmetric—for example, Figure 1 reflects a tendency to shorten rather than lengthen names. Non-exchangeability is one way that our present model differs from (parametric) transformation models (Eisner, 2002) and (non-parametric) transformation processes (Andrews and Eisner, 2011). These too are defined using mutation of strings or other types. From a transformation process, one can draw a distribution over types, from which the tokens are then sampled IID. This results in an exchangeable sequence of tokens, just as in the Dirichlet process. We avoid transformation models here for three reasons. (1) Inference is more expensive. (2) A transformation process seems less realistic as a model of authorship. It constructs a distribution over derivational paths, similar to the paths in Figure 1. It effec</context>
<context position="32352" citStr="Eisner, 2002" startWordPosition="5547" endWordPosition="5548">th unobserved tokens? A deficiency of our method is that it assumes that authors of our corpus have only been exposed to previous tokens in our corpus. In principle, one could also train with U additional tokens (e, w) where we observe neither e nor w, for very large U. This is the “universe of discourse” in which our authors operate.10 In this case, we would need (expensive) new algorithms to reconstruct the strings w. However, this model could infer a more realistic phylogeny by positing unobserved ancestral or intermediate forms that relate the observed tokens, as in transformation models (Eisner, 2002; Andrews and Eisner, 2011). 7 Experimental Evaluation 7.1 Data preparation Scraping Wikipedia. Wikipedia documents many variant names for entities. As a result, it has frequently been used as a source for mining name variations, both within and across languages (Parton et al., 2008; Cucerzan, 2007). We used Wikipedia to create a list of name aliases for different entities. Specifically, we mined English Wikipedia11 for all redirects: page names that lead directly to another page. Redirects are created by Wikipedia users for resolving common name variants to the correct page. For example, the </context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. Transformational priors over grammars. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees, and Sequences—Computer Science and Computational Biology.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="19906" citStr="Gusfield, 1997" startWordPosition="3290" endWordPosition="3291">nerated by a sequence of insertions followed by a copy. These are conditioned as usual on the next character, here Eos&apos;, so the model can learn to insert more or different characters when the input is Q. The parameters 0 determining the conditional probabilities of the different operations and characters are estimated with backoff smoothing. 5 Inference The input to inference is a collection of named entity tokens y. Most are untagged tokens of the form y = (?, w). In a semi-supervised setting, however, some 6This somewhat resembles the traditional affine gap penalty in computational biology (Gusfield, 1997), which makes deletions or insertions cheaper if they are consecutive. We instead make consecutive edits cheaper regardless of the edit type. of the tokens may be tagged tokens of the form y = (e, w), whose true entity is known. The entity tags place a constraint on the phylogeny, since each child subtree of Q must correspond to exactly one entity. 5.1 An unrealistically supervised setting Suppose we were lucky enough to fully observe the sequence of named entity tokens yi = (ei, wi) produced by our generative model. That is, suppose all tokens were tagged and we knew their ordering. Yet there</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees, and Sequences—Computer Science and Computational Biology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08: HLT,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="4680" citStr="Haghighi et al., 2008" startWordPosition="733" endWordPosition="736">lso considered learning transducers or other models of word pairs when 344 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 344–355, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) org</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proc. of ACL-08: HLT, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Dan Klein</author>
</authors>
<title>Finding cognates using phylogenies.</title>
<date>2010</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4746" citStr="Hall and Klein (2010)" startWordPosition="745" endWordPosition="748">when 344 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 344–355, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) organize words into morphological paradigms of a given structure. Aga</context>
</contexts>
<marker>Hall, Klein, 2010</marker>
<rawString>David Hall and Dan Klein. 2010. Finding cognates using phylogenies. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Hall</author>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Unsupervised deduplication using cross-field dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of the ACM SIGKDD International Conference On Knowledge Discovery and Data Mining, KDD &apos;08,</booktitle>
<pages>310--317</pages>
<contexts>
<context position="13504" citStr="Hall et al. (2008)" startWordPosition="2214" endWordPosition="2217">e 14 for further discussion of enriched tokens. 346 3.1 Relationship to other models If we ignore the name strings, we can see that the sequence of entities e1, e2,... eN is being generated from a Chinese restaurant process (CRP) with concentration parameter α. To the extent that α is low (so that o is rarely used), a few randomly chosen entities will dominate the corpus. The CRP is equivalent to sampling e1, e2,... IID from an unknown distribution that was itself drawn from a Dirichlet process with concentration α. This is indeed a standard model of a distribution over entities. For example, Hall et al. (2008) use it to model venues in bibliographic entries. From this characterization of the CRP, one can see that any permutation of this entity sequence would have the same probability. That is, our distribution over sequences of entities e is exchangeable. However, our distribution over sequences of named entities y = (e, w) is non-exchangeable. It assigns different probabilities to different orderings of the same tokens. This is because our model posits that later authors are influenced by earlier authors, copying entity names from them with mutation. So ordering is important. The mutation process </context>
</contexts>
<marker>Hall, Sutton, McCallum, 2008</marker>
<rawString>Rob Hall, Charles Sutton, and Andrew McCallum. 2008. Unsupervised deduplication using cross-field dependencies. In Proc. of the ACM SIGKDD International Conference On Knowledge Discovery and Data Mining, KDD &apos;08, pages 310–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enamul Karim</author>
<author>Andrew Walenstein</author>
<author>Arun Lakhotia</author>
<author>Laxmi Parida</author>
</authors>
<title>Malware phylogeny generation using permutations of code.</title>
<date>2005</date>
<journal>Journal in Computer Virology,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>2--13</pages>
<contexts>
<context position="7461" citStr="Karim et al., 2005" startWordPosition="1179" endWordPosition="1182">n our mutation model. them into a idiosyncratic phylogenetic tree whose nodes are the string types or tokens themselves. Names and words are not the only non-biological objects that are copied with mutation. Documents, database records, bibliographic entries, code, and images can evolve in the same way. Reconstructing these relationships has been considered by a number of papers on authorship attribution, near-duplicate detection, deduplication, record linkage, and plagiarism detection. A few such papers reconstruct a phylogeny, as in the case of chain letters (Bennett et al., 2003), malware (Karim et al., 2005), or images (Dias et al., 2012). In fact, the last of these uses the same minimum spanning tree method that we apply in §5.3. However, these papers do not train a similarity measure as we do. To our knowledge, these two techniques have not been combined outside biology. In molecular evolutionary analysis, phylogenetic techniques have often been combined with estimation of some parametric model of mutation (Tamura et al., 2011). However, names mutate differently from biological sequences, and our mutation model for names (§4, §8) reflects that. We also posit a specific process (§3) that generat</context>
</contexts>
<marker>Karim, Walenstein, Lakhotia, Parida, 2005</marker>
<rawString>Md. Enamul. Karim, Andrew Walenstein, Arun Lakhotia, and Laxmi Parida. 2005. Malware phylogeny generation using permutations of code. Journal in Computer Virology, 1(1– 2):13–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Weakly supervised named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>817--824</pages>
<contexts>
<context position="4657" citStr="Klementiev and Roth, 2006" startWordPosition="729" endWordPosition="732">eral previous papers have also considered learning transducers or other models of word pairs when 344 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 344–355, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreye</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. In Proc. of COLING-ACL, pages 817– 824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--599</pages>
<contexts>
<context position="1365" citStr="Knight and Graehl, 1998" startWordPosition="203" endWordPosition="206"> strings in the collection. Our variational EM learning algorithm alternately reestimates this phylogeny and the transducer parameters. The final learned transducer can quickly link any test name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y I x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguist</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>K. Knight and J. Graehl. 1998. Machine transliteration. Computational Linguistics, 24:599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Amir Globerson</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Structured prediction models via the matrixtree theorem.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="29427" citStr="Koo et al., 2007" startWordPosition="5023" endWordPosition="5026">cted edge from vertex x to vertex y, according to (10), is cxy = E pθ(T) (11) TET♦(G)�(x→y)ET The probability cxy is a “pseudocount” for the expected number of mutations from x to y. This is at most 1 under our assumptions. Calculating cxy requires summing over all spanning trees of G, of which there are nn−2 for a fully connected graph with n vertices. Fortunately, Tutte (1984) shows how to compute this sum by the following method, which extends Kirchhoff&apos;s classical matrix-tree theorem to weighted directed graphs. This result has previously been employed in nonprojective dependency parsing (Koo et al., 2007; Smith and Smith, 2007). Let L E Rnxn denote the Laplacian of G, namely L= �Ex, δ(y |x&apos;) if x = y (12) −δ(y |x) if x =� y Tutte&apos;s theorem relates the determinant of the Laplacian to the spanning trees in graph G. In particular, the cofactor L0,0 is equal to the sum of the weights 350 of all directed spanning trees rooted at 0, which (supposing ♦ is indexed at 0) yields the partition function Z(G). The edge marginals of interest are related to the log partition function by cxy = ∂Z(G) (13) ∂δ(y |x) which has the closed-form solution cxy = f s(y |♦)Lyy if x = y (14) 11 δ( y |x) (L−&apos;− =6 Ley) if</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>Terry Koo, Amir Globerson, Xavier Carreras, and Michael Collins. 2007. Structured prediction models via the matrixtree theorem. In Proc. of EMNLP-CoNLL, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose Oncina</author>
<author>Marc Sebban</author>
</authors>
<title>Using learned conditional distributions as edit distance.</title>
<date>2006</date>
<booktitle>In Proc. of the 2006 Joint IAPR international Conference on Structural, Syntactic, and Statistical Pattern Recognition, SSPR&apos;06/SPR&apos;06,</booktitle>
<pages>403--411</pages>
<contexts>
<context position="16852" citStr="Oncina and Sebban, 2006" startWordPosition="2773" endWordPosition="2776">l in §3 samples the next token y, when it is not simply a faithful copy, from p(y x) or p(y Q) . The key step there is to sample the name string wy from p(wy wx) or p(wy Q) . Our model of these distributions could easily incorporate detailed linguistic knowledge of the mutation process (see §8). Here we describe the specific model that we use in our experiments. Like many such models, it can be regarded as a stochastic finitestate string-to-string transducer parameterized by 0. There is much prior work on stochastic models of edit distance (Ristad and Yianilos, 1998; Bilenko and Mooney, 2003; Oncina and Sebban, 2006; Schafer, 2006a; Bouchard-Côté et al., 2008; Dreyer et al., 2008, among others). For the present experiments, we designed a moderately simple one that employs (1) conditioning on one character of right context, (2) latent “edit” and “no-edit” regions to capture the fact that groups of edits are often made in close proximity, and (3) some simple special handling for the distribution conditioned on the root p(wy Q) . We assume a stochastic mutation process which, when given an input string wx, edits it from left to 4The very fact that x has been frequently observed demonstrates that it has ofte</context>
</contexts>
<marker>Oncina, Sebban, 2006</marker>
<rawString>Jose Oncina and Marc Sebban. 2006. Using learned conditional distributions as edit distance. In Proc. of the 2006 Joint IAPR international Conference on Structural, Syntactic, and Statistical Pattern Recognition, SSPR&apos;06/SPR&apos;06, pages 403–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Parton</author>
<author>Kathleen R McKeown</author>
<author>James Allan</author>
<author>Enrique Henestroza</author>
</authors>
<title>Simultaneous multilingual search for translingual information retrieval.</title>
<date>2008</date>
<booktitle>In Proceeding of the ACM conference on Information and Knowledge Management, CIKM &apos;08,</booktitle>
<pages>719--728</pages>
<contexts>
<context position="32635" citStr="Parton et al., 2008" startWordPosition="5590" endWordPosition="5593">is the “universe of discourse” in which our authors operate.10 In this case, we would need (expensive) new algorithms to reconstruct the strings w. However, this model could infer a more realistic phylogeny by positing unobserved ancestral or intermediate forms that relate the observed tokens, as in transformation models (Eisner, 2002; Andrews and Eisner, 2011). 7 Experimental Evaluation 7.1 Data preparation Scraping Wikipedia. Wikipedia documents many variant names for entities. As a result, it has frequently been used as a source for mining name variations, both within and across languages (Parton et al., 2008; Cucerzan, 2007). We used Wikipedia to create a list of name aliases for different entities. Specifically, we mined English Wikipedia11 for all redirects: page names that lead directly to another page. Redirects are created by Wikipedia users for resolving common name variants to the correct page. For example, the pages titled Barack Obama Junior and Barack Hussein Obama automatically redirect to the page titled Barack Obama. This redirection implies that the first two are name variants of the third. Collecting all such links within English Wikipedia yields a large number of aliases for each </context>
</contexts>
<marker>Parton, McKeown, Allan, Henestroza, 2008</marker>
<rawString>Kristen Parton, Kathleen R. McKeown, James Allan, and Enrique Henestroza. 2008. Simultaneous multilingual search for translingual information retrieval. In Proceeding of the ACM conference on Information and Knowledge Management, CIKM &apos;08, pages 719–728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string edit distance.</title>
<date>1996</date>
<tech>Technical Report CS-TR-532-96,</tech>
<institution>Princeton University, Department of Computer Science.</institution>
<contexts>
<context position="30668" citStr="Ristad and Yianilos, 1996" startWordPosition="5261" endWordPosition="5265">problem of computing edge marginals reduces to that of computing a matrix inverse, which may be done in O(n3) time. At the M step, we retrain the mutation model parameters θ to maximize Exy cxy log p(wy |wx). This is tantamount to maximum conditional likelihood training on a supervised collection of (wx, wy) pairs that are respectively weighted by cxy. The M step is nontrivial because the term p(wy | wx) sums over a hidden alignment between two strings. It may be performed by an inner loop of EM, where the E step uses dynamic programming to efficiently consider all possible alignments, as in (Ristad and Yianilos, 1996). In practice, we have found it effective to take only a single step of this inner loop. Such a Generalized EM procedure enjoys the same convergence properties as EM, but may reach a local optimum faster (Dempster et al., 1977). 6.1 Pruning the graph For large graphs, it is essential to prune the number of edges to avoid considering all n(n − 1) inputoutput pairs. To prune the graph, we eliminate all edges between strings that do not share any common trigrams (case- and diacritic-insensitive), by setting their matrix entries to 0. As a result, the graph Laplacian is a sparse matrix, which ofte</context>
</contexts>
<marker>Ristad, Yianilos, 1996</marker>
<rawString>Eric Sven Ristad and Peter N. Yianilos. 1996. Learning string edit distance. Technical Report CS-TR-532-96, Princeton University, Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Recognition and Machine Intelligence,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="16801" citStr="Ristad and Yianilos, 1998" startWordPosition="2765" endWordPosition="2768">beled with Q. 4 A Mutation Model for Strings Our model in §3 samples the next token y, when it is not simply a faithful copy, from p(y x) or p(y Q) . The key step there is to sample the name string wy from p(wy wx) or p(wy Q) . Our model of these distributions could easily incorporate detailed linguistic knowledge of the mutation process (see §8). Here we describe the specific model that we use in our experiments. Like many such models, it can be regarded as a stochastic finitestate string-to-string transducer parameterized by 0. There is much prior work on stochastic models of edit distance (Ristad and Yianilos, 1998; Bilenko and Mooney, 2003; Oncina and Sebban, 2006; Schafer, 2006a; Bouchard-Côté et al., 2008; Dreyer et al., 2008, among others). For the present experiments, we designed a moderately simple one that employs (1) conditioning on one character of right context, (2) latent “edit” and “no-edit” regions to capture the fact that groups of edits are often made in close proximity, and (3) some simple special handling for the distribution conditioned on the root p(wy Q) . We assume a stochastic mutation process which, when given an input string wx, edits it from left to 4The very fact that x has bee</context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string edit distance. IEEE Transactions on Pattern Recognition and Machine Intelligence, 20(5):522–532, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sajjad</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
</authors>
<title>An algorithm for unsupervised transliteration mining with an application to word alignment.</title>
<date>2011</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>430--439</pages>
<contexts>
<context position="4723" citStr="Sajjad et al., 2011" startWordPosition="741" endWordPosition="744"> models of word pairs when 344 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 344–355, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) organize words into morphological paradigms of</context>
</contexts>
<marker>Sajjad, Fraser, Schmid, 2011</marker>
<rawString>Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2011. An algorithm for unsupervised transliteration mining with an application to word alignment. In Proc. of ACL, pages 430– 439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages.</title>
<date>2002</date>
<booktitle>In Proc. of CONLL,</booktitle>
<pages>146--152</pages>
<contexts>
<context position="40891" citStr="Schafer and Yarowsky, 2002" startWordPosition="6920" endWordPosition="6923">nditioning on name components such as title and middle name; and transliterating across languages.14 In other domains, one could model bibliographic entry propagation, derivational morphology, or historical sound change (again using language tags). Another future direction would be to incorporate the context of tokens in order to help reconstruct which tokens are coreferent. For example, we might extend the generative story to generate a context for token (e, w) conditioned on e. Combining contextual similarity with string similarity has previously proved very useful for identifying cognates (Schafer and Yarowsky, 2002; Schafer, 2006b; Bergsma and Van Durme, 2011). In our setting it would help to distinguish people with identical names, as well as determining whether two people with similar names are really the same. 14These last two points suggest that the mutation model should operate not on simple (entity, string) pairs, but on richer representations in which the name has been parsed into its components (Eisenstein et al., 2011), labeled with a language ID, and perhaps labeled with a phonological pronunciation. These additional properties of a named entity may be either observed or latent in training dat</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In Proc. of CONLL, pages 146–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
</authors>
<title>Novel probabilistic finite-state transducers for cognate and transliteration modeling.</title>
<date>2006</date>
<booktitle>In 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<contexts>
<context position="4629" citStr="Schafer, 2006" startWordPosition="727" endWordPosition="728">Related Work Several previous papers have also considered learning transducers or other models of word pairs when 344 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 344–355, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ances</context>
<context position="16867" citStr="Schafer, 2006" startWordPosition="2777" endWordPosition="2778">token y, when it is not simply a faithful copy, from p(y x) or p(y Q) . The key step there is to sample the name string wy from p(wy wx) or p(wy Q) . Our model of these distributions could easily incorporate detailed linguistic knowledge of the mutation process (see §8). Here we describe the specific model that we use in our experiments. Like many such models, it can be regarded as a stochastic finitestate string-to-string transducer parameterized by 0. There is much prior work on stochastic models of edit distance (Ristad and Yianilos, 1998; Bilenko and Mooney, 2003; Oncina and Sebban, 2006; Schafer, 2006a; Bouchard-Côté et al., 2008; Dreyer et al., 2008, among others). For the present experiments, we designed a moderately simple one that employs (1) conditioning on one character of right context, (2) latent “edit” and “no-edit” regions to capture the fact that groups of edits are often made in close proximity, and (3) some simple special handling for the distribution conditioned on the root p(wy Q) . We assume a stochastic mutation process which, when given an input string wx, edits it from left to 4The very fact that x has been frequently observed demonstrates that it has often chosen to sto</context>
<context position="40906" citStr="Schafer, 2006" startWordPosition="6924" endWordPosition="6925">s such as title and middle name; and transliterating across languages.14 In other domains, one could model bibliographic entry propagation, derivational morphology, or historical sound change (again using language tags). Another future direction would be to incorporate the context of tokens in order to help reconstruct which tokens are coreferent. For example, we might extend the generative story to generate a context for token (e, w) conditioned on e. Combining contextual similarity with string similarity has previously proved very useful for identifying cognates (Schafer and Yarowsky, 2002; Schafer, 2006b; Bergsma and Van Durme, 2011). In our setting it would help to distinguish people with identical names, as well as determining whether two people with similar names are really the same. 14These last two points suggest that the mutation model should operate not on simple (entity, string) pairs, but on richer representations in which the name has been parsed into its components (Eisenstein et al., 2011), labeled with a language ID, and perhaps labeled with a phonological pronunciation. These additional properties of a named entity may be either observed or latent in training data. For example,</context>
</contexts>
<marker>Schafer, 2006</marker>
<rawString>Charles Schafer. 2006a. Novel probabilistic finite-state transducers for cognate and transliteration modeling. In 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
</authors>
<title>Translation Discovery Using Diverse Smilarity Measures.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="4629" citStr="Schafer, 2006" startWordPosition="727" endWordPosition="728">Related Work Several previous papers have also considered learning transducers or other models of word pairs when 344 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 344–355, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ances</context>
<context position="16867" citStr="Schafer, 2006" startWordPosition="2777" endWordPosition="2778">token y, when it is not simply a faithful copy, from p(y x) or p(y Q) . The key step there is to sample the name string wy from p(wy wx) or p(wy Q) . Our model of these distributions could easily incorporate detailed linguistic knowledge of the mutation process (see §8). Here we describe the specific model that we use in our experiments. Like many such models, it can be regarded as a stochastic finitestate string-to-string transducer parameterized by 0. There is much prior work on stochastic models of edit distance (Ristad and Yianilos, 1998; Bilenko and Mooney, 2003; Oncina and Sebban, 2006; Schafer, 2006a; Bouchard-Côté et al., 2008; Dreyer et al., 2008, among others). For the present experiments, we designed a moderately simple one that employs (1) conditioning on one character of right context, (2) latent “edit” and “no-edit” regions to capture the fact that groups of edits are often made in close proximity, and (3) some simple special handling for the distribution conditioned on the root p(wy Q) . We assume a stochastic mutation process which, when given an input string wx, edits it from left to 4The very fact that x has been frequently observed demonstrates that it has often chosen to sto</context>
<context position="40906" citStr="Schafer, 2006" startWordPosition="6924" endWordPosition="6925">s such as title and middle name; and transliterating across languages.14 In other domains, one could model bibliographic entry propagation, derivational morphology, or historical sound change (again using language tags). Another future direction would be to incorporate the context of tokens in order to help reconstruct which tokens are coreferent. For example, we might extend the generative story to generate a context for token (e, w) conditioned on e. Combining contextual similarity with string similarity has previously proved very useful for identifying cognates (Schafer and Yarowsky, 2002; Schafer, 2006b; Bergsma and Van Durme, 2011). In our setting it would help to distinguish people with identical names, as well as determining whether two people with similar names are really the same. 14These last two points suggest that the mutation model should operate not on simple (entity, string) pairs, but on richer representations in which the name has been parsed into its components (Eisenstein et al., 2011), labeled with a language ID, and perhaps labeled with a phonological pronunciation. These additional properties of a named entity may be either observed or latent in training data. For example,</context>
</contexts>
<marker>Schafer, 2006</marker>
<rawString>Charles Schafer. 2006b. Translation Discovery Using Diverse Smilarity Measures. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic models of nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL,</booktitle>
<pages>132--140</pages>
<contexts>
<context position="29451" citStr="Smith and Smith, 2007" startWordPosition="5027" endWordPosition="5030">tex x to vertex y, according to (10), is cxy = E pθ(T) (11) TET♦(G)�(x→y)ET The probability cxy is a “pseudocount” for the expected number of mutations from x to y. This is at most 1 under our assumptions. Calculating cxy requires summing over all spanning trees of G, of which there are nn−2 for a fully connected graph with n vertices. Fortunately, Tutte (1984) shows how to compute this sum by the following method, which extends Kirchhoff&apos;s classical matrix-tree theorem to weighted directed graphs. This result has previously been employed in nonprojective dependency parsing (Koo et al., 2007; Smith and Smith, 2007). Let L E Rnxn denote the Laplacian of G, namely L= �Ex, δ(y |x&apos;) if x = y (12) −δ(y |x) if x =� y Tutte&apos;s theorem relates the determinant of the Laplacian to the spanning trees in graph G. In particular, the cofactor L0,0 is equal to the sum of the weights 350 of all directed spanning trees rooted at 0, which (supposing ♦ is indexed at 0) yields the partition function Z(G). The edge marginals of interest are related to the log partition function by cxy = ∂Z(G) (13) ∂δ(y |x) which has the closed-form solution cxy = f s(y |♦)Lyy if x = y (14) 11 δ( y |x) (L−&apos;− =6 Ley) if x y Thus, the problem o</context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>David A. Smith and Noah A. Smith. 2007. Probabilistic models of nonprojective dependency trees. In Proc. of EMNLPCoNLL, pages 132–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Smythe</author>
<author>H M Mahmoud</author>
</authors>
<title>A survey of recursive trees.</title>
<date>1995</date>
<booktitle>Theory of Probability and Mathematical Statistics,</booktitle>
<pages>51--1</pages>
<contexts>
<context position="16030" citStr="Smythe and Mahmoud, 1995" startWordPosition="2624" endWordPosition="2627">quent name x, other things equal (§2). But surprisingly, inference under the transformation process does not prefer this.4 Another view of our present model comes from the literature on random graphs (e.g., for modeling social networks or the link structure of the web). In a preferential attachment model, a graph’s vertices are added one by one, and each vertex selects some previous vertices as its neighbors. Our phylogeny is a preferential attachment tree, a random directed graph in which each vertex selects a single previous vertex as its parent. Specifically, it is a random recursive tree (Smythe and Mahmoud, 1995) whose vertices are the tokens.5 To this simple random topology we have added a random labeling process with mutation. The first α vertices are labeled with Q. 4 A Mutation Model for Strings Our model in §3 samples the next token y, when it is not simply a faithful copy, from p(y x) or p(y Q) . The key step there is to sample the name string wy from p(wy wx) or p(wy Q) . Our model of these distributions could easily incorporate detailed linguistic knowledge of the mutation process (see §8). Here we describe the specific model that we use in our experiments. Like many such models, it can be reg</context>
</contexts>
<marker>Smythe, Mahmoud, 1995</marker>
<rawString>R. T. Smythe and H. M. Mahmoud. 1995. A survey of recursive trees. Theory of Probability and Mathematical Statistics, 51(1-27).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Barzilay</author>
<author>Kevin Knight</author>
</authors>
<title>A statistical model for lost language decipherment.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1048--1057</pages>
<contexts>
<context position="4701" citStr="Snyder et al., 2010" startWordPosition="737" endWordPosition="740"> transducers or other models of word pairs when 344 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 344–355, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) organize words into morp</context>
</contexts>
<marker>Snyder, Barzilay, Knight, 2010</marker>
<rawString>Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A statistical model for lost language decipherment. In Proc. of ACL, pages 1048–1057.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichiro Tamura</author>
<author>Daniel Peterson</author>
<author>Nicholas Peterson</author>
<author>Glen Stecher</author>
<author>Masatoshi Nei</author>
<author>Sudhir Kumar</author>
</authors>
<title>Mega5: Molecular evolutionary genetics analysis using maximum likelihood, evolutionary distance, and maximum parsimony methods. Molecular Biology and Evolution,</title>
<date>2011</date>
<volume>28</volume>
<issue>10</issue>
<pages>2739</pages>
<contexts>
<context position="7891" citStr="Tamura et al., 2011" startWordPosition="1251" endWordPosition="1254">tion, deduplication, record linkage, and plagiarism detection. A few such papers reconstruct a phylogeny, as in the case of chain letters (Bennett et al., 2003), malware (Karim et al., 2005), or images (Dias et al., 2012). In fact, the last of these uses the same minimum spanning tree method that we apply in §5.3. However, these papers do not train a similarity measure as we do. To our knowledge, these two techniques have not been combined outside biology. In molecular evolutionary analysis, phylogenetic techniques have often been combined with estimation of some parametric model of mutation (Tamura et al., 2011). However, names mutate differently from biological sequences, and our mutation model for names (§4, §8) reflects that. We also posit a specific process (§3) that generates the name phylogeny. 2 An Example A fragment of a phylogeny for person names is shown in Figure 1. Our procedure learned this automatically from a collection of name tokens, without observing any input/output pairs. The nodes of the phylogeny are the observed name types,2 each one associated with a count of observed tokens. Each arrow corresponds to a hypothesized mutation. These mutations reflect linguistic processes such a</context>
</contexts>
<marker>Tamura, Peterson, Peterson, Stecher, Nei, Kumar, 2011</marker>
<rawString>Koichiro Tamura, Daniel Peterson, Nicholas Peterson, Glen Stecher, Masatoshi Nei, and Sudhir Kumar. 2011. Mega5: Molecular evolutionary genetics analysis using maximum likelihood, evolutionary distance, and maximum parsimony methods. Molecular Biology and Evolution, 28(10):2731– 2739.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Tarjan</author>
</authors>
<title>Finding optimum branchings.</title>
<date>1977</date>
<journal>Networks,</journal>
<pages>7--1</pages>
<contexts>
<context position="27586" citStr="Tarjan, 1977" startWordPosition="4714" endWordPosition="4715">weights of its edges: w(T) = fl δ(y |x) (9) (x→y)ET Then the posterior probability of spanning tree T is pθ (T) = Z(G) (10) where Z(G) = ETET♦(G) w(T) is the partition function, i.e. the total probability of generating the data G via any spanning tree of the form we consider. This distribution is determined by the parameters θ of the transducer pθ, along with the ratio α/µ. There exist several algorithms to find the single maximum-probability spanning tree, notably Tarjan&apos;s implementation of the Chu-Liu-Edmonds algorithm, which runs in O(m log n) for a sparse graph or O(n2) for a dense graph (Tarjan, 1977). Figure 1 shows a spanning tree found by our model using Tarjan&apos;s algorithm. Here n is the number of vertices (in our case, types and o), while m is the number of edges (which we can keep small by pruning, §6.1). 6 Training the Transducer with EM Our inference algorithm assumes that we know the transducer parameters θ. We now explain how to optimize θ to maximize the marginal likelihood of the training data. This marginal likelihood sums over all the other latent variables in the model—the spanning tree, the alignments between strings, and the hidden token ordering. The EM procedure repeats t</context>
</contexts>
<marker>Tarjan, 1977</marker>
<rawString>R E Tarjan. 1977. Finding optimum branchings. Networks, 7(1):25-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Tutte</author>
</authors>
<title>Graph Theory.</title>
<date>1984</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="29192" citStr="Tutte (1984)" startWordPosition="4988" endWordPosition="4989">assigns positive probability only to type-based phylogenies. This distribution is given by (10) and minimizes KL(p ||q). We argued in section §5.2 that it should be a good approximation. The posterior marginal probability of a directed edge from vertex x to vertex y, according to (10), is cxy = E pθ(T) (11) TET♦(G)�(x→y)ET The probability cxy is a “pseudocount” for the expected number of mutations from x to y. This is at most 1 under our assumptions. Calculating cxy requires summing over all spanning trees of G, of which there are nn−2 for a fully connected graph with n vertices. Fortunately, Tutte (1984) shows how to compute this sum by the following method, which extends Kirchhoff&apos;s classical matrix-tree theorem to weighted directed graphs. This result has previously been employed in nonprojective dependency parsing (Koo et al., 2007; Smith and Smith, 2007). Let L E Rnxn denote the Laplacian of G, namely L= �Ex, δ(y |x&apos;) if x = y (12) −δ(y |x) if x =� y Tutte&apos;s theorem relates the determinant of the Laplacian to the spanning trees in graph G. In particular, the cofactor L0,0 is equal to the sum of the weights 350 of all directed spanning trees rooted at 0, which (supposing ♦ is indexed at 0)</context>
</contexts>
<marker>Tutte, 1984</marker>
<rawString>W. Tutte. 1984. Graph Theory. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Winkler</author>
</authors>
<title>The state of record linkage and current research problems.</title>
<date>1999</date>
<tech>Technical report, Statistical Research</tech>
<institution>Division, U.S. Census Bureau.</institution>
<contexts>
<context position="38154" citStr="Winkler, 1999" startWordPosition="6495" endWordPosition="6496">entity titles t according to transducer probabilities pe(a I t). A good transducer should assign high probability to transformations from the correct title for the alias. Mean reciprocal rank (MRR) is a commonly used metric to estimate the quality of a ranking, which we report in Figure 4. The reported mean is over all aliases in the test data. In addition to evaluating the ranking for different initializations of our transducer, we compare to two baselines: Levenshtein distance and Jaro-Winkler similarity. JaroWinkler is a measure on strings that was specifically designed for record linkage (Winkler, 1999). The 13We did not find these parameters to be sensitive. 352 Held out log-likelihood 1500000 1 2 3 4 5 6 7 8 9 EM iteration (a) 1000 entities. sup=0 sup=5 sup=25 sup=100 sup=250 90000 100000 140000 110000 120000 130000 150000 160000 170000 Held out log-likelihood 180000 190000 200000 210000 220000 230000 sup=0 sup=5 sup=25 sup=100 sup=250 2400000 1 2 3 4 5 6 7 8 9 EM iteration (b) 1500 entities. Figure 3: Learning curves for different initializations of the transducer parameters. Above, “sup=100” (for instance) means that 100 entities were used as training data to initialize the transducer pa</context>
</contexts>
<marker>Winkler, 1999</marker>
<rawString>William E. Winkler. 1999. The state of record linkage and current research problems. Technical report, Statistical Research Division, U.S. Census Bureau.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007151">
<title confidence="0.983022">
Towards Efficient Named-Entity Rule Induction for Customizability
</title>
<author confidence="0.992063">
Ajay Nagesh1,2 Ganesh Ramakrishnan Laura Chiticariu
</author>
<affiliation confidence="0.954725">
&apos;IITB-Monash Research Academy 2IIT Bombay IBM Research - Almaden
</affiliation>
<email confidence="0.919741">
ajaynagesh@cse.iitb.ac.in ganesh@cse.iitb.ac.in chiti@us.ibm.com
</email>
<author confidence="0.877562">
Rajasekar Krishnamurthy Ankush Dharkar Pushpak Bhattacharyya
</author>
<affiliation confidence="0.852505">
IBM Research - Almaden SASTRA University IIT Bombay
</affiliation>
<email confidence="0.996392">
rajase@us.ibm.com ankushdharkar@cse.sastra.edu pb@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.993859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998811172413793">
Generic rule-based systems for Information
Extraction (IE) have been shown to work
reasonably well out-of-the-box, and achieve
state-of-the-art accuracy with further domain
customization. However, it is generally rec-
ognized that manually building and customiz-
ing rules is a complex and labor intensive pro-
cess. In this paper, we discuss an approach
that facilitates the process of building cus-
tomizable rules for Named-Entity Recognition
(NER) tasks via rule induction, in the Annota-
tion Query Language (AQL). Given a set of
basic features and an annotated document col-
lection, our goal is to generate an initial set
of rules with reasonable accuracy, that are in-
terpretable and thus can be easily refined by
a human developer. We present an efficient
rule induction process, modeled on a four-
stage manual rule development process and
present initial promising results with our sys-
tem. We also propose a simple notion of ex-
tractor complexity as a first step to quantify
the interpretability of an extractor, and study
the effect of induction bias and customization
of basic features on the accuracy and complex-
ity of induced rules. We demonstrate through
experiments that the induced rules have good
accuracy and low complexity according to our
complexity measure.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999481375">
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
Generic NER rules have been shown to work reason-
ably well-out-of-the-box, and with further domain
customization (Chiticariu et al., 2010b), achieve
quality surpassing state-of-the-art results. Table 1
</bodyText>
<table confidence="0.654219666666667">
System Dataset FQ=1
Generic Customized
GATE ACE2002 57.8 82.2
ACE 2005 57.32 88.95
SystemT CoNLL 2003 64.15 91.77
Enron 76.53 85.29
</table>
<tableCaption confidence="0.996111">
Table 1: Quality of generic vs. customized rules.
</tableCaption>
<bodyText confidence="0.999464689655173">
summarizes the quality of NER rules out-of-the-box
and after domain customization in the GATE (Cun-
ningham et al., 2011) and SystemT (Chiticariu et
al., 2010a) systems, as reported in (Maynard et al.,
2003) and (Chiticariu et al., 2010b) respectively.
Rule-based systems are widely used in enterprise
settings due to their explainability. Rules are trans-
parent, which leads to better explainability of errors.
One can easily identify the cause of a false positive
or negative, and refine the rules without affecting
other correct results identified by the system. Fur-
thermore, rules are typically easier to understand by
an IE developer and can be customized for a new
domain without requiring additional labeled data.
Typically, a rule-based NER system consists of a
combination of four categories of rules (Chiticariu et
al., 2010b): (1) Basic Feature (BF) rules to identify
components of an entity such as first name and last
name. (2) Candidate definition (CD) rules to iden-
tify complete occurrences of an entity by combining
the output of multiple BF rules, e.g., first name fol-
lowed by last name is a person candidate. (3) Candi-
date refinement (CR) rules to refine candidates gen-
erated by CD rules. E.g., discard candidate persons
contained within organizations. (4) Consolidation
rules (CO) to resolve overlapping candidates gener-
ated by multiple CD and CR rules.
A well-known drawback that influences the
adoptability of rule-based NER systems is the man-
</bodyText>
<page confidence="0.964603">
128
</page>
<note confidence="0.780708">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 128–138, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999823190476191">
ual effort required to build the rules. A common ap-
proach to address this problem is to build a generic
NER extractor and then customize it for specific do-
mains. While this approach partially alleviates the
problem, substantial manual effort (in the order of
several person weeks) is still required for the two
stages as reported in (Maynard et al., 2003; Chiti-
cariu et al., 2010b). In this paper, we present initial
work towards facilitating the process of building a
generic NER extractor using induction techniques.
Specifically, given as input an annotated docu-
ment corpus, a set of BF rules, and a default CO
rule for each entity type, our goal is to generate a
set of CD and CR rules such that the resulting ex-
tractor constitutes a good starting point for further
refinement by a developer. Since the generic NER
extractor has to be manually customized, a major
challenge is to ensure that the generated rules have
good accuracy, and, at the same time, that they are
not too complex, and consequently interpretable.
The main contributions in this paper are
</bodyText>
<listItem confidence="0.994014115384615">
1. An efficient system for NER rule induction, us-
ing a highly expressive rule language (AQL) as
the target language. The first phase of rule in-
duction uses a combination of clustering and
relative least general generalization (RLGG)
techniques to learn CD rules. The second phase
identifies CR rules using a propositional rule
learner like JRIP to learn accurate composi-
tions of CD rules.
2. Usage of induction biases to enhance the inter-
pretability of rules. These biases capture the
expertise gleaned from manual rule develop-
ment and constrain the search space in our in-
duction system.
3. Definition of an initial notion of extractor com-
plexity to quantify the interpretability of an ex-
tractor and to guide the process of adding in-
duction biases to favor learning less complex
extractors. This is to ensure that the rules are
easily customizable by the developer.
4. Scalable induction process through usage of
SystemT, a state-of-the-art IE system which
serves as a highly efficient theorem prover for
AQL, and performance optimizations such as
clustering of examples and parallelizing vari-
ous modules (E.g.: propositional rule learning).
</listItem>
<bodyText confidence="0.996402111111111">
Roadmap We first describe preliminaries on Sys-
temT and AQL (Section 3) and define the target lan-
guage for our induction algorithm and the notion of
rule complexity (Section 4). We then present our
approach for inducing CD and CR rules, and dis-
cuss induction biases that would favor interpretabil-
ity (Section 5), and discuss the results of an empir-
ical evaluation (Section 6). We conclude with av-
enues for improvement in the future (Section 7).
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999981766666667">
Existing approaches to rule induction for IE focus
on rule-based systems based on the cascading gram-
mar formalism exemplified by the Common Pat-
tern Specification Language (CPSL) (Appelt and
Onyshkevych, 1998), where rules are specified as
a sequence of basic features that describe an en-
tity, with limited predicates in the context of an
entity mention. Patel et al. (2009) and Soderland
(1999) elaborate on top-down techniques for induc-
tion of IE rules, whereas (Califf and Mooney, 1997;
Califf and Mooney, 1999) discuss a bottom-up IE
rule induction system that uses the relative least gen-
eral generalization (RLGG) of examples&apos;. However,
in all these systems, the expressivity of the rule-
representation language is restricted to that of cap-
turing sequence information. As discussed in Sec-
tion 3, contextual clues and higher level rule inter-
actions such as filtering and join are very difficult,
if not impossible to express in such representations
without resorting to custom code. Learning higher
level interactions between rules has received little
attention. Our technique for learning higher level in-
teractions is similar to the induction of ripple down
rules (Gaines and Compton, 1995), which, to the
best of our knowledge, has not been previously ap-
plied to IE. A framework for refining AQL extractors
based on an annotated document corpus described
in (Liu et al., 2010). We present complementary
techniques for inducing an initial extractor that can
be automatically refined in this framework.
</bodyText>
<sectionHeader confidence="0.997262" genericHeader="method">
3 Preliminaries
</sectionHeader>
<bodyText confidence="0.8969674">
SystemT is a declarative IE system based on an al-
gebraic framework. In SystemT, developers write
rules in AQL. To represent annotations in a docu-
&apos;Our work also makes use of RLGGs but computes these
generalizations for clusters of examples, instead of pairs.
</bodyText>
<page confidence="0.998216">
129
</page>
<figureCaption confidence="0.999818">
Figure 1: Example Person extractor in AQL
</figureCaption>
<bodyText confidence="0.999650295774648">
ment, AQL uses a simple relational data model with
three types: a span is a region of text within a docu-
ment identified by its “begin” and “end” positions; a
tuple is a fixed-size list of spans; a relation, or view,
is a multi-set of tuples, where every tuple in the view
must be of the same size.
Figure 1 shows a portion of a Person extractor
written in AQL. The basic building block of AQL
is a view. A view is a logical description of a set of
tuples in terms of (i) the document text (denoted as a
special view called Document), and (ii) the contents
of other views, as specified in the from clauses of
each statement. Figure 1 also illustrates five of the
basic constructs that can be used to define a view,
and which we explain next. The complete specifica-
tion can be found in the AQL manual (IBM, 2012).
In the paper, we will use ‘rules’ and ‘views’ inter-
changeably.
The extract statement specifies basic character-
level extraction primitives such as regular expression
and dictionary matching over text, creating a tuple
for each match. As an example, rule R1 uses the ex-
tract statement to identify matches (Caps spans) of a
regular expression for capitalized words.
The select statement is similar to the SQL select
statement but it contains an additional consolidate
on clause (explained further), along with an exten-
sive collection of text-specific predicates. Rule R5
illustrates a complex example: it selects First spans
immediately followed within zero tokens by a Last
span, where the latter is also a Caps span. The
two conditions are specified using two join predi-
cates: FollowsTok and Equals respectively. For each
triplet of First, Last and Caps spans satisfying the two
predicates, the CombineSpans built-in scalar func-
tion in the select clause constructs larger PersonFirst-
Last spans that begin at the begin position of the First
span, and end at the end position of the Last (also
Caps) span.
The union all statement merges the outputs of two
or more statements. For example, rule R6 unions
person candidates identified by rules R4 and R5.
The minus statement subtracts the output of one
statement from the output of another. For example,
rule R8 defines a view PersonAll by filtering out Per-
sonInvalid tuples from the set of PersonCandidate tu-
ples. Notice that rule R7 used to define the view Per-
sonInvalid illustrates another join predicate of AQL
called Overlaps, which returns true if its two argu-
ment spans overlap in the input text. Therefore, at
a high level, rule R8 removes person candidates that
overlap with an Organization span. (The Organization
extractor is not depicted in the figure.)
The consolidate clause of a select statement re-
moves selected overlapping spans from the indicated
column of the input tuples, according to the spec-
ified policy (for instance, ‘ContainedWithin’). For
example, rule R9 retains PersonAll spans that are not
contained in other PersonAll spans.
Internally, SystemT compiles an AQL extractor
into an executable plan in the form of a graph of
operators. The formal definition of these operators
takes the form of an algebra (Reiss et al., 2008), sim-
ilar to relational algebra, but with extensions for text
processing. The decoupling between AQL and the
operator algebra allows for greater rule expressiv-
ity because the rule language is not constrained by
the need to compile to a finite state transducer, as in
grammar systems based on the CPSL standard. In
fact, join predicates such as Overlaps, as well as fil-
ter operations (minus) are extremely difficult to ex-
</bodyText>
<page confidence="0.981951">
130
</page>
<bodyText confidence="0.9998555">
press in CPSL systems such as GATE without an
escape to custom code (Chiticariu et al., 2010b). In
addition, the decoupling between the AQL specifi-
cation of “what” to extract from “how” to extract
it, allows greater flexibility in choosing an efficient
execution strategy among the many possible opera-
tor graphs that may exist for the same AQL extrac-
tor. Therefore, extractors written in AQL achieve
orders of magnitude higher throughput (Chiticariu
et al., 2010a).
</bodyText>
<sectionHeader confidence="0.977655" genericHeader="method">
4 Induction Target Language
</sectionHeader>
<bodyText confidence="0.999480476190477">
Our goal is to automatically generate NER extrac-
tors with good quality, and at the same time, man-
ageable complexity, so that the extractors can be fur-
ther refined and customized by the developer. To this
end, we focus on inducing extractors using the sub-
set of AQL constructs described in Section 3. We
note that we have chosen a small subset of AQL con-
structs that are sufficient to implement several com-
mon operations required for NER. However, AQL is
a much more expressive language, and incorporating
additional constructs is subject to our future work.
In this section we describe the building blocks of
our target language, and propose a simple definition
for measuring the complexity of an extractor.
Target Language. The components of the target
language are as follows, and summarized in Table 2.
Basic features (BF): BF views are specified using the
extract statement, such as rules R1 to R3 in Figure 1.
In this paper, we assume as input a set of basic fea-
tures, consisting of dictionaries and regular expres-
sions.
Candidate definition (CD): CD views are expressed
using the select statement to combine BF views with
join predicates (e.g., Equals, FollowsTok or Over-
laps), and the CombineSpans scalar function to con-
struct larger candidate spans from input spans. Rules
R4 and R5 in Figure 1 are example CD rules. In
general, a CD view is defined as: “Select all spans
constructed from view1, view2, ..., viewn, such that all
join predicates are satisfied”.
Candidate refinement (CR): CR views are used to
discard spans output by the CD views that may be
incorrect. In general, a CR view is defined as: “From
the list of spans of viewvalid subtract all those spans that
belong to viewinvalid”. viewvalid is obtained by join-
ing all the positive CD clues on the Equals predicate
and viewinvalid is obtained by joining all the nega-
tive overlapping clues with the Overlaps predicate
and subsequently ’union’ing all the negative clues.
(e.g., similar in spirit to rules R6i R7 and R8 in Fig-
ure 1, except that the subtraction is done from a sin-
gle view and not the union of multiple views).
Consolidation (CO): Finally, a select statement with
a fixed consolidate clause is used for each entity type
to remove overlapping spans from CR views. An
example CO view is defined by rule R9 in Figure 1.
Extractor Complexity. Since our goal is to gener-
ate extractors with manageable complexity, we must
introduce a quantitative measure of extractor com-
plexity, in order to (1) judge the complexity of the
extractors generated by our system, and (2) reduce
the search space considered by the induction system.
To this end, we define a simple complexity score
that is a function of the number of rules, and the
number of input views to each rule of the extrac-
tor. In particular, we define the length of rule R,
denoted as L(R), as the number of input views in
the from clause(s) of the view. For example, in Fig-
ure 1, we have L(R4) = 2 and L(R5) = 3, since
R4 and R5 have two, and respectively three views
in the from clause. Furthermore, L(R8) = 2 since
each of the two inner statements of R8 has one from
clause with a single input view. The complexity of
BF rules (e.g., R1 to R3) and CO rules (e.g., R9) is
always 1, since these types of rules have a single in-
put view. We define the complexity of extractor E,
denoted as C(E) as the sum of lengths of all rules of
E. For example, the complexity of the Person extrac-
tor from Figure 1 is 15, plus the length of all rules
involved in defining Organization, which are omitted
from the figure.
Our simple notion of rule length is motivated
by existing literature in the area of database sys-
tems (Abiteboul et al., 1995), where the size of a
conjunctive query is determined only by the number
of atoms in the body of the query (e.g., items in the
FROM clause), and it is independent on the number
of join variables (i.e., items in the WHERE clause),
or the size of the head of the query (e.g., items in the
SELECT clause). As such, our notion of complexity
is rather coarse, and we shall discuss its shortcom-
ings in detail in Section 6.2. However, we shall show
that the complexity score significantly reduces the
search space of our induction techniques leading to
</bodyText>
<page confidence="0.993447">
131
</page>
<table confidence="0.999759444444444">
Phase name AQL statements Prescription Rule Type
Basic Features extract Off-the-shelf, Learning using prior Basic Features Definition
work (Riloff, 1993; Li et al., 2008)
Phase 1 (Clustering and select Bottom-up learning (LGG), Top-down refine- Development of Candidate
RLGG) ment Rules
Phase 2 (Propositional Rule select, union RIPPER, Lightweight Rule Induction Candidate Rules Filtering
Learning) all, minus
Consolidation consolidate, Manually identified consolidation rules, based Consolidation rules
union all on domain knowledge
</table>
<tableCaption confidence="0.934145">
Table 2: Phases in induction, the language constructs invoked in each phase, the prescriptions for inducing rules in the
phase and the corresponding type of rule in manual rule development.
</tableCaption>
<bodyText confidence="0.998011">
simpler and smaller extractors, and therefore consti-
tutes a good basis for more comprehensive measures
of interpretability in the future.
</bodyText>
<sectionHeader confidence="0.979677" genericHeader="method">
5 Induction of Rules
</sectionHeader>
<bodyText confidence="0.999983068965518">
Since the goal is to generate rules that can be cus-
tomized by humans, the overall structure of the in-
duced rules must be similar in spirit to what a devel-
oper following best practices would write. Hence,
the induction process is divided into multiple phases.
Figure 2 shows the correspondence between the
phases of induction and the types of rules. In Ta-
ble 2, we summarize the phases of our induction al-
gorithm, along with the subset of AQL constructs
that comprise the language of the rules learnt in that
phase, the possible methods prescribed for inducing
the rules and their correspondence with the stages in
the manual rule development.
Our induction system generates rules for two of
the four categories, namely CD and CR rules as
highlighted in Figure 2. We assume that we are
given the BFs in the form of dictionaries and reg-
ular expressions. Prior work on learning dictionar-
ies (Riloff, 1993) and regular expressions (Li et al.,
2008) could be leveraged to semi-automate the pro-
cess of defining the basic features.
We represent each example, in conjunction with
relevant background knowledge in the form first
order horn clauses. This background knowledge
will serve as input to our induction system. The
first phase of induction uses a combination of
clustering and relative least general generalization
(RLGG) (Nienhuys-Cheng and Wolf, 1997; Muggle-
ton and Feng, 1992) techniques. At the end of this
phase, we have a number of CD rules. In the sec-
ond phase, we begin by forming a structure called
the span-view table. Broadly speaking, this is an
attribute-value table formed by all the views induced
in the first phase along with the textual spans gener-
ated by them. The attribute-value table is used as
input to a propositional rule learner such as JRIP
to learn accurate compositions of a useful (as deter-
mined by the learning algorithm) subset of the CD
rules. This forms the second phase of our system.
The rules learnt from this phase are the CR rules.
At various phases, several induction biases are intro-
duced to enhance the interpretability of rules. These
biases capture the expertise gleaned from manual
rule development and constrain the search space in
our induction system.
The hypothesis language of our induction sys-
tem is Annotation Query Language (AQL) and we
use SystemT as the theorem prover. SystemT pro-
vides a very fast rule execution engine and is cru-
cial in our induction system as we test multiple hy-
potheses in the search for the more promising ones.
AQL provides a very expressive rule representation
language that is proven to be capable of encoding
all the paradigms that any rule-based representa-
tion can encode. The dual advantages of rich rule-
representation and execution efficiency are the main
motivation behind our choice.
We discuss our induction procedure in detail next.
</bodyText>
<subsectionHeader confidence="0.999334">
5.1 Basic Features and Background Knowledge
</subsectionHeader>
<bodyText confidence="0.999846">
We assume that we are provided with a set of dictio-
naries and regular expressions for defining all our
basic create view statements. R1, R2 and R3 in
Figure 1 are such basic view definitions. The ba-
sic views are compiled and executed in SystemT
over the training document collection and the re-
sulting spans are represented by equivalent predi-
cates in first order logic. Essentially, each train-
ing example is represented as a definite clause,
</bodyText>
<page confidence="0.991204">
132
</page>
<figureCaption confidence="0.998767333333333">
Figure 3: Relative Least General Generalization
Figure 2: Correspondence between Manual Rule devel-
opment and Rule Induction.
</figureCaption>
<bodyText confidence="0.999405666666667">
that includes in its body, the basic view-types en-
coded as background knowledge predicates. To es-
tablish relationships between different background
knowledge predicates for each training example, we
define some additional “glue predicates” such as
contains and before.
</bodyText>
<subsectionHeader confidence="0.99857">
5.2 Induction of Candidate Definition Rules
</subsectionHeader>
<bodyText confidence="0.994165538461538">
Clustering Module. We obtain non-overlapping
clusters of examples within each type, by comput-
ing similarities between their representations as def-
inite clauses. We present the intuition behind this
approach in Figure 3 which illustrates the process
of taking two examples and finding their generaliza-
tion. It is worthwhile to look at generalizations of
instances that are similar. For instance, two token
person names such as Mark Waugh and Mark Twain
are part of a single cluster. However, we would not
be able to generalize a two-token name (e.g., Mark
Waugh) with another name consisting of initials fol-
lowed by a token (e.g., M. Waugh). Using a wrap-
per around the hierarchical agglomerative cluster-
ing implemented in LingPipe2, we cluster examples
and look at generalizations only within each cluster.
Clustering also helps improve efficiency by reduc-
ing the computational overhead, since otherwise, we
would have to consider generalizations of all pairs of
examples (Muggleton and Feng, 1992).
RLGG computation. We compute our CD
rules as the relative least general generalization
(RLGG) (Nienhuys-Cheng and Wolf, 1997; Mug-
gleton and Feng, 1992) of examples in each clus-
ter. Given a set of clauses in first order logic,
their RLGG is the least generalized clause in the
</bodyText>
<footnote confidence="0.5658875">
2http://alias-i.com/lingpipe/demos/tutorial/cluster/read-
me.html
</footnote>
<bodyText confidence="0.997214217391304">
subsumption lattice of the clauses relative to the
background knowledge (Nienhuys-Cheng and Wolf,
1997). RLGG is associative, and we use this fact
to compute RLGGs of sets of examples in a clus-
ter. The RLGG of two bottom clauses as computed
in our system and its translation to an AQL view is
illustrated in Figure 3. We filter out noisy RLGGs
and convert the selected RLGGs into the equivalent
AQL views. Each such AQL view is treated as a
CD rule. We next discuss the process of filtering-
out noisy RLGGs. We interchangeably refer to the
RLGGs and the clusters they represent.
Iterative Clustering and RLGG filtering. Since
clustering is sub-optimal, we expected some clusters
in a single run of clustering to have poor RLGGs, ei-
ther in terms of complexity or precision. We there-
fore use an iterative clustering approach, based on
the separate-and-conquer (F¨urnkranz, 1999) strat-
egy. In each iteration, we pick the clusters with the
best RLGGs and remove all examples covered by
those RLGGs. The best RLGGs must have preci-
sion and number of examples covered above a pre-
specified threshold.
</bodyText>
<subsectionHeader confidence="0.998891">
5.3 Induction of Candidate Refinement Rules
</subsectionHeader>
<bodyText confidence="0.999299615384615">
Span-View Table. The CD views from phase 1
along with the textual spans they generate, yield the
span-view table. The rows of the table correspond
to the set of spans returned by all the CD views. The
columns correspond to the set of CD view names.
Each span either belongs to one of the named en-
tity types (PER, ORG or LOC) or is none of them
(NONE); the type information constitutes its class
label (see Figure 4 for an illustrated example). The
cells in the table correspond to either a match (M) or
a no-match (N) or partial/overlapping match (O) of
a span generated by a CD view. This attribute-value
table is used as input to a propositional rule learner
</bodyText>
<page confidence="0.999085">
133
</page>
<figureCaption confidence="0.999791">
Figure 4: Span-View Table
</figureCaption>
<bodyText confidence="0.979201000000001">
like JRIP to learn compositions of CD views.
Propositional Rule Learning. Based on our study
of different propositional rule learners, we decided
to use RIPPER (F¨urnkranz and Widmer, 1994) im-
plemented as the JRIP classifier in weka (Witten et
al., 2011). Some considerations that favor JRIP are
(i) absence of rule ordering, (ii) ease of conversion
to AQL and (iii) amenability to add induction biases
in the implementation.
A number of syntactic biases were introduced in
JRIP to aid in the interpretability of the induced
rules. We observed in our manually developed rules
that CR rules for a type involve interaction between
CDs for the same type and negations (not-overlaps,
not matches) of CDs of the other types. This bias
was incorporated by constraining a JRIP rule to con-
tain only positive features (CDs) of the same type
(say PER) and negative features (CDs) of only other
types (ORG and LOC, in this case).
The output of the JRIP algorithm is a set of
rules, one set for each of PER, ORG and LOC.
Here is an example rule: PER-CR-Rule -&lt;-- (PerCD
= m) AND (LocCD != o) which is read as : “If a
span matches PerCD and does not overlap with LocCD,
then that span denotes a PER named entity”. Here
PerCD is {[FirstName n CapsPerson][LastName
n CapsPerson]1 3 and LocCD is {[CapsPlace n
CitiesDict]1. This rule filters out wrong person
annotations like “Prince William” in Prince William
Sound. (This is the name of a location but has over-
lapped with a person named entity.) In AQL, this
effect can be achieved most elegantly by the minus
(filter) construct. Such an AQL rule will filter all
those occurrences of Prince William from the list of
3Two consecutive spans where the 1st is FirstName and
CapsPerson and the 2nd is LastName and CapsPerson.
persons that overlap with a city name.
Steps such as clustering, computation of RLGGs,
JRIP, and theorem proving using SystemT were par-
allelized. Once the CR views for each type of
named entity are learnt, many forms of consolida-
tions (COs) are possible, both within and across
types. A simple consolidation policy that we have
incorporated in the system is as follows: union all
the rules of a particular type, then perform a con-
tained within consolidation, resulting in the final set
of consolidated views for each named entity type.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99995353125">
We evaluate our system on CoNLL03 (Tjong
Kim Sang and De Meulder, 2003), a collection
of Reuters news stories. We used the CoNLL03
training set for induction and report results on the
CoNLL03 test collection.
The basic features (BFs) form the primary input to
our induction system. We experimented with three
sets of BFs:
Initial set(E1): The goal in this setup is to induce
an initial set of rules based on a small set of reason-
able BFs. We use a conservative initial set consisting
of 15 BFs (5 regular expressions and 10 dictionar-
ies).
Enhanced set (E2): Based on the results of E1,
we identify a set of additional domain independent
BFs4. Five views were added to the existing set in
E1 (1 regular expression and 4 dictionaries). The
goal is to observe whether our approach yields rea-
sonable accuracies compared to generic rules devel-
oped manually.
Domain customized set (E3): Based on the
knowledge of the domain of the training dataset
(CoNLL03), we introduced a set of features specific
to this dataset. These included sports-related person,
organization and location dictionaries5. These views
were added to the existing set in E2. The intended
goal is to observe what are the best possible accura-
cies that could be achieved with BFs customized to
a particular domain.
The set of parameters for iterative clustering on
which the accuracies reported are : the precision
threshold for the RLGGs of the clusters was 70%
</bodyText>
<footnote confidence="0.993454">
4E.g., the feature preposition dictionary was added in E2 to
help identify organization names such as Bank of England.
5Half of the documents in CoNLL03 are sports-related.
</footnote>
<page confidence="0.994136">
134
</page>
<table confidence="0.987612941176471">
Train Test
Type P R F P R F C(E)
E1 (Initial set)
PER 88.5 41.4 56.4 92.5 39.4 55.3 144
ORG 89.1 7.3 13.4 85.9 5.2 9.7 22
LOC 91.6 54.5 68.3 87.3 55.3 67.8 105
Overall 90.2 35.3 50.7 89.2 33.3 48.5 234
E2 (Enhanced set)
PER 84.7 52.9 65.1 87.5 49.9 63.5 233
ORG 88.2 7.8 14.3 85.8 5.9 11.0 99
LOC 92.1 58.6 71.7 88.6 59.1 70.9 257
Overall 88.6 40.7 55.8 88.0 38.2 53.3 457
E3 (Domain customized set)
PER 89.9 57.3 70.0 91.7 56.0 69.5 430
ORG 86.9 50.9 64.2 86.9 47.5 61.4 348
LOC 90.8 67.0 77.1 84.3 67.3 74.8 356
Overall 89.4 58.7 70.9 87.3 57.0 68.9 844
</table>
<tableCaption confidence="0.9949235">
Table 3: Results on CoNLL03 dataset with different basic
feature sets
</tableCaption>
<bodyText confidence="0.999828">
and the number of examples covered by each RLGG
was 5. We selected the top 5 clusters from each iter-
ation whose RLGGs crossed this threshold. If there
were no such clusters then we would lower the preci-
sion threshold to 35% (half of the threshold). When
no new clusters were formed, we ended the itera-
tions.
</bodyText>
<subsectionHeader confidence="0.917865">
6.1 Experiments and Results
Effect of Augmenting Basic Features. Table 3
</subsectionHeader>
<bodyText confidence="0.99933145">
shows the accuracy and complexity of rules induced
with the three basic feature sets E1, E2 and E3,
respectively 6. The overall F-measure on the test
dataset is 48.5% with E1, it increases to around
53.3% with E2 and is highest at 68.9% with E3.
As we increase the number of BFs, the accuracies
of the induced extractors increases, at the cost of
an increase in complexity. In particular, the re-
call increases significantly across the board, and is
more prominent between E2 and E3, where the ad-
ditional domain specific features result in recall in-
crease from 5.9% to 47.5% for ORG. The precision
increases slightly for PER, but decreases slightly for
LOC and ORG with the addition of domain specific
features.
Comparison with manually developed rules. We
compared the induced extractors with the manually
developed extractors of (Chiticariu et al., 2010b),
heretofore referred to as manual extractors. (For a
detailed analysis, we obtained the extractors from
</bodyText>
<footnote confidence="0.704806">
6These are the results for the configuration with bias.
</footnote>
<bodyText confidence="0.999925697674419">
the authors). Table 4 shows the accuracy and com-
plexity of the induced rules with E2 and E3 and the
manual extractors for the generic domain and, re-
spectively, customized for the CoNLL03 domain.
(In the table, ignore the column Induced (without
bias), which is discussed later). Our technique
compares reasonably with the manually constructed
generic extractor for two of the three entity types;
and on precision for all entity types, especially since
our system generated the rules in 1 hour, whereas the
development of manual rules took much longer 7.
Additional work is required to match the manual
customized extractor’s performance, primarily due
to shortcomings in our current target language. Re-
call that our framework is limited to a small subset
of AQL constructs for expressing CD and CR rules,
and there is a single consolidation rule. In particu-
lar, advanced constructs such as dynamic dictionar-
ies are not supported, and the set of predicates to the
Filter construct supported in our system is restricted
to predicates over other concepts, which is only a
subset of those used in (Chiticariu et al., 2010b).
The manual extractors also contain a larger number
of rules covering many different cases, improving
the accuracy, but also leading to a higher complex-
ity score. To better analyze the complexity, we also
computed the average rule length for each extrac-
tor by dividing the complexity score by the number
of AQL views of the extractor. The average rule
length is 1.78 and 1.87 for the induced extractors
with E2 and E3, respectively, and 1.9 and 2.1 for the
generic and customized extractors of (Chiticariu et
al., 2010b), respectively. The average rule length in-
creases from the generic extractor to the customized
extractor in both cases. On average, however, an in-
dividual induced rule is slightly smaller than a man-
ually developed rule.
Effect of Bias. The goal of this experiment is to
demonstrate the importance of biases in the induc-
tion process. The biases added to the system are
broadly of two types: (i) Partition of basic features
based on types (ii) Restriction on the type of CD
views that can appear in a CR view. 8 Without
</bodyText>
<footnote confidence="0.9547574">
7 (Chiticariu et al., 2010b) mentions that customization for 3
domains required 8 person weeks. It is reasonable to infer that
developing the generic rules took comparable effort.
8For e.g., person CR view can contain only person CD views
as positive clues and CD views of other types as negative clues.
</footnote>
<page confidence="0.997955">
135
</page>
<bodyText confidence="0.998641333333334">
(i) many semantically similar basic features (espe-
cially, regular expressions) would match a given to-
ken, leading to an increase in the length of a CD
a rule. For example, in the CD rule [FirstName-
Dict][CapsPerson ∧ CapsOrg]} (“A FirstNameDict
span followed by a CapsPerson span that is also a Cap-
sOrg span”), CapsPerson and CapsOrg are two very
similar regular expressions identifying capitalized
phrases that look like person, and respectively, orga-
nization names, with small variations (e.g., the for-
mer may allow special characters such as ‘-’). In-
cluding both BFs in a CD rule leads to a larger rule
that is unintuitive for a developer. The former bias
excludes such CD rules from consideration.
The latter type of bias prevents CD rules of one
type to appear as positive clues for a CR rule of
a different type. For instance, without this bias,
one of the CR rules obtained was Per -&lt;-- (OrgCD
= m) AND (LocCD != o) (“If a span matches OrgCD
and does not overlap with LocCD, then that span
denotes a PER named entity”. Here OrgCD was
{[CapsOrg][CapsOrg]} and LocCD was {[CapsLoc
∧ CitiesDict]}. The inclusion of an Organization
CD rule as a positive clue for a Person CR rule is
unintuitive for a developer.
Table 4, shows the effect (for E2 and E3) on the
test dataset of disabling and enabling bias during
the induction of CR rules using JRIP. Adding bias
improves the precision of the induced rules. With-
out bias, however, the system is less constrained in
its search for high recall rules, leading to slightly
higher overall F measure. This comes at the cost
of an increase in extractor complexity and average
rule length. For example, for E2, the average rule
length decreases from 2.17 to 1.78 after adding the
bias. Overall, our results show that biases lead to
less complex extractors with only a very minor ef-
fect on accuracy, thus biases are important factors
contributing to inducing rules that are understand-
able and may be refined by humans.
Comparison with other induction systems. We
also experimented with two other induction systems,
Aleph9 and ALP10, a package that implements one
of the reportedly good information extraction algo-
rithms (Ciravegna, 2001). While induction in Aleph
</bodyText>
<footnote confidence="0.998559666666667">
9A system for inductive logic programming. See
http://www.cs.ox.ac.uk/activities/machlearn/Aleph/aleph.html
10http://code.google.com/p/alpie/
</footnote>
<bodyText confidence="0.9998772">
was performed with the same target language as in
our approach, the target language of ALP is JAPE,
which has been shown (Chiticariu et al., 2010b) to
lack in some of the constructs (such as minus) that
AQL provides and which form a part of our tar-
get language (especially the rule refinement phase).
However, despite experimenting with all possible
parameter configurations for each of these (in each
of E1, E2 and E3 settings), the accuracies obtained
were substantially (30-50%) worse and the extrac-
tor complexity was much (around 60%) higher when
compared to our system (with or without bias). Ad-
ditionally, Aleph takes close to three days for induc-
tion, whereas both ALP and our system require less
than an hour.
</bodyText>
<subsectionHeader confidence="0.919904">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999698129032258">
Weak and Strong CDs reflected in CRs. In
our experiments, we found that varying the pre-
cision and complexity thresholds while inducing
the CDs (c.f Section 5) affected the F1 of the fi-
nal extractor only minimally. But reducing the
precision threshold generally improved the preci-
sion of the final extractor, which seemed counter-
intuitive at first. We found that CR rules learned
by JRIP consist of a strong CD rule (high preci-
sion, typically involving a dictionary) and a weak
CD rule (low precision, typically involving only
regular expressions). The strong CD rule always
corresponded to a positive clue (match) and the
weak CD rule corresponded to the negative clue
(overlaps or not-matches). This is illustrated in
the following CR rule: PER -&lt;-- (PerCD = m) AND
(OrgCD != o) where (PerCD is {[CapsPersonR]
[CapsPersonR ∧ LastNameDict]} and (OrgCD is
{[CapsOrgR][CapsOrgR][CapsOrgR]}. This is
posited to be the way the CR rule learner operates
– it tries to learn conjunctions of weak and strong
clues so as to filter one from the other. Therefore,
setting a precision threshold too high limited the
number of such weak clues and the ability of the CR
rule learner to find such rules.
Interpretability. Measuring interpretability of rules
is a difficult problem. In this work, we have taken
a first step towards measuring interpretability using
a coarse grain measure in the form of a simple no-
tion of complexity score. The complexity is very
helpful in comparing alternative rule sets based on
</bodyText>
<page confidence="0.99462">
136
</page>
<table confidence="0.9968468">
Chiticariu et al. 2010b Induced (With Bias) Induced (Without Bias)
P R F C(E) P R F C(E) P R F C(E)
Generic (E2) PER 82.2 60.3 69.5 945 87.5 49.9 63.5 233 85.8 53.7 66.0 476
ORG 75.7 17.5 28.5 1015 85.8 5.9 11.0 99 74.1 15.7 25.9 327
LOC 72.2 86.1 78.6 921 88.6 59.1 70.9 257 85.9 61.5 71.7 303
Overall 75.9 54.6 63.5 1015 88.0 38.2 53.3 457 84.2 43.5 57.4 907
Customised (E3) PER 96.3 92.2 94.2 2154 91.7 56.0 69.5 430 90.7 60.3 72.4 359
ORG 91.1 85.1 88.0 2154 86.9 47.5 61.4 348 90.4 46.8 61.7 397
LOC 93.3 91.7 92.5 2154 84.3 67.3 74.8 356 83.9 69.1 75.8 486
Overall 93.5 89.6 91.5 2160 87.3 57.0 68.9 844 87.8 58.7 70.4 901
</table>
<tableCaption confidence="0.999954">
Table 4: Comparison of induced rules (with and without bias) and manually developed rules. (CoNLL03 test dataset)
</tableCaption>
<bodyText confidence="0.999395235294118">
the number of rules, and the size of each rule, but
exhibits a number of shortcomings described next.
First, it disregards other components of a rule be-
sides its from clause, for example, the number of
items in the select clause, or the where clause. Sec-
ond, rule developers use semantically meaningful
view names such as those shown in Figure 1 to help
them recall the semantics of a rule at a high-level, an
aspect that is not captured by the complexity mea-
sure. Automatic generation of meaningful names
for induced views is an interesting direction for fu-
ture work. Finally, the overall structure of an extrac-
tors is not considered. In simple terms, an extrac-
tor consisting of 5 rules of size 1 is indistinguish-
able from an extractor consisting of a single rule
of size 5, and it is arguable which of these extrac-
tors is more interpretable. More generally, the ex-
tent of this shortcoming is best explained using an
example. When informally examining the rules in-
duced by our system, we found that CD rules are
similar in spirit to those written by rule develop-
ers. On the other hand, the induced CR rules are
too fine-grained. In general, rule developers group
CD rules with similar semantics, then write refine-
ment rules at the higher level of the group, as op-
posed to the lower level of individual CD views. For
example, one may write multiple CD rules for can-
didate person names of the form (First)(Last), and
multiple CD rules of the form (Last), (First). One
would then union together the candidates from each
of the two groups into two different views, e.g., Per-
FirstLast and PerLastCommaFirst, and write filter
rules at the higher level of these two views, e.g.,
“Remove PerLastCommaFirst spans that overlap with a
PerFirstLast span”. In contrast, our induction algo-
rithm considers CR rules consisting of combinations
of CD rules directly, leading to many semantically
similar CR rules, each operating over small parts of
a larger semantic group (see rule in Section 6.1).
This results in repetition, and qualitatively less in-
terpretable rules, since humans prefer higher levels
of abstraction and generalization. This nuance is not
captured by the complexity score which may deem
an extractor consisting of many rules, where many
of the rules operate at higher levels of groups of can-
didates to be more complex than a smaller extrac-
tor with many fine-grained rules. Indeed, as shown
before, the complexity of the induced extractors is
much smaller compared to that of manual extrac-
tors, although the latter follow the semantic group-
ing principle and are considered more interpretable.
</bodyText>
<sectionHeader confidence="0.998828" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999947">
We presented a system for efficiently inducing
named entity annotation rules in the AQL language.
The design of our approach is aimed at producing
accurate rules that can be understood and refined
by humans, by placing special emphasis on low
complexity and efficient computation of the induced
rules, while mimicking a four stage approach used
for manually constructing rules. The induced rules
have good accuracy and low complexity according
to our complexity measure. While our complexity
measure informs the biases in our system and leads
to simpler, smaller extractors, it captures extrac-
tor interpretability only to a certain extent. There-
fore, we believe more work is required to devise a
more comprehensive quantitative measure for inter-
pretability, and refine our techniques in order to in-
crease the interpretability of induced rules. Other
interesting directions for future work are introduc-
ing more constructs in our framework, and applying
our techniques to other languages.
</bodyText>
<page confidence="0.997066">
137
</page>
<sectionHeader confidence="0.995875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999786223529412">
S. Abiteboul, R. Hull, and V. Vianu. 1995. Foundations
of Databases. Addison Wesley Publishing Co.
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
common pattern specification language. In TIPSTER
workshop.
Mary Elaine Califf and Raymond J. Mooney. 1997. Ap-
plying ilp-based techniques to natural language infor-
mation extraction: An experiment in relational learn-
ing. In IJCAI Workshop on Frontiers of Inductive
Logic Programming.
Mary Elaine Califf and Raymond J. Mooney. 1999. Re-
lational learning of pattern-match rules for information
extraction. In AAAI.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick R. Reiss, and Shivaku-
mar Vaithyanathan. 2010a. Systemt: an algebraic ap-
proach to declarative information extraction. In ACL.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010b. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP.
Fabio Ciravegna. 2001. (lp)2, an adaptive algorithm for
information extraction from web-related texts. In In
Proceedings of the IJCAI-2001 Workshop on Adaptive
Text Extraction and Mining.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
J. F¨urnkranz and G. Widmer. 1994. Incremental reduced
error pruning. pages 70–77.
Johannes F¨urnkranz. 1999. Separate-and-conquer rule
learning. Artif. Intell. Rev., 13(1):3–54, February.
B. R. Gaines and P. Compton. 1995. Induction of ripple-
down rules applied to modeling large databases. J. In-
tell. Inf. Syst., 5:211–228, November.
IBM, 2012. IBM InfoSphere BigInsights - An-
notation Query Language (AQL) reference.
http://publib.boulder.ibm.com/
infocenter/bigins/v1r3/topic/com.
ibm.swg.im.infosphere.biginsights.
doc/doc/biginsights_aqlref_con_
aql-overview.html.
Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan,
Shivakumar Vaithyanathan, and H. V. Jagadish. 2008.
Regular expression learning for information extrac-
tion. In EMNLP.
Bin Liu, Laura Chiticariu, Vivian Chu, H. V. Jagadish,
and Frederick R. Reiss. 2010. Automatic rule refine-
ment for information extraction. Proc. VLDB Endow.,
3:588–597.
Diana Maynard, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Towards a semantic extraction of
named entities. In In Recent Advances in Natural Lan-
guage Processing.
Stephen Muggleton and C. Feng. 1992. Efficient induc-
tion in logic programs. In ILP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30:3–26.
Shan-Hwei Nienhuys-Cheng and Ronald de Wolf. 1997.
Foundations of Inductive Logic Programming.
Anup Patel, Ganesh Ramakrishnan, and Pushpak Bhat-
tacharyya. 2009. Incorporating linguistic expertise
using ilp for named entity recognition in data hungry
indian languages. In ILP.
Frederick Reiss, Sriram Raghavan, Rajasekar Krishna-
murthy, Huaiyu Zhu, and Shivakumar Vaithyanathan.
2008. An algebraic approach to rule-based informa-
tion extraction. In ICDE.
Ellen Riloff. 1993. Automatically constructing a dictio-
nary for information extraction tasks. In AAAI.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Mach.
Learn., 34:233–272.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL.
Ian H. Witten, Eibe Frank, and Mark A. Hall. 2011. Data
Mining: Practical Machine Learning Tools and Tech-
niques. Morgan Kaufmann, Amsterdam, 3rd edition.
</reference>
<page confidence="0.997348">
138
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.381016">
<title confidence="0.999955">Towards Efficient Named-Entity Rule Induction for Customizability</title>
<author confidence="0.7068115">Ramakrishnan Laura Chiticariu Research Academy Bombay IBM Research</author>
<email confidence="0.734879">ajaynagesh@cse.iitb.ac.inganesh@cse.iitb.ac.inchiti@us.ibm.com</email>
<author confidence="0.996715">Rajasekar Krishnamurthy Ankush Dharkar Pushpak Bhattacharyya</author>
<affiliation confidence="0.999026">IBM Research - Almaden SASTRA University IIT Bombay</affiliation>
<email confidence="0.989595">rajase@us.ibm.comankushdharkar@cse.sastra.edupb@cse.iitb.ac.in</email>
<abstract confidence="0.997903966666666">Generic rule-based systems for Information Extraction (IE) have been shown to work reasonably well out-of-the-box, and achieve state-of-the-art accuracy with further domain customization. However, it is generally recognized that manually building and customizing rules is a complex and labor intensive process. In this paper, we discuss an approach that facilitates the process of building customizable rules for Named-Entity Recognition (NER) tasks via rule induction, in the Annotation Query Language (AQL). Given a set of basic features and an annotated document collection, our goal is to generate an initial set of rules with reasonable accuracy, that are interpretable and thus can be easily refined by a human developer. We present an efficient rule induction process, modeled on a fourstage manual rule development process and present initial promising results with our system. We also propose a simple notion of extractor complexity as a first step to quantify the interpretability of an extractor, and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules. We demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abiteboul</author>
<author>R Hull</author>
<author>V Vianu</author>
</authors>
<title>Foundations of Databases.</title>
<date>1995</date>
<publisher>Addison Wesley Publishing Co.</publisher>
<contexts>
<context position="16186" citStr="Abiteboul et al., 1995" startWordPosition="2656" endWordPosition="2659"> 2 since each of the two inner statements of R8 has one from clause with a single input view. The complexity of BF rules (e.g., R1 to R3) and CO rules (e.g., R9) is always 1, since these types of rules have a single input view. We define the complexity of extractor E, denoted as C(E) as the sum of lengths of all rules of E. For example, the complexity of the Person extractor from Figure 1 is 15, plus the length of all rules involved in defining Organization, which are omitted from the figure. Our simple notion of rule length is motivated by existing literature in the area of database systems (Abiteboul et al., 1995), where the size of a conjunctive query is determined only by the number of atoms in the body of the query (e.g., items in the FROM clause), and it is independent on the number of join variables (i.e., items in the WHERE clause), or the size of the head of the query (e.g., items in the SELECT clause). As such, our notion of complexity is rather coarse, and we shall discuss its shortcomings in detail in Section 6.2. However, we shall show that the complexity score significantly reduces the search space of our induction techniques leading to 131 Phase name AQL statements Prescription Rule Type B</context>
</contexts>
<marker>Abiteboul, Hull, Vianu, 1995</marker>
<rawString>S. Abiteboul, R. Hull, and V. Vianu. 1995. Foundations of Databases. Addison Wesley Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
<author>Boyan Onyshkevych</author>
</authors>
<title>The common pattern specification language. In</title>
<date>1998</date>
<booktitle>TIPSTER workshop.</booktitle>
<contexts>
<context position="6876" citStr="Appelt and Onyshkevych, 1998" startWordPosition="1067" endWordPosition="1070">s on SystemT and AQL (Section 3) and define the target language for our induction algorithm and the notion of rule complexity (Section 4). We then present our approach for inducing CD and CR rules, and discuss induction biases that would favor interpretability (Section 5), and discuss the results of an empirical evaluation (Section 6). We conclude with avenues for improvement in the future (Section 7). 2 Related Work Existing approaches to rule induction for IE focus on rule-based systems based on the cascading grammar formalism exemplified by the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998), where rules are specified as a sequence of basic features that describe an entity, with limited predicates in the context of an entity mention. Patel et al. (2009) and Soderland (1999) elaborate on top-down techniques for induction of IE rules, whereas (Califf and Mooney, 1997; Califf and Mooney, 1999) discuss a bottom-up IE rule induction system that uses the relative least general generalization (RLGG) of examples&apos;. However, in all these systems, the expressivity of the rulerepresentation language is restricted to that of capturing sequence information. As discussed in Section 3, contextua</context>
</contexts>
<marker>Appelt, Onyshkevych, 1998</marker>
<rawString>Douglas E. Appelt and Boyan Onyshkevych. 1998. The common pattern specification language. In TIPSTER workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Applying ilp-based techniques to natural language information extraction: An experiment in relational learning.</title>
<date>1997</date>
<booktitle>In IJCAI Workshop on Frontiers of Inductive Logic Programming.</booktitle>
<contexts>
<context position="7155" citStr="Califf and Mooney, 1997" startWordPosition="1114" endWordPosition="1117"> the results of an empirical evaluation (Section 6). We conclude with avenues for improvement in the future (Section 7). 2 Related Work Existing approaches to rule induction for IE focus on rule-based systems based on the cascading grammar formalism exemplified by the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998), where rules are specified as a sequence of basic features that describe an entity, with limited predicates in the context of an entity mention. Patel et al. (2009) and Soderland (1999) elaborate on top-down techniques for induction of IE rules, whereas (Califf and Mooney, 1997; Califf and Mooney, 1999) discuss a bottom-up IE rule induction system that uses the relative least general generalization (RLGG) of examples&apos;. However, in all these systems, the expressivity of the rulerepresentation language is restricted to that of capturing sequence information. As discussed in Section 3, contextual clues and higher level rule interactions such as filtering and join are very difficult, if not impossible to express in such representations without resorting to custom code. Learning higher level interactions between rules has received little attention. Our technique for lear</context>
</contexts>
<marker>Califf, Mooney, 1997</marker>
<rawString>Mary Elaine Califf and Raymond J. Mooney. 1997. Applying ilp-based techniques to natural language information extraction: An experiment in relational learning. In IJCAI Workshop on Frontiers of Inductive Logic Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Relational learning of pattern-match rules for information extraction.</title>
<date>1999</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="7181" citStr="Califf and Mooney, 1999" startWordPosition="1118" endWordPosition="1121">cal evaluation (Section 6). We conclude with avenues for improvement in the future (Section 7). 2 Related Work Existing approaches to rule induction for IE focus on rule-based systems based on the cascading grammar formalism exemplified by the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998), where rules are specified as a sequence of basic features that describe an entity, with limited predicates in the context of an entity mention. Patel et al. (2009) and Soderland (1999) elaborate on top-down techniques for induction of IE rules, whereas (Califf and Mooney, 1997; Califf and Mooney, 1999) discuss a bottom-up IE rule induction system that uses the relative least general generalization (RLGG) of examples&apos;. However, in all these systems, the expressivity of the rulerepresentation language is restricted to that of capturing sequence information. As discussed in Section 3, contextual clues and higher level rule interactions such as filtering and join are very difficult, if not impossible to express in such representations without resorting to custom code. Learning higher level interactions between rules has received little attention. Our technique for learning higher level interact</context>
</contexts>
<marker>Califf, Mooney, 1999</marker>
<rawString>Mary Elaine Califf and Raymond J. Mooney. 1999. Relational learning of pattern-match rules for information extraction. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Chiticariu</author>
<author>Rajasekar Krishnamurthy</author>
<author>Yunyao Li</author>
<author>Sriram Raghavan</author>
<author>Frederick R Reiss</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Systemt: an algebraic approach to declarative information extraction.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2053" citStr="Chiticariu et al., 2010" startWordPosition="292" endWordPosition="295">n extractor, and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules. We demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Generic NER rules have been shown to work reasonably well-out-of-the-box, and with further domain customization (Chiticariu et al., 2010b), achieve quality surpassing state-of-the-art results. Table 1 System Dataset FQ=1 Generic Customized GATE ACE2002 57.8 82.2 ACE 2005 57.32 88.95 SystemT CoNLL 2003 64.15 91.77 Enron 76.53 85.29 Table 1: Quality of generic vs. customized rules. summarizes the quality of NER rules out-of-the-box and after domain customization in the GATE (Cunningham et al., 2011) and SystemT (Chiticariu et al., 2010a) systems, as reported in (Maynard et al., 2003) and (Chiticariu et al., 2010b) respectively. Rule-based systems are widely used in enterprise settings due to their explainability. Rules are trans</context>
<context position="4383" citStr="Chiticariu et al., 2010" startWordPosition="657" endWordPosition="661">n128 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 128–138, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ual effort required to build the rules. A common approach to address this problem is to build a generic NER extractor and then customize it for specific domains. While this approach partially alleviates the problem, substantial manual effort (in the order of several person weeks) is still required for the two stages as reported in (Maynard et al., 2003; Chiticariu et al., 2010b). In this paper, we present initial work towards facilitating the process of building a generic NER extractor using induction techniques. Specifically, given as input an annotated document corpus, a set of BF rules, and a default CO rule for each entity type, our goal is to generate a set of CD and CR rules such that the resulting extractor constitutes a good starting point for further refinement by a developer. Since the generic NER extractor has to be manually customized, a major challenge is to ensure that the generated rules have good accuracy, and, at the same time, that they are not to</context>
<context position="12119" citStr="Chiticariu et al., 2010" startWordPosition="1942" endWordPosition="1945">ph of operators. The formal definition of these operators takes the form of an algebra (Reiss et al., 2008), similar to relational algebra, but with extensions for text processing. The decoupling between AQL and the operator algebra allows for greater rule expressivity because the rule language is not constrained by the need to compile to a finite state transducer, as in grammar systems based on the CPSL standard. In fact, join predicates such as Overlaps, as well as filter operations (minus) are extremely difficult to ex130 press in CPSL systems such as GATE without an escape to custom code (Chiticariu et al., 2010b). In addition, the decoupling between the AQL specification of “what” to extract from “how” to extract it, allows greater flexibility in choosing an efficient execution strategy among the many possible operator graphs that may exist for the same AQL extractor. Therefore, extractors written in AQL achieve orders of magnitude higher throughput (Chiticariu et al., 2010a). 4 Induction Target Language Our goal is to automatically generate NER extractors with good quality, and at the same time, manageable complexity, so that the extractors can be further refined and customized by the developer. To</context>
<context position="30304" citStr="Chiticariu et al., 2010" startWordPosition="5022" endWordPosition="5025">68.9% with E3. As we increase the number of BFs, the accuracies of the induced extractors increases, at the cost of an increase in complexity. In particular, the recall increases significantly across the board, and is more prominent between E2 and E3, where the additional domain specific features result in recall increase from 5.9% to 47.5% for ORG. The precision increases slightly for PER, but decreases slightly for LOC and ORG with the addition of domain specific features. Comparison with manually developed rules. We compared the induced extractors with the manually developed extractors of (Chiticariu et al., 2010b), heretofore referred to as manual extractors. (For a detailed analysis, we obtained the extractors from 6These are the results for the configuration with bias. the authors). Table 4 shows the accuracy and complexity of the induced rules with E2 and E3 and the manual extractors for the generic domain and, respectively, customized for the CoNLL03 domain. (In the table, ignore the column Induced (without bias), which is discussed later). Our technique compares reasonably with the manually constructed generic extractor for two of the three entity types; and on precision for all entity types, es</context>
<context position="31584" citStr="Chiticariu et al., 2010" startWordPosition="5230" endWordPosition="5233">whereas the development of manual rules took much longer 7. Additional work is required to match the manual customized extractor’s performance, primarily due to shortcomings in our current target language. Recall that our framework is limited to a small subset of AQL constructs for expressing CD and CR rules, and there is a single consolidation rule. In particular, advanced constructs such as dynamic dictionaries are not supported, and the set of predicates to the Filter construct supported in our system is restricted to predicates over other concepts, which is only a subset of those used in (Chiticariu et al., 2010b). The manual extractors also contain a larger number of rules covering many different cases, improving the accuracy, but also leading to a higher complexity score. To better analyze the complexity, we also computed the average rule length for each extractor by dividing the complexity score by the number of AQL views of the extractor. The average rule length is 1.78 and 1.87 for the induced extractors with E2 and E3, respectively, and 1.9 and 2.1 for the generic and customized extractors of (Chiticariu et al., 2010b), respectively. The average rule length increases from the generic extractor </context>
<context position="35426" citStr="Chiticariu et al., 2010" startWordPosition="5871" endWordPosition="5874">ctors contributing to inducing rules that are understandable and may be refined by humans. Comparison with other induction systems. We also experimented with two other induction systems, Aleph9 and ALP10, a package that implements one of the reportedly good information extraction algorithms (Ciravegna, 2001). While induction in Aleph 9A system for inductive logic programming. See http://www.cs.ox.ac.uk/activities/machlearn/Aleph/aleph.html 10http://code.google.com/p/alpie/ was performed with the same target language as in our approach, the target language of ALP is JAPE, which has been shown (Chiticariu et al., 2010b) to lack in some of the constructs (such as minus) that AQL provides and which form a part of our target language (especially the rule refinement phase). However, despite experimenting with all possible parameter configurations for each of these (in each of E1, E2 and E3 settings), the accuracies obtained were substantially (30-50%) worse and the extractor complexity was much (around 60%) higher when compared to our system (with or without bias). Additionally, Aleph takes close to three days for induction, whereas both ALP and our system require less than an hour. 6.2 Discussion Weak and Str</context>
<context position="37539" citStr="Chiticariu et al. 2010" startWordPosition="6222" endWordPosition="6225">d to be the way the CR rule learner operates – it tries to learn conjunctions of weak and strong clues so as to filter one from the other. Therefore, setting a precision threshold too high limited the number of such weak clues and the ability of the CR rule learner to find such rules. Interpretability. Measuring interpretability of rules is a difficult problem. In this work, we have taken a first step towards measuring interpretability using a coarse grain measure in the form of a simple notion of complexity score. The complexity is very helpful in comparing alternative rule sets based on 136 Chiticariu et al. 2010b Induced (With Bias) Induced (Without Bias) P R F C(E) P R F C(E) P R F C(E) Generic (E2) PER 82.2 60.3 69.5 945 87.5 49.9 63.5 233 85.8 53.7 66.0 476 ORG 75.7 17.5 28.5 1015 85.8 5.9 11.0 99 74.1 15.7 25.9 327 LOC 72.2 86.1 78.6 921 88.6 59.1 70.9 257 85.9 61.5 71.7 303 Overall 75.9 54.6 63.5 1015 88.0 38.2 53.3 457 84.2 43.5 57.4 907 Customised (E3) PER 96.3 92.2 94.2 2154 91.7 56.0 69.5 430 90.7 60.3 72.4 359 ORG 91.1 85.1 88.0 2154 86.9 47.5 61.4 348 90.4 46.8 61.7 397 LOC 93.3 91.7 92.5 2154 84.3 67.3 74.8 356 83.9 69.1 75.8 486 Overall 93.5 89.6 91.5 2160 87.3 57.0 68.9 844 87.8 58.7 70</context>
</contexts>
<marker>Chiticariu, Krishnamurthy, Li, Raghavan, Reiss, Vaithyanathan, 2010</marker>
<rawString>Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Sriram Raghavan, Frederick R. Reiss, and Shivakumar Vaithyanathan. 2010a. Systemt: an algebraic approach to declarative information extraction. In ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Laura Chiticariu</author>
</authors>
<title>Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan. 2010b. Domain adaptation of rule-based annotators for named-entity recognition tasks.</title>
<booktitle>In EMNLP.</booktitle>
<marker>Chiticariu, </marker>
<rawString>Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan. 2010b. Domain adaptation of rule-based annotators for named-entity recognition tasks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Ciravegna</author>
</authors>
<title>(lp)2, an adaptive algorithm for information extraction from web-related texts. In</title>
<date>2001</date>
<booktitle>In Proceedings of the IJCAI-2001 Workshop on Adaptive Text Extraction and Mining.</booktitle>
<contexts>
<context position="35112" citStr="Ciravegna, 2001" startWordPosition="5834" endWordPosition="5835"> cost of an increase in extractor complexity and average rule length. For example, for E2, the average rule length decreases from 2.17 to 1.78 after adding the bias. Overall, our results show that biases lead to less complex extractors with only a very minor effect on accuracy, thus biases are important factors contributing to inducing rules that are understandable and may be refined by humans. Comparison with other induction systems. We also experimented with two other induction systems, Aleph9 and ALP10, a package that implements one of the reportedly good information extraction algorithms (Ciravegna, 2001). While induction in Aleph 9A system for inductive logic programming. See http://www.cs.ox.ac.uk/activities/machlearn/Aleph/aleph.html 10http://code.google.com/p/alpie/ was performed with the same target language as in our approach, the target language of ALP is JAPE, which has been shown (Chiticariu et al., 2010b) to lack in some of the constructs (such as minus) that AQL provides and which form a part of our target language (especially the rule refinement phase). However, despite experimenting with all possible parameter configurations for each of these (in each of E1, E2 and E3 settings), t</context>
</contexts>
<marker>Ciravegna, 2001</marker>
<rawString>Fabio Ciravegna. 2001. (lp)2, an adaptive algorithm for information extraction from web-related texts. In In Proceedings of the IJCAI-2001 Workshop on Adaptive Text Extraction and Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
<author>Niraj Aswani</author>
<author>Ian Roberts</author>
<author>Genevieve Gorrell</author>
<author>Adam Funk</author>
<author>Angus Roberts</author>
<author>Danica Damljanovic</author>
<author>Thomas Heitz</author>
<author>Mark A Greenwood</author>
<author>Horacio Saggion</author>
<author>Johann Petrak</author>
<author>Yaoyong Li</author>
<author>Wim Peters</author>
</authors>
<date>2011</date>
<journal>Text Processing with GATE (Version</journal>
<volume>6</volume>
<contexts>
<context position="2419" citStr="Cunningham et al., 2011" startWordPosition="346" endWordPosition="350">gnators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Generic NER rules have been shown to work reasonably well-out-of-the-box, and with further domain customization (Chiticariu et al., 2010b), achieve quality surpassing state-of-the-art results. Table 1 System Dataset FQ=1 Generic Customized GATE ACE2002 57.8 82.2 ACE 2005 57.32 88.95 SystemT CoNLL 2003 64.15 91.77 Enron 76.53 85.29 Table 1: Quality of generic vs. customized rules. summarizes the quality of NER rules out-of-the-box and after domain customization in the GATE (Cunningham et al., 2011) and SystemT (Chiticariu et al., 2010a) systems, as reported in (Maynard et al., 2003) and (Chiticariu et al., 2010b) respectively. Rule-based systems are widely used in enterprise settings due to their explainability. Rules are transparent, which leads to better explainability of errors. One can easily identify the cause of a false positive or negative, and refine the rules without affecting other correct results identified by the system. Furthermore, rules are typically easier to understand by an IE developer and can be customized for a new domain without requiring additional labeled data. T</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, Aswani, Roberts, Gorrell, Funk, Roberts, Damljanovic, Heitz, Greenwood, Saggion, Petrak, Li, Peters, 2011</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, Valentin Tablan, Niraj Aswani, Ian Roberts, Genevieve Gorrell, Adam Funk, Angus Roberts, Danica Damljanovic, Thomas Heitz, Mark A. Greenwood, Horacio Saggion, Johann Petrak, Yaoyong Li, and Wim Peters. 2011. Text Processing with GATE (Version 6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F¨urnkranz</author>
<author>G Widmer</author>
</authors>
<title>Incremental reduced error pruning.</title>
<date>1994</date>
<pages>70--77</pages>
<marker>F¨urnkranz, Widmer, 1994</marker>
<rawString>J. F¨urnkranz and G. Widmer. 1994. Incremental reduced error pruning. pages 70–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes F¨urnkranz</author>
</authors>
<title>Separate-and-conquer rule learning.</title>
<date>1999</date>
<journal>Artif. Intell. Rev.,</journal>
<volume>13</volume>
<issue>1</issue>
<marker>F¨urnkranz, 1999</marker>
<rawString>Johannes F¨urnkranz. 1999. Separate-and-conquer rule learning. Artif. Intell. Rev., 13(1):3–54, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B R Gaines</author>
<author>P Compton</author>
</authors>
<title>Induction of rippledown rules applied to modeling large databases.</title>
<date>1995</date>
<journal>J. Intell. Inf. Syst.,</journal>
<pages>5--211</pages>
<contexts>
<context position="7861" citStr="Gaines and Compton, 1995" startWordPosition="1223" endWordPosition="1226"> the relative least general generalization (RLGG) of examples&apos;. However, in all these systems, the expressivity of the rulerepresentation language is restricted to that of capturing sequence information. As discussed in Section 3, contextual clues and higher level rule interactions such as filtering and join are very difficult, if not impossible to express in such representations without resorting to custom code. Learning higher level interactions between rules has received little attention. Our technique for learning higher level interactions is similar to the induction of ripple down rules (Gaines and Compton, 1995), which, to the best of our knowledge, has not been previously applied to IE. A framework for refining AQL extractors based on an annotated document corpus described in (Liu et al., 2010). We present complementary techniques for inducing an initial extractor that can be automatically refined in this framework. 3 Preliminaries SystemT is a declarative IE system based on an algebraic framework. In SystemT, developers write rules in AQL. To represent annotations in a docu&apos;Our work also makes use of RLGGs but computes these generalizations for clusters of examples, instead of pairs. 129 Figure 1: </context>
</contexts>
<marker>Gaines, Compton, 1995</marker>
<rawString>B. R. Gaines and P. Compton. 1995. Induction of rippledown rules applied to modeling large databases. J. Intell. Inf. Syst., 5:211–228, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IBM</author>
</authors>
<title>IBM InfoSphere BigInsights - Annotation Query Language (AQL) reference.</title>
<date>2012</date>
<note>http://publib.boulder.ibm.com/ infocenter/bigins/v1r3/topic/com. ibm.swg.im.infosphere.biginsights. doc/doc/biginsights_aqlref_con_ aql-overview.html.</note>
<contexts>
<context position="9303" citStr="IBM, 2012" startWordPosition="1483" endWordPosition="1484">ation, or view, is a multi-set of tuples, where every tuple in the view must be of the same size. Figure 1 shows a portion of a Person extractor written in AQL. The basic building block of AQL is a view. A view is a logical description of a set of tuples in terms of (i) the document text (denoted as a special view called Document), and (ii) the contents of other views, as specified in the from clauses of each statement. Figure 1 also illustrates five of the basic constructs that can be used to define a view, and which we explain next. The complete specification can be found in the AQL manual (IBM, 2012). In the paper, we will use ‘rules’ and ‘views’ interchangeably. The extract statement specifies basic characterlevel extraction primitives such as regular expression and dictionary matching over text, creating a tuple for each match. As an example, rule R1 uses the extract statement to identify matches (Caps spans) of a regular expression for capitalized words. The select statement is similar to the SQL select statement but it contains an additional consolidate on clause (explained further), along with an extensive collection of text-specific predicates. Rule R5 illustrates a complex example:</context>
</contexts>
<marker>IBM, 2012</marker>
<rawString>IBM, 2012. IBM InfoSphere BigInsights - Annotation Query Language (AQL) reference. http://publib.boulder.ibm.com/ infocenter/bigins/v1r3/topic/com. ibm.swg.im.infosphere.biginsights. doc/doc/biginsights_aqlref_con_ aql-overview.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunyao Li</author>
<author>Rajasekar Krishnamurthy</author>
<author>Sriram Raghavan</author>
<author>Shivakumar Vaithyanathan</author>
<author>H V Jagadish</author>
</authors>
<title>Regular expression learning for information extraction.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="16906" citStr="Li et al., 2008" startWordPosition="2779" endWordPosition="2782">ry (e.g., items in the FROM clause), and it is independent on the number of join variables (i.e., items in the WHERE clause), or the size of the head of the query (e.g., items in the SELECT clause). As such, our notion of complexity is rather coarse, and we shall discuss its shortcomings in detail in Section 6.2. However, we shall show that the complexity score significantly reduces the search space of our induction techniques leading to 131 Phase name AQL statements Prescription Rule Type Basic Features extract Off-the-shelf, Learning using prior Basic Features Definition work (Riloff, 1993; Li et al., 2008) Phase 1 (Clustering and select Bottom-up learning (LGG), Top-down refine- Development of Candidate RLGG) ment Rules Phase 2 (Propositional Rule select, union RIPPER, Lightweight Rule Induction Candidate Rules Filtering Learning) all, minus Consolidation consolidate, Manually identified consolidation rules, based Consolidation rules union all on domain knowledge Table 2: Phases in induction, the language constructs invoked in each phase, the prescriptions for inducing rules in the phase and the corresponding type of rule in manual rule development. simpler and smaller extractors, and therefore</context>
<context position="18564" citStr="Li et al., 2008" startWordPosition="3043" endWordPosition="3046"> the types of rules. In Table 2, we summarize the phases of our induction algorithm, along with the subset of AQL constructs that comprise the language of the rules learnt in that phase, the possible methods prescribed for inducing the rules and their correspondence with the stages in the manual rule development. Our induction system generates rules for two of the four categories, namely CD and CR rules as highlighted in Figure 2. We assume that we are given the BFs in the form of dictionaries and regular expressions. Prior work on learning dictionaries (Riloff, 1993) and regular expressions (Li et al., 2008) could be leveraged to semi-automate the process of defining the basic features. We represent each example, in conjunction with relevant background knowledge in the form first order horn clauses. This background knowledge will serve as input to our induction system. The first phase of induction uses a combination of clustering and relative least general generalization (RLGG) (Nienhuys-Cheng and Wolf, 1997; Muggleton and Feng, 1992) techniques. At the end of this phase, we have a number of CD rules. In the second phase, we begin by forming a structure called the span-view table. Broadly speakin</context>
</contexts>
<marker>Li, Krishnamurthy, Raghavan, Vaithyanathan, Jagadish, 2008</marker>
<rawString>Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan, Shivakumar Vaithyanathan, and H. V. Jagadish. 2008. Regular expression learning for information extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Liu</author>
<author>Laura Chiticariu</author>
<author>Vivian Chu</author>
<author>H V Jagadish</author>
<author>Frederick R Reiss</author>
</authors>
<title>Automatic rule refinement for information extraction.</title>
<date>2010</date>
<booktitle>Proc. VLDB Endow.,</booktitle>
<pages>3--588</pages>
<contexts>
<context position="8048" citStr="Liu et al., 2010" startWordPosition="1256" endWordPosition="1259">rmation. As discussed in Section 3, contextual clues and higher level rule interactions such as filtering and join are very difficult, if not impossible to express in such representations without resorting to custom code. Learning higher level interactions between rules has received little attention. Our technique for learning higher level interactions is similar to the induction of ripple down rules (Gaines and Compton, 1995), which, to the best of our knowledge, has not been previously applied to IE. A framework for refining AQL extractors based on an annotated document corpus described in (Liu et al., 2010). We present complementary techniques for inducing an initial extractor that can be automatically refined in this framework. 3 Preliminaries SystemT is a declarative IE system based on an algebraic framework. In SystemT, developers write rules in AQL. To represent annotations in a docu&apos;Our work also makes use of RLGGs but computes these generalizations for clusters of examples, instead of pairs. 129 Figure 1: Example Person extractor in AQL ment, AQL uses a simple relational data model with three types: a span is a region of text within a document identified by its “begin” and “end” positions;</context>
</contexts>
<marker>Liu, Chiticariu, Chu, Jagadish, Reiss, 2010</marker>
<rawString>Bin Liu, Laura Chiticariu, Vivian Chu, H. V. Jagadish, and Frederick R. Reiss. 2010. Automatic rule refinement for information extraction. Proc. VLDB Endow., 3:588–597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Hamish Cunningham</author>
</authors>
<title>Towards a semantic extraction of named entities.</title>
<date>2003</date>
<booktitle>In In Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="2505" citStr="Maynard et al., 2003" startWordPosition="361" endWordPosition="364">tions (Nadeau and Sekine, 2007). Generic NER rules have been shown to work reasonably well-out-of-the-box, and with further domain customization (Chiticariu et al., 2010b), achieve quality surpassing state-of-the-art results. Table 1 System Dataset FQ=1 Generic Customized GATE ACE2002 57.8 82.2 ACE 2005 57.32 88.95 SystemT CoNLL 2003 64.15 91.77 Enron 76.53 85.29 Table 1: Quality of generic vs. customized rules. summarizes the quality of NER rules out-of-the-box and after domain customization in the GATE (Cunningham et al., 2011) and SystemT (Chiticariu et al., 2010a) systems, as reported in (Maynard et al., 2003) and (Chiticariu et al., 2010b) respectively. Rule-based systems are widely used in enterprise settings due to their explainability. Rules are transparent, which leads to better explainability of errors. One can easily identify the cause of a false positive or negative, and refine the rules without affecting other correct results identified by the system. Furthermore, rules are typically easier to understand by an IE developer and can be customized for a new domain without requiring additional labeled data. Typically, a rule-based NER system consists of a combination of four categories of rule</context>
<context position="4358" citStr="Maynard et al., 2003" startWordPosition="653" endWordPosition="656"> NER systems is the man128 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 128–138, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics ual effort required to build the rules. A common approach to address this problem is to build a generic NER extractor and then customize it for specific domains. While this approach partially alleviates the problem, substantial manual effort (in the order of several person weeks) is still required for the two stages as reported in (Maynard et al., 2003; Chiticariu et al., 2010b). In this paper, we present initial work towards facilitating the process of building a generic NER extractor using induction techniques. Specifically, given as input an annotated document corpus, a set of BF rules, and a default CO rule for each entity type, our goal is to generate a set of CD and CR rules such that the resulting extractor constitutes a good starting point for further refinement by a developer. Since the generic NER extractor has to be manually customized, a major challenge is to ensure that the generated rules have good accuracy, and, at the same t</context>
</contexts>
<marker>Maynard, Bontcheva, Cunningham, 2003</marker>
<rawString>Diana Maynard, Kalina Bontcheva, and Hamish Cunningham. 2003. Towards a semantic extraction of named entities. In In Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Muggleton</author>
<author>C Feng</author>
</authors>
<title>Efficient induction in logic programs.</title>
<date>1992</date>
<booktitle>In ILP.</booktitle>
<contexts>
<context position="18999" citStr="Muggleton and Feng, 1992" startWordPosition="3108" endWordPosition="3112">igure 2. We assume that we are given the BFs in the form of dictionaries and regular expressions. Prior work on learning dictionaries (Riloff, 1993) and regular expressions (Li et al., 2008) could be leveraged to semi-automate the process of defining the basic features. We represent each example, in conjunction with relevant background knowledge in the form first order horn clauses. This background knowledge will serve as input to our induction system. The first phase of induction uses a combination of clustering and relative least general generalization (RLGG) (Nienhuys-Cheng and Wolf, 1997; Muggleton and Feng, 1992) techniques. At the end of this phase, we have a number of CD rules. In the second phase, we begin by forming a structure called the span-view table. Broadly speaking, this is an attribute-value table formed by all the views induced in the first phase along with the textual spans generated by them. The attribute-value table is used as input to a propositional rule learner such as JRIP to learn accurate compositions of a useful (as determined by the learning algorithm) subset of the CD rules. This forms the second phase of our system. The rules learnt from this phase are the CR rules. At variou</context>
<context position="22365" citStr="Muggleton and Feng, 1992" startWordPosition="3653" endWordPosition="3656">similar. For instance, two token person names such as Mark Waugh and Mark Twain are part of a single cluster. However, we would not be able to generalize a two-token name (e.g., Mark Waugh) with another name consisting of initials followed by a token (e.g., M. Waugh). Using a wrapper around the hierarchical agglomerative clustering implemented in LingPipe2, we cluster examples and look at generalizations only within each cluster. Clustering also helps improve efficiency by reducing the computational overhead, since otherwise, we would have to consider generalizations of all pairs of examples (Muggleton and Feng, 1992). RLGG computation. We compute our CD rules as the relative least general generalization (RLGG) (Nienhuys-Cheng and Wolf, 1997; Muggleton and Feng, 1992) of examples in each cluster. Given a set of clauses in first order logic, their RLGG is the least generalized clause in the 2http://alias-i.com/lingpipe/demos/tutorial/cluster/readme.html subsumption lattice of the clauses relative to the background knowledge (Nienhuys-Cheng and Wolf, 1997). RLGG is associative, and we use this fact to compute RLGGs of sets of examples in a cluster. The RLGG of two bottom clauses as computed in our system and</context>
</contexts>
<marker>Muggleton, Feng, 1992</marker>
<rawString>Stephen Muggleton and C. Feng. 1992. Efficient induction in logic programs. In ILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>S Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<booktitle>Linguisticae Investigationes,</booktitle>
<pages>30--3</pages>
<contexts>
<context position="1915" citStr="Nadeau and Sekine, 2007" startWordPosition="272" endWordPosition="275">sing results with our system. We also propose a simple notion of extractor complexity as a first step to quantify the interpretability of an extractor, and study the effect of induction bias and customization of basic features on the accuracy and complexity of induced rules. We demonstrate through experiments that the induced rules have good accuracy and low complexity according to our complexity measure. 1 Introduction Named-entity recognition (NER) is the task of identifying mentions of rigid designators from text belonging to named-entity types such as persons, organizations and locations (Nadeau and Sekine, 2007). Generic NER rules have been shown to work reasonably well-out-of-the-box, and with further domain customization (Chiticariu et al., 2010b), achieve quality surpassing state-of-the-art results. Table 1 System Dataset FQ=1 Generic Customized GATE ACE2002 57.8 82.2 ACE 2005 57.32 88.95 SystemT CoNLL 2003 64.15 91.77 Enron 76.53 85.29 Table 1: Quality of generic vs. customized rules. summarizes the quality of NER rules out-of-the-box and after domain customization in the GATE (Cunningham et al., 2011) and SystemT (Chiticariu et al., 2010a) systems, as reported in (Maynard et al., 2003) and (Chit</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>D. Nadeau and S. Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30:3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shan-Hwei Nienhuys-Cheng</author>
<author>Ronald de Wolf</author>
</authors>
<date>1997</date>
<booktitle>Foundations of Inductive Logic Programming.</booktitle>
<marker>Nienhuys-Cheng, de Wolf, 1997</marker>
<rawString>Shan-Hwei Nienhuys-Cheng and Ronald de Wolf. 1997. Foundations of Inductive Logic Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anup Patel</author>
<author>Ganesh Ramakrishnan</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Incorporating linguistic expertise using ilp for named entity recognition in data hungry indian languages.</title>
<date>2009</date>
<booktitle>In ILP.</booktitle>
<contexts>
<context position="7041" citStr="Patel et al. (2009)" startWordPosition="1096" endWordPosition="1099">ucing CD and CR rules, and discuss induction biases that would favor interpretability (Section 5), and discuss the results of an empirical evaluation (Section 6). We conclude with avenues for improvement in the future (Section 7). 2 Related Work Existing approaches to rule induction for IE focus on rule-based systems based on the cascading grammar formalism exemplified by the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998), where rules are specified as a sequence of basic features that describe an entity, with limited predicates in the context of an entity mention. Patel et al. (2009) and Soderland (1999) elaborate on top-down techniques for induction of IE rules, whereas (Califf and Mooney, 1997; Califf and Mooney, 1999) discuss a bottom-up IE rule induction system that uses the relative least general generalization (RLGG) of examples&apos;. However, in all these systems, the expressivity of the rulerepresentation language is restricted to that of capturing sequence information. As discussed in Section 3, contextual clues and higher level rule interactions such as filtering and join are very difficult, if not impossible to express in such representations without resorting to c</context>
</contexts>
<marker>Patel, Ramakrishnan, Bhattacharyya, 2009</marker>
<rawString>Anup Patel, Ganesh Ramakrishnan, and Pushpak Bhattacharyya. 2009. Incorporating linguistic expertise using ilp for named entity recognition in data hungry indian languages. In ILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Reiss</author>
<author>Sriram Raghavan</author>
<author>Rajasekar Krishnamurthy</author>
<author>Huaiyu Zhu</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>An algebraic approach to rule-based information extraction.</title>
<date>2008</date>
<booktitle>In ICDE.</booktitle>
<contexts>
<context position="11603" citStr="Reiss et al., 2008" startWordPosition="1854" endWordPosition="1857">le R8 removes person candidates that overlap with an Organization span. (The Organization extractor is not depicted in the figure.) The consolidate clause of a select statement removes selected overlapping spans from the indicated column of the input tuples, according to the specified policy (for instance, ‘ContainedWithin’). For example, rule R9 retains PersonAll spans that are not contained in other PersonAll spans. Internally, SystemT compiles an AQL extractor into an executable plan in the form of a graph of operators. The formal definition of these operators takes the form of an algebra (Reiss et al., 2008), similar to relational algebra, but with extensions for text processing. The decoupling between AQL and the operator algebra allows for greater rule expressivity because the rule language is not constrained by the need to compile to a finite state transducer, as in grammar systems based on the CPSL standard. In fact, join predicates such as Overlaps, as well as filter operations (minus) are extremely difficult to ex130 press in CPSL systems such as GATE without an escape to custom code (Chiticariu et al., 2010b). In addition, the decoupling between the AQL specification of “what” to extract f</context>
</contexts>
<marker>Reiss, Raghavan, Krishnamurthy, Zhu, Vaithyanathan, 2008</marker>
<rawString>Frederick Reiss, Sriram Raghavan, Rajasekar Krishnamurthy, Huaiyu Zhu, and Shivakumar Vaithyanathan. 2008. An algebraic approach to rule-based information extraction. In ICDE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="16888" citStr="Riloff, 1993" startWordPosition="2777" endWordPosition="2778">ody of the query (e.g., items in the FROM clause), and it is independent on the number of join variables (i.e., items in the WHERE clause), or the size of the head of the query (e.g., items in the SELECT clause). As such, our notion of complexity is rather coarse, and we shall discuss its shortcomings in detail in Section 6.2. However, we shall show that the complexity score significantly reduces the search space of our induction techniques leading to 131 Phase name AQL statements Prescription Rule Type Basic Features extract Off-the-shelf, Learning using prior Basic Features Definition work (Riloff, 1993; Li et al., 2008) Phase 1 (Clustering and select Bottom-up learning (LGG), Top-down refine- Development of Candidate RLGG) ment Rules Phase 2 (Propositional Rule select, union RIPPER, Lightweight Rule Induction Candidate Rules Filtering Learning) all, minus Consolidation consolidate, Manually identified consolidation rules, based Consolidation rules union all on domain knowledge Table 2: Phases in induction, the language constructs invoked in each phase, the prescriptions for inducing rules in the phase and the corresponding type of rule in manual rule development. simpler and smaller extract</context>
<context position="18522" citStr="Riloff, 1993" startWordPosition="3038" endWordPosition="3039">nce between the phases of induction and the types of rules. In Table 2, we summarize the phases of our induction algorithm, along with the subset of AQL constructs that comprise the language of the rules learnt in that phase, the possible methods prescribed for inducing the rules and their correspondence with the stages in the manual rule development. Our induction system generates rules for two of the four categories, namely CD and CR rules as highlighted in Figure 2. We assume that we are given the BFs in the form of dictionaries and regular expressions. Prior work on learning dictionaries (Riloff, 1993) and regular expressions (Li et al., 2008) could be leveraged to semi-automate the process of defining the basic features. We represent each example, in conjunction with relevant background knowledge in the form first order horn clauses. This background knowledge will serve as input to our induction system. The first phase of induction uses a combination of clustering and relative least general generalization (RLGG) (Nienhuys-Cheng and Wolf, 1997; Muggleton and Feng, 1992) techniques. At the end of this phase, we have a number of CD rules. In the second phase, we begin by forming a structure c</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
</authors>
<title>Learning information extraction rules for semi-structured and free text.</title>
<date>1999</date>
<pages>34--233</pages>
<location>Mach. Learn.,</location>
<contexts>
<context position="7062" citStr="Soderland (1999)" startWordPosition="1101" endWordPosition="1102">nd discuss induction biases that would favor interpretability (Section 5), and discuss the results of an empirical evaluation (Section 6). We conclude with avenues for improvement in the future (Section 7). 2 Related Work Existing approaches to rule induction for IE focus on rule-based systems based on the cascading grammar formalism exemplified by the Common Pattern Specification Language (CPSL) (Appelt and Onyshkevych, 1998), where rules are specified as a sequence of basic features that describe an entity, with limited predicates in the context of an entity mention. Patel et al. (2009) and Soderland (1999) elaborate on top-down techniques for induction of IE rules, whereas (Califf and Mooney, 1997; Califf and Mooney, 1999) discuss a bottom-up IE rule induction system that uses the relative least general generalization (RLGG) of examples&apos;. However, in all these systems, the expressivity of the rulerepresentation language is restricted to that of capturing sequence information. As discussed in Section 3, contextual clues and higher level rule interactions such as filtering and join are very difficult, if not impossible to express in such representations without resorting to custom code. Learning </context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>Stephen Soderland. 1999. Learning information extraction rules for semi-structured and free text. Mach. Learn., 34:233–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: languageindependent named entity recognition. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
<author>Mark A Hall</author>
</authors>
<date>2011</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>Amsterdam,</location>
<note>3rd edition.</note>
<contexts>
<context position="24788" citStr="Witten et al., 2011" startWordPosition="4060" endWordPosition="4063">or is none of them (NONE); the type information constitutes its class label (see Figure 4 for an illustrated example). The cells in the table correspond to either a match (M) or a no-match (N) or partial/overlapping match (O) of a span generated by a CD view. This attribute-value table is used as input to a propositional rule learner 133 Figure 4: Span-View Table like JRIP to learn compositions of CD views. Propositional Rule Learning. Based on our study of different propositional rule learners, we decided to use RIPPER (F¨urnkranz and Widmer, 1994) implemented as the JRIP classifier in weka (Witten et al., 2011). Some considerations that favor JRIP are (i) absence of rule ordering, (ii) ease of conversion to AQL and (iii) amenability to add induction biases in the implementation. A number of syntactic biases were introduced in JRIP to aid in the interpretability of the induced rules. We observed in our manually developed rules that CR rules for a type involve interaction between CDs for the same type and negations (not-overlaps, not matches) of CDs of the other types. This bias was incorporated by constraining a JRIP rule to contain only positive features (CDs) of the same type (say PER) and negative</context>
</contexts>
<marker>Witten, Frank, Hall, 2011</marker>
<rawString>Ian H. Witten, Eibe Frank, and Mark A. Hall. 2011. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, Amsterdam, 3rd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
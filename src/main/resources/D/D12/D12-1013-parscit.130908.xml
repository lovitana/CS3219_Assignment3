<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.992073">
Active Learning for Imbalanced Sentiment Classification
</title>
<author confidence="0.995279">
Shoushan Li†, Shengfeng Ju†, Guodong Zhou† Xiaojun Li‡
</author>
<affiliation confidence="0.7669355">
†Natural Language Processing Lab ‡College of Computer and
School of Computer Science and Technology Information Engineering
</affiliation>
<address confidence="0.7956665">
Soochow University, Suzhou, 215006, China Zhejiang Gongshang University
{shoushan.li, shengfeng.ju}@gmail.com, Hangzhou, 310035, China
</address>
<email confidence="0.997648">
gdzhou@suda.edu.cn lixj@mail.zjgsu.edu.cn
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936740740741">
Active learning is a promising way for
sentiment classification to reduce the
annotation cost. In this paper, we focus on
the imbalanced class distribution scenario
for sentiment classification, wherein the
number of positive samples is quite
different from that of negative samples.
This scenario posits new challenges to
active learning. To address these
challenges, we propose a novel active
learning approach, named co-selecting, by
taking both the imbalanced class
distribution issue and uncertainty into
account. Specifically, our co-selecting
approach employs two feature subspace
classifiers to collectively select most
informative minority-class samples for
manual annotation by leveraging a
certainty measurement and an uncertainty
measurement, and in the meanwhile,
automatically label most informative
majority-class samples, to reduce human-
annotation efforts. Extensive experiments
across four domains demonstrate great
potential and effectiveness of our proposed
co-selecting approach to active learning for
imbalanced sentiment classification.
</bodyText>
<sectionHeader confidence="0.999316" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9962305">
Sentiment classification is the task of identifying
the sentiment polarity (e.g., positive or negative) of
</bodyText>
<note confidence="0.855011">
* Corresponding author
</note>
<page confidence="0.993784">
139
</page>
<bodyText confidence="0.9999262">
a natural language text towards a given topic (Pang
et al., 2002; Turney, 2002) and has become the
core component of many important applications in
opinion analysis (Cui et al., 2006; Li et al., 2009;
Lloret et al., 2009; Zhang and Ye, 2008).
Most of previous studies in sentiment
classification focus on learning models from a
large number of labeled data. However, in many
real-world applications, manual annotation is
expensive and time-consuming. In these situations,
active learning approaches could be helpful by
actively selecting most informative samples for
manual annotation. Compared to traditional active
learning for sentiment classification, active
learning for imbalanced sentiment classification
faces some unique challenges.
As a specific type of sentiment classification,
imbalanced sentiment classification deals with the
situation in which there are many more samples of
one class (called majority class) than the other
class (called minority class), and has attracted
much attention due to its high realistic value in
real-world applications (Li et al., 2011a). In
imbalanced sentiment classification, since the
minority-class samples (denoted as MI samples)
are normally much sparse and thus more precious
and informative for learning compared to the
majority-class ones (denoted as MA samples), it is
worthwhile to spend more on manually annotating
MI samples to guarantee both the quality and
quantity of MI samples. Traditionally, uncertainty
has been popularly used as a basic measurement in
active learning (Lewis and Gale, 2004). Therefore,
how to select most informative MI samples for
manual annotation without violating the basic
</bodyText>
<note confidence="0.5197935">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 139–148, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999614195652174">
uncertainty requirement in active learning is
challenging in imbalanced sentiment classification.
In this paper, we address above challenges in
active learning for imbalanced sentiment
classification. The basic idea of our active learning
approach is to use two complementary classifiers
for collectively selecting most informative MI
samples: one to adopt a certainty measurement for
selecting most possible MI samples and the other
to adopt an uncertainty measurement for selecting
most uncertain MI samples from the most possible
MI samples returned from the first classifier.
Specifically, the two classifiers are trained with
two disjoint feature subspaces to guarantee their
complementariness. This also applies to selecting
most informative MA samples. We call our novel
active learning approach co-selecting due to its
collectively selecting informative samples through
two disjoint feature subspace classifiers. To further
reduce the annotation efforts, we only manually
annotate those most informative MI samples while
those most informative MA samples are
automatically labeled using the predicted labels
provided by the first classifier.
In principle, our active learning approach differs
from existing ones in two main aspects. First, a
certainty measurement and an uncertainty
measurement are employed in two complementary
subspace classifiers respectively to collectively
select most informative MI samples for manual
annotation. Second, most informative MA samples
are automatically labeled to further reduce the
annotation cost. Evaluation across four domains
shows that our active learning approach is effective
for imbalanced sentiment classification and
significantly outperforms the state-of-the-art active
learning alternatives, such as uncertainty sampling
(Lewis and Gale, 2004) and co-testing (Muslea et
al., 2006).
The remainder of this paper is organized as
follows. Section 2 overviews the related work on
sentiment classification and active learning.
Section 3 proposes our active learning approach
for imbalanced sentiment classification. Section 4
reports the experimental results. Finally, Section 5
draws the conclusion and outlines the future work.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998684">
In this section, we give a brief overview on
sentiment classification and active learning.
</bodyText>
<subsectionHeader confidence="0.996258">
2.1 Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999954135135135">
Sentiment classification has become a hot research
topic in NLP community and various kinds of
classification methods have been proposed, such as
unsupervised learning methods (Turney, 2002),
supervised learning methods (Pang et al., 2002),
semi-supervised learning methods (Wan, 2009; Li
et al., 2010), and cross-domain classification
methods (Blitzer et al., 2007; Li and Zong, 2008;
He et al., 2011). However, imbalanced sentiment
classification is relatively new and there are only a
few studies in the literature.
Li et al. (2011a) pioneer the research in
imbalanced sentiment classification and propose a
co-training algorithm to perform semi-supervised
learning for imbalanced sentiment classification
with the help of a great amount of unlabeled
samples. However, their semi-supervised approach
to imbalanced sentiment classification suffers from
the problem that their balanced selection strategy
in co-training would generate many errors in late
iterations due to the imbalanced nature of the
unbalanced data. In comparison, our proposed
active learning approach can effectively avoid this
problem. By the way, it is worth to note that the
experiments therein show the superiority of under-
sampling over other alternatives such as cost-
sensitive and one-class classification for
imbalanced sentiment classification.
Li et al. (2011b) focus on supervised learning
for imbalanced sentiment classification and
propose a clustering-based approach to improve
traditional under-sampling approaches. However,
the improvement of the proposed clustering-based
approach over under-sampling is very limited.
Unlike all the studies mentioned above, our
study pioneers active learning on imbalanced
sentiment classification.
</bodyText>
<subsectionHeader confidence="0.999401">
2.2 Active Learning
</subsectionHeader>
<bodyText confidence="0.9996755">
Active leaning, as a standard machine learning
problem, has been extensively studied in many
research communities and several approaches have
been proposed to address this problem (Settles,
2009). Based on different sample selection
strategies, they can be grouped into two main
categories: (1) uncertainty sampling (Lewis and
Gale, 2004) where the active learner iteratively
select most uncertain unlabeled samples for
manual annotation; and (2) committee-based
</bodyText>
<page confidence="0.994369">
140
</page>
<bodyText confidence="0.999649301886792">
sampling where the active learner selects those
unlabeled samples which have the largest
disagreement among several committee classifiers.
Besides query by committee (QBC) as the first of
such type (Freund et al., 1997), co-testing learns a
committee of member classifiers from different
views and selects those contention points (i.e.,
unlabeled examples on which the views predict
different labels) for manual annotation (Muslea et
al., 2006).
However, most previous studies focus on the
scenario of balanced class distribution and only a
few recent studies address the active learning issue
on imbalanced classification problems including
Yang and Ma (2010), Zhu and Hovy (2007),
Ertekin et al. (2007a) and Ertekin et al. (2007b)2
.
Unfortunately, they straightly adopt the uncertainty
sampling as the active selection strategy to address
active learning in imbalanced classification, which
completely ignores the class imbalance problem in
the selected samples.
Attenberg and Provost (2010) highlights the
importance of selecting samples by considering the
proportion of the classes. Their simulation
experiment on text categorization confirms that
selecting class-balanced samples is more important
than traditional active selection strategies like
uncertainty. However, the proposed experiment is
simulated and non real strategy is proposed to
balance the class distribution of the selected
samples.
Doyle et al. (2011) propose a real strategy to
select balanced samples. They first select a set of
uncertainty samples and then randomly select
balanced samples from the uncertainty-sample set.
However, the classifier used for selecting balanced
samples is the same as the one for supervising
uncertainty, which makes the balance control
unreliable (the selected uncertainty samples take
very low confidences which are unreliable to
correctly predict the class label for controlling the
balance). Different from their study, our approach
possesses two merits: First, two feature subspace
classifiers are trained to finely integrate the
certainty and uncertainty measurements. Second,
the AM samples are automatically annotated,
2 Ertekin et al. (2007a) and Ertekin et al. (2007b) select
samples closest to the hyperplane provided by the SVM
classifier (within the margin). Their strategy can be seen as a
special case of uncertainty sampling.
which reduces the annotation cost in a further
effort.
</bodyText>
<sectionHeader confidence="0.902399" genericHeader="method">
3 Active Learning for Imbalanced
</sectionHeader>
<subsectionHeader confidence="0.896584">
Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.878215666666667">
Generally, active learning can be either stream-
based or pool-based (Sassano, 2002). The main
difference between the two is that the former scans
through the data sequentially and selects
informative samples individually, whereas the
latter evaluates and ranks the entire collection
before selecting most informative samples at batch.
As a large collection of samples can easily
gathered once in sentiment classification, pool-
based active learning is adopted in this study.
Figure 1 illustrates a standard pool-based active
learning approach, where the most important issue
is the sampling strategy, which evaluates the
informativeness of one sample.
Input:
Labeled data L;
Unlabeled pool U;
Output:
New Labeled data L
Procedure:
Loop for I iterations:
</bodyText>
<listItem confidence="0.757097333333333">
(1). Learn a classifier using current L
(2). Use current classifier to label all unlabeled
samples
(3). Use the sampling strategy to select n most
informative samples for manual annotation
(4). Move newly-labeled samples from U to L
</listItem>
<figureCaption confidence="0.996911">
Figure 1: Pool-based active learning
</figureCaption>
<subsectionHeader confidence="0.9933565">
3.1 Sampling Strategy: Uncertainty vs.
Certainty
</subsectionHeader>
<bodyText confidence="0.999908857142857">
As one of the most popular selection strategies in
active learning, uncertainty sampling depends on
an uncertainty measurement to select informative
samples. Since sentiment classification is a binary
classification problem, the uncertainty
measurement of a document d can be simply
defined as follows:
</bodyText>
<equation confidence="0.99254125">
Uncer(d) min (  |)
 P y d
y pos neg
 { , }
</equation>
<bodyText confidence="0.980326">
Where P(y  |d) denotes the posterior probability of
the document d belonging to the class y and {pos,
</bodyText>
<page confidence="0.992047">
141
</page>
<bodyText confidence="0.999691727272727">
neg} denotes the class labels of positive and
negative.
In imbalanced sentiment classification, MI
samples are much sparse yet precious for learning
and thus are believed to be more valuable for
manual annotation. The key in active learning for
imbalanced sentiment classification is to guarantee
both the quality and quantity of newly-added MI
samples. To guarantee the selection of MI samples,
a certainty measurement is necessary. In this study,
the certainty measurement is defined as follows:
</bodyText>
<equation confidence="0.904613">
Cer(d)= max P(y|d)
y pos neg
- { , }
</equation>
<bodyText confidence="0.99993325">
Meanwhile, in order to balance the samples in
the two classes, once an informative MI sample is
manually annotated, an informative MA sample is
automatically labeled. In this way, the annotated
data become more balanced than a random
selection strategy.
However, the two sampling strategies discussed
above are apparently contradicted: while the
uncertainty measurement is prone to selecting the
samples whose posterior probabilities are nearest
to 0.5, the certainty measurement is prone to
selecting the samples whose posterior probabilities
are nearest to 1. Therefore, it is essential to find a
solution to balance uncertainty sampling and
certainty sampling in imbalanced sentiment
classification,
</bodyText>
<subsectionHeader confidence="0.996087">
3.2 Co-selecting with Feature Subspace
Classifiers
</subsectionHeader>
<bodyText confidence="0.999951424242424">
In sentiment classification, a document is
represented as a feature vector generated from the
feature set F ={ f1,..., fm } . When a feature subset,
i.e., FS = { f S,..., fr S } ( r &lt; m ), is used, the
original m-dimensional feature space becomes an
r-dimensional feature subspace. In this study, we
call a classifier trained with a feature subspace a
feature subspace classifier.
Our basic idea of balancing both the uncertainty
measurement and the certainty measurement is to
train two subspace classifiers to adopt them
respectively. In our implementation, we randomly
select two disjoint feature subspaces, each of
which is used to train a subspace classifier. On one
side, one subspace classifier is employed to select
some certain samples; on the other side, the other
classifier is employed to select the most uncertain
sample from those certain samples for manual
annotation. In this way, the selected samples are
certain in terms of one feature subspace for
selecting more possible MI samples. Meanwhile,
the selected sample remains uncertain in terms of
the other feature subspace to introduce uncertain
knowledge into current learning model. We name
this approach as co-selecting because it
collectively selects informative samples by two
separate classifiers. Figure 2 illustrates the co-
selecting algorithm. In our algorithm, we strictly
constrain the balance of the samples between the
two classes, i.e., positive and negative. Therefore,
once two samples are annotated with the same
class label, they will not be added to the labeled
data, as shown in step (7) in Figure 2.
</bodyText>
<figure confidence="0.958339125">
Input:
Labeled data L with balanced samples over the
two classes
Unlabeled pool U
Output:
New Labeled data L
Procedure:
Loop for N iterations:
</figure>
<listItem confidence="0.8778654">
(1). Randomly select a feature subset S
F with
size r (with the proportion B = r / m ) from F
(2). Generate a feature subspace from S
F and
train a corresponding feature subspace
classifier CCer with L
(3). Generate another feature subspace from the
complement set of S
F , i.e., F — F and train
</listItem>
<figure confidence="0.73183">
S
a corresponding feature subspace classifier
CUncer with L
</figure>
<bodyText confidence="0.9778913">
(4). Use CCer to select top certain k positive and k
negative samples, denoted as a sample set
CER1
(5). Use CUncer to select the most uncertain
positive sample and negative sample from
CER1
(6). Manually annotate the two selected samples
(7). If the annotated labels of the two selected
samples are different from each other:
Add the two newly-annotated samples into L
</bodyText>
<figureCaption confidence="0.998996">
Figure 2: The co-selecting algorithm
</figureCaption>
<bodyText confidence="0.999672666666667">
There are two parameters in the algorithm: the
size of the feature subspace for training the first
subspace classifier, i.e., B and the number of
</bodyText>
<page confidence="0.991843">
142
</page>
<bodyText confidence="0.996792">
selected certain samples, i.e., k. Both of the two
parameters will be empirically studied in our
experiments.
</bodyText>
<subsectionHeader confidence="0.937198">
3.3 Co-selecting with Selected MA Samples
</subsectionHeader>
<figure confidence="0.915548">
Automatically Labeled
Input:
Labeled data L with balanced samples over the
two classes
Unlabeled pool U
MA and MI Label (positive or negative)
Output:
New Labeled data L
Procedure:
Loop for N iterations:
</figure>
<listItem confidence="0.56614675">
(1). Randomly select a proportion of features
(with the proportion  ) from F to get a
feature subset FS
(2). Generate a feature subspace from S
</listItem>
<figure confidence="0.956579444444445">
F and
train a corresponding subspace classifier CCer
with L
(3). Generate another feature subspace from the
complement set of S
F , i.e., F  F and train
S
a corresponding subspace classifier CUncer
with L
</figure>
<listItem confidence="0.9682707">
(4). Use CCer to select top certain k positive and k
negative samples, denoted as a sample set
CER1
(5). Use CUncer to select the most uncertain
positive sample and negative sample from
CER1
(6). Manually annotate the sample that is predicted
as a MI sample by CCer and automatically
annotate the sample that is predicted as
majority class
</listItem>
<bodyText confidence="0.979795333333333">
(7). If the annotated labels of the two selected
samples are different from each other:
Add the two newly-annotated samples into L
</bodyText>
<figureCaption confidence="0.9858555">
Figure 3: The co-selecting algorithm with selected
MA samples automatically labeled
</figureCaption>
<bodyText confidence="0.999971380952381">
To minimize manual annotation, it is a good choice
to automatically label those selected MA samples.
In our co-selecting approach, automatically
labeling those selected MA samples is easy and
straightforward: the subspace classifier for
monitoring the certainty measurement provides an
ideal solution to annotate the samples that have
been predicted as majority class. Figure 3 shows
the co-selecting algorithm with those selected MA
samples automatically labeled. The main
difference from the original co-selecting is shown
in Step (6) in Figure 3. Another difference is the
input where a prior knowledge of which class is
majority class or minority class should be known.
In real applications, it is not difficult to know this.
We first use a classifier trained with the initial
labeled data to test all unlabeled data. If the
predicted labels in the classification results are
greatly imbalanced, we can assume that the
unlabeled data is imbalanced, and consider the
dominated class as majority class.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="method">
4 Experimentation
</sectionHeader>
<bodyText confidence="0.99994225">
In this section, we will systematically evaluate our
active learning approach for imbalanced sentiment
classification and compare it with the state-of-the-
art active learning alternatives.
</bodyText>
<subsectionHeader confidence="0.953253">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.970184230769231">
Dataset
We use the same data as used by Li et al. (2011a).
The data collection consists of four domains: Book,
DVD, Electronic, and Kitchen ( Blitzer et al.,
2007). For each domain, we randomly select an
initial balanced labeled data with 50 negative
samples and 50 positive samples. For the unlabeled
data, we randomly select 2000 negative samples,
and 14580/12160/7140/7560 positive samples from
the four domains respectively, keeping the same
imbalanced ratio as the whole data. For the test
data in each domain, we randomly extract 800
negative samples and 800 positive samples.
</bodyText>
<subsectionHeader confidence="0.772859">
Classification algorithm
</subsectionHeader>
<bodyText confidence="0.999724285714286">
The Maximum Entropy (ME) classifier
implemented with the Mallet 3 tool is mainly
adopted, except that in the margin-based active
learning approach (Ertekin et al., 2007a) where
SVM is implemented with light-SVM 4 . The
features for classification are unigram words with
Boolean weights.
</bodyText>
<footnote confidence="0.998745">
3 http://mallet.cs.umass.edu/
4 http://www.cs.cornell.edu/people/tj/svm_light/
</footnote>
<page confidence="0.998025">
143
</page>
<figureCaption confidence="0.998941">
Figure 4: Performance comparison of different active learning approaches on imbalanced sentiment
</figureCaption>
<figure confidence="0.960523823529412">
classification
Book DVD Electronic Kitchen
0.78
0.76
0.74
0.72
G-mean
0.7
0.68
0.66
0.64
0.62
Random SVM-based Uncertainty Certainty
Co-testing Self-selecting Co-selecting-basic Co-selecting-plus
Evaluation metrics
The popular geometric mean
G - mean= TPrate  TI rate is adopted, where TPrate
</figure>
<bodyText confidence="0.9827735">
is the true positive rate (also called positive recall
or sensitivity) and TI rate is the true negative rate
(also called negative recall or specificity) (Kubat
and Matwin, 1997).
</bodyText>
<subsectionHeader confidence="0.976064">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999665">
For thorough comparison, various kinds of active
learning approaches are implemented including:
</bodyText>
<listItem confidence="0.992263842105263">
➢ Random: randomly select the samples from the
unlabeled data for manual annotation;
➢ Margin-based: iteratively select samples
closest to the hyperplane provided by the SVM
classifier, which is suggested by Ertekin et al.
(2007a) and Ertekin et al. (2007b). One sample
is selected in each iteration;
➢ Uncertainty: iteratively select samples using
the uncertainty measurement according to the
output of ME classifier. One sample is selected
in each iteration;
➢ Certainty: iteratively select class-balanced
samples using the certainty measurement
according to the output of ME classifier. One
positive and negative sample (the positive and
negative label is provided by the ME classifier)
are selected in each iteration;
➢ Co-testing: first get contention samples (i.e.,
unlabeled examples on which the member
</listItem>
<bodyText confidence="0.893265766666667">
classifiers predict different labels) and then
select the least confidence one among the
hypotheses of different member classifiers, i.e.,
the aggressive strategy as described Muslea et
al. (2006). Specifically, the member classifiers
are two subspace classifiers trained by splitting
the whole feature space into two disjoint
subspaces of same size;
➢ Self-selecting: first select k uncertainty samples
and then randomly select a positive and
negative sample from the uncertainty-sample
set, which is suggested by Doyle et al. (2011).
We call it self-selecting since only one
classifier is involved to measure uncertainty
and predict class labels.
For those approaches involving random
selection of features, we run 5 times for them and
report the average results. Note that the samples
selected by these approaches are imbalanced. To
address the problem of classification on
imbalanced data, we adopt the under-sampling
strategy which has been shown effective for
supervised imbalanced sentiment classification (Li
et al., 2011a). Our active learning approach
includes two versions: the co-selecting algorithm
as described in Section 3.2 and the co-selecting
with selected MA samples automatically labeled as
described in Section 3.3. For clarity, we refer the
former as co-selecting-basic and the latter as co-
selecting-plus in the following.
</bodyText>
<page confidence="0.991541">
144
</page>
<figure confidence="0.999710625">
0.78
0.76
0.74
0.72
0.68
0.78
0.76
0.74
0.72
0.68
0.66
0.64
0.62
0.8
0.7
0.7
300 600 900 1200 1500 2400 4800 7000
Number of the manually annoated samples
300 600 900 1200 1500 2400 4800 9600
Nubmer of the manually annotated samples
Random Co-testing Co-selecting-plus
Random Co-testing Co-selecting-plus
Electronic
Book
0.78
0.76
0.74
0.72
0.68
0.66
0.64
0.62
0.7
0.82
0.78
0.76
0.74
0.72
0.8
0.7
300 600 900 1200 1500 2400 4800 9600
Nubmer of the manually annotated samples
300 600 900 1200 1500 2400 4800 7000
Number of the manually annoated samples
Random Co-testing Co-selecting-plus
Random Co-testing Co-selecting-plus
Kitchen
DVD
</figure>
<figureCaption confidence="0.93963">
Figure 5: Performance comparison of three active learning approaches: random selection, co-testing
and co-selecting-plus, by varying the number of the selected samples for manually annotation
</figureCaption>
<bodyText confidence="0.926750477272727">
Comparison with other active learning
approaches
Figure 4 compares different active learning
approaches to imbalanced sentiment classification
when 600 unlabeled samples are selected for
annotation. Specifically, the parameters  and k is
set to be 1/16 and 50 respectively. Figure 4
justifies that it is challenging to perform active
learning in imbalanced sentiment classification: the
approaches of margin-based, uncertainty-based
and self-selecting perform no better than random
selection while co-testing only outperforms
random selection in two domains: DVD and
Electronic with only a small improvement (about
1%). In comparison, our approaches, both co-
selecting-basic and co-selecting-plus significantly
outperform the random selection approach on all
the four domains. It also shows that co-selecting-
plus is preferable over co-selecting-basic. This
verifies the effectiveness of automatically labeling
those selected MA samples in imbalanced
sentiment classification.
Specifically, we notice that only using the
certainty measurement (i.e., certainty) performs
worst, which reflects that only considering sample
balance factor in imbalanced sentiment
classification is not helpful.
Figure 5 compares our approach to other active
learning approaches by varying the number of the
selected samples for manually annotation. For
clarity, we only include random selection and co-
testing in comparison and do not show the
performances of the other active learning
approaches due to their similar behavior to random
selection. From this figure, we can see that co-
testing is effective on Book and Electronic when
less than 1500 samples are selected for manual
annotation but it fails to outperform random
selection in the other two domains. In contract, our
co-selecting-plus approach is apparently more
advantageous and significantly outperforms
random selection across all domains (p-value&lt;0.05)
when less than 4800 samples are selected for
manual annotation.
</bodyText>
<subsectionHeader confidence="0.976764">
Sensitiveness of the parameters 
</subsectionHeader>
<bodyText confidence="0.99995275">
The size of the feature subspace is an important
parameter in our approach. Figure 6 shows the
performance of co-selecting-plus with varying
sizes of the feature subspaces for the first subspace
</bodyText>
<page confidence="0.996647">
145
</page>
<bodyText confidence="0.998271625">
classifier CCsr. From Figure 6, we can see that a
choice of the proportion  between 1/8 and 1/32 is
recommended. This result also shows that the size
of the feature subspace for selecting certain
samples should be much less than that for selecting
uncertain samples, which indicates the more
important role of the uncertainty measurement in
active learning.
</bodyText>
<figureCaption confidence="0.99415575">
Figure 6: Performance of co-selecting-plus over
varying sizes of feature subspaces ( )
Figure 7: Performance of co-selecting-plus over
varying numbers of the selected certain samples (k)
</figureCaption>
<subsectionHeader confidence="0.743755">
Sensitiveness of parameter k
</subsectionHeader>
<bodyText confidence="0.984042962962963">
Figure 7 presents the performance of co-selecting-
plus with different numbers of the selected certain
samples in each iteration, i.e., parameter k.
Empirical studies suggest that setting k between 20
and 100 could get a stable performance. Also, this
figure demonstrates that using certainty as the only
query strategy is much less effective (see the result
when k=1). This once again verifies the importance
of the uncertainty strategy in active learning.
Number of MI samples selected for manual
annotation
In Table 1, we investigate the number of the MI
samples selected for manual annotation using
different active learning approaches when a total of
600 unlabeled samples are selected for annotation.
From this table, we can see that almost all the
existing active learning approaches can only select
a small amount of MI samples, taking similar
imbalanced ratios as the whole unlabeled data.
Although the certainty approach could select
many MI samples for annotation, this approach
performs worst due to its totally ignoring the
uncertainty factor. When our approach is applied,
especially co-selecting-plus, more MI samples are
selected for manual annotation and finally included
to learn the models. This greatly improves the
effectiveness of our active learning approach.
</bodyText>
<tableCaption confidence="0.993978333333333">
Table 1: The number of MI samples selected for
manual annotation when 600 samples are
annotated on the whole.
</tableCaption>
<table confidence="0.970007583333333">
Book DVD Electronic Kitchen
Random 71 82 131 123
SVM-based 65 72 135 106
Uncertainty 78 93 137 136
Certainty 160 200 236 227
Co-testing 89 84 136 109
Self-selecting 87 95 141 126
Co-selecting- 101 112 179 174
basic
Co-selecting- 161 156 250 272
plus
Precision of automatically labeled MA samples
</table>
<bodyText confidence="0.9999022">
In co-selecting-plus, all the added MA samples are
automatically labeled by the first subspace
classifier. It is encouraging to observe that 92.5%,
91.25%, 92%, and 93.5% of automatically labeled
MA samples are correctly annotated in Book, DVD,
Electronic, and Kitchen respectively. This suggests
that the subspace classifiers are able to predict the
MA samples with a high precision. This indicates
the rationality of automatically annotating MA
samples.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99975525">
In this paper, we propose a novel active learning
approach, named co-selecting, to reduce the
annotation cost for imbalanced sentiment
classification. It first trains two complementary
</bodyText>
<figure confidence="0.999020807692308">
G-mean
0.78
0.76
0.74
0.72
0.68
0.66
0.64
0.8
0.7
1/2 1/4 1/8 1/16 1/32 1/64 1/128
Proportion of the Selected Features for Subspace
(r /m )
Book DVD Electornic Kitchen
G-mean
0.78
0.76
0.74
0.72
0.68
0.66
0.8
0.7
1 5 20 50 100 150
Number of the selected certainty samples
Book DVD Electornic Kitchen
</figure>
<page confidence="0.995129">
146
</page>
<bodyText confidence="0.999935846153846">
classifiers with two disjoint feature subspaces and
then uses them to collectively select most
informative MI samples for manual annotation,
leaving most informative MA samples for
automatic annotation. Empirical studies show that
our co-selecting approach is capable of greatly
reducing the annotation cost and in the meanwhile,
significantly outperforms several active learning
alternatives
For the future work, we are interested in
applying our co-selecting approach to active
learning for other imbalanced classification tasks,
especially those with much higher imbalanced ratio.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9987808">
The research work described in this paper has been
partially supported by three NSFC grants,
No.61003155, No.60873150 and No.90920004,
one National High-tech Research and
Development Program of China
No.2012AA011102, Open Projects Program of
National Laboratory of Pattern Recognition, and
the NSF grant of Zhejiang Province No.Z1110551.
We also thank the three anonymous reviewers for
their helpful comments.
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999957735294118">
Attenberg J. and F. Provost. 2010. Why Label when you
can Search? Alternatives to Active Learning for
Applying Human Resources to Build Classification
Models Under Extreme Class Imbalance. In
Proceeding of KDD-10, 423-432.
Blitzer J., M. Dredze and F. Pereira. 2007. Biographies,
Bollywood, Boom-boxes and Blenders: Domain
Adaptation for Sentiment Classification. In
Proceedings of ACL-07, 440-447.
Cui H., V. Mittal, and M. Datar. 2006. Comparative
Experiments on Sentiment Classification for Online
Product Reviews. In Proceedings of AAAI-06,
pp.1265-1270.
Doyle S., J. Monaco, M. Feldman, J. Tomaszewski and
A. Madabhushi. 2011. An Active Learning based
Classification Strategy for the Minority Class
Problem: Application to Histopathology Annotation.
BMC Bioinformatics, 12: 424, 1471-2105.
Ertekin S., J. Huang, L. Bottou and C. Giles. 2007a.
Learning on the Border: Active Learning in
Imbalanced Data Classification. In Proceedings of
CIKM-07, 127-136.
Ertekin S., J. Huang, L. Bottou and C. Giles. 2007b.
Active Learning in Class Imbalanced Problem. In
Proceedings of SIGIR-07, 823-824.
Freund Y., H. Seung, E. Shamir and N. Tishby. 1997.
Selective Sampling using the Query by Committee
algorithm. Machine Learning, 28(2-3), 133-168.
He Y., C. Lin and H. Alani. 2011. Automatically
Extracting Polarity-Bearing Topics for Cross-
Domain Sentiment Classification. In Proceeding of
ACL-11, 123-131.
Lewis D. and W. Gale. 1994. Training Text Classifiers
by Uncertainty Sampling. In Proceedings of SIGIR-
94, 3-12.
Li F., Y. Tang, M. Huang and X. Zhu. 2009. Answering
Opinion Questions with Random Walks on Graphs.
In Proceedings of ACL-IJCNLP-09, 737-745.
Li S. and C. Zong. 2008. Multi-domain Sentiment
Classification. In Proceedings of ACL-08, short paper,
pp.257-260.
Li S., C. Huang, G. Zhou and S. Lee. 2010. Employing
Personal/Impersonal Views in Supervised and Semi-
supervised Sentiment Classification. In Proceedings
of ACL-10, pp.414-423.
Li S., Z. Wang, G. Zhou and S. Lee. 2011a. Semi-
supervised Learning for Imbalanced Sentiment
Classification. In Proceeding of IJCAI-11, 826-1831.
Li S., G. Zhou, Z. Wang, S. Lee and R. Wang. 2011b.
Imbalanced Sentiment Classification. In Proceedings
of CIKM-11, poster paper, 2469-2472.
Lloret E., A. Balahur, M. Palomar, and A. Montoyo.
2009. Towards Building a Competitive Opinion
Summarization System. In Proceedings of NAACL-
09 Student Research Workshop and Doctoral
Consortium, 72-77.
Kubat M. and S. Matwin. 1997. Addressing the Curse of
Imbalanced Training Sets: One-Sided Selection. In
Proceedings of ICML-97, 179–186.
Muslea I., S. Minton and C. Knoblock . 2006. Active
Learning with Multiple Views. Journal of Artificial
Intelligence Research, vol.27, 203-233.
Pang B. and L. Lee. 2008. Opinion Mining and
Sentiment Analysis: Foundations and Trends.
Information Retrieval, vol.2(12), 1-135.
Pang B., L. Lee and S. Vaithyanathan. 2002.Thumbs up?
Sentiment Classification using Machine Learning
Techniques. In Proceedings of EMNLP-02, 79-86.
</reference>
<page confidence="0.98104">
147
</page>
<reference confidence="0.999457571428571">
Settles B. 2009. Active Learning Literature Survey.
Computer Sciences Technical Report 1648,
University of Wisconsin, Madison, 2009.
Turney P. 2002. Thumbs up or Thumbs down?
Semantic Orientation Applied to Unsupervised
Classification of reviews. In Proceedings of ACL-02,
417-424.
Wan X. 2009. Co-Training for Cross-Lingual Sentiment
Classification. In Proceedings of ACL-IJCNLP-09,
235–243.
Yang Y. and G. Ma. 2010. Ensemble-based Active
Learning for Class Imbalance Problem. J. Biomedical
Science and Engineering, vol.3,1021-1028.
Zhang M. and X. Ye. 2008. A Generation Model to
Unify Topic Relevance and Lexicon-based Sentiment
for Opinion Retrieval. In Proceedings of SIGIR-08,
411-418.
Zhu J. and E. Hovy. 2007. Active Learning for Word
Sense Disambiguation with Methods for Addressing
the Class Imbalance Problem. In Proceedings of
ACL-07, 783-793.
</reference>
<page confidence="0.99684">
148
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720080">
<title confidence="0.999816">Active Learning for Imbalanced Sentiment Classification</title>
<author confidence="0.944371">Shengfeng Guodong Xiaojun</author>
<affiliation confidence="0.945871666666667">Language Processing Lab of Computer and School of Computer Science and Technology Information Engineering Soochow University, Suzhou, 215006, China Zhejiang Gongshang University</affiliation>
<address confidence="0.997861">shengfeng.ju}@gmail.com, 310035, China</address>
<email confidence="0.879075">gdzhou@suda.edu.cnlixj@mail.zjgsu.edu.cn</email>
<abstract confidence="0.999576785714286">Active learning is a promising way for sentiment classification to reduce the annotation cost. In this paper, we focus on the imbalanced class distribution scenario for sentiment classification, wherein the number of positive samples is quite different from that of negative samples. This scenario posits new challenges to active learning. To address these challenges, we propose a novel active learning approach, named co-selecting, by taking both the imbalanced class distribution issue and uncertainty into account. Specifically, our co-selecting approach employs two feature subspace classifiers to collectively select most informative minority-class samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Attenberg</author>
<author>F Provost</author>
</authors>
<title>Why Label when you can Search? Alternatives to Active Learning for Applying Human Resources to Build Classification Models Under Extreme Class Imbalance.</title>
<date>2010</date>
<booktitle>In Proceeding of KDD-10,</booktitle>
<pages>423--432</pages>
<contexts>
<context position="9049" citStr="Attenberg and Provost (2010)" startWordPosition="1255" endWordPosition="1258">he views predict different labels) for manual annotation (Muslea et al., 2006). However, most previous studies focus on the scenario of balanced class distribution and only a few recent studies address the active learning issue on imbalanced classification problems including Yang and Ma (2010), Zhu and Hovy (2007), Ertekin et al. (2007a) and Ertekin et al. (2007b)2 . Unfortunately, they straightly adopt the uncertainty sampling as the active selection strategy to address active learning in imbalanced classification, which completely ignores the class imbalance problem in the selected samples. Attenberg and Provost (2010) highlights the importance of selecting samples by considering the proportion of the classes. Their simulation experiment on text categorization confirms that selecting class-balanced samples is more important than traditional active selection strategies like uncertainty. However, the proposed experiment is simulated and non real strategy is proposed to balance the class distribution of the selected samples. Doyle et al. (2011) propose a real strategy to select balanced samples. They first select a set of uncertainty samples and then randomly select balanced samples from the uncertainty-sample</context>
</contexts>
<marker>Attenberg, Provost, 2010</marker>
<rawString>Attenberg J. and F. Provost. 2010. Why Label when you can Search? Alternatives to Active Learning for Applying Human Resources to Build Classification Models Under Extreme Class Imbalance. In Proceeding of KDD-10, 423-432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>M Dredze</author>
<author>F Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="6213" citStr="Blitzer et al., 2007" startWordPosition="853" endWordPosition="856">tion. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iteration</context>
<context position="18634" citStr="Blitzer et al., 2007" startWordPosition="2770" endWordPosition="2773">al labeled data to test all unlabeled data. If the predicted labels in the classification results are greatly imbalanced, we can assume that the unlabeled data is imbalanced, and consider the dominated class as majority class. 4 Experimentation In this section, we will systematically evaluate our active learning approach for imbalanced sentiment classification and compare it with the state-of-theart active learning alternatives. 4.1 Experimental Setting Dataset We use the same data as used by Li et al. (2011a). The data collection consists of four domains: Book, DVD, Electronic, and Kitchen ( Blitzer et al., 2007). For each domain, we randomly select an initial balanced labeled data with 50 negative samples and 50 positive samples. For the unlabeled data, we randomly select 2000 negative samples, and 14580/12160/7140/7560 positive samples from the four domains respectively, keeping the same imbalanced ratio as the whole data. For the test data in each domain, we randomly extract 800 negative samples and 800 positive samples. Classification algorithm The Maximum Entropy (ME) classifier implemented with the Mallet 3 tool is mainly adopted, except that in the margin-based active learning approach (Ertekin</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>Blitzer J., M. Dredze and F. Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of ACL-07, 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>V Mittal</author>
<author>M Datar</author>
</authors>
<title>Comparative Experiments on Sentiment Classification for Online Product Reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI-06,</booktitle>
<pages>1265--1270</pages>
<contexts>
<context position="1812" citStr="Cui et al., 2006" startWordPosition="234" endWordPosition="237">e, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of * Corresponding author 139 a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classifi</context>
</contexts>
<marker>Cui, Mittal, Datar, 2006</marker>
<rawString>Cui H., V. Mittal, and M. Datar. 2006. Comparative Experiments on Sentiment Classification for Online Product Reviews. In Proceedings of AAAI-06, pp.1265-1270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Doyle</author>
<author>J Monaco</author>
<author>M Feldman</author>
<author>J Tomaszewski</author>
<author>A Madabhushi</author>
</authors>
<title>An Active Learning based Classification Strategy for the Minority Class Problem: Application to Histopathology Annotation.</title>
<date>2011</date>
<journal>BMC Bioinformatics,</journal>
<volume>12</volume>
<pages>424--1471</pages>
<contexts>
<context position="9480" citStr="Doyle et al. (2011)" startWordPosition="1314" endWordPosition="1317">ctive selection strategy to address active learning in imbalanced classification, which completely ignores the class imbalance problem in the selected samples. Attenberg and Provost (2010) highlights the importance of selecting samples by considering the proportion of the classes. Their simulation experiment on text categorization confirms that selecting class-balanced samples is more important than traditional active selection strategies like uncertainty. However, the proposed experiment is simulated and non real strategy is proposed to balance the class distribution of the selected samples. Doyle et al. (2011) propose a real strategy to select balanced samples. They first select a set of uncertainty samples and then randomly select balanced samples from the uncertainty-sample set. However, the classifier used for selecting balanced samples is the same as the one for supervising uncertainty, which makes the balance control unreliable (the selected uncertainty samples take very low confidences which are unreliable to correctly predict the class label for controlling the balance). Different from their study, our approach possesses two merits: First, two feature subspace classifiers are trained to fine</context>
<context position="21486" citStr="Doyle et al. (2011)" startWordPosition="3180" endWordPosition="3183">-testing: first get contention samples (i.e., unlabeled examples on which the member classifiers predict different labels) and then select the least confidence one among the hypotheses of different member classifiers, i.e., the aggressive strategy as described Muslea et al. (2006). Specifically, the member classifiers are two subspace classifiers trained by splitting the whole feature space into two disjoint subspaces of same size; ➢ Self-selecting: first select k uncertainty samples and then randomly select a positive and negative sample from the uncertainty-sample set, which is suggested by Doyle et al. (2011). We call it self-selecting since only one classifier is involved to measure uncertainty and predict class labels. For those approaches involving random selection of features, we run 5 times for them and report the average results. Note that the samples selected by these approaches are imbalanced. To address the problem of classification on imbalanced data, we adopt the under-sampling strategy which has been shown effective for supervised imbalanced sentiment classification (Li et al., 2011a). Our active learning approach includes two versions: the co-selecting algorithm as described in Sectio</context>
</contexts>
<marker>Doyle, Monaco, Feldman, Tomaszewski, Madabhushi, 2011</marker>
<rawString>Doyle S., J. Monaco, M. Feldman, J. Tomaszewski and A. Madabhushi. 2011. An Active Learning based Classification Strategy for the Minority Class Problem: Application to Histopathology Annotation. BMC Bioinformatics, 12: 424, 1471-2105.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Ertekin</author>
<author>J Huang</author>
<author>L Bottou</author>
<author>C Giles</author>
</authors>
<booktitle>2007a. Learning on the Border: Active Learning in Imbalanced Data Classification. In Proceedings of CIKM-07,</booktitle>
<pages>127--136</pages>
<marker>Ertekin, Huang, Bottou, Giles, </marker>
<rawString>Ertekin S., J. Huang, L. Bottou and C. Giles. 2007a. Learning on the Border: Active Learning in Imbalanced Data Classification. In Proceedings of CIKM-07, 127-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ertekin</author>
<author>J Huang</author>
<author>L Bottou</author>
<author>C Giles</author>
</authors>
<date>2007</date>
<booktitle>Active Learning in Class Imbalanced Problem. In Proceedings of SIGIR-07,</booktitle>
<pages>823--824</pages>
<contexts>
<context position="8758" citStr="Ertekin et al. (2007" startWordPosition="1215" endWordPosition="1218">rgest disagreement among several committee classifiers. Besides query by committee (QBC) as the first of such type (Freund et al., 1997), co-testing learns a committee of member classifiers from different views and selects those contention points (i.e., unlabeled examples on which the views predict different labels) for manual annotation (Muslea et al., 2006). However, most previous studies focus on the scenario of balanced class distribution and only a few recent studies address the active learning issue on imbalanced classification problems including Yang and Ma (2010), Zhu and Hovy (2007), Ertekin et al. (2007a) and Ertekin et al. (2007b)2 . Unfortunately, they straightly adopt the uncertainty sampling as the active selection strategy to address active learning in imbalanced classification, which completely ignores the class imbalance problem in the selected samples. Attenberg and Provost (2010) highlights the importance of selecting samples by considering the proportion of the classes. Their simulation experiment on text categorization confirms that selecting class-balanced samples is more important than traditional active selection strategies like uncertainty. However, the proposed experiment is </context>
<context position="10211" citStr="Ertekin et al. (2007" startWordPosition="1420" endWordPosition="1423">ndomly select balanced samples from the uncertainty-sample set. However, the classifier used for selecting balanced samples is the same as the one for supervising uncertainty, which makes the balance control unreliable (the selected uncertainty samples take very low confidences which are unreliable to correctly predict the class label for controlling the balance). Different from their study, our approach possesses two merits: First, two feature subspace classifiers are trained to finely integrate the certainty and uncertainty measurements. Second, the AM samples are automatically annotated, 2 Ertekin et al. (2007a) and Ertekin et al. (2007b) select samples closest to the hyperplane provided by the SVM classifier (within the margin). Their strategy can be seen as a special case of uncertainty sampling. which reduces the annotation cost in a further effort. 3 Active Learning for Imbalanced Sentiment Classification Generally, active learning can be either streambased or pool-based (Sassano, 2002). The main difference between the two is that the former scans through the data sequentially and selects informative samples individually, whereas the latter evaluates and ranks the entire collection before selec</context>
<context position="19247" citStr="Ertekin et al., 2007" startWordPosition="2862" endWordPosition="2865">, 2007). For each domain, we randomly select an initial balanced labeled data with 50 negative samples and 50 positive samples. For the unlabeled data, we randomly select 2000 negative samples, and 14580/12160/7140/7560 positive samples from the four domains respectively, keeping the same imbalanced ratio as the whole data. For the test data in each domain, we randomly extract 800 negative samples and 800 positive samples. Classification algorithm The Maximum Entropy (ME) classifier implemented with the Mallet 3 tool is mainly adopted, except that in the margin-based active learning approach (Ertekin et al., 2007a) where SVM is implemented with light-SVM 4 . The features for classification are unigram words with Boolean weights. 3 http://mallet.cs.umass.edu/ 4 http://www.cs.cornell.edu/people/tj/svm_light/ 143 Figure 4: Performance comparison of different active learning approaches on imbalanced sentiment classification Book DVD Electronic Kitchen 0.78 0.76 0.74 0.72 G-mean 0.7 0.68 0.66 0.64 0.62 Random SVM-based Uncertainty Certainty Co-testing Self-selecting Co-selecting-basic Co-selecting-plus Evaluation metrics The popular geometric mean G - mean= TPrate  TI rate is adopted, where TPrate is the </context>
</contexts>
<marker>Ertekin, Huang, Bottou, Giles, 2007</marker>
<rawString>Ertekin S., J. Huang, L. Bottou and C. Giles. 2007b. Active Learning in Class Imbalanced Problem. In Proceedings of SIGIR-07, 823-824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>H Seung</author>
<author>E Shamir</author>
<author>N Tishby</author>
</authors>
<title>Selective Sampling using the Query by Committee algorithm.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<volume>28</volume>
<issue>2</issue>
<pages>133--168</pages>
<contexts>
<context position="8274" citStr="Freund et al., 1997" startWordPosition="1143" endWordPosition="1146">xtensively studied in many research communities and several approaches have been proposed to address this problem (Settles, 2009). Based on different sample selection strategies, they can be grouped into two main categories: (1) uncertainty sampling (Lewis and Gale, 2004) where the active learner iteratively select most uncertain unlabeled samples for manual annotation; and (2) committee-based 140 sampling where the active learner selects those unlabeled samples which have the largest disagreement among several committee classifiers. Besides query by committee (QBC) as the first of such type (Freund et al., 1997), co-testing learns a committee of member classifiers from different views and selects those contention points (i.e., unlabeled examples on which the views predict different labels) for manual annotation (Muslea et al., 2006). However, most previous studies focus on the scenario of balanced class distribution and only a few recent studies address the active learning issue on imbalanced classification problems including Yang and Ma (2010), Zhu and Hovy (2007), Ertekin et al. (2007a) and Ertekin et al. (2007b)2 . Unfortunately, they straightly adopt the uncertainty sampling as the active selecti</context>
</contexts>
<marker>Freund, Seung, Shamir, Tishby, 1997</marker>
<rawString>Freund Y., H. Seung, E. Shamir and N. Tishby. 1997. Selective Sampling using the Query by Committee algorithm. Machine Learning, 28(2-3), 133-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>C Lin</author>
<author>H Alani</author>
</authors>
<title>Automatically Extracting Polarity-Bearing Topics for CrossDomain Sentiment Classification.</title>
<date>2011</date>
<booktitle>In Proceeding of ACL-11,</booktitle>
<pages>123--131</pages>
<contexts>
<context position="6250" citStr="He et al., 2011" startWordPosition="861" endWordPosition="864">results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iterations due to the imbalanced nature of the</context>
</contexts>
<marker>He, Lin, Alani, 2011</marker>
<rawString>He Y., C. Lin and H. Alani. 2011. Automatically Extracting Polarity-Bearing Topics for CrossDomain Sentiment Classification. In Proceeding of ACL-11, 123-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>W Gale</author>
</authors>
<title>Training Text Classifiers by Uncertainty Sampling.</title>
<date>1994</date>
<booktitle>In Proceedings of SIGIR94,</booktitle>
<pages>3--12</pages>
<marker>Lewis, Gale, 1994</marker>
<rawString>Lewis D. and W. Gale. 1994. Training Text Classifiers by Uncertainty Sampling. In Proceedings of SIGIR94, 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Li</author>
<author>Y Tang</author>
<author>M Huang</author>
<author>X Zhu</author>
</authors>
<title>Answering Opinion Questions with Random Walks on Graphs.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP-09,</booktitle>
<pages>737--745</pages>
<contexts>
<context position="1829" citStr="Li et al., 2009" startWordPosition="238" endWordPosition="241">abel most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of * Corresponding author 139 a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalance</context>
</contexts>
<marker>Li, Tang, Huang, Zhu, 2009</marker>
<rawString>Li F., Y. Tang, M. Huang and X. Zhu. 2009. Answering Opinion Questions with Random Walks on Graphs. In Proceedings of ACL-IJCNLP-09, 737-745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>C Zong</author>
</authors>
<title>Multi-domain Sentiment Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08,</booktitle>
<pages>257--260</pages>
<contexts>
<context position="6232" citStr="Li and Zong, 2008" startWordPosition="857" endWordPosition="860">s the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iterations due to the imbala</context>
</contexts>
<marker>Li, Zong, 2008</marker>
<rawString>Li S. and C. Zong. 2008. Multi-domain Sentiment Classification. In Proceedings of ACL-08, short paper, pp.257-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>C Huang</author>
<author>G Zhou</author>
<author>S Lee</author>
</authors>
<title>Employing Personal/Impersonal Views in Supervised and Semisupervised Sentiment Classification.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL-10,</booktitle>
<pages>414--423</pages>
<contexts>
<context position="6150" citStr="Li et al., 2010" startWordPosition="845" endWordPosition="848">ctive learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strat</context>
</contexts>
<marker>Li, Huang, Zhou, Lee, 2010</marker>
<rawString>Li S., C. Huang, G. Zhou and S. Lee. 2010. Employing Personal/Impersonal Views in Supervised and Semisupervised Sentiment Classification. In Proceedings of ACL-10, pp.414-423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>Z Wang</author>
<author>G Zhou</author>
<author>S Lee</author>
</authors>
<title>Semisupervised Learning for Imbalanced Sentiment Classification.</title>
<date>2011</date>
<booktitle>In Proceeding of IJCAI-11,</booktitle>
<pages>826--1831</pages>
<contexts>
<context position="2709" citStr="Li et al., 2011" startWordPosition="363" endWordPosition="366">ations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalanced sentiment classification deals with the situation in which there are many more samples of one class (called majority class) than the other class (called minority class), and has attracted much attention due to its high realistic value in real-world applications (Li et al., 2011a). In imbalanced sentiment classification, since the minority-class samples (denoted as MI samples) are normally much sparse and thus more precious and informative for learning compared to the majority-class ones (denoted as MA samples), it is worthwhile to spend more on manually annotating MI samples to guarantee both the quality and quantity of MI samples. Traditionally, uncertainty has been popularly used as a basic measurement in active learning (Lewis and Gale, 2004). Therefore, how to select most informative MI samples for manual annotation without violating the basic Proceedings of the</context>
<context position="6382" citStr="Li et al. (2011" startWordPosition="882" endWordPosition="885">ew on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iterations due to the imbalanced nature of the unbalanced data. In comparison, our proposed active learning approach can effectively avoid this problem. By the way, it is worth t</context>
<context position="18526" citStr="Li et al. (2011" startWordPosition="2753" endWordPosition="2756"> real applications, it is not difficult to know this. We first use a classifier trained with the initial labeled data to test all unlabeled data. If the predicted labels in the classification results are greatly imbalanced, we can assume that the unlabeled data is imbalanced, and consider the dominated class as majority class. 4 Experimentation In this section, we will systematically evaluate our active learning approach for imbalanced sentiment classification and compare it with the state-of-theart active learning alternatives. 4.1 Experimental Setting Dataset We use the same data as used by Li et al. (2011a). The data collection consists of four domains: Book, DVD, Electronic, and Kitchen ( Blitzer et al., 2007). For each domain, we randomly select an initial balanced labeled data with 50 negative samples and 50 positive samples. For the unlabeled data, we randomly select 2000 negative samples, and 14580/12160/7140/7560 positive samples from the four domains respectively, keeping the same imbalanced ratio as the whole data. For the test data in each domain, we randomly extract 800 negative samples and 800 positive samples. Classification algorithm The Maximum Entropy (ME) classifier implemented</context>
<context position="21981" citStr="Li et al., 2011" startWordPosition="3254" endWordPosition="3257">andomly select a positive and negative sample from the uncertainty-sample set, which is suggested by Doyle et al. (2011). We call it self-selecting since only one classifier is involved to measure uncertainty and predict class labels. For those approaches involving random selection of features, we run 5 times for them and report the average results. Note that the samples selected by these approaches are imbalanced. To address the problem of classification on imbalanced data, we adopt the under-sampling strategy which has been shown effective for supervised imbalanced sentiment classification (Li et al., 2011a). Our active learning approach includes two versions: the co-selecting algorithm as described in Section 3.2 and the co-selecting with selected MA samples automatically labeled as described in Section 3.3. For clarity, we refer the former as co-selecting-basic and the latter as coselecting-plus in the following. 144 0.78 0.76 0.74 0.72 0.68 0.78 0.76 0.74 0.72 0.68 0.66 0.64 0.62 0.8 0.7 0.7 300 600 900 1200 1500 2400 4800 7000 Number of the manually annoated samples 300 600 900 1200 1500 2400 4800 9600 Nubmer of the manually annotated samples Random Co-testing Co-selecting-plus Random Co-te</context>
</contexts>
<marker>Li, Wang, Zhou, Lee, 2011</marker>
<rawString>Li S., Z. Wang, G. Zhou and S. Lee. 2011a. Semisupervised Learning for Imbalanced Sentiment Classification. In Proceeding of IJCAI-11, 826-1831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Li</author>
<author>G Zhou</author>
<author>Z Wang</author>
<author>S Lee</author>
<author>R Wang</author>
</authors>
<title>Imbalanced Sentiment Classification.</title>
<date>2011</date>
<booktitle>In Proceedings of CIKM-11,</booktitle>
<pages>2469--2472</pages>
<contexts>
<context position="2709" citStr="Li et al., 2011" startWordPosition="363" endWordPosition="366">ations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalanced sentiment classification deals with the situation in which there are many more samples of one class (called majority class) than the other class (called minority class), and has attracted much attention due to its high realistic value in real-world applications (Li et al., 2011a). In imbalanced sentiment classification, since the minority-class samples (denoted as MI samples) are normally much sparse and thus more precious and informative for learning compared to the majority-class ones (denoted as MA samples), it is worthwhile to spend more on manually annotating MI samples to guarantee both the quality and quantity of MI samples. Traditionally, uncertainty has been popularly used as a basic measurement in active learning (Lewis and Gale, 2004). Therefore, how to select most informative MI samples for manual annotation without violating the basic Proceedings of the</context>
<context position="6382" citStr="Li et al. (2011" startWordPosition="882" endWordPosition="885">ew on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanced selection strategy in co-training would generate many errors in late iterations due to the imbalanced nature of the unbalanced data. In comparison, our proposed active learning approach can effectively avoid this problem. By the way, it is worth t</context>
<context position="18526" citStr="Li et al. (2011" startWordPosition="2753" endWordPosition="2756"> real applications, it is not difficult to know this. We first use a classifier trained with the initial labeled data to test all unlabeled data. If the predicted labels in the classification results are greatly imbalanced, we can assume that the unlabeled data is imbalanced, and consider the dominated class as majority class. 4 Experimentation In this section, we will systematically evaluate our active learning approach for imbalanced sentiment classification and compare it with the state-of-theart active learning alternatives. 4.1 Experimental Setting Dataset We use the same data as used by Li et al. (2011a). The data collection consists of four domains: Book, DVD, Electronic, and Kitchen ( Blitzer et al., 2007). For each domain, we randomly select an initial balanced labeled data with 50 negative samples and 50 positive samples. For the unlabeled data, we randomly select 2000 negative samples, and 14580/12160/7140/7560 positive samples from the four domains respectively, keeping the same imbalanced ratio as the whole data. For the test data in each domain, we randomly extract 800 negative samples and 800 positive samples. Classification algorithm The Maximum Entropy (ME) classifier implemented</context>
<context position="21981" citStr="Li et al., 2011" startWordPosition="3254" endWordPosition="3257">andomly select a positive and negative sample from the uncertainty-sample set, which is suggested by Doyle et al. (2011). We call it self-selecting since only one classifier is involved to measure uncertainty and predict class labels. For those approaches involving random selection of features, we run 5 times for them and report the average results. Note that the samples selected by these approaches are imbalanced. To address the problem of classification on imbalanced data, we adopt the under-sampling strategy which has been shown effective for supervised imbalanced sentiment classification (Li et al., 2011a). Our active learning approach includes two versions: the co-selecting algorithm as described in Section 3.2 and the co-selecting with selected MA samples automatically labeled as described in Section 3.3. For clarity, we refer the former as co-selecting-basic and the latter as coselecting-plus in the following. 144 0.78 0.76 0.74 0.72 0.68 0.78 0.76 0.74 0.72 0.68 0.66 0.64 0.62 0.8 0.7 0.7 300 600 900 1200 1500 2400 4800 7000 Number of the manually annoated samples 300 600 900 1200 1500 2400 4800 9600 Nubmer of the manually annotated samples Random Co-testing Co-selecting-plus Random Co-te</context>
</contexts>
<marker>Li, Zhou, Wang, Lee, Wang, 2011</marker>
<rawString>Li S., G. Zhou, Z. Wang, S. Lee and R. Wang. 2011b. Imbalanced Sentiment Classification. In Proceedings of CIKM-11, poster paper, 2469-2472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lloret</author>
<author>A Balahur</author>
<author>M Palomar</author>
<author>A Montoyo</author>
</authors>
<title>Towards Building a Competitive Opinion Summarization System. In</title>
<date>2009</date>
<booktitle>Proceedings of NAACL09 Student Research Workshop and Doctoral Consortium,</booktitle>
<pages>72--77</pages>
<contexts>
<context position="1850" citStr="Lloret et al., 2009" startWordPosition="242" endWordPosition="245">tive majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of * Corresponding author 139 a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalanced sentiment classific</context>
</contexts>
<marker>Lloret, Balahur, Palomar, Montoyo, 2009</marker>
<rawString>Lloret E., A. Balahur, M. Palomar, and A. Montoyo. 2009. Towards Building a Competitive Opinion Summarization System. In Proceedings of NAACL09 Student Research Workshop and Doctoral Consortium, 72-77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kubat</author>
<author>S Matwin</author>
</authors>
<title>Addressing the Curse of Imbalanced Training Sets: One-Sided Selection.</title>
<date>1997</date>
<booktitle>In Proceedings of ICML-97,</booktitle>
<pages>179--186</pages>
<contexts>
<context position="20018" citStr="Kubat and Matwin, 1997" startWordPosition="2966" endWordPosition="2969"> 4 http://www.cs.cornell.edu/people/tj/svm_light/ 143 Figure 4: Performance comparison of different active learning approaches on imbalanced sentiment classification Book DVD Electronic Kitchen 0.78 0.76 0.74 0.72 G-mean 0.7 0.68 0.66 0.64 0.62 Random SVM-based Uncertainty Certainty Co-testing Self-selecting Co-selecting-basic Co-selecting-plus Evaluation metrics The popular geometric mean G - mean= TPrate  TI rate is adopted, where TPrate is the true positive rate (also called positive recall or sensitivity) and TI rate is the true negative rate (also called negative recall or specificity) (Kubat and Matwin, 1997). 4.2 Experimental Results For thorough comparison, various kinds of active learning approaches are implemented including: ➢ Random: randomly select the samples from the unlabeled data for manual annotation; ➢ Margin-based: iteratively select samples closest to the hyperplane provided by the SVM classifier, which is suggested by Ertekin et al. (2007a) and Ertekin et al. (2007b). One sample is selected in each iteration; ➢ Uncertainty: iteratively select samples using the uncertainty measurement according to the output of ME classifier. One sample is selected in each iteration; ➢ Certainty: ite</context>
</contexts>
<marker>Kubat, Matwin, 1997</marker>
<rawString>Kubat M. and S. Matwin. 1997. Addressing the Curse of Imbalanced Training Sets: One-Sided Selection. In Proceedings of ICML-97, 179–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Muslea</author>
<author>S Minton</author>
<author>C Knoblock</author>
</authors>
<title>Active Learning with Multiple Views.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>27</volume>
<pages>203--233</pages>
<contexts>
<context position="5369" citStr="Muslea et al., 2006" startWordPosition="733" endWordPosition="736"> in two main aspects. First, a certainty measurement and an uncertainty measurement are employed in two complementary subspace classifiers respectively to collectively select most informative MI samples for manual annotation. Second, most informative MA samples are automatically labeled to further reduce the annotation cost. Evaluation across four domains shows that our active learning approach is effective for imbalanced sentiment classification and significantly outperforms the state-of-the-art active learning alternatives, such as uncertainty sampling (Lewis and Gale, 2004) and co-testing (Muslea et al., 2006). The remainder of this paper is organized as follows. Section 2 overviews the related work on sentiment classification and active learning. Section 3 proposes our active learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods hav</context>
<context position="8499" citStr="Muslea et al., 2006" startWordPosition="1175" endWordPosition="1178">(1) uncertainty sampling (Lewis and Gale, 2004) where the active learner iteratively select most uncertain unlabeled samples for manual annotation; and (2) committee-based 140 sampling where the active learner selects those unlabeled samples which have the largest disagreement among several committee classifiers. Besides query by committee (QBC) as the first of such type (Freund et al., 1997), co-testing learns a committee of member classifiers from different views and selects those contention points (i.e., unlabeled examples on which the views predict different labels) for manual annotation (Muslea et al., 2006). However, most previous studies focus on the scenario of balanced class distribution and only a few recent studies address the active learning issue on imbalanced classification problems including Yang and Ma (2010), Zhu and Hovy (2007), Ertekin et al. (2007a) and Ertekin et al. (2007b)2 . Unfortunately, they straightly adopt the uncertainty sampling as the active selection strategy to address active learning in imbalanced classification, which completely ignores the class imbalance problem in the selected samples. Attenberg and Provost (2010) highlights the importance of selecting samples by</context>
<context position="21148" citStr="Muslea et al. (2006)" startWordPosition="3130" endWordPosition="3133">to the output of ME classifier. One sample is selected in each iteration; ➢ Certainty: iteratively select class-balanced samples using the certainty measurement according to the output of ME classifier. One positive and negative sample (the positive and negative label is provided by the ME classifier) are selected in each iteration; ➢ Co-testing: first get contention samples (i.e., unlabeled examples on which the member classifiers predict different labels) and then select the least confidence one among the hypotheses of different member classifiers, i.e., the aggressive strategy as described Muslea et al. (2006). Specifically, the member classifiers are two subspace classifiers trained by splitting the whole feature space into two disjoint subspaces of same size; ➢ Self-selecting: first select k uncertainty samples and then randomly select a positive and negative sample from the uncertainty-sample set, which is suggested by Doyle et al. (2011). We call it self-selecting since only one classifier is involved to measure uncertainty and predict class labels. For those approaches involving random selection of features, we run 5 times for them and report the average results. Note that the samples selected</context>
</contexts>
<marker>Muslea, Minton, Knoblock, 2006</marker>
<rawString>Muslea I., S. Minton and C. Knoblock . 2006. Active Learning with Multiple Views. Journal of Artificial Intelligence Research, vol.27, 203-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis: Foundations and Trends.</title>
<date>2008</date>
<journal>Information Retrieval,</journal>
<volume>2</volume>
<issue>12</issue>
<pages>1--135</pages>
<marker>Pang, Lee, 2008</marker>
<rawString>Pang B. and L. Lee. 2008. Opinion Mining and Sentiment Analysis: Foundations and Trends. Information Retrieval, vol.2(12), 1-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-02,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1694" citStr="Pang et al., 2002" startWordPosition="215" endWordPosition="218">samples for manual annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of * Corresponding author 139 a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active l</context>
<context position="6087" citStr="Pang et al., 2002" startWordPosition="836" endWordPosition="839">ment classification and active learning. Section 3 proposes our active learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classificati</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang B., L. Lee and S. Vaithyanathan. 2002.Thumbs up? Sentiment Classification using Machine Learning Techniques. In Proceedings of EMNLP-02, 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>Active Learning Literature Survey. Computer Sciences</title>
<date>2009</date>
<tech>Technical Report 1648,</tech>
<institution>University of Wisconsin,</institution>
<location>Madison,</location>
<contexts>
<context position="7783" citStr="Settles, 2009" startWordPosition="1074" endWordPosition="1075">n. Li et al. (2011b) focus on supervised learning for imbalanced sentiment classification and propose a clustering-based approach to improve traditional under-sampling approaches. However, the improvement of the proposed clustering-based approach over under-sampling is very limited. Unlike all the studies mentioned above, our study pioneers active learning on imbalanced sentiment classification. 2.2 Active Learning Active leaning, as a standard machine learning problem, has been extensively studied in many research communities and several approaches have been proposed to address this problem (Settles, 2009). Based on different sample selection strategies, they can be grouped into two main categories: (1) uncertainty sampling (Lewis and Gale, 2004) where the active learner iteratively select most uncertain unlabeled samples for manual annotation; and (2) committee-based 140 sampling where the active learner selects those unlabeled samples which have the largest disagreement among several committee classifiers. Besides query by committee (QBC) as the first of such type (Freund et al., 1997), co-testing learns a committee of member classifiers from different views and selects those contention point</context>
</contexts>
<marker>Settles, 2009</marker>
<rawString>Settles B. 2009. Active Learning Literature Survey. Computer Sciences Technical Report 1648, University of Wisconsin, Madison, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or Thumbs down? Semantic Orientation Applied to Unsupervised Classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-02,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="1709" citStr="Turney, 2002" startWordPosition="219" endWordPosition="220">annotation by leveraging a certainty measurement and an uncertainty measurement, and in the meanwhile, automatically label most informative majority-class samples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of * Corresponding author 139 a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imb</context>
<context position="6038" citStr="Turney, 2002" startWordPosition="831" endWordPosition="832">ection 2 overviews the related work on sentiment classification and active learning. Section 3 proposes our active learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervi</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney P. 2002. Thumbs up or Thumbs down? Semantic Orientation Applied to Unsupervised Classification of reviews. In Proceedings of ACL-02, 417-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Co-Training for Cross-Lingual Sentiment Classification.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP-09,</booktitle>
<pages>235--243</pages>
<contexts>
<context position="6132" citStr="Wan, 2009" startWordPosition="843" endWordPosition="844">poses our active learning approach for imbalanced sentiment classification. Section 4 reports the experimental results. Finally, Section 5 draws the conclusion and outlines the future work. 2 Related Work In this section, we give a brief overview on sentiment classification and active learning. 2.1 Sentiment Classification Sentiment classification has become a hot research topic in NLP community and various kinds of classification methods have been proposed, such as unsupervised learning methods (Turney, 2002), supervised learning methods (Pang et al., 2002), semi-supervised learning methods (Wan, 2009; Li et al., 2010), and cross-domain classification methods (Blitzer et al., 2007; Li and Zong, 2008; He et al., 2011). However, imbalanced sentiment classification is relatively new and there are only a few studies in the literature. Li et al. (2011a) pioneer the research in imbalanced sentiment classification and propose a co-training algorithm to perform semi-supervised learning for imbalanced sentiment classification with the help of a great amount of unlabeled samples. However, their semi-supervised approach to imbalanced sentiment classification suffers from the problem that their balanc</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Wan X. 2009. Co-Training for Cross-Lingual Sentiment Classification. In Proceedings of ACL-IJCNLP-09, 235–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>G Ma</author>
</authors>
<title>Ensemble-based Active Learning for Class Imbalance Problem.</title>
<date>2010</date>
<journal>J. Biomedical Science and Engineering,</journal>
<pages>3--1021</pages>
<contexts>
<context position="8715" citStr="Yang and Ma (2010)" startWordPosition="1207" endWordPosition="1210">those unlabeled samples which have the largest disagreement among several committee classifiers. Besides query by committee (QBC) as the first of such type (Freund et al., 1997), co-testing learns a committee of member classifiers from different views and selects those contention points (i.e., unlabeled examples on which the views predict different labels) for manual annotation (Muslea et al., 2006). However, most previous studies focus on the scenario of balanced class distribution and only a few recent studies address the active learning issue on imbalanced classification problems including Yang and Ma (2010), Zhu and Hovy (2007), Ertekin et al. (2007a) and Ertekin et al. (2007b)2 . Unfortunately, they straightly adopt the uncertainty sampling as the active selection strategy to address active learning in imbalanced classification, which completely ignores the class imbalance problem in the selected samples. Attenberg and Provost (2010) highlights the importance of selecting samples by considering the proportion of the classes. Their simulation experiment on text categorization confirms that selecting class-balanced samples is more important than traditional active selection strategies like uncert</context>
</contexts>
<marker>Yang, Ma, 2010</marker>
<rawString>Yang Y. and G. Ma. 2010. Ensemble-based Active Learning for Class Imbalance Problem. J. Biomedical Science and Engineering, vol.3,1021-1028.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>X Ye</author>
</authors>
<title>A Generation Model to Unify Topic Relevance and Lexicon-based Sentiment for Opinion Retrieval.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR-08,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="1871" citStr="Zhang and Ye, 2008" startWordPosition="246" endWordPosition="249">amples, to reduce humanannotation efforts. Extensive experiments across four domains demonstrate great potential and effectiveness of our proposed co-selecting approach to active learning for imbalanced sentiment classification. 1 Introduction Sentiment classification is the task of identifying the sentiment polarity (e.g., positive or negative) of * Corresponding author 139 a natural language text towards a given topic (Pang et al., 2002; Turney, 2002) and has become the core component of many important applications in opinion analysis (Cui et al., 2006; Li et al., 2009; Lloret et al., 2009; Zhang and Ye, 2008). Most of previous studies in sentiment classification focus on learning models from a large number of labeled data. However, in many real-world applications, manual annotation is expensive and time-consuming. In these situations, active learning approaches could be helpful by actively selecting most informative samples for manual annotation. Compared to traditional active learning for sentiment classification, active learning for imbalanced sentiment classification faces some unique challenges. As a specific type of sentiment classification, imbalanced sentiment classification deals with the </context>
</contexts>
<marker>Zhang, Ye, 2008</marker>
<rawString>Zhang M. and X. Ye. 2008. A Generation Model to Unify Topic Relevance and Lexicon-based Sentiment for Opinion Retrieval. In Proceedings of SIGIR-08, 411-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhu</author>
<author>E Hovy</author>
</authors>
<title>Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-07,</booktitle>
<pages>783--793</pages>
<contexts>
<context position="8736" citStr="Zhu and Hovy (2007)" startWordPosition="1211" endWordPosition="1214">les which have the largest disagreement among several committee classifiers. Besides query by committee (QBC) as the first of such type (Freund et al., 1997), co-testing learns a committee of member classifiers from different views and selects those contention points (i.e., unlabeled examples on which the views predict different labels) for manual annotation (Muslea et al., 2006). However, most previous studies focus on the scenario of balanced class distribution and only a few recent studies address the active learning issue on imbalanced classification problems including Yang and Ma (2010), Zhu and Hovy (2007), Ertekin et al. (2007a) and Ertekin et al. (2007b)2 . Unfortunately, they straightly adopt the uncertainty sampling as the active selection strategy to address active learning in imbalanced classification, which completely ignores the class imbalance problem in the selected samples. Attenberg and Provost (2010) highlights the importance of selecting samples by considering the proportion of the classes. Their simulation experiment on text categorization confirms that selecting class-balanced samples is more important than traditional active selection strategies like uncertainty. However, the p</context>
</contexts>
<marker>Zhu, Hovy, 2007</marker>
<rawString>Zhu J. and E. Hovy. 2007. Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem. In Proceedings of ACL-07, 783-793.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
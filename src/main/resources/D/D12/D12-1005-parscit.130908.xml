<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.726365">
Streaming Analysis of Discourse Participants
</title>
<author confidence="0.981429">
Benjamin Van Durme
</author>
<affiliation confidence="0.9787085">
Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.988379" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999569157894737">
Inferring attributes of discourse participants
has been treated as a batch-processing task:
data such as all tweets from a given author
are gathered in bulk, processed, analyzed for
a particular feature, then reported as a result
of academic interest. Given the sources and
scale of material used in these efforts, along
with potential use cases of such analytic tools,
discourse analysis should be reconsidered as
a streaming challenge. We show that un-
der certain common formulations, the batch-
processing analytic framework can be decom-
posed into a sequential series of updates, us-
ing as an example the task of gender classifi-
cation. Once in a streaming framework, and
motivated by large data sets generated by so-
cial media services, we present novel results in
approximate counting, showing its applicabil-
ity to space efficient streaming classification.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916538461538">
The rapid growth in social media has led to an
equally rapid growth in the desire to mine it for use-
ful information: the content of public discussions,
such as found in tweets, or in posts to online forums,
can support a variety of data mining tasks. Infer-
ring the underlying properties of those that engage
with these platforms, the discourse participants, has
become an active topic of research: predicting indi-
vidual attributes such as age, gender, and political
preferences (Rao et al., 2010), or relationships be-
tween communicants, such as organizational domi-
nance (Diehl et al., 2007). This research can bene-
fit areas such as: (A) commercial applications, e.g.,
</bodyText>
<page confidence="0.987968">
48
</page>
<bodyText confidence="0.999674117647059">
improved models for advertising placement, or de-
tecting fraudulent or otherwise unhelpful product re-
views (Jindal and Liu, 2008; Ott et al., 2011); and
(B) in enhanced models of civic discourse, e.g., in-
expensive, large-scale, passive polling of popular
opinion (O’Connor et al., 2010).
Classification with streaming data has usually
been taken in the computational linguistics commu-
nity to mean individual decisions made on items that
are presented over time. For example: assigning
a label to each newly posted product review as to
whether it contains positive or negative sentiment,
or whether the latest tweet signals a novel topic that
should be tagged for tracking (Petrovic et al., 2010).
Here we consider a distinct form of stream-based
classification: we wish to assign, then dynamically
update, labels on discourse participants based on
their associated streaming communications. For in-
stance, rather than classifying individual reviews as
to their sentiment polarity, we might wish to classify
the underlying author as to whether they are gen-
uine or paid-advertising, and then update that deci-
sion as they continue to post new reviews. As the
scale of social media continues to grow, we desire
that our model be aggressively space efficient, which
precludes a naive solution of storing the full commu-
nication history for all users.
In this paper we make two contributions: (1) we
make explicit that a standard bag-of-words classifi-
cation model for predicting latent author attributes
can be simply decomposed into a series of stream-
ing updates; then (2) show how the randomized al-
gorithm, Reservoir Counting (Van Durme and Lall,
2011), can be extended to maintain approximate av-
</bodyText>
<note confidence="0.936792833333333">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 48–58, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
erages, allowing for significant space savings in our w · f(C) = Xd wj fj(C)
classification model. Our running example task is j=1
gender prediction, based on spoken communication
and microblogs/Twitter feeds.
</note>
<sectionHeader confidence="0.953768" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.9992065">
Assume that each discourse participant (e.g.,
speaker, author) a has an associated stream of com-
munications (e.g., tweets, utterances, emails, etc.):
(ci) = C. Then let Ct = (c1, ..., ct) represent the
first t elements of C.
Assume access to a pretrained classifier Φ:1
</bodyText>
<equation confidence="0.945434666666667">
~ 1 if w · f(C) ≥ 0,
Φ(a) =
0 otherwise,
</equation>
<bodyText confidence="0.997789875">
which we initially take to be linear: author labels are
determined by computing the sign of the dot product
between a weight vector w, and feature vector f(C),
each of dimension d. Note that f(C) is a feature
vector over the entire set of communications from a
given author.
For example, Φ might be trained to classify author
gender:
</bodyText>
<equation confidence="0.712696">
~ Male if w · f(C) ≥ 0,
Gender(a) =
Female otherwise.
</equation>
<bodyText confidence="0.995906">
We now make explicit how under certain common
restrictions on the feature space, the classification
decision can be decomposed into a series of decision
updates over the elements of C.
Define ˆf(ci) to be the vector containing the lo-
cal, count-based feature values of communication
cj.2 For convenience let us assume that f (ci) ∈ Nd.
Where |v|1 = Pi |vi |is the L1-norm of vector v, let
zt be the normalizing constant at t:
Now define fj(C), the j-th entry of f(C), as:
</bodyText>
<equation confidence="0.92091125">
Pn ˆfj(ci)
i=1
fj(C) =
zn
</equation>
<bodyText confidence="0.993474666666667">
Thus f(C) represents the global relative fre-
quency of each local, count-based feature. This al-
lows us to rearrange terms:
</bodyText>
<footnote confidence="0.935219">
1While here we assume binary decision tasks, dynamic clas-
sification in a multiclass, or regression, setting is an interesting
avenue of exploration, for which these definitions generalize.
2As seen later in Table 1, we have in mind features such as
the frequency of the n-gram my wife.
</footnote>
<bodyText confidence="0.995607">
which pairs the observed rolling sum, st with the
feature stream length zt.
The classifier decision after seeing everything up
to and including communication ct is thus a simple
average:
</bodyText>
<equation confidence="0.9958238">
�Φt (a) = 1 if zt ≥ 0,
0 otherwise.
Finally we reach the observation that:
st = st−1 + w · ˆf(ct)
zt = zt−1 +  |ˆf(ct)|1
</equation>
<bodyText confidence="0.956592166666667">
which means that from an engineering standpoint we
can process a stream of communication one element
at a time, without the need to preserve the history
explicitly. That is: for each author, for each attribute
being analyzed, an online system only need main-
tain a state pair (st, zt) by extracting and weighting
features locally for each new communication. Be-
yond the computational savings of not needing to
store communications nor explicit feature vectors in
memory, there are potential privacy benefits as well:
analytic systems need not have a lasting record of
discourse, they can instead glean whatever signal is
required locally in the stream, and then discard the
actual communications.
Log-linear Rather than a strictly linear Φ, such as
instantiated via perceptron or SVM with linear ker-
nel, many prefer log-linear models as their classifi-
cation framework:
</bodyText>
<equation confidence="0.9916021875">
1
Φ(a) = 1 if 1+exp(−w·f(C)) ≥ 0.5,
0 otherwise.
= 1 Xd wk( Xn ˆfj(ci))
zn j=1 i=1
wk ˆfj(ci))
Let (st, zt) be the current state of the classifier:
(st, zt) = . ( Xt Xd wk ˆfk(cj), zt)
i=1 k=1
= 1 Xn d
zn i=1 (X
=1
Xt
i=1
zt =
 |ˆf(ci)|1
</equation>
<page confidence="0.999265">
49
</page>
<figure confidence="0.99632175">
50% 90% 5% 85% 86%
50% 10% 95% 15% 14%
C1 C2 ... Ct-1 Ct
...
</figure>
<figureCaption confidence="0.984058">
Figure 1: A streaming analytic model should update its decision with each new communication, becoming more
stable in its prediction as evidence is acquired.
</figureCaption>
<bodyText confidence="0.999612666666667">
In either setting, the state of the classifier is suf-
ficiently captured by the pair (st, zt), under the re-
strictions on f.3
</bodyText>
<subsectionHeader confidence="0.972378">
2.1 Validation
</subsectionHeader>
<bodyText confidence="0.9999767">
As an example of a model decomposed into a
stream, we revist the task of gender classifica-
tion based on speech transcripts, as explored by
Boulis and Ostendorf (2005) and later Garera and
Yarowsky (2009). In the original problem definition,
one would collect all transcribed utterances from a
given speaker in a corpus such as Fisher (Cieri et
al., 2004) or Switchboard (Godfrey et al., 1992),
known as a side of the conversation. Then by col-
lapsing these utterances into a single document, one
could classify it as to whether it was generated by a
male or female. Here we define the task as: starting
from scratch, report the classifier probability of the
speaker being male, as each utterance is presented.
Intuitively we would expect that as more utter-
ances are observed, the better our classification ac-
curacy. Researchers such as Burger et al. (2011)
have considered this point, but by comparing the
classification accuracy based on the volume of batch
data available per author (in that case, tweets): the
more prolific the author had been, the better able
they were to correctly classify their gender. We con-
firm here this can be reframed: as a speaker (author)
continues to emit a stream of communication, a dy-
namic model tends to improve its online prediction.
Our collection based on Switchboard consisted
of 520 unique speakers (240 female, 280 male),
with a total of roughly 400k utterances. Simi-
lar to Boulis and Ostendorf, we extracted unigram
and bigram counts as features, but without further
</bodyText>
<footnote confidence="0.91551725">
3Note that some non-linear kernels can be maintained online
in a similar fashion. For instance, a polynomial kernel of degree
p decomposes as: (f(C�) · w)P = (��
�� )P.
</footnote>
<figureCaption confidence="0.993466333333333">
Figure 2: Accuracy on Switchboard gender classifica-
tion, reported at every fifth utterance, using a dynamic
log-linear model with 10-fold cross validation.
</figureCaption>
<bodyText confidence="0.999398833333333">
TFIDF reweighting. Ngrams were required to oc-
cur at least 10 times in the training set, recom-
puted for each split of 10-fold cross validation.
Weights were computed under a log-linear model
using LibLinear (Fan et al., 2008), with 5% of
training held out for tuning an L2 regularizing term.
Feature extraction and dynamic aspects were han-
dled through additions to the Jerboa package (Van
Durme, 2012). Similar to previous work, we found
intuitive features such as my husband to be weighted
heavily (see Table 1), along with certain non-lexical
vocalizations such as transcribed laughter.
</bodyText>
<tableCaption confidence="0.998632">
Table 1: Top ten features by gender.
</tableCaption>
<bodyText confidence="0.56469225">
Male a, wife, is, my wife, right, of, the, uh, ac-
tually, [vocalized-noise]
Female have, and, [laughter], my husband, really,
husband, children, are, would
</bodyText>
<figure confidence="0.9935218">
1 50 100 150 200 250
Communications
Accuracy
0.9
0.8
0.7
0.6
0.5
●
● ●
●
●
●
●
●
● ● ●
●●
50
0 50 100 150 200 250
Communications
</figure>
<figureCaption confidence="0.99786025">
Figure 3: Streaming analysis of eight randomly sam-
pled speakers, four per gender (red-solid: female, blue-
dashed: male). Being a log-linear model, the decision
boundary is marked at y = 0.5.
</figureCaption>
<bodyText confidence="0.9999816">
As seen in Figure 2, accuracy indeed improves
as more content is emitted. Figure 3 highlights the
streaming perspective: individual speakers can be
viewed as distinct trajectories through [0, 1], based
on features of their utterances.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="method">
3 Randomized Model
</sectionHeader>
<bodyText confidence="0.964697625">
Now situated within a streaming context we exact
space savings through approximation, extending the
approach of Van Durme and Lall (2011), there con-
cerned with online Locality Sensitive Hashing, here
initially concerned with taking averages.
When calculating the average over a sequence of
values, Xn = (x1, ..., xn), we divide the sum of
the sequence, sum(Xn) = �ni�1 xi, by its length,
</bodyText>
<equation confidence="0.996962">
length(Xn) = |Xn|:
avg(Xn) = sum(Xn)
length(Xn)
</equation>
<bodyText confidence="0.999634125">
Our goal in this section is to maintain a space ef-
ficient approximation of avg(Xt), as t increases, by
using a bit-saving approximation of both the sum,
and the length of the sequence.
We begin by reviewing the method of Reservoir
Counting, then extend it to a new notion we refer to
as Reservoir Averaging. This will allow in the sub-
sequent section to map our analytic model to a form
</bodyText>
<figureCaption confidence="0.785996">
Figure 4: Social media platforms such as Facebook or
</figureCaption>
<bodyText confidence="0.713322">
Twitter deal with a very large number of individuals, each
with a variety of implicit attributes (such as gender). This
motivates a desire for online space efficiency.
explicitly amenable to keeping an online average.
</bodyText>
<subsectionHeader confidence="0.993844">
3.1 Reservoir Counting
</subsectionHeader>
<bodyText confidence="0.99866225">
Reservoir Counting plays on the folklore algorithm
of reservoir sampling, first described in the literature
by Vitter (1985). As applied to a stream of arbitrary
elements, reservoir sampling maintains a list (reser-
voir) of length k, where the contents of the reser-
voir represents a uniform random sample over all el-
ements 1...t observed thus far in the stream.
When the stream is a sequence of positive
and negative integers, reservoir counting implicitly
views each value as being unrolled into a sequence
made up of either 1 or -1. For instance, the sequence:
(3, -2, 1) would be viewed as:
</bodyText>
<equation confidence="0.722347">
(1, 1, 1, -1, -1, 1)
</equation>
<bodyText confidence="0.99994725">
Since there are only two distinct values in this
stream, the contents of the reservoir can be char-
acterized by knowing the fixed value k, and then
s: how many elements in the reservoir are 1.4
This led to Van Durme and Lall defining a method,
ReservoirUpdate, here abbreviated to ResUp,
that allows for maintaining an approximate sum, de-
fined as t(k − 1), through updating these two pa-
rameters t and s with each newly observed element.
Expected accuracy of the approximation varies with
the size of the sample, k. Reservoir Counting ex-
ploits the fact that the reservoir need only be con-
sidered implicitly, where s represented as a b-bit un-
signed integer can be used to characterize a reser-
voir of size k = 2b − 1. This allowed those authors
to show a 50% space reduction in the task of online
</bodyText>
<footnote confidence="0.481223">
4As the number of -1 values is simply: k − s.
</footnote>
<figure confidence="0.996376">
a2
al
Cl C2 ... Ct-1 Ct
Cl C2 ... Ct-1 Ct
...
Cl C2 ... Ct-1 Ct
...
am
...
...
1.0
0.8
Probability of Male
0.6
0.4
0.2
0.0
</figure>
<page confidence="0.993137">
51
</page>
<bodyText confidence="0.999229">
Locality Sensitive Hashing, at similar levels of accu-
racy, by replacing explicit 32-bit counting variables
with approximate counters of smaller size. See (Van
Durme and Lall, 2011) for further details.
</bodyText>
<subsectionHeader confidence="0.999378">
3.2 Reservoir Averaging
</subsectionHeader>
<bodyText confidence="0.999692666666667">
For a given integer x, let m = |x |be the magnitude
of x, and σ = sign(x). For a given sequence, let m∗
be the largest such value of m.
Modifying the earlier implicit construction, con-
sider the sequence (3, -2, 1), with m∗ = 3, mapped
to the sequence:
</bodyText>
<equation confidence="0.918557">
(1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1)
</equation>
<bodyText confidence="0.987615142857143">
where each value x is replaced with m∗ + m ele-
ments of σ, and m∗ −m elements of −σ. This views
x as a sequence of length 2m∗, made up of 1s and
-1s, where each x in the discrete range [−m∗, m∗]
has a unique number of 1s.
Now recognize that the average over the original
sequence, here 3−2+1
</bodyText>
<equation confidence="0.804751666666667">
3 = 23, is proportional to the
average over the implicit sequence, 1+1+...−1−1 =
4 = 2 1 18 ).
</equation>
<bodyText confidence="0.968089363636364">
Generally for a sequence (x1,..., xn), with m∗ as
defined, the average times m∗1 is equal to:
where n2m∗ is the total number of 1s and -1s
observed in the implicit stream, up to and including
the mapping of element xn. If applying Reser-
voir Counting, s would then record the sampled
number of 1s, as per norm, where t maintained as
the implicit stream length can also be viewed as
storing t = n2m∗. At any point in the stream, the
average over the original value sequence can then
be approximated as: (1) the approximate sum of the
implicit stream; divided by (2) the implicit stream
length; times (3) m∗ to cancel the m∗1 term:
(t(2sk − 1))1(1t )2(m∗)3 = (2sk − 1)m∗
Granularity As defined this scheme operates on
streams of integers. We extend the definition to work
with a stream of fixed precision floating point vari-
ables. Let g be a positive integer that we refer to
as the granularity. Modify the mapping of value x
from a sequence of length 2m∗, to a sequence of
length g, comprised of m∗+m
2m∗ g instances of σ, and
(1−m∗−m
2m∗ )g instances of -σ. As seen in line 4 of Al-
gorithm 1, a random coin flip determines placement
of the remainder.
For example, the value 1.3, with m∗ = 3, and
g = 10, would now be represented as a sequence
of 3+1.3
6 g = 7.16 E (7,8) instances of 1, followed
by however many instances of -1 that lead to a
sequence of length g, after probabilistic rounding.
The possible sequences are thus:
</bodyText>
<equation confidence="0.9899545">
(1, 1, 1, 1, 1, 1, 1, -1, -1, -1)
(1, 1, 1, 1, 1, 1, 1, 1, -1, -1)
</equation>
<bodyText confidence="0.9844975">
with the former more likely.
At this point we have described the framework
captured by Algorithm 1, where Van Durme and Lall
(2011) defined ResUp.
</bodyText>
<equation confidence="0.9414166">
Algorithm 1 UPDATEAVERAGE(n, k, m, m∗, σ, g, s)
Parameters:
n : size of stream
k : size of reservoir, also maximum value of s
m : magnitude of update
m* : maximum magnitude of all updates
σ : sign of update
g : granularity
s : current value of reservoir
1: if m = 0 or σ = 0 then
</equation>
<listItem confidence="0.990002285714286">
2: Return without doing anything
3: v := m+m* g
2m*
4: v := �v] with probability v − Lv], Lv] otherwise
5: s&apos; := ResUp(ng, k, v, σ, s)
6: s&apos; := ResUp((ng + v), k, g − v, −σ, s&apos;)
7: Return s&apos;
</listItem>
<bodyText confidence="0.9756199">
Log-scale Counting For additional space savings
we might approximate the length parameter t with
a small bit representation, using the approximate
counting scheme of Morris (1978). The method en-
ables counting in log-scale by probabilistically in-
crementing a counter, where it becomes less and
less likely to update the counter after each incre-
ment. This scheme is popularly known and used
in a variety of contexts, recently in the community
by Talbot (2009) and Van Durme and Lall (2009)
</bodyText>
<figure confidence="0.988636684210526">
= En
i=1 miσ
nm*
En Xi
xi
n
(1= 1
m*) n2m*
i=1
n
m
(
m
σi +
−σi)
� *+mi
l=1
� *�mi
l=1
</figure>
<page confidence="0.60574">
52
</page>
<figureCaption confidence="0.993756">
Figure 5: Results on averaging randomly generated se-
quences, with m∗ = 100, g = 100, and using an 8 bit
Morris-style counter of base 2. Larger reservoir sizes lead
to better approximation, at higher cost in bits.
</figureCaption>
<bodyText confidence="0.9996564">
to provide a streaming extension to the Bloom-filter
based count-storage mechanism of Talbot and Os-
borne (2007a) and Talbot and Osborne (2007b). See
(Flajolet, 1985) for a detailed analysis of Morris-
style counting.
</bodyText>
<subsectionHeader confidence="0.990226">
3.3 Experiment
</subsectionHeader>
<bodyText confidence="0.999941083333333">
We show through experimentation on synthetic data
that this approach gives reasonable levels of accu-
racy at space efficient sizes of the length and sum
parameter. Random sequences of 1,000 values were
generated by: (1) fix a value for m*; (2) draw a po-
larity bias term p uniformly from the range [0,1];
then (3) for each value, x: (a) Q was positive with
probability p; (b) m was drawn from [0, m*]. Fig-
ure 5 shows results for varying reservoir sizes (us-
ing 4, 8 or 12 bits) when g = 100, m* = 100, and
the length parameter was represented with an 8 bit
Morris-style counter of base 2.
</bodyText>
<subsectionHeader confidence="0.963241">
3.4 Justification
</subsectionHeader>
<bodyText confidence="0.9999833">
Before we close this section, one might ask why this
extension is needed in the first place. As Reservoir
Counting already allows for keeping an online sum,
and pairs it with a length parameter, then this would
presumably be what is needed to get the average we
are focussed on. Unfortunately that is not the case:
the parameter recording the current stream length,
here called t, tracks the length of the implicit stream
of 1s and -1s, it does not track the length of the origi-
nal stream of values that gave rise to the mapped ver-
sion. As an example, consider again the sequence:
(3, -2, 1), as compared to: (2,1,-1,-1,1). Both have
the same sum, and would therefore be viewed the
same under the pre-existing Reservoir Counting al-
gorithm, giving rise to implicit streams of the same
length. But critically the sequences have different
averages: 23 =� 2�, which we cannot detect based on
the original counting algorithm.
Finally, we restate the constraint: for the sequence
to averaged, one must know m* ahead of time.
</bodyText>
<sectionHeader confidence="0.97958" genericHeader="method">
4 Application to Classification
</sectionHeader>
<bodyText confidence="0.998787090909091">
Going back to our streaming analysis model, we
have a situation that can be viewed as a sequence
of values, such that we do know m*. First reinter-
pret the fraction ��
�� equivalently as the normalized
sum of a stream of elements sampled from w:
The value m* is then: m* = maxj |wj|, over a
sequence of length zt. Rather than updating st and
zt through basic addition, we can now use a smaller
bit-wise representation for each variable, and update
via Reservoir Averaging.
</bodyText>
<subsectionHeader confidence="0.998427">
4.1 Problems in Practice
</subsectionHeader>
<bodyText confidence="0.999971142857143">
Reconsidering the earlier classification experiment,
we found this approximation method led to terri-
ble results: while our experiments on synthetic data
worked well, those sequences were sampled some-
what uniformly over the range of possible values. As
seen in Figure 6, sequences arising from observed
feature weights in a practical setting may not be so
broadly distributed. In brief: the more the maxi-
mum possible update, m*, can be viewed as an out-
lier, then the more the resulting implicit encoding
of g elements per observed weight becomes domi-
nated by “filler”. As few observed elements will in
that case require the provided range, then the im-
plicit representation will be a mostly balanced set of
</bodyText>
<figure confidence="0.983519392857143">
−40−20 020 40
True Average
Approximate Average
−50
50
0
Reservoir.Size.Bits
4 8 12
st
zt
1
zt
t
Z=1
d
j=1
.f;(ci)
�
t=1
=
wj
53
6000
4000
2000
0
−50 0 50
Weight Values
</figure>
<figureCaption confidence="0.99644175">
Figure 6: Frequency of individual feature weights ob-
served over a full set of communications by a single ex-
ample speaker. Most observed features have relatively
small magnitude weight. The mean value is 1.3, with
</figureCaption>
<equation confidence="0.889917">
1 = 0.79 &gt; 0.5, which properly classifies the
</equation>
<bodyText confidence="0.985333166666667">
1 and -1 values. These mostly balanced encodings
make it difficult to maintain an adequate approxima-
tion of the true average, when reliant on a small, im-
plicit uniform sample. Here we leave further analy-
sis aside, focusing instead on a modified solution for
the classification model under consideration.
</bodyText>
<subsectionHeader confidence="0.99718">
4.2 Rewriting History
</subsectionHeader>
<bodyText confidence="0.999945315789473">
Practically we would like to restrict our range to the
dense region of weight updates, while at the same
time not throwing away or truncating larger weights
that appear outside a reduced window. We do this
by fitting a replacement to m*: m&apos; &lt; m*, based on
the classifier’s training data, such that too-large ele-
ments will be accommodated into the stream by im-
plicitly assuming that the portion of a value that falls
outside the restricted window is “spread out” over
the previously observed values. That is, we mod-
ify the contents of the implicit reservoir by rewriting
history: pretending that earlier elements were larger
than they were, but still within the reduced window.
As long as we don’t see too many values that are
overly large, then there will be room to accommo-
date the overflow without any theoretical damage to
the implicit stream: all count mass may still be ac-
counted for. If a moderately high number of overly
large elements are observed, then we expect in prac-
tice for this to have a negligible impact on down-
stream performance. If an exceptional number of
elements are overly large, then the training data was
not representative of the test set.
The newly introduced parameter m&apos; is used in
MODIFIEDUPDATEAVERAGE (MUA), which relies
on REWRITEHISTORY. Note that MUA uses the
same value of n when calling REWRITEHISTORY
as it does in the subsequent line calling UPDATEAV-
ERAGE: we modify the state of the reservoir without
incrementing the stream length, taking the current
overflow and pretending we saw it earlier, spread
out across previous elements. This happens by first
estimating the number of 1 values seen thus far in
the stream: kn, then adding in twice the overflow
value, which represents removing o instances of −σ
from the stream, and then adding o instances of σ.
We probabilistically round the resultant fraction to
achieve a modified version of s, which is returned.
</bodyText>
<construct confidence="0.432832">
Algorithm 2 MUA(n, k, m, m&apos;, σ, g, s)
</construct>
<listItem confidence="0.9072375">
1: if m &lt; m&apos; then
2: Return UPDATEAVERAGE(n, k, m, m&apos;, σ, g, s)
3: s&apos; := REWRITEHISTORY(n, k, m, m&apos;, σ, g, s)
4: Return UPDATEAVERAGE(n, k, m&apos;, m&apos;, σ, g, s&apos;)
</listItem>
<figure confidence="0.712596666666667">
Algorithm 3 REWRITEHISTORY(n, k, m, m&apos;, σ, g, s)
Parameters:
o : overflow to be accommodated
1:
o := ,.�,.z g
2,.0
</figure>
<listItem confidence="0.926574555555555">
2: if σ &gt; 0 then
3: if s = k then
4: Return s
5: p:= min(1.0, ���2°�)
6: else
7: if s = 0 then
8: Return s
9: p:= max(0.0, ��−2°�)
10: Return rpk] with prob. pk − Lpk], Lpk] otherwise
</listItem>
<subsectionHeader confidence="0.926665">
4.3 Experiment
</subsectionHeader>
<figureCaption confidence="0.907867">
Figure 7 compares the results seen in Figure 2 to
a version of the experiment when using approxima-
tion. Parameters were: g = 100; k = 255; and a
Morris-style counter for stream length using 8 bits
</figureCaption>
<figure confidence="0.985864909090909">
Frequency Observed
1+e−1.s
speaker as MALE.
54
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
m&apos;
Loss
45000
40000
25000
35000
30000
1 50 100 150 200 250
Communications
Accuracy
0.9
0.8
0.7
0.6
Approximate
FALSE
TRUE
</figure>
<figureCaption confidence="0.995757">
Figure 7: Comparison between using explicit count-
ing and approximation on the Switchboard dataset, with
bands reflecting 95% confidence.
</figureCaption>
<bodyText confidence="0.9999331">
and a base of 1.3. The value m&apos; was fit indepen-
dently for each split of 10-fold cross validation, by
finding through simple line search that which mini-
mized the number of prediction errors on the origi-
nal training data (see Figure 8). This result shows
our ability to replace 2 variables of 32 bits (sum
and length) with 2 approximation variables of 8 bits
(reservoir status s, and stream length n), leading to
a 75% reduction in the cost of maintaining online
classifier state, with no significant cost in accuracy.
</bodyText>
<sectionHeader confidence="0.974472" genericHeader="method">
5 Real World Stream: Twitter
</sectionHeader>
<subsectionHeader confidence="0.98863">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999970888888889">
Based on the tweet IDs from the data used by
Burger et al. (2011), we recovered 2,958,107 of their
roughly 4 million original tweets.5 These tweets
were then matched against the gender labels estab-
lished in that prior work. As reported by Burger
et al., the dominant language in the collection is
English (66.7% reported), followed by Portuguese
(14.4%) then Spanish (6.0%), with a large variety of
other languages with small numbers of examples.
</bodyText>
<footnote confidence="0.599000833333333">
5Standard practice in Twitter data exchanges is to share only
the unique tweet identifications and then requery the content
from Twitter, thus allowing, e.g., the individual authors the abil-
ity to delete previous posts and have that reflected in future data
collects. While respectful of author privacy, it does pose a chal-
lenge for scientific reproducibility.
</footnote>
<figureCaption confidence="0.98793175">
Figure 8: Summed 0/1 loss over all utterances by each
speaker in the Switchboard training set, across 10 splits.
A value of m&apos; = 5 was on average that which minimized
the number of mistakes made.
</figureCaption>
<bodyText confidence="0.9999745">
Content was lowercased, then processed by regu-
lar expression to collapse the following into respec-
tive single symbols: emoticons; URLs; usernames
(@mentions); and hashtags. Any content deemed
to be a retweet (following the characters RT) was
removed. Text was then tokenized according to a
modified version of the Penn TreeBank tokenization
standard6 that was less English-centric.
</bodyText>
<subsectionHeader confidence="0.997988">
5.2 Experiment
</subsectionHeader>
<bodyText confidence="0.999198333333333">
A log-linear classifier was built using all those au-
thors in the training set7 with at least 10 tweets.
Similar to the previous experiment, unigrams and
bigrams features were used exclusively, with the pa-
rameter m&apos; fit on the training data.
As seen in Figure 9, results were as in Switch-
board: accuracy improves as more data streams in
per author, and our approximate model sacrifices
perhaps a point of accuracy in return for a 75% re-
duction in memory requirements per author.
Table 2 gives the top features per gender. We
see similarities to Switchboard in terms such as my
</bodyText>
<footnote confidence="0.9964492">
6Such as codified in http://www.cis.upenn.edu/
˜treebank/tokenizer.sed
7The same training, development and test set partitions were
used as by Burger et al. (2011), minus those tweets we were
unable to retrieve (as previously discussed).
</footnote>
<page confidence="0.996118">
55
</page>
<figure confidence="0.735363">
Communications
</figure>
<figureCaption confidence="0.996610666666667">
Figure 9: Comparison between using explicit counting
and approximation, on the Twitter dataset, with bands re-
flecting 95% confidence.
</figureCaption>
<bodyText confidence="0.999686666666667">
wife, along with terms suggesting a more youthful
population. Foreign terms are recognized by their
parenthetical translation and 1st- or 2nd-person +
Male/Female gender marking. For example, the Por-
tuguese obrigado can be taken to be literally saying:
I’m obliged (thank you), and I’m male.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99954475">
Streaming algorithms have been developed within
the applied communities of networking, and (very
large) databases, as well as being a popular topic in
the theoretical computer science literature. A sum-
</bodyText>
<tableCaption confidence="0.975701">
Table 2: Top thirty-five features by gender in Twitter.
Male obrigado (thank you [1M]), wife, my wife,
</tableCaption>
<bodyText confidence="0.833926628571429">
bro, cansado (tired [1M]), gay, mate, dude,
[@username] why, buddy, windows, album,
dope, beer, [@username] yo, sir, ps3, comics,
galera (folks/people), amigo (friend [2M]),
man !, fuckin, omg omg, cheers, ai n’t
Female obrigada (thank you [1F]), hubby, husband,
cute, my husband, ?, cansada (tired [1F]),
hair, dress, soooo, lovely, etsy, boyfriend,
jonas, loved, book, sooo, girl, s´e (I),
lindo (cute), shopping, amiga (friend [2F]),
yummy, ppl, cupcakes
mary of the streaming algorithms community is be-
yond the scope of this work: interested readers are
directed to Muthukrishnan (2005) as a starting point.
Within computational linguistics interest in
streaming approaches is a more recent development;
we provide here examples of representative work,
beyond those described in previous sections. Leven-
berg and Osborne (2009) gave a streaming variant of
the earlier perfect hashing language model of Talbot
and Brants (2008), which operated in batch-mode.
Using a similar decomposition to that here, Van
Durme and Lall (2010) showed that Locality Sen-
sitive Hash (LSH) signatures (Indyk and Motwani,
1998; Charikar, 2002) built using count-based fea-
ture vectors can be maintained online, as compared
to their earlier uses in the community (Ravichandran
et al., 2005; Bhagat and Ravichandran, 2008). Fi-
nally, Goyal et al. (2009) applied the frequent items8
algorithm of Manku and Motwani (2002) to lan-
guage modeling.
For further background in predicting author at-
tributes such as gender, see (Garera and Yarowsky,
2009) for an overview of previous work and (non-
streaming) methodology.
</bodyText>
<sectionHeader confidence="0.996025" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.928975476190476">
We have taken the predominately batch-oriented
process of analyzing communication data and shown
it to be fertile territory for research in large-scale
streaming algorithms. Using the example task of au-
tomatic gender detection, on both spoken transcripts
and microblogs, we showed that classification can
be thought of as a continuously running process, be-
coming more robust as further communications be-
come available. Once positioned within a stream-
ing framework, we presented a novel approximation
technique for compressing the streaming memory
requirements of the classifier (per author) by 75%.
There are a number of avenues to explore based
on this framework. For instance, while here we as-
sumed a static, pre-built classifier which was then
applied to streaming data, future work may consider
the interplay with online learning, based on meth-
ods such as by Crammer et al. (2006). In the appli-
8See the survey by Cormode and Hadjieleftheriou (2009) for
approaches to the frequent items problem, otherwise known as
finding heavy hitters.
</bodyText>
<figure confidence="0.9991964">
20 40 60 80 100
Accuracy
0.85
0.80
0.75
0.70
0.65
Approximate
No
Yes
</figure>
<page confidence="0.985766">
56
</page>
<bodyText confidence="0.908479941176471">
cations arena, one might take the savings provided
here to run multiple models in parallel, either for
more robust predictions (perhaps “triangulating” on
language ID and/or domain over the stream), or pre-
dicting additional properties, such as age, national-
ity, political orientation, and so forth. Finally, we
assumed here strictly count-based features; stream-
ing log-counting methods, tailored Bloom-filters for
binary feature storage, and other related topics are
assuredly applicable, and should give rise to many
interesting new results.
Acknowledgments I thank the reviewers and my
colleagues at Johns Hopkins University for help-
ful feedback, in particular Matt Post, Mark Dredze,
Glen Coppersmith and David Yarowsky. Thanks to
David Yarowsky and Theresa Wilson for their assis-
tance in collecting data.
</bodyText>
<sectionHeader confidence="0.974482" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873642857143">
Rahul Bhagat and Deepak Ravichandran. 2008. Large
Scale Acquisition of Paraphrases for Learning Surface
Patterns. In Proceedings of ACL.
Constantinos Boulis and Mari Ostendorf. 2005. A quan-
titative analysis of lexical differences between genders
in telephone conversations. In Proceedings of ACL.
John D. Burger, John Henderson, George Kim, and Guido
Zarrella. 2011. Discriminating gender on twitter. In
Proceedings of EMNLP.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of STOC.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The fisher corpus: a resource for the next gen-
erations of speech-to-text. In Proceedings of LREC.
Graham Cormode and Marios Hadjieleftheriou. 2009.
Finding the frequent items in streams of data. Com-
munications of the ACM, 52(10):97–105.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Christopher P. Diehl, Galileo Namata, and Lise Getoor.
2007. Relationship identification for social network
discovery. In Proceedings of AAAI.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Philippe Flajolet. 1985. Approximate counting: a de-
tailed analysis. BIT, 25(1):113–134.
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of ACL.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proceedings of
ICASSP.
Amit Goyal, Hal Daum´e III, and Suresh Venkatasubra-
manian. 2009. Streaming for large scale NLP: Lan-
guage Modeling. In Proceedings of NAACL.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Nitin Jindal and Bing Liu. 2008. Opinion spam and anal-
ysis. In Proceedings of the International Conference
on Web Search and Wed Data Mining, pages 219–230.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of EMNLP.
Gurmeet Singh Manku and Rajeev Motwani. 2002. Ap-
proximate frequency counts over data streams. In Pro-
ceedings of the 28th international conference on Very
Large Data Bases (VLDB).
Robert Morris. 1978. Counting large numbers of events
in small registers. Communications of the ACM,
21(10):840–842.
S. Muthu Muthukrishnan. 2005. Data streams: Algo-
rithms and applications. Foundations and Trends in
Theoretical Computer Science, 1(2).
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to public
opinion time series. In Proceedings of ICWSM.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of ACL.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of NAACL.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd In-
ternational Workshop on Search and Mining User-
generated Contents (SMUC).
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proceedings of ACL.
David Talbot and Thorsten Brants. 2008. Randomized
language models via perfect hash functions. In Pro-
ceedings of ACL.
David Talbot and Miles Osborne. 2007a. Randomised
language modelling for statistical machine translation.
In Proceedings of ACL.
</reference>
<page confidence="0.980305">
57
</page>
<reference confidence="0.9998177">
David Talbot and Miles Osborne. 2007b. Smoothed
Bloom filter language models: Tera-Scale LMs on the
Cheap. In Proceedings of EMNLP.
David Talbot. 2009. Succinct approximate counting of
skewed data. In Proceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2009. Proba-
bilistic Counting with Randomized Storage. In Pro-
ceedings of IJCAI.
Benjamin Van Durme and Ashwin Lall. 2010. Online
Generation of Locality Sensitive Hash Signatures. In
Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2011. Effi-
cient Online Locality Sensitive Hashing via Reservoir
Counting. In Proceedings of ACL.
Benjamin Van Durme. 2012. Jerboa: A toolkit for
randomized and streaming algorithms. Technical Re-
port 7, Human Language Technology Center of Excel-
lence, Johns Hopkins University.
Jeffrey S. Vitter. 1985. Random sampling with a reser-
voir. ACM Trans. Math. Softw., 11:37–57, March.
</reference>
<page confidence="0.999263">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928904">
<title confidence="0.999924">Streaming Analysis of Discourse Participants</title>
<author confidence="0.999613">Benjamin Van_Durme</author>
<affiliation confidence="0.9839595">Human Language Technology Center of Johns Hopkins University</affiliation>
<abstract confidence="0.9978357">Inferring attributes of discourse participants been treated as a task: data such as all tweets from a given author are gathered in bulk, processed, analyzed for a particular feature, then reported as a result of academic interest. Given the sources and scale of material used in these efforts, along with potential use cases of such analytic tools, discourse analysis should be reconsidered as We show that under certain common formulations, the batchprocessing analytic framework can be decomposed into a sequential series of updates, using as an example the task of gender classification. Once in a streaming framework, and motivated by large data sets generated by social media services, we present novel results in approximate counting, showing its applicability to space efficient streaming classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Large Scale Acquisition of Paraphrases for Learning Surface Patterns.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="28542" citStr="Bhagat and Ravichandran, 2008" startWordPosition="4929" endWordPosition="4932">oaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (nonstreaming) methodology. 7 Conclusions and Future Work We have taken the predominately batch-oriented process of analyzing communication data and shown it to be fertile territory for research in large-scale streaming algorithms. Using the example task of automatic gender detection, on both spoken transcripts and microblogs, we showed th</context>
</contexts>
<marker>Bhagat, Ravichandran, 2008</marker>
<rawString>Rahul Bhagat and Deepak Ravichandran. 2008. Large Scale Acquisition of Paraphrases for Learning Surface Patterns. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantinos Boulis</author>
<author>Mari Ostendorf</author>
</authors>
<title>A quantitative analysis of lexical differences between genders in telephone conversations.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7372" citStr="Boulis and Ostendorf (2005)" startWordPosition="1226" endWordPosition="1229">classifier: (st, zt) = . ( Xt Xd wk ˆfk(cj), zt) i=1 k=1 = 1 Xn d zn i=1 (X =1 Xt i=1 zt = |ˆf(ci)|1 49 50% 90% 5% 85% 86% 50% 10% 95% 15% 14% C1 C2 ... Ct-1 Ct ... Figure 1: A streaming analytic model should update its decision with each new communication, becoming more stable in its prediction as evidence is acquired. In either setting, the state of the classifier is sufficiently captured by the pair (st, zt), under the restrictions on f.3 2.1 Validation As an example of a model decomposed into a stream, we revist the task of gender classification based on speech transcripts, as explored by Boulis and Ostendorf (2005) and later Garera and Yarowsky (2009). In the original problem definition, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are o</context>
</contexts>
<marker>Boulis, Ostendorf, 2005</marker>
<rawString>Constantinos Boulis and Mari Ostendorf. 2005. A quantitative analysis of lexical differences between genders in telephone conversations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating gender on twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8061" citStr="Burger et al. (2011)" startWordPosition="1342" endWordPosition="1345">tion, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are observed, the better our classification accuracy. Researchers such as Burger et al. (2011) have considered this point, but by comparing the classification accuracy based on the volume of batch data available per author (in that case, tweets): the more prolific the author had been, the better able they were to correctly classify their gender. We confirm here this can be reframed: as a speaker (author) continues to emit a stream of communication, a dynamic model tends to improve its online prediction. Our collection based on Switchboard consisted of 520 unique speakers (240 female, 280 male), with a total of roughly 400k utterances. Similar to Boulis and Ostendorf, we extracted unigr</context>
<context position="24328" citStr="Burger et al. (2011)" startWordPosition="4273" endWordPosition="4276">nd a base of 1.3. The value m&apos; was fit independently for each split of 10-fold cross validation, by finding through simple line search that which minimized the number of prediction errors on the original training data (see Figure 8). This result shows our ability to replace 2 variables of 32 bits (sum and length) with 2 approximation variables of 8 bits (reservoir status s, and stream length n), leading to a 75% reduction in the cost of maintaining online classifier state, with no significant cost in accuracy. 5 Real World Stream: Twitter 5.1 Setup Based on the tweet IDs from the data used by Burger et al. (2011), we recovered 2,958,107 of their roughly 4 million original tweets.5 These tweets were then matched against the gender labels established in that prior work. As reported by Burger et al., the dominant language in the collection is English (66.7% reported), followed by Portuguese (14.4%) then Spanish (6.0%), with a large variety of other languages with small numbers of examples. 5Standard practice in Twitter data exchanges is to share only the unique tweet identifications and then requery the content from Twitter, thus allowing, e.g., the individual authors the ability to delete previous posts</context>
<context position="26403" citStr="Burger et al. (2011)" startWordPosition="4606" endWordPosition="4609">previous experiment, unigrams and bigrams features were used exclusively, with the parameter m&apos; fit on the training data. As seen in Figure 9, results were as in Switchboard: accuracy improves as more data streams in per author, and our approximate model sacrifices perhaps a point of accuracy in return for a 75% reduction in memory requirements per author. Table 2 gives the top features per gender. We see similarities to Switchboard in terms such as my 6Such as codified in http://www.cis.upenn.edu/ ˜treebank/tokenizer.sed 7The same training, development and test set partitions were used as by Burger et al. (2011), minus those tweets we were unable to retrieve (as previously discussed). 55 Communications Figure 9: Comparison between using explicit counting and approximation, on the Twitter dataset, with bands reflecting 95% confidence. wife, along with terms suggesting a more youthful population. Foreign terms are recognized by their parenthetical translation and 1st- or 2nd-person + Male/Female gender marking. For example, the Portuguese obrigado can be taken to be literally saying: I’m obliged (thank you), and I’m male. 6 Related Work Streaming algorithms have been developed within the applied commun</context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D. Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on twitter. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of STOC.</booktitle>
<contexts>
<context position="28366" citStr="Charikar, 2002" startWordPosition="4904" endWordPosition="4905"> the scope of this work: interested readers are directed to Muthukrishnan (2005) as a starting point. Within computational linguistics interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (nonstreaming) methodology. 7 Conclusions and Future Work We have taken the predominately batch-oriented process of analyzing communication data and shown it to be f</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>David Miller</author>
<author>Kevin Walker</author>
</authors>
<title>The fisher corpus: a resource for the next generations of speech-to-text.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="7560" citStr="Cieri et al., 2004" startWordPosition="1257" endWordPosition="1260">model should update its decision with each new communication, becoming more stable in its prediction as evidence is acquired. In either setting, the state of the classifier is sufficiently captured by the pair (st, zt), under the restrictions on f.3 2.1 Validation As an example of a model decomposed into a stream, we revist the task of gender classification based on speech transcripts, as explored by Boulis and Ostendorf (2005) and later Garera and Yarowsky (2009). In the original problem definition, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are observed, the better our classification accuracy. Researchers such as Burger et al. (2011) have considered this point, but by comparing the classification accuracy based on the volume of ba</context>
</contexts>
<marker>Cieri, Miller, Walker, 2004</marker>
<rawString>Christopher Cieri, David Miller, and Kevin Walker. 2004. The fisher corpus: a resource for the next generations of speech-to-text. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Cormode</author>
<author>Marios Hadjieleftheriou</author>
</authors>
<title>Finding the frequent items in streams of data.</title>
<date>2009</date>
<journal>Communications of the ACM,</journal>
<volume>52</volume>
<issue>10</issue>
<contexts>
<context position="29811" citStr="Cormode and Hadjieleftheriou (2009)" startWordPosition="5129" endWordPosition="5132">f as a continuously running process, becoming more robust as further communications become available. Once positioned within a streaming framework, we presented a novel approximation technique for compressing the streaming memory requirements of the classifier (per author) by 75%. There are a number of avenues to explore based on this framework. For instance, while here we assumed a static, pre-built classifier which was then applied to streaming data, future work may consider the interplay with online learning, based on methods such as by Crammer et al. (2006). In the appli8See the survey by Cormode and Hadjieleftheriou (2009) for approaches to the frequent items problem, otherwise known as finding heavy hitters. 20 40 60 80 100 Accuracy 0.85 0.80 0.75 0.70 0.65 Approximate No Yes 56 cations arena, one might take the savings provided here to run multiple models in parallel, either for more robust predictions (perhaps “triangulating” on language ID and/or domain over the stream), or predicting additional properties, such as age, nationality, political orientation, and so forth. Finally, we assumed here strictly count-based features; streaming log-counting methods, tailored Bloom-filters for binary feature storage, a</context>
</contexts>
<marker>Cormode, Hadjieleftheriou, 2009</marker>
<rawString>Graham Cormode and Marios Hadjieleftheriou. 2009. Finding the frequent items in streams of data. Communications of the ACM, 52(10):97–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="29743" citStr="Crammer et al. (2006)" startWordPosition="5118" endWordPosition="5121">oblogs, we showed that classification can be thought of as a continuously running process, becoming more robust as further communications become available. Once positioned within a streaming framework, we presented a novel approximation technique for compressing the streaming memory requirements of the classifier (per author) by 75%. There are a number of avenues to explore based on this framework. For instance, while here we assumed a static, pre-built classifier which was then applied to streaming data, future work may consider the interplay with online learning, based on methods such as by Crammer et al. (2006). In the appli8See the survey by Cormode and Hadjieleftheriou (2009) for approaches to the frequent items problem, otherwise known as finding heavy hitters. 20 40 60 80 100 Accuracy 0.85 0.80 0.75 0.70 0.65 Approximate No Yes 56 cations arena, one might take the savings provided here to run multiple models in parallel, either for more robust predictions (perhaps “triangulating” on language ID and/or domain over the stream), or predicting additional properties, such as age, nationality, political orientation, and so forth. Finally, we assumed here strictly count-based features; streaming log-co</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher P Diehl</author>
<author>Galileo Namata</author>
<author>Lise Getoor</author>
</authors>
<title>Relationship identification for social network discovery.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="1606" citStr="Diehl et al., 2007" startWordPosition="249" endWordPosition="252">assification. 1 Introduction The rapid growth in social media has led to an equally rapid growth in the desire to mine it for useful information: the content of public discussions, such as found in tweets, or in posts to online forums, can support a variety of data mining tasks. Inferring the underlying properties of those that engage with these platforms, the discourse participants, has become an active topic of research: predicting individual attributes such as age, gender, and political preferences (Rao et al., 2010), or relationships between communicants, such as organizational dominance (Diehl et al., 2007). This research can benefit areas such as: (A) commercial applications, e.g., 48 improved models for advertising placement, or detecting fraudulent or otherwise unhelpful product reviews (Jindal and Liu, 2008; Ott et al., 2011); and (B) in enhanced models of civic discourse, e.g., inexpensive, large-scale, passive polling of popular opinion (O’Connor et al., 2010). Classification with streaming data has usually been taken in the computational linguistics community to mean individual decisions made on items that are presented over time. For example: assigning a label to each newly posted produc</context>
</contexts>
<marker>Diehl, Namata, Getoor, 2007</marker>
<rawString>Christopher P. Diehl, Galileo Namata, and Lise Getoor. 2007. Relationship identification for social network discovery. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsief</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<contexts>
<context position="9264" citStr="Fan et al., 2008" startWordPosition="1540" endWordPosition="1543">xtracted unigram and bigram counts as features, but without further 3Note that some non-linear kernels can be maintained online in a similar fashion. For instance, a polynomial kernel of degree p decomposes as: (f(C�) · w)P = (�� �� )P. Figure 2: Accuracy on Switchboard gender classification, reported at every fifth utterance, using a dynamic log-linear model with 10-fold cross validation. TFIDF reweighting. Ngrams were required to occur at least 10 times in the training set, recomputed for each split of 10-fold cross validation. Weights were computed under a log-linear model using LibLinear (Fan et al., 2008), with 5% of training held out for tuning an L2 regularizing term. Feature extraction and dynamic aspects were handled through additions to the Jerboa package (Van Durme, 2012). Similar to previous work, we found intuitive features such as my husband to be weighted heavily (see Table 1), along with certain non-lexical vocalizations such as transcribed laughter. Table 1: Top ten features by gender. Male a, wife, is, my wife, right, of, the, uh, actually, [vocalized-noise] Female have, and, [laughter], my husband, really, husband, children, are, would 1 50 100 150 200 250 Communications Accuracy</context>
</contexts>
<marker>Fan, Chang, Hsief, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, (9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Flajolet</author>
</authors>
<title>Approximate counting: a detailed analysis.</title>
<date>1985</date>
<journal>BIT,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="17081" citStr="Flajolet, 1985" startWordPosition="2986" endWordPosition="2987">rement. This scheme is popularly known and used in a variety of contexts, recently in the community by Talbot (2009) and Van Durme and Lall (2009) = En i=1 miσ nm* En Xi xi n (1= 1 m*) n2m* i=1 n m ( m σi + −σi) � *+mi l=1 � *�mi l=1 52 Figure 5: Results on averaging randomly generated sequences, with m∗ = 100, g = 100, and using an 8 bit Morris-style counter of base 2. Larger reservoir sizes lead to better approximation, at higher cost in bits. to provide a streaming extension to the Bloom-filter based count-storage mechanism of Talbot and Osborne (2007a) and Talbot and Osborne (2007b). See (Flajolet, 1985) for a detailed analysis of Morrisstyle counting. 3.3 Experiment We show through experimentation on synthetic data that this approach gives reasonable levels of accuracy at space efficient sizes of the length and sum parameter. Random sequences of 1,000 values were generated by: (1) fix a value for m*; (2) draw a polarity bias term p uniformly from the range [0,1]; then (3) for each value, x: (a) Q was positive with probability p; (b) m was drawn from [0, m*]. Figure 5 shows results for varying reservoir sizes (using 4, 8 or 12 bits) when g = 100, m* = 100, and the length parameter was represe</context>
</contexts>
<marker>Flajolet, 1985</marker>
<rawString>Philippe Flajolet. 1985. Approximate counting: a detailed analysis. BIT, 25(1):113–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7409" citStr="Garera and Yarowsky (2009)" startWordPosition="1232" endWordPosition="1235">k(cj), zt) i=1 k=1 = 1 Xn d zn i=1 (X =1 Xt i=1 zt = |ˆf(ci)|1 49 50% 90% 5% 85% 86% 50% 10% 95% 15% 14% C1 C2 ... Ct-1 Ct ... Figure 1: A streaming analytic model should update its decision with each new communication, becoming more stable in its prediction as evidence is acquired. In either setting, the state of the classifier is sufficiently captured by the pair (st, zt), under the restrictions on f.3 2.1 Validation As an example of a model decomposed into a stream, we revist the task of gender classification based on speech transcripts, as explored by Boulis and Ostendorf (2005) and later Garera and Yarowsky (2009). In the original problem definition, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are observed, the better our classificatio</context>
<context position="28763" citStr="Garera and Yarowsky, 2009" startWordPosition="4965" endWordPosition="4968">model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (nonstreaming) methodology. 7 Conclusions and Future Work We have taken the predominately batch-oriented process of analyzing communication data and shown it to be fertile territory for research in large-scale streaming algorithms. Using the example task of automatic gender detection, on both spoken transcripts and microblogs, we showed that classification can be thought of as a continuously running process, becoming more robust as further communications become available. Once positioned within a streaming framework, we presented a novel approximation tech</context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Edward C Holliman</author>
<author>Jane McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="7598" citStr="Godfrey et al., 1992" startWordPosition="1263" endWordPosition="1266">h each new communication, becoming more stable in its prediction as evidence is acquired. In either setting, the state of the classifier is sufficiently captured by the pair (st, zt), under the restrictions on f.3 2.1 Validation As an example of a model decomposed into a stream, we revist the task of gender classification based on speech transcripts, as explored by Boulis and Ostendorf (2005) and later Garera and Yarowsky (2009). In the original problem definition, one would collect all transcribed utterances from a given speaker in a corpus such as Fisher (Cieri et al., 2004) or Switchboard (Godfrey et al., 1992), known as a side of the conversation. Then by collapsing these utterances into a single document, one could classify it as to whether it was generated by a male or female. Here we define the task as: starting from scratch, report the classifier probability of the speaker being male, as each utterance is presented. Intuitively we would expect that as more utterances are observed, the better our classification accuracy. Researchers such as Burger et al. (2011) have considered this point, but by comparing the classification accuracy based on the volume of batch data available per author (in that</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Streaming for large scale NLP: Language Modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Goyal, Daum´e, Venkatasubramanian, 2009</marker>
<rawString>Amit Goyal, Hal Daum´e III, and Suresh Venkatasubramanian. 2009. Streaming for large scale NLP: Language Modeling. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of STOC.</booktitle>
<contexts>
<context position="28349" citStr="Indyk and Motwani, 1998" startWordPosition="4900" endWordPosition="4903">ithms community is beyond the scope of this work: interested readers are directed to Muthukrishnan (2005) as a starting point. Within computational linguistics interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (nonstreaming) methodology. 7 Conclusions and Future Work We have taken the predominately batch-oriented process of analyzing communication data and</context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of STOC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Web Search and Wed Data Mining,</booktitle>
<pages>219--230</pages>
<contexts>
<context position="1814" citStr="Jindal and Liu, 2008" startWordPosition="281" endWordPosition="284">r in posts to online forums, can support a variety of data mining tasks. Inferring the underlying properties of those that engage with these platforms, the discourse participants, has become an active topic of research: predicting individual attributes such as age, gender, and political preferences (Rao et al., 2010), or relationships between communicants, such as organizational dominance (Diehl et al., 2007). This research can benefit areas such as: (A) commercial applications, e.g., 48 improved models for advertising placement, or detecting fraudulent or otherwise unhelpful product reviews (Jindal and Liu, 2008; Ott et al., 2011); and (B) in enhanced models of civic discourse, e.g., inexpensive, large-scale, passive polling of popular opinion (O’Connor et al., 2010). Classification with streaming data has usually been taken in the computational linguistics community to mean individual decisions made on items that are presented over time. For example: assigning a label to each newly posted product review as to whether it contains positive or negative sentiment, or whether the latest tweet signals a novel topic that should be tagged for tracking (Petrovic et al., 2010). Here we consider a distinct for</context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of the International Conference on Web Search and Wed Data Mining, pages 219–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
</authors>
<title>Streambased randomised language models for smt.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="28071" citStr="Levenberg and Osborne (2009)" startWordPosition="4855" endWordPosition="4859">rs, ai n’t Female obrigada (thank you [1F]), hubby, husband, cute, my husband, ?, cansada (tired [1F]), hair, dress, soooo, lovely, etsy, boyfriend, jonas, loved, book, sooo, girl, s´e (I), lindo (cute), shopping, amiga (friend [2F]), yummy, ppl, cupcakes mary of the streaming algorithms community is beyond the scope of this work: interested readers are directed to Muthukrishnan (2005) as a starting point. Within computational linguistics interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For furthe</context>
</contexts>
<marker>Levenberg, Osborne, 2009</marker>
<rawString>Abby Levenberg and Miles Osborne. 2009. Streambased randomised language models for smt. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurmeet Singh Manku</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate frequency counts over data streams.</title>
<date>2002</date>
<booktitle>In Proceedings of the 28th international conference on Very Large Data Bases (VLDB).</booktitle>
<contexts>
<context position="28638" citStr="Manku and Motwani (2002)" startWordPosition="4945" endWordPosition="4948">cribed in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (nonstreaming) methodology. 7 Conclusions and Future Work We have taken the predominately batch-oriented process of analyzing communication data and shown it to be fertile territory for research in large-scale streaming algorithms. Using the example task of automatic gender detection, on both spoken transcripts and microblogs, we showed that classification can be thought of as a continuously running process, becoming more robust as f</context>
</contexts>
<marker>Manku, Motwani, 2002</marker>
<rawString>Gurmeet Singh Manku and Rajeev Motwani. 2002. Approximate frequency counts over data streams. In Proceedings of the 28th international conference on Very Large Data Bases (VLDB).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Morris</author>
</authors>
<title>Counting large numbers of events in small registers.</title>
<date>1978</date>
<journal>Communications of the ACM,</journal>
<volume>21</volume>
<issue>10</issue>
<contexts>
<context position="16304" citStr="Morris (1978)" startWordPosition="2842" endWordPosition="2843">, σ, g, s) Parameters: n : size of stream k : size of reservoir, also maximum value of s m : magnitude of update m* : maximum magnitude of all updates σ : sign of update g : granularity s : current value of reservoir 1: if m = 0 or σ = 0 then 2: Return without doing anything 3: v := m+m* g 2m* 4: v := �v] with probability v − Lv], Lv] otherwise 5: s&apos; := ResUp(ng, k, v, σ, s) 6: s&apos; := ResUp((ng + v), k, g − v, −σ, s&apos;) 7: Return s&apos; Log-scale Counting For additional space savings we might approximate the length parameter t with a small bit representation, using the approximate counting scheme of Morris (1978). The method enables counting in log-scale by probabilistically incrementing a counter, where it becomes less and less likely to update the counter after each increment. This scheme is popularly known and used in a variety of contexts, recently in the community by Talbot (2009) and Van Durme and Lall (2009) = En i=1 miσ nm* En Xi xi n (1= 1 m*) n2m* i=1 n m ( m σi + −σi) � *+mi l=1 � *�mi l=1 52 Figure 5: Results on averaging randomly generated sequences, with m∗ = 100, g = 100, and using an 8 bit Morris-style counter of base 2. Larger reservoir sizes lead to better approximation, at higher co</context>
</contexts>
<marker>Morris, 1978</marker>
<rawString>Robert Morris. 1978. Counting large numbers of events in small registers. Communications of the ACM, 21(10):840–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Muthu Muthukrishnan</author>
</authors>
<title>Data streams: Algorithms and applications. Foundations and Trends in</title>
<date>2005</date>
<journal>Theoretical Computer Science,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="27831" citStr="Muthukrishnan (2005)" startWordPosition="4824" endWordPosition="4825">ank you [1M]), wife, my wife, bro, cansado (tired [1M]), gay, mate, dude, [@username] why, buddy, windows, album, dope, beer, [@username] yo, sir, ps3, comics, galera (folks/people), amigo (friend [2M]), man !, fuckin, omg omg, cheers, ai n’t Female obrigada (thank you [1F]), hubby, husband, cute, my husband, ?, cansada (tired [1F]), hair, dress, soooo, lovely, etsy, boyfriend, jonas, loved, book, sooo, girl, s´e (I), lindo (cute), shopping, amiga (friend [2F]), yummy, ppl, cupcakes mary of the streaming algorithms community is beyond the scope of this work: interested readers are directed to Muthukrishnan (2005) as a starting point. Within computational linguistics interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online</context>
</contexts>
<marker>Muthukrishnan, 2005</marker>
<rawString>S. Muthu Muthukrishnan. 2005. Data streams: Algorithms and applications. Foundations and Trends in Theoretical Computer Science, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1833" citStr="Ott et al., 2011" startWordPosition="285" endWordPosition="288">orums, can support a variety of data mining tasks. Inferring the underlying properties of those that engage with these platforms, the discourse participants, has become an active topic of research: predicting individual attributes such as age, gender, and political preferences (Rao et al., 2010), or relationships between communicants, such as organizational dominance (Diehl et al., 2007). This research can benefit areas such as: (A) commercial applications, e.g., 48 improved models for advertising placement, or detecting fraudulent or otherwise unhelpful product reviews (Jindal and Liu, 2008; Ott et al., 2011); and (B) in enhanced models of civic discourse, e.g., inexpensive, large-scale, passive polling of popular opinion (O’Connor et al., 2010). Classification with streaming data has usually been taken in the computational linguistics community to mean individual decisions made on items that are presented over time. For example: assigning a label to each newly posted product review as to whether it contains positive or negative sentiment, or whether the latest tweet signals a novel topic that should be tagged for tracking (Petrovic et al., 2010). Here we consider a distinct form of stream-based c</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2381" citStr="Petrovic et al., 2010" startWordPosition="371" endWordPosition="374">herwise unhelpful product reviews (Jindal and Liu, 2008; Ott et al., 2011); and (B) in enhanced models of civic discourse, e.g., inexpensive, large-scale, passive polling of popular opinion (O’Connor et al., 2010). Classification with streaming data has usually been taken in the computational linguistics community to mean individual decisions made on items that are presented over time. For example: assigning a label to each newly posted product review as to whether it contains positive or negative sentiment, or whether the latest tweet signals a novel topic that should be tagged for tracking (Petrovic et al., 2010). Here we consider a distinct form of stream-based classification: we wish to assign, then dynamically update, labels on discourse participants based on their associated streaming communications. For instance, rather than classifying individual reviews as to their sentiment polarity, we might wish to classify the underlying author as to whether they are genuine or paid-advertising, and then update that decision as they continue to post new reviews. As the scale of social media continues to grow, we desire that our model be aggressively space efficient, which precludes a naive solution of stori</context>
</contexts>
<marker>Petrovic, Osborne, Lavrenko, 2010</marker>
<rawString>Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to twitter. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2nd International Workshop on Search and Mining Usergenerated Contents (SMUC).</booktitle>
<contexts>
<context position="1512" citStr="Rao et al., 2010" startWordPosition="235" endWordPosition="238">l results in approximate counting, showing its applicability to space efficient streaming classification. 1 Introduction The rapid growth in social media has led to an equally rapid growth in the desire to mine it for useful information: the content of public discussions, such as found in tweets, or in posts to online forums, can support a variety of data mining tasks. Inferring the underlying properties of those that engage with these platforms, the discourse participants, has become an active topic of research: predicting individual attributes such as age, gender, and political preferences (Rao et al., 2010), or relationships between communicants, such as organizational dominance (Diehl et al., 2007). This research can benefit areas such as: (A) commercial applications, e.g., 48 improved models for advertising placement, or detecting fraudulent or otherwise unhelpful product reviews (Jindal and Liu, 2008; Ott et al., 2011); and (B) in enhanced models of civic discourse, e.g., inexpensive, large-scale, passive polling of popular opinion (O’Connor et al., 2010). Classification with streaming data has usually been taken in the computational linguistics community to mean individual decisions made on </context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in twitter. In Proceedings of the 2nd International Workshop on Search and Mining Usergenerated Contents (SMUC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="28510" citStr="Ravichandran et al., 2005" startWordPosition="4925" endWordPosition="4928"> interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an overview of previous work and (nonstreaming) methodology. 7 Conclusions and Future Work We have taken the predominately batch-oriented process of analyzing communication data and shown it to be fertile territory for research in large-scale streaming algorithms. Using the example task of automatic gender detection, on both spoken transcri</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Thorsten Brants</author>
</authors>
<title>Randomized language models via perfect hash functions.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="28170" citStr="Talbot and Brants (2008)" startWordPosition="4872" endWordPosition="4875"> hair, dress, soooo, lovely, etsy, boyfriend, jonas, loved, book, sooo, girl, s´e (I), lindo (cute), shopping, amiga (friend [2F]), yummy, ppl, cupcakes mary of the streaming algorithms community is beyond the scope of this work: interested readers are directed to Muthukrishnan (2005) as a starting point. Within computational linguistics interest in streaming approaches is a more recent development; we provide here examples of representative work, beyond those described in previous sections. Levenberg and Osborne (2009) gave a streaming variant of the earlier perfect hashing language model of Talbot and Brants (2008), which operated in batch-mode. Using a similar decomposition to that here, Van Durme and Lall (2010) showed that Locality Sensitive Hash (LSH) signatures (Indyk and Motwani, 1998; Charikar, 2002) built using count-based feature vectors can be maintained online, as compared to their earlier uses in the community (Ravichandran et al., 2005; Bhagat and Ravichandran, 2008). Finally, Goyal et al. (2009) applied the frequent items8 algorithm of Manku and Motwani (2002) to language modeling. For further background in predicting author attributes such as gender, see (Garera and Yarowsky, 2009) for an</context>
</contexts>
<marker>Talbot, Brants, 2008</marker>
<rawString>David Talbot and Thorsten Brants. 2008. Randomized language models via perfect hash functions. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17026" citStr="Talbot and Osborne (2007" startWordPosition="2975" endWordPosition="2979">ecomes less and less likely to update the counter after each increment. This scheme is popularly known and used in a variety of contexts, recently in the community by Talbot (2009) and Van Durme and Lall (2009) = En i=1 miσ nm* En Xi xi n (1= 1 m*) n2m* i=1 n m ( m σi + −σi) � *+mi l=1 � *�mi l=1 52 Figure 5: Results on averaging randomly generated sequences, with m∗ = 100, g = 100, and using an 8 bit Morris-style counter of base 2. Larger reservoir sizes lead to better approximation, at higher cost in bits. to provide a streaming extension to the Bloom-filter based count-storage mechanism of Talbot and Osborne (2007a) and Talbot and Osborne (2007b). See (Flajolet, 1985) for a detailed analysis of Morrisstyle counting. 3.3 Experiment We show through experimentation on synthetic data that this approach gives reasonable levels of accuracy at space efficient sizes of the length and sum parameter. Random sequences of 1,000 values were generated by: (1) fix a value for m*; (2) draw a polarity bias term p uniformly from the range [0,1]; then (3) for each value, x: (a) Q was positive with probability p; (b) m was drawn from [0, m*]. Figure 5 shows results for varying reservoir sizes (using 4, 8 or 12 bits) when </context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007a. Randomised language modelling for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Smoothed Bloom filter language models: Tera-Scale LMs on the Cheap.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17026" citStr="Talbot and Osborne (2007" startWordPosition="2975" endWordPosition="2979">ecomes less and less likely to update the counter after each increment. This scheme is popularly known and used in a variety of contexts, recently in the community by Talbot (2009) and Van Durme and Lall (2009) = En i=1 miσ nm* En Xi xi n (1= 1 m*) n2m* i=1 n m ( m σi + −σi) � *+mi l=1 � *�mi l=1 52 Figure 5: Results on averaging randomly generated sequences, with m∗ = 100, g = 100, and using an 8 bit Morris-style counter of base 2. Larger reservoir sizes lead to better approximation, at higher cost in bits. to provide a streaming extension to the Bloom-filter based count-storage mechanism of Talbot and Osborne (2007a) and Talbot and Osborne (2007b). See (Flajolet, 1985) for a detailed analysis of Morrisstyle counting. 3.3 Experiment We show through experimentation on synthetic data that this approach gives reasonable levels of accuracy at space efficient sizes of the length and sum parameter. Random sequences of 1,000 values were generated by: (1) fix a value for m*; (2) draw a polarity bias term p uniformly from the range [0,1]; then (3) for each value, x: (a) Q was positive with probability p; (b) m was drawn from [0, m*]. Figure 5 shows results for varying reservoir sizes (using 4, 8 or 12 bits) when </context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007b. Smoothed Bloom filter language models: Tera-Scale LMs on the Cheap. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
</authors>
<title>Succinct approximate counting of skewed data.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="16582" citStr="Talbot (2009)" startWordPosition="2889" endWordPosition="2890"> v := m+m* g 2m* 4: v := �v] with probability v − Lv], Lv] otherwise 5: s&apos; := ResUp(ng, k, v, σ, s) 6: s&apos; := ResUp((ng + v), k, g − v, −σ, s&apos;) 7: Return s&apos; Log-scale Counting For additional space savings we might approximate the length parameter t with a small bit representation, using the approximate counting scheme of Morris (1978). The method enables counting in log-scale by probabilistically incrementing a counter, where it becomes less and less likely to update the counter after each increment. This scheme is popularly known and used in a variety of contexts, recently in the community by Talbot (2009) and Van Durme and Lall (2009) = En i=1 miσ nm* En Xi xi n (1= 1 m*) n2m* i=1 n m ( m σi + −σi) � *+mi l=1 � *�mi l=1 52 Figure 5: Results on averaging randomly generated sequences, with m∗ = 100, g = 100, and using an 8 bit Morris-style counter of base 2. Larger reservoir sizes lead to better approximation, at higher cost in bits. to provide a streaming extension to the Bloom-filter based count-storage mechanism of Talbot and Osborne (2007a) and Talbot and Osborne (2007b). See (Flajolet, 1985) for a detailed analysis of Morrisstyle counting. 3.3 Experiment We show through experimentation on s</context>
</contexts>
<marker>Talbot, 2009</marker>
<rawString>David Talbot. 2009. Succinct approximate counting of skewed data. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Probabilistic Counting with Randomized Storage.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<marker>Van Durme, Lall, 2009</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2009. Probabilistic Counting with Randomized Storage. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Online Generation of Locality Sensitive Hash Signatures.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Van Durme, Lall, 2010</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2010. Online Generation of Locality Sensitive Hash Signatures. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
<author>Ashwin Lall</author>
</authors>
<title>Efficient Online Locality Sensitive Hashing via Reservoir Counting.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Van Durme, Lall, 2011</marker>
<rawString>Benjamin Van Durme and Ashwin Lall. 2011. Efficient Online Locality Sensitive Hashing via Reservoir Counting. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Jerboa: A toolkit for randomized and streaming algorithms.</title>
<date>2012</date>
<tech>Technical Report 7,</tech>
<institution>Human Language Technology Center of Excellence, Johns Hopkins University.</institution>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012. Jerboa: A toolkit for randomized and streaming algorithms. Technical Report 7, Human Language Technology Center of Excellence, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey S Vitter</author>
</authors>
<title>Random sampling with a reservoir.</title>
<date>1985</date>
<journal>ACM Trans. Math. Softw.,</journal>
<pages>11--37</pages>
<contexts>
<context position="11633" citStr="Vitter (1985)" startWordPosition="1938" endWordPosition="1939">in by reviewing the method of Reservoir Counting, then extend it to a new notion we refer to as Reservoir Averaging. This will allow in the subsequent section to map our analytic model to a form Figure 4: Social media platforms such as Facebook or Twitter deal with a very large number of individuals, each with a variety of implicit attributes (such as gender). This motivates a desire for online space efficiency. explicitly amenable to keeping an online average. 3.1 Reservoir Counting Reservoir Counting plays on the folklore algorithm of reservoir sampling, first described in the literature by Vitter (1985). As applied to a stream of arbitrary elements, reservoir sampling maintains a list (reservoir) of length k, where the contents of the reservoir represents a uniform random sample over all elements 1...t observed thus far in the stream. When the stream is a sequence of positive and negative integers, reservoir counting implicitly views each value as being unrolled into a sequence made up of either 1 or -1. For instance, the sequence: (3, -2, 1) would be viewed as: (1, 1, 1, -1, -1, 1) Since there are only two distinct values in this stream, the contents of the reservoir can be characterized by</context>
</contexts>
<marker>Vitter, 1985</marker>
<rawString>Jeffrey S. Vitter. 1985. Random sampling with a reservoir. ACM Trans. Math. Softw., 11:37–57, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
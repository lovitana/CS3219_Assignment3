<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000076">
<title confidence="0.997382">
Entropy-based Pruning for Phrase-based Machine Translation
</title>
<author confidence="0.993774">
Wang Ling, Jo˜ao Grac¸a, Isabel Trancoso, Alan Black
</author>
<affiliation confidence="0.961192">
L2F Spoken Systems Lab, INESC-ID, Lisboa, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
</affiliation>
<email confidence="0.7031705">
{wang.ling,joao.graca,isabel.trancoso}@inesc-id.pt
awb@cs.cmu.edu
</email>
<sectionHeader confidence="0.998564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999005583333333">
Phrase-based machine translation models
have shown to yield better translations than
Word-based models, since phrase pairs en-
code the contextual information that is needed
for a more accurate translation. However,
many phrase pairs do not encode any rele-
vant context, which means that the transla-
tion event encoded in that phrase pair is led
by smaller translation events that are indepen-
dent from each other, and can be found on
smaller phrase pairs, with little or no loss in
translation accuracy. In this work, we pro-
pose a relative entropy model for translation
models, that measures how likely a phrase pair
encodes a translation event that is derivable
using smaller translation events with similar
probabilities. This model is then applied to
phrase table pruning. Tests show that con-
siderable amounts of phrase pairs can be ex-
cluded, without much impact on the transla-
tion quality. In fact, we show that better trans-
lations can be obtained using our pruned mod-
els, due to the compression of the search space
during decoding.
</bodyText>
<sectionHeader confidence="0.999451" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998556952380953">
Phrase-based Machine Translation Models (Koehn
et al., 2003) model n-to-m translations of n source
words to m target words, which are encoded in
phrase pairs and stored in the translation model.
This approach has an advantage over Word-based
Translation Models (Brown et al., 1993), since trans-
lating multiple source words allows the context for
each source word to be considered during trans-
lation. For instance, the translation of the En-
glish word “in” by itself to Portuguese is not ob-
vious, since we do not have any context for the
word. This word can be translated in the con-
text of “in (the box)” to “dentro”, or in the con-
text of “in (China)” as “na”. In fact, the lexical
entry for “in” has more than 10 good translations
in Portuguese. Consequently, the lexical translation
entry for Word-based models splits the probabilis-
tic mass between different translations, leaving the
choice based on context to the language model. On
the other hand, in Phrase-based Models, we would
have a phrase pair p(in the box, dentro da caixa)
and p(in china, na china), where the words “in the
box” and “in China” can be translated together to
“dentro da caixa” and “na China”, which substan-
tially reduces the ambiguity. In this case, both the
translation and language models contribute to find
the best translation based on the local context, which
generally leads to better translations.
However, not all words add the same amount of
contextual information. Using the same example for
“in”, if we add the context “(hid the key) in”, it is
still not possible to accurately identify the best trans-
lation for the word “in”. The phrase extraction algo-
rithm (Ling et al., 2010) does not discriminate which
phrases pairs encode contextual information, and ex-
tracts all phrase pairs with consistent alignments.
Hence, phrases that add no contextual information,
such as, p(hid the key in, escondeu a chave na)
and p(hid the key in, escondeu a chave dentro)
are extracted. This is undesirable because we are
populating translation models with redundant phrase
pairs, whose translations can be obtained using com-
</bodyText>
<page confidence="0.940752">
962
</page>
<note confidence="0.8378385">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 962–971, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999324909090909">
binations of other phrases with the same probabil-
ities, namely p(hid the key, escondeu a chave),
p(in, dentro) and p(in, na). This is a problem
that is also found in language modeling, where
large amounts of redundant higher-order n-grams
can make the model needlessly large. For backoff
language models, multiple pruning strategies based
on relative entropy have been proposed (Seymore
and Rosenfeld, 1996) (Stolcke, 1998), where the ob-
jective is to prune n-grams in a way to minimize the
relative entropy between the model before and after
pruning.
While the concept of using relative entropy for
pruning is not new and frequently used in backoff
language models, there are no such models for ma-
chine translation. Thus, the main contribution of
our work is to propose a relative entropy pruning
model for translation models used in Phrase-based
Machine Translation. It is shown that our pruning
algorithm can eliminate phrase pairs with little or
no impact in the predictions made in our translation
model. In fact, by reducing the search space, less
search errors are made during decoding, which leads
to improvements in translation quality.
This paper is organized as follows. We describe
and contrast the state of the art pruning algorithms
in section 2. In section 3, we describe our relative-
entropy model for machine translation. Afterwards,
in section 4, we apply our model for pruning in
Phrase-based Machine Translation systems. We per-
form experiments with our pruning algorithm based
on phrase pair independence and analyse the results
in section 5. Finally, we conclude in section 6.
</bodyText>
<sectionHeader confidence="0.978466" genericHeader="introduction">
2 Phrase Table Pruning
</sectionHeader>
<bodyText confidence="0.999188">
Phrase table pruning algorithms are important in
translation, since they efficiently reduce the size of
the translation model, without having a large nega-
tive impact in the translation quality. This is espe-
cially relevant in environments where memory con-
straints are imposed, such as translation systems for
small devices like cellphones, and also when time
constraints for the translation are defined, such as
online Speech-to-Speech systems.
</bodyText>
<subsectionHeader confidence="0.996385">
2.1 Significance Pruning
</subsectionHeader>
<bodyText confidence="0.999986666666667">
A relevant reference in phrase table pruning is the
work of (Johnson and Martin, 2007), where it is
shown that a significant portion of the phrase ta-
ble can be discarded without a considerable negative
impact on translation quality, or even positive one.
This work computes the probability, named p-value,
that the joint occurrence event of the source phrase
s and target phrase t occurring in same sentence pair
happens by chance, and are actually statistically in-
dependent. Phrase pairs that have a high p-value,
are more likely to be spurious and more prone to
be pruned. This work is followed in (Tomeh et al.,
2009), where phrase pairs are treated discriminately
based on their complexity. Significance-based prun-
ing has also been successfully applied in language
modeling in (Moore and Quirk, 2009).
Our work has a similar objective, but instead
of trying to predict the independence between the
source and target phrases in each phrase pair, we at-
tempt to predict the independence between a phrase
pair and other phrase pairs in the model.
</bodyText>
<subsectionHeader confidence="0.998209">
2.2 Relevance Pruning
</subsectionHeader>
<bodyText confidence="0.999756304347826">
Another proposed approach (Matthias Eck and
Waibel, 2007) consists at collecting usage statistics
for phrase pairs. This algorithm decodes the train-
ing corpora and extracts the number of times each
phrase pair is used in the 1-best translation hypoth-
esis. Thus, phrase pairs that are rarely used during
decoding are excluded first during pruning.
This method considers the relationship between
phrase pairs in the model, since it tests whether
the decoder is more prone to use some phrase pairs
than others. However, it leads to some undesirable
pruning choices. Let us consider a source phrase
“the box in China” and 2 translation hypotheses,
where the first hypothesis uses the phrase transla-
tion p(the key in China, a chave na China) with
probability 70%, and the second hypothesis uses
two phrase translations p(the key, a chave) and
p(in China, na China) with probability 65%. This
approach will lean towards pruning the phrase pairs
in the second hypothesis, since the decoder will use
the first hypothesis. This is generally not desired,
since the 2 smaller phrase pairs can be used to trans-
late the same source sentence with a small probabil-
</bodyText>
<page confidence="0.998011">
963
</page>
<bodyText confidence="0.999950222222222">
ity loss (5%), even if the longer phrase is pruned.
On the other hand, if the smaller phrases are pruned,
the longer phrase can not be used to translate smaller
chunks, such as “the key in Portugal”. This matter is
aggravated due to the fact that the training corpora is
used to decode, so longer phrase pairs will be used
more frequently than when translating unseen sen-
tences, which will make the model more biased into
pruning shorter phrase pairs.
</bodyText>
<sectionHeader confidence="0.817649" genericHeader="method">
3 Relative Entropy Model For
Phrase-based Translation Models
</sectionHeader>
<bodyText confidence="0.999965625">
In this section, we shall define our entropy model
for phrase pairs. We start by introducing some no-
tation to distinguish different types of phrase pairs
and show why some phrase pairs are more redun-
dant than others. Afterwards, we illustrate our no-
tion of relative entropy between phrase pairs. Then,
we describe our entropy model, its computation and
its application to phrase table pruning.
</bodyText>
<subsectionHeader confidence="0.999019">
3.1 Atomic and Composite Phrase Pairs
</subsectionHeader>
<bodyText confidence="0.999949842857143">
We discriminate between 2 types of phrase pairs:
atomic phrase pairs and composite phrase pairs.
Atomic phrase pairs define the smallest transla-
tion units, such that given an atomic phrase pair that
translates from s to t, the same translation cannot
be obtained using any combination of other phrase
pairs. Removing these phrase pairs reduces the
range of translations that our model is capable of
translating and also the possible translations.
Composite phrase pairs define translations of a
given sequence of words that can also be obtained
using atomic or other smaller composite phrase
pairs. Each combination is called a derivation or
translation hypothesis. Removing these phrase pairs
does not change the amount of sentences that the
model can translate, since all translations encoded
in these phrases can still be translated using other
phrases, but these will lead to different translation
probabilities.
Considering table 1, we can see that atomic
phrases encode one elementary translation event,
while composite phrases encode joint events that are
encoded in atomic phrase pairs. If we look at the
source phrase “in”, there is a multitude of possible
translations for this word in most target languages.
Taking Portuguese as the target language, the proba-
bility that “in” is translated to “em” is relatively low,
since it can also be translated to “no”, “na”, “den-
tro”, “dentro de” and many others.
However, if we add another word such as “Por-
tugal” forming “in Portugal”, it is more likely that
“in” is translated to “em”. Thus, we define the
joint event of “in” translating to “em” (A1) and
“Portugal” to “Portugal” (B1), denoted as A1 n B1,
in the phrase pair p(in Portugal, em Portugal).
Without this phrase pair it is assumed that these
are independent events with probability given by
P(A1)P(B1)1, which would be 10%, leading to a
60% reduction. In this case, it would be more likely,
that in Portugal is translated to no Portugal or
na Portugal, which would be incorrect.
Some words, such as “John”, forming “John in”,
do not influence the translations for the word “in”,
since it can still be translated to “em”, “no”, “na”,
“dentro” or “dentro de” depending on the word that
follows. By definition, if the presence of phrase
p(John, John) does not influence the translation of
p(in, em) and viceversa, we can say that probability
of the joint event P(A1 n C1) is equal to the product
of the probabilities of the events P(A1)P(C1).
If we were given a choice of pruning either the
composite phrase pairs p(John in, John em) or
p(in Portugal, em Portugal), the obvious choice
would be the former, since the probability of the
event encoded in that phrase pair is composed by 2
independent events, in which case the decoder will
inherently consider the hypothesis that “John in” is
translated to “John em” with the same probability. In
another words, the model’s predictions even, with-
out this phrase pair will remain the same.
The example above shows an extreme case,
where the event encoded in the phrase pair
p(John in, John em) is decomposed into indepen-
dent events, and can be removed without chang-
ing the model’s prediction. However, finding and
pruning phrase pairs that are independent, based on
smaller events is impractical, since most translation
events are not strictly independent. However, many
phrase pairs can be replaced with derivations using
smaller phrases with a small loss in the model’s pre-
</bodyText>
<footnote confidence="0.995734">
1For simplicity, we assume at this stage that no reordering
model is used
</footnote>
<page confidence="0.989725">
964
</page>
<table confidence="0.997020125">
Phrase Pair Prob Event
Atomic Phrase Pairs
in —i em 10% A1
in —i na 20% A2
in —i no 20% A3
in —i dentro 5% A4
in —i dentro de 5% A5
Portugal —i Portugal 100% B1
John —i John 100% C1
Composite Phrase Pairs
in Portugal —i em Portugal 70% A1 n B1
John in —i John em 10% C1 n A1
John in —i John na 20% C1 n A2
John in —i John no 20% C1 n A3
John in —i John dentro 5% C1 n A4
John in —i John dentro de 5% C1 n A5
</table>
<tableCaption confidence="0.999776">
Table 1: Phrase Translation Table with associated events
</tableCaption>
<bodyText confidence="0.998875555555556">
dictions.
Hence, we would like to define a metric for phrase
pairs that allows us evaluate how discarding each
phrase pair will affect the pruned model’s predic-
tions. By removing phrase pairs that can be derived
using smaller phrase pairs with similar probability,
it is possible to discard a significant portion of the
translation model, while minimizing the impact on
the model’s predictions.
</bodyText>
<subsectionHeader confidence="0.9104535">
3.2 Relative Entropy Model for Machine
Translation
</subsectionHeader>
<bodyText confidence="0.9987175">
For each phrase pair pa, we define the supporting
set SP(pa(s, t)) = S1, ..., Sk, where each element
Si = pi, ..., pj is a distinct derivation of pa(s, t) that
translates s to t, with probability P(Si) = P(pi) x
...xP(pj). A phrase pair can have multiple elements
in its supporting set. For instance, the phrase pair
p(John in Portugal, John em Portugal), has 3
elements in the support set:
</bodyText>
<listItem confidence="0.999518333333333">
• S1 = {p(John, John), p(in, em), p(Portugal, Portugal)}
• S2 = {p(John, John), p(in Portugal, em Portugal)}
• S3 = {p(John in, John em), p(Portugal, Portugal)}
</listItem>
<bodyText confidence="0.984540368421053">
S1, S2 and S3 encode 3 different assumptions
about the event of translating “John in Portugal”
to “John em Portugal”. S1 assumes that the event
is composed by 3 independent events A1, B1 and
C1, S2 assumes that A1 and B1 are dependent, and
groups them into a single composite event A1 n B1,
which is independent from C1, and S3 groups A1
and C1 independently from B1. As expected, the
event encoded in the phrase pair p itself is A1 nB1 n
C1, which assumes that A1, B1 and C1 are all depen-
dent. We can see that if any of the events S1, S2 or
S3 has a “similar probability” as the event coded in
the phrase pair, we can remove this phrase pair with
a minimal impact in the phrase prediction.
To formalize our notion of “similar probabil-
ity”, we apply the relative entropy or the Kullback-
Leibler divergence, and define the divergence be-
tween a pruned translation model Pp(s, t) and the
unpruned model P(s, t) as:
</bodyText>
<equation confidence="0.79705225">
� Pp(t|s)
D(Pp||P) = − P(s, t)log P (t|s) (1)
s,t
Where PW
</equation>
<bodyText confidence="0.999212">
I ) , measures the deviation from the
probability emission from the pruned model and the
original probability from the unpruned model, for
each source-target pair s, t. This is weighted by
the frequency that the pair s, t is observed, given by
P(s, t).
Our objective is to minimize D(Pp||P), which
can be done locally by removing phrase pairs p(s, t)
with the lowest values for −P(s, t)log pP(t  |s) . Ide-
ally, we would want to minimize the relative entropy
for all possible source and target sentences, rather
than all phrases in our model. However, minimiz-
ing such an objective function would be intractable
due to reordering, since the probability assigned to a
phrase pair in a sentence pair by each model would
depend on the positioning of all other phrase pairs
used in the sentence. Because of these dependen-
cies, we would not be able to reduce this problem to
a local minimization problem. Thus, we assume that
all phrase pairs have the same probability regardless
of their context in a sentence.
Thus, our pruning algorithm takes a threshold 6
and prunes all phrase pairs that fail to meet the fol-
lowing criteria:
</bodyText>
<equation confidence="0.9972585">
Pp(t|s)
−P(s, t)log P (t|s) �6 (2)
</equation>
<bodyText confidence="0.9941615">
The main components of this function is the ratio
between the emission from the pruned model and
</bodyText>
<page confidence="0.98949">
965
</page>
<bodyText confidence="0.841806">
unpruned models given by Pp(t|s)
</bodyText>
<equation confidence="0.412044">
P (t|s) , and the weight
</equation>
<bodyText confidence="0.999757333333333">
given to each s, t pair given by P(s, t). In the re-
mainder of this section, we will focus on how to
model each of these components in equation 2.
</bodyText>
<subsectionHeader confidence="0.993302">
3.3 Computing P(s, t)
</subsectionHeader>
<bodyText confidence="0.999862166666667">
The term P(s, t) can be seen as a weighting function
for each s, t pair. There is no obvious optimal dis-
tribution to model P(s, t). In this work, we apply 2
different distributions for P(s, t). First, an uniform
distribution, where all phrases are weighted equally.
Secondly, a multinomial function defined as:
</bodyText>
<equation confidence="0.996679666666667">
N(s, t)
P(s, t) = (3)
N
</equation>
<bodyText confidence="0.999810333333333">
where N is the number of sentence pairs in the paral-
lel data, and N(s, t) is the number of sentence pairs
where s was observed in the source sentence and t
was observed in the target sentence. Using this dis-
tribution, the model is more biased in pruning phrase
pairs with s, t pairs that do not occur frequently.
</bodyText>
<subsectionHeader confidence="0.685342">
3.4 Computing Pp(t|s)
</subsectionHeader>
<equation confidence="0.846259333333333">
P (t|s)
The computation of Pp(t|s)
P (t|s) depends on how the de-
</equation>
<bodyText confidence="0.9998825">
coder adapts when a phrase pair is pruned from the
model. In the case of back-off language models,
this can be solved by calculating the difference of
the logs between the n-gram estimate and the back-
off estimate. However, a translation decoder gen-
erally functions differently. In our work, we will
assume that the decoding will be performed using
a Viterbi decoder, such as MOSES (Koehn et al.,
2007), where the translation with the highest score
is chosen.
In the example above, where s=”John in Portu-
gal” and t=”John em Portugal”, the decoder would
choose the derivation with the highest probability
from s to t. Using the unpruned model, the possi-
ble derivations are either using phrase p(s, t) or one
element of its support set S1, S2 or S3. On the other
hand, on the pruned model where p(s, t) does not
exist, only S1, S2 and S3 can be used. Thus, given
a s, t pair one of three situations may occur. First, if
the probability of the phrase pair p(s, t) is lower than
the highest probability element in SP(p(s, t)), then
both the models will choose that element, in which
</bodyText>
<equation confidence="0.993554">
Pp(t|s)
P (t|s) = 1. This can happen, if we define
</equation>
<bodyText confidence="0.9623055">
features that penalize longer phrase pairs, such as
lexical weighting, or if we apply smoothing (Foster
et al., 2006). Secondly, if the probability of p(s, t)
is equal to the most likely element in SP(p(s, t)),
regardless of whether the unpruned model choses to
use p(s, t) or that element, the probability emissions
of the pruned and unpruned model will be identi-
cal. Thus, for this case Pp(t|s)
</bodyText>
<equation confidence="0.985192">
P (t|s) = 1. Finally, if the
</equation>
<bodyText confidence="0.997581444444445">
probability of p(s, t) is higher than other possible
derivations, the unpruned model will choose to emit
the probability of p(s, t), while the pruned model
will emit the most likely element in SP(p(s, t)).
Hence, the probability loss between the 2 models,
will be the ratio between the probability of p(s, t)
and the probability of the most likely element in
SP(p(s, t)).
From the example above, we can generalize the
</bodyText>
<equation confidence="0.9550568">
function for Pp(t|s)
P (t|s) as:
11
p&apos;Eargmax(SP(p(s,t))) P(pI) (4)
P(p(s, t))
</equation>
<bodyText confidence="0.982764857142857">
Where P(p(s, t)) denotes the probability of
p(s, t) and 11p&apos;Eargmax(SP(p(s,t))) P(pI) the most
likely sequence of phrasal translations that translates
s to t, with the probability equal to the product of all
phrase translation probabilities in that sequence.
Replacing in equation 2, our final condition that
must be satisfied for keeping a phrase pair is:
</bodyText>
<equation confidence="0.9900835">
−P(s t)log11p&apos;Eargmax(SP(p(s,t))) P(pI) &gt; δ (5)
P(p(s, t))
</equation>
<sectionHeader confidence="0.9931455" genericHeader="method">
4 Application for Phrase-based Machine
Translation
</sectionHeader>
<bodyText confidence="0.9999578">
We will now show how we apply our entropy prun-
ing model in the state-of-the-art phrase-based trans-
lation system MOSES and describe the problems
that need to be addressed during the implementation
of this model.
</bodyText>
<subsectionHeader confidence="0.910055">
4.1 Translation Model
</subsectionHeader>
<bodyText confidence="0.9722818">
The translation model in Moses is composed by
a phrase translation model and a phrase reorder-
ing model. The first one models, for each phrase
pair p(s, t), the probability of translating the s to
t by combining multiple features φi, weighted by
</bodyText>
<page confidence="0.8287395">
case,
966
</page>
<bodyText confidence="0.938359">
wTi , as PT (p) = �ni=1 Oi(p)w�% . The reordering
model is similar, but models the local reordering be-
tween p, given the previous and next phrase accord-
ing to the target side, pP/, and pN, or more formally,
PR(p|pP,pN) = Hmi=1 4&apos;i(p|pP,pP)wR
</bodyText>
<subsectionHeader confidence="0.994231">
4.2 Building the Support Set
</subsectionHeader>
<bodyText confidence="0.999440952380953">
Essentially, implementing our model is equiva-
lent to calculating the components described in
equation 5. These are P(s, t), P(p(s|t)) and
argmax(SP(p(s, t))). Calculating the uniform dis-
tribution and multinomial distributions for P(s, t)
is simple, the uniform distribution just assumes the
same value for all s and t, and the multinomial dis-
tribution can be modeled by extracting counts from
the parallel corpora.
Calculating P(s|t) is also trivial, since it only en-
volves calculating PT(p(s, t)), which can be done
by retrieving the translation features of p and apply-
ing the weights for each feature.
The most challenging task is to calculate
argmax(SP(p(s, t))), which is similar to the de-
coding task in machine translation, where we need to
find the best translation t� for a sentence s, that is, t� =
argmaxtP(s|t)P(t). In practice, we are not search-
ing in the space of possible translations, but in the
space of possible derivations, which are sequences
of phrase translations p1(s1, t1), ..., pn(sn, tn) that
can be applied to s to generate an output t with the
score given by P(t) Hni=1 P(si, ti).
Our algorithm to determine SP(p(s, t)) can be
described as an adaptation to the decoding algorithm
in Moses, where we restrict the search space to the
subspace SP(p(s, t)), that is, our search space is
only composed by derivations that output t, with-
out using p itself. This can be done using the forced
decoding algorithm proposed in (Schwartz, 2008).
Secondly, the score of a given translation hypothesis
does not depend on the language model probability
P(t), since all derivations in this search space have
the same t, thus we discard this probability from
the score function. Finally, rather than using beam
search, we exhaustively search all the search space,
to reduce the hypothesis of incurring a search error
at this stage. This is possible, since phrase pairs are
generally smaller than text (less than 8 words), and
because we are constraining the search space to t,
which is an order of magnitude smaller than the reg-
ular search space with all possible translations.
</bodyText>
<subsectionHeader confidence="0.998047">
4.3 Pruning Algorithm
</subsectionHeader>
<bodyText confidence="0.878890375">
The algorithm to generate a pruned translation
model is shown in 1. We iterate over all phrase pairs
p1(s1, t1), ..., pn(sn, tn), decode using our forced
decoding algorithm from si to ti, to obtain the best
path S. If no path is found then it means that the pi
is atomic. Then, we prune pi based on condition 5.
Algorithm 1 Independence Pruning
Require: pruning threshold 6,
</bodyText>
<equation confidence="0.886169363636364">
unpruned model {p1(s1, t1), ..., pn(sn, tn)}
for pi(si, ti) E {p1(s1, t1), ..., pn(sn, tn)} do
S := argmax(SP(pi)) \ pi
score := 00
if S =� {} then
score := −P(s, t) log Hp0(30X )ES) (s0 |t0)
end if
if score &lt; 6 then
prune(pi)
end if
end for
</equation>
<bodyText confidence="0.985784166666667">
return pruned model
The main bottle neck in this algorithm is find-
ing argmax(SP(pi)). While this appears relatively
simple and similar to a document decoding task, the
size of our task is on a different order of magni-
tude, since we need to decode every phrase pair in
the translation model, which might not be tractable
for large models with millions of phrase pairs. We
address this problem in section 5.3.
Another problem with this algorithm is that the
decision to prune each phrase pair is made assuming
that all other phrase pairs will remain in the model.
Thus, there is a chance a phrase pair p1 is pruned
because of a derivation using p2 and p3 that leads to
the same translation. However, if p3 also happens to
be pruned, such a derivation will no longer be pos-
sible. One possible solution to address this problem
is to perform pruning iteratively, from the smallest
phrase pairs (number of words) and increase the size
at each iteration. However, we find this undesirable,
since the model will be biased into removing smaller
phrase pairs, which are generally more useful, since
they can be used in multiple derivation to replace
larger phrase pairs. In the example above, the model
</bodyText>
<page confidence="0.994026">
967
</page>
<bodyText confidence="0.99993">
would eliminate p3 and keep p1, yet the best deci-
sion could be to keep p3 and remove p1, if p3 is also
frequently used in derivations of other phrase pairs.
Thus, we leave the problem of finding the best set of
phrases to prune as future work.
</bodyText>
<sectionHeader confidence="0.999414" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999997222222222">
We tested the performance of our system under two
different environments. The first is the small scale
DIALOG translation task for IWSLT 2010 evalua-
tion (Paul et al., 2010) using a small corpora for
the Chinese-English language pair (henceforth re-
ferred to as “IWSLT”). The second one is a large
scale test using the complete EUROPARL (Koehn,
2005) corpora for the Portuguese-English language
pair, which we will denote by “EUROPARL”.
</bodyText>
<subsectionHeader confidence="0.978914">
5.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999949">
The IWSLT model was trained with 30K training
sentences. The development corpus and test corpus
were taken from the evaluation dataset in IWSLT
2006 (489 tuning and 500 test sentences with 7 ref-
erences). The EUROPARL model was trained using
the EUROPARL corpora with approximately 1.3M
sentence pairs, leaving out 1K sentences for tuning
and another 1K sentences for tests.
</bodyText>
<subsectionHeader confidence="0.99654">
5.2 Setup
</subsectionHeader>
<bodyText confidence="0.999997285714286">
In the IWSLT experiment, word alignments were
generated using an HMM model (Vogel et al., 1996),
with symmetric posterior constraints (V. Grac¸a et
al., 2010), using the Geppetto toolkit2. This setup
was used in the official evaluation in (Ling et al.,
2010). For the EUROPARL experiment the word
alignments were generated using IBM model 4. In
both experiments, the translation model was built
using the phrase extraction algorithm (Paul et al.,
2010), with commonly used features in Moses (Ex:
probability, lexical weighting, lexicalized reordering
model). The optimization of the translation model
weights was done using MERT tuning (Och, 2003)
and the results were evaluated using BLEU-4.
</bodyText>
<subsectionHeader confidence="0.999754">
5.3 Pruning Setup
</subsectionHeader>
<bodyText confidence="0.999803">
Our pruning algorithm is applied after the translation
model weight optimization with MERT. We gener-
</bodyText>
<footnote confidence="0.849084">
2http://code.google.com/p/geppetto/
</footnote>
<bodyText confidence="0.999763548387097">
ate multiple translation models by setting different
values for 6, so that translation models of different
sizes are generated at intervals of 5%. We also run
the significance pruning (Johnson and Martin, 2007)
algorithm in these conditions.
While the IWSLT translation model has only
88,424 phrase pairs, for the EUROPARL exper-
iment, the translation model was composed by
48,762,372 phrase pairs, which had to be decoded.
The average time to decode each phrase pair us-
ing the full translation model is 4 seconds per sen-
tence, since the table must be read from disk due to
its size. This would make translating 48M phrase
pairs unfeasible. To address this problem, we di-
vide the phrase pairs in the translation model into
blocks of K phrase pairs, that are processed sepa-
rately. For each block, we resort to the approach
used in MERT tuning, where the model is filtered to
only include the phrase pairs that are used for trans-
lating tuning sentences. We filter each block with
phrase pairs from K to 2K with the source sentences
sK, ..., s2K. Furthermore, since we are force de-
coding using the target sentences, we also filter the
remaining translation models using the target sen-
tences tK, ..., t2K. We used blocks of 10,000 phrase
pairs and each filtered table was reduced to less than
1% of the translation table on average, reducing the
average decoding time to 0.03 seconds per sentence.
Furthermore, each block can be processed in parallel
allowing multiple processes to be used for the task,
depending on the resources that are available.
</bodyText>
<subsectionHeader confidence="0.745218">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.999882933333333">
Figure 1 shows the BLEU results for different sizes
of the translation model for the IWSLT experiment
using the uniform and multinomial distributions for
P(s, t). We observe that there is a range of values
from 65% to 95% where we actually observe im-
provements caused by our pruning algorithm, with
the peak at 85% for the uniform distribution, where
we improve from 15.68 to 15.82 (0.9% improve-
ment). Between 26% and 65%, the BLEU score is
lower than the baseline at 100%, with the minimum
at 26% with 15.54, where only atomic phrase pairs
remain and both the multinomial and uniform distri-
bution have the same performance, obviously. This
is a considerable reduction in phrase table size by
sacrificing 0.14 BLEU points. Regarding the com-
</bodyText>
<page confidence="0.996373">
968
</page>
<figureCaption confidence="0.98560925">
Figure 1: Results for the IWSLT experiment. The x-
axis shows the percentage of the phrase table used. The
BLEU scores are shown in the y-axis. Two distributions
for P(s, t) were tested Uniform and Multinomial.
</figureCaption>
<bodyText confidence="0.999134838709677">
parison between the uniform and multinomial distri-
bution, we can see that both distributions yield sim-
ilar results, specially when a low number of phrase
pairs is pruned. In theory, the multinomial distri-
bution should yield better results, since the pruning
model will prefer to prune phrase pairs that are more
likely to be observed. However, longer phrase pairs,
which tend compete with other long phrase pairs on
which get pruned first. These phrase pairs gener-
ally occur only once or twice, so the multinomial
model will act similarly to the uniform model re-
garding longer phrase pairs. On the other hand, as
the model size reduces, we can see that using multi-
nomial distribution seems to start to improve over
the uniform distribution.
The comparison between our pruning model and
pruning based on significance is shown in table 2.
These models are hard to compare, since not all
phrase table sizes can be obtained using both met-
rics. For instance, the significance metric can ei-
ther keep or remove all phrase pairs that only appear
once, leaving a large gap of phrase table sizes that
cannot be attained. In the EUROPARL experiment
the sizes of the table suddenly drops from 60% to
8%. The same happens with our metric that cannot
distinguish atomic phrase pairs. In the EUROPARL
experiment, we cannot generate phrase tables with
sizes smaller than 15%. Thus, we only show re-
sults at points where both algorithms can produce
a phrase table.
Significant improvements are observed in the
</bodyText>
<table confidence="0.984007538461539">
Table size Significance Entropy (u) Entropy (m)
Pruning Pruning Pruning
IWSLT
57K (65%) 14.82 15.77 15.78
71K (80%) 15.14 15.76 15.77
80K (90%) 15.31 15.73 15.72
88K (100%) 15.68 15.68 15.68
EUROPARL
29M (60%) 28.64 28.82 28.91
34M (70%) 28.84 28.94 28.99
39M (80%) 28.86 28.99 28.99
44M (90%) 28.91 29.00 29.02
49M (100%) 29.18 29.18 29.18
</table>
<tableCaption confidence="0.98960025">
Table 2: Comparison between Significance Pruning (Sig-
nificance Pruning) and Entropy-based pruning using the
uniform (Entropy (u) Pruning) and multinomial distribu-
tions (Entropy (m) Pruning).
</tableCaption>
<bodyText confidence="0.999157611111111">
IWSLT experiment, where significance pruning
does not perform as well. On the other hand, on the
EUROPARL experiment, our model only achieves
slightly higher results. We believe that this is re-
lated by the fact the EUROPARL corpora is gener-
ated from automatically aligning documents, which
means that there are misaligned sentence pairs.
Thus, many spurious phrase pairs are extracted. Sig-
nificance pruning performs well under these condi-
tions, since the measure is designed for this purpose.
In our metric, we do not have any means for detect-
ing spurious phrase pairs, in fact, spurious phrase
pairs are probably kept in the phrase table, since
each distinct spurious phrase pair is only extracted
once, and thus, they have very few derivations in
its support set. This suggests, that the significance
score can be integrated in our model to improve our
model, which we leave as future work.
</bodyText>
<figureCaption confidence="0.9856075">
Figure 2: Translation order in for different reordering
starting from left to right.
</figureCaption>
<bodyText confidence="0.829022">
We believe that in language pairs such as Chinese-
</bodyText>
<figure confidence="0.997105414634146">
IWSLT Results
15.85
15.8
15.75
15.7
15.65
15.6
15.55
15.5
Uniform
Multinomial
25%
30%
35%
40%
45%
50%
55%
60%
65%
70%
75%
80%
85%
90%
95%
100%
b)
a)
married
married
in
John married Portugal
married
in
John
in
Portugal
in
Portugal
Portugal
</figure>
<page confidence="0.994445">
969
</page>
<bodyText confidence="0.999921412698413">
English with large distance reorderings between
phrases are more prone to search errors and benefit
more from our pruning algorithm. To illustrate this,
let us consider the source sentence “John married
in Portugal”, and translating either using the blocks
“John”, “married” and “in Portugal” or the blocks
“John”, “married in”, “Portugal”, the first hypoth-
esis would be much more viable, since the word
“Portugal” is more relevant as the context for the
word “in”. Thus, the key choice for the decoder is
to decide whether to translate using “married” with
or without “in”, and it is only able to predict that
it is better to translate “married” by itself until it
finds that “in” is better translated with “Portugal”.
Thus, a search error occurs if the hypothesis where
“married” is translated by itself is removed. In fig-
ure 2, we can see the order that blocks are consid-
ered for different reorderings, starting from left to
right. In a), we illustrate the case for a monotonous
translation. We observe that the correct decision be-
tween translating “married in” or just “married” is
found immediately, since the blocks “Portugal” and
“in Portugal” are considered right afterwards. In this
case, it is unlikely that the hypothesis using “mar-
ried” is removed. However, if we consider that due
to reordering, “John” is translated after “married”
and before “Portugal”, which is shown in b). Then,
the correct decision can only be found after consid-
ering “John”. In this case, “John” does not have
many translations, so the likelihood of eliminating
the correct hypothesis. However, if there were many
translations for John, it is highly likely that the cor-
rect partial hypothesis is eliminated. Furthermore,
the more words exist between “married” and “Portu-
gal”, the more likely will the correct hypothesis not
exist when we reach “Portugal”. By pruning the hy-
pothesis “married in” a priori, we contribute in pre-
venting such search errors.
We observe that some categories of phrase pairs
that are systematically pruned, but these cannot
be generalized in rules, since there are many ex-
ceptions. The most obvious type of phrase pairs
are phrases with punctuations, such as “i fi f.” to
“thanks .” and “. i f i f” to “thanks .”, since “.”
is translated independently from most contextual
words. However, this rule should not be general-
ized, since in some cases “.” is a relevant contextual
marker. For instance, the word “please” is translated
to “iq” in the sentence ‘open the door, please.” and
translated to “ItA in “please my advisors”. An-
other example are sequences of numbers, which are
generally translated literally. For instance, “A(8)
7:(3) A(8)” is translated to “eight three eight” (Ex:
“room eight three eight”). Thus, phrase pairs for
number sequences can be removed, since those num-
bers can be translated one by one. However, for se-
quences such as “—(1) A(8)”, we need a phrase pair
to represent this specifically. This is because “—(1)”
can be translated to “one”, but also to “a”, “an”, “sin-
gle”. Other exceptions include “—(1) —(1)”, which
tends to be translated as “eleven”, and which tends to
be translated to “o”, rather than “zero” in sequences
(“room eleven o five”).
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999958294117647">
We present a pruning algorithm for Machine Trans-
lation based on relative entropy, where we assess
whether the translation event encoded in a phrase
pair can be decomposed into combinations of events
encoded in other phrase pairs. We show that such
phrase pairs can be removed from the translation
model with little negative impact or even a positive
one in the overall translation quality. Tests show that
our method yields comparable or better results with
state of the art pruning algorithms.
As future work, we would like to combine our
approach with significance pruning, since both ap-
proaches are orthogonal and address different issues.
We also plan to improve the pruning step of our algo-
rithm to find the optimal set of phrase pairs to prune
given the pruning threshold.
The code used in this work will be made available.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999870875">
This work was partially supported by FCT (INESC-
ID multiannual funding) through the PIDDAC Pro-
gram funds, and also through projects CMU-
PT/HuMach/0039/2008 and CMU-PT/0005/2007.
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors also wish
to thank the anonymous reviewers for many helpful
comments.
</bodyText>
<page confidence="0.99531">
970
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999947851851852">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19:263–311, June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’06, pages 53–61, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
J Howard Johnson and Joel Martin. 2007. Improv-
ing translation quality by discarding most of the
phrasetable. In In Proceedings of EMNLP-CoNLL’07,
pages 967–975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 48–54, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-burch, Richard Zens, Rwth Aachen, Alexan-
dra Constantin, Marcello Federico, Nicola Bertoldi,
Chris Dyer, Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.
Wang Ling, Tiago Lu´ıs, Jo˜ao Grac¸a, Lu´ısa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT ’10:
International Workshop on Spoken Language Transla-
tion, pages 313–320, Paris, France.
Stephen Vogal Matthias Eck and Alex Waibel. 2007. Es-
timating phrase pair relevance for translation model
pruning. MTSummit XI.
Robert C. Moore and Chris Quirk. 2009. Less is more:
significance-based n-gram selection for smaller, bet-
ter language models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ’09,
pages 746–755, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ’03, pages 160–
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Michael Paul, Marcello Federico, and Sebastian St¨uker.
2010. Overview of the iwslt 2010 evaluation cam-
paign. In IWSLT ’10: International Workshop on Spo-
ken Language Translation, pages 3–27.
Lane Schwartz. 2008. Multi-source translation methods.
In Proceedings of AMTA, pages 279–288.
Kristie Seymore and Ronald Rosenfeld. 1996. Scalable
backoff language models. In In Proceedings of ICSLP,
pages 232–235.
Andreas Stolcke. 1998. Entropy-based pruning of back-
off language models. In In Proc. DARPA Broad-
cast News Transcription and Understanding Work-
shop, pages 270–274.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for sta-
tistical machine translation. MTSummit XII, Aug.
Jo˜ao V. Grac¸a, Kuzman Ganchev, and Ben Taskar. 2010.
Learning Tractable Word Alignment Models with
Complex Constraints. Comput. Linguist., 36:481–504.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-
based word alignment in statistical translation. In
Proceedings of the 16th conference on Computational
linguistics-Volume 2, pages 836–841. Association for
Computational Linguistics.
</reference>
<page confidence="0.998385">
971
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.603094">
<title confidence="0.986999">Entropy-based Pruning for Phrase-based Machine Translation</title>
<author confidence="0.925119">Isabel Trancoso Ling</author>
<author confidence="0.925119">Alan</author>
<affiliation confidence="0.8291965">Spoken Systems Lab, INESC-ID, Lisboa, Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA,</affiliation>
<email confidence="0.999321">awb@cs.cmu.edu</email>
<abstract confidence="0.99910408">Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation. However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy. In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<contexts>
<context position="1648" citStr="Brown et al., 1993" startWordPosition="246" endWordPosition="249">bilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding. 1 Introduction Phrase-based Machine Translation Models (Koehn et al., 2003) model n-to-m translations of n source words to m target words, which are encoded in phrase pairs and stored in the translation model. This approach has an advantage over Word-based Translation Models (Brown et al., 1993), since translating multiple source words allows the context for each source word to be considered during translation. For instance, the translation of the English word “in” by itself to Portuguese is not obvious, since we do not have any context for the word. This word can be translated in the context of “in (the box)” to “dentro”, or in the context of “in (China)” as “na”. In fact, the lexical entry for “in” has more than 10 good translations in Portuguese. Consequently, the lexical translation entry for Word-based models splits the probabilistic mass between different translations, leaving </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist., 19:263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>53--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18369" citStr="Foster et al., 2006" startWordPosition="3113" endWordPosition="3116">unpruned model, the possible derivations are either using phrase p(s, t) or one element of its support set S1, S2 or S3. On the other hand, on the pruned model where p(s, t) does not exist, only S1, S2 and S3 can be used. Thus, given a s, t pair one of three situations may occur. First, if the probability of the phrase pair p(s, t) is lower than the highest probability element in SP(p(s, t)), then both the models will choose that element, in which Pp(t|s) P (t|s) = 1. This can happen, if we define features that penalize longer phrase pairs, such as lexical weighting, or if we apply smoothing (Foster et al., 2006). Secondly, if the probability of p(s, t) is equal to the most likely element in SP(p(s, t)), regardless of whether the unpruned model choses to use p(s, t) or that element, the probability emissions of the pruned and unpruned model will be identical. Thus, for this case Pp(t|s) P (t|s) = 1. Finally, if the probability of p(s, t) is higher than other possible derivations, the unpruned model will choose to emit the probability of p(s, t), while the pruned model will emit the most likely element in SP(p(s, t)). Hence, the probability loss between the 2 models, will be the ratio between the proba</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable smoothing for statistical machine translation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 53–61, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Howard Johnson</author>
<author>Joel Martin</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable. In</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL’07,</booktitle>
<pages>967--975</pages>
<contexts>
<context position="5880" citStr="Johnson and Martin, 2007" startWordPosition="929" endWordPosition="932"> section 5. Finally, we conclude in section 6. 2 Phrase Table Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large negative impact in the translation quality. This is especially relevant in environments where memory constraints are imposed, such as translation systems for small devices like cellphones, and also when time constraints for the translation are defined, such as online Speech-to-Speech systems. 2.1 Significance Pruning A relevant reference in phrase table pruning is the work of (Johnson and Martin, 2007), where it is shown that a significant portion of the phrase table can be discarded without a considerable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their com</context>
<context position="26469" citStr="Johnson and Martin, 2007" startWordPosition="4476" endWordPosition="4479"> (Paul et al., 2010), with commonly used features in Moses (Ex: probability, lexical weighting, lexicalized reordering model). The optimization of the translation model weights was done using MERT tuning (Och, 2003) and the results were evaluated using BLEU-4. 5.3 Pruning Setup Our pruning algorithm is applied after the translation model weight optimization with MERT. We gener2http://code.google.com/p/geppetto/ ate multiple translation models by setting different values for 6, so that translation models of different sizes are generated at intervals of 5%. We also run the significance pruning (Johnson and Martin, 2007) algorithm in these conditions. While the IWSLT translation model has only 88,424 phrase pairs, for the EUROPARL experiment, the translation model was composed by 48,762,372 phrase pairs, which had to be decoded. The average time to decode each phrase pair using the full translation model is 4 seconds per sentence, since the table must be read from disk due to its size. This would make translating 48M phrase pairs unfeasible. To address this problem, we divide the phrase pairs in the translation model into blocks of K phrase pairs, that are processed separately. For each block, we resort to th</context>
</contexts>
<marker>Johnson, Martin, 2007</marker>
<rawString>J Howard Johnson and Joel Martin. 2007. Improving translation quality by discarding most of the phrasetable. In In Proceedings of EMNLP-CoNLL’07, pages 967–975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1427" citStr="Koehn et al., 2003" startWordPosition="210" endWordPosition="213"> accuracy. In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding. 1 Introduction Phrase-based Machine Translation Models (Koehn et al., 2003) model n-to-m translations of n source words to m target words, which are encoded in phrase pairs and stored in the translation model. This approach has an advantage over Word-based Translation Models (Brown et al., 1993), since translating multiple source words allows the context for each source word to be considered during translation. For instance, the translation of the English word “in” by itself to Portuguese is not obvious, since we do not have any context for the word. This word can be translated in the context of “in (the box)” to “dentro”, or in the context of “in (China)” as “na”. I</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-burch</author>
<author>Richard Zens</author>
<author>Rwth Aachen</author>
<author>Alexandra Constantin</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Chris Dyer</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Ondrej Bojar</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17524" citStr="Koehn et al., 2007" startWordPosition="2958" endWordPosition="2961">t sentence. Using this distribution, the model is more biased in pruning phrase pairs with s, t pairs that do not occur frequently. 3.4 Computing Pp(t|s) P (t|s) The computation of Pp(t|s) P (t|s) depends on how the decoder adapts when a phrase pair is pruned from the model. In the case of back-off language models, this can be solved by calculating the difference of the logs between the n-gram estimate and the backoff estimate. However, a translation decoder generally functions differently. In our work, we will assume that the decoding will be performed using a Viterbi decoder, such as MOSES (Koehn et al., 2007), where the translation with the highest score is chosen. In the example above, where s=”John in Portugal” and t=”John em Portugal”, the decoder would choose the derivation with the highest probability from s to t. Using the unpruned model, the possible derivations are either using phrase p(s, t) or one element of its support set S1, S2 or S3. On the other hand, on the pruned model where p(s, t) does not exist, only S1, S2 and S3 can be used. Thus, given a s, t pair one of three situations may occur. First, if the probability of the phrase pair p(s, t) is lower than the highest probability ele</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-burch, Zens, Aachen, Constantin, Federico, Bertoldi, Dyer, Cowan, Shen, Moran, Bojar, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard Zens, Rwth Aachen, Alexandra Constantin, Marcello Federico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen, Christine Moran, and Ondrej Bojar. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand. AAMT, AAMT.</location>
<contexts>
<context position="24930" citStr="Koehn, 2005" startWordPosition="4244" endWordPosition="4245">l 967 would eliminate p3 and keep p1, yet the best decision could be to keep p3 and remove p1, if p3 is also frequently used in derivations of other phrase pairs. Thus, we leave the problem of finding the best set of phrases to prune as future work. 5 Experiments We tested the performance of our system under two different environments. The first is the small scale DIALOG translation task for IWSLT 2010 evaluation (Paul et al., 2010) using a small corpora for the Chinese-English language pair (henceforth referred to as “IWSLT”). The second one is a large scale test using the complete EUROPARL (Koehn, 2005) corpora for the Portuguese-English language pair, which we will denote by “EUROPARL”. 5.1 Corpus The IWSLT model was trained with 30K training sentences. The development corpus and test corpus were taken from the evaluation dataset in IWSLT 2006 (489 tuning and 500 test sentences with 7 references). The EUROPARL model was trained using the EUROPARL corpora with approximately 1.3M sentence pairs, leaving out 1K sentences for tuning and another 1K sentences for tests. 5.2 Setup In the IWSLT experiment, word alignments were generated using an HMM model (Vogel et al., 1996), with symmetric poster</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
</authors>
<title>Tiago Lu´ıs, Jo˜ao Grac¸a, Lu´ısa Coheur, and Isabel Trancoso.</title>
<date>2010</date>
<booktitle>In IWSLT ’10: International Workshop on Spoken Language Translation,</booktitle>
<pages>313--320</pages>
<location>Paris, France.</location>
<marker>Ling, 2010</marker>
<rawString>Wang Ling, Tiago Lu´ıs, Jo˜ao Grac¸a, Lu´ısa Coheur, and Isabel Trancoso. 2010. Towards a general and extensible phrase-extraction algorithm. In IWSLT ’10: International Workshop on Spoken Language Translation, pages 313–320, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Vogal Matthias Eck</author>
<author>Alex Waibel</author>
</authors>
<title>Estimating phrase pair relevance for translation model pruning. MTSummit XI.</title>
<date>2007</date>
<contexts>
<context position="6920" citStr="Eck and Waibel, 2007" startWordPosition="1099" endWordPosition="1102"> p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their complexity. Significance-based pruning has also been successfully applied in language modeling in (Moore and Quirk, 2009). Our work has a similar objective, but instead of trying to predict the independence between the source and target phrases in each phrase pair, we attempt to predict the independence between a phrase pair and other phrase pairs in the model. 2.2 Relevance Pruning Another proposed approach (Matthias Eck and Waibel, 2007) consists at collecting usage statistics for phrase pairs. This algorithm decodes the training corpora and extracts the number of times each phrase pair is used in the 1-best translation hypothesis. Thus, phrase pairs that are rarely used during decoding are excluded first during pruning. This method considers the relationship between phrase pairs in the model, since it tests whether the decoder is more prone to use some phrase pairs than others. However, it leads to some undesirable pruning choices. Let us consider a source phrase “the box in China” and 2 translation hypotheses, where the fir</context>
</contexts>
<marker>Eck, Waibel, 2007</marker>
<rawString>Stephen Vogal Matthias Eck and Alex Waibel. 2007. Estimating phrase pair relevance for translation model pruning. MTSummit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>Chris Quirk</author>
</authors>
<title>Less is more: significance-based n-gram selection for smaller, better language models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>746--755</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6598" citStr="Moore and Quirk, 2009" startWordPosition="1046" endWordPosition="1049">nsiderable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their complexity. Significance-based pruning has also been successfully applied in language modeling in (Moore and Quirk, 2009). Our work has a similar objective, but instead of trying to predict the independence between the source and target phrases in each phrase pair, we attempt to predict the independence between a phrase pair and other phrase pairs in the model. 2.2 Relevance Pruning Another proposed approach (Matthias Eck and Waibel, 2007) consists at collecting usage statistics for phrase pairs. This algorithm decodes the training corpora and extracts the number of times each phrase pair is used in the 1-best translation hypothesis. Thus, phrase pairs that are rarely used during decoding are excluded first duri</context>
</contexts>
<marker>Moore, Quirk, 2009</marker>
<rawString>Robert C. Moore and Chris Quirk. 2009. Less is more: significance-based n-gram selection for smaller, better language models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 746–755, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26059" citStr="Och, 2003" startWordPosition="4419" endWordPosition="4420">nts were generated using an HMM model (Vogel et al., 1996), with symmetric posterior constraints (V. Grac¸a et al., 2010), using the Geppetto toolkit2. This setup was used in the official evaluation in (Ling et al., 2010). For the EUROPARL experiment the word alignments were generated using IBM model 4. In both experiments, the translation model was built using the phrase extraction algorithm (Paul et al., 2010), with commonly used features in Moses (Ex: probability, lexical weighting, lexicalized reordering model). The optimization of the translation model weights was done using MERT tuning (Och, 2003) and the results were evaluated using BLEU-4. 5.3 Pruning Setup Our pruning algorithm is applied after the translation model weight optimization with MERT. We gener2http://code.google.com/p/geppetto/ ate multiple translation models by setting different values for 6, so that translation models of different sizes are generated at intervals of 5%. We also run the significance pruning (Johnson and Martin, 2007) algorithm in these conditions. While the IWSLT translation model has only 88,424 phrase pairs, for the EUROPARL experiment, the translation model was composed by 48,762,372 phrase pairs, wh</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160– 167, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Marcello Federico</author>
<author>Sebastian St¨uker</author>
</authors>
<title>Overview of the iwslt 2010 evaluation campaign.</title>
<date>2010</date>
<booktitle>In IWSLT ’10: International Workshop on Spoken Language Translation,</booktitle>
<pages>3--27</pages>
<marker>Paul, Federico, St¨uker, 2010</marker>
<rawString>Michael Paul, Marcello Federico, and Sebastian St¨uker. 2010. Overview of the iwslt 2010 evaluation campaign. In IWSLT ’10: International Workshop on Spoken Language Translation, pages 3–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
</authors>
<title>Multi-source translation methods.</title>
<date>2008</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>279--288</pages>
<contexts>
<context position="21859" citStr="Schwartz, 2008" startWordPosition="3702" endWordPosition="3703">re not searching in the space of possible translations, but in the space of possible derivations, which are sequences of phrase translations p1(s1, t1), ..., pn(sn, tn) that can be applied to s to generate an output t with the score given by P(t) Hni=1 P(si, ti). Our algorithm to determine SP(p(s, t)) can be described as an adaptation to the decoding algorithm in Moses, where we restrict the search space to the subspace SP(p(s, t)), that is, our search space is only composed by derivations that output t, without using p itself. This can be done using the forced decoding algorithm proposed in (Schwartz, 2008). Secondly, the score of a given translation hypothesis does not depend on the language model probability P(t), since all derivations in this search space have the same t, thus we discard this probability from the score function. Finally, rather than using beam search, we exhaustively search all the search space, to reduce the hypothesis of incurring a search error at this stage. This is possible, since phrase pairs are generally smaller than text (less than 8 words), and because we are constraining the search space to t, which is an order of magnitude smaller than the regular search space wit</context>
</contexts>
<marker>Schwartz, 2008</marker>
<rawString>Lane Schwartz. 2008. Multi-source translation methods. In Proceedings of AMTA, pages 279–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Scalable backoff language models. In</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>232--235</pages>
<contexts>
<context position="4113" citStr="Seymore and Rosenfeld, 1996" startWordPosition="647" endWordPosition="650"> 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 962–971, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics binations of other phrases with the same probabilities, namely p(hid the key, escondeu a chave), p(in, dentro) and p(in, na). This is a problem that is also found in language modeling, where large amounts of redundant higher-order n-grams can make the model needlessly large. For backoff language models, multiple pruning strategies based on relative entropy have been proposed (Seymore and Rosenfeld, 1996) (Stolcke, 1998), where the objective is to prune n-grams in a way to minimize the relative entropy between the model before and after pruning. While the concept of using relative entropy for pruning is not new and frequently used in backoff language models, there are no such models for machine translation. Thus, the main contribution of our work is to propose a relative entropy pruning model for translation models used in Phrase-based Machine Translation. It is shown that our pruning algorithm can eliminate phrase pairs with little or no impact in the predictions made in our translation model</context>
</contexts>
<marker>Seymore, Rosenfeld, 1996</marker>
<rawString>Kristie Seymore and Ronald Rosenfeld. 1996. Scalable backoff language models. In In Proceedings of ICSLP, pages 232–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models. In</title>
<date>1998</date>
<booktitle>In Proc. DARPA Broadcast News Transcription and Understanding Workshop,</booktitle>
<pages>270--274</pages>
<contexts>
<context position="4129" citStr="Stolcke, 1998" startWordPosition="651" endWordPosition="652">rical Methods in Natural Language Processing and Computational Natural Language Learning, pages 962–971, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics binations of other phrases with the same probabilities, namely p(hid the key, escondeu a chave), p(in, dentro) and p(in, na). This is a problem that is also found in language modeling, where large amounts of redundant higher-order n-grams can make the model needlessly large. For backoff language models, multiple pruning strategies based on relative entropy have been proposed (Seymore and Rosenfeld, 1996) (Stolcke, 1998), where the objective is to prune n-grams in a way to minimize the relative entropy between the model before and after pruning. While the concept of using relative entropy for pruning is not new and frequently used in backoff language models, there are no such models for machine translation. Thus, the main contribution of our work is to propose a relative entropy pruning model for translation models used in Phrase-based Machine Translation. It is shown that our pruning algorithm can eliminate phrase pairs with little or no impact in the predictions made in our translation model. In fact, by re</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>Andreas Stolcke. 1998. Entropy-based pruning of backoff language models. In In Proc. DARPA Broadcast News Transcription and Understanding Workshop, pages 270–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadi Tomeh</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
</authors>
<title>Complexity-based phrase-table filtering for statistical machine translation. MTSummit XII,</title>
<date>2009</date>
<contexts>
<context position="6414" citStr="Tomeh et al., 2009" startWordPosition="1020" endWordPosition="1023"> relevant reference in phrase table pruning is the work of (Johnson and Martin, 2007), where it is shown that a significant portion of the phrase table can be discarded without a considerable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their complexity. Significance-based pruning has also been successfully applied in language modeling in (Moore and Quirk, 2009). Our work has a similar objective, but instead of trying to predict the independence between the source and target phrases in each phrase pair, we attempt to predict the independence between a phrase pair and other phrase pairs in the model. 2.2 Relevance Pruning Another proposed approach (Matthias Eck and Waibel, 2007) consists at collecting usage statistics for phrase pairs. This algorithm decodes the training</context>
</contexts>
<marker>Tomeh, Cancedda, Dymetman, 2009</marker>
<rawString>Nadi Tomeh, Nicola Cancedda, and Marc Dymetman. 2009. Complexity-based phrase-table filtering for statistical machine translation. MTSummit XII, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo˜ao V Grac¸a</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
</authors>
<title>Learning Tractable Word Alignment Models with Complex Constraints.</title>
<date>2010</date>
<journal>Comput. Linguist.,</journal>
<pages>36--481</pages>
<marker>Grac¸a, Ganchev, Taskar, 2010</marker>
<rawString>Jo˜ao V. Grac¸a, Kuzman Ganchev, and Ben Taskar. 2010. Learning Tractable Word Alignment Models with Complex Constraints. Comput. Linguist., 36:481–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>Hmmbased word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics-Volume 2,</booktitle>
<pages>836--841</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25507" citStr="Vogel et al., 1996" startWordPosition="4334" endWordPosition="4337"> using the complete EUROPARL (Koehn, 2005) corpora for the Portuguese-English language pair, which we will denote by “EUROPARL”. 5.1 Corpus The IWSLT model was trained with 30K training sentences. The development corpus and test corpus were taken from the evaluation dataset in IWSLT 2006 (489 tuning and 500 test sentences with 7 references). The EUROPARL model was trained using the EUROPARL corpora with approximately 1.3M sentence pairs, leaving out 1K sentences for tuning and another 1K sentences for tests. 5.2 Setup In the IWSLT experiment, word alignments were generated using an HMM model (Vogel et al., 1996), with symmetric posterior constraints (V. Grac¸a et al., 2010), using the Geppetto toolkit2. This setup was used in the official evaluation in (Ling et al., 2010). For the EUROPARL experiment the word alignments were generated using IBM model 4. In both experiments, the translation model was built using the phrase extraction algorithm (Paul et al., 2010), with commonly used features in Moses (Ex: probability, lexical weighting, lexicalized reordering model). The optimization of the translation model weights was done using MERT tuning (Och, 2003) and the results were evaluated using BLEU-4. 5.</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. Hmmbased word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics-Volume 2, pages 836–841. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
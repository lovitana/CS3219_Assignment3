<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.996366">
Multi-instance Multi-label Learning for Relation Extraction
</title>
<author confidence="0.956369">
Mihai Surdeanu†, Julie Tibshirani†, Ramesh Nallapati*, Christopher D. Manning†† Stanford University, Stanford, CA 94305
</author>
<email confidence="0.919559">
{mihais,jtibs,manning}@stanford.edu
</email>
<affiliation confidence="0.402869">
* Artificial Intelligence Center, SRI International
</affiliation>
<email confidence="0.990191">
nallapat@ai.sri.com
</email>
<sectionHeader confidence="0.990399" genericHeader="abstract">
Abstract
</sectionHeader>
<equation confidence="0.685015333333333">
C
BornIn(Barack Obama, United States) 1
DB = EmployedBy(Barack Obama, United States))
</equation>
<bodyText confidence="0.99954345">
Distant supervision for relation extraction
(RE) – gathering training data by aligning a
database of facts with text – is an efficient ap-
proach to scale RE to thousands of different
relations. However, this introduces a challeng-
ing learning scenario where the relation ex-
pressed by a pair of entities found in a sen-
tence is unknown. For example, a sentence
containing Balzac and France may express
BornIn or Died, an unknown relation, or no re-
lation at all. Because of this, traditional super-
vised learning, which assumes that each ex-
ample is explicitly mapped to a label, is not
appropriate. We propose a novel approach
to multi-instance multi-label learning for RE,
which jointly models all the instances of a pair
of entities in text and all their labels using
a graphical model with latent variables. Our
model performs competitively on two difficult
domains.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.955591">
Information extraction (IE), defined as the task of
extracting structured information (e.g., events, bi-
nary relations, etc.) from free text, has received re-
newed interest in the “big data” era, when petabytes
of natural-language text containing thousands of dif-
ferent structure types are readily available. How-
ever, traditional supervised methods are unlikely to
scale in this context, as training data is either lim-
ited or nonexistent for most of these structures. One
of the most promising approaches to IE that ad-
dresses this limitation is distant supervision, which
generates training data automatically by aligning a
Sentence Latent Label
Barack Obama is the 44th and current President EmployedBy
of the United States.
Obama was born in the United States just as he BornIn
has always said.
United States President Barack Obama meets EmployedBy
with Chinese Vice President Xi Jinping today.
Obama ran for the United States Senate in 2004. –
</bodyText>
<figureCaption confidence="0.953489">
Figure 1: Training sentences generated through distant
supervision for a database containing two facts.
</figureCaption>
<bodyText confidence="0.999428347826087">
database of facts with text (Craven and Kumlien,
1999; Bunescu and Mooney, 2007).
In this paper we focus on distant supervision for
relation extraction (RE), a subproblem of IE that ad-
dresses the extraction of labeled relations between
two named entities. Figure 1 shows a simple exam-
ple for a RE domain with two labels. Distant super-
vision introduces two modeling challenges, which
we highlight in the table. The first challenge is
that some training examples obtained through this
heuristic are not valid, e.g., the last sentence in Fig-
ure 1 is not a correct example for any of the known
labels for the tuple. The percentage of such false
positives can be quite high. For example, Riedel
et al. (2010) report up to 31% of false positives in
a corpus that matches Freebase relations with New
York Times articles. The second challenge is that
the same pair of entities may have multiple labels
and it is unclear which label is instantiated by any
textual mention of the given tuple. For example, in
Figure 1, the tuple (Barack Obama, United States)
has two valid labels: BornIn and EmployedBy, each
(latently) instantiated in different sentences. In the
</bodyText>
<page confidence="0.987035">
455
</page>
<note confidence="0.9912555">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 455–465, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9765112">
Figure 2: Overview of multi-instance multi-label learn-
ing. To contrast, in traditional supervised learning there
is one instance and one label per object. For relation ex-
traction the object is a tuple of two named entities. Each
mention of this tuple in text generates a different instance.
</figureCaption>
<bodyText confidence="0.999302888888889">
Riedel corpus, 7.5% of the entity tuples in the train-
ing partition have more than one label.
We summarize this multi-instance multi-label
(MIML) learning problem in Figure 2. In this pa-
per we propose a novel graphical model, which we
called MIML-RE, that targets MIML learning for re-
lation extraction. Our work makes the following
contributions:
(a) To our knowledge, MIML-RE is the first RE ap-
proach that jointly models both multiple instances
(by modeling the latent labels assigned to instances)
and multiple labels (by providing a simple method to
capture dependencies between labels). For example,
our model learns that certain labels tend to be gener-
ated jointly while others cannot be jointly assigned
to the same tuple.
(b) We show that MIML-RE performs competitively
on two difficult domains.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999932153846154">
Distant supervision for IE was introduced by Craven
and Kumlien (1999), who focused on the ex-
traction of binary relations between proteins and
cells/tissues/diseases/drugs using the Yeast Protein
Database as a source of distant supervision. Since
then, the approach grew in popularity (Bunescu and
Mooney, 2007; Bellare and McCallum, 2007; Wu
and Weld, 2007; Mintz et al., 2009; Riedel et al.,
2010; Hoffmann et al., 2011; Nguyen and Moschitti,
2011; Sun et al., 2011; Surdeanu et al., 2011a).
However, most of these approaches make one or
more approximations in learning. For example,
most proposals heuristically transform distant super-
vision to traditional supervised learning (i.e., single-
instance single-label) (Bellare and McCallum, 2007;
Wu and Weld, 2007; Mintz et al., 2009; Nguyen
and Moschitti, 2011; Sun et al., 2011; Surdeanu
et al., 2011a). Bunescu and Mooney (2007) and
Riedel et al. (2010) model distant supervision for
relation extraction as a multi-instance single-label
problem, which allows multiple mentions for the
same tuple but disallows more than one label per ob-
ject. Our work is closest to Hoffmann et al. (2011).
They address the same problem we do (binary rela-
tion extraction) with a MIML model, but they make
two approximations. First, they use a deterministic
model that aggregates latent instance labels into a
set of labels for the corresponding tuple by OR-ing
the classification results. We use instead an object-
level classifier that is trained jointly with the clas-
sifier that assigns latent labels to instances and can
capture dependencies between labels. Second, they
use a Perceptron-style additive parameter update ap-
proach, whereas we train in a Bayesian framework.
We show in Section 5 that these approximations gen-
erally have a negative impact on performance.
MIML learning has been used in fields other than
natural language processing. For example, Zhou
and Zhang (2007) use MIML for scene classifica-
tion. In this problem, each image may be assigned
multiple labels corresponding to the different scenes
captured. Furthermore, each image contains a set of
patches, which forms the bag of instances assigned
to the given object (image). Zhou and Zhang pro-
pose two algorithms that reduce the MIML problem
to a more traditional supervised learning task. In
one algorithm, for example, they convert the task to
a multi-instance single-label problem by creating a
separate bag for each label. Due to this, the pro-
posed approach cannot model inter-label dependen-
cies. Moreover, the authors make a series of approx-
imations, e.g., they assume that each instance in a
bag shares the bag’s overall label. We instead model
all these issues explicitly in our approach.
In general, our approach belongs to the category
of models that learn in the presence of incomplete or
incorrect labels. There has been interest among ma-
chine learning researchers in the general problem of
noisy data, especially in the area of instance-based
learning. Brodley and Friedl (1999) summarize
past approaches and present a simple, all-purpose
method to filter out incorrect data before training.
While potentially applicable to our problem, this ap-
proach is completely general and cannot incorporate
our domain-specific knowledge about how the noisy
</bodyText>
<figure confidence="0.99693475">
instance
instance
instance
...
object
label
label
...
</figure>
<page confidence="0.997409">
456
</page>
<bodyText confidence="0.931256">
data is generated.
</bodyText>
<sectionHeader confidence="0.958145" genericHeader="method">
3 Distant Supervision for Relation Extraction
</sectionHeader>
<bodyText confidence="0.999940935483871">
Here we focus on distant supervision for the ex-
traction of relations between two entities. We de-
fine a relation as the construct r(e1, e2), where r is
the relation name, e.g., BornIn in Figure 1, and e1
and e2 are two entity names, e.g., Barack Obama
and United States. Note that there are entity tu-
ples (e1, e2) that participate in multiple relations,
r1, ... , ri. In other words, the tuple (e1, e2) is the
object illustrated in Figure 2 and the different rela-
tion names are the labels. We define an entity men-
tion as a sequence of text tokens that matches the
corresponding entity name in some text, and relation
mention (for a given relation r(e1, e2)) as a pair of
entity mentions of e1 and e2 in the same sentence.
Relation mentions thus correspond to the instances
in Figure 2.1 As the latter definition indicates, we
focus on the extraction of relations expressed in a
single sentence. Furthermore, we assume that entity
mentions are extracted by a different process, such
as a named entity recognizer.
We define the task of relation extraction as a func-
tion that takes as input a document collection (C), a
set of entity mentions extracted from C (£), a set of
known relation labels (L) and an extraction model,
and outputs a set of relations (R) such that any of the
relations extracted is supported by at least one sen-
tence in C. To train the extraction model, we use a
database of relations (D) that are instantiated at least
once in C. Using distant supervision, D is aligned
with sentences in C, producing relation mentions for
all relations in D.
</bodyText>
<sectionHeader confidence="0.993807" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.9997155">
Our model assumes that each relation mention in-
volving an entity pair has exactly one label, but al-
lows the pair to exhibit multiple labels across differ-
ent mentions. Since we do not know the actual re-
lation label of a mention in the distantly supervised
setting, we model it using a latent variable z that
can take one of the k pre-specified relation labels
as well as an additional NIL label, if no relation is
expressed by the corresponding mention. We model
the multiple relation labels an entity pair can assume
</bodyText>
<footnote confidence="0.9991535">
1For this reason, we use relation mention and relation in-
stance interchangeably in this paper.
</footnote>
<figureCaption confidence="0.9947206">
Figure 3: MIML model plate diagram. We unrolled the
y plate to emphasize that it is a collection of binary clas-
sifiers (one per relation label), whereas the z classifier is
multi-class. Each z and yj classifier has an additional
prior parameter, which is omitted here for clarity.
</figureCaption>
<bodyText confidence="0.999307916666667">
using a multi-label classifier that takes as input the
latent relation types of the all the mentions involving
that pair. The two-layer hierarchical model is shown
graphically in Figure 3, and is described more for-
mally below. The model includes one multi-class
classifier (for z) and a set of binary classifiers (for
each yj). The z classifier assigns latent labels from
L to individual relation mentions or NIL if no rela-
tion is expressed by the mention. Each yj classifier
decides if relation j holds for the given entity tu-
ple, using the mention-level classifications as input.
Specifically, in the figure:
</bodyText>
<listItem confidence="0.989457090909091">
• n is the number of distinct entity tuples in D;
• Mi is the set of mentions for the ith entity pair;
• x is a sentence and z is the latent relation clas-
sification for that sentence;
• wz is the weight vector for the multi-class
mention-level classifier;
• k is the number of known relation labels in L;
• yj is the top-level classification decision for the
entity pair as to whether the jth relation holds;
• wj is the weight vector for the binary top-level
classifier for the jth relation.
</listItem>
<bodyText confidence="0.9997468">
Additionally, we define Pi (Ni) as the set of all
known positive (negative) relation labels for the ith
entity tuple. In this paper, we construct Ni as L \ Pi,
but, in general, other scenarios are possible. For
example, both Sun et al. (2011) and Surdeanu et
</bodyText>
<equation confidence="0.807720666666667">
. . . . . .
. . .
. . .
</equation>
<page confidence="0.963373">
457
</page>
<bodyText confidence="0.99668655">
al. (2011a) proposed models where Ni for the ith tu-
ple (e1, e2) is defined as: {rj  |rj(e1, ek) E A ek =�
e2, rj E/ Pit, which is a subset of L\Pi. That is, en-
tity e2 is considered a negative example for relation
rj (in the context of entity e1) only if rj exists in the
training data with a different value.
The addition of the object-level layer (for y) is an
important contribution of this work. This layer can
capture information that cannot be modeled by the
mention-level classifier. For example, it can learn
that two relation labels (e.g., BornIn and SpouseOf)
cannot be generated jointly for the same entity tu-
ple. So, if the z classifier outputs both these la-
bels for different mentions of the same tuple, the y
layer can cancel one of them. Furthermore, the y
classifiers can learn when two labels tend to appear
jointly, e.g., CapitalOf and Contained between two
locations, and use this occurrence as positive rein-
forcement for these labels. We discuss the features
that implement these ideas in Section 5.
</bodyText>
<subsectionHeader confidence="0.996727">
4.1 Training
</subsectionHeader>
<bodyText confidence="0.993540307692308">
We train the proposed model using hard discrimina-
tive Expectation Maximization (EM). In the Expec-
tation (E) step we assign latent mention labels us-
ing the current model (i.e., the mention and relation
level classifiers). In the Maximization (M) step we
retrain the model to maximize the log likelihood of
the data using the current latent assignments.
In the equations that follow, we refer to
w1, ... , wk collectively as wy for compactness.
The vector zi contains the latent mention-level clas-
sifications for the ith entity pair, while yi represents
the corresponding set of gold-standard labels (that
is, y(r)
</bodyText>
<equation confidence="0.754948833333333">
i = 1 if r E Pi, and y(r)
i = 0 for r E Ni.)
Using these notations, the log-likelihood of the data
is given by:
p(yi, zi|xi, wy, wz)
= p(zi|xi, wz)p(yi|zi, wy)
</equation>
<bodyText confidence="0.999473875">
where the last step follows from conditional inde-
pendence. Thus the log-likelihood for this problem
is not convex (it includes a sum of products). How-
ever, we can still use EM, but the optimization fo-
cuses on maximizing the lower bound of the log-
likelihood, i.e., we maximize the above joint proba-
bility for each entity pair in the database. Rewriting
this probability in log space, we obtain:
</bodyText>
<equation confidence="0.765467">
log p(yi, zi|xi, wy, wz) (1)
</equation>
<bodyText confidence="0.879308428571428">
The algorithm proceeds as follows.
E-step: In this step we infer the mention-level
classifications zi for each entity tuple, given all its
mentions, the gold labels yi, and current model, i.e.,
wz and wy weights. Formally, we seek to find:
zi* = arg max p(z|yi, xi, wy, wz)
z
However it is computationally intractable to con-
sider all vectors z as there is an exponential num-
ber of possible assignments, so we approximate and
consider each mention separately. Concretely,
p(m
zi |yi, xi, wy, wz)
oc p(yi, z(m)
</bodyText>
<equation confidence="0.99757488">
i |xi, wy, wz)
� p(z(m)
i |x(m)
i , wz)p(yi|z&gt;, wy)
= p(z(m)
i |x(m)
i , wz) Y p(y(r)
i |z� i,w(r)
y )
rEPiUNi
Y= Y p(y(r)
mEMi p(z(m) i |zi, w(r)
i |x(m) y )
i ,wz)
rEPiUNi
X= log p(z(m)
mEMi i |x(m) i ,wz)+
X log p(y(r)
rEPiUNi i |zi, w(r)
y )
LL(wy, wz) = Xn log p(yi|xi, wy, wz)
i=1
= Xn X p(yi, zi|xi, wy, wz)
i=1 log
z;
</equation>
<bodyText confidence="0.999941333333333">
where z� contains the previously inferred mention
labels for group i, with the exception of compo-
nent m whose label is replaced by z(m)
</bodyText>
<equation confidence="0.785553857142857">
i . So for
i = 1, ... , n, and for each m E Mi we calculate:
p(z|x(m)
i , wz)X (2)
zi = arg max
(m)*
z
</equation>
<bodyText confidence="0.998468333333333">
The joint probability in the inner summation can be Y p(y(r)
broken up into simpler parts: rEPiUNi i |z�i, w(r)
y )
</bodyText>
<page confidence="0.952376">
458
</page>
<bodyText confidence="0.999976666666667">
Intuitively, the above equation indicates that men-
tion labels are chosen to maximize: (a) the prob-
abilities assigned by the mention-level model; (b)
the probability that the correct relation labels are as-
signed to the corresponding tuple; and (c) the prob-
ability that the labels known to be incorrect are not
assigned to the tuple. For example, if a particular
mention label receives a high mention-level proba-
bility but it is known to be a negative label for that
tuple, it will receive a low overall score.
M-step: In this step we find wy, wz that maxi-
mize the lower bound of the log-likelihood, i.e., the
probability in equation (1), given the current assign-
ments for zi. From equation (1) it is clear that this
can be maximized separately with respect to wy and
wz. Intuitively, this step amounts to learning the
weights for the mention-level classifier (wz) and the
weights for each of the k top-level classifiers (wy).
The updates are given by:
Note that these are standard updates for logistic re-
gression. We obtained these weights using k + 1
logistic classifiers: one multi-class classifier for wz
and k binary classifiers for each relation label r E L.
We implemented all using the L2-regularized logis-
tic regression from the publicly-downloadable Stan-
ford CoreNLP package.2 The main difference be-
tween the classifiers is how features are generated:
the mention-level classifier computes its features
based on xi, whereas the relation-level classifiers
generate features based on the current assignments
for zi and the corresponding relation label r. We
discuss the actual features used in our experiments
in Section 5.
</bodyText>
<sectionHeader confidence="0.592228" genericHeader="method">
4.2 Inference
</sectionHeader>
<bodyText confidence="0.9999695">
Given an entity tuple, we obtain its relation labels as
follows. We first classify its mentions:
</bodyText>
<equation confidence="0.877302">
p(z|x(m)
i , wz) (5)
2nlp.stanford.edu/software/corenlp.shtml
</equation>
<bodyText confidence="0.919829">
then decide on the final relation labels using the top-
level classifiers:
</bodyText>
<equation confidence="0.98619575">
yi = arg max
(r)* p(y|z*i , w(r)
y ) (6)
yE10,11
</equation>
<subsectionHeader confidence="0.9914">
4.3 Implementation Details
</subsectionHeader>
<bodyText confidence="0.997551047619048">
We discuss next several details that are crucial for
the correct implementation of the above model.
Initialization: Since EM is not guaranteed to con-
verge at the global maximum of the observed data
likelihood, it is important to provide it with good
starting values. In our context, the initial values are
labels assigned to zi, which are required to compute
equation (2) in the first iteration (z�). We generate
these values using a local logistic regression classi-
fier that uses the same features as the mention-level
classifier in the joint model but treats each relation
mention independently. We train this classifier using
“traditional” distant supervision: for each relation in
the database D we assume that all the corresponding
mentions are positive examples for the correspond-
ing label (Mintz et al., 2009). Note that this heuris-
tic repeats relation mentions with different labels for
the tuples that participate in multiple relations. For
example, all the relation mentions in Figure 1 will
yield datums with both the EmployedBy and BornIn
labels. Despite this limitation, we found that this is
a better initialization heuristic than random assign-
ment.
For the second part of equation (2), we initial-
ize the relation-level classifier with a model that
replicates the at least one heuristic of Hoffmann et
al. (2011). Each w(r)
y model has a single feature with
a high positive weight that is triggered when label r
is assigned to any of the mentions in z*i .
Avoiding overfitting: A naive implementation of
our approach leads to an unrealistic training scenario
where the z classifier generates predictions (in equa-
tion (2)) for the same datums it has seen in training
in the previous iteration. To avoid this overfitting
problem we used cross validation: we divided the
training tuples in K distinct folds and trained K dif-
ferent mention-level classifiers. Each classifier out-
puts p(z|x(m)
i , wz) for tuples in a given fold during
the E-step (equation (2)) and is trained (equation (3))
using tuples from all other folds.
</bodyText>
<equation confidence="0.976464">
1&lt;i&lt;n s.t. rEPiUNi
n
w*z = arg max
w i=1
� log p(z(m)* i|xi (m), w) (3)
mEMi
wy = arg max
(r)*
w
log p(y(r) i|z*i , w) (4)
zi = arg max
(m)*
z
</equation>
<page confidence="0.978849">
459
</page>
<bodyText confidence="0.9880385">
At testing time, we compute p(z�x(m)
i , wz) in
equation (5) as the average of the probabilities of
the above set of mention classifiers:
</bodyText>
<equation confidence="0.935621333333333">
1 p
p(z�x(m) w ) — �Kj= (z�xZm),wjz)
i , z K
</equation>
<bodyText confidence="0.999960333333333">
where wjz are the weights of the mention classifier
responsible for fold j. We found that this simple
bagging model performs slightly better in practice
(a couple of tenths of a percent) than training a sin-
gle mention classifier on the latent mention labels
generated in the last training iteration.
Inference during training: During the inference
process in the E-step, the algorithm incrementally
“flips” mention labels based on equation (2), for
each group of mentions Mi. Thus, z� changes as the
algorithm progresses, which may impact the label
assigned to the remaining mentions in that group. To
avoid any potential bias introduced by the arbitrary
order of mentions as seen in the data, we randomize
each group Mi before we inspect its mentions.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="method">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.991212">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.99040255882353">
We evaluate our algorithm on two corpora. The first
was developed by Riedel et al. (2010) by aligning
Freebase3 relations with the New York Times (NYT)
corpus. They used the Stanford named entity recog-
nizer (Finkel et al., 2005) to find entity mentions in
text and constructed relation mentions only between
entity mentions in the same sentence.
Riedel et al. (2010) observes that evaluating on
this corpus underestimates true extraction accuracy
because Freebase is incomplete. Thus, some re-
lations extracted during testing will be incorrectly
marked as wrong, simply because Freebase has no
information on them. To mitigate this issue, Riedel
et al. (2010) and Hoffman et al. (2011) perform a
second evaluation where they compute the accuracy
of labels assigned to a set of relation mentions that
they manually annotated. To avoid any potential an-
notation biases, we instead evaluate on a second cor-
pus that has comprehensive annotations generated
by experts for all test relations.
We constructed this second dataset using mainly
resources distributed for the 2010 and 2011 KBP
3freebase.com
shared tasks (Ji et al., 2010; Ji et al., 2011). We gen-
erated training relations from the knowledge base
provided by the task organizers, which is a subset
of the English Wikipedia infoboxes from a 2008
snapshot. Similarly to the corpus of Riedel et al.,
these infoboxes contain open-domain relations be-
tween named entities, but with a different focus.
For example, more than half of the relations in
the evaluation data are alternate names of organi-
zations or persons (e.g., org:alternate names) or re-
lations associated with employment and member-
ship (e.g., per:employee of) (Ji et al., 2011). We
aligned these relations against a document collec-
tion that merges two distinct sources: (a) the col-
lection provided by the shared task, which contains
approximately 1.5 million documents from a vari-
ety of sources, including newswire, blogs and tele-
phone conversation transcripts; and (b) a complete
snapshot of the English Wikipedia from June 2010.
During training, for each entity tuple (e1, e2), we
retrieved up to 50 sentences that contain both en-
tity mentions.4 We used Stanford’s CoreNLP pack-
age to find entity mentions in text and, similarly to
Riedel et al. (2010), we construct relation mention
candidates only between entity mentions in the same
sentence. We analyzed a set of over 2,000 relation
mentions and we found that 39% of the mentions
where e1 is an organization name and 36% of men-
tions where e1 is a person name do not express the
corresponding relation.
At evaluation time, the KBP shared task requires
the extraction of all relations r(e1, e2) given a query
that contains only the first entity e1. To accommo-
date this setup, we adjusted our sentence extraction
component to use just e1 as the retrieval query and
we kept up to 50 sentences that contain a mention
of the input entity for each evaluation query. For
tuning and testing we used the 200 queries from the
2010 and 2011 evaluations. We randomly selected
40 queries for development and used the remaining
160 for the formal evaluation.
To address the large number of negative examples
in training, Riedel et al. subsampled them randomly
with a retention probability of 10%. For the KBP
corpus, we followed the same strategy, but we used
</bodyText>
<footnote confidence="0.995473333333333">
4Sentences were ranked using the similarity between their
parent document and the query that concatenates the two entity
names. We used the default Lucene similarity measure.
</footnote>
<page confidence="0.995783">
460
</page>
<table confidence="0.939343615384615">
# of gold # of gold % of gold entity tuples % of gold entity tuples
relations relations with more than one label with multiple mentions in text
in training in testing in training in training
Riedel 4,700 1,950 7.5% 46.4%
KBP 183,062 3,334 2.8% 65.1%
% of mentions that
# of relation labels
do not express
their relation
51
up to 31%
41
up to 39%
</table>
<tableCaption confidence="0.7715098">
Table 1: Statistics about the two corpora used in this paper. Some of the numbers for the Riedel dataset is from (Riedel
et al., 2010; Hoffmann et al., 2011).
a subsampling probability of 5% because this led to
the best results in development for all models.
Table 1 provides additional statistics about the
</tableCaption>
<bodyText confidence="0.937512">
two corpora. The table indicates that having mul-
tiple mentions for an entity tuple is a very common
phenomenon in both corpora, and that having mul-
tiple labels per tuple is more common in the Riedel
dataset than KBP (7.5% vs. 2.8%).
</bodyText>
<subsectionHeader confidence="0.944438">
5.2 Features
</subsectionHeader>
<bodyText confidence="0.999124463414634">
Our model requires two sets of features: one for the
mention classifier (z) and one for the relation clas-
sifier (y). In the Riedel dataset, we used the same
features as Riedel et al. (2010) and Hoffmann et
al. (2011) for the mention classifier. In the KBP
dataset, we used a feature set that was developed in
our previous work (Surdeanu et al., 2011b). These
features can be grouped in three classes: (a) features
that model the two entities, such as their head words;
(b) features that model the syntactic context of the
relation mention, such as the dependency path be-
tween the two entity mentions; and (c) features that
model the surface context, such as the sequence of
part of speech tags between the two entity mentions.
We used these features for all the models evaluated
on the KBP dataset.5
For the relation-level classifier, we developed two
feature groups. The first models Hoffmann et al.’s
at least one heuristic using a single feature, which
is set to true if at least one mention in zi has the la-
bel r, which is modeled by the current relation clas-
sifier. The second group models the dependencies
between relation labels. This is implemented by a
set of |G |− 1 features, where feature j is instan-
tiated whenever the label modeled (r) is predicted
jointly with another label rj (rj E G, rj 7� r) in zi.
These features learn both positive and negative re-
inforcements between labels. For example, if labels
5To avoid an excessive number of features in the KBP exper-
iments, we removed features seen less than five times in train-
ing.
r1 and r2 tend to be generated jointly, the feature for
the corresponding dependency will receive a posi-
tive weight in the models for r1 and r2. Similarly, if
r1 and r2 cannot be generated jointly, the model will
assign a negative weight to feature 2 in r1’s classi-
fier and to feature 1 in r2’s classifier. Note that this
feature is asymmetric, i.e., feature 1 in r2’s classi-
fier may have a different value than feature 2 in r1’s
classifier, depending on the accuracy of the individ-
ual predictions for r1 and r2.
</bodyText>
<subsectionHeader confidence="0.998484">
5.3 Baselines
</subsectionHeader>
<bodyText confidence="0.99947676">
We compare our approach against three models:
Mintz++ – This is the model used to initialize the
mention-level classifier in our model. As discussed
in Section 4.3, this model follows the “traditional”
distant supervision heuristic, similarly to (Mintz et
al., 2009). However, our implementation has several
advantages over the original model: (a) we model
each relation mention independently, whereas Mintz
et al. collapsed all the mentions of the same entity
tuple into a single datum; (b) we allow multi-label
outputs for a given entity tuple at prediction time
by OR-ing the predictions for the individual rela-
tion mentions corresponding to the tuple (similarly
to (Hoffmann et al., 2011))6; and (c) we use the
simple bagging strategy described in Section 4.3 to
combine multiple models. Empirically, we observed
that these changes yield a significant improvement
over the original proposal. For this reason, we con-
sider this model a strong baseline on its own.
Riedel – This is the “at-least-once” model reported
in (Riedel et al., 2010), which had the best perfor-
mance in that work. This approach models the task
as a multi-instance single-label problem. Note that
this is the only model shown here that does not allow
multi-label outputs for an entity tuple.
</bodyText>
<footnote confidence="0.99897525">
6We also allow multiple labels per tuple at training time,
in which case we replicate the corresponding datum for each
label. However, this did not improve performance significantly
compared to selecting a single label per datum during training.
</footnote>
<page confidence="0.999009">
461
</page>
<bodyText confidence="0.999909125">
Hoffmann – This is the “MultiR” model, which per-
formed the best in (Hoffmann et al., 2011). This
models RE as a MIML problem, but learns using
a Perceptron algorithm and uses a deterministic “at
least one” decision instead of a relation classifier.
We used Hoffman’s publicly released code7 for the
experiments on the Riedel dataset and our own im-
plementation for the KBP experiments.8
</bodyText>
<subsectionHeader confidence="0.815123">
5.4 Results
</subsectionHeader>
<bodyText confidence="0.999976965517241">
We tuned all models using three-fold cross valida-
tion for the Riedel dataset and using the develop-
ment queries for the KBP dataset. MIML-RE has
two parameters that require tuning: the number of
EM epochs (T) and the number of folds for the men-
tion classifiers (K).9 The values obtained after tun-
ing are T = 15, K = 5 for the Riedel dataset and
T = 8, K = 3 for KBP. Similarly, we tuned the
number of epochs for the Hoffmann model on the
KBP dataset, obtaining an optimal value of 20.
On the Riedel dataset we evaluate all models us-
ing standard precision and recall measures. For the
KBP evaluation we used the official KBP scorer,10
with two changes: (a) we score with the parame-
ter anydoc set to true, which configures the scorer
to accept relation mentions as correct regardless of
their supporting document; and (b) we score only
on the subset of gold relations that have at least one
mention in our sentences. The first decision is neces-
sary because the gold KBP answers contain support-
ing documents only from the corpus provided by the
organizers but we retrieve candidate answers from
multiple collections. The second is required because
the focus of this work is not on sentence retrieval but
on RE, which should be evaluated in isolation.11
Similarly to previous work, we report preci-
sion/recall curves in Figure 4. We evaluate two
variants of MIML-RE: one that includes all the
features for the y model, and another (MIML-RE
</bodyText>
<footnote confidence="0.9216651">
7cs.washington.edu/homes/raphaelh/mr/
8The decision to reimplement the Hoffmann model was a
practical one, driven by incompatibilities between their imple-
mentation and our KBP framework.
9We could also tune the prior parameters for both our model
and Mintz++, but we found in early experiments that the default
value of 1 yields the best scores for all priors.
10nlp.cs.qc.cuny.edu/kbp/2011/scoring.html
11Due to these changes, the scores reported in this paper are
not directly comparable with the shared task scores.
</footnote>
<bodyText confidence="0.978756833333334">
At-Least-One) which has only the at least one
feature. For all the Bayesian models implemented
here, we sorted the predicted relations by the noisy-
or score of the top predictions for their mentions.
Formally, we rank a relation r predicted for group i,
i.e., r E y;, using:
</bodyText>
<equation confidence="0.9350566">
11 noisyOri(r) = 1 −
mEM%
where s(m)
i (r) = p(rjx(m)
i , w.) if r = z(m)�
</equation>
<bodyText confidence="0.9998963125">
i or 0 oth-
erwise. The noisy-or formula performs well for
ranking because it integrates model confidence (the
higher the probabilities, the higher the score) and re-
dundancy (the more mentions are predicted with a
label, the higher that label’s score). Note that the
above ranking score does not include the probability
of the relation classifier (equation (6)) for MIML-RE.
While we use equation (6) to generate y;, we found
that the corresponding probabilities are too coarse
to provide a good ranking score. This is caused by
the fact that our relation-level classifier works with
a small number of (noisy) features. Lastly, for our
implementation of the Hoffmann et al. model, we
used their ranking heuristic (sorting predictions by
the maximum extraction score for that relation).
</bodyText>
<sectionHeader confidence="0.998253" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.997875809523809">
Figure 4 indicates that MIML-RE generally outper-
forms the current state of the art. In the Riedel
dataset, MIML-RE has higher overall recall than the
Riedel et al. model, and, for the same recall point,
MIML-RE’s precision is between 2 and 15 points
higher. For most of the curve, our model obtains
better precision for the same recall point than the
Hoffmann model, which currently has the best re-
ported results on this dataset. The difference is as
high as 5 precision points around the middle of the
curve. The Hoffmann model performs better close to
the extremities of the curve (low/high recall). Nev-
ertheless, we argue that our model is more stable
than Hoffmann’s: MIML-RE yields a smoother pre-
cision/recall curve, without most of the depressions
seen in the Hoffmann results. In the KBP dataset,
MIML-RE performs consistently better than our im-
plementation of Hoffmann’s model, with higher pre-
cision values for the same recall point, and much
higher overall recall. We believe that these dif-
ferences are caused by our Bayesian framework,
</bodyText>
<equation confidence="0.9091095">
(1 − s(m)
i (r))
</equation>
<page confidence="0.976766">
462
</page>
<figure confidence="0.999667296296296">
0 0.05 0.1 0.15 0.2 0.25 0.3
Recall
Hoffmann (our implementation)
Mintz++
MIML-RE
MIML-RE At-Least-One
0.6
0.3
Hoffmann
Riedel
Mintz++
MIML-RE
MIML-RE At-Least-One
Precision 0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.7
0.2
Precision 0.5
0.4
0 0.05 0.1 0.15 0.2 0.25 0.3
Recall
</figure>
<figureCaption confidence="0.99673">
Figure 4: Results in the Riedel dataset (top) and the KBP dataset (bottom). The Hoffmann scores in the KBP dataset
were generated using our implementation. The other Hoffmann and Riedel results were taken from their papers.
</figureCaption>
<bodyText confidence="0.999468166666667">
which provides a more formal implementation of the
MIML problem.
Figure 4 also indicates that MIML-RE yields a con-
sistent improvement over Mintz++ (with the excep-
tion of a few points in the low-recall portion of the
KBP curves). The difference in precision for the
same recall point is as high as 25 precision points in
the Riedel dataset and up to 5 points in KBP. Over-
all, the best F1 score of MIML-RE is slightly over 1
point higher than the best F1 score of Mintz++ in
the Riedel dataset and 3 points higher in KBP. Con-
sidering that Mintz++ is a strong baseline and we
evaluate on two challenging domains, we consider
these results proof that the correct modeling of the
MIML scenario is beneficial.
Lastly, Figure 4 shows that MIML-RE outper-
forms its variant without label-dependency fea-
tures (MIML-RE At-Least-One) in the higher-
recall part of the curve in the Riedel dataset. The im-
provement is approximately 1 F1 point throughout
the last segment of the curve. The overall increase
in F1 was found to be significant (p = 0.0296) in a
one-sided, paired t-test over randomly sampled test
data. We see a smaller improvement in KBP (con-
centrated around the middle of the curve), likely be-
cause the number of entity tuples with multiple la-
bels in training is small (see Table 1). Neverthe-
less, this exercise shows that, when dependencies
between labels exist in a dataset, modeling them,
which can be trivially done in MIML-RE, is useful.
</bodyText>
<page confidence="0.998846">
463
</page>
<table confidence="0.9998584">
P R F1
Hoffmann (our implementation) 48.6 29.8 37.0
Mintz++ 43.8 36.8 40.0
MIML-RE 64.8 31.6 42.6
MIML-RE At-Least-One 56.1 32.5 41.1
</table>
<tableCaption confidence="0.981955333333333">
Table 2: Results at the highest F1 point in the preci-
sion/recall curve on the dataset that contains groups with
at least 10 mentions.
</tableCaption>
<bodyText confidence="0.999868086956521">
In a similar vein, we tested the models previ-
ously described on a subset of the Riedel evalua-
tion dataset that only includes groups with at least
10 mentions. This corpus contains approximately
2% of the groups from the original testing partition,
out of which 90 tuples have at least one known label
and 1410 groups serve as negative examples.
For conciseness, we do not include the entire
precision/recall curves for this experiment, but sum-
marize them in Table 2, which lists the performance
peak (highest F1 score) for each of the models
investigated. The table shows that MIML-RE obtains
the highest F1 score overall, 1.5 points higher than
MIML-RE At-Least-One and 2.6 points higher
than Mintz++. More importantly, for approximately
the same recall point, MIML-RE obtains a precision
that is over 8 percentage points higher than that of
MIML-RE At-Least-One. A post-hoc inspection
of the results indicates that, indeed, MIML-RE suc-
cessfully eliminates undesired labels when two
(or more) incompatible labels are jointly assigned
to the same tuple. Take for example the tuple
(Mexico City, Mexico), for which the correct re-
lation is /location/administrative division/country.
MIML-RE At-Least-One incorrectly predicts
the additional /location/location/contains relation,
while MIML-RE does not make this prediction
because it recognizes that these two labels are in-
compatible in general: one location cannot both be
within another location and contain it. Indeed, ex-
amining the weights assigned to label-dependency
features in MIML-RE, we see that the model has
assigned a large negative weight to the depen-
dency feature between /location/location/contains
and /location/administrative division/country
for the /location/location/contains class. We
also observe positive dependencies between la-
bels. For example, MIML-RE learns that the
relations /people/person/place lived and /peo-
ple/person/place of birth tend to co-occur and
assigns a positive weight to this dependency feature
for the corresponding classes.
These results strongly suggest that when all as-
pects of the MIML scenario are present, our model
can successfully capture them and make use of the
additional structure to improve performance.
</bodyText>
<sectionHeader confidence="0.999351" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999734269230769">
In this paper we showed that distant supervision
for RE, which generates training data by aligning a
database of facts with text, poses a distinct multi-
instance multi-label learning scenario. In this set-
ting, each entity pair to be modeled typically has
multiple instances in the text and may have multiple
labels in the database. This is considerably differ-
ent from traditional supervised learning, where each
instance has a single, explicit label.
We argued that this MIML scenario should be
formally addressed. We proposed, to our knowl-
edge, the first approach that models all aspects of the
MIML setting, i.e., the latent assignment of labels to
instances and dependencies between labels assigned
to the same entity pair.
We evaluated our model on two challenging do-
mains and obtained state-of-the-art results on both.
Our model performs well even when not all aspects
of the MIML scenario are common, and as seen in
the discussion, shows significant improvement when
evaluated on entity pairs with many labels or men-
tions. When all aspects of the MIML scenario are
present, our model is well-equipped to handle them.
The code and data used in the experiments re-
ported in this paper are available at: http://nlp.
stanford.edu/software/mimlre.shtml.
</bodyText>
<sectionHeader confidence="0.999016" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9433402">
We gratefully acknowledge the support of Defense Ad-
vanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA, AFRL,
or the US government. We gratefully thank Raphael
Hoffmann and Sebastian Riedel for sharing their code
and data and for the many useful discussions.
</bodyText>
<page confidence="0.999425">
464
</page>
<sectionHeader confidence="0.998326" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702452054795">
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Proceedings of the Sixth International
Workshop on Information Extraction on the Web.
Carla Brodley and Mark Friedl. 1999. Identifying mis-
labeled training data. Journal of Artificial Intelligence
Research (JAIR).
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
Heng Ji, Ralph Grishman, Hoa T. Dang, Kira Griffitt, and
Joe Ellis. 2010. Overview of the TAC 2010 knowl-
edge base population track. In Proceedings of the Text
Analytics Conference.
Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011.
Overview of the TAC 2011 knowledge base popula-
tion track. In Proceedings of the Text Analytics Con-
ference.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ’10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP slot
filling. In Proceedings of the Text Analytics Confer-
ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011a. Stanford’s distantly-
supervised slot-filling system. In Proceedings of the
Text Analytics Conference.
Mihai Surdeanu, David McClosky, Mason R. Smith, An-
drey Gusev, and Christopher D. Manning. 2011b.
Customizing an information extraction system to a
new domain. In Proceedings of the Workshop on Re-
lational Models of Semantics, Portland, Oregon, June.
Fei Wu and Dan Weld. 2007. Autonomously semanti-
fying Wikipedia. In Proceedings of the International
Conference on Information and Knowledge Manage-
ment (CIKM).
Z.H. Zhou and M.L. Zhang. 2007. Multi-instance multi-
label learning with application to scene classification.
In Advances in Neural Information Processing Sys-
tems (NIPS).
</reference>
<page confidence="0.99959">
465
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828812">
<title confidence="0.999898">Multi-instance Multi-label Learning for Relation Extraction</title>
<author confidence="0.996631">Julie Ramesh Christopher D University</author>
<author confidence="0.996631">CA Stanford</author>
<affiliation confidence="0.902201">Intelligence Center, SRI</affiliation>
<email confidence="0.998395">nallapat@ai.sri.com</email>
<abstract confidence="0.996366136363637">C Distant supervision for relation extraction (RE) – gathering training data by aligning a database of facts with text – is an efficient approach to scale RE to thousands of different relations. However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown. For example, a sentence express an unknown relation, or no relation at all. Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate. We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables. Our model performs competitively on two difficult domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Andrew McCallum</author>
</authors>
<title>Learning extractors from unlabeled text using relevant databases.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixth International Workshop on Information Extraction on the Web.</booktitle>
<contexts>
<context position="5154" citStr="Bellare and McCallum, 2007" startWordPosition="803" endWordPosition="806">ing a simple method to capture dependencies between labels). For example, our model learns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for re</context>
</contexts>
<marker>Bellare, McCallum, 2007</marker>
<rawString>Kedar Bellare and Andrew McCallum. 2007. Learning extractors from unlabeled text using relevant databases. In Proceedings of the Sixth International Workshop on Information Extraction on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carla Brodley</author>
<author>Mark Friedl</author>
</authors>
<title>Identifying mislabeled training data.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research (JAIR).</journal>
<contexts>
<context position="7812" citStr="Brodley and Friedl (1999)" startWordPosition="1228" endWordPosition="1231">ngle-label problem by creating a separate bag for each label. Due to this, the proposed approach cannot model inter-label dependencies. Moreover, the authors make a series of approximations, e.g., they assume that each instance in a bag shares the bag’s overall label. We instead model all these issues explicitly in our approach. In general, our approach belongs to the category of models that learn in the presence of incomplete or incorrect labels. There has been interest among machine learning researchers in the general problem of noisy data, especially in the area of instance-based learning. Brodley and Friedl (1999) summarize past approaches and present a simple, all-purpose method to filter out incorrect data before training. While potentially applicable to our problem, this approach is completely general and cannot incorporate our domain-specific knowledge about how the noisy instance instance instance ... object label label ... 456 data is generated. 3 Distant Supervision for Relation Extraction Here we focus on distant supervision for the extraction of relations between two entities. We define a relation as the construct r(e1, e2), where r is the relation name, e.g., BornIn in Figure 1, and e1 and e2</context>
</contexts>
<marker>Brodley, Friedl, 1999</marker>
<rawString>Carla Brodley and Mark Friedl. 1999. Identifying mislabeled training data. Journal of Artificial Intelligence Research (JAIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2391" citStr="Bunescu and Mooney, 2007" startWordPosition="359" endWordPosition="362">o IE that addresses this limitation is distant supervision, which generates training data automatically by aligning a Sentence Latent Label Barack Obama is the 44th and current President EmployedBy of the United States. Obama was born in the United States just as he BornIn has always said. United States President Barack Obama meets EmployedBy with Chinese Vice President Xi Jinping today. Obama ran for the United States Senate in 2004. – Figure 1: Training sentences generated through distant supervision for a database containing two facts. database of facts with text (Craven and Kumlien, 1999; Bunescu and Mooney, 2007). In this paper we focus on distant supervision for relation extraction (RE), a subproblem of IE that addresses the extraction of labeled relations between two named entities. Figure 1 shows a simple example for a RE domain with two labels. Distant supervision introduces two modeling challenges, which we highlight in the table. The first challenge is that some training examples obtained through this heuristic are not valid, e.g., the last sentence in Figure 1 is not a correct example for any of the known labels for the tuple. The percentage of such false positives can be quite high. For exampl</context>
<context position="5126" citStr="Bunescu and Mooney, 2007" startWordPosition="799" endWordPosition="802">multiple labels (by providing a simple method to capture dependencies between labels). For example, our model learns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) mode</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology.</booktitle>
<contexts>
<context position="2364" citStr="Craven and Kumlien, 1999" startWordPosition="355" endWordPosition="358">ost promising approaches to IE that addresses this limitation is distant supervision, which generates training data automatically by aligning a Sentence Latent Label Barack Obama is the 44th and current President EmployedBy of the United States. Obama was born in the United States just as he BornIn has always said. United States President Barack Obama meets EmployedBy with Chinese Vice President Xi Jinping today. Obama ran for the United States Senate in 2004. – Figure 1: Training sentences generated through distant supervision for a database containing two facts. database of facts with text (Craven and Kumlien, 1999; Bunescu and Mooney, 2007). In this paper we focus on distant supervision for relation extraction (RE), a subproblem of IE that addresses the extraction of labeled relations between two named entities. Figure 1 shows a simple example for a RE domain with two labels. Distant supervision introduces two modeling challenges, which we highlight in the table. The first challenge is that some training examples obtained through this heuristic are not valid, e.g., the last sentence in Figure 1 is not a correct example for any of the known labels for the tuple. The percentage of such false positives ca</context>
<context position="4886" citStr="Craven and Kumlien (1999)" startWordPosition="764" endWordPosition="767"> MIML learning for relation extraction. Our work makes the following contributions: (a) To our knowledge, MIML-RE is the first RE approach that jointly models both multiple instances (by modeling the latent labels assigned to instances) and multiple labels (by providing a simple method to capture dependencies between labels). For example, our model learns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher D Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="20808" citStr="Finkel et al., 2005" startWordPosition="3506" endWordPosition="3509">tally “flips” mention labels based on equation (2), for each group of mentions Mi. Thus, z� changes as the algorithm progresses, which may impact the label assigned to the remaining mentions in that group. To avoid any potential bias introduced by the arbitrary order of mentions as seen in the data, we randomize each group Mi before we inspect its mentions. 5 Experimental Results 5.1 Data We evaluate our algorithm on two corpora. The first was developed by Riedel et al. (2010) by aligning Freebase3 relations with the New York Times (NYT) corpus. They used the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence. Riedel et al. (2010) observes that evaluating on this corpus underestimates true extraction accuracy because Freebase is incomplete. Thus, some relations extracted during testing will be incorrectly marked as wrong, simply because Freebase has no information on them. To mitigate this issue, Riedel et al. (2010) and Hoffman et al. (2011) perform a second evaluation where they compute the accuracy of labels assigned to a set of relation mentions that they manually annotated. To a</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledgebased weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5237" citStr="Hoffmann et al., 2011" startWordPosition="819" endWordPosition="822">ns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple m</context>
<context position="18756" citStr="Hoffmann et al. (2011)" startWordPosition="3144" endWordPosition="3147">ssume that all the corresponding mentions are positive examples for the corresponding label (Mintz et al., 2009). Note that this heuristic repeats relation mentions with different labels for the tuples that participate in multiple relations. For example, all the relation mentions in Figure 1 will yield datums with both the EmployedBy and BornIn labels. Despite this limitation, we found that this is a better initialization heuristic than random assignment. For the second part of equation (2), we initialize the relation-level classifier with a model that replicates the at least one heuristic of Hoffmann et al. (2011). Each w(r) y model has a single feature with a high positive weight that is triggered when label r is assigned to any of the mentions in z*i . Avoiding overfitting: A naive implementation of our approach leads to an unrealistic training scenario where the z classifier generates predictions (in equation (2)) for the same datums it has seen in training in the previous iteration. To avoid this overfitting problem we used cross validation: we divided the training tuples in K distinct folds and trained K different mention-level classifiers. Each classifier outputs p(z|x(m) i , wz) for tuples in a </context>
<context position="24573" citStr="Hoffmann et al., 2011" startWordPosition="4136" endWordPosition="4139">rent document and the query that concatenates the two entity names. We used the default Lucene similarity measure. 460 # of gold # of gold % of gold entity tuples % of gold entity tuples relations relations with more than one label with multiple mentions in text in training in testing in training in training Riedel 4,700 1,950 7.5% 46.4% KBP 183,062 3,334 2.8% 65.1% % of mentions that # of relation labels do not express their relation 51 up to 31% 41 up to 39% Table 1: Statistics about the two corpora used in this paper. Some of the numbers for the Riedel dataset is from (Riedel et al., 2010; Hoffmann et al., 2011). a subsampling probability of 5% because this led to the best results in development for all models. Table 1 provides additional statistics about the two corpora. The table indicates that having multiple mentions for an entity tuple is a very common phenomenon in both corpora, and that having multiple labels per tuple is more common in the Riedel dataset than KBP (7.5% vs. 2.8%). 5.2 Features Our model requires two sets of features: one for the mention classifier (z) and one for the relation classifier (y). In the Riedel dataset, we used the same features as Riedel et al. (2010) and Hoffmann </context>
<context position="27734" citStr="Hoffmann et al., 2011" startWordPosition="4682" endWordPosition="4685">del used to initialize the mention-level classifier in our model. As discussed in Section 4.3, this model follows the “traditional” distant supervision heuristic, similarly to (Mintz et al., 2009). However, our implementation has several advantages over the original model: (a) we model each relation mention independently, whereas Mintz et al. collapsed all the mentions of the same entity tuple into a single datum; (b) we allow multi-label outputs for a given entity tuple at prediction time by OR-ing the predictions for the individual relation mentions corresponding to the tuple (similarly to (Hoffmann et al., 2011))6; and (c) we use the simple bagging strategy described in Section 4.3 to combine multiple models. Empirically, we observed that these changes yield a significant improvement over the original proposal. For this reason, we consider this model a strong baseline on its own. Riedel – This is the “at-least-once” model reported in (Riedel et al., 2010), which had the best performance in that work. This approach models the task as a multi-instance single-label problem. Note that this is the only model shown here that does not allow multi-label outputs for an entity tuple. 6We also allow multiple la</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledgebased weak supervision for information extraction of overlapping relations. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa T Dang</author>
<author>Kira Griffitt</author>
<author>Joe Ellis</author>
</authors>
<title>knowledge base population track.</title>
<date>2010</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="21704" citStr="Ji et al., 2010" startWordPosition="3646" endWordPosition="3649"> during testing will be incorrectly marked as wrong, simply because Freebase has no information on them. To mitigate this issue, Riedel et al. (2010) and Hoffman et al. (2011) perform a second evaluation where they compute the accuracy of labels assigned to a set of relation mentions that they manually annotated. To avoid any potential annotation biases, we instead evaluate on a second corpus that has comprehensive annotations generated by experts for all test relations. We constructed this second dataset using mainly resources distributed for the 2010 and 2011 KBP 3freebase.com shared tasks (Ji et al., 2010; Ji et al., 2011). We generated training relations from the knowledge base provided by the task organizers, which is a subset of the English Wikipedia infoboxes from a 2008 snapshot. Similarly to the corpus of Riedel et al., these infoboxes contain open-domain relations between named entities, but with a different focus. For example, more than half of the relations in the evaluation data are alternate names of organizations or persons (e.g., org:alternate names) or relations associated with employment and membership (e.g., per:employee of) (Ji et al., 2011). We aligned these relations against</context>
</contexts>
<marker>Ji, Grishman, Dang, Griffitt, Ellis, 2010</marker>
<rawString>Heng Ji, Ralph Grishman, Hoa T. Dang, Kira Griffitt, and Joe Ellis. 2010. Overview of the TAC 2010 knowledge base population track. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
<author>Hoa T Dang</author>
</authors>
<title>knowledge base population track.</title>
<date>2011</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="21722" citStr="Ji et al., 2011" startWordPosition="3650" endWordPosition="3653">ill be incorrectly marked as wrong, simply because Freebase has no information on them. To mitigate this issue, Riedel et al. (2010) and Hoffman et al. (2011) perform a second evaluation where they compute the accuracy of labels assigned to a set of relation mentions that they manually annotated. To avoid any potential annotation biases, we instead evaluate on a second corpus that has comprehensive annotations generated by experts for all test relations. We constructed this second dataset using mainly resources distributed for the 2010 and 2011 KBP 3freebase.com shared tasks (Ji et al., 2010; Ji et al., 2011). We generated training relations from the knowledge base provided by the task organizers, which is a subset of the English Wikipedia infoboxes from a 2008 snapshot. Similarly to the corpus of Riedel et al., these infoboxes contain open-domain relations between named entities, but with a different focus. For example, more than half of the relations in the evaluation data are alternate names of organizations or persons (e.g., org:alternate names) or relations associated with employment and membership (e.g., per:employee of) (Ji et al., 2011). We aligned these relations against a document collec</context>
</contexts>
<marker>Ji, Grishman, Dang, 2011</marker>
<rawString>Heng Ji, Ralph Grishman, and Hoa T. Dang. 2011. Overview of the TAC 2011 knowledge base population track. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5193" citStr="Mintz et al., 2009" startWordPosition="811" endWordPosition="814">ween labels). For example, our model learns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance s</context>
<context position="18246" citStr="Mintz et al., 2009" startWordPosition="3062" endWordPosition="3065"> likelihood, it is important to provide it with good starting values. In our context, the initial values are labels assigned to zi, which are required to compute equation (2) in the first iteration (z�). We generate these values using a local logistic regression classifier that uses the same features as the mention-level classifier in the joint model but treats each relation mention independently. We train this classifier using “traditional” distant supervision: for each relation in the database D we assume that all the corresponding mentions are positive examples for the corresponding label (Mintz et al., 2009). Note that this heuristic repeats relation mentions with different labels for the tuples that participate in multiple relations. For example, all the relation mentions in Figure 1 will yield datums with both the EmployedBy and BornIn labels. Despite this limitation, we found that this is a better initialization heuristic than random assignment. For the second part of equation (2), we initialize the relation-level classifier with a model that replicates the at least one heuristic of Hoffmann et al. (2011). Each w(r) y model has a single feature with a high positive weight that is triggered whe</context>
<context position="27308" citStr="Mintz et al., 2009" startWordPosition="4615" endWordPosition="4618">t be generated jointly, the model will assign a negative weight to feature 2 in r1’s classifier and to feature 1 in r2’s classifier. Note that this feature is asymmetric, i.e., feature 1 in r2’s classifier may have a different value than feature 2 in r1’s classifier, depending on the accuracy of the individual predictions for r1 and r2. 5.3 Baselines We compare our approach against three models: Mintz++ – This is the model used to initialize the mention-level classifier in our model. As discussed in Section 4.3, this model follows the “traditional” distant supervision heuristic, similarly to (Mintz et al., 2009). However, our implementation has several advantages over the original model: (a) we model each relation mention independently, whereas Mintz et al. collapsed all the mentions of the same entity tuple into a single datum; (b) we allow multi-label outputs for a given entity tuple at prediction time by OR-ing the predictions for the individual relation mentions corresponding to the tuple (similarly to (Hoffmann et al., 2011))6; and (c) we use the simple bagging strategy described in Section 4.3 to combine multiple models. Empirically, we observed that these changes yield a significant improvemen</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>End-to-end relation extraction using distant supervision from external semantic repositories.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="5265" citStr="Nguyen and Moschitti, 2011" startWordPosition="823" endWordPosition="826">tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tuple b</context>
</contexts>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</booktitle>
<contexts>
<context position="3014" citStr="Riedel et al. (2010)" startWordPosition="467" endWordPosition="470">n this paper we focus on distant supervision for relation extraction (RE), a subproblem of IE that addresses the extraction of labeled relations between two named entities. Figure 1 shows a simple example for a RE domain with two labels. Distant supervision introduces two modeling challenges, which we highlight in the table. The first challenge is that some training examples obtained through this heuristic are not valid, e.g., the last sentence in Figure 1 is not a correct example for any of the known labels for the tuple. The percentage of such false positives can be quite high. For example, Riedel et al. (2010) report up to 31% of false positives in a corpus that matches Freebase relations with New York Times articles. The second challenge is that the same pair of entities may have multiple labels and it is unclear which label is instantiated by any textual mention of the given tuple. For example, in Figure 1, the tuple (Barack Obama, United States) has two valid labels: BornIn and EmployedBy, each (latently) instantiated in different sentences. In the 455 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 4</context>
<context position="5214" citStr="Riedel et al., 2010" startWordPosition="815" endWordPosition="818">ample, our model learns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, </context>
<context position="20669" citStr="Riedel et al. (2010)" startWordPosition="3483" endWordPosition="3486">els generated in the last training iteration. Inference during training: During the inference process in the E-step, the algorithm incrementally “flips” mention labels based on equation (2), for each group of mentions Mi. Thus, z� changes as the algorithm progresses, which may impact the label assigned to the remaining mentions in that group. To avoid any potential bias introduced by the arbitrary order of mentions as seen in the data, we randomize each group Mi before we inspect its mentions. 5 Experimental Results 5.1 Data We evaluate our algorithm on two corpora. The first was developed by Riedel et al. (2010) by aligning Freebase3 relations with the New York Times (NYT) corpus. They used the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in text and constructed relation mentions only between entity mentions in the same sentence. Riedel et al. (2010) observes that evaluating on this corpus underestimates true extraction accuracy because Freebase is incomplete. Thus, some relations extracted during testing will be incorrectly marked as wrong, simply because Freebase has no information on them. To mitigate this issue, Riedel et al. (2010) and Hoffman et al. (2011) perf</context>
<context position="22844" citStr="Riedel et al. (2010)" startWordPosition="3832" endWordPosition="3835">ip (e.g., per:employee of) (Ji et al., 2011). We aligned these relations against a document collection that merges two distinct sources: (a) the collection provided by the shared task, which contains approximately 1.5 million documents from a variety of sources, including newswire, blogs and telephone conversation transcripts; and (b) a complete snapshot of the English Wikipedia from June 2010. During training, for each entity tuple (e1, e2), we retrieved up to 50 sentences that contain both entity mentions.4 We used Stanford’s CoreNLP package to find entity mentions in text and, similarly to Riedel et al. (2010), we construct relation mention candidates only between entity mentions in the same sentence. We analyzed a set of over 2,000 relation mentions and we found that 39% of the mentions where e1 is an organization name and 36% of mentions where e1 is a person name do not express the corresponding relation. At evaluation time, the KBP shared task requires the extraction of all relations r(e1, e2) given a query that contains only the first entity e1. To accommodate this setup, we adjusted our sentence extraction component to use just e1 as the retrieval query and we kept up to 50 sentences that cont</context>
<context position="24549" citStr="Riedel et al., 2010" startWordPosition="4132" endWordPosition="4135">rity between their parent document and the query that concatenates the two entity names. We used the default Lucene similarity measure. 460 # of gold # of gold % of gold entity tuples % of gold entity tuples relations relations with more than one label with multiple mentions in text in training in testing in training in training Riedel 4,700 1,950 7.5% 46.4% KBP 183,062 3,334 2.8% 65.1% % of mentions that # of relation labels do not express their relation 51 up to 31% 41 up to 39% Table 1: Statistics about the two corpora used in this paper. Some of the numbers for the Riedel dataset is from (Riedel et al., 2010; Hoffmann et al., 2011). a subsampling probability of 5% because this led to the best results in development for all models. Table 1 provides additional statistics about the two corpora. The table indicates that having multiple mentions for an entity tuple is a very common phenomenon in both corpora, and that having multiple labels per tuple is more common in the Riedel dataset than KBP (7.5% vs. 2.8%). 5.2 Features Our model requires two sets of features: one for the mention classifier (z) and one for the relation classifier (y). In the Riedel dataset, we used the same features as Riedel et </context>
<context position="28084" citStr="Riedel et al., 2010" startWordPosition="4739" endWordPosition="4742">ed all the mentions of the same entity tuple into a single datum; (b) we allow multi-label outputs for a given entity tuple at prediction time by OR-ing the predictions for the individual relation mentions corresponding to the tuple (similarly to (Hoffmann et al., 2011))6; and (c) we use the simple bagging strategy described in Section 4.3 to combine multiple models. Empirically, we observed that these changes yield a significant improvement over the original proposal. For this reason, we consider this model a strong baseline on its own. Riedel – This is the “at-least-once” model reported in (Riedel et al., 2010), which had the best performance in that work. This approach models the task as a multi-instance single-label problem. Note that this is the only model shown here that does not allow multi-label outputs for an entity tuple. 6We also allow multiple labels per tuple at training time, in which case we replicate the corresponding datum for each label. However, this did not improve performance significantly compared to selecting a single label per datum during training. 461 Hoffmann – This is the “MultiR” model, which performed the best in (Hoffmann et al., 2011). This models RE as a MIML problem, </context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ang Sun</author>
<author>Ralph Grishman</author>
<author>Wei Xu</author>
<author>Bonan Min</author>
</authors>
<title>system for KBP slot filling.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<location>New York University</location>
<contexts>
<context position="5283" citStr="Sun et al., 2011" startWordPosition="827" endWordPosition="830"> while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tuple but disallows more </context>
<context position="12012" citStr="Sun et al. (2011)" startWordPosition="1961" endWordPosition="1964">ce and z is the latent relation classification for that sentence; • wz is the weight vector for the multi-class mention-level classifier; • k is the number of known relation labels in L; • yj is the top-level classification decision for the entity pair as to whether the jth relation holds; • wj is the weight vector for the binary top-level classifier for the jth relation. Additionally, we define Pi (Ni) as the set of all known positive (negative) relation labels for the ith entity tuple. In this paper, we construct Ni as L \ Pi, but, in general, other scenarios are possible. For example, both Sun et al. (2011) and Surdeanu et . . . . . . . . . . . . 457 al. (2011a) proposed models where Ni for the ith tuple (e1, e2) is defined as: {rj |rj(e1, ek) E A ek =� e2, rj E/ Pit, which is a subset of L\Pi. That is, entity e2 is considered a negative example for relation rj (in the context of entity e1) only if rj exists in the training data with a different value. The addition of the object-level layer (for y) is an important contribution of this work. This layer can capture information that cannot be modeled by the mention-level classifier. For example, it can learn that two relation labels (e.g., BornIn a</context>
</contexts>
<marker>Sun, Grishman, Xu, Min, 2011</marker>
<rawString>Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min. 2011. New York University 2011 system for KBP slot filling. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sonal Gupta</author>
<author>John Bauer</author>
<author>David McClosky</author>
<author>Angel X Chang</author>
<author>Valentin I Spitkovsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford’s distantlysupervised slot-filling system.</title>
<date>2011</date>
<booktitle>In Proceedings of the Text Analytics Conference.</booktitle>
<contexts>
<context position="5306" citStr="Surdeanu et al., 2011" startWordPosition="831" endWordPosition="834">ot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tuple but disallows more than one label per obje</context>
<context position="25319" citStr="Surdeanu et al., 2011" startWordPosition="4268" endWordPosition="4271">onal statistics about the two corpora. The table indicates that having multiple mentions for an entity tuple is a very common phenomenon in both corpora, and that having multiple labels per tuple is more common in the Riedel dataset than KBP (7.5% vs. 2.8%). 5.2 Features Our model requires two sets of features: one for the mention classifier (z) and one for the relation classifier (y). In the Riedel dataset, we used the same features as Riedel et al. (2010) and Hoffmann et al. (2011) for the mention classifier. In the KBP dataset, we used a feature set that was developed in our previous work (Surdeanu et al., 2011b). These features can be grouped in three classes: (a) features that model the two entities, such as their head words; (b) features that model the syntactic context of the relation mention, such as the dependency path between the two entity mentions; and (c) features that model the surface context, such as the sequence of part of speech tags between the two entity mentions. We used these features for all the models evaluated on the KBP dataset.5 For the relation-level classifier, we developed two feature groups. The first models Hoffmann et al.’s at least one heuristic using a single feature,</context>
</contexts>
<marker>Surdeanu, Gupta, Bauer, McClosky, Chang, Spitkovsky, Manning, 2011</marker>
<rawString>Mihai Surdeanu, Sonal Gupta, John Bauer, David McClosky, Angel X. Chang, Valentin I. Spitkovsky, and Christopher D. Manning. 2011a. Stanford’s distantlysupervised slot-filling system. In Proceedings of the Text Analytics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>David McClosky</author>
<author>Mason R Smith</author>
<author>Andrey Gusev</author>
<author>Christopher D Manning</author>
</authors>
<title>Customizing an information extraction system to a new domain.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Relational Models of Semantics,</booktitle>
<location>Portland, Oregon,</location>
<contexts>
<context position="5306" citStr="Surdeanu et al., 2011" startWordPosition="831" endWordPosition="834">ot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tuple but disallows more than one label per obje</context>
<context position="25319" citStr="Surdeanu et al., 2011" startWordPosition="4268" endWordPosition="4271">onal statistics about the two corpora. The table indicates that having multiple mentions for an entity tuple is a very common phenomenon in both corpora, and that having multiple labels per tuple is more common in the Riedel dataset than KBP (7.5% vs. 2.8%). 5.2 Features Our model requires two sets of features: one for the mention classifier (z) and one for the relation classifier (y). In the Riedel dataset, we used the same features as Riedel et al. (2010) and Hoffmann et al. (2011) for the mention classifier. In the KBP dataset, we used a feature set that was developed in our previous work (Surdeanu et al., 2011b). These features can be grouped in three classes: (a) features that model the two entities, such as their head words; (b) features that model the syntactic context of the relation mention, such as the dependency path between the two entity mentions; and (c) features that model the surface context, such as the sequence of part of speech tags between the two entity mentions. We used these features for all the models evaluated on the KBP dataset.5 For the relation-level classifier, we developed two feature groups. The first models Hoffmann et al.’s at least one heuristic using a single feature,</context>
</contexts>
<marker>Surdeanu, McClosky, Smith, Gusev, Manning, 2011</marker>
<rawString>Mihai Surdeanu, David McClosky, Mason R. Smith, Andrey Gusev, and Christopher D. Manning. 2011b. Customizing an information extraction system to a new domain. In Proceedings of the Workshop on Relational Models of Semantics, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Dan Weld</author>
</authors>
<title>Autonomously semantifying Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="5173" citStr="Wu and Weld, 2007" startWordPosition="807" endWordPosition="810">re dependencies between labels). For example, our model learns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction a</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Dan Weld. 2007. Autonomously semantifying Wikipedia. In Proceedings of the International Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z H Zhou</author>
<author>M L Zhang</author>
</authors>
<title>Multi-instance multilabel learning with application to scene classification.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="6731" citStr="Zhou and Zhang (2007)" startWordPosition="1053" endWordPosition="1056">el that aggregates latent instance labels into a set of labels for the corresponding tuple by OR-ing the classification results. We use instead an objectlevel classifier that is trained jointly with the classifier that assigns latent labels to instances and can capture dependencies between labels. Second, they use a Perceptron-style additive parameter update approach, whereas we train in a Bayesian framework. We show in Section 5 that these approximations generally have a negative impact on performance. MIML learning has been used in fields other than natural language processing. For example, Zhou and Zhang (2007) use MIML for scene classification. In this problem, each image may be assigned multiple labels corresponding to the different scenes captured. Furthermore, each image contains a set of patches, which forms the bag of instances assigned to the given object (image). Zhou and Zhang propose two algorithms that reduce the MIML problem to a more traditional supervised learning task. In one algorithm, for example, they convert the task to a multi-instance single-label problem by creating a separate bag for each label. Due to this, the proposed approach cannot model inter-label dependencies. Moreover</context>
</contexts>
<marker>Zhou, Zhang, 2007</marker>
<rawString>Z.H. Zhou and M.L. Zhang. 2007. Multi-instance multilabel learning with application to scene classification. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996794">
Collocation Polarity Disambiguation Using Web-based Pseudo Contexts
</title>
<author confidence="0.999553">
Yanyan Zhao, Bing Qin and Ting Liu∗
</author>
<affiliation confidence="0.997677">
Harbin Institute of Technology, Harbin, China
</affiliation>
<email confidence="0.995062">
{yyzhao, bqin, tliu}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.995638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996847368421">
This paper focuses on the task of colloca-
tion polarity disambiguation. The collocation
refers to a binary tuple of a polarity word and
a target (such as (long, battery life) or (long,
startup)), in which the sentiment orientation of
the polarity word (“long”) changes along with
different targets (“battery life” or “startup”).
To disambiguate a collocation’s polarity, pre-
vious work always turned to investigate the
polarities of its surrounding contexts, and then
assigned the majority polarity to the collo-
cation. However, these contexts are limited,
thus the resulting polarity is insufficient to be
reliable. We therefore propose an unsuper-
vised three-component framework to expand
some pseudo contexts from web, to help dis-
ambiguate a collocation’s polarity.Without us-
ing any additional labeled data, experiments
show that our method is effective.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795090909091">
In recent years, more attention has been paid to
sentiment analysis as it has been widely used in
various natural language processing applications,
such as question answering (Wiebe et al., 2003;
Yu and Hatzivassiloglou, 2003), information extrac-
tion (Riloff et al., 2005) and opinion-oriented sum-
marization (Hu and Liu, 2004; Liu et al., 2005).
Meanwhile, it also brings us lots of interesting and
challenging research topics, such as subjectivity
analysis (Riloff and Wiebe, 2003), sentiment clas-
sification (Pang et al., 2002; Kim and Hovy, 2005;
</bodyText>
<note confidence="0.76699">
*Correspondence author: tliu@ir.hit.edu.cn
</note>
<bodyText confidence="0.92463046875">
Wilson et al., 2009; He et al., 2011), opinion re-
trieval (Zhang et al., 2007; Zhang and Ye, 2008;
Li et al., 2010) and so on.
One fundamental task for sentiment analysis is
to determine the semantic orientations of words.
For example, the word “beautiful” is positive, while
“ugly” is negative. Many researchers have devel-
oped several algorithms for this purpose and gener-
ated large static lexicons of words marked with prior
polarities (Hatzivassiloglou and McKeown, 1997;
Turney et al., 2003; Esuli, 2008; Mohammad et al.,
2009; Velikovich et al., 2010). However, there exist
some polarity-ambiguous words, which can dynam-
ically reflect different polarities along with different
contexts. A typical polarity-ambiguous word “长”
(“long” in English) is shown with two example sen-
tences as follows.
1.该相机的[电池寿命]t很[长]p。(Positive)
Translated as: The [battery life]t of this camera
is [long]p. (Positive)
2. 该相机的[启动时间]t很[长]p。(Negative)
Translated as: This camera has [long]p
[startup]t. (Negative)
The phrases marked with p superscript are the
polarity-ambiguous words, and the phrases marked
with t superscript are targets modified by the polar-
ity words. In the above two sentences, the sentiment
orientation of the polarity word “长” (“long” in En-
glish) changes along with different targets. When
modifying the target “电池寿命” (“battery life” in
English), its polarity is positive; and when modify-
ing “启动时间” (“startup” in English), its polarity is
</bodyText>
<page confidence="0.960298">
160
</page>
<note confidence="0.77344">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 160–170, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.998337981481481">
negative. In this paper, we especially define the col-
location as a binary tuple of the polarity-ambiguous
word and its modified target, such as ⟨-K_,����⟩
(⟨long, battery life⟩ in English) or ⟨-K_, �jRfN⟩
(⟨long, startup⟩ in English). This paper concentrates
on the task of collocation polarity disambiguation.
This is an important task as the problem of
polarity-ambiguity is frequent. We analyze 4,861
common binary tuples of polarity words and their
modified targets from 478 reviews l, and find that
over 20% of them are the collocations defined in this
paper. Therefore, the task of collocation polarity dis-
ambiguation is worthy of study.
For a sentence s containing such a collocation c,
since the in-sentence features are always ambiguous,
it is difficult to disambiguate the polarity of c by us-
ing them. Thus some previous work turned to in-
vestigate its surrounding contexts’ polarities (such
as the sentences before or after s), and then assigned
the majority polarity to the collocation c (Hatzivas-
siloglou and McKeown, 1997; Hu and Liu, 2004;
Kanayama and Nasukawa, 2006). However, since
the amount of contexts from the original review is
very limited, the final resulting polarity for the col-
location c is insufficient to be reliable.
Fortunately, most collocations may appear multi-
ple times, in different forms, both within the same
review and within topically-related reviews. Thus
for a collocation, we can collect large amounts of
contexts from other reviews to improve its polarity
disambiguation. These expanded contexts are called
pseudo contexts in this paper. Some previous work
used the similar methods. For example, Ding (Ding
et al., 2008) expanded some pseudo contexts from
a topically-related review set. But since the review
set is limited, the expanded contexts are still lim-
ited and unreliable. In order to overcome this prob-
lem, we propose an unsupervised three-component
framework to expand more pseudo contexts from
web for the collocation polarity disambiguation.
Without using any labeled data, experiments on
a Chinese data set from four product domains show
that the three-component framework is feasible and
the web-based pseudo contexts are useful for the
collocation polarity disambiguation. Compared to
other previous work, our method achieves an F1
&apos;The dataset will be introduced in Section 4.1 in detail.
score of 72.02%, which is about 15% higher.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work. Section
3 shows the proposed approach including three in-
dependent components. Section 4 and 5 presents the
experiments and results. Finally we conclude this
paper in Section 6.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999877815789474">
The key of the collocation polarity disambigua-
tion task is to recognize the polarity word’s sen-
timent orientation of a collocation. There are ba-
sically two types of approaches for word polar-
ity recognition: corpus-based and dictionary-based
approaches. Corpus-based approaches find co-
occurrence patterns of words in the large corpora
to determine the word sentiments, such as the work
in (Hatzivassiloglou and McKeown, 1997; Wiebe,
2000; Riloff and Wiebe, 2003; Turney et al., 2003;
Kaji and Kitsuregawa, 2007; Velikovich et al.,
2010). On the other hand, dictionary-based ap-
proaches use synonyms and antonyms in WordNet
to determine word sentiments based on a set of seed
polarity words. Such approaches are studied in (Kim
and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps
et al., 2004). Overall, most of the above approaches
aim to generate a large static polarity word lexicon
marked with prior polarities.
However, it is not sensible to predict a word’s sen-
timent orientation without considering its context.
In fact, even in the same domain, a word may indi-
cate different polarities depending on what targets it
is applied to, especially for the polarity-ambiguous
words, such as “-K_” (“long” in English) shown in
Section 1. Based on these, we need to consider both
the polarity words and their modified targets, i.e.,
the collocations mentioned in this paper, rather than
only the polarity words.
To date, the task in this paper is similar with
much previous work. Some researchers exploited
the features of the sentences containing colloca-
tions to help disambiguate the polarity of the
polarity-ambiguous word. For example, Hatzivas-
siloglou (Hatzivassiloglou and McKeown, 1997)
and Kanayama (Kanayama and Nasukawa, 2006)
used conjunction rules to solve this problem from
large domain corpora. Suzuki (Suzuki et al., 2006)
</bodyText>
<page confidence="0.98806">
161
</page>
<figure confidence="0.99679945">
start
A Chinese
collocation
in a review
Query
expansion
Searching
Web
snippets
Original context
acquisition
Pseudo context
acquisition
Combination
Sentiment
analysis
Sentiment
analysis
end
Pos/Neg
</figure>
<bodyText confidence="0.9998714375">
took into account many contextual information of
the word within the sentence, such as exclamation
words, emoticons and so on. However, the experi-
mental results show that these in-sentence features
are not rich enough.
Instead of considering the current sentence alone,
some researchers exploited external information and
evidences in other sentences or other reviews to infer
the collocation’s polarity. For a collocation, Hu (Hu
and Liu, 2004) analyzed its surrounding sentences’
polarities to disambiguate its polarity. Ding (Ding
et al., 2008) proposed a holistic lexicon-based ap-
proach of using global information to solve this
problem. However, the contexts or evidences from
these two methods are limited and unreliable. Ex-
cept for the above unsupervised methods, some re-
searchers (Wilson et al., 2005; Wilson et al., 2009)
proposed supervised methods for this task, which
need large annotated corpora.
In addition, many related works tried to learn
word polarity in a specific domain, but ignored the
problem that even the same word in the same do-
main may indicate different polarities (Jijkoun et al.,
2010; Bollegala et al., 2011). And some work (Lu et
al., 2011) combined difference sources of informa-
tion, especially the lexicons and heuristic rules for
this task, but ignored the important role of the con-
text. Besides, there exists some research focusing
on word sense subjectivity disambiguation, which
aims to classify a word sense into subjective or ob-
jective (Wiebe and Mihalcea, 2006; Su and Markert,
2009). Obviously, this task is different from ours.
</bodyText>
<sectionHeader confidence="0.98136" genericHeader="method">
3 The Proposed Approach
</sectionHeader>
<subsectionHeader confidence="0.988923">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.995918">
The motivation of our approach is to make full use of
web sources to collect more useful pseudo contexts
for a collocation, whose original contexts are lim-
ited or unreliable. The framework of our approach
is illustrated in Figure 1.
In order to disambiguate a collocation’s polarity,
three components are carried out:
</bodyText>
<listItem confidence="0.9083585">
1. Query Expansion and Pseudo Context Ac-
quisition: This paper uses the collocation as query.
</listItem>
<bodyText confidence="0.827319">
For a collocation, three heuristic query expansion
strategies are used to generate more flexible queries,
which have the same or completely opposite polar-
</bodyText>
<figureCaption confidence="0.999555">
Figure 1: The framework of our approach.
</figureCaption>
<bodyText confidence="0.988981">
ity with this collocation. Searching these queries in
the domain-related websites, lots of snippets can be
acquired. Then we can extract the pseudo contexts
from these snippets.
</bodyText>
<listItem confidence="0.909354625">
2. Sentiment Analysis: For both original con-
texts and the expanded pseudo contexts from web, a
simple lexicon-based sentiment computing method
is used to recognize each context’s polarity.
3. Combination: Two strategies are designed to
integrate the polarities of the original and pseudo
contexts, under the assumption that these two kinds
of contexts can be complementary to each other.
</listItem>
<bodyText confidence="0.9998332">
It is worth noting that this three-component
framework is flexible and we can try to design dif-
ferent strategies for each component. Next sections
will give a simple example strategy for each compo-
nent to show its feasibility and effectiveness.
</bodyText>
<subsectionHeader confidence="0.958889">
3.2 Query Expansion and Pseudo Context
Acquisition
3.2.1 Why Expanding Queries
</subsectionHeader>
<bodyText confidence="0.999874272727273">
For a collocation, such as (长,电池寿命) ((long,
battery life) in English), the most intuitive query
used for searching is constructed by the form of “tar-
get + polarity word”, i.e., 电池寿命长 (battery
life long in English). Even if we search this query
alone, a great many web snippets covering the po-
larity word and target will be retrieved. But why do
we still need to expand the queries?
In fact, for a collocation, though the amount of the
retrieved snippets is large, lots of them cannot pro-
vide accurate pseudo contexts. The reason is that the
</bodyText>
<page confidence="0.989603">
162
</page>
<bodyText confidence="0.999869">
polarity words in some snippets do not really mod-
ify the targets, such as in the sentence “The battery
life is short, and finds few buyers for a long time.”
There exist no modifying relation between “battery
life” and “long”.
In order to filter these meaningless snippets, we
can simply search with a new query “电池寿命长”
by surrounding it with quotes (noted as Strategy0).
However, this can drastically decline the amount of
snippets. In addition, as the new query is short, in
many retrieved snippets, there also exist no modify-
ing relations between the polarity words and targets.
As a result, if we just use this query strategy, the ex-
panded pseudo contexts are limited and cannot yield
ideal performance.
Therefore, we need to design some effective
query expansion strategies to ensure that (1) the po-
larity words do modify the targets in the retrieved
web snippets, and (2) the snippets are more enough.
</bodyText>
<subsectionHeader confidence="0.817973">
3.2.2 Query Expansion Strategy
</subsectionHeader>
<bodyText confidence="0.999773347826087">
We first investigate the modifying relations be-
tween polarity words and the targets, and then con-
struct effective queries.
Observed from previous work (Bloom et al.,
2007; Kobayashi et al., 2004; Popescu and Etzioni,
2005), there are two kinds of common relations be-
tween the polarity words and their targets. One is
the “subject-copula-predicate” relation, such as the
relationship between “long” and “battery life” in the
sentence “The battery life of this camera is long”.
The other is the “attribute-head” relation, such as
the relationship between them in the sentence “This
camera has long battery life”.
As a result, three heuristic query expansion strate-
gies are adopted to construct efficient queries for
searching. Take the collocation (长,电 池 寿 命)
((long, battery life) in English) as an example, the
strategies are described as follows.
Strategy1: target + modifier + polarity word:
Such as the query “电池寿命TM长” or “电池寿命
49R长” (“the battery life is very long” in English).
Different from Strategy0, this strategy adds a mod-
ifier element. It refers to the words that are used to
change the degree of a polarity word, such as “很” or
“非常” (“very” in English). Due to the usage of the
modifiers, the queries from this strategy can satisfy
the “subject-copula-predicate” relation.
Strategy2: modifier + polarity word + 的+ tar-
get: Such as the query “TM长的电池寿命” or “49
R长的电池寿命” (“very long battery life” in En-
glish). This strategy also uses modifiers to modify
polarity words, and the generated queries can satisfy
the “attribute-head” relation.
Strategy3: negation word + polarity word + 的+
target: Such as the query “T长的电池寿命” or “&amp;
A长的电池寿命” (“not long battery life” in En-
glish). This strategy uses negation words to modify
the polarity words. And the queries from this strat-
egy can satisfy the “attribute-head” relation. The
only difference is that the polarity of this kind of
queries is opposite to that of the collocation.
Similar to the queries from Strategy0, the queries
generated by Strategy1-3 are all searched with
quotes. In addition, note that the modifier and the
negation word are taken from Modifier Lexicon and
Negation Lexicon introduced in Table 2.
</bodyText>
<subsectionHeader confidence="0.81932">
3.2.3 Pseudo Context Acquisition
</subsectionHeader>
<bodyText confidence="0.999931133333333">
For each query from Strategy0-3, we search it in
some websites to acquire the related snippets. If we
directly search it using Google without site restric-
tions, it does return all the snippets containing the
query, but lots of them are non-reviews. Further, the
pseudo contexts generated by these non-reviews are
useless or even harmful. To overcome this problem,
the advanced search of Google is used to search the
query within the forum sites of the product domain.
We can flexibly choose several popular forum sites
for each domain. The URLs of the forum sites used
in this paper are listed in Table 1.
Formally, given a collocation c, the expanded
pseudo contexts Conx(c) can be obtained using the
following function:
</bodyText>
<equation confidence="0.9920695">
Conx(c) = ∪3 0 f (Queryi) ( )
∪i=0 ∪n
j=1f (queryij)
1
</equation>
<bodyText confidence="0.999527571428571">
Here, Queryi is the query set generated by the ith
query expansion strategy; queryij is the jth query
generated by the ith strategy. And the parameter n is
the total number of queries from the ith query expan-
sion strategy. From this function, we can collect the
contexts of c by summing up all the pseudo contexts
from every queryij.
</bodyText>
<page confidence="0.988581">
163
</page>
<figure confidence="0.9992765">
Domain URL
Camera http://www.qqdc.com.cn
http://forums.nphoto.net
http://dc.pconline.com.cn
http://photobbs.it168.com
http://club.tech.sina.com.cn/dc
Car http://bbs.chetx.com
http://bbs.pcauto.com.cn
http://club.autohome.com.cn
http://bbs.cheshi.com
http://www.xcar.com.cn
http://www.autohome.com.cn
Notebook http://benyouhui.it168.com/index.php
http://nbbbs.zol.com.cn
http://www.ibijiben.com
http://notebook.pconline.com.cn
http://nbbbs.enet.com.cn
Phone http://bbs.imobile.com.cn
http://sjbbs.zol.com.cn
http://bbs.shouji.com.cn
http://bbs.cnmo.com
http://forum.younet.com
</figure>
<tableCaption confidence="0.968758">
Table 1: The URLs used in context expansion for differ-
ent domains.
</tableCaption>
<bodyText confidence="0.999007666666667">
In detail, the pseudo context acquisition algorithm
for a collocation c is illustrated in Figure 2. Note
that, the original context acquisition of c can be con-
sidered as a simplified version of the pseudo context
acquisition. That’s because the current review con-
taining c can be considered as only one snippet in
pseudo context acquisition. Thus, we can just carry
out the two steps in (2) of Figure 2 to obtain the orig-
inal contexts.
Analyzing either the pseudo contexts or the orig-
inal contexts, we can find that not all of them are
useful contexts. Thus we will simply filter the noisy
ones by context sentiment computation, and choose
the contexts showing sentiment orientations as the
useful contexts.
</bodyText>
<subsectionHeader confidence="0.999652">
3.3 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.993875285714286">
For both the original and expanded pseudo contexts,
we employ the lexicon-based sentiment computing
method (Hu and Liu, 2004) to compute the polarity
value for each context. This unsupervised approach
is quite straightforward and makes use of the senti-
ment lexicons in Table 2.
The polarity value Polarity(con) for a context con
</bodyText>
<figure confidence="0.426594818181818">
Algorithm: Pseudo Context Expansion Algorithm
Input: A collocation c and the URL list
Output: The pseudo context set Conx(c)
1. Use Strategy0-3 to expand c and the expanded queries
are saved as a set Query(c).
2. For any query q Query(c),
∈ acquire its pseudo
contexts Conx(q) as follows:
(1) search q in the domain-related URL list, the top 100
retrieved snippets for each URL are collected as Snip(q)
(2) for each snippet sp Snip(q)
∈
find the sentence s containing q
obtain the two sentences before and after s as the
contexts of q in this sp, noted as Conx(q, sp)
Conx(q) _
sp Snip q
∈ ( )
3. Conx(c) _ U Conx (q) _ Conx q sp
( , )
q Query c sp Snip q
∈ ( ) ∈ ( )
</figure>
<figureCaption confidence="0.999327">
Figure 2: The algorithm for pseudo context acquisition.
</figureCaption>
<table confidence="0.998687545454545">
Lexicon Content
Modifier Lexicon 很, 比较, 非常, 十分, 太, 特,
特别, 挺, 相当, 格外, 分外
(“very” or “quite” in English)
Negation Lexicon 没有, 不, 不是
(“no” or “not” in English)
Positive Lexicon There are 3,730 Chinese words
are collected from HOWNET1.
Negative Lexicon There are 3,116 Chinese words
are collected from HOWNET.
1 http://www.keenage.com/html/e index.html.
</table>
<tableCaption confidence="0.999819">
Table 2: The lexicons used in this paper.
</tableCaption>
<bodyText confidence="0.9995028">
is computed by summing up the polarity values of all
words in con, making use of both the word polarity
defined in the positive and negative lexicons and the
contextual shifters defined in the negation lexicon.
The algorithm is illustrated in Figure 3.
In this algorithm, n is the parameter controlling
the window size within which the negation words
have influence on the polarity words, and here n is
set to 3.
Normally, if the polarity value Polarity(con) is
more than 0, the context con is labeled as positive; if
less than 0, the context is negative. We also consider
the transitional words, such as “但是” (“but” in En-
glish). Finally, the contexts with positive/negative
polarities are used as the useful contexts.
</bodyText>
<table confidence="0.898146916666667">
Conx q sp
( , )
q Query c
∈ ( )
164
Domain # of reviews # of c # of single c Sig / All # of multiple c
(All) (Sig) (%) / kinds of multiple c
Camera 138 295 183 62.03 112 / 35
Car 161 232 131 56.47 101 / 33
Notebook 56 147 94 63.95 53 / 20
Phone 123 327 192 58.72 135 / 35
Total 478 1001 600 59.94 401 / 123 ≈ 3.3
</table>
<tableCaption confidence="0.921711">
Table 3: Statistics for the Chinese collocation corpus.
</tableCaption>
<figure confidence="0.79923475">
Algorithm: Sentiment Analysis
Input: a context con, and three lexicons: Positive�Dic,
Negative�Dic, Negation�Dic
Output: Polarity value Polarity(con)
1. Segment con into word set W(con)
2. For each word w W(con),
E compute its polarity value
Polarity(w) as follows:
(1) if w Positive�Dic,
E Polarity(w) _ 1;
(2) if w Negative�Dic,
E Polarity(w) _ -1;
(3) otherwise, Polarity(w) _ 0;
(4) Within the window of n words previous to w, if
there is a word w&apos; Negation�Dic,
E
Polarity(w) _-Polarity(w)
3. Polarity(con) _ E
wW con
∈ ( )
</figure>
<figureCaption confidence="0.999957">
Figure 3: The algorithm for context polarity computation.
</figureCaption>
<subsectionHeader confidence="0.919904">
3.4 Combination
</subsectionHeader>
<bodyText confidence="0.963705714285714">
After the pseudo context acquisition and polarity
computation, two kinds of effective contexts: orig-
inal contexts and pseudo contexts, and their corre-
sponding polarities can be obtained.
In order to yield a relatively accurate polarity Po-
larity(c) for a collocation c, we exploit the following
combination methods:
</bodyText>
<listItem confidence="0.964874818181818">
1. Majority Voting: Rather than considering the
difference between the two kinds of contexts, this
combination method relies on the polarity tag of
each context. Suppose c has n effective contexts
(including original and pseudo contexts), it can ob-
tain n polarity tags based on the individual sentiment
analysis algorithm. The polarity tag receiving more
votes is chosen as the final polarity of c.
2. Complementation: For a collocation c, we
first employ “Majority Voting” method just on the
expanded pseudo contexts to obtain the polarity tag.
</listItem>
<bodyText confidence="0.999540666666667">
If the polarity of c cannot be recognized2, the ma-
jority polarity tag voted on the original contexts is
chosen as the final polarity tag.
</bodyText>
<sectionHeader confidence="0.999883" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.995699">
4.1 Dataset and Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999792629629629">
We conduct the experiments on a Chinese colloca-
tion corpus of four product domains, which is from
the Task3 of the Chinese Opinion Analysis Evalua-
tion (COAE)3 (Zhao et al., 2008). Table 3 describes
the corpus in detail.
From 478 reviews, 1,001 collocations (454 pos-
itive and 547 negative) with polarity-ambiguous
words are found and manually annotated by two an-
notators. Cohen’s kappa (Cohen, 1960), a measure
of inter-annotator agreement ranging from zero to
one, is 0.83, indicating a good strength of agree-
ment 4. In Table 3, Sig of the fourth column denotes
the collocations that appear once in all the domain-
related reviews. And multiple in the last column
denotes the collocations that appear several times.
From Table 3, we can find that among all the re-
views, nearly 60% collocations only appear once.
Even for the multiple collocations, they averagely
appear less than 4 times. Therefore, for a colloca-
tion, if we only consider its original contexts alone
or the expanded pseudo contexts from the domain-
related review set alone, the contexts are obviously
limited and unreliable.
Instead of using accuracy, we use precision (P),
recall (R) and F-measure (F1) to measure the perfor-
mance of this task. That’s because two kinds of col-
locations’ polarities cannot be disambiguated. One
</bodyText>
<footnote confidence="0.953517">
2The reason will be explained in the last paragraph of Sec-
tion 4.1.
3http://www.ir-china.org.cn/coae2008.html
4A small number of collocations are still difficult to be dis-
ambiguated from contexts.
</footnote>
<equation confidence="0.569941">
Polarity (w)
</equation>
<page confidence="0.994203">
165
</page>
<bodyText confidence="0.909794">
is the sparse collocations, which obtain no effective
contexts. The other is the collocations that acquire
the same amount of positive and negative contexts.
The metrics are defined as follows.
correctly disambiguated collocations
</bodyText>
<equation confidence="0.995042">
P = (2)
disambiguated collocations
R
= correctly disambiguated collocations (3)
all collocations
2P R
F1 = (4)
P � R
</equation>
<subsectionHeader confidence="0.932781">
4.2 System Description
</subsectionHeader>
<bodyText confidence="0.999974333333333">
In order to compare our method with previous work,
we build several systems as follows:
NoExp: Following the method proposed by
Hu (Hu and Liu, 2004), without using the expanded
pseudo contexts, we only consider the two original
contexts Senbef and Senaft of a collocation c in the
current review. If Senbef expresses the polarity po-
lar, then Polarity(ac) = polar. Else if Senaft
expresses the polarity polar′, then Polarity(ac) =
polar′. Else, this method cannot disambiguate the
polarity of c. In this method, the transitional words,
such as “但是” (“but” in English) are considered.
Expdataset: Following the method proposed by
Ding (Ding et al., 2008), we solve this task with the
help of the pseudo contexts in the domain-related re-
view dataset. For a collocation c appearing in many
domain-related reviews, this method refers to the po-
larities of the same c in other reviews. The majority
polarity is chosen as final polarity.
Expweb+sig: This method is the same as our
method in this paper, except for (1) not combining
the original contexts, and (2) not using all the three
query expansion strategies, but just using the sin-
gle (abbv. sig) Strategy0. This method expands the
pseudo contexts from the web. The majority polarity
is chosen as the final polarity.
Expweb+exp: This method is the same as our pro-
posed method in this paper, except for not combin-
ing the original contexts. It expands the pseudo con-
texts from the web. And the “exp” in the subscript
means that this method uses all the query expansion
strategies. The majority polarity of all the pseudo
contexts is chosen as the final polarity.
</bodyText>
<equation confidence="0.658266">
Expmv/c
web+exp+com: This is the method proposed
</equation>
<bodyText confidence="0.99961875">
in this paper, which combines the original and ex-
panded pseudo contexts. The superscript “mv/c” is
short for the two combination methods: Majority
Voting and Complementation.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999671">
5.1 Comparisons among All the Systems
</subsectionHeader>
<bodyText confidence="0.978154692307692">
In fact, all the systems shown in Section 4.2 can be
considered as context based methods. The essential
difference among them lies in the contexts they used.
For a collocation, the contexts for NoExp are two
original contexts from the current review. Breaking
down the boundary of the current review, Expdataset
explores the pseudo contexts from other domain-
related reviews. Further, Expweb+sig, Expweb+exp
and Expmv/c
web+exp+com expand the pseudo contexts
from web, which can be considered as a large corpus
and can provide more evidences for the collocation
polarity disambiguation.
</bodyText>
<table confidence="0.99970925">
System P(%) R(%) F1(%)
NoExp 67.32 41.16 51.08
Expdataset 68.14 47.85 56.22
Expweb+sig 70.00 53.85 60.87
Expweb+exp 74.97 63.14 68.55
Expmv 75.53 67.83 71.47
web+exp+com
Expcweb+exp+com 74.36 69.83 72.02
</table>
<tableCaption confidence="0.92855325">
Table 4: Comparative results for the collocation polarity
disambiguation task.
Table 4 illustrates the comparative results of all
systems for collocation polarity disambiguation. It
</tableCaption>
<bodyText confidence="0.926870533333333">
can be observed that our system Expmv
web+exp+com
and Expcweb+exp+com outperform all the other sys-
tems. We discuss the experimental results as fol-
lows:
NoExp yields the worst performance, especially
on the recall. The reason is that the original con-
texts used in this system are limited, and some of
them are even noisy. In comparison, Expdataset
adds a post-processing step of expanding pseudo
contexts from the topically-related review dataset,
which achieves a better result with an absolute im-
provement of 5.14% (F1). This suggests that the
contexts expanded from other reviews are helpful in
disambiguating the collocation’s polarity.
</bodyText>
<page confidence="0.997227">
166
</page>
<bodyText confidence="0.993422424242424">
However, Expdataset is just effective in disam-
biguating the polarity of such a collocation c, which
appears many times in the domain-related reviews.
From Table 3, we can notice that this kind of collo-
cations only accounts for 40% in all the collocations,
and further they appear less than 4 times on average.
Thus, for such a collocation c, the pseudo contexts
expanded from other reviews that contain the same
c are still far from enough, since the review set size
in this system is not very large.
In order to avoid the context limitation problem,
we expand more pseudo contexts from web for each
collocation. We first try to use a simple query
form (Strategy0) for web mining. Table 4 illustrates
that the corresponding system Expweb+sig outper-
forms the system Expdataset. It can demonstrate
that our web mining based pseudo context expan-
sion is useful for disambiguating the collocation’s
polarity, since this system can explore more con-
texts. However, we can find that the performance
is not very ideal. This system can generate some
harmful contexts for the reason of the wrong mod-
ifying relations between polarity words and targets
in the retrieved snippets.
Thus this paper adds three query expansion strate-
gies to generate more and accurate pseudo con-
texts. Table 4 shows that the corresponding sys-
tem Expweb+exp can achieve a better result with F1
= 68.55%, which is significantly (x2 test with p &lt;
0.01) outperforms Expweb+sig. It demonstrates that
the query expansion strategies are useful.
Finally, Table 4 gives the results of our method in
this paper, Expmv
</bodyText>
<subsubsectionHeader confidence="0.795092">
web+exp+com and Expcweb+exp+com,
</subsubsectionHeader>
<bodyText confidence="0.942744555555556">
which combines the original and expanded pseudo
contexts to yield a final polarity. We can ob-
serve that both of these systems outperform the sys-
tem NoExp of just using the original contexts and
the system Expweb+exp of just using the expanded
pseudo contexts. This can illustrate that the two
kinds of contexts are complementary to each other.
In addition, we can also find that the two combi-
nation methods produce similar results. In detail,
mv
xpweb+exp+com
679 of them are correct; Expcweb+exp+com disam-
biguates 940 collocations, 699 of them are correct.
We can further find that, although the amount of
original contexts is small, it also plays an important
role in disambiguating the polarities of the collo-
cations that cannot be recognized by the expanded
pseudo contexts.
</bodyText>
<subsectionHeader confidence="0.9954895">
5.2 The Contributions of the Query Expansion
Strategies
</subsectionHeader>
<bodyText confidence="0.9989212">
The expanded pseudo contexts from our method can
be partly credited to the query expansion strategies.
Based on this, this section aims to analyze the differ-
ent contributions of the query expansion strategies in
our method.
</bodyText>
<table confidence="0.999751">
Strategy P(%) R(%) F1(%) Avg(#)
Strategy0 70.00 53.85 60.87 71
Strategy1 74.14 55.84 63.70 112
Strategy2 61.84 37.56 46.74 26
Strategy3 64.34 33.17 43.77 20
Expweb+exp 74.97 63.14 68.55 229
</table>
<tableCaption confidence="0.944210666666667">
Table 5: The performance of our method based on each
query expansion strategy for collocation polarity disam-
biguation.
</tableCaption>
<bodyText confidence="0.998426">
Table 5 provides the performance of our method
based on each query expansion strategy for collo-
cation polarity disambiguation. For each strategy,
“Avg” in Table 5 denotes the average number of
the expanded pseudo contexts for each collocation.
From this table, we can find that the larger the “Avg”
is, the better (F1) the strategy is. In detail, Strategy1
with the largest “Avg” has the best performance; and
Strategy3 with the fewest “Avg” has the worst per-
formance. This can further demonstrate our idea
that more and effective pseudo contexts can improve
the performance of the collocation polarity disam-
biguation task. Expweb+exp integrates all the query
expansion strategies and obtains much more “Avg”.
Therefore, this can significantly increase the recall
value, and further produce a better result. On the
other hand, the results in Table 5 show that these
heuristic query expansion strategies are effective.
</bodyText>
<subsectionHeader confidence="0.9988945">
5.3 Deep Experiments in the
Three-Component Framework
</subsectionHeader>
<bodyText confidence="0.977135333333333">
In order to do a detailed analysis into our three-
component framework, some deep experiments are
made:
Query Expansion The aim of query expansion
is to retrieve lots of relative snippets, from which
we can extract the useful pseudo contexts. For each
</bodyText>
<footnote confidence="0.242428">
disambiguates 899 collocations,
</footnote>
<page confidence="0.822277">
167
</page>
<table confidence="0.9995204">
Strategy0 Strategy1 Strategy2 Strategy3
(%) (%) (%) (%)
Query Expansion 76.75 94.50 85.50 85.25
Pseudo Context 71.25 73.50 67.50 74.50
Sentiment Analysis 63.00 68.25 59.00 69.75
</table>
<tableCaption confidence="0.9996">
Table 6: The accuracies of the query expansion, pseudo context and sentiment analysis for each strategy.
</tableCaption>
<bodyText confidence="0.996797081632653">
snippet, if the polarity word of the collocation does
modify the target, we consider this snippet as a cor-
rect query expansion result.
Pseudo Context For each expanded pseudo con-
text from web, if it shows the same sentiment ori-
entation with the collocation (or opposite with the
collocation’s polarity because of the usage of transi-
tional words), we consider this context as a correct
pseudo context.
Sentiment Analysis For each expanded pseudo
context, if its polarity can be correctly recognized
by the polarity computation method in Figure 3, and
meanwhile it shows the same sentiment orientation
with the collocation, we consider this context as a
correct one.
Table 6 illustrates the accuracy of each experi-
ment for each strategy in detail, where 400 web re-
trieved snippets for Query Expansion and 400 ex-
panded pseudo contexts for Pseudo Context and
Sentiment Analysis are randomly selected and man-
ually evaluated for each strategy.
Seen from Table 6, we can find that:
1. For Query Expansion, all strategies yield good
accuracies except for Strategy0. This can draw a
same conclusion with our analysis in Section 3.2.1.
The queries from Strategy0 are short, thus in many
retrieved snippets, there exist no modifying relations
between the polarity words and targets. Accord-
ingly, the pseudo contexts from these snippets are
incorrect. This can result in the low accuracy of
Strategy0. On the other hand, we can find that the
other three query expansion strategies perform well.
2. Although the final result of our three-
component framework is good, the accuracies of
Pseudo Context and Sentiment Analysis for each
strategy is not very high. This is perhaps caused by
unrefined work on the specific sub-stages. For ex-
ample, we get all the pseudo contexts using the al-
gorithm in Figure 2. However, in some reviews, the
two sentences before and after the target sentence
have no polarity relation with the target sentence it-
self. This can bring in some noises. On the other
hand, the context polarity computation algorithm in
Figure 3 is just a simple attempt, which is not the
best way to compute the context’s polarity.
In fact, this paper aims to try some simple algo-
rithms for each component to validate the effective-
ness of the three-component framework. We will
polish every component of our framework in future.
</bodyText>
<sectionHeader confidence="0.997489" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.923363518518519">
This paper proposes a web-based context expan-
sion framework for collocation polarity disambigua-
tion. The basic assumption of this framework is
that, if a collocation appears in different forms, both
within the same review and within topically-related
reviews, then the large amounts of pseudo contexts
from these reviews can help to disambiguate such
a collocation’s polarity. Based on this assumption,
this framework includes three independent compo-
nents. First, the heuristic query expansion strate-
gies are adopted to expand pseudo contexts from
web; then a simple but effective polarity computa-
tion method is used to recognize the polarities for
both the original contexts and the expanded pseudo
contexts; and finally, we integrate the polarities from
the original and pseudo contexts as the collocation’s
polarity. Without using any additional labeled data,
experiments on a Chinese data set from four product
domains show that the proposed framework outper-
forms other previous work.
This paper can be concluded as follows:
1. A framework including three independent com-
ponents is proposed for collocation polarity
disambiguation. We can try other different al-
gorithms for each component.
2. Web-based pseudo contexts are effective for
disambiguating a collocation’s polarity.
</bodyText>
<page confidence="0.994999">
168
</page>
<bodyText confidence="0.978883615384615">
3. The query expansion strategies are promising,
which can generate more useful and correct
contexts.
4. The initial contexts from current reviews and
the expanded contexts from web are comple-
mentary to each other.
The immediate extension of our work is to polish
each component of this framework, such as improv-
ing the accuracy of query expansion and pseudo con-
text acquisition, using other effective polarity com-
puting methods for each context and so on. In ad-
dition, we will explore other query expansion strate-
gies to generate more effective contexts.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999134777777778">
We thank the anonymous reviewers for their helpful
comments. This work was supported by National
Natural Science Foundation of China (NSFC) via
grant 61133012, the National “863” Leading Tech-
nology Research Project via grant 2012AA011102,
the Ministry of Education Research of Social Sci-
ences Youth funded projects via grant 12YJCZH304
and the Fundamental Research Funds for the Central
Universities via grant No.HIT.NSRIF.2013090.
</bodyText>
<sectionHeader confidence="0.998556" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999556671052631">
Kenneth Bloom, Navendu Garg, and Shlomo Argamon.
2007. Extracting appraisal expressions. In HLT-
NAACL 2007, pages 308–315.
D. Bollegala, D. Weir, and J. Carroll. 2011. Using mul-
tiple sources to construct a sentiment sensitive the-
saurus for cross-domain sentiment classification. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 132–141. Asso-
ciation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):37–46.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining. In
Proceedings of the Conference on Web Search and Web
Data Mining (WSDM), pages 231–240.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis. In
Proceedings of the ACM SIGIR Conference on Infor-
mation and Knowledge Management (CIKM), pages
617–624.
A. Esuli. 2008. Automatic generation of lexical re-
sources for opinion mining: models, algorithms and
applications. In ACM SIGIR Forum, volume 42, pages
105–106. ACM.
V. Hatzivassiloglou and K.R. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Proceed-
ings of the eighth conference on European chapter of
the Association for Computational Linguistics, pages
174–181. Association for Computational Linguistics.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Auto-
matically extracting polarity-bearing topics for cross-
domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 123–131, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168–177. ACM.
V. Jijkoun, M. De Rijke, and W. Weerkamp. 2010. Gen-
erating focused topic-specific sentiment lexicons. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 585–594.
Association for Computational Linguistics.
N. Kaji and M. Kitsuregawa. 2007. Building lexicon
for sentiment analysis from massive collection of html
documents. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1075–1083.
Jaap Kamps, Maarten Marx, R. ort. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings of
LREC-2004, pages 1115–1118.
H. Kanayama and T. Nasukawa. 2006. Fully auto-
matic lexicon expansion for domain-oriented senti-
ment analysis. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 355–363. Association for Computational
Linguistics.
Soo-Min Kim and Eduard Hovy. 2005. Automatic detec-
tion of opinion bearing words and sentences. In Pro-
ceedings of IJCNLP-2005, pages 61–66.
S.-M. Kim and E. Hovy. 2006. Identifying and analyz-
ing judgment opinions. In Proceedings of the Joint
Human Language Technology/North American Chap-
ter of the ACL Conference (HLT-NAACL), pages 200–
207.
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji
Tateishi, and Toshikazu Fukushima. 2004. Collecting
evaluative expressions for opinion extraction. In Pro-
ceedings of the International Joint Conference on Nat-
ural Language Processing (IJCNLP), pages 584–589.
</reference>
<page confidence="0.986087">
169
</page>
<reference confidence="0.999884606741573">
Binyang Li, Lanjun Zhou, Shi Feng, and Kam-Fai Wong.
2010. A unified graph model for sentence-based opin-
ion retrieval. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
page 1367–1375.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of WWW-2005, pages
342–351.
Y. Lu, M. Castellanos, U. Dayal, and C.X. Zhai. 2011.
Automatic construction of a context-aware sentiment
lexicon: an optimization approach. In Proceedings of
the 20th international conference on World wide web,
pages 347–356. ACM.
S. Mohammad, C. Dunne, and B. Dorr. 2009. Generat-
ing high-coverage semantic orientation lexicons from
overtly marked words and a thesaurus. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 2-Volume 2, pages
599–608. Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP-
2002, pages 79–86.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
hltemnlp2005, pages 339–346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP-2003, pages 105–112.
Ellen Riloff, Janyce Wiebe, and William Phillips. 2005.
Exploiting subjectivity classification to improve in-
formation extraction. In Proceedings of AAAI-2005,
pages 1106–1111.
Fangzhong Su and Katja Markert. 2009. Subjectivity
recognition on word senses via semi-supervised min-
cuts. In Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the ACL, pages 1–9.
Yasuhiro Suzuki, Hiroya Takamura, and Manabu Oku-
mura. 2006. Application of semi-supervised learn-
ing to evaluative expression classification. In Com-
putational Linguistics and Intelligent Text Processing,
pages 502–513.
P. Turney, M.L. Littman, et al. 2003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems (TOIS), 21(4):315–346.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In The 2010 Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 777–785.
Jan Wiebe and Rada Mihalcea. 2006. Word sense and
subjectivity. In Proceedings of the Conference on
Computational Linguistics /Association for Computa-
tional Linguistics (COLING/ACL), pages 1065–1072.
Janyce Wiebe, Eric Breck, and Chris Buckley. 2003.
Recognizing and Organizing Opinions Expressed in
the World Press. In Papers from the AAAI Spring
Symposium on New Directions in Question Answering,
pages 24–26.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of AAAI, pages 735–
740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP-
2005, pages 347–354.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: an exploration
of features for phrase-level sentiment analysis. Com-
putational Linguistics, 35(3).
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP-2003, pages 129–
136.
Min Zhang and Xingyao Ye. 2008. A generation model
to unify topic relevance and lexicon-based sentiment
for opinion retrieval. In Proceedings of the ACM Spe-
cial Interest Group on Information Retrieval (SIGIR),
pages 411–419.
Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opin-
ion retrieval from blogs. In In proceedings of CIKM,
page 831–840.
Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,
Kang Liu, and Qi Zhang. 2008. Overview of chinese
opinion analysis evaluation 2008.
</reference>
<page confidence="0.997422">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872433">
<title confidence="0.998737">Collocation Polarity Disambiguation Using Web-based Pseudo Contexts</title>
<author confidence="0.906533">Bing Qin Zhao</author>
<author confidence="0.906533">Ting</author>
<affiliation confidence="0.988983">Harbin Institute of Technology, Harbin,</affiliation>
<email confidence="0.984226">bqin,</email>
<abstract confidence="0.9988905">This paper focuses on the task of collocapolarity disambiguation. The refers to a binary tuple of a polarity word and target (such as battery in which the sentiment orientation of the polarity word (“long”) changes along with different targets (“battery life” or “startup”). To disambiguate a collocation’s polarity, previous work always turned to investigate the polarities of its surrounding contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth Bloom</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Extracting appraisal expressions. In HLTNAACL</title>
<date>2007</date>
<pages>308--315</pages>
<contexts>
<context position="12806" citStr="Bloom et al., 2007" startWordPosition="1993" endWordPosition="1996"> snippets, there also exist no modifying relations between the polarity words and targets. As a result, if we just use this query strategy, the expanded pseudo contexts are limited and cannot yield ideal performance. Therefore, we need to design some effective query expansion strategies to ensure that (1) the polarity words do modify the targets in the retrieved web snippets, and (2) the snippets are more enough. 3.2.2 Query Expansion Strategy We first investigate the modifying relations between polarity words and the targets, and then construct effective queries. Observed from previous work (Bloom et al., 2007; Kobayashi et al., 2004; Popescu and Etzioni, 2005), there are two kinds of common relations between the polarity words and their targets. One is the “subject-copula-predicate” relation, such as the relationship between “long” and “battery life” in the sentence “The battery life of this camera is long”. The other is the “attribute-head” relation, such as the relationship between them in the sentence “This camera has long battery life”. As a result, three heuristic query expansion strategies are adopted to construct efficient queries for searching. Take the collocation (长,电 池 寿 命) ((long, batt</context>
</contexts>
<marker>Bloom, Garg, Argamon, 2007</marker>
<rawString>Kenneth Bloom, Navendu Garg, and Shlomo Argamon. 2007. Extracting appraisal expressions. In HLTNAACL 2007, pages 308–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>D Weir</author>
<author>J Carroll</author>
</authors>
<title>Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>132--141</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9188" citStr="Bollegala et al., 2011" startWordPosition="1402" endWordPosition="1405">rity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of web sources to collect more useful pseudo contexts for a collocation, whose o</context>
</contexts>
<marker>Bollegala, Weir, Carroll, 2011</marker>
<rawString>D. Bollegala, D. Weir, and J. Carroll. 2011. Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 132–141. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="21917" citStr="Cohen, 1960" startWordPosition="3481" endWordPosition="3482">n the polarity tag. If the polarity of c cannot be recognized2, the majority polarity tag voted on the original contexts is chosen as the final polarity tag. 4 Experimental Setup 4.1 Dataset and Evaluation Metrics We conduct the experiments on a Chinese collocation corpus of four product domains, which is from the Task3 of the Chinese Opinion Analysis Evaluation (COAE)3 (Zhao et al., 2008). Table 3 describes the corpus in detail. From 478 reviews, 1,001 collocations (454 positive and 547 negative) with polarity-ambiguous words are found and manually annotated by two annotators. Cohen’s kappa (Cohen, 1960), a measure of inter-annotator agreement ranging from zero to one, is 0.83, indicating a good strength of agreement 4. In Table 3, Sig of the fourth column denotes the collocations that appear once in all the domainrelated reviews. And multiple in the last column denotes the collocations that appear several times. From Table 3, we can find that among all the reviews, nearly 60% collocations only appear once. Even for the multiple collocations, they averagely appear less than 4 times. Therefore, for a collocation, if we only consider its original contexts alone or the expanded pseudo contexts f</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A holistic lexicon-based approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Web Search and Web Data Mining (WSDM),</booktitle>
<pages>231--240</pages>
<contexts>
<context position="5003" citStr="Ding et al., 2008" startWordPosition="753" endWordPosition="756">u, 2004; Kanayama and Nasukawa, 2006). However, since the amount of contexts from the original review is very limited, the final resulting polarity for the collocation c is insufficient to be reliable. Fortunately, most collocations may appear multiple times, in different forms, both within the same review and within topically-related reviews. Thus for a collocation, we can collect large amounts of contexts from other reviews to improve its polarity disambiguation. These expanded contexts are called pseudo contexts in this paper. Some previous work used the similar methods. For example, Ding (Ding et al., 2008) expanded some pseudo contexts from a topically-related review set. But since the review set is limited, the expanded contexts are still limited and unreliable. In order to overcome this problem, we propose an unsupervised three-component framework to expand more pseudo contexts from web for the collocation polarity disambiguation. Without using any labeled data, experiments on a Chinese data set from four product domains show that the three-component framework is feasible and the web-based pseudo contexts are useful for the collocation polarity disambiguation. Compared to other previous work,</context>
<context position="8595" citStr="Ding et al., 2008" startWordPosition="1306" endWordPosition="1309">uisition Combination Sentiment analysis Sentiment analysis end Pos/Neg took into account many contextual information of the word within the sentence, such as exclamation words, emoticons and so on. However, the experimental results show that these in-sentence features are not rich enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to disambiguate its polarity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And s</context>
<context position="24048" citStr="Ding et al., 2008" startWordPosition="3821" endWordPosition="3824"> previous work, we build several systems as follows: NoExp: Following the method proposed by Hu (Hu and Liu, 2004), without using the expanded pseudo contexts, we only consider the two original contexts Senbef and Senaft of a collocation c in the current review. If Senbef expresses the polarity polar, then Polarity(ac) = polar. Else if Senaft expresses the polarity polar′, then Polarity(ac) = polar′. Else, this method cannot disambiguate the polarity of c. In this method, the transitional words, such as “但是” (“but” in English) are considered. Expdataset: Following the method proposed by Ding (Ding et al., 2008), we solve this task with the help of the pseudo contexts in the domain-related review dataset. For a collocation c appearing in many domain-related reviews, this method refers to the polarities of the same c in other reviews. The majority polarity is chosen as final polarity. Expweb+sig: This method is the same as our method in this paper, except for (1) not combining the original contexts, and (2) not using all the three query expansion strategies, but just using the single (abbv. sig) Strategy0. This method expands the pseudo contexts from the web. The majority polarity is chosen as the fin</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the Conference on Web Search and Web Data Mining (WSDM), pages 231–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>Determining the semantic orientation of terms through gloss analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACM SIGIR Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>617--624</pages>
<contexts>
<context position="6783" citStr="Esuli and Sebastiani, 2005" startWordPosition="1032" endWordPosition="1035"> basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may indicate different polarities depending on what targets it is applied to, especially for the polarity-ambiguous words, such as “-K_” (“long” in English) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocations mentioned in this paper,</context>
</contexts>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>A. Esuli and F. Sebastiani. 2005. Determining the semantic orientation of terms through gloss analysis. In Proceedings of the ACM SIGIR Conference on Information and Knowledge Management (CIKM), pages 617–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
</authors>
<title>Automatic generation of lexical resources for opinion mining: models, algorithms and applications. In</title>
<date>2008</date>
<journal>ACM SIGIR Forum,</journal>
<volume>42</volume>
<pages>105--106</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2161" citStr="Esuli, 2008" startWordPosition="324" endWordPosition="325">timent classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. 1.该相机的[电池寿命]t很[长]p。(Positive) Translated as: The [battery life]t of this camera is [long]p. (Positive) 2. 该相机的[启动时间]t很[长]p。(Negative) Translated as: This camera has [long]p [startup]t. (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript</context>
</contexts>
<marker>Esuli, 2008</marker>
<rawString>A. Esuli. 2008. Automatic generation of lexical resources for opinion mining: models, algorithms and applications. In ACM SIGIR Forum, volume 42, pages 105–106. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>K R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2127" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="316" endWordPosition="319">ch as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. 1.该相机的[电池寿命]t很[长]p。(Positive) Translated as: The [battery life]t of this camera is [long]p. (Positive) 2. 该相机的[启动时间]t很[长]p。(Negative) Translated as: This camera has [long]p [startup]t. (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the</context>
<context position="4374" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="654" endWordPosition="658">861 common binary tuples of polarity words and their modified targets from 478 reviews l, and find that over 20% of them are the collocations defined in this paper. Therefore, the task of collocation polarity disambiguation is worthy of study. For a sentence s containing such a collocation c, since the in-sentence features are always ambiguous, it is difficult to disambiguate the polarity of c by using them. Thus some previous work turned to investigate its surrounding contexts’ polarities (such as the sentences before or after s), and then assigned the majority polarity to the collocation c (Hatzivassiloglou and McKeown, 1997; Hu and Liu, 2004; Kanayama and Nasukawa, 2006). However, since the amount of contexts from the original review is very limited, the final resulting polarity for the collocation c is insufficient to be reliable. Fortunately, most collocations may appear multiple times, in different forms, both within the same review and within topically-related reviews. Thus for a collocation, we can collect large amounts of contexts from other reviews to improve its polarity disambiguation. These expanded contexts are called pseudo contexts in this paper. Some previous work used the similar methods. For exam</context>
<context position="6439" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="976" endWordPosition="979"> the related work. Section 3 shows the proposed approach including three independent components. Section 4 and 5 presents the experiments and results. Finally we conclude this paper in Section 6. 2 Related Work The key of the collocation polarity disambiguation task is to recognize the polarity word’s sentiment orientation of a collocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, </context>
<context position="7702" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="1177" endWordPosition="1180">ord may indicate different polarities depending on what targets it is applied to, especially for the polarity-ambiguous words, such as “-K_” (“long” in English) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocations mentioned in this paper, rather than only the polarity words. To date, the task in this paper is similar with much previous work. Some researchers exploited the features of the sentences containing collocations to help disambiguate the polarity of the polarity-ambiguous word. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. Suzuki (Suzuki et al., 2006) 161 start A Chinese collocation in a review Query expansion Searching Web snippets Original context acquisition Pseudo context acquisition Combination Sentiment analysis Sentiment analysis end Pos/Neg took into account many contextual information of the word within the sentence, such as exclamation words, emoticons and so on. However, the experimental results show that these in-sentence features are not rich enough. Instead of considering the current </context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>V. Hatzivassiloglou and K.R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, pages 174–181. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Chenghua Lin</author>
<author>Harith Alani</author>
</authors>
<title>Automatically extracting polarity-bearing topics for crossdomain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>123--131</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1692" citStr="He et al., 2011" startWordPosition="247" endWordPosition="250">rs, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflec</context>
</contexts>
<marker>He, Lin, Alani, 2011</marker>
<rawString>Yulan He, Chenghua Lin, and Harith Alani. 2011. Automatically extracting polarity-bearing topics for crossdomain sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 123–131, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1388" citStr="Hu and Liu, 2004" startWordPosition="201" endWordPosition="204">ity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 1 Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed severa</context>
<context position="4392" citStr="Hu and Liu, 2004" startWordPosition="659" endWordPosition="662"> words and their modified targets from 478 reviews l, and find that over 20% of them are the collocations defined in this paper. Therefore, the task of collocation polarity disambiguation is worthy of study. For a sentence s containing such a collocation c, since the in-sentence features are always ambiguous, it is difficult to disambiguate the polarity of c by using them. Thus some previous work turned to investigate its surrounding contexts’ polarities (such as the sentences before or after s), and then assigned the majority polarity to the collocation c (Hatzivassiloglou and McKeown, 1997; Hu and Liu, 2004; Kanayama and Nasukawa, 2006). However, since the amount of contexts from the original review is very limited, the final resulting polarity for the collocation c is insufficient to be reliable. Fortunately, most collocations may appear multiple times, in different forms, both within the same review and within topically-related reviews. Thus for a collocation, we can collect large amounts of contexts from other reviews to improve its polarity disambiguation. These expanded contexts are called pseudo contexts in this paper. Some previous work used the similar methods. For example, Ding (Ding et</context>
<context position="8493" citStr="Hu and Liu, 2004" startWordPosition="1292" endWordPosition="1295">on in a review Query expansion Searching Web snippets Original context acquisition Pseudo context acquisition Combination Sentiment analysis Sentiment analysis end Pos/Neg took into account many contextual information of the word within the sentence, such as exclamation words, emoticons and so on. However, the experimental results show that these in-sentence features are not rich enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to disambiguate its polarity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in t</context>
<context position="17453" citStr="Hu and Liu, 2004" startWordPosition="2699" endWordPosition="2702">e the current review containing c can be considered as only one snippet in pseudo context acquisition. Thus, we can just carry out the two steps in (2) of Figure 2 to obtain the original contexts. Analyzing either the pseudo contexts or the original contexts, we can find that not all of them are useful contexts. Thus we will simply filter the noisy ones by context sentiment computation, and choose the contexts showing sentiment orientations as the useful contexts. 3.3 Sentiment Analysis For both the original and expanded pseudo contexts, we employ the lexicon-based sentiment computing method (Hu and Liu, 2004) to compute the polarity value for each context. This unsupervised approach is quite straightforward and makes use of the sentiment lexicons in Table 2. The polarity value Polarity(con) for a context con Algorithm: Pseudo Context Expansion Algorithm Input: A collocation c and the URL list Output: The pseudo context set Conx(c) 1. Use Strategy0-3 to expand c and the expanded queries are saved as a set Query(c). 2. For any query q Query(c), ∈ acquire its pseudo contexts Conx(q) as follows: (1) search q in the domain-related URL list, the top 100 retrieved snippets for each URL are collected as S</context>
<context position="23544" citStr="Hu and Liu, 2004" startWordPosition="3741" endWordPosition="3744">l number of collocations are still difficult to be disambiguated from contexts. Polarity (w) 165 is the sparse collocations, which obtain no effective contexts. The other is the collocations that acquire the same amount of positive and negative contexts. The metrics are defined as follows. correctly disambiguated collocations P = (2) disambiguated collocations R = correctly disambiguated collocations (3) all collocations 2P R F1 = (4) P � R 4.2 System Description In order to compare our method with previous work, we build several systems as follows: NoExp: Following the method proposed by Hu (Hu and Liu, 2004), without using the expanded pseudo contexts, we only consider the two original contexts Senbef and Senaft of a collocation c in the current review. If Senbef expresses the polarity polar, then Polarity(ac) = polar. Else if Senaft expresses the polarity polar′, then Polarity(ac) = polar′. Else, this method cannot disambiguate the polarity of c. In this method, the transitional words, such as “但是” (“but” in English) are considered. Expdataset: Following the method proposed by Ding (Ding et al., 2008), we solve this task with the help of the pseudo contexts in the domain-related review dataset. </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Jijkoun</author>
<author>M De Rijke</author>
<author>W Weerkamp</author>
</authors>
<title>Generating focused topic-specific sentiment lexicons.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>585--594</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Jijkoun, De Rijke, Weerkamp, 2010</marker>
<rawString>V. Jijkoun, M. De Rijke, and W. Weerkamp. 2010. Generating focused topic-specific sentiment lexicons. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 585–594. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kaji</author>
<author>M Kitsuregawa</author>
</authors>
<title>Building lexicon for sentiment analysis from massive collection of html documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1075--1083</pages>
<contexts>
<context position="6525" citStr="Kaji and Kitsuregawa, 2007" startWordPosition="990" endWordPosition="993">s. Section 4 and 5 presents the experiments and results. Finally we conclude this paper in Section 6. 2 Related Work The key of the collocation polarity disambiguation task is to recognize the polarity word’s sentiment orientation of a collocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may indicate different polarities depending on what ta</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>N. Kaji and M. Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of html documents. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1075–1083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mokken</author>
<author>Maarten de Rijke</author>
</authors>
<title>Using wordnet to measure semantic orientation of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC-2004,</booktitle>
<pages>1115--1118</pages>
<marker>Mokken, de Rijke, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, R. ort. Mokken, and Maarten de Rijke. 2004. Using wordnet to measure semantic orientation of adjectives. In Proceedings of LREC-2004, pages 1115–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kanayama</author>
<author>T Nasukawa</author>
</authors>
<title>Fully automatic lexicon expansion for domain-oriented sentiment analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--363</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4422" citStr="Kanayama and Nasukawa, 2006" startWordPosition="663" endWordPosition="666">odified targets from 478 reviews l, and find that over 20% of them are the collocations defined in this paper. Therefore, the task of collocation polarity disambiguation is worthy of study. For a sentence s containing such a collocation c, since the in-sentence features are always ambiguous, it is difficult to disambiguate the polarity of c by using them. Thus some previous work turned to investigate its surrounding contexts’ polarities (such as the sentences before or after s), and then assigned the majority polarity to the collocation c (Hatzivassiloglou and McKeown, 1997; Hu and Liu, 2004; Kanayama and Nasukawa, 2006). However, since the amount of contexts from the original review is very limited, the final resulting polarity for the collocation c is insufficient to be reliable. Fortunately, most collocations may appear multiple times, in different forms, both within the same review and within topically-related reviews. Thus for a collocation, we can collect large amounts of contexts from other reviews to improve its polarity disambiguation. These expanded contexts are called pseudo contexts in this paper. Some previous work used the similar methods. For example, Ding (Ding et al., 2008) expanded some pseu</context>
<context position="7745" citStr="Kanayama and Nasukawa, 2006" startWordPosition="1183" endWordPosition="1186"> what targets it is applied to, especially for the polarity-ambiguous words, such as “-K_” (“long” in English) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocations mentioned in this paper, rather than only the polarity words. To date, the task in this paper is similar with much previous work. Some researchers exploited the features of the sentences containing collocations to help disambiguate the polarity of the polarity-ambiguous word. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. Suzuki (Suzuki et al., 2006) 161 start A Chinese collocation in a review Query expansion Searching Web snippets Original context acquisition Pseudo context acquisition Combination Sentiment analysis Sentiment analysis end Pos/Neg took into account many contextual information of the word within the sentence, such as exclamation words, emoticons and so on. However, the experimental results show that these in-sentence features are not rich enough. Instead of considering the current sentence alone, some researchers exploited </context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>H. Kanayama and T. Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 355–363. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic detection of opinion bearing words and sentences.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP-2005,</booktitle>
<pages>61--66</pages>
<contexts>
<context position="1610" citStr="Kim and Hovy, 2005" startWordPosition="236" endWordPosition="239">ed data, experiments show that our method is effective. 1 Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010)</context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2005. Automatic detection of opinion bearing words and sentences. In Proceedings of IJCNLP-2005, pages 61–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-M Kim</author>
<author>E Hovy</author>
</authors>
<title>Identifying and analyzing judgment opinions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Human Language Technology/North American Chapter of the ACL Conference (HLT-NAACL),</booktitle>
<pages>200--207</pages>
<contexts>
<context position="6755" citStr="Kim and Hovy, 2006" startWordPosition="1028" endWordPosition="1031">llocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may indicate different polarities depending on what targets it is applied to, especially for the polarity-ambiguous words, such as “-K_” (“long” in English) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocati</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>S.-M. Kim and E. Hovy. 2006. Identifying and analyzing judgment opinions. In Proceedings of the Joint Human Language Technology/North American Chapter of the ACL Conference (HLT-NAACL), pages 200– 207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
<author>Kenji Tateishi</author>
<author>Toshikazu Fukushima</author>
</authors>
<title>Collecting evaluative expressions for opinion extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>584--589</pages>
<contexts>
<context position="12830" citStr="Kobayashi et al., 2004" startWordPosition="1997" endWordPosition="2000">o exist no modifying relations between the polarity words and targets. As a result, if we just use this query strategy, the expanded pseudo contexts are limited and cannot yield ideal performance. Therefore, we need to design some effective query expansion strategies to ensure that (1) the polarity words do modify the targets in the retrieved web snippets, and (2) the snippets are more enough. 3.2.2 Query Expansion Strategy We first investigate the modifying relations between polarity words and the targets, and then construct effective queries. Observed from previous work (Bloom et al., 2007; Kobayashi et al., 2004; Popescu and Etzioni, 2005), there are two kinds of common relations between the polarity words and their targets. One is the “subject-copula-predicate” relation, such as the relationship between “long” and “battery life” in the sentence “The battery life of this camera is long”. The other is the “attribute-head” relation, such as the relationship between them in the sentence “This camera has long battery life”. As a result, three heuristic query expansion strategies are adopted to construct efficient queries for searching. Take the collocation (长,电 池 寿 命) ((long, battery life) in English) as</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, Tateishi, Fukushima, 2004</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji Tateishi, and Toshikazu Fukushima. 2004. Collecting evaluative expressions for opinion extraction. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP), pages 584–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binyang Li</author>
<author>Lanjun Zhou</author>
<author>Shi Feng</author>
<author>Kam-Fai Wong</author>
</authors>
<title>A unified graph model for sentence-based opinion retrieval.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1367--1375</pages>
<contexts>
<context position="1769" citStr="Li et al., 2010" startWordPosition="262" endWordPosition="265">used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambi</context>
</contexts>
<marker>Li, Zhou, Feng, Wong, 2010</marker>
<rawString>Binyang Li, Lanjun Zhou, Shi Feng, and Kam-Fai Wong. 2010. A unified graph model for sentence-based opinion retrieval. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, page 1367–1375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of WWW-2005,</booktitle>
<pages>342--351</pages>
<contexts>
<context position="1407" citStr="Liu et al., 2005" startWordPosition="205" endWordPosition="208">t to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 1 Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for th</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Proceedings of WWW-2005, pages 342–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lu</author>
<author>M Castellanos</author>
<author>U Dayal</author>
<author>C X Zhai</author>
</authors>
<title>Automatic construction of a context-aware sentiment lexicon: an optimization approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web,</booktitle>
<pages>347--356</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9221" citStr="Lu et al., 2011" startWordPosition="1409" endWordPosition="1412">a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of web sources to collect more useful pseudo contexts for a collocation, whose original contexts are limited or u</context>
</contexts>
<marker>Lu, Castellanos, Dayal, Zhai, 2011</marker>
<rawString>Y. Lu, M. Castellanos, U. Dayal, and C.X. Zhai. 2011. Automatic construction of a context-aware sentiment lexicon: an optimization approach. In Proceedings of the 20th international conference on World wide web, pages 347–356. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>C Dunne</author>
<author>B Dorr</author>
</authors>
<title>Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>599--608</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2184" citStr="Mohammad et al., 2009" startWordPosition="326" endWordPosition="329">fication (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. 1.该相机的[电池寿命]t很[长]p。(Positive) Translated as: The [battery life]t of this camera is [long]p. (Positive) 2. 该相机的[启动时间]t很[长]p。(Negative) Translated as: This camera has [long]p [startup]t. (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified b</context>
</contexts>
<marker>Mohammad, Dunne, Dorr, 2009</marker>
<rawString>S. Mohammad, C. Dunne, and B. Dorr. 2009. Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 599–608. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP2002,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1590" citStr="Pang et al., 2002" startWordPosition="232" endWordPosition="235">ny additional labeled data, experiments show that our method is effective. 1 Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Veli</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMNLP2002, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In hltemnlp2005,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="12858" citStr="Popescu and Etzioni, 2005" startWordPosition="2001" endWordPosition="2004">ations between the polarity words and targets. As a result, if we just use this query strategy, the expanded pseudo contexts are limited and cannot yield ideal performance. Therefore, we need to design some effective query expansion strategies to ensure that (1) the polarity words do modify the targets in the retrieved web snippets, and (2) the snippets are more enough. 3.2.2 Query Expansion Strategy We first investigate the modifying relations between polarity words and the targets, and then construct effective queries. Observed from previous work (Bloom et al., 2007; Kobayashi et al., 2004; Popescu and Etzioni, 2005), there are two kinds of common relations between the polarity words and their targets. One is the “subject-copula-predicate” relation, such as the relationship between “long” and “battery life” in the sentence “The battery life of this camera is long”. The other is the “attribute-head” relation, such as the relationship between them in the sentence “This camera has long battery life”. As a result, three heuristic query expansion strategies are adopted to construct efficient queries for searching. Take the collocation (长,电 池 寿 命) ((long, battery life) in English) as an example, the strategies </context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In hltemnlp2005, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="1545" citStr="Riloff and Wiebe, 2003" startWordPosition="225" endWordPosition="228">sambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 1 Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2</context>
<context position="6476" citStr="Riloff and Wiebe, 2003" startWordPosition="982" endWordPosition="985">pproach including three independent components. Section 4 and 5 presents the experiments and results. Finally we conclude this paper in Section 6. 2 Related Work The key of the collocation polarity disambiguation task is to recognize the polarity word’s sentiment orientation of a collocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may i</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceedings of EMNLP-2003, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Janyce Wiebe</author>
<author>William Phillips</author>
</authors>
<title>Exploiting subjectivity classification to improve information extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI-2005,</booktitle>
<pages>1106--1111</pages>
<contexts>
<context position="1335" citStr="Riloff et al., 2005" startWordPosition="193" endWordPosition="196">ver, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 1 Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly</context>
</contexts>
<marker>Riloff, Wiebe, Phillips, 2005</marker>
<rawString>Ellen Riloff, Janyce Wiebe, and William Phillips. 2005. Exploiting subjectivity classification to improve information extraction. In Proceedings of AAAI-2005, pages 1106–1111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangzhong Su</author>
<author>Katja Markert</author>
</authors>
<title>Subjectivity recognition on word senses via semi-supervised mincuts.</title>
<date>2009</date>
<booktitle>In Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="9573" citStr="Su and Markert, 2009" startWordPosition="1464" endWordPosition="1467">pora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of web sources to collect more useful pseudo contexts for a collocation, whose original contexts are limited or unreliable. The framework of our approach is illustrated in Figure 1. In order to disambiguate a collocation’s polarity, three components are carried out: 1. Query Expansion and Pseudo Context Acquisition: This paper uses the collocation as query. For a collocation, three heuristic query expansion strategies are used to generate more flexible queries,</context>
</contexts>
<marker>Su, Markert, 2009</marker>
<rawString>Fangzhong Su and Katja Markert. 2009. Subjectivity recognition on word senses via semi-supervised mincuts. In Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhiro Suzuki</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Application of semi-supervised learning to evaluative expression classification.</title>
<date>2006</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>502--513</pages>
<contexts>
<context position="7846" citStr="Suzuki et al., 2006" startWordPosition="1199" endWordPosition="1202">) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocations mentioned in this paper, rather than only the polarity words. To date, the task in this paper is similar with much previous work. Some researchers exploited the features of the sentences containing collocations to help disambiguate the polarity of the polarity-ambiguous word. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. Suzuki (Suzuki et al., 2006) 161 start A Chinese collocation in a review Query expansion Searching Web snippets Original context acquisition Pseudo context acquisition Combination Sentiment analysis Sentiment analysis end Pos/Neg took into account many contextual information of the word within the sentence, such as exclamation words, emoticons and so on. However, the experimental results show that these in-sentence features are not rich enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s pol</context>
</contexts>
<marker>Suzuki, Takamura, Okumura, 2006</marker>
<rawString>Yasuhiro Suzuki, Hiroya Takamura, and Manabu Okumura. 2006. Application of semi-supervised learning to evaluative expression classification. In Computational Linguistics and Intelligent Text Processing, pages 502–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
<author>M L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>21</volume>
<issue>4</issue>
<marker>Turney, Littman, 2003</marker>
<rawString>P. Turney, M.L. Littman, et al. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems (TOIS), 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of webderived polarity lexicons.</title>
<date>2010</date>
<booktitle>In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>777--785</pages>
<contexts>
<context position="2210" citStr="Velikovich et al., 2010" startWordPosition="330" endWordPosition="333">2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. 1.该相机的[电池寿命]t很[长]p。(Positive) Translated as: The [battery life]t of this camera is [long]p. (Positive) 2. 该相机的[启动时间]t很[长]p。(Negative) Translated as: This camera has [long]p [startup]t. (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by the polarity words. In t</context>
<context position="6551" citStr="Velikovich et al., 2010" startWordPosition="994" endWordPosition="997">the experiments and results. Finally we conclude this paper in Section 6. 2 Related Work The key of the collocation polarity disambiguation task is to recognize the polarity word’s sentiment orientation of a collocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may indicate different polarities depending on what targets it is applied to, es</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of webderived polarity lexicons. In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 777–785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics /Association for Computational Linguistics (COLING/ACL),</booktitle>
<pages>1065--1072</pages>
<contexts>
<context position="9550" citStr="Wiebe and Mihalcea, 2006" startWordPosition="1460" endWordPosition="1463">h need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of web sources to collect more useful pseudo contexts for a collocation, whose original contexts are limited or unreliable. The framework of our approach is illustrated in Figure 1. In order to disambiguate a collocation’s polarity, three components are carried out: 1. Query Expansion and Pseudo Context Acquisition: This paper uses the collocation as query. For a collocation, three heuristic query expansion strategies are used to generate</context>
</contexts>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Jan Wiebe and Rada Mihalcea. 2006. Word sense and subjectivity. In Proceedings of the Conference on Computational Linguistics /Association for Computational Linguistics (COLING/ACL), pages 1065–1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Eric Breck</author>
<author>Chris Buckley</author>
</authors>
<title>Recognizing and Organizing Opinions Expressed in the World Press.</title>
<date>2003</date>
<booktitle>In Papers from the AAAI Spring Symposium on New Directions in Question Answering,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="1257" citStr="Wiebe et al., 2003" startWordPosition="182" endWordPosition="185">g contexts, and then assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 1 Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic ori</context>
</contexts>
<marker>Wiebe, Breck, Buckley, 2003</marker>
<rawString>Janyce Wiebe, Eric Breck, and Chris Buckley. 2003. Recognizing and Organizing Opinions Expressed in the World Press. In Papers from the AAAI Spring Symposium on New Directions in Question Answering, pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<title>Learning subjective adjectives from corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>735--740</pages>
<contexts>
<context position="6452" citStr="Wiebe, 2000" startWordPosition="980" endWordPosition="981">he proposed approach including three independent components. Section 4 and 5 presents the experiments and results. Finally we conclude this paper in Section 6. 2 Related Work The key of the collocation polarity disambiguation task is to recognize the polarity word’s sentiment orientation of a collocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the s</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In Proceedings of AAAI, pages 735– 740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP2005,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="8856" citStr="Wilson et al., 2005" startWordPosition="1347" endWordPosition="1350">features are not rich enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to disambiguate its polarity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguati</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of HLT/EMNLP2005, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: an exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="1674" citStr="Wilson et al., 2009" startWordPosition="243" endWordPosition="246">duction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can </context>
<context position="8878" citStr="Wilson et al., 2009" startWordPosition="1351" endWordPosition="1354"> enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to disambiguate its polarity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to clas</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: an exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003,</booktitle>
<pages>129--136</pages>
<contexts>
<context position="1289" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="186" endWordPosition="189"> assigned the majority polarity to the collocation. However, these contexts are limited, thus the resulting polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 1 Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example,</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP-2003, pages 129– 136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Xingyao Ye</author>
</authors>
<title>A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACM Special Interest Group on Information Retrieval (SIGIR),</booktitle>
<pages>411--419</pages>
<contexts>
<context position="1751" citStr="Zhang and Ye, 2008" startWordPosition="258" endWordPosition="261"> it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typ</context>
</contexts>
<marker>Zhang, Ye, 2008</marker>
<rawString>Min Zhang and Xingyao Ye. 2008. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In Proceedings of the ACM Special Interest Group on Information Retrieval (SIGIR), pages 411–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Zhang</author>
<author>Clement Yu</author>
<author>Weiyi Meng</author>
</authors>
<title>Opinion retrieval from blogs.</title>
<date>2007</date>
<booktitle>In In proceedings of CIKM,</booktitle>
<pages>831--840</pages>
<contexts>
<context position="1731" citStr="Zhang et al., 2007" startWordPosition="254" endWordPosition="257">entiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; *Correspondence author: tliu@ir.hit.edu.cn Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with diffe</context>
</contexts>
<marker>Zhang, Yu, Meng, 2007</marker>
<rawString>Wei Zhang, Clement Yu, and Weiyi Meng. 2007. Opinion retrieval from blogs. In In proceedings of CIKM, page 831–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhao</author>
</authors>
<title>Hongbo Xu, Xuanjing Huang, Songbo Tan,</title>
<date>2008</date>
<location>Kang</location>
<marker>Zhao, 2008</marker>
<rawString>Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan, Kang Liu, and Qi Zhang. 2008. Overview of chinese opinion analysis evaluation 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
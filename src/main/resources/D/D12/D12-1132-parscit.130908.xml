<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999658">
Unified Dependency Parsing of Chinese Morphological
and Syntactic Structures
</title>
<author confidence="0.996491">
Zhongguo Li Guodong Zhou
</author>
<affiliation confidence="0.833939666666667">
Natural Language Processing Laboratory
School of Computer Science and Technology
Soochow University, Suzhou, Jiangsu Province 215006, China
</affiliation>
<email confidence="0.997707">
{lzg, gdzhou}@suda.edu.cn
</email>
<sectionHeader confidence="0.995619" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816285714286">
Most previous approaches to syntactic pars-
ing of Chinese rely on a preprocessing step
of word segmentation, thereby assuming there
was a clearly defined boundary between mor-
phology and syntax in Chinese. We show
how this assumption can fail badly, leading
to many out-of-vocabulary words and incom-
patible annotations. Hence in practice the
strict separation of morphology and syntax in
the Chinese language proves to be untenable.
We present a unified dependency parsing ap-
proach for Chinese which takes unsegmented
sentences as input and outputs both morpho-
logical and syntactic structures with a single
model and algorithm. By removing the inter-
mediate word segmentation, the unified parser
no longer needs separate notions for words
and phrases. Evaluation proves the effective-
ness of the unified model and algorithm in
parsing structures of words, phrases and sen-
tences simultaneously. 1
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999666">
The formulation of the concept of words has baf-
fled linguists from ancient to modern times (Hock-
ett, 1969). Things are even worse for Chinese, partly
due to the fact that its written form does not delimit
words explicitly. While we have no doubt that there
are linguistic units which are definitely words (or
phrases, for that matter), it’s a sad truth that in many
cases we cannot manage to draw such a clear bound-
ary between morphology and syntax, for which we
now give two arguments.
</bodyText>
<footnote confidence="0.826178">
1Corresponding author is Guodong Zhou.
</footnote>
<bodyText confidence="0.999750147058824">
The first argument is that many sub-word linguis-
tic units (such as suffixes and prefixes) are so pro-
ductive that they can lead to a huge number of out-
of-vocabulary words for natural language process-
ing systems. This phenomenon brings us into an
awkward situation if we adhere to a rigid separa-
tion of morphology and syntax. Consider charac-
ter * ‘someone’ as an example. On the one hand,
there is strong evidence that it’s not a word as it can
never be used alone. On the other hand, taking it as a
mere suffix leads to many out-of-vocabulary words
because of the productivity of such characters. For
instance, Penn Chinese Treebank (CTB6) contains
失败* ‘one that fails’ as a word but not 成功*
‘one that succeeds’, even with the word 成功 ‘suc-
ceed’ appearing 207 times. We call words like 成
功* ‘one that succeeds’ pseudo OOVs. By defini-
tion, pseudo OOVs are OOVs since they do not occur
in the training corpus, though their components are
frequently-seen words. Our estimation is that over
60% of OOVs in Chinese are of this kind (Section 2).
Of course, the way out of this dilemma is to parse
the internal structures of these words. That is to
say, we can still regard characters like * as suf-
fixes, taking into account the fact that they cannot be
used alone. Meanwhile, pseudo OOVs can be largely
eliminated through analyzing their structures, thus
greatly facilitating syntactic and semantic analysis
of sentences. In fact, previous studies have revealed
other good reasons for parsing internal structures of
words (Zhao, 2009; Li, 2011).
The second argument is that in Chinese many lin-
guistic units can form both words and phrases with
exactly the same meaning and part-of-speech, which
</bodyText>
<page confidence="0.930208">
1445
</page>
<note confidence="0.855224">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1445–1454, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
环境 NN *护 NN 法 NN 刑 NN 法 NN
</note>
<figureCaption confidence="0.85994">
Figure 1: Unified parsing of words and phrases.
</figureCaption>
<note confidence="0.549387555555556">
MOD
MOD
MOD
corpus OOV pseudo percent
CTB6 158 112 70.9
MSR 1,783 1,307 73.3
PKU 2,860 1,836 64.2
AS 3,020 2,143 71.0
CITYU 1,665 1,100 66.0
</note>
<bodyText confidence="0.998592146341463">
causes lots of incompatible annotations in currently
available corpora. Take character 法 ‘law’ as an ex-
ample. It is head of both 刑法 ‘criminal law’ and 环
境*护法 ‘environmental protection law’, but CTB
treat it as a suffix in the former (with the annotation
being 刑法_NN) and a word in the later (the anno-
tation is 环境_NN *护_NN 法_NN). These annota-
tions are incompatible since in both cases the char-
acter 法 ‘law’ bears exactly the same meaning and
usage (e.g. part-of-speech). We examined several
widely used corpora and found that about 90% of
affixes were annotated incompatibly (Section 2). In-
compatibility can be avoided through parsing struc-
tures of both words and phrases. Figure 1 conveys
this idea. A further benefit of unified parsing is to
reduce data sparseness. As an example, in CTB6 3
‘machine’ appears twice in phrases but 377 times in
words (e.g. ba速3 ‘accelerator’). Word structures
in Chinese can be excellent guide for parsing phrase
structures, and vice versa, due to their similarity.
The present paper makes two contributions in
light of these issues. Firstly, in order to get rid of
pseudo OOVs and incompatible annotations, we have
annotated structures of words in CTB6, after which
statistical models can learn structures of words as
well as phrases from the augmented treebank (Sec-
tion 4). Although previous authors have noticed
the importance of word-structure parsing (Li, 2011;
Zhao, 2009), no detailed description about annota-
tion of word structures has been provided in the liter-
ature. Secondly, we designed a unified dependency
parser whose input is unsegmented sentences and
its output incorporates both morphological and syn-
tactic structures with a single model and algorithm
(Section 5). By removing the intermediate step of
word segmentation, our unified parser no longer de-
pends on the unsound notion that there is a clear
boundary between words and phrases. Evaluation
(Section 6) shows that our unified parser achieves
satisfactory accuracies in parsing both morphologi-
cal and syntactic structures.
</bodyText>
<tableCaption confidence="0.998769">
Table 1: Statistics of pseudo OOVs for five corpora.
</tableCaption>
<sectionHeader confidence="0.9154375" genericHeader="introduction">
2 Pseudo OOVs and Incompatible
Annotations
</sectionHeader>
<bodyText confidence="0.999161724137931">
In this section we show the surprisingly pervasive
nature of pseudo OOVs and incompatible annota-
tions through analysis of five segmented corpora,
which are CTB6 and corpus by MSR, PKU, AS
and CITYU provided in SIGHAN word segmentation
Bakeoffs 2.
First we use the standard split of training and test-
ing data and extract all OOVs for each corpus, then
count the number of pseudo OOVs. Table 1 gives the
result. It’s amazing that for every corpus, over 60%
of OOVs are pseudo, meaning they can be avoided if
their internal structures were parsed. Reduction of
OOVs at such a large scale can benefit greatly down-
stream natural language processing systems.
We then sample 200 word types containing a pro-
ductive affix from each corpus, and check whether
the affix also occurs somewhere else in a phrase,
i.e, the affix is annotated as a word in the phrase.
The results are in Table 2. It’s clear and somewhat
shocking that most affixes are annotated incompat-
ibly. We believe it is not the annotators to blame,
rather the root cause lies deeply in the unique char-
acteristics of the Chinese language. This becomes
obvious in comparison with English, where suffix
like ‘-ism’ in ‘capitalism’ cannot be used alone as a
word in phrases. 3 Incompatible annotations can
be removed only through unified parsing of word
and phrase structures, as mentioned earlier and il-
lustrated in Figure 1.
</bodyText>
<footnote confidence="0.990549428571429">
2http://www.sighan.org/bakeoff2005/
3Actually English allows examples like “pre- and post-war
imperialism” where a prefix like “pre” can appear on its own as
long as the hyphen is present and it is in a coordination struc-
ture. Note that such examples are much rarer than what we
discuss in this paper for Chinese. We thank the reviewer very
much for pointing this out and providing this example for us.
</footnote>
<page confidence="0.980069">
1446
</page>
<table confidence="0.956632">
corpus incompatible percent
CTB6 190 95
MSR 178 89
PKU 192 96
AS 182 91
CITYU 194 97
</table>
<tableCaption confidence="0.7727995">
Table 2: Statistics of incompatibly annotated affixes in
200 sampled words for five segmented corpora.
</tableCaption>
<figure confidence="0.753574">
-W肃 NR 4 NN I;k VV *W NN 业 NN
</figure>
<figureCaption confidence="0.994133">
Figure 2: Example output of unified dependency parsing
of Chinese morphological and syntactic structures.
</figureCaption>
<sectionHeader confidence="0.988532" genericHeader="method">
3 Unified Parsing Defined
</sectionHeader>
<bodyText confidence="0.999874576923077">
Given an unsegmented sentence -W肃4I;k*W
业 ‘Gansu province attaches great importance to in-
surance industry’, the output of unified dependency
parser is shown in Figure 2. As can be seen, this
output contains information about word (such as I
;k_VV) as well as phrase structures (such as I
;k_VV *W_NN 业_NN), which is what we mean
by ‘unified’ parsing. Now, it’s no longer vital to dif-
ferentiate between morphology and syntax for Chi-
nese. People could regard *W业 ‘insurance indus-
try’ as a word or phrase, but either way, there will be
no disagreements about its internal structure. From
the perspective of the unified parser, linguistic units
are given the same labels as long as they function
similarly (e.g, they have the same parts-of-speech).
As a bonus, output of unified parsing incorpo-
rates Chinese word segmentation, part-of-speech
tagging and dependency parsing. To achieve these
goals, previous systems usually used a pipelined
approach by combining several statistical models,
which was further complicated by different decod-
ing algorithms for each of these models. The present
paper shows that a single model does all these jobs.
Besides being much simpler in engineering such a
parser, this approach is also a lot more plausible for
modeling human language understanding.
</bodyText>
<sectionHeader confidence="0.734644" genericHeader="method">
4 Annotation of Word Structures
</sectionHeader>
<bodyText confidence="0.999712142857143">
Unified parsing requires a corpus annotated with
both morphological and syntactic structures. Such
a corpus can be built with the least effort if we be-
gin with an existing treebank such as CTB6 already
annotated with syntactic structures. It only remains
for us to annotate internal structures of words in this
treebank.
</bodyText>
<subsectionHeader confidence="0.999871">
4.1 Scope of Annotation
</subsectionHeader>
<bodyText confidence="0.999986851851852">
In order to get rid of pseudo OOVs and incompati-
ble annotations, internal structures are annotated for
two kinds of words. The first kind contains words
with a productive component such as suffix or pre-
fix. One example is 陈述人 ‘speaker’ whose suffix
is the very productive 人 ‘person’ (e.g, in CTB6 there
are about 400 words having this suffix). The second
kind includes words with compositional semantics.
Examples are 星期一 ‘Monday’ and 星期天 ‘Sun-
day’. Though 星期 ‘week’ is not very productive,
the meaning of words with this prefix is deducible
from semantics of their components.
Other compound words such as 研究 ‘research’
have no productive components and are not a cause
of pseudo OOVs. They are universally consid-
ered as words instead of phrases due to their non-
compositional semantics. Hence their structures are
not annotated in the present research. Meanwhile,
for single-morpheme words with no structures what-
soever, like 伊拉克 ‘Iraq’ and 蝙蝠 ‘bat’, annotation
of internal structures is of course unnecessary either.
Of all the 54, 214 word types in CTB6, 35% are
annotated, while the percentage is 24% for the 782,
901 word tokens. Around 80% of sentences contain
words whose structures need annotation. Our anno-
tations will be made publicly available for research
purposes.
</bodyText>
<subsectionHeader confidence="0.984717">
4.2 From Part-of-speeches to Constituents
</subsectionHeader>
<bodyText confidence="0.999784125">
Of all 33 part-of-speech tags in CTB, annotation of
word structures is needed for nine tags: NN, VV, JJ,
CD, NT, NR, AD, VA and OD. Since part-of-speech
tags are preterminals and can only have one terminal
word as its child, POS tags of words become con-
stituent labels after annotation of word structures.
The mapping rules from POS tags to constituent la-
bels are listed in Table 3. Readers should note that
</bodyText>
<figure confidence="0.97285875">
MOD
SUBJ
OBJ
MOD
</figure>
<page confidence="0.807469">
1447
</page>
<table confidence="0.886142166666667">
POS tags constituent label
NR, NN, NT NP
JJ ADJP
AD ADVP
CD, OD QP
VV, VA VP
</table>
<tableCaption confidence="0.980436">
Table 3: Correspondence between POS tags and con-
stituent labels after annotation.
</tableCaption>
<equation confidence="0.578406">
跟 NN 跟 NN NN
教育_i 教育 _i
</equation>
<figureCaption confidence="0.993658666666667">
Figure 3: Example annotation for the word 教育_i NN
in CTB6: POS tag NN changes to constituent label NP
after annotation.
</figureCaption>
<bodyText confidence="0.999787">
such mapping is not arbitrary. The constraint is that
in the treebank the POS tag must somewhere be the
unique child of the constituent label. Figure 3 de-
picts an example annotation, in which we also have
an example of NP having a tag NN as its only child.
</bodyText>
<subsectionHeader confidence="0.997995">
4.3 Recursive Annotation
</subsectionHeader>
<bodyText confidence="0.999979666666667">
Some words in CTB have very complex structures.
Examples include 原子核物理学家 ‘physicist ma-
joring in nuclear physics’, 反托拉斯法 ‘anti-trust
laws’ etc. Structures of these words are anno-
tated to their full possible depth. Existence of such
words are characteristic of the Chinese language,
since they are further demonstrations of the blurred
boundary between morphology and syntax. A full-
fledged parser is needed to analyze structures of
these words, which incidentally provides us with an-
other motivation for unified morphological and syn-
tactic parsing of Chinese.
</bodyText>
<sectionHeader confidence="0.974404" genericHeader="method">
5 Unified Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999473625">
All previous dependency parsers for Chinese take it
for granted that the input sentence is already seg-
mented into words (Li et al., 2011). Most systems
even require words to be tagged with their part-of-
speeches (Zhang and Nivre, 2011). Hence current
off-the-shelf algorithms are inadequate for parsing
unsegmented sentences. Instead, a new unified pars-
ing algorithm is given in this section.
</bodyText>
<subsectionHeader confidence="0.981712">
5.1 Transitions
</subsectionHeader>
<bodyText confidence="0.999794095238095">
To map a raw sentence directly to output shown in
Figure 2, we define four transitions for the unified
dependency parser. They act on a stack containing
the incremental parsing results, and a queue holding
the incoming Chinese characters of the sentence:
SHIFT: the first character in the queue is shifted into
the stack as the start of a new word. The queue
should not be empty.
LEFT: the top two words of the stack are connected
with an arc, with the top one being the head. There
should be at least two elements on the stack.
RIGHT: the top two words of the stack are con-
nected, but with the top word being the child. The
precondition is the same as that of LEFT.
APPEND: the first character in the queue is appended
to the word at the top of the stack. There are two
preconditions. First, the queue should not be empty.
Second, the top of the stack must be a word with no
arcs connected to other words (i.e, up to now it has
got neither children nor parent).
We see that these transitions mimic the general arc-
standard dependency parsing models. The first three
of them were used, for example, by Yamada and
Matsumoto (2003) to parse English sentences. The
only novel addition is APPEND, which is necessary
because we are dealing with raw sentences. Its sole
purpose is to assemble characters into words with
no internal structures, such as 西 雅 图 ‘Seattle’.
Thus this transition is the key for removing the need
of Chinese word segmentation and parsing unseg-
mented sentences directly.
To also output part-of-speech tags and depen-
dency labels, the transitions above can be aug-
mented accordingly. Hence we can change SHIFT to
SHIFT·X where X represents a certain POS tag. Also,
LEFT and RIGHT should be augmented with appro-
priate dependency relations, such as LEFT·SUBJ for
a dependency between verb and subject.
As a demonstration of the usage of these tran-
sitions, consider sentence 我喜欢西雅图 “I love
Seattle”. Table 4 lists all steps of the parsing pro-
cess. Readers interested in implementing their own
</bodyText>
<figure confidence="0.791154625">
PP
❩❩
✚ ✚
P NP
� PP
✟ ✟ ❍❍
P NP
★ ★ ❝❝
</figure>
<page confidence="0.853904">
1448
</page>
<table confidence="0.999836928571429">
step stack queue action
1 我 PN 我喜欢西雅图 SHIFT·PN
2 我 PN 喜 VV 喜欢西雅图 SHIFT·VV
3 我 PN 喜欢 VV 欢西雅图 APPEND
4 我 PN SUBJ 西雅图 LEFT·SUBJ
5 ←−−− 喜欢 VV 西雅图 SHIFT·NR
6 我 PN SUBJ 雅图 APPEND
7 ←−−− 喜欢 VV 西 NR 图 APPEND
8 我 PN SUBJ RIGHT·OBJ
9 ←−−− 喜欢 VV 西雅 NR STOP
SU
我 PN ←−−−BJ 喜欢 VV 西雅图 NR
←−−− 喜欢 VV OBJ
我 PN SUBJ −−→ 西雅图 NR
</table>
<tableCaption confidence="0.760085">
Table 4: Parsing process of a short sentence with the four transitions defined above.
unified dependency parsers are invited to study this
example carefully.
</tableCaption>
<subsectionHeader confidence="0.988875">
5.2 Model
</subsectionHeader>
<bodyText confidence="0.999829111111111">
Due to structural ambiguity, there might be quite a
lot of possibilities for parsing a given raw sentence.
Hence at each step in the parsing process, all four
transitions defined above may be applicable. To re-
solve ambiguities, each candidate parse is scored
with a global linear model defined as follows.
For an input sentence x, the parsing result F(x) is
the one with the highest score in all possible struc-
tures for this x:
</bodyText>
<equation confidence="0.772240666666667">
F(x) = arg max Score(y) (1)
yEGEN(x)
Here GEN(x) is a set of all possible parses for sen-
tence x, and Score(y) is a real-valued linear func-
tion:
Score(y) = 4b(y) · w� (2)
</equation>
<bodyText confidence="0.999896333333333">
where 4b(y) is a global feature vector extracted from
parsing result y, and w� is a vector of weighting pa-
rameters. Because of its linearity, Score(y) can be
computed incrementally, following the transition of
each parsing step. Parameter vector w� is trained
with the generalized perceptron algorithm of Collins
(2002). The early-update strategy of Collins and
Roark (2004) is used so as to improve accuracy and
speed up the training.
</bodyText>
<subsectionHeader confidence="0.990349">
5.3 Feature Templates
</subsectionHeader>
<bodyText confidence="0.9997805">
For a particular parse y, we now describe the way
of computing its feature vector 4b(y) in the linear
</bodyText>
<figure confidence="0.929418769230769">
Description Feature Templates
1 top of S S0wt; S0w; S0t
2 next top of S S1wt; S1w; S1t
3 S0 and S1 S1wtS0wt; S1wtS0w
S1wS0wt; S1wtS0t
S1tS0wt; S1wS0w; S1tS0t
4 char unigrams Q0; Q1; Q2; Q3
5 char bigrams Q0Q1; Q1Q2; Q2Q3
6 char trigrams Q0Q1Q2; Q1Q2Q3
7 ST+unigrams STwtQ0; STwQ0; STtQ0
8 ST+bigrams STwtQ0Q1; STwQ0Q1
STtQ0Q1
9 ST+trigrams STwtQ0Q1Q2
</figure>
<table confidence="0.712960285714286">
STwQ0Q1Q2; STtQ0Q1Q2
10 parent P of ST PtSTtQ0; PtSTtQ0Q1
PtSTtQ0Q1Q2
11 leftmost child STtLCtQ0; STtLCtQ0Q1
LC and STtLCtQ0Q1Q2
rightmost STtRCtQ0; STtRCtQ0Q1
child RC STtRCtQ0Q1Q2
</table>
<tableCaption confidence="0.994085">
Table 5: Transition-based feature templates. Q0 is the
first character in Q, etc. w = word, t = POS tag.
</tableCaption>
<bodyText confidence="0.999751333333334">
model of Equation (2). If S denotes the stack hold-
ing the partial results, and Q the queue storing the
incoming Chinese characters of a raw sentence, then
transition-based parsing features are extracted from
S and Q according to those feature templates in Ta-
ble 5.
Although we employ transition-based parsing,
nothing prevents us from using graph-based fea-
tures. As shown by Zhang and Clark (2011), depen-
</bodyText>
<page confidence="0.978555">
1449
</page>
<figure confidence="0.764045428571429">
Description Feature Templates
1 parent word Pwt; Pw; Pt
2 child word Cwt; Cw; Ct
3 P and C PwtCwt; PwtCw; PwCwt
PtCwt; PwCw; PtCt
PwtCt
4 neighbor word PtPLtCtCLt; PtPLtCtCRt
of P and C PtPRtCtCLt; PtPRtCtCRt
left (L) or PtPLtCLt; PtPLtCRt
right (R) PtPRtCLt; PtPRtCRt
PLtCtCLt; PLtCtCRt
PRtCtCLt; PRtCtCRt
PtCtCLt; PtCtCRt
PtPLtCt; PtPRtCt
5 sibling(S) of C CwSw;CtSt; CwSt
CtSw; PtCtSt
6 leftmost and PtCtCLCt
rightmost child PtCtCRCt
7 left (la) and Ptla; Ptra
right (ra) Pwtla; Pwtra
arity of P Pwla; Pwra
</figure>
<tableCaption confidence="0.747623">
Table 6: Graph-based feature templates for the unified
parser. Most of these templates are adapted from those
used by Zhang and Clark (2011). w = word; t = POS tag.
</tableCaption>
<bodyText confidence="0.999774625">
dency parsers using both transition-based and graph-
based features tend to achieve higher accuracy than
parsers which only make use of one kind of features.
Table 6 gives the graph-based feature templates used
in our parser. All such templates are instantiated at
the earliest possible time, in order to reduce as much
as possible situations where correct parses fall out of
the beam during decoding.
</bodyText>
<subsectionHeader confidence="0.994698">
5.4 Decoding Algorithm
</subsectionHeader>
<bodyText confidence="0.9791724">
We use beam-search to find the best parse for a given
raw sentence (Algorithm 1). This algorithm uses
double beams. The first beam contains unfinished
parsing results, while the second holds completed
parses. Double beams are necessary because the
number of transitions might well be different for dif-
ferent parses, and those parses that finished earlier
are not necessarily better parses. During the search-
ing process, correct parse could fall off the beams,
resulting in a search error. However, in practice
beam search decoding algorithm works quite well.
In addition, it’s not feasible to use dynamic program-
ming because of the complicated features used in the
model.
The B in Algorithm 1 is the width of the two
beams. In our experiments we set B to 64. This
value of B was determined empirically by using the
standard development set of the data, with the goal
of achieving the highest possible accuracy within
reasonable time. Note that in line 20 of the algo-
rithm, the beam for completed parsers are pruned at
each iteration of the parsing process. The purpose
of this action is to keep this beam from growing too
big, resulting in a waste of memory space.
Algorithm 1 Beam Search Decoding
</bodyText>
<listItem confidence="0.987751571428571">
1: candidates +— {STARTITEM()}
2: agenda +— φ
3: completed +— φ
4: loop
5: for all candidate in candidates do
6: for all legal action of candidate do
7: newc +— EXPAND(candidate, action)
8: if COMPLETED(newc) then
9: completed.INSERT(newc)
10: else
11: agenda.INSERT(newc)
12: end if
13: end for
14: end for
15: if EMPTY(agenda) then
16: return TOP(completed)
17: end if
18: candidates +— TOPB(agenda, B)
19: agenda +— φ
20: completed +— TOPB(completed, B)
21: end loop
</listItem>
<sectionHeader confidence="0.983734" genericHeader="evaluation">
6 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999756833333333">
We describe the experiments carried out and our
method of evaluation of the unified dependency
parser. We used Penn2Malt 4 to convert constituent
trees of CTB to dependency relations. The head rules
for this conversion was given by Zhang and Clark
(2008). In all experiments, we followed the stan-
</bodyText>
<footnote confidence="0.9939655">
4http://w3.msi.vxu.se/˜nivre/research/
Penn2Malt.html
</footnote>
<page confidence="0.948112">
1450
</page>
<table confidence="0.99835975">
P R F
our method, labeled 78.54 80.93 79.72
our method, unlabeled 81.01 83.77 82.37
ZC2011, unlabeled N/A N/A 75.09
</table>
<tableCaption confidence="0.937338666666667">
Table 7: Evaluation results on the original CTB5. N/A
means the value is not available to us. ZC2011 is Zhang
and Clark (2011).
</tableCaption>
<bodyText confidence="0.996341666666667">
dard split of the data into training, testing and devel-
opment data (Zhang and Clark, 2011). Though we
annotated structures of words in CTB6, most previ-
ously results were on CTB5, a subset of the former
treebank. Hence we report our results of evaluation
on CTB5 for better comparability.
</bodyText>
<subsectionHeader confidence="0.99086">
6.1 Dependency Parsing of Morphological and
Syntactic Structures
</subsectionHeader>
<bodyText confidence="0.980589266666667">
If we look back at the Figure 2, it’s clear that a
dependency relation is correctly parsed if and only
if three conditions are met: Firstly, words at both
ends of the dependency are correctly segmented.
Secondly, part-of-speech tags are correct for both
words. Thirdly, the direction of the dependency re-
lation are correct. Of course, if labeled precision and
recall is to be measured, the label of the dependency
relation should also be correctly recovered. Let nc
be the number of dependencies correctly parsed with
respect to these criterion, no be the total number of
dependencies in the output, and nr the number of
dependencies in the reference. Then precision is de-
fined to be p = nc/no and recall is defined to be
r = nc/nr.
</bodyText>
<subsubsectionHeader confidence="0.397999">
6.1.1 Results on the Original CTB5
</subsubsectionHeader>
<bodyText confidence="0.99978825">
We first train our unified dependency parser with
the original treebank CTB5. In this case, all words
are considered to be flat, with no internal structures.
The result are shown in Table 7. Note that on ex-
actly the same testing data, i.e, the original CTB5,
unified parser performs much better than the result
of a pipelined approach reported by Zhang and Clark
(2011). There are about 30% of relative error reduc-
tion for the unlabeled dependency parsing results.
This is yet another evidence of the advantage of joint
modeling in natural language processing, details of
which will be discussed in Section 7.
</bodyText>
<table confidence="0.999691714285714">
P R F
original dependencies 82.13 84.49 83.29
in CTB5
ZN2011 with Gold N/A N/A 84.40
segmentation &amp; POS
original dependencies 85.71 87.18 86.44
plus word structures
</table>
<tableCaption confidence="0.999292">
Table 8: Evaluation results on CTB5 with word structures
annotated. All results are labeled scores.
</tableCaption>
<subsectionHeader confidence="0.3871725">
6.1.2 Results on CTB with Structures of Words
Annotated
</subsectionHeader>
<bodyText confidence="0.999993285714286">
Then we train the parser with CTB5 augmented
with our annotations of internal structures of words.
For purpose of better comparability, we report re-
sults on both the original dependencies of CTB5 and
on the dependencies of CTB5 plus those of the in-
ternal structures of words. The results are shown
in Table 8. First, note that compared to another re-
sult by Zhang and Nivre (2011), whose input were
sentences with gold standard word segmentation and
POS tags, our F-score is only slightly lower even
with input of unsegmented sentences. This is un-
derstandable since gold-standard segmentation and
POS tags greatly reduced the uncertainty of parsing
results.
For the unified parser, the improvement of F-score
from 79.72% to 83.29% is attributed to the fact that
with internal structures of words annotated, parsing
of syntactic structures is also improved due to the
similarity of word and phrase structures mentioned
in Section 1, and also due to the fact that many
phrase level dependencies are now facing a much
less severe problem of data sparsity. The improve-
ment of F-score from 83.29% to 86.44% is attributed
to the annotation of word structures. Internal struc-
tures of words are be mostly local in comparison
with phrase and sentence structures. Therefore, with
the addition of word structures, the overall depen-
dency parsing accuracy naturally can be improved.
</bodyText>
<subsectionHeader confidence="0.999717">
6.2 Chinese Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999864">
From the example in Figure 2, it is clear that output
of unified parser contains Chinese word segmenta-
tion information. Therefore, we can get results of
word segmentation for each sentence in the test sets,
</bodyText>
<page confidence="0.964006">
1451
</page>
<table confidence="0.999708666666667">
P R F
K2009 N/A N/A 97.87
This Paper 97.63 97.38 97.50
</table>
<tableCaption confidence="0.609473">
Table 9: Word segmentation results of our parser and
</tableCaption>
<table confidence="0.955391833333333">
the best performance reported in literature on the same
dataset. K2009 is the result of Kruengkrai et al. (2009).
P R F
K2009 N/A N/A 93.67
ZC2011 N/A N/A 93.67
This Paper 93.42 93.20 93.31
</table>
<tableCaption confidence="0.922859333333333">
Table 10: Joint word segmentation and POS tagging
scores. K2009 is result of Kruengkrai et al. (2009).
ZC2011 is result of Zhang and Clark (2011).
</tableCaption>
<bodyText confidence="0.999898111111111">
and evaluate their accuracies. For maximal compa-
rability, we train the unified parser on the original
CTB5 data used by previous studies. The result is
in Table 9. Despite the fact that the performance
of our unified parser does not exceed the best re-
ported result so far, which probably might be caused
by some minute implementation specific details, it’s
fair to say that our parser performs at the level of
state-of-the-art in Chinese word segmentation.
</bodyText>
<subsectionHeader confidence="0.999574">
6.3 Joint Word Segmentation and POS Tagging
</subsectionHeader>
<bodyText confidence="0.999944545454545">
From Figure 2 we see that besides word segmenta-
tion, output of the unified parser also includes part-
of-speech tags. Therefore, it’s natural that we evalu-
ate the accuracy of joint Chinese word segmentation
and part of speech tagging, as reported in previous
literature (Kruengkrai et al., 2009). The results are
in Table 10, in which for ease of comparison, again
we train the unified parser with the vanilla version
of CTB5. We can see that unified parser performs at
virtually the same level of accuracy compared with
previous best systems.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999344857142857">
Researchers have noticed the necessity of parsing
the internal structures of words in Chinese. Li
(2011) gave an method that could take raw sentences
as input and output phrase structures and internal
structures of words. This paper assumes that the in-
put are unsegmented, too, and our output also in-
cludes both word and phrase structures. There are
</bodyText>
<equation confidence="0.58617">
甘 肃 省 重 视 保 险 业
</equation>
<figureCaption confidence="0.992408">
Figure 4: Example output of Zhao’s parser.
</figureCaption>
<bodyText confidence="0.999174024390244">
two key differences, though. The first is we output
dependency relations instead of constituent struc-
tures. Although dependencies can be extracted from
the constituent trees of Li (2011), the time complex-
ity of their algorithm is O(n5) while our parser runs
in linear time. Secondly, we specify the details of
annotating structures of words, with the annotations
being made publicly available.
Zhao (2009) presented a dependency parser which
regards each Chinese character as a word and then
analyzes the dependency relations between charac-
ters, using ordinary dependency parsing algorithms.
Our parser is different in two important ways. The
first is we output both part-of-speech tags and la-
beled dependency relations, both of which were ab-
sent in Zhao’s parser. More importantly, the AP-
PEND transition for handling flat words were unseen
in previous studies as far as we know. The difference
can best be described with an example: For the sen-
tence in Section 3, Zhao’s parser output the result in
Figure 4 while in contrast our output is Figure 2.
In recent years, considerable efforts have been
made in joint modeling and learning in natural lan-
guage processing (Lee et al., 2011; Sun, 2011; Li et
al., 2011; Finkel and Manning, 2009; Kruengkrai et
al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty,
2008). Joint modeling can improve the performance
of NLP systems due to the obvious reason of being
able to make use of various levels of information si-
multaneously. However, the thesis of this paper, i.e,
unified parsing of Chinese word and phrase struc-
tures, bears a deeper meaning. As demonstrated in
Section 1 and by Li (2011), structures of words and
phrases usually have significant similarity, and the
distinction between them is very difficult to define,
even for expert linguists. But for real world applica-
tions, such subtle matters can safely be ignored if we
could analyzed morphological and syntactic struc-
tures in a unified framework. What applications re-
ally cares is structures instead of whether a linguistic
unit is a word or phrase.
</bodyText>
<page confidence="0.983627">
1452
</page>
<bodyText confidence="0.999932590909091">
Another notable line of research closely related
to the present work is to annotate and parse the flat
structures of noun phrases (NP) (Vadas and Curran,
2007; Vadas and Curran, 2011). This paper dif-
fers from those previous work on parsing NPs in at
least two significant ways. First, we aim to parse
all kinds of words (e.g, nouns, verbs, adverbs, ad-
jectives etc) whose structures are not annotated by
CTB, and whose presence could cause lots of pseudo
OOVs and incompatible annotations. Second, the
problem we are trying to solve is a crucial obser-
vation specific to Chinese language, that is, in lots
of cases forcing a separation of words and phrases
leads to awkward situations for NLP systems. Re-
member that in Section 2 we demonstrated that all
corpora we examined had the problem of pseudo
OOVs and incompatible annotations. In comparison,
the problem Vadas and Curran (2007) tried to solve
is a lack of annotation for structures of NPs in cur-
rently available treebanks, or to put it in another way,
a problem more closely related to treebanks rather
than certain languages.
</bodyText>
<sectionHeader confidence="0.991946" genericHeader="discussions">
8 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.99998145">
Chinese word segmentation is an indispensable step
for traditional approaches to syntactic parsing of
Chinese. The purpose of word segmentation is to
decide what goes to words, with the remaining pro-
cessing (e.g, parsing) left to higher level structures
of phrases and sentences. This paper shows that it
could be very difficult to make such a distinction
between words and phrases. This difficulty cannot
be left unheeded, as we have shown quantitatively
that in practice it causes lots of real troubles such as
too many OOVs and incompatible annotations. We
showed how these undesirable consequences can be
resolved by annotation of the internal structures of
words, and by unified parsing of morphological and
syntactic structures in Chinese.
Unified parsing of morphological and syntactic
structures of Chinese can also be implemented with
a pipelined approach, in which we first segment in-
put sentences into words or affixes (i.e, with the
finest possible granularity), and then we do part-
of-speech tagging followed by dependency (or con-
stituent) parsing. However, a unified parsing ap-
proach using a single model as presented in this
paper offers several advantages over pipelined ap-
proaches. The first one is that joint modeling tends
to result in higher accuracy and suffer less from er-
ror propagation than do pipelined methods. Sec-
ondly, both the unified model and the algorithm
are conceptually much more simpler than pipelined
approaches. We only need one implementation of
the model and algorithm, instead of several ones in
pipelined approaches. Thirdly, our model and al-
gorithm might comes closer to modeling the pro-
cess of human language understanding, because hu-
man brain is more likely a parallel machine in un-
derstanding languages than an alternative pipelined
processor. Hence this work, together with previ-
ous studies by other authors like Li (2011) and Zhao
(2009), open up a possibly new direction for future
research efforts in parsing the Chinese language.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995374">
Reviewers of this paper offered many detailed and
highly valuable suggestions for improvement in pre-
sentation. The authors are supported by NSFC under
Grant No. 90920004 and National 863 Program un-
der Grant No. 2012AA011102.
</bodyText>
<sectionHeader confidence="0.999212" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999532347826087">
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL’04), Main Volume, pages 111–
118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1–8. Association for
Computational Linguistics, July.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings ofHuman Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
326–334, Boulder, Colorado, June. Association for
Computational Linguistics.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of ACL-08: HLT,
pages 371–379, Columbus, Ohio, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.596671">
1453
</page>
<reference confidence="0.999914641975309">
C. F. Hockett. 1969. A Course in Modern Linguistics.
Macmillan.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008. A cascaded linear model for joint Chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL-08: HLT, pages 897–904, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513–521, Suntec, Singapore,
August. Association for Computational Linguistics.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 885–894, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-
liang Chen, and Haizhou Li. 2011. Joint models for
Chinese POS tagging and dependency parsing. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1180–
1191, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for Chinese word segmenta-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1405–1414, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1385–1394, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 240–247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
David Vadas and James R. Curran. 2011. Parsing noun
phrases in the penn treebank. Computational Linguis-
tics, 37(4):753–809.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector machines.
In Proceeding of the 8th International Workshop of
Parsing Technologies (IWPT), pages 195–206, Nancy,
France.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 562–571, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2011. Syntactic process-
ing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105–151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188–193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Hai Zhao. 2009. Character-level dependencies in Chi-
nese: Usefulness and learning. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 879–887, Athens, Greece, March.
Association for Computational Linguistics.
</reference>
<page confidence="0.995013">
1454
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.253752">
<title confidence="0.9956045">Unified Dependency Parsing of Chinese and Syntactic Structures</title>
<author confidence="0.985958">Zhongguo Li Guodong Zhou</author>
<affiliation confidence="0.9563685">Natural Language Processing School of Computer Science and</affiliation>
<address confidence="0.324105">Soochow University, Suzhou, Jiangsu Province 215006,</address>
<abstract confidence="0.992811590909091">Most previous approaches to syntactic parsing of Chinese rely on a preprocessing step of word segmentation, thereby assuming there was a clearly defined boundary between morphology and syntax in Chinese. We show how this assumption can fail badly, leading to many out-of-vocabulary words and incompatible annotations. Hence in practice the strict separation of morphology and syntax in the Chinese language proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sensimultaneously.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="16611" citStr="Collins and Roark (2004)" startWordPosition="2802" endWordPosition="2805">t F(x) is the one with the highest score in all possible structures for this x: F(x) = arg max Score(y) (1) yEGEN(x) Here GEN(x) is a set of all possible parses for sentence x, and Score(y) is a real-valued linear function: Score(y) = 4b(y) · w� (2) where 4b(y) is a global feature vector extracted from parsing result y, and w� is a vector of weighting parameters. Because of its linearity, Score(y) can be computed incrementally, following the transition of each parsing step. Parameter vector w� is trained with the generalized perceptron algorithm of Collins (2002). The early-update strategy of Collins and Roark (2004) is used so as to improve accuracy and speed up the training. 5.3 Feature Templates For a particular parse y, we now describe the way of computing its feature vector 4b(y) in the linear Description Feature Templates 1 top of S S0wt; S0w; S0t 2 next top of S S1wt; S1w; S1t 3 S0 and S1 S1wtS0wt; S1wtS0w S1wS0wt; S1wtS0t S1tS0wt; S1wS0w; S1tS0t 4 char unigrams Q0; Q1; Q2; Q3 5 char bigrams Q0Q1; Q1Q2; Q2Q3 6 char trigrams Q0Q1Q2; Q1Q2Q3 7 ST+unigrams STwtQ0; STwQ0; STtQ0 8 ST+bigrams STwtQ0Q1; STwQ0Q1 STtQ0Q1 9 ST+trigrams STwtQ0Q1Q2 STwQ0Q1Q2; STtQ0Q1Q2 10 parent P of ST PtSTtQ0; PtSTtQ0Q1 PtSTt</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 111– 118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="16556" citStr="Collins (2002)" startWordPosition="2796" endWordPosition="2797">s. For an input sentence x, the parsing result F(x) is the one with the highest score in all possible structures for this x: F(x) = arg max Score(y) (1) yEGEN(x) Here GEN(x) is a set of all possible parses for sentence x, and Score(y) is a real-valued linear function: Score(y) = 4b(y) · w� (2) where 4b(y) is a global feature vector extracted from parsing result y, and w� is a vector of weighting parameters. Because of its linearity, Score(y) can be computed incrementally, following the transition of each parsing step. Parameter vector w� is trained with the generalized perceptron algorithm of Collins (2002). The early-update strategy of Collins and Roark (2004) is used so as to improve accuracy and speed up the training. 5.3 Feature Templates For a particular parse y, we now describe the way of computing its feature vector 4b(y) in the linear Description Feature Templates 1 top of S S0wt; S0w; S0t 2 next top of S S1wt; S1w; S1t 3 S0 and S1 S1wtS0wt; S1wtS0w S1wS0wt; S1wtS0t S1tS0wt; S1wS0w; S1tS0t 4 char unigrams Q0; Q1; Q2; Q3 5 char bigrams Q0Q1; Q1Q2; Q2Q3 6 char trigrams Q0Q1Q2; Q1Q2Q3 7 ST+unigrams STwtQ0; STwQ0; STtQ0 8 ST+bigrams STwtQ0Q1; STwQ0Q1 STtQ0Q1 9 ST+trigrams STwtQ0Q1Q2 STwQ0Q1Q</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 1–8. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings ofHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>326--334</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="28050" citStr="Finkel and Manning, 2009" startWordPosition="4716" endWordPosition="4719">nt in two important ways. The first is we output both part-of-speech tags and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the APPEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such su</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In Proceedings ofHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 326–334, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>371--379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="28125" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="4728" endWordPosition="4731">gs and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the APPEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such subtle matters can safely be ignored if we could analyzed morphological and s</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proceedings of ACL-08: HLT, pages 371–379, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Hockett</author>
</authors>
<title>A Course in Modern Linguistics.</title>
<date>1969</date>
<publisher>Macmillan.</publisher>
<contexts>
<context position="1290" citStr="Hockett, 1969" startWordPosition="189" endWordPosition="191">anguage proves to be untenable. We present a unified dependency parsing approach for Chinese which takes unsegmented sentences as input and outputs both morphological and syntactic structures with a single model and algorithm. By removing the intermediate word segmentation, the unified parser no longer needs separate notions for words and phrases. Evaluation proves the effectiveness of the unified model and algorithm in parsing structures of words, phrases and sentences simultaneously. 1 1 Introduction The formulation of the concept of words has baffled linguists from ancient to modern times (Hockett, 1969). Things are even worse for Chinese, partly due to the fact that its written form does not delimit words explicitly. While we have no doubt that there are linguistic units which are definitely words (or phrases, for that matter), it’s a sad truth that in many cases we cannot manage to draw such a clear boundary between morphology and syntax, for which we now give two arguments. 1Corresponding author is Guodong Zhou. The first argument is that many sub-word linguistic units (such as suffixes and prefixes) are so productive that they can lead to a huge number of outof-vocabulary words for natura</context>
</contexts>
<marker>Hockett, 1969</marker>
<rawString>C. F. Hockett. 1969. A Course in Modern Linguistics. Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>897--904</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Jiang, Huang, Liu, L¨u, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u. 2008. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of ACL-08: HLT, pages 897–904, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>513--521</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="25121" citStr="Kruengkrai et al. (2009)" startWordPosition="4222" endWordPosition="4225">mparison with phrase and sentence structures. Therefore, with the addition of word structures, the overall dependency parsing accuracy naturally can be improved. 6.2 Chinese Word Segmentation From the example in Figure 2, it is clear that output of unified parser contains Chinese word segmentation information. Therefore, we can get results of word segmentation for each sentence in the test sets, 1451 P R F K2009 N/A N/A 97.87 This Paper 97.63 97.38 97.50 Table 9: Word segmentation results of our parser and the best performance reported in literature on the same dataset. K2009 is the result of Kruengkrai et al. (2009). P R F K2009 N/A N/A 93.67 ZC2011 N/A N/A 93.67 This Paper 93.42 93.20 93.31 Table 10: Joint word segmentation and POS tagging scores. K2009 is result of Kruengkrai et al. (2009). ZC2011 is result of Zhang and Clark (2011). and evaluate their accuracies. For maximal comparability, we train the unified parser on the original CTB5 data used by previous studies. The result is in Table 9. Despite the fact that the performance of our unified parser does not exceed the best reported result so far, which probably might be caused by some minute implementation specific details, it’s fair to say that o</context>
<context position="28075" citStr="Kruengkrai et al., 2009" startWordPosition="4720" endWordPosition="4723">The first is we output both part-of-speech tags and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the APPEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such subtle matters can safely b</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 513–521, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
</authors>
<title>A discriminative model for joint morphological disambiguation and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>885--894</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="27996" citStr="Lee et al., 2011" startWordPosition="4706" endWordPosition="4709">ency parsing algorithms. Our parser is different in two important ways. The first is we output both part-of-speech tags and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the APPEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expe</context>
</contexts>
<marker>Lee, Naradowsky, Smith, 2011</marker>
<rawString>John Lee, Jason Naradowsky, and David A. Smith. 2011. A discriminative model for joint morphological disambiguation and dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 885–894, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Wenliang Chen</author>
<author>Haizhou Li</author>
</authors>
<title>Joint models for Chinese POS tagging and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1180--1191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="12841" citStr="Li et al., 2011" startWordPosition="2120" endWordPosition="2123">ics’, 反托拉斯法 ‘anti-trust laws’ etc. Structures of these words are annotated to their full possible depth. Existence of such words are characteristic of the Chinese language, since they are further demonstrations of the blurred boundary between morphology and syntax. A fullfledged parser is needed to analyze structures of these words, which incidentally provides us with another motivation for unified morphological and syntactic parsing of Chinese. 5 Unified Dependency Parsing All previous dependency parsers for Chinese take it for granted that the input sentence is already segmented into words (Li et al., 2011). Most systems even require words to be tagged with their part-ofspeeches (Zhang and Nivre, 2011). Hence current off-the-shelf algorithms are inadequate for parsing unsegmented sentences. Instead, a new unified parsing algorithm is given in this section. 5.1 Transitions To map a raw sentence directly to output shown in Figure 2, we define four transitions for the unified dependency parser. They act on a stack containing the incremental parsing results, and a queue holding the incoming Chinese characters of the sentence: SHIFT: the first character in the queue is shifted into the stack as the s</context>
<context position="28024" citStr="Li et al., 2011" startWordPosition="4712" endWordPosition="4715">parser is different in two important ways. The first is we output both part-of-speech tags and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the APPEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real w</context>
</contexts>
<marker>Li, Zhang, Che, Liu, Chen, Li, 2011</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wenliang Chen, and Haizhou Li. 2011. Joint models for Chinese POS tagging and dependency parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1180– 1191, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
</authors>
<title>Parsing the internal structure of words: A new paradigm for Chinese word segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1405--1414</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="3246" citStr="Li, 2011" startWordPosition="530" endWordPosition="531">ts are frequently-seen words. Our estimation is that over 60% of OOVs in Chinese are of this kind (Section 2). Of course, the way out of this dilemma is to parse the internal structures of these words. That is to say, we can still regard characters like * as suffixes, taking into account the fact that they cannot be used alone. Meanwhile, pseudo OOVs can be largely eliminated through analyzing their structures, thus greatly facilitating syntactic and semantic analysis of sentences. In fact, previous studies have revealed other good reasons for parsing internal structures of words (Zhao, 2009; Li, 2011). The second argument is that in Chinese many linguistic units can form both words and phrases with exactly the same meaning and part-of-speech, which 1445 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1445–1454, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 环境 NN *护 NN 法 NN 刑 NN 法 NN Figure 1: Unified parsing of words and phrases. MOD MOD MOD corpus OOV pseudo percent CTB6 158 112 70.9 MSR 1,783 1,307 73.3 PKU 2,860 1,836 64.2 AS 3,020 2,143 71.0 CITYU 1,66</context>
<context position="5257" citStr="Li, 2011" startWordPosition="863" endWordPosition="864">3 ‘machine’ appears twice in phrases but 377 times in words (e.g. ba速3 ‘accelerator’). Word structures in Chinese can be excellent guide for parsing phrase structures, and vice versa, due to their similarity. The present paper makes two contributions in light of these issues. Firstly, in order to get rid of pseudo OOVs and incompatible annotations, we have annotated structures of words in CTB6, after which statistical models can learn structures of words as well as phrases from the augmented treebank (Section 4). Although previous authors have noticed the importance of word-structure parsing (Li, 2011; Zhao, 2009), no detailed description about annotation of word structures has been provided in the literature. Secondly, we designed a unified dependency parser whose input is unsegmented sentences and its output incorporates both morphological and syntactic structures with a single model and algorithm (Section 5). By removing the intermediate step of word segmentation, our unified parser no longer depends on the unsound notion that there is a clear boundary between words and phrases. Evaluation (Section 6) shows that our unified parser achieves satisfactory accuracies in parsing both morphol</context>
<context position="26508" citStr="Li (2011)" startWordPosition="4458" endWordPosition="4459"> output of the unified parser also includes partof-speech tags. Therefore, it’s natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al., 2009). The results are in Table 10, in which for ease of comparison, again we train the unified parser with the vanilla version of CTB5. We can see that unified parser performs at virtually the same level of accuracy compared with previous best systems. 7 Related Work Researchers have noticed the necessity of parsing the internal structures of words in Chinese. Li (2011) gave an method that could take raw sentences as input and output phrase structures and internal structures of words. This paper assumes that the input are unsegmented, too, and our output also includes both word and phrase structures. There are 甘 肃 省 重 视 保 险 业 Figure 4: Example output of Zhao’s parser. two key differences, though. The first is we output dependency relations instead of constituent structures. Although dependencies can be extracted from the constituent trees of Li (2011), the time complexity of their algorithm is O(n5) while our parser runs in linear time. Secondly, we specify </context>
<context position="28450" citStr="Li (2011)" startWordPosition="4786" endWordPosition="4787">ur output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such subtle matters can safely be ignored if we could analyzed morphological and syntactic structures in a unified framework. What applications really cares is structures instead of whether a linguistic unit is a word or phrase. 1452 Another notable line of research closely related to the present work is to annotate and parse the flat structures of noun phrases (NP) (Vadas and Curran, 2007; Vadas and Cur</context>
</contexts>
<marker>Li, 2011</marker>
<rawString>Zhongguo Li. 2011. Parsing the internal structure of words: A new paradigm for Chinese word segmentation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1405–1414, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1385--1394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="28007" citStr="Sun, 2011" startWordPosition="4710" endWordPosition="4711">ithms. Our parser is different in two important ways. The first is we output both part-of-speech tags and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the APPEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figure 4 while in contrast our output is Figure 2. In recent years, considerable efforts have been made in joint modeling and learning in natural language processing (Lee et al., 2011; Sun, 2011; Li et al., 2011; Finkel and Manning, 2009; Kruengkrai et al., 2009; Jiang et al., 2008; Goldberg and Tsarfaty, 2008). Joint modeling can improve the performance of NLP systems due to the obvious reason of being able to make use of various levels of information simultaneously. However, the thesis of this paper, i.e, unified parsing of Chinese word and phrase structures, bears a deeper meaning. As demonstrated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguist</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint chinese word segmentation and part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1385–1394, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James Curran</author>
</authors>
<title>Adding noun phrase structure to the penn treebank.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>240--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="29035" citStr="Vadas and Curran, 2007" startWordPosition="4879" endWordPosition="4882">strated in Section 1 and by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such subtle matters can safely be ignored if we could analyzed morphological and syntactic structures in a unified framework. What applications really cares is structures instead of whether a linguistic unit is a word or phrase. 1452 Another notable line of research closely related to the present work is to annotate and parse the flat structures of noun phrases (NP) (Vadas and Curran, 2007; Vadas and Curran, 2011). This paper differs from those previous work on parsing NPs in at least two significant ways. First, we aim to parse all kinds of words (e.g, nouns, verbs, adverbs, adjectives etc) whose structures are not annotated by CTB, and whose presence could cause lots of pseudo OOVs and incompatible annotations. Second, the problem we are trying to solve is a crucial observation specific to Chinese language, that is, in lots of cases forcing a separation of words and phrases leads to awkward situations for NLP systems. Remember that in Section 2 we demonstrated that all corpor</context>
</contexts>
<marker>Vadas, Curran, 2007</marker>
<rawString>David Vadas and James Curran. 2007. Adding noun phrase structure to the penn treebank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 240–247, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Parsing noun phrases in the penn treebank.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="29060" citStr="Vadas and Curran, 2011" startWordPosition="4883" endWordPosition="4886"> by Li (2011), structures of words and phrases usually have significant similarity, and the distinction between them is very difficult to define, even for expert linguists. But for real world applications, such subtle matters can safely be ignored if we could analyzed morphological and syntactic structures in a unified framework. What applications really cares is structures instead of whether a linguistic unit is a word or phrase. 1452 Another notable line of research closely related to the present work is to annotate and parse the flat structures of noun phrases (NP) (Vadas and Curran, 2007; Vadas and Curran, 2011). This paper differs from those previous work on parsing NPs in at least two significant ways. First, we aim to parse all kinds of words (e.g, nouns, verbs, adverbs, adjectives etc) whose structures are not annotated by CTB, and whose presence could cause lots of pseudo OOVs and incompatible annotations. Second, the problem we are trying to solve is a crucial observation specific to Chinese language, that is, in lots of cases forcing a separation of words and phrases leads to awkward situations for NLP systems. Remember that in Section 2 we demonstrated that all corpora we examined had the pro</context>
</contexts>
<marker>Vadas, Curran, 2011</marker>
<rawString>David Vadas and James R. Curran. 2011. Parsing noun phrases in the penn treebank. Computational Linguistics, 37(4):753–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceeding of the 8th International Workshop of Parsing Technologies (IWPT),</booktitle>
<pages>195--206</pages>
<location>Nancy, France.</location>
<contexts>
<context position="14240" citStr="Yamada and Matsumoto (2003)" startWordPosition="2369" endWordPosition="2372"> two elements on the stack. RIGHT: the top two words of the stack are connected, but with the top word being the child. The precondition is the same as that of LEFT. APPEND: the first character in the queue is appended to the word at the top of the stack. There are two preconditions. First, the queue should not be empty. Second, the top of the stack must be a word with no arcs connected to other words (i.e, up to now it has got neither children nor parent). We see that these transitions mimic the general arcstandard dependency parsing models. The first three of them were used, for example, by Yamada and Matsumoto (2003) to parse English sentences. The only novel addition is APPEND, which is necessary because we are dealing with raw sentences. Its sole purpose is to assemble characters into words with no internal structures, such as 西 雅 图 ‘Seattle’. Thus this transition is the key for removing the need of Chinese word segmentation and parsing unsegmented sentences directly. To also output part-of-speech tags and dependency labels, the transitions above can be augmented accordingly. Hence we can change SHIFT to SHIFT·X where X represents a certain POS tag. Also, LEFT and RIGHT should be augmented with appropri</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceeding of the 8th International Workshop of Parsing Technologies (IWPT), pages 195–206, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="20893" citStr="Zhang and Clark (2008)" startWordPosition="3520" endWordPosition="3523">on of candidate do 7: newc +— EXPAND(candidate, action) 8: if COMPLETED(newc) then 9: completed.INSERT(newc) 10: else 11: agenda.INSERT(newc) 12: end if 13: end for 14: end for 15: if EMPTY(agenda) then 16: return TOP(completed) 17: end if 18: candidates +— TOPB(agenda, B) 19: agenda +— φ 20: completed +— TOPB(completed, B) 21: end loop 6 Experiments and Evaluation We describe the experiments carried out and our method of evaluation of the unified dependency parser. We used Penn2Malt 4 to convert constituent trees of CTB to dependency relations. The head rules for this conversion was given by Zhang and Clark (2008). In all experiments, we followed the stan4http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 1450 P R F our method, labeled 78.54 80.93 79.72 our method, unlabeled 81.01 83.77 82.37 ZC2011, unlabeled N/A N/A 75.09 Table 7: Evaluation results on the original CTB5. N/A means the value is not available to us. ZC2011 is Zhang and Clark (2011). dard split of the data into training, testing and development data (Zhang and Clark, 2011). Though we annotated structures of words in CTB6, most previously results were on CTB5, a subset of the former treebank. Hence we report our results of evaluation o</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562–571, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="17832" citStr="Zhang and Clark (2011)" startWordPosition="3006" endWordPosition="3009">tQ0Q1Q2 11 leftmost child STtLCtQ0; STtLCtQ0Q1 LC and STtLCtQ0Q1Q2 rightmost STtRCtQ0; STtRCtQ0Q1 child RC STtRCtQ0Q1Q2 Table 5: Transition-based feature templates. Q0 is the first character in Q, etc. w = word, t = POS tag. model of Equation (2). If S denotes the stack holding the partial results, and Q the queue storing the incoming Chinese characters of a raw sentence, then transition-based parsing features are extracted from S and Q according to those feature templates in Table 5. Although we employ transition-based parsing, nothing prevents us from using graph-based features. As shown by Zhang and Clark (2011), depen1449 Description Feature Templates 1 parent word Pwt; Pw; Pt 2 child word Cwt; Cw; Ct 3 P and C PwtCwt; PwtCw; PwCwt PtCwt; PwCw; PtCt PwtCt 4 neighbor word PtPLtCtCLt; PtPLtCtCRt of P and C PtPRtCtCLt; PtPRtCtCRt left (L) or PtPLtCLt; PtPLtCRt right (R) PtPRtCLt; PtPRtCRt PLtCtCLt; PLtCtCRt PRtCtCLt; PRtCtCRt PtCtCLt; PtCtCRt PtPLtCt; PtPRtCt 5 sibling(S) of C CwSw;CtSt; CwSt CtSw; PtCtSt 6 leftmost and PtCtCLCt rightmost child PtCtCRCt 7 left (la) and Ptla; Ptra right (ra) Pwtla; Pwtra arity of P Pwla; Pwra Table 6: Graph-based feature templates for the unified parser. Most of these t</context>
<context position="21236" citStr="Zhang and Clark (2011)" startWordPosition="3573" endWordPosition="3576">periments and Evaluation We describe the experiments carried out and our method of evaluation of the unified dependency parser. We used Penn2Malt 4 to convert constituent trees of CTB to dependency relations. The head rules for this conversion was given by Zhang and Clark (2008). In all experiments, we followed the stan4http://w3.msi.vxu.se/˜nivre/research/ Penn2Malt.html 1450 P R F our method, labeled 78.54 80.93 79.72 our method, unlabeled 81.01 83.77 82.37 ZC2011, unlabeled N/A N/A 75.09 Table 7: Evaluation results on the original CTB5. N/A means the value is not available to us. ZC2011 is Zhang and Clark (2011). dard split of the data into training, testing and development data (Zhang and Clark, 2011). Though we annotated structures of words in CTB6, most previously results were on CTB5, a subset of the former treebank. Hence we report our results of evaluation on CTB5 for better comparability. 6.1 Dependency Parsing of Morphological and Syntactic Structures If we look back at the Figure 2, it’s clear that a dependency relation is correctly parsed if and only if three conditions are met: Firstly, words at both ends of the dependency are correctly segmented. Secondly, part-of-speech tags are correct </context>
<context position="22728" citStr="Zhang and Clark (2011)" startWordPosition="3828" endWordPosition="3831">th respect to these criterion, no be the total number of dependencies in the output, and nr the number of dependencies in the reference. Then precision is defined to be p = nc/no and recall is defined to be r = nc/nr. 6.1.1 Results on the Original CTB5 We first train our unified dependency parser with the original treebank CTB5. In this case, all words are considered to be flat, with no internal structures. The result are shown in Table 7. Note that on exactly the same testing data, i.e, the original CTB5, unified parser performs much better than the result of a pipelined approach reported by Zhang and Clark (2011). There are about 30% of relative error reduction for the unlabeled dependency parsing results. This is yet another evidence of the advantage of joint modeling in natural language processing, details of which will be discussed in Section 7. P R F original dependencies 82.13 84.49 83.29 in CTB5 ZN2011 with Gold N/A N/A 84.40 segmentation &amp; POS original dependencies 85.71 87.18 86.44 plus word structures Table 8: Evaluation results on CTB5 with word structures annotated. All results are labeled scores. 6.1.2 Results on CTB with Structures of Words Annotated Then we train the parser with CTB5 aug</context>
<context position="25344" citStr="Zhang and Clark (2011)" startWordPosition="4263" endWordPosition="4266">is clear that output of unified parser contains Chinese word segmentation information. Therefore, we can get results of word segmentation for each sentence in the test sets, 1451 P R F K2009 N/A N/A 97.87 This Paper 97.63 97.38 97.50 Table 9: Word segmentation results of our parser and the best performance reported in literature on the same dataset. K2009 is the result of Kruengkrai et al. (2009). P R F K2009 N/A N/A 93.67 ZC2011 N/A N/A 93.67 This Paper 93.42 93.20 93.31 Table 10: Joint word segmentation and POS tagging scores. K2009 is result of Kruengkrai et al. (2009). ZC2011 is result of Zhang and Clark (2011). and evaluate their accuracies. For maximal comparability, we train the unified parser on the original CTB5 data used by previous studies. The result is in Table 9. Despite the fact that the performance of our unified parser does not exceed the best reported result so far, which probably might be caused by some minute implementation specific details, it’s fair to say that our parser performs at the level of state-of-the-art in Chinese word segmentation. 6.3 Joint Word Segmentation and POS Tagging From Figure 2 we see that besides word segmentation, output of the unified parser also includes p</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>188--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="12938" citStr="Zhang and Nivre, 2011" startWordPosition="2136" endWordPosition="2139">ssible depth. Existence of such words are characteristic of the Chinese language, since they are further demonstrations of the blurred boundary between morphology and syntax. A fullfledged parser is needed to analyze structures of these words, which incidentally provides us with another motivation for unified morphological and syntactic parsing of Chinese. 5 Unified Dependency Parsing All previous dependency parsers for Chinese take it for granted that the input sentence is already segmented into words (Li et al., 2011). Most systems even require words to be tagged with their part-ofspeeches (Zhang and Nivre, 2011). Hence current off-the-shelf algorithms are inadequate for parsing unsegmented sentences. Instead, a new unified parsing algorithm is given in this section. 5.1 Transitions To map a raw sentence directly to output shown in Figure 2, we define four transitions for the unified dependency parser. They act on a stack containing the incremental parsing results, and a queue holding the incoming Chinese characters of the sentence: SHIFT: the first character in the queue is shifted into the stack as the start of a new word. The queue should not be empty. LEFT: the top two words of the stack are conne</context>
<context position="23669" citStr="Zhang and Nivre (2011)" startWordPosition="3985" endWordPosition="3988"> 84.40 segmentation &amp; POS original dependencies 85.71 87.18 86.44 plus word structures Table 8: Evaluation results on CTB5 with word structures annotated. All results are labeled scores. 6.1.2 Results on CTB with Structures of Words Annotated Then we train the parser with CTB5 augmented with our annotations of internal structures of words. For purpose of better comparability, we report results on both the original dependencies of CTB5 and on the dependencies of CTB5 plus those of the internal structures of words. The results are shown in Table 8. First, note that compared to another result by Zhang and Nivre (2011), whose input were sentences with gold standard word segmentation and POS tags, our F-score is only slightly lower even with input of unsegmented sentences. This is understandable since gold-standard segmentation and POS tags greatly reduced the uncertainty of parsing results. For the unified parser, the improvement of F-score from 79.72% to 83.29% is attributed to the fact that with internal structures of words annotated, parsing of syntactic structures is also improved due to the similarity of word and phrase structures mentioned in Section 1, and also due to the fact that many phrase level </context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 188–193, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
</authors>
<title>Character-level dependencies in Chinese: Usefulness and learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>879--887</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="3235" citStr="Zhao, 2009" startWordPosition="528" endWordPosition="529">eir components are frequently-seen words. Our estimation is that over 60% of OOVs in Chinese are of this kind (Section 2). Of course, the way out of this dilemma is to parse the internal structures of these words. That is to say, we can still regard characters like * as suffixes, taking into account the fact that they cannot be used alone. Meanwhile, pseudo OOVs can be largely eliminated through analyzing their structures, thus greatly facilitating syntactic and semantic analysis of sentences. In fact, previous studies have revealed other good reasons for parsing internal structures of words (Zhao, 2009; Li, 2011). The second argument is that in Chinese many linguistic units can form both words and phrases with exactly the same meaning and part-of-speech, which 1445 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1445–1454, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics 环境 NN *护 NN 法 NN 刑 NN 法 NN Figure 1: Unified parsing of words and phrases. MOD MOD MOD corpus OOV pseudo percent CTB6 158 112 70.9 MSR 1,783 1,307 73.3 PKU 2,860 1,836 64.2 AS 3,020 2,143 71.0</context>
<context position="5270" citStr="Zhao, 2009" startWordPosition="865" endWordPosition="866">’ appears twice in phrases but 377 times in words (e.g. ba速3 ‘accelerator’). Word structures in Chinese can be excellent guide for parsing phrase structures, and vice versa, due to their similarity. The present paper makes two contributions in light of these issues. Firstly, in order to get rid of pseudo OOVs and incompatible annotations, we have annotated structures of words in CTB6, after which statistical models can learn structures of words as well as phrases from the augmented treebank (Section 4). Although previous authors have noticed the importance of word-structure parsing (Li, 2011; Zhao, 2009), no detailed description about annotation of word structures has been provided in the literature. Secondly, we designed a unified dependency parser whose input is unsegmented sentences and its output incorporates both morphological and syntactic structures with a single model and algorithm (Section 5). By removing the intermediate step of word segmentation, our unified parser no longer depends on the unsound notion that there is a clear boundary between words and phrases. Evaluation (Section 6) shows that our unified parser achieves satisfactory accuracies in parsing both morphological and sy</context>
<context position="27218" citStr="Zhao (2009)" startWordPosition="4578" endWordPosition="4579">ructures of words. This paper assumes that the input are unsegmented, too, and our output also includes both word and phrase structures. There are 甘 肃 省 重 视 保 险 业 Figure 4: Example output of Zhao’s parser. two key differences, though. The first is we output dependency relations instead of constituent structures. Although dependencies can be extracted from the constituent trees of Li (2011), the time complexity of their algorithm is O(n5) while our parser runs in linear time. Secondly, we specify the details of annotating structures of words, with the annotations being made publicly available. Zhao (2009) presented a dependency parser which regards each Chinese character as a word and then analyzes the dependency relations between characters, using ordinary dependency parsing algorithms. Our parser is different in two important ways. The first is we output both part-of-speech tags and labeled dependency relations, both of which were absent in Zhao’s parser. More importantly, the APPEND transition for handling flat words were unseen in previous studies as far as we know. The difference can best be described with an example: For the sentence in Section 3, Zhao’s parser output the result in Figur</context>
</contexts>
<marker>Zhao, 2009</marker>
<rawString>Hai Zhao. 2009. Character-level dependencies in Chinese: Usefulness and learning. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 879–887, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.598311">
Parse, Price and Cut—Delayed Column and Row Generation for Graph
Based Parsers
</note>
<author confidence="0.980122">
Sebastian Riedel David Smith Andrew McCallum
</author>
<affiliation confidence="0.998744">
Department of Computer Science
University of Massachusetts, Amherst
</affiliation>
<email confidence="0.998932">
{riedel,dasmith,mccallum}@cs.umass.edu
</email>
<sectionHeader confidence="0.994783" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999555">
Graph-based dependency parsers suffer from
the sheer number of higher order edges they
need to (a) score and (b) consider during opti-
mization. Here we show that when working
with LP relaxations, large fractions of these
edges can be pruned before they are fully
scored—without any loss of optimality guar-
antees and, hence, accuracy. This is achieved
by iteratively parsing with a subset of higher-
order edges, adding higher-order edges that
may improve the score of the current solu-
tion, and adding higher-order edges that are
implied by the current best first order edges.
This amounts to delayed column and row gen-
eration in the LP relaxation and is guaranteed
to provide the optimal LP solution. For second
order grandparent models, our method consid-
ers, or scores, no more than 6–13% of the sec-
ond order edges of the full model. This yields
up to an eightfold parsing speedup, while pro-
viding the same empirical accuracy and cer-
tificates of optimality as working with the full
LP relaxation. We also provide a tighter LP
formulation for grandparent models that leads
to a smaller integrality gap and higher speed.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929333333333">
Many problems in NLP, and structured prediction in
general, can be cast as finding high-scoring struc-
tures based on a large set of candidate parts. For
example, in second order graph-based dependency
parsing (Kübler et al., 2009) we have to choose a
quadratic number of first order and a cubic number
of second order edges such that the graph is both
high-scoring and a tree. In coreference, we have
to select high-scoring clusters of mentions from an
exponential number of candidate clusters, such that
each mention is in exactly one cluster (Culotta et
al., 2007). In segmentation of citation strings, we
need to consider a quadratic number of possible seg-
ments such that every token is part of exactly one
segment (Poon and Domingos, 2007).
What makes such problems challenging is the
large number of possible parts to consider. This
number not only affects the cost of search or opti-
mization but also slows down the process of scor-
ing parts before they enter the optimization prob-
lem. For example, the cubic grandparent edges in
second-order dependency parsing slow down dy-
namic programs (McDonald and Pereira, 2006), be-
lief propagation (Smith and Eisner, 2008) and LP
solvers (Martins et al., 2009), since there are more
value functions to evaluate, more messages to pass,
or more variables to consider. But to even calculate
the score for each part we need a cubic number of
operations that usually involve expensive feature ex-
traction. This step often becomes a major bottleneck
in parsing, and structured prediction in general.
Candidate parts can often be heuristically pruned.
In the case of dependency parsing, previous work
has used coarse-to-fine strategies where simpler first
order models are used to prune unlikely first or-
der edges, and hence all corresponding higher or-
der edges (Koo and Collins, 2010; Martins et al.,
2009; Riedel and Clarke, 2006). While such meth-
ods can be effective, they are more convoluted, often
require training of addition models as well as tuning
of thresholding hyper-parameters, and usually pro-
vide no guarantees of optimality.
We present an approach that can solve problems
with large sets of candidate parts without consider-
ing all of these parts in either optimization or scor-
</bodyText>
<page confidence="0.942917">
732
</page>
<note confidence="0.802605">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999230512195122">
ing. And in contrast to most pruning heuristics, our
algorithm can give certificates of optimality before
having optimized over, or even scored, all parts. It
does so without the need of auxiliary models or tun-
ing of threshold parameters. This is achieved by a
delayed column and row generation algorithm that
iteratively solves an LP relaxation over a small sub-
set of current candidate parts, and then finds new
candidates that score highly and can be inserted into
the current optimal solution without removing high
scoring existing structure. The latter step subtracts
from the cost of a part the price of resources the part
requires, and is often referred as pricing. Sometimes
parts may score highly after pricing, but are neces-
sary in order to make the current solution feasible.
We add such parts in a step that roughly amounts to
violated cuts to the LP.
We illustrate our approach in terms of a second-
order grandparent model for dependency parsing.
We solve these models by iteratively parsing, pric-
ing, and cutting. To this end we use a variant of the
LP relaxation formulated by Martins et al. (2009).
Our variant of this LP is designed to be amenable to
column generation. It also turns out to be a tighter
outer bound that leads to fewer fractional solutions
and faster runtimes. To find high scoring grandpar-
ent edges without explicitly enumerating all of them,
we prune out a large fraction using factorized upper
bounds on grandparent scores.
Our parse, price and cut algorithm is evaluated
using a non-projective grandparent model on three
languages. Compared to a brute force approach of
solving the full LP, we only score about 10% of the
grandparent edges, consider only 8% in optimiza-
tion, and so observe an increase in parsing speed of
up to 750%. This is possible without loss of opti-
mality, and hence accuracy. We also find that our
extended LP formulation leads to a 15% reduction
of fractional solutions, up to 12 times higher speed,
and generally higher accuracy when compared to the
grandparent formulation of Martins et al. (2009).
</bodyText>
<sectionHeader confidence="0.966692" genericHeader="method">
2 Graph-Based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999935545454546">
Dependency trees are representations of the syntac-
tic structure of a sentence (Nivre et al., 2007). They
determine, for each token of a sentence, the syntac-
tic head the token is modifying. As a lightweight al-
ternative to phrase-based constituency trees, depen-
dency representations have by now seen widespread
use in the community in various domains such as
question answering, machine translation, and infor-
mation extraction.
To simplify further exposition, we now formalize
the task, and mostly follow the notation of Martins et
al. (2009). Consider a sentence x = (to, ti, ... , tn)
where ti, ... , tn correspond to the n tokens of the
sentence, and to is an artificial root token. Let
V °_ 10, ... , n} be a set of vertices corresponding
to the tokens in x, and C C V x V a set of candidate
directed edges. Then a directed graph y C C is a
legal dependency parse if and only if it is a tree over
V rooted at vertex 0. Given a sentence x, we use Y
to denote the set of its legal parses. Note that all of
the above definitions depend on x, but for simplicity
we omit this dependency in our notation.
</bodyText>
<subsectionHeader confidence="0.834141">
2.1 Arc-Factored Models
</subsectionHeader>
<bodyText confidence="0.999795625">
Graph-based models define parametrized scoring
functions that are trained to discriminate between
correct and incorrect parse trees. So called arc-
factored or first order models are the most basic
variant of such functions: they assess the quality of a
tree by scoring each edge in isolation (McDonald et
al., 2005b; McDonald et al., 2005a). Formally, arc-
factored models are scoring functions of the form
</bodyText>
<equation confidence="0.958243">
s (y; x, w) = � s(h,m) (x, w) (1)
(h,m)Ey
</equation>
<bodyText confidence="0.998109">
where w is a weight vector and s(h,m) (x, w) scores
the edge (h, m) with respect to sentence x and
weights w. From here on we will omit both x and w
from our notation if they are clear from the context.
Given such a scoring function, parsing amounts to
solving:
</bodyText>
<equation confidence="0.993201">
� s(h,m)
(h,m)Ey (2)
</equation>
<bodyText confidence="0.851391">
subject to y E Y.
</bodyText>
<subsectionHeader confidence="0.998695">
2.2 Higher Order Models
</subsectionHeader>
<bodyText confidence="0.999003">
Arc-factored models cannot capture higher order de-
pendencies between two or more edges. Higher
order models remedy this by introducing scores
for larger configurations of edges appearing in the
</bodyText>
<equation confidence="0.590722">
maximize
y
</equation>
<page confidence="0.99214">
733
</page>
<bodyText confidence="0.8150578">
tree (McDonald and Pereira, 2006). For example,
in grandparent models, the score of a tree also in-
cludes a score sgp
(g,p,c) for each grandparent-parent-
child triple (g, p, c):
</bodyText>
<equation confidence="0.963410333333333">
�
s (y) =
(h,m)Ey
</equation>
<bodyText confidence="0.999296857142857">
There are other variants of higher order models
that include, in addition to grandparent triples, pairs
of siblings (adjacent or not) or third order edges.
However, to illustrate our approach we will focus
on grandparent models and note that most of what
we present can be generalized to other higher order
models.
</bodyText>
<subsectionHeader confidence="0.998765">
2.3 Feature Templates
</subsectionHeader>
<bodyText confidence="0.999982571428571">
For our later exposition the factored and
parametrized nature of the scoring functions
will be crucial. In the following we therefore
illustrate this property in more detail.
The scoring functions for arcs or higher order
edges usually decompose into a sum of feature tem-
plate scores. For example, the grandparent edge
</bodyText>
<equation confidence="0.9654034">
score sgp
(g,p,c) is defined as
sgp,t (4)
(g,p,c)
tETgp
</equation>
<bodyText confidence="0.907866">
where Tgp is the set of grandparent templates, and
each template t E Tgp defines a scoring func-
tion sgp,t
(g,p,c) to assess a specific property of the
grandparent-parent-child edge (g, p, c).
The template scores again decompose. Consider-
ing grandparent scores, we get
</bodyText>
<equation confidence="0.9649605">
st (g,p,c) °_ w� t ft �ht g, ht p, ht c, dt � (5)
g,p,c
</equation>
<bodyText confidence="0.838912">
where ht i is an attribute of token ti, say h101
</bodyText>
<equation confidence="0.93926">
i =
</equation>
<bodyText confidence="0.6372108">
Part-of-Speech (ti). The term dtg,p,c corresponds to
a representation of the relation between tokens cor-
responding to g, p and g. For example, for template
101 it could return their relative positions to each
other:
</bodyText>
<equation confidence="0.6656445">
d101
g,p,c g (ff [g &gt; p] , f [g &gt; c] ,f [p &gt; c]) . (6)
</equation>
<bodyText confidence="0.99951175">
The feature function ft maps the representations
of g, p and c into a vector space. For the purposes of
our work this mapping is not important, and hence
we omit details.
</bodyText>
<subsectionHeader confidence="0.998941">
2.4 Learning
</subsectionHeader>
<bodyText confidence="0.999913666666667">
The scoring functions we consider are parametrized
by a family of per-template weight vectors w =
(wt)tET. During learning we need to estimate w
such that our scoring functions learns to differenti-
ate between correct and incorrect parse trees. This
can be achieved in many ways: large margin train-
ing, maximizing conditional likelihood, or variants
in between. In this work we follow Smith and Eis-
ner (2008) and train the models with stochastic gra-
dient descent on the conditional log-likelihood of the
training data, using belief propagation in order to
calculate approximate gradients.
</bodyText>
<sectionHeader confidence="0.984243" genericHeader="method">
3 LP and ILP Formulations
</sectionHeader>
<bodyText confidence="0.999923333333333">
Riedel and Clarke (2006) showed that dependency
parsing can be framed as Integer Linear Pro-
gram (ILP), and efficiently solved using an off-the-
shelf optimizer if a cutting plane approach is used.1
Compared to tailor made dynamic programs, such
generic solvers give the practitioner more modeling
flexibility (Martins et al., 2009), albeit at the cost
of efficiency. Likewise, compared to approximate
solvers, ILP and Linear Program (LP) formulations
can give strong guarantees of optimality. The study
of Linear LP relaxations of dependency parsing has
also lead to effective alternative methods for parsing,
such as dual decomposition (Koo et al., 2010; Rush
et al., 2010). As we see later, the capability of LP
solvers to calculate dual solutions is also crucial for
efficient and exact pruning. Note, however, that dy-
namic programs provide dual solutions as well (see
section 4.5 for more details).
</bodyText>
<subsectionHeader confidence="0.995609">
3.1 Arc-Factored Models
</subsectionHeader>
<bodyText confidence="0.9999121">
To represent a parse y E Y we first introduce an
vector of variables z °_ (za)a where za is 1 if a E y
and 0 otherwise. With this representation parsing
amounts to finding a vector z that corresponds to a
legal parse tree and that maximizes Ea zasa. One
way to achieve this is to search through the convex
hull of all legal incidence vectors, knowing that any
linear objectives would take on its maximum on one
of the hull’s vertices. We will use i to denote this
convex hull of incidence vectors of legal parse trees,
</bodyText>
<footnote confidence="0.964257">
1Such as the highly efficient and free-for-academic-use
Gurobi solver.
</footnote>
<equation confidence="0.9909365">
s(h,m) + � sgp (3)
(g,p)Ey,(p,c)Ey (g,p,c)
�
A
sgp
(g,p,c) —
</equation>
<page confidence="0.979667">
734
</page>
<bodyText confidence="0.99822085">
and call i the arborescence polytope (Martins et al.,
2009). The Minkowski-Weyl theorem tells us that i
can be represented as an intersection of halfspaces,
or constraints, i = {zJAz &lt; b}. Hence optimal
dependency parsing, in theory, can be addressed us-
ing LPs.
However, it is difficult to describe i with a com-
pact number of constraints and variables that lend
themselves to efficient optimization. In general we
therefore work with relaxations, or outer bounds, on
i. Such outer bounds are designed to cut off all
illegal integer solutions of the problem, but still al-
low for fractional solutions. In case the optimum is
achieved at an integer vertex of the outer bound, it
is clear that we have found the optimal solution to
the original problem. In case we find a fractional
point, we need to map it onto i (e.g., by projection
or rounding). Alternatively, we can use the outer
bound together with 0/1 constraints on z, and then
employ an ILP solver (say, branch-and-bound) to
find the true optimum. Given the NP-hardness of
ILP, this will generally be slow.
In the following we will present the outer bound
i _D i proposed by Martins et al. (2009).
Compared to the representation Riedel and Clarke
(2006), this bound has the benefit a small polyno-
mial number of constraints. Note, however, that of-
ten exponentially many constraints can be efficiently
handled if polynomial separation algorithms exists,
and that such representations can lead to tighter
outer bounds.
The constraints we employ are:
No Head For Root In a dependency tree the root
node never has a head. While this could be captured
through linear constraints, it is easier to simply re-
strict the candidate set C to never contain edges of
the form (·, 0).
Exactly One Head for Non-Roots Any non-root
token has to have exactly one head token. We can
enforce this property through the set of constraints:
</bodyText>
<equation confidence="0.861734">
Xm &gt; 0 : z(h,m) = 1. (OneHead)
h
</equation>
<bodyText confidence="0.997554">
No Cycles A parse tree cannot have cycles. This is
equivalent, together with the head constraints above,
to enforcing that the tree be fully connected. Mar-
tins et al. (2009) capture this connectivity constraint
using a single commodity flow formulation. This
requires the introduction of flow variables 0i°_
(φa)aEC. By enforcing that token 0 has n outgoing
flow,
</bodyText>
<equation confidence="0.996896">
X φ(0,m) = n, (Source)
m&gt;0
that any other token consumes one unit offlow,
t &gt; 0 : X Xφ(h,t) − φ(t,m) = 1 (Consume)
h m&gt;0
</equation>
<bodyText confidence="0.830329">
and that flow is zero on disabled arcs
</bodyText>
<equation confidence="0.9899355">
φ(h,m) &lt; nz(h,m), (NoFlow)
connectivity can be ensured.
</equation>
<bodyText confidence="0.99886">
Assuming we have such a representation, parsing
with an LP relaxation amounts to solving
</bodyText>
<listItem confidence="0.465129">
subject to A &lt; b.
</listItem>
<equation confidence="0.9616645">
� z �
� (7)
</equation>
<subsectionHeader confidence="0.998832">
3.2 Higher Order Models
</subsectionHeader>
<bodyText confidence="0.997303578947368">
The 1st-Order LP can be easily extended to capture
second (or higher) order models. For for the case
of grandparent models, this amounts to introduc-
ing another class of variables, zgp
g,p,c, that indicate if
the parse contains both the edge (g, p) and the edge
(p, c). With the help of the indicators zgp we can rep-
resent the second order objective as a linear function.
We now need an outer bound on the convex hull of
vectors (z, zgp) where z is a legal parse tree and zgp
is a consistent set of grandparent indicators. We will
refer to this convex hull as the grandparent polytope
igp.
We can re-use the constraints A of section 3.1 to
ensure that z is in i. To make sure zgp is consistent
with z, Martins et al. (2009) linearize the equiva-
lence zgp
g,p,c zg,p n zp,c we know to hold for legal
incidence vectors, yielding
</bodyText>
<equation confidence="0.927207">
g, p, c : z(g,p) + z(p,c) − zgp
(g,p,c) &lt; 1 (ArcGP)
and
g, p, c : z(g,p) � zgp
(g,p,c), z(p,c) � zgp
(g,p,c) (GPArc)
</equation>
<bodyText confidence="0.998839">
There are additional constraints we know to hold in
igp. First, we know that for any active edge (p, c) E
</bodyText>
<figure confidence="0.8017314">
maximize
Z&gt;o
zasa
X
aEA
</figure>
<page confidence="0.981061">
735
</page>
<bodyText confidence="0.9231336">
y with p &gt; 0 there is exactly one grandparent edge
(g, p, c). Likewise, for an inactive edge (p, c) V y
there must be no grandparent edge (g, p, c). This
can be captured through the constraint:
responding dual problem:
</bodyText>
<figure confidence="0.864619">
Primal
maximize
Z&gt;o
sTz
Dual
minimize ATb
a&gt;o
�p &gt; 0, c : gp subject to Az &lt; b subject to ATA &gt; s.
g z(g,p,c) = z(p,c). (OneGP)
</figure>
<bodyText confidence="0.97799935">
We also know that if an edge (g, p) in inactive,
there must not be any grandparent edge (g, p, c) that
goes through (g, p):
�g, p : gp (
c z(g,p,c) nz(g,p) . (NoGP)
It can be easily shown that for integer solu-
tions the constraints ArcGP and GPArc of Martins
et al. (2009) are sufficient conditions for consis-
tency between z and zgp. It can equally be shown
that the same holds for the constraints OneGP and
NoGP. However, when working with LP relax-
ations, the two polytopes have different fractional
vertices. Hence, by combining both constraint sets,
we can get a tighter outer bound on the grandparent
polytope igp. In section 6 we show empirically that
this combined polytope in fact leads to fewer frac-
tional solutions. Note that when using the union of
all four types of constraints, the NoGP constraint is
implied by the constraint GPArc (left) by summing
over c on both sides, and can hence be omitted.
</bodyText>
<sectionHeader confidence="0.8245" genericHeader="method">
4 Parse, Price and Cut
</sectionHeader>
<bodyText confidence="0.99991">
We now introduce our parsing algorithm. To this
end, we first give a general description of column
and row generation for LPs; then, we illustrate how
these techniques can be applied to dependency pars-
ing.
</bodyText>
<subsectionHeader confidence="0.996699">
4.1 Column and Row Generation
</subsectionHeader>
<bodyText confidence="0.98993674">
LPs often have too many variables and constraints
to be efficiently solved. In such cases delayed
column and row generation can substantially re-
duce runtime by lazily adding variables only when
needed (Gilmore and Gomory, 1961; Lübbecke and
Desrosiers, 2004).
To illustrate column and row generation let us
consider the following general primal LP and its cor-
Say you are given a primal feasible z&apos; and a dual fea-
sible A&apos; for which complementary slackness holds:
for all variables i we have z&apos; i &gt; 0 ==&gt;. si = E j A&apos;jai,j
and for all constraints j we have A&apos;j &gt; 0 ==&gt;. bj =
Ei z&apos;iai,j. In this case it is easy to show that z&apos; is
an optimal primal solution, A&apos; and optimal dual so-
lution, and that both objectives meet at these val-
ues (Bertsekas, 1999).
The idea behind delayed column and row gener-
ation is to only consider a small subset of variables
(or columns) I and subset of constraints (or rows) J.
Optimizing over this restricted problem, either with
an off-the-shelf solver or a more specialized method,
yields the pair (z&apos;I, A&apos; ) of partial primal and dual
J
solutions. This pair is feasible and complementary
with respect to variables I and constraints J. We
can extend it to a solution (z&apos;, y&apos;) over all variables
and constraints by heuristically setting the remain-
ing primal and dual variables. If it so happens that
(z&apos;, y&apos;) is feasible and complementary for all vari-
ables and constraints, we have found the optimal so-
lution. If not, we add the constraints and variables
for which feasibility and slackness are violated, and
resolve the new partial problem.
In practice, the uninstantiated primal and dual
variables are often set to 0. In this case complemen-
tary slackness holds trivially, and we only need to
find violated primal and dual constraints. For primal
constraints, Ei ziai,j &lt; bi, searching for violating
constraints j is the well-known separation step in
cutting plane algorithms. For the dual constraints,
Ej Ajai,j &gt; si, the same problem is referred to
as pricing. Pricing is often framed as searching for
all, or some, variables i with positive reduced cost
ri °= si −Ej Ajai,j. Note that while these problems
are, naturally, dual to each other, they can have very
different flavors. When we assess dual constraints
we need to calculate a cost si for variable i, and
usually this cost would be different for different i.
For primal constraints the corresponding right-hand-
sides are usually much more homogenous.
</bodyText>
<page confidence="0.991822">
736
</page>
<table confidence="0.519145">
Algorithm 1 Parse, Price and Cut.
Require: Initial candidate edges and hyperedges P.
Ensure: The optimal z.
</table>
<listItem confidence="0.987542428571428">
1: repeat
2: z, A +— parse(P)
3: N +— price(A)
4: M +— cut(z)
5: P + —P U N U M
6: until N = 0 ∧ M = 0
7: return z
</listItem>
<bodyText confidence="0.999929277777778">
The reduced cost ri = si − Ej Ajai,j has sev-
eral interesting interpretations. First, intuitively it
measures the score we could gain by setting zi = 1,
and subtracts an estimate of what we would loose
because zi = 1 may compete with other variables
for shared resources (constraints). Second, it cor-
responds to the coefficient of zi in the Lagrangian
L (A, z) °= sTz + A [b − Az]. For any A, Uzi=k =
maxz≥0,zi=k L (A, z) is an upper bound on the best
possible primal objective with zi = k. This means
that ri = Uzi=1 − Uzi=0 is the difference between
an upper bound that considers zi = 1, and one that
considers zi = 0. The tighter the bound Uzi=0 is,
the closer ri is to an upper bound on the maximal
increase we can get for setting zi to 1. At conver-
gence of column generation, complementary slack-
ness guarantees that Uzi=0 is tight for all z0 i = 0, and
hence ri is a true an upper bound.
</bodyText>
<subsectionHeader confidence="0.989301">
4.2 Application to Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.993671826086957">
The grandparent formulation in section 3.2 has a cu-
bic number of variables zhg,p,ci as well as a cubic
number of constraints. For longer sentences this
number can slow us down in two ways. First, the
optimizer works with a large search space, and will
naturally become slower. Second, for every grand-
parent edge we need to calculate the score shg,p,ci,
and this calculation can often be a major bottleneck,
in particular when using complex feature functions.
To overcome this bottleneck, our parse, price and cut
algorithm, as shown in algorithm 1, uses column and
row generation. In particular, it lazily instantiates
the grandparent edge variables zgp, and the cor-
hg,p,ci
responding cubic number of constraints. All unin-
stantiated variables are implicitly set to 0.
The algorithm requires some initial set of vari-
ables to start with. In our case this set P contains all
first-order edges (h, m) in the candidate set C, and
for each of these one grandparent edge (0, h, m).
The primary purpose of these grandparent edges is
to ensure feasibility of the OneGP constraints.
In step 2, the algorithm parses with the current
set of candidates P by solving the corresponding LP
relaxation. The LP contains all columns and con-
straints that involve the edges and grandparent edges
of P. The solver returns both the best primal solu-
tion z (for both edges and grandparents), and a com-
plementary dual solution A.
In step 3 the dual variables A are used to find unin-
stantiated grandparent edges (g, p, c) with positive
reduced cost. The price routine returns such edges
in N. In step 4 the primal solution is inspected for
violations of constraint ArcGP. The cut routine per-
forms this operation, and returns M, the set of edges
(g, p, c) that violate ArcGP.
In step 5 the algorithm converges if no more con-
straint violations, or promising new columns, can
be found. If there have been violations (M =� 0)
or promising columns (N =� 0), steps 2 to 4 are
repeated, with the newly found parts added to the
problem. Note that LP solvers can be efficiently
warm-started after columns and rows have been
added, and hence the cost of calls to the solver in
step 2 is substantially reduced after the first itera-
tion.
</bodyText>
<subsectionHeader confidence="0.99757">
4.3 Pricing
</subsectionHeader>
<bodyText confidence="0.914095181818182">
In the pricing step we need to efficiently find a
set of grandparent edge variables zgpwith posi-
hg,p,ci
tive reduced cost, or the empty set if no such vari-
ables exist. Let �OneGP
hp,ci be the dual variables for
the OneGP constraints and ANoGP the duals for con-
hg,pi
straints NoGP. Then for the reduced cost of zgp
hg,p,ci
we know that:
</bodyText>
<equation confidence="0.993138666666667">
rhg,p,ci = shg,p,ci − AOneGP
hp,ci − ANoGP
hg,pi &apos; (8)
</equation>
<bodyText confidence="0.997107">
Notice that the duals for the remaining two con-
straints ArcGP and GPArc do not appear in this
equation. This is valid because we can safely set
their duals to zero without violating dual feasibility
or complementary slackness of the solution returned
by the solver.
</bodyText>
<page confidence="0.992299">
737
</page>
<subsubsectionHeader confidence="0.450714">
4.3.1 Upper Bounds for Efficient Pricing
</subsubsectionHeader>
<bodyText confidence="0.956945142857143">
A naive pricing implementation would exhaus-
tively iterate over all hg, p, ci and evaluate rhg,p,ci
for each. In this case we can still substantially re-
duce the number of grandparent variables that en-
ter the LP, provided many of these variables have
non-positive reduced cost. However, we still need to
calculate the score shg,p,ci for each hg, p, ci, an ex-
pensive operation we hope to avoid. In the follow-
ing we present an upper bound on the reduced cost,
¯rgp ≥ rgp , which decomposes in a way that
hg,p,ci hg,p,ci
allows for more efficient search. Using this bound,
we find all new grandparent edges N¯ for which this
upper bound is positive:
</bodyText>
<equation confidence="0.99126725">
n g
(, P, c ) I - (
1V r )
hggP Pe) &gt; OJ1 . 9
</equation>
<bodyText confidence="0.999879">
Next we prune away all but the grandparent edges
for which the exact reduced cost is positive:
</bodyText>
<equation confidence="0.776521142857143">
N ← N\ {e : rgp
e &gt; 0}. (10)
Our bound ¯rgp
hg,p,ci on the reduced cost of hg, p, ci
is based on an upper bound ¯sgp
hg,p,·i ≥ maxc sgp
hg,p,ci
</equation>
<bodyText confidence="0.969174">
on the grandparent score involving hg, pi as grand-
parent and parent, and the bound ¯sgp ≥
</bodyText>
<equation confidence="0.69439625">
h·,p,ci
maxg sgp
hg,p,ci on the grandparent score involving
hp, ci as parent and child. Concretely, we have
� �
¯rgp ° min ¯sgp ¯sgp − λOneGP − λNoGP
hg,p,ci hg,p,·i, h·,p,ci hp,ci hg,pi
(11)
</equation>
<bodyText confidence="0.902061">
To find edges hg, p, ci for which this bound is
positive, we can filter out all edges hp, ci such that
sgp
h·,p,ci − λOneGP hp,ciis non-positive. This is possible be-
cause NoGP is a ≤ constraint and therefore λNoGP hg,pi ≥
0.2 Hence ¯rgp
</bodyText>
<equation confidence="0.893914">
hg,p,ci is at most ¯sgp
h·,p,ci − λOneGP
</equation>
<bodyText confidence="0.960491875">
hp,ci .This
filtering step cuts off a substantial number of edges,
and is the main reason why can avoid scoring all
edges.
Next we filter, for each remaining hp, ci, all pos-
sible grandparents g according to the definition of
¯rgp
hg,p,ci. This again allows us to avoid calling the
</bodyText>
<footnote confidence="0.8887495">
2Notice that in section 4.1 we discussed the LP dual in
case were all constraints are inequalities. When equality con-
straints are used, the corresponding dual variables have no sign
constraints. Hence we could not make the same argument for
</footnote>
<equation confidence="0.362644">
AOneGP
����� .
</equation>
<bodyText confidence="0.847876272727273">
grandparent scoring function on hg, p, ci, and yields
the candidate set ¯N. Only if ¯rgp
hg,p,ci is positive do we
have to evaluate the exact reduced cost and score.
4.3.2 Upper Bounds on Scores
What remains to be done is the calculation of up-
per bounds ¯sgp
hg,p,·i and ¯sgp
h·,p,ci. Our bounds factor
into per-template bounds according to the definitions
in section 2.3. In particular, we have
</bodyText>
<equation confidence="0.867172">
gp,t (12 )
sh·,p,ci
</equation>
<bodyText confidence="0.987508">
where ¯sth·,p,ci is a per-template upper bound defined
as
</bodyText>
<equation confidence="0.996269666666667">
¯sgp,t ° max
h·,p,civ∈range(ht)
e∈range(dt)
</equation>
<bodyText confidence="0.9994256">
That is, we maximize over all possible attribute val-
ues v any token g could have, and any possible rela-
tion e a token g can have to p and c.
Notice that these bounds can be calculated offline,
and hence amortize after deployment of the parser.
</bodyText>
<subsectionHeader confidence="0.907001">
4.3.3 Tightening Duals
</subsectionHeader>
<bodyText confidence="0.9794496">
To price variables, we use the duals returned by
the solver. This is a valid default strategy, but may
lead to A with overcautious reduced costs. Note,
however, that we can arbitrary alter A to minimize
reduced costs of uninstantiated variables, as long as
we ensure that feasibility and complementary slack-
ness are maintained for the instantiated problem.
We use this flexibility for increasing λOneGP
hp,ci , and
hence lowering reduced costs zgp
</bodyText>
<equation confidence="0.923428">
hg,p,ci for all tokens c.
Assume that zhp,ci = 0 and let rhp,ci = λOneGP
hp,ci + K
</equation>
<bodyText confidence="0.952839090909091">
be the current reduced cost for zhp,ci in the instanti-
ated problem. Here K is a value depending on shp,ci
and the remaining constraints zhp,ci is involved in.
We know that rhp,ci ≤ 0 due to dual feasibility
and hence rhp,ci may be 0, but note that rhp,ci &lt; 0 in
many cases. In such cases we can increase λOneGP
hp,ci
to −K and get rhp,ci = 0. With respect to zhp,ci this
maintains dual feasibility (because rhp,ci ≤ 0) and
complementary slackness (because zhp,ci = 0). Fur-
thermore, with respect to the zgp for all tokens c
</bodyText>
<equation confidence="0.802344166666667">
hg,p,ci
this also maintains feasibility (because the increased
λOneGP
hp,ci appears with negative sign in 8) and com-
plementary slackness (because zgp
hg,p,ci = 0 due to
zhp,ci = 0).
X
¯sgp
h·,p,ci �
t∈Tgp
w&gt;t ft (v, ht p, htc, e~ . (13)
</equation>
<page confidence="0.994072">
738
</page>
<subsectionHeader confidence="0.983086">
4.4 Separation
</subsectionHeader>
<bodyText confidence="0.999203882352941">
What happens if both zhg,pi and zhp ci are active
while zgphg,p,ci is still implicitly set to 0? In this case
we violate constraint ArcGP. We could remedy this
by adding the cut zhg,pi + zhp,ci &lt; 1, resolve the
LP, and then use the dual variable corresponding to
this constraint to get an updated reduced cost rhg,p,ci.
However, in practice we found this does not happen
as often, and when it does, it is cheaper for us to add
the corresponding column rhg,p,ci right away instead
of waiting to the next iteration to price it.
To find all pairs of variables for zhg,pi + zhp,ci &lt; 1
is violated, we first filter out all edges (h, m) for
which zhh,mi = 0 as these automatically satisfy
any ArcGP constraint they appear in. Now for each
zhg,pi &gt; 0 all zhp,ci &gt; 0 are found, and if their sum
is larger than 1, the corresponding grandparent edge
(g, p, c) is returned in the result set.
</bodyText>
<subsectionHeader confidence="0.998227">
4.5 Column Generation in Dynamic Programs
</subsectionHeader>
<bodyText confidence="0.99993075">
Column and Row Generation can substantially re-
duce the runtime of an off-the-shelf LP solver, as
we will find in section 6. Perhaps somewhat sur-
prisingly, it can also be applied in the context of dy-
namic programs. It is well known that for each dy-
namic program there is an equivalent polynomial LP
formulation (Martin et al., 1990). Roughly speak-
ing, in this formulation primal variables correspond
to state transitions, and dual variables to value func-
tions (e.g., the forward scores in the Viterbi algo-
rithm).
In pilot studies we have already used DCG to
speed up (exact) Viterbi on linear chains (Belanger
et al., 2012). We believe it could be equally applied
to dynamic programs for higher order dependency
parsing.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999945421052632">
Our work is most similar in spirit to the relaxation
method presented by Riedel and Smith (2010) that
incrementally adds second order edges to a graphi-
cal model based on a gain measure—the analog of
our reduced cost. However, they always score every
higher order edge, and also provide no certificates of
optimality.
Several works in parsing, and in MAP inference
in general, perform some variant of row genera-
tion (Riedel and Clarke, 2006; Tromble and Eis-
ner, 2006; Sontag and Jaakkola, 2007; Sontag et al.,
2008). However, none of the corresponding methods
lazily add columns, too. The cutting plane method
of Riedel (2008) can omit columns, but only if their
coefficient is negative. By using the notion of re-
duced costs we can also omit columns with positive
coefficient. Niepert (2010) applies column gener-
ation, but his method is limited to the case of k-
Bounded MAP Inference.
Several ILP and LP formulations of dependency
parsing have been proposed. Our formulation is in-
spired by Martins et al. (2009), and hence uses fewer
constraints than Riedel and Clarke (2006). For the
case of grandparent edges, our formulation also im-
proves upon the outer bound of Martins et al. (2009)
in terms of speed, tightness, and utility for column
generation. Other recent LP relaxations are based
on dual decomposition (Rush et al., 2010; Koo et
al., 2010; Martins et al., 2011). These relaxations
allow the practitioner to utilize tailor-made dynamic
programs for tractable substructure, but still every
edge needs to be scored. Given that column gener-
ation can also be applied in dynamic programs (see
section 4.5), our algorithm could in fact accelerate
dual decomposition parsing as well.
Pruning methods are a major part of many struc-
tured prediction algorithms in general, and of pars-
ing algorithms in particular (Charniak and Johnson,
2005; Martins et al., 2009; Koo and Collins, 2010;
Rush and Petrov, 2012). Generally these meth-
ods follow a coarse-to-fine scheme in which sim-
pler models filter out large fractions of edges. Such
methods are effective, but require tuning of thresh-
old parameters, training of additional models, and
generally lead to more complex pipelines that are
harder to analyze and have fewer theoretical guar-
antees.
A* search (Ahuja et al., 1993) has been used
to search for optimal parse trees, for example by
Klein and Manning (2003) or, for dependency pars-
ing, by Dienes et al. (2003). There is a direct rela-
tion between both A* and Column Generation based
on an LP formulation of the shortest path problem.
Roughly speaking, in this formulation any feasible
dual assignments correspond to a consistent (and
thus admissible) heuristic, and the corresponding re-
duced costs can be used as edge weights. Run-
</bodyText>
<page confidence="0.99629">
739
</page>
<bodyText confidence="0.999873857142857">
ning Dijkstra’s algorithm with these weights then
amounts to A*. Column generation for the shortest
path problem can then be understood as a method to
lazily construct a consistent heuristic. In every step
this method finds edges for which consistency is vi-
olated, and updates the heuristic such that all these
edges are consistent.
</bodyText>
<sectionHeader confidence="0.998215" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99997655">
We claim that LP relaxations for higher order pars-
ing can be solved without considering, and scoring,
all candidate higher order edges. In practice, how
many grandparent edges do we need to score, and
how many do we need to add to the optimization
problem? And what kind of reduction in runtime
does this reduction in edges lead to?
We have also pointed out that our outer bound on
the grandparent polytope of legal edge and grand-
parent vectors is tighter than the one presented by
Martins et al. (2009). What effect does this bound
have on the number of fractional solutions and the
overall accuracy?
To answer these questions we will focus on a set
of non-projective grandparent models, but point out
that our method and formulation can be easily ex-
tended to projective parsing as well as other types
of higher order edges. We use the Danish test data
of Buchholz and Marsi (2006) and the Italian and
Hungarian test datasets of Nivre et al. (2007).
</bodyText>
<subsectionHeader confidence="0.999884">
6.1 Impact of Price and Cut
</subsectionHeader>
<bodyText confidence="0.999950918918919">
Table 1 compares brute force optimization (BF) with
the full model, in spirit of Martins et al. (2009),
to running parse, price and cut (PPC) on the same
model. This model contains all constraints presented
in 3.2. The table shows the average number of
parsed sentences per second, the average objective,
number of grandparent edges scored and added, all
relative to the brute force approach. We also present
the average unlabeled accuracy, and the percentage
of sentences with integer solutions. This number
shows us how often we not only found the optimal
solution to the LP relaxation, but also the optimal
solution to the full ILP.
We first note that both systems achieve the same
objective, and therefore, also the same accuracy.
This is expected, given that column and row gen-
eration are known to yield optimal solutions. Next
we see that the number of grandparent edges scored
and added to the problem is reduced to 5–13% of the
full model. This leads to up to 760% improvement
in speed. This improvement comes for free, without
any sacrifice in optimality or guarantees. We also
notice that in all cases at least 97% of the sentences
have no fractional solutions, and are therefore opti-
mal even with respect to the ILP. Table 1 also shows
that our bounds on reduced costs are relatively tight.
For example, in the case of Italian we score only
one percent more grandparent edges than we actu-
ally need to add.
Our fastest PCC parser processes about one sen-
tence per second. This speed falls below the reported
numbers of Martins et al. (2009) of about 0.6 sec-
onds per sentence. Crucially, however, in contrast to
their work, our speed is achieved without any first-
order pruning. In addition, we expect further im-
provements in runtime by optimizing the implemen-
tation of our pricing algorithm.
</bodyText>
<subsectionHeader confidence="0.996365">
6.2 Tighter Grandparent Polytope
</subsectionHeader>
<bodyText confidence="0.999912058823529">
To investigate how the additional grandparent con-
straints in section 3.2 help, we compare three mod-
els, this time without PPC. The first model follows
Martins et al. (2009) and uses constraints ArcGP and
GPArc only. The second model uses only constraints
OneGP and NoGP. The final model incorporates all
four constraints.
Table 2 shows speed relative to the baseline model
with constraints ArcGP and GPArc, as well as the
percentage of integer solutions and the average un-
labeled accuracy—all for the Italian and Hungarian
datasets. We notice that the full model has less frac-
tional solutions than the partial models, and either
substantially (Italian) or slightly (Hungarian) faster
runtimes than ArcGP+GPArc. Interestingly, both
sets of constraints in isolation perform worse, in par-
ticular the OneGP and NoGP model.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999248">
We have presented a novel method for parsing in
second order grandparent models, and a general
blueprint for more efficient and optimal structured
prediction. Our method lazily instantiates candidate
parts based on their reduced cost, and on constraint
</bodyText>
<page confidence="0.985166">
740
</page>
<table confidence="0.99908075">
Italian Hungarian Danish
BF PPC BF PPC BF PPC
Sent./sec. relative to BF 100% 760% 100% 380% 100% 390%
GPs Scored relative to BF 100% 6% 100% 12% 100% 13%
GPs Added relative to BF 100% 5% 100% 7% 100% 7%
Objective rel. to BF 100% 100% 100% 100% 100% 100%
% of Integer Solutions 98% 98% 97% 97% 97% 97%
Unlabeled Acc. 88% 88% 81% 81% 88% 88%
</table>
<tableCaption confidence="0.99761">
Table 1: Parse, Price and Cut (PPC) vs Brute Force (BF). Speed is the number of sentences per second,
relative to the speed of BF. Objective, GPs scored and added are also relative to BF.
</tableCaption>
<table confidence="0.9852485">
GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 1000% 1200%
% Integer 77% 9% 98%
Unlabeled Acc. 87% 85% 88%
(a) Italian
GPArc+ OneGP+
Constraints ArcGP NoGP All
Sent./sec. 100% 162% 105%
% Integer 71% 3% 97%
Unlabeled Acc. 80% 77% 81%
(b) Hungarian
</table>
<tableCaption confidence="0.639703333333333">
Table 2: Different outer bounds on the grandpar-
ent polytope, for nonprojective parsing of Italian and
Danish.
</tableCaption>
<bodyText confidence="0.9997245">
violations. This allows us to discard a large fraction
of parts during both scoring and optimization, lead-
ing to nearly 800% speed-ups without loss of accu-
racy and certificates. We also present a tighter bound
on the grandparent polytope that is useful in its own
right.
Delayed column and row generation is very useful
when solving large LPs with off-the-shelf solvers.
Given the multitude of work in NLP that uses LPs
and ILPs in this way (Roth and Yih, 2004; Clarke
and Lapata, 2007), we hope that our approach will
prove itself useful for other applications. We stress
that this approach can also be used when working
with dynamic programs, as pointed out in section
4.5, and therefore also in the context of dual de-
composition. This suggests even wider applicabil-
ity, and usefulness in various structured prediction
problems.
The underlying paradigm could also be useful for
more approximate methods. In this paradigm, al-
gorithms maintain an estimate of the cost of certain
resources (duals), and use these estimates to guide
search and the propose new structures. For exam-
ple, a local-search based dependency parser could
estimate how contested certain tokens, or edges, are,
and then use these estimates to choose better next
proposals. The notion of reduced cost can give guid-
ance on what such estimates should look like.
</bodyText>
<sectionHeader confidence="0.993974" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999927083333333">
This work was supported in part by the Center for
Intelligent Information Retrieval and the Univer-
sity of Massachusetts and in part by UPenn NSF
medium IIS-0803847. We gratefully acknowledge
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.
</bodyText>
<sectionHeader confidence="0.998431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9733665">
Ravindra K. Ahuja, Thomas L. Magnanti, and James B.
Orlin. 1993. Network Flows: Theory, Algorithms, and
Applications. Prentice Hall, 1 edition, February.
David Belanger, Alexandre Passos, Sebastian Riedel, and
Andrew McCallum. 2012. A column generation ap-
proach to connecting regularization and map infer-
ence. In Inferning: Interactions between Inference
and Learning, ICML 2012 Workshop.
</reference>
<page confidence="0.988245">
741
</page>
<reference confidence="0.999761371428572">
Dimitri P. Bertsekas. 1999. Nonlinear Programming.
Athena Scientific, 2nd edition, September.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natu-
ral Language Learning (CoNLL’ 06), CoNLL-X ’06,
pages 149–164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL ’05),
pages 173–180.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
’07), pages 1–11.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ’07), pages 81–88.
Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical a-star dependency parsing. In Pro-
ceedings of the workshop on Prospects and Advances
of the Syntax/Semantics Interface, Nancy, 2003, pp.85-
89.
P.C. Gilmore and R.E. Gomory. 1961. A linear program-
ming approach to the cutting-stock problem. Opera-
tions research, pages 849–859.
Dan Klein and Christopher D. Manning. 2003. A* pars-
ing: Fast exact viterbi parse selection. In Proceedings
of the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL ’03), pages 119–126.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL ’11).
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with nonprojective head automata. In
Proceedings of the Conference on Empirical methods
in natural language processing (EMNLP ’10).
Sandra Kübler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan &amp; Claypool
Publishers.
Marco Lübbecke and Jacques Desrosiers. 2004. Selected
topics in column generation. Operations Research,
53:1007–1023.
R. Kipp Martin, Ronald L. Rardin, and Brian A. Camp-
bell. 1990. Polyhedral characterization of discrete
dynamic programming. Oper. Res., 38(1):127–138,
February.
André F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP (ACL
’09), pages 342–350, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
André F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,
and Mário A. T. Figueiredo. 2011. Dual decomposi-
tion with many overlapping components. In Proceed-
ings of the Conference on Empirical methods in natu-
ral language processing (EMNLP ’11), EMNLP ’11,
pages 238–249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proceedings of the 11th Conference of the European
Chapter of the ACL (EACL ’06), pages 81–88.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ’05), pages
91–98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In HLT-EMNLP, 2005.
Mathias Niepert. 2010. A delayed column generation
strategy for exact k-bounded map inference in markov
logic networks. In Proceedings of the 26th Annual
Conference on Uncertainty in AI (UAI ’10), pages
384–391, Corvallis, Oregon. AUAI Press.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Conference on Em-
pirical Methods in Natural Language Processing and
Natural Language Learning, pages 915—932.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
22nd AAAI Conference on Artificial Intelligence (AAAI
’07), pages 913–918.
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’06), pages 129–137.
Sebastian Riedel and David A. Smith. 2010. Relaxed
marginal inference and its application to dependency
</reference>
<page confidence="0.968121">
742
</page>
<reference confidence="0.999728955555556">
parsing. In Joint Human Language Technology Con-
ference/Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL ’10), pages 760–768, Los Angeles, Cal-
ifornia, June. Association for Computational Linguis-
tics.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for markov logic. In Pro-
ceedings of the 24th Annual Conference on Uncer-
tainty in AI (UAI ’08), pages 468–475.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of the 8th Conference on Computational
Natural Language Learning (CoNLL’ 04), pages 1–8.
Alexander Rush and Slav Petrov. 2012. Vine pruning for
efficient multi-pass dependency parsing. In Joint Hu-
man Language Technology Conference/Annual Meet-
ing of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL ’12).
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’10).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 145–156, Hon-
olulu, October.
D. Sontag and T. Jaakkola. 2007. New outer bounds on
the marginal polytope. In Advances in Neural Infor-
mation Processing Systems (NIPS ’07), pages 1393–
1400.
David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proceedings of the 24th An-
nual Conference on Uncertainty in AI (UAI ’08).
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Joint Human Lan-
guage Technology Conference/Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ’06), pages 423–
430.
</reference>
<page confidence="0.998933">
743
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974037">
<title confidence="0.9997435">Parse, Price and Cut—Delayed Column and Row Generation for Graph Based Parsers</title>
<author confidence="0.99993">Sebastian Riedel David Smith Andrew McCallum</author>
<affiliation confidence="0.9957775">Department of Computer University of Massachusetts, Amherst</affiliation>
<email confidence="0.999691">riedel@cs.umass.edu</email>
<email confidence="0.999691">dasmith@cs.umass.edu</email>
<email confidence="0.999691">mccallum@cs.umass.edu</email>
<abstract confidence="0.999247461538461">Graph-based dependency parsers suffer from the sheer number of higher order edges they need to (a) score and (b) consider during optimization. Here we show that when working with LP relaxations, large fractions of these edges can be pruned before they are fully scored—without any loss of optimality guarantees and, hence, accuracy. This is achieved by iteratively parsing with a subset of higherorder edges, adding higher-order edges that may improve the score of the current solution, and adding higher-order edges that are implied by the current best first order edges. This amounts to delayed column and row generation in the LP relaxation and is guaranteed to provide the optimal LP solution. For second order grandparent models, our method considers, or scores, no more than 6–13% of the second order edges of the full model. This yields up to an eightfold parsing speedup, while providing the same empirical accuracy and certificates of optimality as working with the full LP relaxation. We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ravindra K Ahuja</author>
<author>Thomas L Magnanti</author>
<author>James B Orlin</author>
</authors>
<title>Network Flows: Theory, Algorithms, and Applications.</title>
<date>1993</date>
<volume>1</volume>
<pages>edition,</pages>
<publisher>Prentice Hall,</publisher>
<contexts>
<context position="31750" citStr="Ahuja et al., 1993" startWordPosition="5564" endWordPosition="5567">ual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of the shortest path problem. Roughly speaking, in this formulation any feasible dual assignments correspond to a consistent (and thus admissible) heuristic, and the corresponding reduced costs can be used as edge weights. Run739 ning Dijkstra’s algorithm with these weights then amounts to A*. Column generation for the shortest path problem can then be understood</context>
</contexts>
<marker>Ahuja, Magnanti, Orlin, 1993</marker>
<rawString>Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. 1993. Network Flows: Theory, Algorithms, and Applications. Prentice Hall, 1 edition, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Belanger</author>
<author>Alexandre Passos</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>A column generation approach to connecting regularization and map inference.</title>
<date>2012</date>
<booktitle>In Inferning: Interactions between Inference and Learning, ICML 2012 Workshop.</booktitle>
<contexts>
<context position="29368" citStr="Belanger et al., 2012" startWordPosition="5173" endWordPosition="5176">Column and Row Generation can substantially reduce the runtime of an off-the-shelf LP solver, as we will find in section 6. Perhaps somewhat surprisingly, it can also be applied in the context of dynamic programs. It is well known that for each dynamic program there is an equivalent polynomial LP formulation (Martin et al., 1990). Roughly speaking, in this formulation primal variables correspond to state transitions, and dual variables to value functions (e.g., the forward scores in the Viterbi algorithm). In pilot studies we have already used DCG to speed up (exact) Viterbi on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola,</context>
</contexts>
<marker>Belanger, Passos, Riedel, McCallum, 2012</marker>
<rawString>David Belanger, Alexandre Passos, Sebastian Riedel, and Andrew McCallum. 2012. A column generation approach to connecting regularization and map inference. In Inferning: Interactions between Inference and Learning, ICML 2012 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri P Bertsekas</author>
</authors>
<title>Nonlinear Programming. Athena Scientific, 2nd edition,</title>
<date>1999</date>
<contexts>
<context position="17941" citStr="Bertsekas, 1999" startWordPosition="3111" endWordPosition="3112">antially reduce runtime by lazily adding variables only when needed (Gilmore and Gomory, 1961; Lübbecke and Desrosiers, 2004). To illustrate column and row generation let us consider the following general primal LP and its corSay you are given a primal feasible z&apos; and a dual feasible A&apos; for which complementary slackness holds: for all variables i we have z&apos; i &gt; 0 ==&gt;. si = E j A&apos;jai,j and for all constraints j we have A&apos;j &gt; 0 ==&gt;. bj = Ei z&apos;iai,j. In this case it is easy to show that z&apos; is an optimal primal solution, A&apos; and optimal dual solution, and that both objectives meet at these values (Bertsekas, 1999). The idea behind delayed column and row generation is to only consider a small subset of variables (or columns) I and subset of constraints (or rows) J. Optimizing over this restricted problem, either with an off-the-shelf solver or a more specialized method, yields the pair (z&apos;I, A&apos; ) of partial primal and dual J solutions. This pair is feasible and complementary with respect to variables I and constraints J. We can extend it to a solution (z&apos;, y&apos;) over all variables and constraints by heuristically setting the remaining primal and dual variables. If it so happens that (z&apos;, y&apos;) is feasible a</context>
</contexts>
<marker>Bertsekas, 1999</marker>
<rawString>Dimitri P. Bertsekas. 1999. Nonlinear Programming. Athena Scientific, 2nd edition, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL’ 06), CoNLL-X ’06,</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="33444" citStr="Buchholz and Marsi (2006)" startWordPosition="5855" endWordPosition="5858">And what kind of reduction in runtime does this reduction in edges lead to? We have also pointed out that our outer bound on the grandparent polytope of legal edge and grandparent vectors is tighter than the one presented by Martins et al. (2009). What effect does this bound have on the number of fractional solutions and the overall accuracy? To answer these questions we will focus on a set of non-projective grandparent models, but point out that our method and formulation can be easily extended to projective parsing as well as other types of higher order edges. We use the Danish test data of Buchholz and Marsi (2006) and the Italian and Hungarian test datasets of Nivre et al. (2007). 6.1 Impact of Price and Cut Table 1 compares brute force optimization (BF) with the full model, in spirit of Martins et al. (2009), to running parse, price and cut (PPC) on the same model. This model contains all constraints presented in 3.2. The table shows the average number of parsed sentences per second, the average objective, number of grandparent edges scored and added, all relative to the brute force approach. We also present the average unlabeled accuracy, and the percentage of sentences with integer solutions. This n</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL’ 06), CoNLL-X ’06, pages 149–164, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05),</booktitle>
<pages>173--180</pages>
<contexts>
<context position="31319" citStr="Charniak and Johnson, 2005" startWordPosition="5494" endWordPosition="5497">ss, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05), pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Modelling compression with discourse constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL ’07),</booktitle>
<pages>1--11</pages>
<contexts>
<context position="37845" citStr="Clarke and Lapata, 2007" startWordPosition="6605" endWordPosition="6608">. 80% 77% 81% (b) Hungarian Table 2: Different outer bounds on the grandparent polytope, for nonprojective parsing of Italian and Danish. violations. This allows us to discard a large fraction of parts during both scoring and optimization, leading to nearly 800% speed-ups without loss of accuracy and certificates. We also present a tighter bound on the grandparent polytope that is useful in its own right. Delayed column and row generation is very useful when solving large LPs with off-the-shelf solvers. Given the multitude of work in NLP that uses LPs and ILPs in this way (Roth and Yih, 2004; Clarke and Lapata, 2007), we hope that our approach will prove itself useful for other applications. We stress that this approach can also be used when working with dynamic programs, as pointed out in section 4.5, and therefore also in the context of dual decomposition. This suggests even wider applicability, and usefulness in various structured prediction problems. The underlying paradigm could also be useful for more approximate methods. In this paradigm, algorithms maintain an estimate of the cost of certain resources (duals), and use these estimates to guide search and the propose new structures. For example, a l</context>
</contexts>
<marker>Clarke, Lapata, 2007</marker>
<rawString>James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL ’07), pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>M Wick</author>
<author>R Hall</author>
<author>A McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution.</title>
<date>2007</date>
<booktitle>In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’07),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="1937" citStr="Culotta et al., 2007" startWordPosition="309" endWordPosition="312">maller integrality gap and higher speed. 1 Introduction Many problems in NLP, and structured prediction in general, can be cast as finding high-scoring structures based on a large set of candidate parts. For example, in second order graph-based dependency parsing (Kübler et al., 2009) we have to choose a quadratic number of first order and a cubic number of second order edges such that the graph is both high-scoring and a tree. In coreference, we have to select high-scoring clusters of mentions from an exponential number of candidate clusters, such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008)</context>
</contexts>
<marker>Culotta, Wick, Hall, McCallum, 2007</marker>
<rawString>A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007. First-order probabilistic models for coreference resolution. In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’07), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Dienes</author>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
</authors>
<title>Statistical a-star dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the workshop on Prospects and Advances of the Syntax/Semantics Interface,</booktitle>
<pages>85--89</pages>
<location>Nancy,</location>
<contexts>
<context position="31891" citStr="Dienes et al. (2003)" startWordPosition="5590" endWordPosition="5593">rithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of the shortest path problem. Roughly speaking, in this formulation any feasible dual assignments correspond to a consistent (and thus admissible) heuristic, and the corresponding reduced costs can be used as edge weights. Run739 ning Dijkstra’s algorithm with these weights then amounts to A*. Column generation for the shortest path problem can then be understood as a method to lazily construct a consistent heuristic. In every step this method finds edges for which consistency is violated, and updates</context>
</contexts>
<marker>Dienes, Koller, Kuhlmann, 2003</marker>
<rawString>Peter Dienes, Alexander Koller, and Marco Kuhlmann. 2003. Statistical a-star dependency parsing. In Proceedings of the workshop on Prospects and Advances of the Syntax/Semantics Interface, Nancy, 2003, pp.85-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Gilmore</author>
<author>R E Gomory</author>
</authors>
<title>A linear programming approach to the cutting-stock problem. Operations research,</title>
<date>1961</date>
<pages>849--859</pages>
<contexts>
<context position="17418" citStr="Gilmore and Gomory, 1961" startWordPosition="3006" endWordPosition="3009">s of constraints, the NoGP constraint is implied by the constraint GPArc (left) by summing over c on both sides, and can hence be omitted. 4 Parse, Price and Cut We now introduce our parsing algorithm. To this end, we first give a general description of column and row generation for LPs; then, we illustrate how these techniques can be applied to dependency parsing. 4.1 Column and Row Generation LPs often have too many variables and constraints to be efficiently solved. In such cases delayed column and row generation can substantially reduce runtime by lazily adding variables only when needed (Gilmore and Gomory, 1961; Lübbecke and Desrosiers, 2004). To illustrate column and row generation let us consider the following general primal LP and its corSay you are given a primal feasible z&apos; and a dual feasible A&apos; for which complementary slackness holds: for all variables i we have z&apos; i &gt; 0 ==&gt;. si = E j A&apos;jai,j and for all constraints j we have A&apos;j &gt; 0 ==&gt;. bj = Ei z&apos;iai,j. In this case it is easy to show that z&apos; is an optimal primal solution, A&apos; and optimal dual solution, and that both objectives meet at these values (Bertsekas, 1999). The idea behind delayed column and row generation is to only consider a sma</context>
</contexts>
<marker>Gilmore, Gomory, 1961</marker>
<rawString>P.C. Gilmore and R.E. Gomory. 1961. A linear programming approach to the cutting-stock problem. Operations research, pages 849–859.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL ’03),</booktitle>
<pages>119--126</pages>
<contexts>
<context position="31839" citStr="Klein and Manning (2003)" startWordPosition="5580" endWordPosition="5583">ed prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of the shortest path problem. Roughly speaking, in this formulation any feasible dual assignments correspond to a consistent (and thus admissible) heuristic, and the corresponding reduced costs can be used as edge weights. Run739 ning Dijkstra’s algorithm with these weights then amounts to A*. Column generation for the shortest path problem can then be understood as a method to lazily construct a consistent heuristic. In every step this method finds </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A* parsing: Fast exact viterbi parse selection. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL ’03), pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’11).</booktitle>
<contexts>
<context position="3191" citStr="Koo and Collins, 2010" startWordPosition="514" endWordPosition="517">2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo and Collins, 2010; Martins et al., 2009; Riedel and Clarke, 2006). While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality. We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor732 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 2012.</context>
<context position="31364" citStr="Koo and Collins, 2010" startWordPosition="5502" endWordPosition="5505">t LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL ’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’10).</booktitle>
<contexts>
<context position="11044" citStr="Koo et al., 2010" startWordPosition="1855" endWordPosition="1858"> dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To represent a parse y E Y we first introduce an vector of variables z °_ (za)a where za is 1 if a E y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a legal parse tree and that maximizes Ea zasa. One way to achieve this is to search through the convex hull of all legal inc</context>
<context position="30828" citStr="Koo et al., 2010" startWordPosition="5418" endWordPosition="5421">also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coars</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kübler</author>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency Parsing. Synthesis Lectures on Human Language Technologies.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1601" citStr="Kübler et al., 2009" startWordPosition="251" endWordPosition="254">thod considers, or scores, no more than 6–13% of the second order edges of the full model. This yields up to an eightfold parsing speedup, while providing the same empirical accuracy and certificates of optimality as working with the full LP relaxation. We also provide a tighter LP formulation for grandparent models that leads to a smaller integrality gap and higher speed. 1 Introduction Many problems in NLP, and structured prediction in general, can be cast as finding high-scoring structures based on a large set of candidate parts. For example, in second order graph-based dependency parsing (Kübler et al., 2009) we have to choose a quadratic number of first order and a cubic number of second order edges such that the graph is both high-scoring and a tree. In coreference, we have to select high-scoring clusters of mentions from an exponential number of candidate clusters, such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to conside</context>
</contexts>
<marker>Kübler, McDonald, Nivre, 2009</marker>
<rawString>Sandra Kübler, Ryan T. McDonald, and Joakim Nivre. 2009. Dependency Parsing. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lübbecke</author>
<author>Jacques Desrosiers</author>
</authors>
<title>Selected topics in column generation.</title>
<date>2004</date>
<journal>Operations Research,</journal>
<pages>53--1007</pages>
<contexts>
<context position="17450" citStr="Lübbecke and Desrosiers, 2004" startWordPosition="3010" endWordPosition="3013"> constraint is implied by the constraint GPArc (left) by summing over c on both sides, and can hence be omitted. 4 Parse, Price and Cut We now introduce our parsing algorithm. To this end, we first give a general description of column and row generation for LPs; then, we illustrate how these techniques can be applied to dependency parsing. 4.1 Column and Row Generation LPs often have too many variables and constraints to be efficiently solved. In such cases delayed column and row generation can substantially reduce runtime by lazily adding variables only when needed (Gilmore and Gomory, 1961; Lübbecke and Desrosiers, 2004). To illustrate column and row generation let us consider the following general primal LP and its corSay you are given a primal feasible z&apos; and a dual feasible A&apos; for which complementary slackness holds: for all variables i we have z&apos; i &gt; 0 ==&gt;. si = E j A&apos;jai,j and for all constraints j we have A&apos;j &gt; 0 ==&gt;. bj = Ei z&apos;iai,j. In this case it is easy to show that z&apos; is an optimal primal solution, A&apos; and optimal dual solution, and that both objectives meet at these values (Bertsekas, 1999). The idea behind delayed column and row generation is to only consider a small subset of variables (or colum</context>
</contexts>
<marker>Lübbecke, Desrosiers, 2004</marker>
<rawString>Marco Lübbecke and Jacques Desrosiers. 2004. Selected topics in column generation. Operations Research, 53:1007–1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kipp Martin</author>
<author>Ronald L Rardin</author>
<author>Brian A Campbell</author>
</authors>
<title>Polyhedral characterization of discrete dynamic programming.</title>
<date>1990</date>
<journal>Oper. Res.,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="29077" citStr="Martin et al., 1990" startWordPosition="5125" endWordPosition="5128"> which zhh,mi = 0 as these automatically satisfy any ArcGP constraint they appear in. Now for each zhg,pi &gt; 0 all zhp,ci &gt; 0 are found, and if their sum is larger than 1, the corresponding grandparent edge (g, p, c) is returned in the result set. 4.5 Column Generation in Dynamic Programs Column and Row Generation can substantially reduce the runtime of an off-the-shelf LP solver, as we will find in section 6. Perhaps somewhat surprisingly, it can also be applied in the context of dynamic programs. It is well known that for each dynamic program there is an equivalent polynomial LP formulation (Martin et al., 1990). Roughly speaking, in this formulation primal variables correspond to state transitions, and dual variables to value functions (e.g., the forward scores in the Viterbi algorithm). In pilot studies we have already used DCG to speed up (exact) Viterbi on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog </context>
</contexts>
<marker>Martin, Rardin, Campbell, 1990</marker>
<rawString>R. Kipp Martin, Ronald L. Rardin, and Brian A. Campbell. 1990. Polyhedral characterization of discrete dynamic programming. Oper. Res., 38(1):127–138, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL ’09),</booktitle>
<pages>342--350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2575" citStr="Martins et al., 2009" startWordPosition="415" endWordPosition="418"> of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo a</context>
<context position="4952" citStr="Martins et al. (2009)" startWordPosition="801" endWordPosition="804">ution without removing high scoring existing structure. The latter step subtracts from the cost of a part the price of resources the part requires, and is often referred as pricing. Sometimes parts may score highly after pricing, but are necessary in order to make the current solution feasible. We add such parts in a step that roughly amounts to violated cuts to the LP. We illustrate our approach in terms of a secondorder grandparent model for dependency parsing. We solve these models by iteratively parsing, pricing, and cutting. To this end we use a variant of the LP relaxation formulated by Martins et al. (2009). Our variant of this LP is designed to be amenable to column generation. It also turns out to be a tighter outer bound that leads to fewer fractional solutions and faster runtimes. To find high scoring grandparent edges without explicitly enumerating all of them, we prune out a large fraction using factorized upper bounds on grandparent scores. Our parse, price and cut algorithm is evaluated using a non-projective grandparent model on three languages. Compared to a brute force approach of solving the full LP, we only score about 10% of the grandparent edges, consider only 8% in optimization, </context>
<context position="6475" citStr="Martins et al. (2009)" startWordPosition="1050" endWordPosition="1053">andparent formulation of Martins et al. (2009). 2 Graph-Based Dependency Parsing Dependency trees are representations of the syntactic structure of a sentence (Nivre et al., 2007). They determine, for each token of a sentence, the syntactic head the token is modifying. As a lightweight alternative to phrase-based constituency trees, dependency representations have by now seen widespread use in the community in various domains such as question answering, machine translation, and information extraction. To simplify further exposition, we now formalize the task, and mostly follow the notation of Martins et al. (2009). Consider a sentence x = (to, ti, ... , tn) where ti, ... , tn correspond to the n tokens of the sentence, and to is an artificial root token. Let V °_ 10, ... , n} be a set of vertices corresponding to the tokens in x, and C C V x V a set of candidate directed edges. Then a directed graph y C C is a legal dependency parse if and only if it is a tree over V rooted at vertex 0. Given a sentence x, we use Y to denote the set of its legal parses. Note that all of the above definitions depend on x, but for simplicity we omit this dependency in our notation. 2.1 Arc-Factored Models Graph-based mod</context>
<context position="10721" citStr="Martins et al., 2009" startWordPosition="1806" endWordPosition="1809">al likelihood, or variants in between. In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. 3 LP and ILP Formulations Riedel and Clarke (2006) showed that dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To r</context>
<context position="12031" citStr="Martins et al., 2009" startWordPosition="2031" endWordPosition="2034"> if a E y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a legal parse tree and that maximizes Ea zasa. One way to achieve this is to search through the convex hull of all legal incidence vectors, knowing that any linear objectives would take on its maximum on one of the hull’s vertices. We will use i to denote this convex hull of incidence vectors of legal parse trees, 1Such as the highly efficient and free-for-academic-use Gurobi solver. s(h,m) + � sgp (3) (g,p)Ey,(p,c)Ey (g,p,c) � A sgp (g,p,c) — 734 and call i the arborescence polytope (Martins et al., 2009). The Minkowski-Weyl theorem tells us that i can be represented as an intersection of halfspaces, or constraints, i = {zJAz &lt; b}. Hence optimal dependency parsing, in theory, can be addressed using LPs. However, it is difficult to describe i with a compact number of constraints and variables that lend themselves to efficient optimization. In general we therefore work with relaxations, or outer bounds, on i. Such outer bounds are designed to cut off all illegal integer solutions of the problem, but still allow for fractional solutions. In case the optimum is achieved at an integer vertex of the</context>
<context position="14050" citStr="Martins et al. (2009)" startWordPosition="2380" endWordPosition="2384"> outer bounds. The constraints we employ are: No Head For Root In a dependency tree the root node never has a head. While this could be captured through linear constraints, it is easier to simply restrict the candidate set C to never contain edges of the form (·, 0). Exactly One Head for Non-Roots Any non-root token has to have exactly one head token. We can enforce this property through the set of constraints: Xm &gt; 0 : z(h,m) = 1. (OneHead) h No Cycles A parse tree cannot have cycles. This is equivalent, together with the head constraints above, to enforcing that the tree be fully connected. Martins et al. (2009) capture this connectivity constraint using a single commodity flow formulation. This requires the introduction of flow variables 0i°_ (φa)aEC. By enforcing that token 0 has n outgoing flow, X φ(0,m) = n, (Source) m&gt;0 that any other token consumes one unit offlow, t &gt; 0 : X Xφ(h,t) − φ(t,m) = 1 (Consume) h m&gt;0 and that flow is zero on disabled arcs φ(h,m) &lt; nz(h,m), (NoFlow) connectivity can be ensured. Assuming we have such a representation, parsing with an LP relaxation amounts to solving subject to A &lt; b. � z � � (7) 3.2 Higher Order Models The 1st-Order LP can be easily extended to capture</context>
<context position="15322" citStr="Martins et al. (2009)" startWordPosition="2617" endWordPosition="2620">of grandparent models, this amounts to introducing another class of variables, zgp g,p,c, that indicate if the parse contains both the edge (g, p) and the edge (p, c). With the help of the indicators zgp we can represent the second order objective as a linear function. We now need an outer bound on the convex hull of vectors (z, zgp) where z is a legal parse tree and zgp is a consistent set of grandparent indicators. We will refer to this convex hull as the grandparent polytope igp. We can re-use the constraints A of section 3.1 to ensure that z is in i. To make sure zgp is consistent with z, Martins et al. (2009) linearize the equivalence zgp g,p,c zg,p n zp,c we know to hold for legal incidence vectors, yielding g, p, c : z(g,p) + z(p,c) − zgp (g,p,c) &lt; 1 (ArcGP) and g, p, c : z(g,p) � zgp (g,p,c), z(p,c) � zgp (g,p,c) (GPArc) There are additional constraints we know to hold in igp. First, we know that for any active edge (p, c) E maximize Z&gt;o zasa X aEA 735 y with p &gt; 0 there is exactly one grandparent edge (g, p, c). Likewise, for an inactive edge (p, c) V y there must be no grandparent edge (g, p, c). This can be captured through the constraint: responding dual problem: Primal maximize Z&gt;o sTz Dua</context>
<context position="30490" citStr="Martins et al. (2009)" startWordPosition="5361" endWordPosition="5364"> variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5),</context>
<context position="33065" citStr="Martins et al. (2009)" startWordPosition="5789" endWordPosition="5792"> for which consistency is violated, and updates the heuristic such that all these edges are consistent. 6 Experiments We claim that LP relaxations for higher order parsing can be solved without considering, and scoring, all candidate higher order edges. In practice, how many grandparent edges do we need to score, and how many do we need to add to the optimization problem? And what kind of reduction in runtime does this reduction in edges lead to? We have also pointed out that our outer bound on the grandparent polytope of legal edge and grandparent vectors is tighter than the one presented by Martins et al. (2009). What effect does this bound have on the number of fractional solutions and the overall accuracy? To answer these questions we will focus on a set of non-projective grandparent models, but point out that our method and formulation can be easily extended to projective parsing as well as other types of higher order edges. We use the Danish test data of Buchholz and Marsi (2006) and the Italian and Hungarian test datasets of Nivre et al. (2007). 6.1 Impact of Price and Cut Table 1 compares brute force optimization (BF) with the full model, in spirit of Martins et al. (2009), to running parse, pr</context>
<context position="35090" citStr="Martins et al. (2009)" startWordPosition="6141" endWordPosition="6144"> of the full model. This leads to up to 760% improvement in speed. This improvement comes for free, without any sacrifice in optimality or guarantees. We also notice that in all cases at least 97% of the sentences have no fractional solutions, and are therefore optimal even with respect to the ILP. Table 1 also shows that our bounds on reduced costs are relatively tight. For example, in the case of Italian we score only one percent more grandparent edges than we actually need to add. Our fastest PCC parser processes about one sentence per second. This speed falls below the reported numbers of Martins et al. (2009) of about 0.6 seconds per sentence. Crucially, however, in contrast to their work, our speed is achieved without any firstorder pruning. In addition, we expect further improvements in runtime by optimizing the implementation of our pricing algorithm. 6.2 Tighter Grandparent Polytope To investigate how the additional grandparent constraints in section 3.2 help, we compare three models, this time without PPC. The first model follows Martins et al. (2009) and uses constraints ArcGP and GPArc only. The second model uses only constraints OneGP and NoGP. The final model incorporates all four constra</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>André F. T. Martins, Noah A. Smith, and Eric P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL ’09), pages 342–350, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>Mário A T Figueiredo</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’11), EMNLP ’11,</booktitle>
<pages>238--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="30851" citStr="Martins et al., 2011" startWordPosition="5422" endWordPosition="5425">with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in whi</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>André F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and Mário A. T. Figueiredo. 2011. Dual decomposition with many overlapping components. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’11), EMNLP ’11, pages 238–249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL ’06),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2492" citStr="McDonald and Pereira, 2006" startWordPosition="401" endWordPosition="404"> such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prun</context>
<context position="8073" citStr="McDonald and Pereira, 2006" startWordPosition="1351" endWordPosition="1354">tions of the form s (y; x, w) = � s(h,m) (x, w) (1) (h,m)Ey where w is a weight vector and s(h,m) (x, w) scores the edge (h, m) with respect to sentence x and weights w. From here on we will omit both x and w from our notation if they are clear from the context. Given such a scoring function, parsing amounts to solving: � s(h,m) (h,m)Ey (2) subject to y E Y. 2.2 Higher Order Models Arc-factored models cannot capture higher order dependencies between two or more edges. Higher order models remedy this by introducing scores for larger configurations of edges appearing in the maximize y 733 tree (McDonald and Pereira, 2006). For example, in grandparent models, the score of a tree also includes a score sgp (g,p,c) for each grandparent-parentchild triple (g, p, c): � s (y) = (h,m)Ey There are other variants of higher order models that include, in addition to grandparent triples, pairs of siblings (adjacent or not) or third order edges. However, to illustrate our approach we will focus on grandparent models and note that most of what we present can be generalized to other higher order models. 2.3 Feature Templates For our later exposition the factored and parametrized nature of the scoring functions will be crucial</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL ’06), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05),</booktitle>
<pages>91--98</pages>
<contexts>
<context position="7373" citStr="McDonald et al., 2005" startWordPosition="1224" endWordPosition="1227">directed graph y C C is a legal dependency parse if and only if it is a tree over V rooted at vertex 0. Given a sentence x, we use Y to denote the set of its legal parses. Note that all of the above definitions depend on x, but for simplicity we omit this dependency in our notation. 2.1 Arc-Factored Models Graph-based models define parametrized scoring functions that are trained to discriminate between correct and incorrect parse trees. So called arcfactored or first order models are the most basic variant of such functions: they assess the quality of a tree by scoring each edge in isolation (McDonald et al., 2005b; McDonald et al., 2005a). Formally, arcfactored models are scoring functions of the form s (y; x, w) = � s(h,m) (x, w) (1) (h,m)Ey where w is a weight vector and s(h,m) (x, w) scores the edge (h, m) with respect to sentence x and weights w. From here on we will omit both x and w from our notation if they are clear from the context. Given such a scoring function, parsing amounts to solving: � s(h,m) (h,m)Ey (2) subject to y E Y. 2.2 Higher Order Models Arc-factored models cannot capture higher order dependencies between two or more edges. Higher order models remedy this by introducing scores </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05), pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP,</booktitle>
<contexts>
<context position="7373" citStr="McDonald et al., 2005" startWordPosition="1224" endWordPosition="1227">directed graph y C C is a legal dependency parse if and only if it is a tree over V rooted at vertex 0. Given a sentence x, we use Y to denote the set of its legal parses. Note that all of the above definitions depend on x, but for simplicity we omit this dependency in our notation. 2.1 Arc-Factored Models Graph-based models define parametrized scoring functions that are trained to discriminate between correct and incorrect parse trees. So called arcfactored or first order models are the most basic variant of such functions: they assess the quality of a tree by scoring each edge in isolation (McDonald et al., 2005b; McDonald et al., 2005a). Formally, arcfactored models are scoring functions of the form s (y; x, w) = � s(h,m) (x, w) (1) (h,m)Ey where w is a weight vector and s(h,m) (x, w) scores the edge (h, m) with respect to sentence x and weights w. From here on we will omit both x and w from our notation if they are clear from the context. Given such a scoring function, parsing amounts to solving: � s(h,m) (h,m)Ey (2) subject to y E Y. 2.2 Higher Order Models Arc-factored models cannot capture higher order dependencies between two or more edges. Higher order models remedy this by introducing scores </context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In HLT-EMNLP, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Niepert</author>
</authors>
<title>A delayed column generation strategy for exact k-bounded map inference in markov logic networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 26th Annual Conference on Uncertainty in AI (UAI ’10),</booktitle>
<pages>384--391</pages>
<publisher>AUAI Press.</publisher>
<location>Corvallis, Oregon.</location>
<contexts>
<context position="30271" citStr="Niepert (2010)" startWordPosition="5325" endWordPosition="5326"> measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations </context>
</contexts>
<marker>Niepert, 2010</marker>
<rawString>Mathias Niepert. 2010. A delayed column generation strategy for exact k-bounded map inference in markov logic networks. In Proceedings of the 26th Annual Conference on Uncertainty in AI (UAI ’10), pages 384–391, Corvallis, Oregon. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The conll</title>
<date>2007</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing and Natural Language Learning,</booktitle>
<pages>915--932</pages>
<contexts>
<context position="6033" citStr="Nivre et al., 2007" startWordPosition="981" endWordPosition="984">red to a brute force approach of solving the full LP, we only score about 10% of the grandparent edges, consider only 8% in optimization, and so observe an increase in parsing speed of up to 750%. This is possible without loss of optimality, and hence accuracy. We also find that our extended LP formulation leads to a 15% reduction of fractional solutions, up to 12 times higher speed, and generally higher accuracy when compared to the grandparent formulation of Martins et al. (2009). 2 Graph-Based Dependency Parsing Dependency trees are representations of the syntactic structure of a sentence (Nivre et al., 2007). They determine, for each token of a sentence, the syntactic head the token is modifying. As a lightweight alternative to phrase-based constituency trees, dependency representations have by now seen widespread use in the community in various domains such as question answering, machine translation, and information extraction. To simplify further exposition, we now formalize the task, and mostly follow the notation of Martins et al. (2009). Consider a sentence x = (to, ti, ... , tn) where ti, ... , tn correspond to the n tokens of the sentence, and to is an artificial root token. Let V °_ 10, .</context>
<context position="33511" citStr="Nivre et al. (2007)" startWordPosition="5867" endWordPosition="5870">o? We have also pointed out that our outer bound on the grandparent polytope of legal edge and grandparent vectors is tighter than the one presented by Martins et al. (2009). What effect does this bound have on the number of fractional solutions and the overall accuracy? To answer these questions we will focus on a set of non-projective grandparent models, but point out that our method and formulation can be easily extended to projective parsing as well as other types of higher order edges. We use the Danish test data of Buchholz and Marsi (2006) and the Italian and Hungarian test datasets of Nivre et al. (2007). 6.1 Impact of Price and Cut Table 1 compares brute force optimization (BF) with the full model, in spirit of Martins et al. (2009), to running parse, price and cut (PPC) on the same model. This model contains all constraints presented in 3.2. The table shows the average number of parsed sentences per second, the average objective, number of grandparent edges scored and added, all relative to the brute force approach. We also present the average unlabeled accuracy, and the percentage of sentences with integer solutions. This number shows us how often we not only found the optimal solution to </context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The conll 2007 shared task on dependency parsing. In Conference on Empirical Methods in Natural Language Processing and Natural Language Learning, pages 915—932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint inference in information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 22nd AAAI Conference on Artificial Intelligence (AAAI ’07),</booktitle>
<pages>913--918</pages>
<contexts>
<context position="2114" citStr="Poon and Domingos, 2007" startWordPosition="339" endWordPosition="342">large set of candidate parts. For example, in second order graph-based dependency parsing (Kübler et al., 2009) we have to choose a quadratic number of first order and a cubic number of second order edges such that the graph is both high-scoring and a tree. In coreference, we have to select high-scoring clusters of mentions from an exponential number of candidate clusters, such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score f</context>
</contexts>
<marker>Poon, Domingos, 2007</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2007. Joint inference in information extraction. In Proceedings of the 22nd AAAI Conference on Artificial Intelligence (AAAI ’07), pages 913–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>James Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’06),</booktitle>
<pages>129--137</pages>
<contexts>
<context position="3239" citStr="Riedel and Clarke, 2006" startWordPosition="522" endWordPosition="525">o evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo and Collins, 2010; Martins et al., 2009; Riedel and Clarke, 2006). While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality. We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor732 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistic</context>
<context position="10416" citStr="Riedel and Clarke (2006)" startWordPosition="1759" endWordPosition="1762">nctions we consider are parametrized by a family of per-template weight vectors w = (wt)tET. During learning we need to estimate w such that our scoring functions learns to differentiate between correct and incorrect parse trees. This can be achieved in many ways: large margin training, maximizing conditional likelihood, or variants in between. In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. 3 LP and ILP Formulations Riedel and Clarke (2006) showed that dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual dec</context>
<context position="13181" citStr="Riedel and Clarke (2006)" startWordPosition="2230" endWordPosition="2233">nal solutions. In case the optimum is achieved at an integer vertex of the outer bound, it is clear that we have found the optimal solution to the original problem. In case we find a fractional point, we need to map it onto i (e.g., by projection or rounding). Alternatively, we can use the outer bound together with 0/1 constraints on z, and then employ an ILP solver (say, branch-and-bound) to find the true optimum. Given the NP-hardness of ILP, this will generally be slow. In the following we will present the outer bound i _D i proposed by Martins et al. (2009). Compared to the representation Riedel and Clarke (2006), this bound has the benefit a small polynomial number of constraints. Note, however, that often exponentially many constraints can be efficiently handled if polynomial separation algorithms exists, and that such representations can lead to tighter outer bounds. The constraints we employ are: No Head For Root In a dependency tree the root node never has a head. While this could be captured through linear constraints, it is easier to simply restrict the candidate set C to never contain edges of the form (·, 0). Exactly One Head for Non-Roots Any non-root token has to have exactly one head token</context>
<context position="29920" citStr="Riedel and Clarke, 2006" startWordPosition="5265" endWordPosition="5268">CG to speed up (exact) Viterbi on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constra</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>Sebastian Riedel and James Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’06), pages 129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>David A Smith</author>
</authors>
<title>Relaxed marginal inference and its application to dependency parsing.</title>
<date>2010</date>
<booktitle>In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’10),</booktitle>
<pages>760--768</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="29577" citStr="Riedel and Smith (2010)" startWordPosition="5208" endWordPosition="5211">rams. It is well known that for each dynamic program there is an equivalent polynomial LP formulation (Martin et al., 1990). Roughly speaking, in this formulation primal variables correspond to state transitions, and dual variables to value functions (e.g., the forward scores in the Viterbi algorithm). In pilot studies we have already used DCG to speed up (exact) Viterbi on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using </context>
</contexts>
<marker>Riedel, Smith, 2010</marker>
<rawString>Sebastian Riedel and David A. Smith. 2010. Relaxed marginal inference and its application to dependency parsing. In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’10), pages 760–768, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
</authors>
<title>Improving the accuracy and efficiency of MAP inference for markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 24th Annual Conference on Uncertainty in AI (UAI ’08),</booktitle>
<pages>468--475</pages>
<contexts>
<context position="30106" citStr="Riedel (2008)" startWordPosition="5297" endWordPosition="5298">most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utili</context>
</contexts>
<marker>Riedel, 2008</marker>
<rawString>Sebastian Riedel. 2008. Improving the accuracy and efficiency of MAP inference for markov logic. In Proceedings of the 24th Annual Conference on Uncertainty in AI (UAI ’08), pages 468–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL’ 04),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="37819" citStr="Roth and Yih, 2004" startWordPosition="6601" endWordPosition="6604">3% 97% Unlabeled Acc. 80% 77% 81% (b) Hungarian Table 2: Different outer bounds on the grandparent polytope, for nonprojective parsing of Italian and Danish. violations. This allows us to discard a large fraction of parts during both scoring and optimization, leading to nearly 800% speed-ups without loss of accuracy and certificates. We also present a tighter bound on the grandparent polytope that is useful in its own right. Delayed column and row generation is very useful when solving large LPs with off-the-shelf solvers. Given the multitude of work in NLP that uses LPs and ILPs in this way (Roth and Yih, 2004; Clarke and Lapata, 2007), we hope that our approach will prove itself useful for other applications. We stress that this approach can also be used when working with dynamic programs, as pointed out in section 4.5, and therefore also in the context of dual decomposition. This suggests even wider applicability, and usefulness in various structured prediction problems. The underlying paradigm could also be useful for more approximate methods. In this paradigm, algorithms maintain an estimate of the cost of certain resources (duals), and use these estimates to guide search and the propose new st</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL’ 04), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Rush</author>
<author>Slav Petrov</author>
</authors>
<title>Vine pruning for efficient multi-pass dependency parsing.</title>
<date>2012</date>
<booktitle>In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’12).</booktitle>
<contexts>
<context position="31388" citStr="Rush and Petrov, 2012" startWordPosition="5506" endWordPosition="5509">sed on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of </context>
</contexts>
<marker>Rush, Petrov, 2012</marker>
<rawString>Alexander Rush and Slav Petrov. 2012. Vine pruning for efficient multi-pass dependency parsing. In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’10).</booktitle>
<contexts>
<context position="11064" citStr="Rush et al., 2010" startWordPosition="1859" endWordPosition="1862">g can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To represent a parse y E Y we first introduce an vector of variables z °_ (za)a where za is 1 if a E y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a legal parse tree and that maximizes Ea zasa. One way to achieve this is to search through the convex hull of all legal incidence vectors, know</context>
<context position="30810" citStr="Rush et al., 2010" startWordPosition="5414" endWordPosition="5417">duced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these meth</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>145--156</pages>
<location>Honolulu,</location>
<contexts>
<context position="2537" citStr="Smith and Eisner, 2008" startWordPosition="408" endWordPosition="411"> (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all c</context>
<context position="10185" citStr="Smith and Eisner (2008)" startWordPosition="1723" endWordPosition="1727">] , f [g &gt; c] ,f [p &gt; c]) . (6) The feature function ft maps the representations of g, p and c into a vector space. For the purposes of our work this mapping is not important, and hence we omit details. 2.4 Learning The scoring functions we consider are parametrized by a family of per-template weight vectors w = (wt)tET. During learning we need to estimate w such that our scoring functions learns to differentiate between correct and incorrect parse trees. This can be achieved in many ways: large margin training, maximizing conditional likelihood, or variants in between. In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. 3 LP and ILP Formulations Riedel and Clarke (2006) showed that dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approx</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 145–156, Honolulu, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sontag</author>
<author>T Jaakkola</author>
</authors>
<title>New outer bounds on the marginal polytope.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS ’07),</booktitle>
<pages>1393--1400</pages>
<contexts>
<context position="29973" citStr="Sontag and Jaakkola, 2007" startWordPosition="5274" endWordPosition="5277">langer et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of g</context>
</contexts>
<marker>Sontag, Jaakkola, 2007</marker>
<rawString>D. Sontag and T. Jaakkola. 2007. New outer bounds on the marginal polytope. In Advances in Neural Information Processing Systems (NIPS ’07), pages 1393– 1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sontag</author>
<author>T Meltzer</author>
<author>A Globerson</author>
<author>T Jaakkola</author>
<author>Y Weiss</author>
</authors>
<title>Tightening LP relaxations for MAP using message passing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 24th Annual Conference on Uncertainty in AI (UAI ’08).</booktitle>
<contexts>
<context position="29995" citStr="Sontag et al., 2008" startWordPosition="5278" endWordPosition="5281">lieve it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our </context>
</contexts>
<marker>Sontag, Meltzer, Globerson, Jaakkola, Weiss, 2008</marker>
<rawString>David Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. 2008. Tightening LP relaxations for MAP using message passing. In Proceedings of the 24th Annual Conference on Uncertainty in AI (UAI ’08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy W Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>A fast finite-state relaxation method for enforcing global constraints on sequence decoding.</title>
<date>2006</date>
<booktitle>In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’06),</booktitle>
<pages>423--430</pages>
<contexts>
<context position="29946" citStr="Tromble and Eisner, 2006" startWordPosition="5269" endWordPosition="5273">terbi on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row generation (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clark</context>
</contexts>
<marker>Tromble, Eisner, 2006</marker>
<rawString>Roy W. Tromble and Jason Eisner. 2006. A fast finite-state relaxation method for enforcing global constraints on sequence decoding. In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’06), pages 423– 430.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000210">
<title confidence="0.981685">
Exploring Topic Coherence over many models and many topics
</title>
<author confidence="0.997689">
Keith Stevens&apos;,2 Philip Kegelmeyer3 David Andrzejewski2 David Buttler2
</author>
<affiliation confidence="0.932192333333333">
&apos;University of California Los Angeles; Los Angeles, California, USA
2Lawrence Livermore National Lab; Livermore, California, USA
3Sandia National Lab; Livermore, California, USA
</affiliation>
<email confidence="0.982195">
{stevens35,andrzejewski1,buttler1}@llnl.gov
wpk@sandia.gov
</email>
<sectionHeader confidence="0.993769" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878388888889">
We apply two new automated semantic eval-
uations to three distinct latent topic models.
Both metrics have been shown to align with
human evaluations and provide a balance be-
tween internal measures of information gain
and comparisons to human ratings of coher-
ent topics. We improve upon the measures
by introducing new aggregate measures that
allows for comparing complete topic models.
We further compare the automated measures
to other metrics for topic models, compar-
ison to manually crafted semantic tests and
document classification. Our experiments re-
veal that LDA and LSA each have different
strengths; LDA best learns descriptive topics
while LSA is best at creating a compact se-
mantic representation of documents and words
in a corpus.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999897634615385">
Topic models learn bags of related words from large
corpora without any supervision. Based on the
words used within a document, they mine topic level
relations by assuming that a single document cov-
ers a small set of concise topics. Once learned,
these topics often correlate well with human con-
cepts. For example, one model might produce topics
that cover ideas such as government affairs, sports,
and movies. With these unsupervised methods, we
can extract useful semantic information in a variety
of tasks that depend on identifying unique topics or
concepts, such as distributional semantics (Jurgens
and Stevens, 2010), word sense induction (Van de
Cruys and Apidianaki, 2011; Brody and Lapata,
2009), and information retrieval (Andrzejewski and
Buttler, 2011).
When using a topic model, we are primarily con-
cerned with the degree to which the learned top-
ics match human judgments and help us differen-
tiate between ideas. But until recently, the evalua-
tion of these models has been ad hoc and applica-
tion specific. Evaluations have ranged from fully
automated intrinsic evaluations to manually crafted
extrinsic evaluations. Previous extrinsic evaluations
have used the learned topics to compactly represent
a small fixed vocabulary and compared this distribu-
tional space to human judgments of similarity (Jur-
gens and Stevens, 2010). But these evaluations are
hand constructed and often costly to perform for
domain-specific topics. Conversely, intrinsic mea-
sures have evaluated the amount of information en-
coded by the topics, where perplexity is one com-
mon example(Wallach et al., 2009), however, Chang
et al. (2009) found that these intrinsic measures do
not always correlate with semantically interpretable
topics. Furthermore, few evaluations have used the
same metrics to compare distinct approaches such
as Latent Dirichlet Allocation (LDA) (Blei et al.,
2003), Latent Semantic Analysis (LSA) (Landauer
and Dutnais, 1997), and Non-negative Matrix Fac-
torization (NMF) (Lee and Seung, 2000). This has
made it difficult to know which method is most use-
ful for a given application, or in terms of extracting
useful topics.
We now provide a comprehensive and automated
evaluation of these three distinct models (LDA,
LSA, NMF), for automatically learning semantic
topics. While these models have seen significant im-
provements, they still represent the core differences
between each approach to modeling topics. For our
evaluation, we use two recent automated coherence
measures (Mimno et al., 2011; Newman et al., 2010)
</bodyText>
<page confidence="0.945816">
952
</page>
<note confidence="0.833718">
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 952–961, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9899995">
originally designed for LDA that bridge the gap be-
tween comparisons to human judgments and intrin-
sic measures such as perplexity. We consider several
key questions:
</bodyText>
<listItem confidence="0.997477">
1. How many topics should be learned?
2. How many learned topics are useful?
3. How do these topics relate to often used semantic tests?
4. How well do these topics identify similar documents?
</listItem>
<bodyText confidence="0.999859166666667">
We begin by summarizing the three topic mod-
els and highlighting their key differences. We then
describe the two metrics. Afterwards, we focus on
a series of experiments that address our four key
questions and finally conclude with some overall re-
marks.
</bodyText>
<sectionHeader confidence="0.997135" genericHeader="method">
2 Topic Models
</sectionHeader>
<bodyText confidence="0.931658">
We evaluate three latent factor models that have seen
widespread usage:
</bodyText>
<listItem confidence="0.99211">
1. Latent Dirichlet Allocation
2. Latent Semantic Analysis with Singular Value De-
composition
3. Latent Semantic Analysis with Non-negative Ma-
trix Factorization
</listItem>
<bodyText confidence="0.998700055555556">
Each of these models were designed with differ-
ent goals and are supported by different statistical
theories. We consider both LSA models as topic
models as they have been used in a variety of sim-
ilar contexts such as distributional similarity (Jur-
gens and Stevens, 2010) and word sense induction
(Van de Cruys and Apidianaki, 2011; Brody and
Lapata, 2009). We evaluate these distinct models
on two shared tasks (1) grouping together similar
words while separating unrelated words and (2) dis-
tinguishing between documents focusing on differ-
ent concepts.
We distill the different models into a shared repre-
sentation consisting of two sets of learned relations:
how words interact with topics and how topics inter-
act with documents. For a corpus with D documents
and V words, we denote these relations in terms of
T topics as
</bodyText>
<listItem confidence="0.99525675">
(1) a V x T matrix, W, that indicates the strength
each word has in each topic, and
(2) a T x D matrix, H, that indicates the strength
each topic has in each document.
</listItem>
<bodyText confidence="0.651276">
T serves as a common parameter to each model.
</bodyText>
<subsectionHeader confidence="0.995623">
2.1 Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.999337666666667">
Latent Dirichlet Allocation (Blei et al., 2003) learns
the relationships between words, topics, and docu-
ments by assuming documents are generated by a
particular probabilistic model. It first assumes that
there are a fixed set of topics, T used throughout the
corpus, and each topic z is associated with a multi-
nomial distribution over the vocabulary 4b, which is
drawn from a Dirichlet prior Dir(Q). A given docu-
ment Di is then generated by the following process
</bodyText>
<listItem confidence="0.99394725">
1. Choose Oi — Dir(α), a topic distribution for Di
2. For each word wj E Di:
(a) Select a topic zj — Oi
(b) Select the word wj — 4)z
</listItem>
<bodyText confidence="0.999968416666667">
In this model, the O distributions represent the
probability of each topic appearing in each docu-
ment and the 4b distributions represent the proba-
bility of words being used for each topic. These
two sets of distributions correspond to our H and W
matrices, respectively. The process above defines a
generative model; given the observed corpus, we use
collapsed Gibbs sampling implementation found in
Mallet1 to infer the values of the latent variables 4b
and O (Griffiths and Steyvers, 2004). The model re-
lies only on two additional hyper parameters, α and
Q, that guide the distributions.
</bodyText>
<subsectionHeader confidence="0.999705">
2.2 Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.998170133333333">
Latent Semantic Analysis (Landauer and Dutnais,
1997; Landauer et al., 1998) learns topics by first
forming a traditional term by document matrix used
in information retrieval and then smoothing the
counts to enhance the weight of informative words.
Based on the original LSA model, we use the Log-
Entropy transform. LSA then decomposes this
smoothed, term by document matrix in order to gen-
eralize observed relations between words and docu-
ments. For both LSA models, we used implementa-
tions found in the S-Space package.2
Traditionally, LSA has used the Singular Value
Decomposition, but we also consider Non-negative
Matrix Factorization as we’ve seen NMF applied
in similar situations (Pauca et al., 2004) and others
</bodyText>
<footnote confidence="0.857637">
lhttp://mallet.cs.umass.edu/
zhttps://github.com/fozziethebeat/S-Space
</footnote>
<page confidence="0.997012">
953
</page>
<subsectionHeader confidence="0.656509">
Model Label Top Words
High Quality Topics
</subsectionHeader>
<bodyText confidence="0.759094">
LDA interview told asked wanted interview people made thought time called knew
wine wine wines bottle grapes made winery cabernet grape pinot red
NMF grilling grilled sweet spicy fried pork dish shrimp menu dishes sauce
cloning embryonic cloned embryo human research stem embryos cell cloning cells
SVD cooking sauce food restaurant water oil salt chicken pepper wine cup
stocks fund funds investors weapons stocks mutual stock movie film show
</bodyText>
<subsectionHeader confidence="0.819015">
Low Quality Topics
</subsectionHeader>
<bodyText confidence="0.589288333333333">
LDA rates 10-yr rate 3-month percent 6-month bds bd 30-yr funds robot
charity fund contributions .com family apartment charities rent 22d children assistance
NMF plants stem fruitful stems trunk fruiting currants branches fence currant espalier
farming buzzards groundhog prune hoof pruned pruning vines wheelbarrow tree clematis
SVD city building city area buildings p.m. floors house listed eat-in a.m.
time p.m. system study a.m. office political found school night yesterday
</bodyText>
<figure confidence="0.474260769230769">
UMass UCI
-2.52 1.29
-1.97 1.30
-1.01 1.98
-1.84 1.46
-1.87 -1.21
-2.30 -1.88
-1.94 -12.32
-2.43 -8.88
-3.12 -12.59
-1.90 -12.56
-2.70 -8.03
-1.67 -7.02
</figure>
<tableCaption confidence="0.985258">
Table 1: Top 10 words from several high and low quality topics when ordered by the UCI Coherence
Measure. Topic labels were chosen in an ad hoc manner only to briefly summarize the topic’s focus.
</tableCaption>
<bodyText confidence="0.945863571428571">
have found a connection between NMF and Proba-
bilistic Latent Semantic Analysis (Ding et al., 2008),
an extension to LSA. We later refer to these two LSA
models simply as SVD and NMF to emphasize the
difference in factorization method.
Singular Value Decomposition decomposes M
into three smaller matrices
</bodyText>
<equation confidence="0.982978">
M = UEVT
</equation>
<bodyText confidence="0.999860181818182">
and minimizes Frobenius norm of M’s reconstruc-
tion error with the constraint that the rows of U and
V are orthonormal eigenvectors. Interestingly, the
decomposition is agnostic to the number of desired
dimensions. Instead, the rows and columns in U and
V T are ordered based on their descriptive power, i.e.
how well they remove noise, which is encoded by
the diagonal singular value matrix E. As such, re-
duction is done by retaining the first T rows and
columns from U and V T . For our generalization,
we use W = UE and H = EVT . We note that
values in U and VT can be both negative and pos-
itive, preventing a straightforward interpretation as
unnormalized probabilities
Non-negative Matrix Factorization also factor-
izes M by minimizing the reconstruction error, but
with only one constraint: the decomposed matrices
consist of only non-negative values. In this respect,
we can consider it to be learning an unnormalized
probability distributions over topics. We use the
original Euclidean least squares definition of NMF3.
Formally, NMF is defined as
</bodyText>
<equation confidence="0.382177">
M=WH
</equation>
<bodyText confidence="0.9999626">
where H and W map directly onto our generaliza-
tion. As in the original NMF work, we learn these
unnormalized probabilities by initializing each set of
probabilities at random and update them according
to the following iterative update rules
</bodyText>
<equation confidence="0.9039625">
MHT WT M
W = W WHHT H = HWT WH
</equation>
<sectionHeader confidence="0.980577" genericHeader="method">
3 Coherence Measures
</sectionHeader>
<bodyText confidence="0.999662142857143">
Topic Coherence measures score a single topic by
measuring the degree of semantic similarity between
high scoring words in the topic. These measure-
ments help distinguish between topics that are se-
mantically interpretable topics and topics that are ar-
tifacts of statistical inference, see Table 1 for exam-
ples ordered by the UCI measure. For our evalua-
tions, we consider two new coherence measures de-
signed for LDA, both of which have been shown to
match well with human judgements of topic quality:
(1) The UCI measure (Newman et al., 2010) and (2)
The UMass measure (Mimno et al., 2011).
Both measures compute the coherence of a topic
as the sum of pairwise distributional similarity
</bodyText>
<footnote confidence="0.9672815">
3We note that the alternative KL-Divergence form of NMF
has been directly linked to PLSA (Ding et al., 2008)
</footnote>
<page confidence="0.997521">
954
</page>
<bodyText confidence="0.99525">
scores over the set of topic words, V . We generalize
this as
</bodyText>
<equation confidence="0.980354">
�coherence(V ) = score(vi, vj, E)
(vi,vj)EV
</equation>
<bodyText confidence="0.999667">
where V is a set of word describing the topic and c
indicates a smoothing factor which guarantees that
score returns real numbers. (We will be exploring
the effect of the choice of c; the original authors used
</bodyText>
<equation confidence="0.859675">
E = 1.)
</equation>
<bodyText confidence="0.999075333333333">
The UCI metric defines a word pair’s score to
be the pointwise mutual information (PMI) between
two words, i.e.
</bodyText>
<equation confidence="0.9918035">
score(vi, vj, E) = log p(vi, vj) + �
p(vi)p(vj)
</equation>
<bodyText confidence="0.999839714285714">
The word probabilities are computed by counting
word co-occurrence frequencies in a sliding window
over an external corpus, such as Wikipedia. To some
degree, this metric can be thought of as an external
comparison to known semantic evaluations.
The UMass metric defines the score to be based
on document co-occurrence:
</bodyText>
<equation confidence="0.998135">
D(vi, vj) + �
score(vi, vj, �) = log D(vj)
</equation>
<bodyText confidence="0.99999175">
where D(x, y) counts the number of documents con-
taining words x and y and D(x) counts the num-
ber of documents containing x. Significantly, the
UMass metric computes these counts over the orig-
inal corpus used to train the topic models, rather
than an external corpus. This metric is more intrin-
sic in nature. It attempts to confirm that the models
learned data known to be in the corpus.
</bodyText>
<sectionHeader confidence="0.998417" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999978769230769">
We evaluate the quality of our three topic models
(LDA, SVD, and NMF) with three experiments. We
focus first on evaluating aggregate coherence meth-
ods for a complete topic model and consider the
differences between each model as we learn an in-
creasing number of topics. Secondly, we compare
coherence scores to previous semantic evaluations.
Lastly, we use the learned topics in a classifica-
tion task and evaluate whether or not coherent top-
ics are equally informative when discriminating be-
tween documents.
For all our experiments, we trained our models on
92,600 New York Times articles from 2003 (Sand-
haus, 2008). For all articles, we removed stop words
and any words occurring less than 200 times in the
corpus, which left 35,836 unique tokens. All doc-
uments were tokenized with OpenNLP’s MaxEnt4
tokenizer. For the UCI measure, we compute the
PMI between words using a 20 word sliding win-
dow passed over the WaCkypedia corpus (Baroni et
al., 2009). In all experiments, we compute the co-
herence with the top 10 words from each topic that
had the highest weight, in terms of LDA and NMF
this corresponds with a high probability of the term
describing the topic but for SVD there is no clear
semantic interpretation.
</bodyText>
<subsectionHeader confidence="0.817802">
4.1 Aggregate methods for topic coherence
</subsectionHeader>
<bodyText confidence="0.99889972">
Before we can compare topic models, we require an
aggregate measure that represents the quality of a
complete model, rather than individual topics. We
consider two aggregates methods: (1) the average
coherence of all topics and (2) the entropy of the co-
herence for all topics. The average coherence pro-
vides a quick summarization of a model’s quality
whereas the entropy provides an alternate summa-
rization that differentiates between two interesting
situations. Since entropy measures the complexity
of a probability distribution, it can easily differenti-
ate between uniform distributions and multimodal,
distributions. This distinction is relevant when users
prefer to have roughly uniform topic quality instead
of a wide gap between high- and low-quality topics,
or vice versa. We compute the entropy by dropping
the log and c factor from each scoring function.
Figure 1 shows the average coherence scores for
each model as we vary the number of topics. These
average scores indicate some simple relationships
between the models: LDA and NMF have approx-
imately the same performance and both models are
consistently better than SVD. All of the models
quickly reach a stable average score at around 100
topics. This initially suggests that learning more
</bodyText>
<footnote confidence="0.978757">
4http://incubator.apache.org/opennlp/
</footnote>
<page confidence="0.990605">
955
</page>
<figure confidence="0.998494363636364">
Average Topic Coherence
−1
−2
−3
−4
−5
0
Average Topic Coherence
−10
−2
−4
−6
−8
2
0
Method
LDA
NMF
SVD
100 200 300 400 500 100 200 300 400 500
Number of topics Number of topics
(a) UMass (b) UCI
</figure>
<figureCaption confidence="0.9999935">
Figure 1: Average Topic Coherence for each model
Figure 2: Entropy of the Topic Coherence for each model
</figureCaption>
<figure confidence="0.99763868">
100 200 300 400 500 100 200 300 400 500
Number of topics Number of topics
(a) UMass (b) UCI
Coherence Entropy
7
6
5
4
3
2
0
1
Coherence Entropy
7
6
5
4
3
2
0
1
Method
LDA
NMF
SVD
</figure>
<bodyText confidence="0.999053857142857">
topics neither increases or decreases the quality of
the model, but Figure 2 indicates otherwise. While
the entropy for the UMass score stays stable for all
models, NMF produces erratic entropy results under
the UCI score as we learn more topics. As entropy is
higher for even distributions and lower for all other
distributions, these results suggest that the NMF is
learning topics with drastically different levels of
quality, i.e. some with high quality and some with
very low quality, but the average coherence over all
topics do not account for this.
Low quality topics may be composed of highly
unrelated words that can’t be fit into another topic,
and in this case, our smoothing factor, E, may be ar-
tificially increasing the score for unrelated words.
Following the practice of the original use of these
metrics, in Figures 1 and 2 we set E = 1. In Fig-
ure 3, we consider E = 10−12, which should sig-
nificantly reduce the score for completely unrelated
words. Here, we see a significant change in the per-
formance of NMF, the average coherence decreases
dramatically as we learn more topics. Similarly, per-
formance of SVD drops dramatically and well below
the other models. In figure 4 we lastly compute the
average coherence using only the top 10% most co-
herence topics with E = 10−12. Here, NMF again
performs on par with LDA. With the top 10% topics
still having a high average coherence but the full set
</bodyText>
<page confidence="0.992711">
956
</page>
<figure confidence="0.998738409090909">
Average Topic Coherence
−1
−2
−3
−4
−5
0
Average Topic Coherence
−10
−2
−4
−6
−8
2
0
Method
LDA
NMF
SVD
100 200 300 400 500 100 200 300 400 500
Number of topics Number of topics
(a) UMass (b) UCI
</figure>
<figureCaption confidence="0.9999895">
Figure 3: Average Topic Coherence with E = 10−12
Figure 4: Average Topic Coherence of the top 10% topics with E = 10−12
</figureCaption>
<figure confidence="0.997518181818182">
100 200 300 400 500 100 200 300 400 500
Number of topics Number of topics
(a) UMass (b) UCI
Average Coherence of top 10%
−1
−2
−3
−4
−5
0
Average Coherence of top 10%
−10
−2
−4
−6
−8
2
0
..ate
LDA
NMF
SVD
</figure>
<bodyText confidence="0.9983855">
of topics having a low coherence, NMF appears to
be learning more low quality topics once it’s learned
the first 100 topics, whereas LDA learns fewer low
quality topics in general.
</bodyText>
<subsectionHeader confidence="0.996072">
4.2 Word Similarity Tasks
</subsectionHeader>
<bodyText confidence="0.999969181818182">
The initial evaluations for each coherence mea-
sure asked human judges to directly evaluate top-
ics (Newman et al., 2010; Mimno et al., 2011). We
expand upon this comparison to human judgments
by considering word similarity tasks that have of-
ten been used to evaluate distributional semantic
spaces (Jurgens and Stevens, 2010). Here, we use
the learned topics as generalized semantics describ-
ing our knowledge about words. If a model’s topics
generalize the knowledge accurately, we would ex-
pect similar words, such as “cat” and “dog”, to be
represented with a similar set of topics. Rather than
evaluating individual topics, this similarity task con-
siders the knowledge within the entire set of topics,
the topics act as more compact representation for the
known words in a corpus.
We use the Rubenstein and Goodenough (1965)
and Finkelstein et al. (2002) word similarity tasks.
In each task, human judges were asked to evaluate
the similarity or relatedness between different sets of
word pairs. Fifty-One Evaluators for the Rubenstein
and Goodenough (1965) dataset were given 65 pairs
</bodyText>
<page confidence="0.974102">
957
</page>
<figure confidence="0.999635571428571">
model
LDA
NMF
SVD
100 200 300 400 500
T
(a) Rubenstein &amp; Goodenough
(b) Wordsim 353/Finklestein et. al.
100 200 300 400 500
T
score
0.5
0.4
0.3
0.2
0.1
0.0
model
LDA
NMF
SVD
score 0.6
0.5
0.4
0.3
0.2
0.1
0.0
</figure>
<figureCaption confidence="0.9999835">
Figure 5: Word Similarity Evaluations for each model
Figure 7: Correlation between topic coherence and topic ranking in classification
</figureCaption>
<figure confidence="0.996102529411765">
100 200 300 400 500 100 200 300 400 500
Topics Topics
(a) UMass (b) UCI
Correlation
0.6
0.4
0.2
0.0
Correlation
0.6
0.4
0.2
0.0
model
LDA
NMF
SVD
</figure>
<bodyText confidence="0.993596695652174">
of words and asked to rate their similarity on a scale
from 0 to 4, where a higher score indicates a more
similar word pair. Finkelstein et al. (2002) broadens
the word similarity evaluation and asked 13 to 16
different subjects to rate 353 word pairs on a scale
from 0 to 10 based on their relatedness, where relat-
edness includes similarity and other semantic rela-
tions. We can evaluate each topic model by comput-
ing the cosine similarity between each pair of words
in the evaluate set and then compare the model’s
ratings to the human ratings by ranked correlation.
A high correlation signifies that the topics closely
model human judgments.
Figure 5 displays the results. SVD and LDA
both surpass NMF on the Rubenstein &amp; Goode-
nough test while SVD is clearly the best model on
the Finklestein et. al test. While our first experi-
ment showed that SVD was the worst model in terms
of topic coherence scores, this experiment indicates
that SVD provides an accurate, stable, and reliable
approximation to human judgements of similarity
and relatedness between word pairs in comparison
to other topic models.
</bodyText>
<subsectionHeader confidence="0.999458">
4.3 Coherence versus Classification
</subsectionHeader>
<bodyText confidence="0.999974">
For our final experiment, we examine the relation-
ship between topic coherence and classification ac-
curacy for each topic model. We suspect that highly
</bodyText>
<page confidence="0.985059">
958
</page>
<figure confidence="0.999651">
Correlation
0.07
0.06
0.05
0.04
0.03
0.02
0.01
Correlation
0.07
0.06
0.05
0.04
0.03
0.02
0.01
model
LDA
NMF
SVD
−25 −20 −15 −10 −5 −30 −20 −10 0
score score
(a) UMass (b) UCI
</figure>
<figureCaption confidence="0.983792">
Figure 8: Comparison between topic coherence and topic rank with 500 topics
</figureCaption>
<table confidence="0.332537">
100 200 300 400 500
</table>
<subsectionHeader confidence="0.4589">
Topics
</subsectionHeader>
<bodyText confidence="0.9987272">
label is applied to at least 2000 documents. This re-
sults in 57,696 articles with label distributions listed
in Table 2. We then represent each document using
columns in the topic by document matrix H learned
for each topic model.
</bodyText>
<table confidence="0.955769">
LDA Count Label Count
NMF
SVD Label
New York and Region 11219 U.S. 3675
Paid Death Notices 11152 Arts 3437
Opinion 8038 World 3330
Business 7494 Style 2137
Sports 7214
</table>
<figure confidence="0.938528875">
Accuracy 80
70
60
50
40
30
20
Model
</figure>
<figureCaption confidence="0.9223485">
Figure 6: Classification accuracy for each model Table 2: Section label counts for New York Times
articles used for classification
</figureCaption>
<bodyText confidence="0.999923407407407">
coherent topics, and coherent topic models, will per-
form better for classification. We address this ques-
tion by performing a document classification task
using the topic representations of documents as in-
put features and examine the relationship between
topic coherence and the usefulness of the corre-
sponding feature for classification.
We trained each topic model with all 92,600 New
York Times articles as before. We use the sec-
tion labels provided for each article as class labels,
where each label indicates the on-line section(s) un-
der which the article was published and should thus
be related to the topics contained in each article. To
reduce the noise in our data set we narrow down the
articles to those that have only one label and whose
For each topic model trained on N topics, we
performed stratified 10-fold cross-validation on the
57,696 labeled articles. In each fold, we build an
automatically-sized bagged ensemble of unpruned
CART-style decision trees(Banfield et al., 2007) on
90% of the datasets, use that ensemble to assign la-
bels to the other 10%, and measure the accuracy of
that assignment. Figure 6 shows the average classifi-
cation accuracy over all ten folds for each model. In-
terestingly, SVD has slightly, but statistically signif-
icantly, higher accuracy results than both NMF and
LDA. Furthermore, performance quickly increases
</bodyText>
<footnote confidence="0.917151333333333">
5The precise choice of the classifier scheme matters little, as
long as it is accurate, speedy, and robust to label noise; all of
which is true of the choice here.
</footnote>
<page confidence="0.996755">
959
</page>
<bodyText confidence="0.999907565217391">
and plateaus with well under 50 topics.
Our bagged decision trees can also determine the
importance of each feature during classification. We
evaluate the strength of each topic during classifi-
cation by tracking the number of times each node
in our decision trees observe each topic, please see
(Caruana et al., 2006) for more details. Figure 8 plot
the relationship between this feature ranking and the
topic coherence for each topic when training LDA,
SVD, and NMF on 500 topics. Most topics for each
model provide little classification information, but
SVD shows a much higher rank for several topics
with a relatively higher coherence score. Interest-
ingly, for all models, the most coherent topics are not
the most informative. Figure 7 plots a more compact
view of this same relationship: the Spearman rank
correlation between classification feature rank and
topic coherence. NMF shows the highest correlation
between rank and coherence, but none of the mod-
els show a high correlation when using more than
100 topics. SVD has the lowest correlation, which
is probably due to the model’s overall low coherence
yet high classification accuracy.
</bodyText>
<sectionHeader confidence="0.996714" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999987363636364">
Through our experiments, we made several excit-
ing and interesting discoveries. First, we discov-
ered that the coherence metrics depend heavily on
the smoothing factor c. The original value, 1.0 cre-
ated a positive bias towards NMF from both met-
rics even when NMF generated incoherent topics.
The high smoothing factor also gave a significant in-
crease to SVD scores. We suspect that this was not
an issue in previous studies with the coherence mea-
sures as LDA prefers to form topics from words that
co-occur frequently, whereas NMF and SVD have
no such preferences and often create low quality top-
ics from completely unrelated words. Therefore, we
suggest a smaller c value in general.
We also found that the UCI measure often agreed
with the UMass measure, but the UCI-entropy ag-
gregate method induced more separation between
LSA, SVD, and NMF in terms of topic coherence.
This measure also revealed the importance of the
smoothing factor for topic coherence measures.
With respects to human judgements, we found
that coherence scores do not always indicate a bet-
ter representation of distributional information. The
SVD model consistently out performed both LDA
and NMF models, which each had higher coherence
scores, when attempting to predict human judge-
ments of similarity.
Lastly, we found all models capable of producing
topics that improved document classification. At the
same time, SVD provided the most information dur-
ing classification and outperformed the other mod-
els, which again had more coherent topics. Our com-
parison between topic coherence scores and feature
importance in classification revealed that relatively
high quality topics, but not the most coherent topics,
drive most of the classification decisions, and most
topics do not affect the accuracy.
Overall, we see that each topic model paradigm
has it’s own strengths and weaknesses. Latent Se-
mantic Analysis with Singular Value Decomposition
fails to form individual topics that aggregate similar
words, but it does remarkably well when consider-
ing all the learned topics as similar words develop
a similar topic representation. These topics simi-
larly perform well during classification. Conversely,
both Non Negative Matrix factorization and Latent
Dirichlet Allocation learn concise and coherent top-
ics and achieved similar performance on our evalua-
tions. However, NMF learns more incoherent topics
than LDA and SVD. For applications in which a hu-
man end-user will interact with learned topics, the
flexibility of LDA and the coherence advantages of
LDA warrant strong consideration. All of code for
this work will be made available through an open
source project.6
</bodyText>
<sectionHeader confidence="0.999594" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999840333333333">
This work was performed under the auspices of
the U.S. Department of Energy by Lawrence Liv-
ermore National Laboratory under Contract DE-
AC52-07NA27344 (LLNL-CONF-522871) and by
Sandia National Laboratory under Contract DE-
AC04-94AL85000.
</bodyText>
<sectionHeader confidence="0.998157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8643595">
David Andrzejewski and David Buttler. 2011. Latent
topic feedback for information retrieval. In Proceed-
</reference>
<footnote confidence="0.801456">
6https://github.com/fozziethebeat/TopicModelComparison
</footnote>
<page confidence="0.988999">
960
</page>
<reference confidence="0.999656703296703">
ings of the 17th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
’11, pages 600–608, New York, NY, USA. ACM.
Robert E. Banfield, Lawrence O. Hall, Kevin W. Bowyer,
and W. Philip Kegelmeyer. 2007. A comparison of de-
cision tree ensemble creation techniques. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
29(1):173–180, January.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, EACL ’09, pages 103–
111, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Rich Caruana, Mohamed Elhawary, Art Munson, Mirek
Riedewald, Daria Sorokina, Daniel Fink, Wesley M.
Hochachka, and Steve Kelling. 2006. Mining cit-
izen science data to predict orevalence of wild bird
species. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ’06, pages 909–915, New York,
NY, USA. ACM.
Jonathan Chang, Sean Gerrish, Chong Wang, and
David M Blei. 2009. Reading tea leaves : How hu-
mans interpret topic models. New York, 31:1–9.
Chris Ding, Tao Li, and Wei Peng. 2008. On the equiv-
alence between non-negative matrix factorization and
probabilistic latent semantic indexing. Comput. Stat.
Data Anal., 52:3913–3927, April.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: the concept
revisited. ACM Trans. Inf. Syst., 20:116–131, January.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235, April.
David Jurgens and Keith Stevens. 2010. The s-space
package: an open source package for word space mod-
els. In Proceedings of the ACL 2010 System Demon-
strations, ACLDemos ’10, pages 30–35, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Thomas K Landauer and Susan T. Dutnais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211–240.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham.
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, (25):259–284.
Daniel D. Lee and H. Sebastian Seung. 2000. Algo-
rithms for non-negative matrix factorization. In In
NIPS, pages 556–562. MIT Press.
David Mimno, Hanna Wallach, Edmund Talley, Miriam
Leenders, and Andrew McCallum. 2011. Optimizing
semantic coherence in topic models. In Proceedings of
the 2011 Conference on Emperical Methods in Natu-
ral Language Processing, pages 262–272, Edinburgh,
Scotland, UK. Association of Computational Linguis-
tics.
David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin. 2010. Evaluating topic
models for digital libraries. In Proceedings of the 10th
annual joint conference on Digital libraries, JCDL
’10, pages 215–224, New York, NY, USA. ACM.
V Paul Pauca, Farial Shahnaz, Michael W Berry, and
Robert J Plemmons, 2004. Text mining using nonnega-
tive matrix factorizations, volume 54, pages 452–456.
SIAM.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8:627–633, October.
Evan Sandhaus. 2008. The New York Times Annotated
Corpus.
Tim Van de Cruys and Marianna Apidianaki. 2011. La-
tent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, HLT ’11,
pages 1476–1485, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In Proceedings of the 26th International Con-
ference on Machine Learning (ICML). Omnipress.
</reference>
<page confidence="0.998183">
961
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.449474">
<title confidence="0.99972">Exploring Topic Coherence over many models and many topics</title>
<author confidence="0.999934">Philip David David</author>
<affiliation confidence="0.8671435">of California Los Angeles; Los Angeles, California, Livermore National Lab; Livermore, California,</affiliation>
<address confidence="0.664867">National Lab; Livermore, California,</address>
<email confidence="0.999015">wpk@sandia.gov</email>
<abstract confidence="0.999739421052632">We apply two new automated semantic evaluations to three distinct latent topic models. Both metrics have been shown to align with human evaluations and provide a balance between internal measures of information gain and comparisons to human ratings of coherent topics. We improve upon the measures by introducing new aggregate measures that allows for comparing complete topic models. We further compare the automated measures to other metrics for topic models, comparison to manually crafted semantic tests and document classification. Our experiments reveal that LDA and LSA each have different strengths; LDA best learns descriptive topics while LSA is best at creating a compact semantic representation of documents and words in a corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>David Buttler</author>
</authors>
<title>Latent topic feedback for information retrieval.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’11,</booktitle>
<pages>600--608</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1899" citStr="Andrzejewski and Buttler, 2011" startWordPosition="277" endWordPosition="280">ine topic level relations by assuming that a single document covers a small set of concise topics. Once learned, these topics often correlate well with human concepts. For example, one model might produce topics that cover ideas such as government affairs, sports, and movies. With these unsupervised methods, we can extract useful semantic information in a variety of tasks that depend on identifying unique topics or concepts, such as distributional semantics (Jurgens and Stevens, 2010), word sense induction (Van de Cruys and Apidianaki, 2011; Brody and Lapata, 2009), and information retrieval (Andrzejewski and Buttler, 2011). When using a topic model, we are primarily concerned with the degree to which the learned topics match human judgments and help us differentiate between ideas. But until recently, the evaluation of these models has been ad hoc and application specific. Evaluations have ranged from fully automated intrinsic evaluations to manually crafted extrinsic evaluations. Previous extrinsic evaluations have used the learned topics to compactly represent a small fixed vocabulary and compared this distributional space to human judgments of similarity (Jurgens and Stevens, 2010). But these evaluations are </context>
</contexts>
<marker>Andrzejewski, Buttler, 2011</marker>
<rawString>David Andrzejewski and David Buttler. 2011. Latent topic feedback for information retrieval. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’11, pages 600–608, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Banfield</author>
<author>Lawrence O Hall</author>
<author>Kevin W Bowyer</author>
<author>W Philip Kegelmeyer</author>
</authors>
<title>A comparison of decision tree ensemble creation techniques.</title>
<date>2007</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="22783" citStr="Banfield et al., 2007" startWordPosition="3787" endWordPosition="3790">l 92,600 New York Times articles as before. We use the section labels provided for each article as class labels, where each label indicates the on-line section(s) under which the article was published and should thus be related to the topics contained in each article. To reduce the noise in our data set we narrow down the articles to those that have only one label and whose For each topic model trained on N topics, we performed stratified 10-fold cross-validation on the 57,696 labeled articles. In each fold, we build an automatically-sized bagged ensemble of unpruned CART-style decision trees(Banfield et al., 2007) on 90% of the datasets, use that ensemble to assign labels to the other 10%, and measure the accuracy of that assignment. Figure 6 shows the average classification accuracy over all ten folds for each model. Interestingly, SVD has slightly, but statistically significantly, higher accuracy results than both NMF and LDA. Furthermore, performance quickly increases 5The precise choice of the classifier scheme matters little, as long as it is accurate, speedy, and robust to label noise; all of which is true of the choice here. 959 and plateaus with well under 50 topics. Our bagged decision trees c</context>
</contexts>
<marker>Banfield, Hall, Bowyer, Kegelmeyer, 2007</marker>
<rawString>Robert E. Banfield, Lawrence O. Hall, Kevin W. Bowyer, and W. Philip Kegelmeyer. 2007. A comparison of decision tree ensemble creation techniques. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(1):173–180, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="13837" citStr="Baroni et al., 2009" startWordPosition="2235" endWordPosition="2238">luations. Lastly, we use the learned topics in a classification task and evaluate whether or not coherent topics are equally informative when discriminating between documents. For all our experiments, we trained our models on 92,600 New York Times articles from 2003 (Sandhaus, 2008). For all articles, we removed stop words and any words occurring less than 200 times in the corpus, which left 35,836 unique tokens. All documents were tokenized with OpenNLP’s MaxEnt4 tokenizer. For the UCI measure, we compute the PMI between words using a 20 word sliding window passed over the WaCkypedia corpus (Baroni et al., 2009). In all experiments, we compute the coherence with the top 10 words from each topic that had the highest weight, in terms of LDA and NMF this corresponds with a high probability of the term describing the topic but for SVD there is no clear semantic interpretation. 4.1 Aggregate methods for topic coherence Before we can compare topic models, we require an aggregate measure that represents the quality of a complete model, rather than individual topics. We consider two aggregates methods: (1) the average coherence of all topics and (2) the entropy of the coherence for all topics. The average co</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky wide web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="3006" citStr="Blei et al., 2003" startWordPosition="446" endWordPosition="449">s distributional space to human judgments of similarity (Jurgens and Stevens, 2010). But these evaluations are hand constructed and often costly to perform for domain-specific topics. Conversely, intrinsic measures have evaluated the amount of information encoded by the topics, where perplexity is one common example(Wallach et al., 2009), however, Chang et al. (2009) found that these intrinsic measures do not always correlate with semantically interpretable topics. Furthermore, few evaluations have used the same metrics to compare distinct approaches such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Latent Semantic Analysis (LSA) (Landauer and Dutnais, 1997), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 2000). This has made it difficult to know which method is most useful for a given application, or in terms of extracting useful topics. We now provide a comprehensive and automated evaluation of these three distinct models (LDA, LSA, NMF), for automatically learning semantic topics. While these models have seen significant improvements, they still represent the core differences between each approach to modeling topics. For our evaluation, we use two recent automated cohere</context>
<context position="5878" citStr="Blei et al., 2003" startWordPosition="909" endWordPosition="912">hing between documents focusing on different concepts. We distill the different models into a shared representation consisting of two sets of learned relations: how words interact with topics and how topics interact with documents. For a corpus with D documents and V words, we denote these relations in terms of T topics as (1) a V x T matrix, W, that indicates the strength each word has in each topic, and (2) a T x D matrix, H, that indicates the strength each topic has in each document. T serves as a common parameter to each model. 2.1 Latent Dirichlet Allocation Latent Dirichlet Allocation (Blei et al., 2003) learns the relationships between words, topics, and documents by assuming documents are generated by a particular probabilistic model. It first assumes that there are a fixed set of topics, T used throughout the corpus, and each topic z is associated with a multinomial distribution over the vocabulary 4b, which is drawn from a Dirichlet prior Dir(Q). A given document Di is then generated by the following process 1. Choose Oi — Dir(α), a topic distribution for Di 2. For each word wj E Di: (a) Select a topic zj — Oi (b) Select the word wj — 4)z In this model, the O distributions represent the p</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1839" citStr="Brody and Lapata, 2009" startWordPosition="270" endWordPosition="273">n. Based on the words used within a document, they mine topic level relations by assuming that a single document covers a small set of concise topics. Once learned, these topics often correlate well with human concepts. For example, one model might produce topics that cover ideas such as government affairs, sports, and movies. With these unsupervised methods, we can extract useful semantic information in a variety of tasks that depend on identifying unique topics or concepts, such as distributional semantics (Jurgens and Stevens, 2010), word sense induction (Van de Cruys and Apidianaki, 2011; Brody and Lapata, 2009), and information retrieval (Andrzejewski and Buttler, 2011). When using a topic model, we are primarily concerned with the degree to which the learned topics match human judgments and help us differentiate between ideas. But until recently, the evaluation of these models has been ad hoc and application specific. Evaluations have ranged from fully automated intrinsic evaluations to manually crafted extrinsic evaluations. Previous extrinsic evaluations have used the learned topics to compactly represent a small fixed vocabulary and compared this distributional space to human judgments of simila</context>
<context position="5117" citStr="Brody and Lapata, 2009" startWordPosition="777" endWordPosition="780">ome overall remarks. 2 Topic Models We evaluate three latent factor models that have seen widespread usage: 1. Latent Dirichlet Allocation 2. Latent Semantic Analysis with Singular Value Decomposition 3. Latent Semantic Analysis with Non-negative Matrix Factorization Each of these models were designed with different goals and are supported by different statistical theories. We consider both LSA models as topic models as they have been used in a variety of similar contexts such as distributional similarity (Jurgens and Stevens, 2010) and word sense induction (Van de Cruys and Apidianaki, 2011; Brody and Lapata, 2009). We evaluate these distinct models on two shared tasks (1) grouping together similar words while separating unrelated words and (2) distinguishing between documents focusing on different concepts. We distill the different models into a shared representation consisting of two sets of learned relations: how words interact with topics and how topics interact with documents. For a corpus with D documents and V words, we denote these relations in terms of T topics as (1) a V x T matrix, W, that indicates the strength each word has in each topic, and (2) a T x D matrix, H, that indicates the streng</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, pages 103– 111, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich Caruana</author>
<author>Mohamed Elhawary</author>
<author>Art Munson</author>
<author>Mirek Riedewald</author>
<author>Daria Sorokina</author>
<author>Daniel Fink</author>
<author>Wesley M Hochachka</author>
<author>Steve Kelling</author>
</authors>
<title>Mining citizen science data to predict orevalence of wild bird species.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06,</booktitle>
<pages>909--915</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="23633" citStr="Caruana et al., 2006" startWordPosition="3929" endWordPosition="3932">as slightly, but statistically significantly, higher accuracy results than both NMF and LDA. Furthermore, performance quickly increases 5The precise choice of the classifier scheme matters little, as long as it is accurate, speedy, and robust to label noise; all of which is true of the choice here. 959 and plateaus with well under 50 topics. Our bagged decision trees can also determine the importance of each feature during classification. We evaluate the strength of each topic during classification by tracking the number of times each node in our decision trees observe each topic, please see (Caruana et al., 2006) for more details. Figure 8 plot the relationship between this feature ranking and the topic coherence for each topic when training LDA, SVD, and NMF on 500 topics. Most topics for each model provide little classification information, but SVD shows a much higher rank for several topics with a relatively higher coherence score. Interestingly, for all models, the most coherent topics are not the most informative. Figure 7 plots a more compact view of this same relationship: the Spearman rank correlation between classification feature rank and topic coherence. NMF shows the highest correlation be</context>
</contexts>
<marker>Caruana, Elhawary, Munson, Riedewald, Sorokina, Fink, Hochachka, Kelling, 2006</marker>
<rawString>Rich Caruana, Mohamed Elhawary, Art Munson, Mirek Riedewald, Daria Sorokina, Daniel Fink, Wesley M. Hochachka, and Steve Kelling. 2006. Mining citizen science data to predict orevalence of wild bird species. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’06, pages 909–915, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David M Blei</author>
</authors>
<title>Reading tea leaves : How humans interpret topic models.</title>
<date>2009</date>
<pages>31--1</pages>
<location>New York,</location>
<contexts>
<context position="2757" citStr="Chang et al. (2009)" startWordPosition="411" endWordPosition="414">ation specific. Evaluations have ranged from fully automated intrinsic evaluations to manually crafted extrinsic evaluations. Previous extrinsic evaluations have used the learned topics to compactly represent a small fixed vocabulary and compared this distributional space to human judgments of similarity (Jurgens and Stevens, 2010). But these evaluations are hand constructed and often costly to perform for domain-specific topics. Conversely, intrinsic measures have evaluated the amount of information encoded by the topics, where perplexity is one common example(Wallach et al., 2009), however, Chang et al. (2009) found that these intrinsic measures do not always correlate with semantically interpretable topics. Furthermore, few evaluations have used the same metrics to compare distinct approaches such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Latent Semantic Analysis (LSA) (Landauer and Dutnais, 1997), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 2000). This has made it difficult to know which method is most useful for a given application, or in terms of extracting useful topics. We now provide a comprehensive and automated evaluation of these three distinct models (LDA,</context>
</contexts>
<marker>Chang, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Sean Gerrish, Chong Wang, and David M Blei. 2009. Reading tea leaves : How humans interpret topic models. New York, 31:1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Tao Li</author>
<author>Wei Peng</author>
</authors>
<title>On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing.</title>
<date>2008</date>
<journal>Comput. Stat. Data Anal.,</journal>
<pages>52--3913</pages>
<contexts>
<context position="9272" citStr="Ding et al., 2008" startWordPosition="1452" endWordPosition="1455">lbarrow tree clematis SVD city building city area buildings p.m. floors house listed eat-in a.m. time p.m. system study a.m. office political found school night yesterday UMass UCI -2.52 1.29 -1.97 1.30 -1.01 1.98 -1.84 1.46 -1.87 -1.21 -2.30 -1.88 -1.94 -12.32 -2.43 -8.88 -3.12 -12.59 -1.90 -12.56 -2.70 -8.03 -1.67 -7.02 Table 1: Top 10 words from several high and low quality topics when ordered by the UCI Coherence Measure. Topic labels were chosen in an ad hoc manner only to briefly summarize the topic’s focus. have found a connection between NMF and Probabilistic Latent Semantic Analysis (Ding et al., 2008), an extension to LSA. We later refer to these two LSA models simply as SVD and NMF to emphasize the difference in factorization method. Singular Value Decomposition decomposes M into three smaller matrices M = UEVT and minimizes Frobenius norm of M’s reconstruction error with the constraint that the rows of U and V are orthonormal eigenvectors. Interestingly, the decomposition is agnostic to the number of desired dimensions. Instead, the rows and columns in U and V T are ordered based on their descriptive power, i.e. how well they remove noise, which is encoded by the diagonal singular value </context>
<context position="11633" citStr="Ding et al., 2008" startWordPosition="1853" endWordPosition="1856">en topics that are semantically interpretable topics and topics that are artifacts of statistical inference, see Table 1 for examples ordered by the UCI measure. For our evaluations, we consider two new coherence measures designed for LDA, both of which have been shown to match well with human judgements of topic quality: (1) The UCI measure (Newman et al., 2010) and (2) The UMass measure (Mimno et al., 2011). Both measures compute the coherence of a topic as the sum of pairwise distributional similarity 3We note that the alternative KL-Divergence form of NMF has been directly linked to PLSA (Ding et al., 2008) 954 scores over the set of topic words, V . We generalize this as �coherence(V ) = score(vi, vj, E) (vi,vj)EV where V is a set of word describing the topic and c indicates a smoothing factor which guarantees that score returns real numbers. (We will be exploring the effect of the choice of c; the original authors used E = 1.) The UCI metric defines a word pair’s score to be the pointwise mutual information (PMI) between two words, i.e. score(vi, vj, E) = log p(vi, vj) + � p(vi)p(vj) The word probabilities are computed by counting word co-occurrence frequencies in a sliding window over an exte</context>
</contexts>
<marker>Ding, Li, Peng, 2008</marker>
<rawString>Chris Ding, Tao Li, and Wei Peng. 2008. On the equivalence between non-negative matrix factorization and probabilistic latent semantic indexing. Comput. Stat. Data Anal., 52:3913–3927, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: the concept revisited.</title>
<date>2002</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<pages>20--116</pages>
<contexts>
<context position="18923" citStr="Finkelstein et al. (2002)" startWordPosition="3128" endWordPosition="3131">hat have often been used to evaluate distributional semantic spaces (Jurgens and Stevens, 2010). Here, we use the learned topics as generalized semantics describing our knowledge about words. If a model’s topics generalize the knowledge accurately, we would expect similar words, such as “cat” and “dog”, to be represented with a similar set of topics. Rather than evaluating individual topics, this similarity task considers the knowledge within the entire set of topics, the topics act as more compact representation for the known words in a corpus. We use the Rubenstein and Goodenough (1965) and Finkelstein et al. (2002) word similarity tasks. In each task, human judges were asked to evaluate the similarity or relatedness between different sets of word pairs. Fifty-One Evaluators for the Rubenstein and Goodenough (1965) dataset were given 65 pairs 957 model LDA NMF SVD 100 200 300 400 500 T (a) Rubenstein &amp; Goodenough (b) Wordsim 353/Finklestein et. al. 100 200 300 400 500 T score 0.5 0.4 0.3 0.2 0.1 0.0 model LDA NMF SVD score 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Figure 5: Word Similarity Evaluations for each model Figure 7: Correlation between topic coherence and topic ranking in classification 100 200 300 400 500 1</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: the concept revisited. ACM Trans. Inf. Syst., 20:116–131, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="6919" citStr="Griffiths and Steyvers, 2004" startWordPosition="1090" endWordPosition="1093">cess 1. Choose Oi — Dir(α), a topic distribution for Di 2. For each word wj E Di: (a) Select a topic zj — Oi (b) Select the word wj — 4)z In this model, the O distributions represent the probability of each topic appearing in each document and the 4b distributions represent the probability of words being used for each topic. These two sets of distributions correspond to our H and W matrices, respectively. The process above defines a generative model; given the observed corpus, we use collapsed Gibbs sampling implementation found in Mallet1 to infer the values of the latent variables 4b and O (Griffiths and Steyvers, 2004). The model relies only on two additional hyper parameters, α and Q, that guide the distributions. 2.2 Latent Semantic Analysis Latent Semantic Analysis (Landauer and Dutnais, 1997; Landauer et al., 1998) learns topics by first forming a traditional term by document matrix used in information retrieval and then smoothing the counts to enhance the weight of informative words. Based on the original LSA model, we use the LogEntropy transform. LSA then decomposes this smoothed, term by document matrix in order to generalize observed relations between words and documents. For both LSA models, we us</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>The s-space package: an open source package for word space models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10,</booktitle>
<pages>30--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1757" citStr="Jurgens and Stevens, 2010" startWordPosition="257" endWordPosition="260">on Topic models learn bags of related words from large corpora without any supervision. Based on the words used within a document, they mine topic level relations by assuming that a single document covers a small set of concise topics. Once learned, these topics often correlate well with human concepts. For example, one model might produce topics that cover ideas such as government affairs, sports, and movies. With these unsupervised methods, we can extract useful semantic information in a variety of tasks that depend on identifying unique topics or concepts, such as distributional semantics (Jurgens and Stevens, 2010), word sense induction (Van de Cruys and Apidianaki, 2011; Brody and Lapata, 2009), and information retrieval (Andrzejewski and Buttler, 2011). When using a topic model, we are primarily concerned with the degree to which the learned topics match human judgments and help us differentiate between ideas. But until recently, the evaluation of these models has been ad hoc and application specific. Evaluations have ranged from fully automated intrinsic evaluations to manually crafted extrinsic evaluations. Previous extrinsic evaluations have used the learned topics to compactly represent a small fi</context>
<context position="5032" citStr="Jurgens and Stevens, 2010" startWordPosition="762" endWordPosition="766"> a series of experiments that address our four key questions and finally conclude with some overall remarks. 2 Topic Models We evaluate three latent factor models that have seen widespread usage: 1. Latent Dirichlet Allocation 2. Latent Semantic Analysis with Singular Value Decomposition 3. Latent Semantic Analysis with Non-negative Matrix Factorization Each of these models were designed with different goals and are supported by different statistical theories. We consider both LSA models as topic models as they have been used in a variety of similar contexts such as distributional similarity (Jurgens and Stevens, 2010) and word sense induction (Van de Cruys and Apidianaki, 2011; Brody and Lapata, 2009). We evaluate these distinct models on two shared tasks (1) grouping together similar words while separating unrelated words and (2) distinguishing between documents focusing on different concepts. We distill the different models into a shared representation consisting of two sets of learned relations: how words interact with topics and how topics interact with documents. For a corpus with D documents and V words, we denote these relations in terms of T topics as (1) a V x T matrix, W, that indicates the stren</context>
<context position="18393" citStr="Jurgens and Stevens, 2010" startWordPosition="3041" endWordPosition="3044">of top 10% −1 −2 −3 −4 −5 0 Average Coherence of top 10% −10 −2 −4 −6 −8 2 0 ..ate LDA NMF SVD of topics having a low coherence, NMF appears to be learning more low quality topics once it’s learned the first 100 topics, whereas LDA learns fewer low quality topics in general. 4.2 Word Similarity Tasks The initial evaluations for each coherence measure asked human judges to directly evaluate topics (Newman et al., 2010; Mimno et al., 2011). We expand upon this comparison to human judgments by considering word similarity tasks that have often been used to evaluate distributional semantic spaces (Jurgens and Stevens, 2010). Here, we use the learned topics as generalized semantics describing our knowledge about words. If a model’s topics generalize the knowledge accurately, we would expect similar words, such as “cat” and “dog”, to be represented with a similar set of topics. Rather than evaluating individual topics, this similarity task considers the knowledge within the entire set of topics, the topics act as more compact representation for the known words in a corpus. We use the Rubenstein and Goodenough (1965) and Finkelstein et al. (2002) word similarity tasks. In each task, human judges were asked to evalu</context>
</contexts>
<marker>Jurgens, Stevens, 2010</marker>
<rawString>David Jurgens and Keith Stevens. 2010. The s-space package: an open source package for word space models. In Proceedings of the ACL 2010 System Demonstrations, ACLDemos ’10, pages 30–35, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dutnais</author>
</authors>
<title>A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review,</title>
<date>1997</date>
<pages>211--240</pages>
<contexts>
<context position="3067" citStr="Landauer and Dutnais, 1997" startWordPosition="454" endWordPosition="457">ity (Jurgens and Stevens, 2010). But these evaluations are hand constructed and often costly to perform for domain-specific topics. Conversely, intrinsic measures have evaluated the amount of information encoded by the topics, where perplexity is one common example(Wallach et al., 2009), however, Chang et al. (2009) found that these intrinsic measures do not always correlate with semantically interpretable topics. Furthermore, few evaluations have used the same metrics to compare distinct approaches such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Latent Semantic Analysis (LSA) (Landauer and Dutnais, 1997), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 2000). This has made it difficult to know which method is most useful for a given application, or in terms of extracting useful topics. We now provide a comprehensive and automated evaluation of these three distinct models (LDA, LSA, NMF), for automatically learning semantic topics. While these models have seen significant improvements, they still represent the core differences between each approach to modeling topics. For our evaluation, we use two recent automated coherence measures (Mimno et al., 2011; Newman et al., 2010) 952 Pr</context>
<context position="7099" citStr="Landauer and Dutnais, 1997" startWordPosition="1118" endWordPosition="1121">the probability of each topic appearing in each document and the 4b distributions represent the probability of words being used for each topic. These two sets of distributions correspond to our H and W matrices, respectively. The process above defines a generative model; given the observed corpus, we use collapsed Gibbs sampling implementation found in Mallet1 to infer the values of the latent variables 4b and O (Griffiths and Steyvers, 2004). The model relies only on two additional hyper parameters, α and Q, that guide the distributions. 2.2 Latent Semantic Analysis Latent Semantic Analysis (Landauer and Dutnais, 1997; Landauer et al., 1998) learns topics by first forming a traditional term by document matrix used in information retrieval and then smoothing the counts to enhance the weight of informative words. Based on the original LSA model, we use the LogEntropy transform. LSA then decomposes this smoothed, term by document matrix in order to generalize observed relations between words and documents. For both LSA models, we used implementations found in the S-Space package.2 Traditionally, LSA has used the Singular Value Decomposition, but we also consider Non-negative Matrix Factorization as we’ve seen</context>
</contexts>
<marker>Landauer, Dutnais, 1997</marker>
<rawString>Thomas K Landauer and Susan T. Dutnais. 1997. A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="7123" citStr="Landauer et al., 1998" startWordPosition="1122" endWordPosition="1125">c appearing in each document and the 4b distributions represent the probability of words being used for each topic. These two sets of distributions correspond to our H and W matrices, respectively. The process above defines a generative model; given the observed corpus, we use collapsed Gibbs sampling implementation found in Mallet1 to infer the values of the latent variables 4b and O (Griffiths and Steyvers, 2004). The model relies only on two additional hyper parameters, α and Q, that guide the distributions. 2.2 Latent Semantic Analysis Latent Semantic Analysis (Landauer and Dutnais, 1997; Landauer et al., 1998) learns topics by first forming a traditional term by document matrix used in information retrieval and then smoothing the counts to enhance the weight of informative words. Based on the original LSA model, we use the LogEntropy transform. LSA then decomposes this smoothed, term by document matrix in order to generalize observed relations between words and documents. For both LSA models, we used implementations found in the S-Space package.2 Traditionally, LSA has used the Singular Value Decomposition, but we also consider Non-negative Matrix Factorization as we’ve seen NMF applied in similar </context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An Introduction to Latent Semantic Analysis. Discourse Processes, (25):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization. In</title>
<date>2000</date>
<booktitle>In NIPS,</booktitle>
<pages>556--562</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3134" citStr="Lee and Seung, 2000" startWordPosition="464" endWordPosition="467">d and often costly to perform for domain-specific topics. Conversely, intrinsic measures have evaluated the amount of information encoded by the topics, where perplexity is one common example(Wallach et al., 2009), however, Chang et al. (2009) found that these intrinsic measures do not always correlate with semantically interpretable topics. Furthermore, few evaluations have used the same metrics to compare distinct approaches such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Latent Semantic Analysis (LSA) (Landauer and Dutnais, 1997), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 2000). This has made it difficult to know which method is most useful for a given application, or in terms of extracting useful topics. We now provide a comprehensive and automated evaluation of these three distinct models (LDA, LSA, NMF), for automatically learning semantic topics. While these models have seen significant improvements, they still represent the core differences between each approach to modeling topics. For our evaluation, we use two recent automated coherence measures (Mimno et al., 2011; Newman et al., 2010) 952 Proceedings of the 2012 Joint Conference on Empirical Methods in Natu</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for non-negative matrix factorization. In In NIPS, pages 556–562. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing semantic coherence in topic models.</title>
<date>2011</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the 2011 Conference on Emperical Methods in Natural Language Processing,</booktitle>
<pages>262--272</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="3638" citStr="Mimno et al., 2011" startWordPosition="543" endWordPosition="546">tic Analysis (LSA) (Landauer and Dutnais, 1997), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 2000). This has made it difficult to know which method is most useful for a given application, or in terms of extracting useful topics. We now provide a comprehensive and automated evaluation of these three distinct models (LDA, LSA, NMF), for automatically learning semantic topics. While these models have seen significant improvements, they still represent the core differences between each approach to modeling topics. For our evaluation, we use two recent automated coherence measures (Mimno et al., 2011; Newman et al., 2010) 952 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 952–961, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics originally designed for LDA that bridge the gap between comparisons to human judgments and intrinsic measures such as perplexity. We consider several key questions: 1. How many topics should be learned? 2. How many learned topics are useful? 3. How do these topics relate to often used semantic tests? 4. How well do these topics iden</context>
<context position="11427" citStr="Mimno et al., 2011" startWordPosition="1819" endWordPosition="1822">H = HWT WH 3 Coherence Measures Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference, see Table 1 for examples ordered by the UCI measure. For our evaluations, we consider two new coherence measures designed for LDA, both of which have been shown to match well with human judgements of topic quality: (1) The UCI measure (Newman et al., 2010) and (2) The UMass measure (Mimno et al., 2011). Both measures compute the coherence of a topic as the sum of pairwise distributional similarity 3We note that the alternative KL-Divergence form of NMF has been directly linked to PLSA (Ding et al., 2008) 954 scores over the set of topic words, V . We generalize this as �coherence(V ) = score(vi, vj, E) (vi,vj)EV where V is a set of word describing the topic and c indicates a smoothing factor which guarantees that score returns real numbers. (We will be exploring the effect of the choice of c; the original authors used E = 1.) The UCI metric defines a word pair’s score to be the pointwise mu</context>
<context position="18208" citStr="Mimno et al., 2011" startWordPosition="3013" endWordPosition="3016">ure 4: Average Topic Coherence of the top 10% topics with E = 10−12 100 200 300 400 500 100 200 300 400 500 Number of topics Number of topics (a) UMass (b) UCI Average Coherence of top 10% −1 −2 −3 −4 −5 0 Average Coherence of top 10% −10 −2 −4 −6 −8 2 0 ..ate LDA NMF SVD of topics having a low coherence, NMF appears to be learning more low quality topics once it’s learned the first 100 topics, whereas LDA learns fewer low quality topics in general. 4.2 Word Similarity Tasks The initial evaluations for each coherence measure asked human judges to directly evaluate topics (Newman et al., 2010; Mimno et al., 2011). We expand upon this comparison to human judgments by considering word similarity tasks that have often been used to evaluate distributional semantic spaces (Jurgens and Stevens, 2010). Here, we use the learned topics as generalized semantics describing our knowledge about words. If a model’s topics generalize the knowledge accurately, we would expect similar words, such as “cat” and “dog”, to be represented with a similar set of topics. Rather than evaluating individual topics, this similarity task considers the knowledge within the entire set of topics, the topics act as more compact repres</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing semantic coherence in topic models. In Proceedings of the 2011 Conference on Emperical Methods in Natural Language Processing, pages 262–272, Edinburgh, Scotland, UK. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Youn Noh</author>
<author>Edmund Talley</author>
<author>Sarvnaz Karimi</author>
<author>Timothy Baldwin</author>
</authors>
<title>Evaluating topic models for digital libraries.</title>
<date>2010</date>
<booktitle>In Proceedings of the 10th annual joint conference on Digital libraries, JCDL ’10,</booktitle>
<pages>215--224</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3660" citStr="Newman et al., 2010" startWordPosition="547" endWordPosition="550">Landauer and Dutnais, 1997), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 2000). This has made it difficult to know which method is most useful for a given application, or in terms of extracting useful topics. We now provide a comprehensive and automated evaluation of these three distinct models (LDA, LSA, NMF), for automatically learning semantic topics. While these models have seen significant improvements, they still represent the core differences between each approach to modeling topics. For our evaluation, we use two recent automated coherence measures (Mimno et al., 2011; Newman et al., 2010) 952 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 952–961, Jeju Island, Korea, 12–14 July 2012. c�2012 Association for Computational Linguistics originally designed for LDA that bridge the gap between comparisons to human judgments and intrinsic measures such as perplexity. We consider several key questions: 1. How many topics should be learned? 2. How many learned topics are useful? 3. How do these topics relate to often used semantic tests? 4. How well do these topics identify similar documents</context>
<context position="11380" citStr="Newman et al., 2010" startWordPosition="1810" endWordPosition="1813">wing iterative update rules MHT WT M W = W WHHT H = HWT WH 3 Coherence Measures Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference, see Table 1 for examples ordered by the UCI measure. For our evaluations, we consider two new coherence measures designed for LDA, both of which have been shown to match well with human judgements of topic quality: (1) The UCI measure (Newman et al., 2010) and (2) The UMass measure (Mimno et al., 2011). Both measures compute the coherence of a topic as the sum of pairwise distributional similarity 3We note that the alternative KL-Divergence form of NMF has been directly linked to PLSA (Ding et al., 2008) 954 scores over the set of topic words, V . We generalize this as �coherence(V ) = score(vi, vj, E) (vi,vj)EV where V is a set of word describing the topic and c indicates a smoothing factor which guarantees that score returns real numbers. (We will be exploring the effect of the choice of c; the original authors used E = 1.) The UCI metric def</context>
<context position="18187" citStr="Newman et al., 2010" startWordPosition="3009" endWordPosition="3012">ce with E = 10−12 Figure 4: Average Topic Coherence of the top 10% topics with E = 10−12 100 200 300 400 500 100 200 300 400 500 Number of topics Number of topics (a) UMass (b) UCI Average Coherence of top 10% −1 −2 −3 −4 −5 0 Average Coherence of top 10% −10 −2 −4 −6 −8 2 0 ..ate LDA NMF SVD of topics having a low coherence, NMF appears to be learning more low quality topics once it’s learned the first 100 topics, whereas LDA learns fewer low quality topics in general. 4.2 Word Similarity Tasks The initial evaluations for each coherence measure asked human judges to directly evaluate topics (Newman et al., 2010; Mimno et al., 2011). We expand upon this comparison to human judgments by considering word similarity tasks that have often been used to evaluate distributional semantic spaces (Jurgens and Stevens, 2010). Here, we use the learned topics as generalized semantics describing our knowledge about words. If a model’s topics generalize the knowledge accurately, we would expect similar words, such as “cat” and “dog”, to be represented with a similar set of topics. Rather than evaluating individual topics, this similarity task considers the knowledge within the entire set of topics, the topics act a</context>
</contexts>
<marker>Newman, Noh, Talley, Karimi, Baldwin, 2010</marker>
<rawString>David Newman, Youn Noh, Edmund Talley, Sarvnaz Karimi, and Timothy Baldwin. 2010. Evaluating topic models for digital libraries. In Proceedings of the 10th annual joint conference on Digital libraries, JCDL ’10, pages 215–224, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Paul Pauca</author>
<author>Farial Shahnaz</author>
<author>Michael W Berry</author>
<author>Robert J Plemmons</author>
</authors>
<title>Text mining using nonnegative matrix factorizations,</title>
<date>2004</date>
<volume>54</volume>
<pages>452--456</pages>
<publisher>SIAM.</publisher>
<contexts>
<context position="7754" citStr="Pauca et al., 2004" startWordPosition="1222" endWordPosition="1225">cs by first forming a traditional term by document matrix used in information retrieval and then smoothing the counts to enhance the weight of informative words. Based on the original LSA model, we use the LogEntropy transform. LSA then decomposes this smoothed, term by document matrix in order to generalize observed relations between words and documents. For both LSA models, we used implementations found in the S-Space package.2 Traditionally, LSA has used the Singular Value Decomposition, but we also consider Non-negative Matrix Factorization as we’ve seen NMF applied in similar situations (Pauca et al., 2004) and others lhttp://mallet.cs.umass.edu/ zhttps://github.com/fozziethebeat/S-Space 953 Model Label Top Words High Quality Topics LDA interview told asked wanted interview people made thought time called knew wine wine wines bottle grapes made winery cabernet grape pinot red NMF grilling grilled sweet spicy fried pork dish shrimp menu dishes sauce cloning embryonic cloned embryo human research stem embryos cell cloning cells SVD cooking sauce food restaurant water oil salt chicken pepper wine cup stocks fund funds investors weapons stocks mutual stock movie film show Low Quality Topics LDA rate</context>
</contexts>
<marker>Pauca, Shahnaz, Berry, Plemmons, 2004</marker>
<rawString>V Paul Pauca, Farial Shahnaz, Michael W Berry, and Robert J Plemmons, 2004. Text mining using nonnegative matrix factorizations, volume 54, pages 452–456. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Commun. ACM,</journal>
<pages>8--627</pages>
<contexts>
<context position="18893" citStr="Rubenstein and Goodenough (1965)" startWordPosition="3123" endWordPosition="3126">y considering word similarity tasks that have often been used to evaluate distributional semantic spaces (Jurgens and Stevens, 2010). Here, we use the learned topics as generalized semantics describing our knowledge about words. If a model’s topics generalize the knowledge accurately, we would expect similar words, such as “cat” and “dog”, to be represented with a similar set of topics. Rather than evaluating individual topics, this similarity task considers the knowledge within the entire set of topics, the topics act as more compact representation for the known words in a corpus. We use the Rubenstein and Goodenough (1965) and Finkelstein et al. (2002) word similarity tasks. In each task, human judges were asked to evaluate the similarity or relatedness between different sets of word pairs. Fifty-One Evaluators for the Rubenstein and Goodenough (1965) dataset were given 65 pairs 957 model LDA NMF SVD 100 200 300 400 500 T (a) Rubenstein &amp; Goodenough (b) Wordsim 353/Finklestein et. al. 100 200 300 400 500 T score 0.5 0.4 0.3 0.2 0.1 0.0 model LDA NMF SVD score 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Figure 5: Word Similarity Evaluations for each model Figure 7: Correlation between topic coherence and topic ranking in classi</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM, 8:627–633, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<date>2008</date>
<publisher>The</publisher>
<location>New York Times Annotated Corpus.</location>
<contexts>
<context position="13500" citStr="Sandhaus, 2008" startWordPosition="2178" endWordPosition="2180">We evaluate the quality of our three topic models (LDA, SVD, and NMF) with three experiments. We focus first on evaluating aggregate coherence methods for a complete topic model and consider the differences between each model as we learn an increasing number of topics. Secondly, we compare coherence scores to previous semantic evaluations. Lastly, we use the learned topics in a classification task and evaluate whether or not coherent topics are equally informative when discriminating between documents. For all our experiments, we trained our models on 92,600 New York Times articles from 2003 (Sandhaus, 2008). For all articles, we removed stop words and any words occurring less than 200 times in the corpus, which left 35,836 unique tokens. All documents were tokenized with OpenNLP’s MaxEnt4 tokenizer. For the UCI measure, we compute the PMI between words using a 20 word sliding window passed over the WaCkypedia corpus (Baroni et al., 2009). In all experiments, we compute the coherence with the top 10 words from each topic that had the highest weight, in terms of LDA and NMF this corresponds with a high probability of the term describing the topic but for SVD there is no clear semantic interpretati</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times Annotated Corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Marianna Apidianaki</author>
</authors>
<title>Latent semantic word sense induction and disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>1476--1485</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Van de Cruys, Apidianaki, 2011</marker>
<rawString>Tim Van de Cruys and Marianna Apidianaki. 2011. Latent semantic word sense induction and disambiguation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 1476–1485, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation methods for topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning (ICML).</booktitle>
<publisher>Omnipress.</publisher>
<contexts>
<context position="2727" citStr="Wallach et al., 2009" startWordPosition="406" endWordPosition="409">odels has been ad hoc and application specific. Evaluations have ranged from fully automated intrinsic evaluations to manually crafted extrinsic evaluations. Previous extrinsic evaluations have used the learned topics to compactly represent a small fixed vocabulary and compared this distributional space to human judgments of similarity (Jurgens and Stevens, 2010). But these evaluations are hand constructed and often costly to perform for domain-specific topics. Conversely, intrinsic measures have evaluated the amount of information encoded by the topics, where perplexity is one common example(Wallach et al., 2009), however, Chang et al. (2009) found that these intrinsic measures do not always correlate with semantically interpretable topics. Furthermore, few evaluations have used the same metrics to compare distinct approaches such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Latent Semantic Analysis (LSA) (Landauer and Dutnais, 1997), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 2000). This has made it difficult to know which method is most useful for a given application, or in terms of extracting useful topics. We now provide a comprehensive and automated evaluation of the</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. Evaluation methods for topic models. In Proceedings of the 26th International Conference on Machine Learning (ICML). Omnipress.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>